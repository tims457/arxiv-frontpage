{"created":"2024-06-07 17:59:59","title":"3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs","abstract":"The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research. Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs. Project website: https://3d-grand.github.io","sentences":["The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world.","While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages.","A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes.","In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions.","Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs.","As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models.","Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research.","Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans.","Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs.","Project website: https://3d-grand.github.io"],"url":"http://arxiv.org/abs/2406.05132v1","category":"cs.CV"}
{"created":"2024-06-07 17:58:36","title":"DVOS: Self-Supervised Dense-Pattern Video Object Segmentation","abstract":"Video object segmentation approaches primarily rely on large-scale pixel-accurate human-annotated datasets for model development. In Dense Video Object Segmentation (DVOS) scenarios, each video frame encompasses hundreds of small, dense, and partially occluded objects. Accordingly, the labor-intensive manual annotation of even a single frame often takes hours, which hinders the development of DVOS for many applications. Furthermore, in videos with dense patterns, following a large number of objects that move in different directions poses additional challenges. To address these challenges, we proposed a semi-self-supervised spatiotemporal approach for DVOS utilizing a diffusion-based method through multi-task learning. Emulating real videos' optical flow and simulating their motion, we developed a methodology to synthesize computationally annotated videos that can be used for training DVOS models; The model performance was further improved by utilizing weakly labeled (computationally generated but imprecise) data. To demonstrate the utility and efficacy of the proposed approach, we developed DVOS models for wheat head segmentation of handheld and drone-captured videos, capturing wheat crops in fields of different locations across various growth stages, spanning from heading to maturity. Despite using only a few manually annotated video frames, the proposed approach yielded high-performing models, achieving a Dice score of 0.82 when tested on a drone-captured external test set. While we showed the efficacy of the proposed approach for wheat head segmentation, its application can be extended to other crops or DVOS in other domains, such as crowd analysis or microscopic image analysis.","sentences":["Video object segmentation approaches primarily rely on large-scale pixel-accurate human-annotated datasets for model development.","In Dense Video Object Segmentation (DVOS) scenarios, each video frame encompasses hundreds of small, dense, and partially occluded objects.","Accordingly, the labor-intensive manual annotation of even a single frame often takes hours, which hinders the development of DVOS for many applications.","Furthermore, in videos with dense patterns, following a large number of objects that move in different directions poses additional challenges.","To address these challenges, we proposed a semi-self-supervised spatiotemporal approach for DVOS utilizing a diffusion-based method through multi-task learning.","Emulating real videos' optical flow and simulating their motion, we developed a methodology to synthesize computationally annotated videos that can be used for training DVOS models; The model performance was further improved by utilizing weakly labeled (computationally generated but imprecise) data.","To demonstrate the utility and efficacy of the proposed approach, we developed DVOS models for wheat head segmentation of handheld and drone-captured videos, capturing wheat crops in fields of different locations across various growth stages, spanning from heading to maturity.","Despite using only a few manually annotated video frames, the proposed approach yielded high-performing models, achieving a Dice score of 0.82 when tested on a drone-captured external test set.","While we showed the efficacy of the proposed approach for wheat head segmentation, its application can be extended to other crops or DVOS in other domains, such as crowd analysis or microscopic image analysis."],"url":"http://arxiv.org/abs/2406.05131v1","category":"cs.CV"}
{"created":"2024-06-07 17:58:11","title":"An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models","abstract":"Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.","sentences":["Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks.","However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters.","To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs.","We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained.","This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs.","We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination.","We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets.","Across all experiments, we show that the adapter is the best-performing PEFT method.","At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs.","Code and data are available at https://github.com/alenai97/PEFT-MLLM.git."],"url":"http://arxiv.org/abs/2406.05130v1","category":"cs.CL"}
{"created":"2024-06-07 17:55:11","title":"GR-Athena++: magnetohydrodynamical evolution with dynamical space-time","abstract":"We present a self-contained overview of GR-Athena++, a general-relativistic magnetohydrodynamics (GRMHD) code, that incorporates treatment of dynamical space-time, based on the recent work of (Daszuta+, 2021)[49] and (Cook+, 2023)[45].   General aspects of the Athena++ framework we build upon, such as oct-tree based, adaptive mesh refinement (AMR) and constrained transport, together with our modifications, incorporating the Z4c formulation of numerical relativity, judiciously coupled, enables GRMHD with dynamical space-times.   Initial verification testing of GR-Athena++ is performed through benchmark problems that involve isolated and binary neutron star space-times. This leads to stable and convergent results. Gravitational collapse of a rapidly rotating star through black hole formation is shown to be correctly handled. In the case of non-rotating stars, magnetic field instabilities are demonstrated to be correctly captured with total relative violation of the divergence-free constraint remaining near machine precision.   The use of AMR is show-cased through investigation of the Kelvin-Helmholtz instability which is resolved at the collisional interface in a merger of magnetised binary neutron stars.   The underlying task-based computational model enables GR-Athena++ to achieve strong scaling efficiencies above $80\\%$ in excess of $10^5$ CPU cores and excellent weak scaling up to $\\sim 5 \\times 10^5$ CPU cores in a realistic production setup. GR-Athena++ thus provides a viable path towards robust simulation of GRMHD flows in strong and dynamical gravity with exascale high performance computational infrastructure.","sentences":["We present a self-contained overview of GR-Athena++, a general-relativistic magnetohydrodynamics (GRMHD) code, that incorporates treatment of dynamical space-time, based on the recent work of (Daszuta+, 2021)[49] and (Cook+, 2023)[45].   ","General aspects of the Athena++ framework we build upon, such as oct-tree based, adaptive mesh refinement (AMR) and constrained transport, together with our modifications, incorporating the Z4c formulation of numerical relativity, judiciously coupled, enables GRMHD with dynamical space-times.   ","Initial verification testing of GR-Athena++ is performed through benchmark problems that involve isolated and binary neutron star space-times.","This leads to stable and convergent results.","Gravitational collapse of a rapidly rotating star through black hole formation is shown to be correctly handled.","In the case of non-rotating stars, magnetic field instabilities are demonstrated to be correctly captured with total relative violation of the divergence-free constraint remaining near machine precision.   ","The use of AMR is show-cased through investigation of the Kelvin-Helmholtz instability which is resolved at the collisional interface in a merger of magnetised binary neutron stars.   ","The underlying task-based computational model enables GR-Athena++ to achieve strong scaling efficiencies above $80\\%$ in excess of $10^5$ CPU cores and excellent weak scaling up to $\\sim 5 \\times 10^5$ CPU cores in a realistic production setup.","GR-Athena++ thus provides a viable path towards robust simulation of GRMHD flows in strong and dynamical gravity with exascale high performance computational infrastructure."],"url":"http://arxiv.org/abs/2406.05126v1","category":"gr-qc"}
{"created":"2024-06-07 17:52:23","title":"Energy Propagation in Scattering Convolution Networks Can Be Arbitrarily Slow","abstract":"We analyze energy decay for deep convolutional neural networks employed as feature extractors, such as Mallat's wavelet scattering transform. For time-frequency scattering transforms based on Gabor filters, it has been established that energy decay is exponential, for arbitrary square-integrable input signals. Our main results allow to prove that this is wrong for wavelet scattering in arbitrary dimensions. In this setting, the energy decay of the scattering transform acting on a generic square-integrable signal turns out to be arbitrarily slow. The fact that this behavior holds for dense subsets of $L^2(\\mathbb{R}^d)$ emphasizes that fast energy decay is generally not a stable property of signals.   We complement these findings with positive results allowing to conclude fast (up to exponential) energy decay for generalized Sobolev spaces that are tailored to the frequency localization of the underlying filter bank.   Both negative and positive results highlight that energy decay in scattering networks critically depends on the interplay of the respective frequency localizations of the signal on the one hand, and of the employed filters on the other.","sentences":["We analyze energy decay for deep convolutional neural networks employed as feature extractors, such as Mallat's wavelet scattering transform.","For time-frequency scattering transforms based on Gabor filters, it has been established that energy decay is exponential, for arbitrary square-integrable input signals.","Our main results allow to prove that this is wrong for wavelet scattering in arbitrary dimensions.","In this setting, the energy decay of the scattering transform acting on a generic square-integrable signal turns out to be arbitrarily slow.","The fact that this behavior holds for dense subsets of $L^2(\\mathbb{R}^d)$ emphasizes that fast energy decay is generally not a stable property of signals.   ","We complement these findings with positive results allowing to conclude fast (up to exponential) energy decay for generalized Sobolev spaces that are tailored to the frequency localization of the underlying filter bank.   ","Both negative and positive results highlight that energy decay in scattering networks critically depends on the interplay of the respective frequency localizations of the signal on the one hand, and of the employed filters on the other."],"url":"http://arxiv.org/abs/2406.05121v1","category":"math.FA"}
{"created":"2024-06-07 17:50:18","title":"Contextual fusion enhances robustness to image blurring","abstract":"Mammalian brains handle complex reasoning by integrating information across brain regions specialized for particular sensory modalities. This enables improved robustness and generalization versus deep neural networks, which typically process one modality and are vulnerable to perturbations. While defense methods exist, they do not generalize well across perturbations. We developed a fusion model combining background and foreground features from CNNs trained on Imagenet and Places365. We tested its robustness to human-perceivable perturbations on MS COCO. The fusion model improved robustness, especially for classes with greater context variability. Our proposed solution for integrating multiple modalities provides a new approach to enhance robustness and may be complementary to existing methods.","sentences":["Mammalian brains handle complex reasoning by integrating information across brain regions specialized for particular sensory modalities.","This enables improved robustness and generalization versus deep neural networks, which typically process one modality and are vulnerable to perturbations.","While defense methods exist, they do not generalize well across perturbations.","We developed a fusion model combining background and foreground features from CNNs trained on Imagenet and Places365.","We tested its robustness to human-perceivable perturbations on MS COCO.","The fusion model improved robustness, especially for classes with greater context variability.","Our proposed solution for integrating multiple modalities provides a new approach to enhance robustness and may be complementary to existing methods."],"url":"http://arxiv.org/abs/2406.05120v1","category":"cs.CV"}
{"created":"2024-06-07 17:48:12","title":"An event generator for Lepton-Hadron Deep Inelastic Scattering at NLO+PS with POWHEG including mass effects","abstract":"We present a generator for lepton nucleon collisions in the DIS regime, focusing in particular on processes with a massive lepton and/or a massive quark in the final state. We have built a full code matching NLO QCD corrections to parton shower Monte Carlo programs in the POWHEG-BOX framework. Our code can be used to compute NLO+PS accurate fully differential predictions for neutral current and charged current processes, including processes with an incoming tau neutrino, and/or including charm quarks in the final state. We also made comparisons with available data and predictions for the new neutrino experiments at CERN.","sentences":["We present a generator for lepton nucleon collisions in the DIS regime, focusing in particular on processes with a massive lepton and/or a massive quark in the final state.","We have built a full code matching NLO QCD corrections to parton shower Monte Carlo programs in the POWHEG-BOX framework.","Our code can be used to compute NLO+PS accurate fully differential predictions for neutral current and charged current processes, including processes with an incoming tau neutrino, and/or including charm quarks in the final state.","We also made comparisons with available data and predictions for the new neutrino experiments at CERN."],"url":"http://arxiv.org/abs/2406.05115v1","category":"hep-ph"}
{"created":"2024-06-07 17:44:32","title":"LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment","abstract":"We introduce LlavaGuard, a family of VLM-based safeguard models, offering a versatile framework for evaluating the safety compliance of visual content. Specifically, we designed LlavaGuard for dataset annotation and generative model safeguarding. To this end, we collected and annotated a high-quality visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs on context-aware safety risks. As a key innovation, LlavaGuard's new responses contain comprehensive information, including a safety rating, the violated safety categories, and an in-depth rationale. Further, our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios. Our experiments highlight the capabilities of LlavaGuard in complex and real-world applications. We provide checkpoints ranging from 7B to 34B parameters demonstrating state-of-the-art performance, with even the smallest models outperforming baselines like GPT-4. We make our dataset and model weights publicly available and invite further research to address the diverse needs of communities and contexts.","sentences":["We introduce LlavaGuard, a family of VLM-based safeguard models, offering a versatile framework for evaluating the safety compliance of visual content.","Specifically, we designed LlavaGuard for dataset annotation and generative model safeguarding.","To this end, we collected and annotated a high-quality visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs on context-aware safety risks.","As a key innovation, LlavaGuard's new responses contain comprehensive information, including a safety rating, the violated safety categories, and an in-depth rationale.","Further, our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios.","Our experiments highlight the capabilities of LlavaGuard in complex and real-world applications.","We provide checkpoints ranging from 7B to 34B parameters demonstrating state-of-the-art performance, with even the smallest models outperforming baselines like GPT-4.","We make our dataset and model weights publicly available and invite further research to address the diverse needs of communities and contexts."],"url":"http://arxiv.org/abs/2406.05113v1","category":"cs.CV"}
{"created":"2024-06-07 17:42:41","title":"Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place","abstract":"As the permeability of AI systems in interpersonal domains like the home expands, their technical capabilities of generating explanations are required to be aligned with user expectations for transparency and reasoning. This paper presents insights from our ongoing work in understanding the effectiveness of explanations in Conversational AI systems for older adults aging in place and their family caregivers. We argue that in collaborative and multi-user environments like the home, AI systems will make recommendations based on a host of information sources to generate explanations. These sources may be more or less salient based on user mental models of the system and the specific task. We highlight the need for cross technological collaboration between AI systems and other available sources of information in the home to generate multiple explanations for a single user query. Through example scenarios in a caregiving home setting, this paper provides an initial framework for categorizing these sources and informing a potential design space for AI explanations surrounding everyday tasks in the home.","sentences":["As the permeability of AI systems in interpersonal domains like the home expands, their technical capabilities of generating explanations are required to be aligned with user expectations for transparency and reasoning.","This paper presents insights from our ongoing work in understanding the effectiveness of explanations in Conversational AI systems for older adults aging in place and their family caregivers.","We argue that in collaborative and multi-user environments like the home, AI systems will make recommendations based on a host of information sources to generate explanations.","These sources may be more or less salient based on user mental models of the system and the specific task.","We highlight the need for cross technological collaboration between AI systems and other available sources of information in the home to generate multiple explanations for a single user query.","Through example scenarios in a caregiving home setting, this paper provides an initial framework for categorizing these sources and informing a potential design space for AI explanations surrounding everyday tasks in the home."],"url":"http://arxiv.org/abs/2406.05111v1","category":"cs.HC"}
{"created":"2024-06-07 17:41:47","title":"Large Generative Graph Models","abstract":"Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains. This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content. However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields. To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains. We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models. Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization. Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e., \"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e., \"The graph has a low average degree, suitable for modeling social media interactions.\"). This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs. We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/.","sentences":["Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains.","This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content.","However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields.","To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains.","We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models.","Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization.","Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e., \"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e., \"The graph has a low average degree, suitable for modeling social media interactions.\").","This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs.","We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/."],"url":"http://arxiv.org/abs/2406.05109v1","category":"cs.LG"}
{"created":"2024-06-07 17:37:05","title":"LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration","abstract":"Data exploration is a challenging process in which users examine a dataset by iteratively employing a series of queries. While in some cases the user explores a new dataset to become familiar with it, more often, the exploration process is conducted with a specific analysis goal or question in mind. To assist users in exploring a new dataset, Automated Data Exploration (ADE) systems have been devised in previous work. These systems aim to auto-generate a full exploration session, containing a sequence of queries that showcase interesting elements of the data. However, existing ADE systems are often constrained by a predefined objective function, thus always generating the same session for a given dataset. Therefore, their effectiveness in goal-oriented exploration, in which users need to answer specific questions about the data, are extremely limited.   To this end, this paper presents LINX, a generative system augmented with a natural language interface for goal-oriented ADE. Given an input dataset and an analytical goal described in natural language, LINX generates a personalized exploratory session that is relevant to the user's goal. LINX utilizes a Large Language Model (LLM) to interpret the input analysis goal, and then derive a set of specifications for the desired output exploration session. These specifications are then transferred to a novel, modular ADE engine based on Constrained Deep Reinforcement Learning (CDRL), which can adapt its output according to the specified instructions.   To validate LINX's effectiveness, we introduce a new benchmark dataset for goal-oriented exploration and conduct an extensive user study. Our analysis underscores LINX's superior capability in producing exploratory notebooks that are significantly more relevant and beneficial than those generated by existing solutions, including ChatGPT, goal-agnostic ADE, and commercial systems.","sentences":["Data exploration is a challenging process in which users examine a dataset by iteratively employing a series of queries.","While in some cases the user explores a new dataset to become familiar with it, more often, the exploration process is conducted with a specific analysis goal or question in mind.","To assist users in exploring a new dataset, Automated Data Exploration (ADE) systems have been devised in previous work.","These systems aim to auto-generate a full exploration session, containing a sequence of queries that showcase interesting elements of the data.","However, existing ADE systems are often constrained by a predefined objective function, thus always generating the same session for a given dataset.","Therefore, their effectiveness in goal-oriented exploration, in which users need to answer specific questions about the data, are extremely limited.   ","To this end, this paper presents LINX, a generative system augmented with a natural language interface for goal-oriented ADE.","Given an input dataset and an analytical goal described in natural language, LINX generates a personalized exploratory session that is relevant to the user's goal.","LINX utilizes a Large Language Model (LLM) to interpret the input analysis goal, and then derive a set of specifications for the desired output exploration session.","These specifications are then transferred to a novel, modular ADE engine based on Constrained Deep Reinforcement Learning (CDRL), which can adapt its output according to the specified instructions.   ","To validate LINX's effectiveness, we introduce a new benchmark dataset for goal-oriented exploration and conduct an extensive user study.","Our analysis underscores LINX's superior capability in producing exploratory notebooks that are significantly more relevant and beneficial than those generated by existing solutions, including ChatGPT, goal-agnostic ADE, and commercial systems."],"url":"http://arxiv.org/abs/2406.05107v1","category":"cs.DB"}
{"created":"2024-06-07 17:28:37","title":"Canonicalizing zeta generators: genus zero and genus one","abstract":"Zeta generators are derivations associated with odd Riemann zeta values that act freely on the Lie algebra of the fundamental group of Riemann surfaces with marked points. The genus-zero incarnation of zeta generators are Ihara derivations of certain Lie polynomials in two generators that can be obtained from the Drinfeld associator. We characterize a canonical choice of these polynomials, together with their non-Lie counterparts at even degrees $w\\geq 2$, through the action of the dual space of formal and motivic multizeta values. Based on these canonical polynomials, we propose a canonical isomorphism that maps motivic multizeta values into the $f$-alphabet. The canonical Lie polynomials from the genus-zero setup determine canonical zeta generators in genus one that act on the two generators of Enriquez' elliptic associators. Up to a single contribution at fixed degree, the zeta generators in genus one are systematically expanded in terms of Tsunogai's geometric derivations dual to holomorphic Eisenstein series, leading to a wealth of explicit high-order computations. Earlier ambiguities in defining the non-geometric part of genus-one zeta generators are resolved by imposing a new representation-theoretic condition. The tight interplay between zeta generators in genus zero and genus one unravelled in this work connects the construction of single-valued multiple polylogarithms on the sphere with iterated-Eisenstein-integral representations of modular graph forms.","sentences":["Zeta generators are derivations associated with odd Riemann zeta values that act freely on the Lie algebra of the fundamental group of Riemann surfaces with marked points.","The genus-zero incarnation of zeta generators are Ihara derivations of certain Lie polynomials in two generators that can be obtained from the Drinfeld associator.","We characterize a canonical choice of these polynomials, together with their non-Lie counterparts at even degrees $w\\geq 2$, through the action of the dual space of formal and motivic multizeta values.","Based on these canonical polynomials, we propose a canonical isomorphism that maps motivic multizeta values into the $f$-alphabet.","The canonical Lie polynomials from the genus-zero setup determine canonical zeta generators in genus one that act on the two generators of Enriquez' elliptic associators.","Up to a single contribution at fixed degree, the zeta generators in genus one are systematically expanded in terms of Tsunogai's geometric derivations dual to holomorphic Eisenstein series, leading to a wealth of explicit high-order computations.","Earlier ambiguities in defining the non-geometric part of genus-one zeta generators are resolved by imposing a new representation-theoretic condition.","The tight interplay between zeta generators in genus zero and genus one unravelled in this work connects the construction of single-valued multiple polylogarithms on the sphere with iterated-Eisenstein-integral representations of modular graph forms."],"url":"http://arxiv.org/abs/2406.05099v1","category":"math.QA"}
{"created":"2024-06-07 17:21:10","title":"A Novel Time Series-to-Image Encoding Approach for Weather Phenomena Classification","abstract":"Rainfall estimation through the analysis of its impact on electromagnetic waves has sparked increasing interest in the research community. Recent studies have delved into its effects on cellular network performance, demonstrating the potential to forecast rainfall levels based on electromagnetic wave attenuation during precipitations. This paper aims to solve the problem of identifying the nature of specific weather phenomena from the received signal level (RSL) in 4G/LTE mobile terminals. Specifically, utilizing time-series data representing RSL, we propose a novel approach to encode time series as images and model the task as an image classification problem, which we finally address using convolutional neural networks (CNNs). The main benefit of the abovementioned procedure is the opportunity to utilize various data augmentation techniques simultaneously. This encompasses applying traditional approaches, such as moving averages, to the time series and enhancing the generated images. We have investigated various image data augmentation methods to identify the most effective combination for this scenario. In the upcoming sections, we will introduce the task of rainfall estimation and conduct a comprehensive analysis of the dataset used. Subsequently, we will formally propose a new approach for converting time series into images. To conclude, the paper's final section will present and discuss the experiments conducted, providing the reader with a brief yet comprehensive overview of the results.","sentences":["Rainfall estimation through the analysis of its impact on electromagnetic waves has sparked increasing interest in the research community.","Recent studies have delved into its effects on cellular network performance, demonstrating the potential to forecast rainfall levels based on electromagnetic wave attenuation during precipitations.","This paper aims to solve the problem of identifying the nature of specific weather phenomena from the received signal level (RSL) in 4G/LTE mobile terminals.","Specifically, utilizing time-series data representing RSL, we propose a novel approach to encode time series as images and model the task as an image classification problem, which we finally address using convolutional neural networks (CNNs).","The main benefit of the abovementioned procedure is the opportunity to utilize various data augmentation techniques simultaneously.","This encompasses applying traditional approaches, such as moving averages, to the time series and enhancing the generated images.","We have investigated various image data augmentation methods to identify the most effective combination for this scenario.","In the upcoming sections, we will introduce the task of rainfall estimation and conduct a comprehensive analysis of the dataset used.","Subsequently, we will formally propose a new approach for converting time series into images.","To conclude, the paper's final section will present and discuss the experiments conducted, providing the reader with a brief yet comprehensive overview of the results."],"url":"http://arxiv.org/abs/2406.05096v1","category":"cs.CV"}
{"created":"2024-06-07 17:17:48","title":"On orientations with forbidden out-degrees","abstract":"Let $G$ be a $d$-regular graph and let $F\\subseteq\\{0, 1, 2, \\ldots, d\\}$ be a list of forbidden out-degrees. Akbari, Dalirrooyfard, Ehsani, Ozeki, and Sherkati conjectured that if $|F|<\\tfrac{1}{2}d$, then $G$ should admit an $F$-avoiding orientation, i.e., an orientation where no out-degrees are in the forbidden list $F$. The conjecture is known for $d\\leq 4$ due to work of Ma and Lu, and here we extend this to $d\\leq 6$. The conjecture has also been studied in a generalized version, where $d, F$ are changed from constant values to functions $d(v), F(v)$ that vary over all $v\\in V(G)$. We provide support for this generalized version by verifying it for some new cases, including when $G$ is 2-degenerate and when every $F(v)$ has some specific structure.","sentences":["Let $G$ be a $d$-regular graph and let $F\\subseteq\\{0, 1, 2, \\ldots, d\\}$ be a list of forbidden out-degrees.","Akbari, Dalirrooyfard, Ehsani, Ozeki, and Sherkati conjectured that if $|F|<\\tfrac{1}{2}d$, then $G$ should admit an $F$-avoiding orientation, i.e., an orientation where no out-degrees are in the forbidden list $F$.","The conjecture is known for $d\\leq 4$ due to work of Ma and Lu, and here we extend this to $d\\leq 6$.","The conjecture has also been studied in a generalized version, where $d, F$ are changed from constant values to functions $d(v), F(v)$ that vary over all $v\\in V(G)$. We provide support for this generalized version by verifying it for some new cases, including when $G$ is 2-degenerate and when every $F(v)$ has some specific structure."],"url":"http://arxiv.org/abs/2406.05095v1","category":"math.CO"}
{"created":"2024-06-07 17:04:23","title":"Searching for stellar population in the molecular cloud GRSMC 045.49+00.04","abstract":"Understanding the characteristics of young stellar populations is essential for deriving insights into star formation processes within parent molecular clouds and the influence of massive stars. This study focuses on YSOs in the G 045.49+00.04 molecular cloud, including three UC HII regions. Using infrared photometric data and radiation models, we identified 1482 YSOs with masses ranging from 1.5 to 22 Msol. Among these, 315 form dense clusters near IRAS sources, with high-mass stars responsible for ionization. The non-cluster YSOs (1168) are uniformly distributed on the field. The distribution of YSOs from both samples on the colour-magnitude diagram and by the evolutionary ages is different. About 75% of objects in the IRAS clusters are concentrated around the ZAMS and have a well-defined peak at an age of Log(Age[years]) = 6.75, with a narrow spread. The non-cluster objects have two concentrations located to the right and left of the 0.1 Myr isochrone and two well-defined peaks at Log(Age) = 6.25 and 5.25. The K luminosity functions alpha slopes for the IRAS clusters and non-cluster objects are 0.32 and 0.72, respectively. The steeper alpha slope is suggesting that the non-cluster objects are less evolved, which is well consistent with the evolutionary age. Similar patterns were observed in the neighboring G 45.14+00.14 region. The both regions (G 045.49+00.04 and G 45.14+00.14) are located and distinguished by their brightness and density at the edge of the bubble around the highly variable X-ray binary GRS 1915+105, which includes a black hole and a K-giant companion. Based on the above, we can assume that the process of star formation in the young IRAS clusters was triggered by the GRS 915+105-initiated shock front inside the ISM massive condensation, through the process of \"collecting and collapse\". Most non-cluster objects probably belong to a later generation.","sentences":["Understanding the characteristics of young stellar populations is essential for deriving insights into star formation processes within parent molecular clouds and the influence of massive stars.","This study focuses on YSOs in the G 045.49+00.04 molecular cloud, including three UC HII regions.","Using infrared photometric data and radiation models, we identified 1482 YSOs with masses ranging from 1.5 to 22 Msol.","Among these, 315 form dense clusters near IRAS sources, with high-mass stars responsible for ionization.","The non-cluster YSOs (1168) are uniformly distributed on the field.","The distribution of YSOs from both samples on the colour-magnitude diagram and by the evolutionary ages is different.","About 75% of objects in the IRAS clusters are concentrated around the ZAMS and have a well-defined peak at an age of Log(Age[years])","= 6.75, with a narrow spread.","The non-cluster objects have two concentrations located to the right and left of the 0.1 Myr isochrone and two well-defined peaks at Log(Age)","= 6.25 and 5.25.","The K luminosity functions alpha slopes for the IRAS clusters and non-cluster objects are 0.32 and 0.72, respectively.","The steeper alpha slope is suggesting that the non-cluster objects are less evolved, which is well consistent with the evolutionary age.","Similar patterns were observed in the neighboring G 45.14+00.14 region.","The both regions (G 045.49+00.04 and G 45.14+00.14) are located and distinguished by their brightness and density at the edge of the bubble around the highly variable X-ray binary GRS 1915+105, which includes a black hole and a K-giant companion.","Based on the above, we can assume that the process of star formation in the young IRAS clusters was triggered by the GRS 915+105-initiated shock front inside the ISM massive condensation, through the process of \"collecting and collapse\".","Most non-cluster objects probably belong to a later generation."],"url":"http://arxiv.org/abs/2406.05091v1","category":"astro-ph.GA"}
{"created":"2024-06-07 17:03:43","title":"Provably Better Explanations with Optimized Aggregation of Feature Attributions","abstract":"Using feature attributions for post-hoc explanations is a common practice to understand and verify the predictions of opaque machine learning models. Despite the numerous techniques available, individual methods often produce inconsistent and unstable results, putting their overall reliability into question. In this work, we aim to systematically improve the quality of feature attributions by combining multiple explanations across distinct methods or their variations. For this purpose, we propose a novel approach to derive optimal convex combinations of feature attributions that yield provable improvements of desired quality criteria such as robustness or faithfulness to the model behavior. Through extensive experiments involving various model architectures and popular feature attribution techniques, we demonstrate that our combination strategy consistently outperforms individual methods and existing baselines.","sentences":["Using feature attributions for post-hoc explanations is a common practice to understand and verify the predictions of opaque machine learning models.","Despite the numerous techniques available, individual methods often produce inconsistent and unstable results, putting their overall reliability into question.","In this work, we aim to systematically improve the quality of feature attributions by combining multiple explanations across distinct methods or their variations.","For this purpose, we propose a novel approach to derive optimal convex combinations of feature attributions that yield provable improvements of desired quality criteria such as robustness or faithfulness to the model behavior.","Through extensive experiments involving various model architectures and popular feature attribution techniques, we demonstrate that our combination strategy consistently outperforms individual methods and existing baselines."],"url":"http://arxiv.org/abs/2406.05090v1","category":"cs.LG"}
{"created":"2024-06-07 17:03:04","title":"Discovery of An Apparent Red, High-Velocity Type Ia Supernova at z = 2.9 with JWST","abstract":"We present the JWST discovery of SN 2023adsy, a transient object located in a host galaxy JADES-GS$+53.13485$$-$$27.82088$ with a host spectroscopic redshift of $2.903\\pm0.007$. The transient was identified in deep James Webb Space Telescope (JWST)/NIRCam imaging from the JWST Advanced Deep Extragalactic Survey (JADES) program. Photometric and spectroscopic followup with NIRCam and NIRSpec, respectively, confirm the redshift and yield UV-NIR light-curve, NIR color, and spectroscopic information all consistent with a Type Ia classification. Despite its classification as a likely SN Ia, SN 2023adsy is both fairly red (E(B-V)$\\sim0.9$) despite a host galaxy with low-extinction and has a high Ca II velocity ($19,000\\pm2,000$km/s) compared to the general population of SNe Ia. While these characteristics are consistent with some Ca-rich SNe Ia, particularly SN 2016hnk, SN 2023adsy is intrinsically brighter than the low-z Ca-rich population. Although such an object is too red for any low-z cosmological sample, we apply a fiducial standardization approach to SN 2023adsy and find that the SN 2023adsy luminosity distance measurement is in excellent agreement ($\\lesssim1\\sigma$) with $\\Lambda$CDM. Therefore unlike low-z Ca-rich SNe Ia, SN 2023adsy is standardizable and gives no indication that SN Ia standardized luminosities change significantly with redshift. A larger sample of distant SNe Ia is required to determine if SN Ia population characteristics at high-z truly diverge from their low-z counterparts, and to confirm that standardized luminosities nevertheless remain constant with redshift.","sentences":["We present the JWST discovery of SN 2023adsy, a transient object located in a host galaxy JADES-GS$+53.13485$$-$$27.82088$ with a host spectroscopic redshift of $2.903\\pm0.007$. The transient was identified in deep James Webb Space Telescope (JWST)/NIRCam imaging from the JWST Advanced Deep Extragalactic Survey (JADES) program.","Photometric and spectroscopic followup with NIRCam and NIRSpec, respectively, confirm the redshift and yield UV-NIR light-curve, NIR color, and spectroscopic information all consistent with a Type Ia classification.","Despite its classification as a likely SN Ia, SN 2023adsy is both fairly red (E(B-V)$\\sim0.9$) despite a host galaxy with low-extinction and has a high Ca II velocity ($19,000\\pm2,000$km/s) compared to the general population of SNe Ia.","While these characteristics are consistent with some Ca-rich SNe Ia, particularly SN 2016hnk, SN 2023adsy is intrinsically brighter than the low-z Ca-rich population.","Although such an object is too red for any low-z cosmological sample, we apply a fiducial standardization approach to SN 2023adsy and find that the SN 2023adsy luminosity distance measurement is in excellent agreement ($\\lesssim1\\sigma$) with $\\Lambda$CDM.","Therefore unlike low-z Ca-rich SNe Ia, SN 2023adsy is standardizable and gives no indication that SN Ia standardized luminosities change significantly with redshift.","A larger sample of distant SNe Ia is required to determine if SN Ia population characteristics at high-z truly diverge from their low-z counterparts, and to confirm that standardized luminosities nevertheless remain constant with redshift."],"url":"http://arxiv.org/abs/2406.05089v1","category":"astro-ph.GA"}
{"created":"2024-06-07 17:02:35","title":"Corpus Poisoning via Approximate Greedy Gradient Descent","abstract":"Dense retrievers are widely used in information retrieval and have also been successfully extended to other knowledge intensive areas such as language models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they have recently been shown to be vulnerable to corpus poisoning attacks in which a malicious user injects a small fraction of adversarial passages into the retrieval corpus to trick the system into returning these passages among the top-ranked results for a broad set of user queries. Further study is needed to understand the extent to which these attacks could limit the deployment of dense retrievers in real-world applications. In this work, we propose Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval systems based on the widely used HotFlip method for efficiently generating adversarial passages. We demonstrate that AGGD can select a higher quality set of token-level perturbations than HotFlip by replacing its random token sampling with a more structured search. Experimentally, we show that our method achieves a high attack success rate on several datasets and using several retrievers, and can generalize to unseen queries and new domains. Notably, our method is extremely effective in attacking the ANCE retrieval model, achieving attack success rates that are 17.6\\% and 13.37\\% higher on the NQ and MS MARCO datasets, respectively, compared to HotFlip. Additionally, we demonstrate AGGD's potential to replace HotFlip in other adversarial attacks, such as knowledge poisoning of RAG systems.\\footnote{Code can be find in \\url{https://github.com/JinyanSu1/AGGD}}","sentences":["Dense retrievers are widely used in information retrieval and have also been successfully extended to other knowledge intensive areas such as language models, e.g., Retrieval-Augmented Generation (RAG) systems.","Unfortunately, they have recently been shown to be vulnerable to corpus poisoning attacks in which a malicious user injects a small fraction of adversarial passages into the retrieval corpus to trick the system into returning these passages among the top-ranked results for a broad set of user queries.","Further study is needed to understand the extent to which these attacks could limit the deployment of dense retrievers in real-world applications.","In this work, we propose Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval systems based on the widely used HotFlip method for efficiently generating adversarial passages.","We demonstrate that AGGD can select a higher quality set of token-level perturbations than HotFlip by replacing its random token sampling with a more structured search.","Experimentally, we show that our method achieves a high attack success rate on several datasets and using several retrievers, and can generalize to unseen queries and new domains.","Notably, our method is extremely effective in attacking the ANCE retrieval model, achieving attack success rates that are 17.6\\% and 13.37\\% higher on the NQ and MS MARCO datasets, respectively, compared to HotFlip.","Additionally, we demonstrate AGGD's potential to replace HotFlip in other adversarial attacks, such as knowledge poisoning of RAG systems.\\footnote{Code can be find in \\url{https://github.com/JinyanSu1/AGGD}}"],"url":"http://arxiv.org/abs/2406.05087v1","category":"cs.IR"}
{"created":"2024-06-07 17:01:45","title":"Robust Reward Design for Markov Decision Processes","abstract":"The problem of reward design examines the interaction between a leader and a follower, where the leader aims to shape the follower's behavior to maximize the leader's payoff by modifying the follower's reward function. Current approaches to reward design rely on an accurate model of how the follower responds to reward modifications, which can be sensitive to modeling inaccuracies. To address this issue of sensitivity, we present a solution that offers robustness against uncertainties in modeling the follower, including 1) how the follower breaks ties in the presence of nonunique best responses, 2) inexact knowledge of how the follower perceives reward modifications, and 3) bounded rationality of the follower. Our robust solution is guaranteed to exist under mild conditions and can be obtained numerically by solving a mixed-integer linear program. Numerical experiments on multiple test cases demonstrate that our solution improves robustness compared to the standard approach without incurring significant additional computing costs.","sentences":["The problem of reward design examines the interaction between a leader and a follower, where the leader aims to shape the follower's behavior to maximize the leader's payoff by modifying the follower's reward function.","Current approaches to reward design rely on an accurate model of how the follower responds to reward modifications, which can be sensitive to modeling inaccuracies.","To address this issue of sensitivity, we present a solution that offers robustness against uncertainties in modeling the follower, including 1) how the follower breaks ties in the presence of nonunique best responses, 2) inexact knowledge of how the follower perceives reward modifications, and 3) bounded rationality of the follower.","Our robust solution is guaranteed to exist under mild conditions and can be obtained numerically by solving a mixed-integer linear program.","Numerical experiments on multiple test cases demonstrate that our solution improves robustness compared to the standard approach without incurring significant additional computing costs."],"url":"http://arxiv.org/abs/2406.05086v1","category":"math.OC"}
{"created":"2024-06-07 16:59:38","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","abstract":"Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving motivation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, synthetic datasets, and real-world use cases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores.","sentences":["Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses.","Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents.","Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all.","This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents.","The driving motivation is that different attention heads can learn to capture different data aspects.","Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries.","We provide an evaluation methodology and metrics, synthetic datasets, and real-world use cases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in relevance over standard RAG baselines.","MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores."],"url":"http://arxiv.org/abs/2406.05085v1","category":"cs.CL"}
{"created":"2024-06-07 16:58:15","title":"On Ambiguity and the Expressive Function of Law: The Role of Pragmatics in Smart Legal Ecosystems","abstract":"This is a long paper, an essay, on ambiguity, pragmatics, legal ecosystems, and the expressive function of law. It is divided into two parts and fifteen sections. The first part (Pragmatics) addresses ambiguity from the perspective of linguistic and cognitive pragmatics in the legal field. The second part (Computing) deals with this issue from the point of view of human-centered design and artificial intelligence, specifically focusing on the notion and modelling of rules and what it means to comply with the rules. This is necessary for the scaffolding of smart legal ecosystems (SLE). I will develop this subject with the example of the architecture, information flows, and smart ecosystem of OPTIMAI, an EU project of Industry 4.0 for zero-defect manufacturing (Optimizing Manufacturing Processes through Artificial Intelligence and Virtualization).","sentences":["This is a long paper, an essay, on ambiguity, pragmatics, legal ecosystems, and the expressive function of law.","It is divided into two parts and fifteen sections.","The first part (Pragmatics) addresses ambiguity from the perspective of linguistic and cognitive pragmatics in the legal field.","The second part (Computing) deals with this issue from the point of view of human-centered design and artificial intelligence, specifically focusing on the notion and modelling of rules and what it means to comply with the rules.","This is necessary for the scaffolding of smart legal ecosystems (SLE).","I will develop this subject with the example of the architecture, information flows, and smart ecosystem of OPTIMAI, an EU project of Industry 4.0 for zero-defect manufacturing (Optimizing Manufacturing Processes through Artificial Intelligence and Virtualization)."],"url":"http://arxiv.org/abs/2406.05084v1","category":"cs.CY"}
{"created":"2024-06-07 16:56:42","title":"CoNo: Consistency Noise Injection for Tuning-free Long Video Diffusion","abstract":"Tuning-free long video diffusion has been proposed to generate extended-duration videos with enriched content by reusing the knowledge from pre-trained short video diffusion model without retraining. However, most works overlook the fine-grained long-term video consistency modeling, resulting in limited scene consistency (i.e., unreasonable object or background transitions), especially with multiple text inputs. To mitigate this, we propose the Consistency Noise Injection, dubbed CoNo, which introduces the \"look-back\" mechanism to enhance the fine-grained scene transition between different video clips, and designs the long-term consistency regularization to eliminate the content shifts when extending video contents through noise prediction. In particular, the \"look-back\" mechanism breaks the noise scheduling process into three essential parts, where one internal noise prediction part is injected into two video-extending parts, intending to achieve a fine-grained transition between two video clips. The long-term consistency regularization focuses on explicitly minimizing the pixel-wise distance between the predicted noises of the extended video clip and the original one, thereby preventing abrupt scene transitions. Extensive experiments have shown the effectiveness of the above strategies by performing long-video generation under both single- and multi-text prompt conditions. The project has been available in https://wxrui182.github.io/CoNo.github.io/.","sentences":["Tuning-free long video diffusion has been proposed to generate extended-duration videos with enriched content by reusing the knowledge from pre-trained short video diffusion model without retraining.","However, most works overlook the fine-grained long-term video consistency modeling, resulting in limited scene consistency (i.e., unreasonable object or background transitions), especially with multiple text inputs.","To mitigate this, we propose the Consistency Noise Injection, dubbed CoNo, which introduces the \"look-back\" mechanism to enhance the fine-grained scene transition between different video clips, and designs the long-term consistency regularization to eliminate the content shifts when extending video contents through noise prediction.","In particular, the \"look-back\" mechanism breaks the noise scheduling process into three essential parts, where one internal noise prediction part is injected into two video-extending parts, intending to achieve a fine-grained transition between two video clips.","The long-term consistency regularization focuses on explicitly minimizing the pixel-wise distance between the predicted noises of the extended video clip and the original one, thereby preventing abrupt scene transitions.","Extensive experiments have shown the effectiveness of the above strategies by performing long-video generation under both single- and multi-text prompt conditions.","The project has been available in https://wxrui182.github.io/CoNo.github.io/."],"url":"http://arxiv.org/abs/2406.05082v1","category":"cs.CV"}
{"created":"2024-06-07 16:56:26","title":"Pulsar Timing Arrays require hierarchical models","abstract":"Pulsar Timing Array projects have found evidence of a stochastic background of gravitational waves (GWB) using data from an ensemble of pulsars. In the literature, minimal assumptions are made about the signal and noise processes that affect data from these pulsars, such as pulsar spin noise. These assumptions are encoded as uninformative priors in Bayesian searches, though Frequentist approaches make similar assumptions. Uninformative priors are not suitable for (noise) properties of pulsars in an ensemble, and they bias estimates of model parameters such as gravitational-wave signal parameters. Both Frequentist and Bayesian searches are affected. In this letter, more appropriate priors are proposed in the language of Hierarchical Bayesian Modeling, where the properties of the ensemble of pulsars are jointly described with the properties of the individual components of the ensemble. Results by Pulsar Timing Array projects should be re-evaluated using Hierarchical Models.","sentences":["Pulsar Timing Array projects have found evidence of a stochastic background of gravitational waves (GWB) using data from an ensemble of pulsars.","In the literature, minimal assumptions are made about the signal and noise processes that affect data from these pulsars, such as pulsar spin noise.","These assumptions are encoded as uninformative priors in Bayesian searches, though Frequentist approaches make similar assumptions.","Uninformative priors are not suitable for (noise) properties of pulsars in an ensemble, and they bias estimates of model parameters such as gravitational-wave signal parameters.","Both Frequentist and Bayesian searches are affected.","In this letter, more appropriate priors are proposed in the language of Hierarchical Bayesian Modeling, where the properties of the ensemble of pulsars are jointly described with the properties of the individual components of the ensemble.","Results by Pulsar Timing Array projects should be re-evaluated using Hierarchical Models."],"url":"http://arxiv.org/abs/2406.05081v1","category":"astro-ph.IM"}
{"created":"2024-06-07 16:52:57","title":"I2EDL: Interactive Instruction Error Detection and Localization","abstract":"In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language. However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g. \"turn left\" instead of \"turn right\"). In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors. We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation. We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations. In such way, the agent is able to query the user for a timely correction, without demanding the user's cognitive load, as we locate the probable errors to a precise part of the instruction. We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness. We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions.","sentences":["In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language.","However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g. \"turn left\" instead of \"turn right\").","In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors.","We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation.","We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations.","In such way, the agent is able to query the user for a timely correction, without demanding the user's cognitive load, as we locate the probable errors to a precise part of the instruction.","We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness.","We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions."],"url":"http://arxiv.org/abs/2406.05080v1","category":"cs.RO"}
{"created":"2024-06-07 16:49:21","title":"SUMIE: A Synthetic Benchmark for Incremental Entity Summarization","abstract":"No existing dataset adequately tests how well language models can incrementally update entity summaries - a crucial ability as these models rapidly advance. The Incremental Entity Summarization (IES) task is vital for maintaining accurate, up-to-date knowledge. To address this, we introduce SUMIE, a fully synthetic dataset designed to expose real-world IES challenges. This dataset effectively highlights problems like incorrect entity association and incomplete information presentation. Unlike common synthetic datasets, ours captures the complexity and nuances found in real-world data. We generate informative and diverse attributes, summaries, and unstructured paragraphs in sequence, ensuring high quality. The alignment between generated summaries and paragraphs exceeds 96%, confirming the dataset's quality. Extensive experiments demonstrate the dataset's difficulty - state-of-the-art LLMs struggle to update summaries with an F1 higher than 80.4%. We will open source the benchmark and the evaluation metrics to help the community make progress on IES tasks.","sentences":["No existing dataset adequately tests how well language models can incrementally update entity summaries - a crucial ability as these models rapidly advance.","The Incremental Entity Summarization (IES) task is vital for maintaining accurate, up-to-date knowledge.","To address this, we introduce SUMIE, a fully synthetic dataset designed to expose real-world IES challenges.","This dataset effectively highlights problems like incorrect entity association and incomplete information presentation.","Unlike common synthetic datasets, ours captures the complexity and nuances found in real-world data.","We generate informative and diverse attributes, summaries, and unstructured paragraphs in sequence, ensuring high quality.","The alignment between generated summaries and paragraphs exceeds 96%, confirming the dataset's quality.","Extensive experiments demonstrate the dataset's difficulty - state-of-the-art LLMs struggle to update summaries with an F1 higher than 80.4%.","We will open source the benchmark and the evaluation metrics to help the community make progress on IES tasks."],"url":"http://arxiv.org/abs/2406.05079v1","category":"cs.CL"}
{"created":"2024-06-07 16:48:42","title":"Enhancing LEO Mega-Constellations with Inter-Satellite Links: Vision and Challenges","abstract":"Low Earth orbit (LEO) satellites have been envisioned as a significant component of the sixth generation (6G) network architecture for achieving ubiquitous coverage and seamless access. However, the implementation of LEO satellites is largely restricted by the deployment of ground stations. Inter-satellite links (ISLs) have been regarded as a promising technique to fully exploit the potentials of LEO mega constellations by concatenating multiple satellites to constitute an autonomous space network. In this article, we present the merits of implementing ISLs in LEO mega constellations and the representative applications empowered/inspired by ISLs. Moreover, we outline several key technical challenges as well as potential solutions related to LEO satellite networks with ISLs, including performance analysis for system design, routing and load balancing, and resource allocation. Particularly, the potential of using ISLs in enhancing in-flight connectivity is showcased with a preliminary performance evaluation. Finally, some open issues are discussed to inspire future research.","sentences":["Low Earth orbit (LEO) satellites have been envisioned as a significant component of the sixth generation (6G) network architecture for achieving ubiquitous coverage and seamless access.","However, the implementation of LEO satellites is largely restricted by the deployment of ground stations.","Inter-satellite links (ISLs) have been regarded as a promising technique to fully exploit the potentials of LEO mega constellations by concatenating multiple satellites to constitute an autonomous space network.","In this article, we present the merits of implementing ISLs in LEO mega constellations and the representative applications empowered/inspired by ISLs.","Moreover, we outline several key technical challenges as well as potential solutions related to LEO satellite networks with ISLs, including performance analysis for system design, routing and load balancing, and resource allocation.","Particularly, the potential of using ISLs in enhancing in-flight connectivity is showcased with a preliminary performance evaluation.","Finally, some open issues are discussed to inspire future research."],"url":"http://arxiv.org/abs/2406.05078v1","category":"cs.IT"}
{"created":"2024-06-07 16:47:49","title":"Improved Mechanisms and Prophet Inequalities for Graphical Dependencies","abstract":"Over the past two decades, significant strides have been made in stochastic problems such as revenue-optimal auction design and prophet inequalities, traditionally modeled with $n$ independent random variables to represent the values of $n$ items. However, in many applications, this assumption of independence often diverges from reality. Given the strong impossibility results associated with arbitrary correlations, recent research has pivoted towards exploring these problems under models of mild dependency.   In this work, we study the optimal auction and prophet inequalities problems within the framework of the popular graphical model of Markov Random Fields (MRFs), a choice motivated by its ability to capture complex dependency structures. Specifically, for the problem of selling $n$ items to a single buyer to maximize revenue, we show that the max of SRev and BRev is an $O(\\Delta)$-approximation to the optimal revenue for subadditive buyers, where $\\Delta$ is the maximum weighted degree of the underlying MRF. This is a generalization as well as an exponential improvement on the $\\exp(O(\\Delta))$-approximation results of Cai and Oikonomou (EC 2021) for additive and unit-demand buyers. We also obtain a similar exponential improvement for the prophet inequality problem, which is asymptotically optimal as we show a matching upper bound.","sentences":["Over the past two decades, significant strides have been made in stochastic problems such as revenue-optimal auction design and prophet inequalities, traditionally modeled with $n$ independent random variables to represent the values of $n$ items.","However, in many applications, this assumption of independence often diverges from reality.","Given the strong impossibility results associated with arbitrary correlations, recent research has pivoted towards exploring these problems under models of mild dependency.   ","In this work, we study the optimal auction and prophet inequalities problems within the framework of the popular graphical model of Markov Random Fields (MRFs), a choice motivated by its ability to capture complex dependency structures.","Specifically, for the problem of selling $n$ items to a single buyer to maximize revenue, we show that the max of SRev and BRev is an $O(\\Delta)$-approximation to the optimal revenue for subadditive buyers, where $\\Delta$ is the maximum weighted degree of the underlying MRF.","This is a generalization as well as an exponential improvement on the $\\exp(O(\\Delta))$-approximation results of Cai and Oikonomou (EC 2021) for additive and unit-demand buyers.","We also obtain a similar exponential improvement for the prophet inequality problem, which is asymptotically optimal as we show a matching upper bound."],"url":"http://arxiv.org/abs/2406.05077v1","category":"cs.GT"}
{"created":"2024-06-07 16:46:10","title":"Diving Deep into the Motion Representation of Video-Text Models","abstract":"Videos are more informative than images because they capture the dynamics of the scene. By representing motion in videos, we can capture dynamic activities. In this work, we introduce GPT-4 generated motion descriptions that capture fine-grained motion descriptions of activities and apply them to three action datasets. We evaluated several video-text models on the task of retrieval of motion descriptions. We found that they fall far behind human expert performance on two action datasets, raising the question of whether video-text models understand motion in videos. To address it, we introduce a method of improving motion understanding in video-text models by utilizing motion descriptions. This method proves to be effective on two action datasets for the motion description retrieval task. The results draw attention to the need for quality captions involving fine-grained motion information in existing datasets and demonstrate the effectiveness of the proposed pipeline in understanding fine-grained motion during video-text retrieval.","sentences":["Videos are more informative than images because they capture the dynamics of the scene.","By representing motion in videos, we can capture dynamic activities.","In this work, we introduce GPT-4 generated motion descriptions that capture fine-grained motion descriptions of activities and apply them to three action datasets.","We evaluated several video-text models on the task of retrieval of motion descriptions.","We found that they fall far behind human expert performance on two action datasets, raising the question of whether video-text models understand motion in videos.","To address it, we introduce a method of improving motion understanding in video-text models by utilizing motion descriptions.","This method proves to be effective on two action datasets for the motion description retrieval task.","The results draw attention to the need for quality captions involving fine-grained motion information in existing datasets and demonstrate the effectiveness of the proposed pipeline in understanding fine-grained motion during video-text retrieval."],"url":"http://arxiv.org/abs/2406.05075v1","category":"cs.CV"}
{"created":"2024-06-07 16:45:17","title":"Reconstruction of phase-amplitude dynamics from electrophysiological signals","abstract":"Signals from interacting brain regions display transient synchronization of phases and amplitudes in different frequencies. Commonly, the interaction between regions of the brain is quantitatively described by either analyzing the correlations of amplitudes of the measured signals or by calculating phase-synchronization measures. However, for a complete picture of the interactions it is important to analyze the dynamics of both the amplitude and the phase. In this work, we present a new method for finding the coupling between brain regions by reconstructing the phase-amplitude dynamics directly from the measured electrophysiological signals. For this purpose, we use the recent advances in the field of phase-amplitude reduction of oscillatory systems, which allow the representation of an uncoupled oscillatory system as a phase-amplitude oscillator in a unique form using transformations (parametrizations) related to the eigenfunctions of the Koopman operator. By combining the parametrization method and the Fourier-Laplace averaging method of finding the eigenfunctions of the Koopman operator, we developed a novel method of assessing the transformation functions from the signals of the interacting oscillatory systems. The resulting reconstructed dynamical system is a network of phase-amplitude oscillators with the interactions between them represented as coupling functions in phase and amplitude coordinates. Using synthetic signals generated from several models with known and unknown theoretical phase-amplitude reduced forms, we demonstrate that our method is capable of finding the proper unique dynamic form of these oscillatory systems in the reduced phase-amplitude space. Our method can be applied to describe any network of interacting oscillators as a dynamical system using signals of the network elements.","sentences":["Signals from interacting brain regions display transient synchronization of phases and amplitudes in different frequencies.","Commonly, the interaction between regions of the brain is quantitatively described by either analyzing the correlations of amplitudes of the measured signals or by calculating phase-synchronization measures.","However, for a complete picture of the interactions it is important to analyze the dynamics of both the amplitude and the phase.","In this work, we present a new method for finding the coupling between brain regions by reconstructing the phase-amplitude dynamics directly from the measured electrophysiological signals.","For this purpose, we use the recent advances in the field of phase-amplitude reduction of oscillatory systems, which allow the representation of an uncoupled oscillatory system as a phase-amplitude oscillator in a unique form using transformations (parametrizations) related to the eigenfunctions of the Koopman operator.","By combining the parametrization method and the Fourier-Laplace averaging method of finding the eigenfunctions of the Koopman operator, we developed a novel method of assessing the transformation functions from the signals of the interacting oscillatory systems.","The resulting reconstructed dynamical system is a network of phase-amplitude oscillators with the interactions between them represented as coupling functions in phase and amplitude coordinates.","Using synthetic signals generated from several models with known and unknown theoretical phase-amplitude reduced forms, we demonstrate that our method is capable of finding the proper unique dynamic form of these oscillatory systems in the reduced phase-amplitude space.","Our method can be applied to describe any network of interacting oscillators as a dynamical system using signals of the network elements."],"url":"http://arxiv.org/abs/2406.05073v1","category":"math.DS"}
{"created":"2024-06-07 16:41:05","title":"Massively Multiagent Minigames for Training Generalist Agents","abstract":"We present Meta MMO, a collection of many-agent minigames for use as a reinforcement learning benchmark. Meta MMO is built on top of Neural MMO, a massively multiagent environment that has been the subject of two previous NeurIPS competitions. Our work expands Neural MMO with several computationally efficient minigames. We explore generalization across Meta MMO by learning to play several minigames with a single set of weights. We release the environment, baselines, and training code under the MIT license. We hope that Meta MMO will spur additional progress on Neural MMO and, more generally, will serve as a useful benchmark for many-agent generalization.","sentences":["We present Meta MMO, a collection of many-agent minigames for use as a reinforcement learning benchmark.","Meta MMO is built on top of Neural MMO, a massively multiagent environment that has been the subject of two previous NeurIPS competitions.","Our work expands Neural MMO with several computationally efficient minigames.","We explore generalization across Meta MMO by learning to play several minigames with a single set of weights.","We release the environment, baselines, and training code under the MIT license.","We hope that Meta MMO will spur additional progress on Neural MMO and, more generally, will serve as a useful benchmark for many-agent generalization."],"url":"http://arxiv.org/abs/2406.05071v1","category":"cs.AI"}
{"created":"2024-06-07 16:41:02","title":"Targeted Mining Precise-positioning Episode Rules","abstract":"The era characterized by an exponential increase in data has led to the widespread adoption of data intelligence as a crucial task. Within the field of data mining, frequent episode mining has emerged as an effective tool for extracting valuable and essential information from event sequences. Various algorithms have been developed to discover frequent episodes and subsequently derive episode rules using the frequency function and anti-monotonicity principles. However, currently, there is a lack of algorithms specifically designed for mining episode rules that encompass user-specified query episodes. To address this challenge and enable the mining of target episode rules, we introduce the definition of targeted precise-positioning episode rules and formulate the problem of targeted mining precise-positioning episode rules. Most importantly, we develop an algorithm called Targeted Mining Precision Episode Rules (TaMIPER) to address the problem and optimize it using four proposed strategies, leading to significant reductions in both time and space resource requirements. As a result, TaMIPER offers high accuracy and efficiency in mining episode rules of user interest and holds promising potential for prediction tasks in various domains, such as weather observation, network intrusion, and e-commerce. Experimental results on six real datasets demonstrate the exceptional performance of TaMIPER.","sentences":["The era characterized by an exponential increase in data has led to the widespread adoption of data intelligence as a crucial task.","Within the field of data mining, frequent episode mining has emerged as an effective tool for extracting valuable and essential information from event sequences.","Various algorithms have been developed to discover frequent episodes and subsequently derive episode rules using the frequency function and anti-monotonicity principles.","However, currently, there is a lack of algorithms specifically designed for mining episode rules that encompass user-specified query episodes.","To address this challenge and enable the mining of target episode rules, we introduce the definition of targeted precise-positioning episode rules and formulate the problem of targeted mining precise-positioning episode rules.","Most importantly, we develop an algorithm called Targeted Mining Precision Episode Rules (TaMIPER) to address the problem and optimize it using four proposed strategies, leading to significant reductions in both time and space resource requirements.","As a result, TaMIPER offers high accuracy and efficiency in mining episode rules of user interest and holds promising potential for prediction tasks in various domains, such as weather observation, network intrusion, and e-commerce.","Experimental results on six real datasets demonstrate the exceptional performance of TaMIPER."],"url":"http://arxiv.org/abs/2406.05070v1","category":"cs.DB"}
{"created":"2024-06-07 16:37:50","title":"Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations","abstract":"Decision processes of computer vision models - especially deep neural networks - are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are - as the explainability methods themselves - often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing. The used code can be found at https://github.com/lelo204/ClassificationMetricsForImageExplanations .","sentences":["Decision processes of computer vision models - especially deep neural networks - are opaque in nature, meaning that these decisions cannot be understood by humans.","Thus, over the last years, many methods to provide human-understandable explanations have been proposed.","For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images.","But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth.","To overcome this, a slew of different proxy metrics have been defined, which are - as the explainability methods themselves - often built on intuition and thus, are possibly unreliable.","In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet.","In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.","The used code can be found at https://github.com/lelo204/ClassificationMetricsForImageExplanations ."],"url":"http://arxiv.org/abs/2406.05068v1","category":"cs.CV"}
{"created":"2024-06-07 16:37:20","title":"Affine $\\imath$quantum groups and twisted Yangians in Drinfeld presentations","abstract":"We formulate a family of algebras, twisted Yangians (of split type) in current generators and relations, via a degeneration of the Drinfeld presentation of affine $\\imath$quantum groups (associated with split Satake diagrams). These new algebras admit PBW type bases and are shown to be a deformation of twisted current algebras; presentations for twisted current algebras are also provided. For type AI, it matches with the Drinfeld presentation of twisted Yangian obtained via Gauss decomposition. We conjecture that our split twisted Yangians are isomorphic to the corresponding ones in RTT presentation.","sentences":["We formulate a family of algebras, twisted Yangians (of split type) in current generators and relations, via a degeneration of the Drinfeld presentation of affine $\\imath$quantum groups (associated with split Satake diagrams).","These new algebras admit PBW type bases and are shown to be a deformation of twisted current algebras; presentations for twisted current algebras are also provided.","For type AI, it matches with the Drinfeld presentation of twisted Yangian obtained via Gauss decomposition.","We conjecture that our split twisted Yangians are isomorphic to the corresponding ones in RTT presentation."],"url":"http://arxiv.org/abs/2406.05067v1","category":"math.QA"}
{"created":"2024-06-07 16:34:31","title":"Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning","abstract":"In this paper, we study multi-task structured bandit problem where the goal is to learn a near-optimal algorithm that minimizes cumulative regret. The tasks share a common structure and the algorithm exploits the shared structure to minimize the cumulative regret for an unseen but related test task. We use a transformer as a decision-making algorithm to learn this shared structure so as to generalize to the test task. The prior work of pretrained decision transformers like DPT requires access to the optimal action during training which may be hard in several scenarios. Diverging from these works, our learning algorithm does not need the knowledge of optimal action per task during training but predicts a reward vector for each of the actions using only the observed offline data from the diverse training tasks. Finally, during inference time, it selects action using the reward predictions employing various exploration strategies in-context for an unseen test task. Our model outperforms other SOTA methods like DPT, and Algorithmic Distillation over a series of experiments on several structured bandit problems (linear, bilinear, latent, non-linear). Interestingly, we show that our algorithm, without the knowledge of the underlying problem structure, can learn a near-optimal policy in-context by leveraging the shared structure across diverse tasks. We further extend the field of pre-trained decision transformers by showing that they can leverage unseen tasks with new actions and still learn the underlying latent structure to derive a near-optimal policy. We validate this over several experiments to show that our proposed solution is very general and has wide applications to potentially emergent online and offline strategies at test time. Finally, we theoretically analyze the performance of our algorithm and obtain generalization bounds in the in-context multi-task learning setting.","sentences":["In this paper, we study multi-task structured bandit problem where the goal is to learn a near-optimal algorithm that minimizes cumulative regret.","The tasks share a common structure and the algorithm exploits the shared structure to minimize the cumulative regret for an unseen but related test task.","We use a transformer as a decision-making algorithm to learn this shared structure so as to generalize to the test task.","The prior work of pretrained decision transformers like DPT requires access to the optimal action during training which may be hard in several scenarios.","Diverging from these works, our learning algorithm does not need the knowledge of optimal action per task during training but predicts a reward vector for each of the actions using only the observed offline data from the diverse training tasks.","Finally, during inference time, it selects action using the reward predictions employing various exploration strategies in-context for an unseen test task.","Our model outperforms other SOTA methods like DPT, and Algorithmic Distillation over a series of experiments on several structured bandit problems (linear, bilinear, latent, non-linear).","Interestingly, we show that our algorithm, without the knowledge of the underlying problem structure, can learn a near-optimal policy in-context by leveraging the shared structure across diverse tasks.","We further extend the field of pre-trained decision transformers by showing that they can leverage unseen tasks with new actions and still learn the underlying latent structure to derive a near-optimal policy.","We validate this over several experiments to show that our proposed solution is very general and has wide applications to potentially emergent online and offline strategies at test time.","Finally, we theoretically analyze the performance of our algorithm and obtain generalization bounds in the in-context multi-task learning setting."],"url":"http://arxiv.org/abs/2406.05064v1","category":"cs.LG"}
{"created":"2024-06-07 16:33:43","title":"Are Large Language Models More Empathetic than Humans?","abstract":"With the emergence of large language models (LLMs), investigating if they can surpass humans in areas such as emotion recognition and empathetic responding has become a focal point of research. This paper presents a comprehensive study exploring the empathetic responding capabilities of four state-of-the-art LLMs: GPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in comparison to a human baseline. We engaged 1,000 participants in a between-subjects user study, assessing the empathetic quality of responses generated by humans and the four LLMs to 2,000 emotional dialogue prompts meticulously selected to cover a broad spectrum of 32 distinct positive and negative emotions. Our findings reveal a statistically significant superiority of the empathetic responding capability of LLMs over humans. GPT-4 emerged as the most empathetic, marking approximately 31% increase in responses rated as \"Good\" compared to the human benchmark. It was followed by LLaMA-2, Mixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%, and 10% in \"Good\" ratings, respectively. We further analyzed the response ratings at a finer granularity and discovered that some LLMs are significantly better at responding to specific emotions compared to others. The suggested evaluation framework offers a scalable and adaptable approach for assessing the empathy of new LLMs, avoiding the need to replicate this study's findings in future research.","sentences":["With the emergence of large language models (LLMs), investigating if they can surpass humans in areas such as emotion recognition and empathetic responding has become a focal point of research.","This paper presents a comprehensive study exploring the empathetic responding capabilities of four state-of-the-art LLMs: GPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in comparison to a human baseline.","We engaged 1,000 participants in a between-subjects user study, assessing the empathetic quality of responses generated by humans and the four LLMs to 2,000 emotional dialogue prompts meticulously selected to cover a broad spectrum of 32 distinct positive and negative emotions.","Our findings reveal a statistically significant superiority of the empathetic responding capability of LLMs over humans.","GPT-4 emerged as the most empathetic, marking approximately 31% increase in responses rated as \"Good\" compared to the human benchmark.","It was followed by LLaMA-2, Mixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%, and 10% in \"Good\" ratings, respectively.","We further analyzed the response ratings at a finer granularity and discovered that some LLMs are significantly better at responding to specific emotions compared to others.","The suggested evaluation framework offers a scalable and adaptable approach for assessing the empathy of new LLMs, avoiding the need to replicate this study's findings in future research."],"url":"http://arxiv.org/abs/2406.05063v1","category":"cs.CL"}
{"created":"2024-06-07 16:33:08","title":"Progressive Entropic Optimal Transport Solvers","abstract":"Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets. In this context, given two large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map. While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\varepsilon$. Setting $\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps. We take advantage of several opportunities to optimize the computation of EOT solutions by dividing mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and conquering each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to standard solvers when computing couplings at large scales, even outperforming neural network-based approaches. We also prove statistical consistency of our approach for estimating optimal transport maps.","sentences":["Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets.","In this context, given two large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map.","While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\varepsilon$. Setting $\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias.","In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps.","We take advantage of several opportunities to optimize the computation of EOT solutions by dividing mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and conquering each of these steps using EOT with properly scheduled parameters.","We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to standard solvers when computing couplings at large scales, even outperforming neural network-based approaches.","We also prove statistical consistency of our approach for estimating optimal transport maps."],"url":"http://arxiv.org/abs/2406.05061v1","category":"stat.ML"}
{"created":"2024-06-07 16:31:41","title":"GenHeld: Generating and Editing Handheld Objects","abstract":"Grasping is an important human activity that has long been studied in robotics, computer vision, and cognitive science. Most existing works study grasping from the perspective of synthesizing hand poses conditioned on 3D or 2D object representations. We propose GenHeld to address the inverse problem of synthesizing held objects conditioned on 3D hand model or 2D image. Given a 3D model of hand, GenHeld 3D can select a plausible held object from a large dataset using compact object representations called object codes.The selected object is then positioned and oriented to form a plausible grasp without changing hand pose. If only a 2D hand image is available, GenHeld 2D can edit this image to add or replace a held object. GenHeld 2D operates by combining the abilities of GenHeld 3D with diffusion-based image editing. Results and experiments show that we outperform baselines and can generate plausible held objects in both 2D and 3D. Our experiments demonstrate that our method achieves high quality and plausibility of held object synthesis in both 3D and 2D.","sentences":["Grasping is an important human activity that has long been studied in robotics, computer vision, and cognitive science.","Most existing works study grasping from the perspective of synthesizing hand poses conditioned on 3D or 2D object representations.","We propose GenHeld to address the inverse problem of synthesizing held objects conditioned on 3D hand model or 2D image.","Given a 3D model of hand, GenHeld 3D can select a plausible held object from a large dataset using compact object representations called object codes.","The selected object is then positioned and oriented to form a plausible grasp without changing hand pose.","If only a 2D hand image is available, GenHeld 2D can edit this image to add or replace a held object.","GenHeld 2D operates by combining the abilities of GenHeld 3D with diffusion-based image editing.","Results and experiments show that we outperform baselines and can generate plausible held objects in both 2D and","3D.","Our experiments demonstrate that our method achieves high quality and plausibility of held object synthesis in both 3D and 2D."],"url":"http://arxiv.org/abs/2406.05059v1","category":"cs.CV"}
{"created":"2024-06-07 16:28:19","title":"Accurate stochastic simulation algorithm for multiscale models of infectious diseases","abstract":"In the infectious disease literature, significant effort has been devoted to studying dynamics at a single scale. For example, compartmental models describing population-level dynamics are often formulated using differential equations. In cases where small numbers or noise play a crucial role, these differential equations are replaced with memoryless Markovian models, where discrete individuals can be members of a compartment and transition stochastically. Classic stochastic simulation algorithms, such as Gillespie's algorithm and the next reaction method, can be employed to solve these Markovian models exactly. The intricate coupling between models at different scales underscores the importance of multiscale modelling in infectious diseases. However, several computational challenges arise when the multiscale model becomes non-Markovian. In this paper, we address these challenges by developing a novel exact stochastic simulation algorithm. We apply it to a showcase multiscale system where all individuals share the same deterministic within-host model while the population-level dynamics are governed by a stochastic formulation. We demonstrate that as long as the within-host information is harvested at a reasonable resolution, the novel algorithm we develop will always be accurate. Moreover, the novel algorithm we develop is general and can be easily applied to other multiscale models in (or outside) the realm of infectious diseases.","sentences":["In the infectious disease literature, significant effort has been devoted to studying dynamics at a single scale.","For example, compartmental models describing population-level dynamics are often formulated using differential equations.","In cases where small numbers or noise play a crucial role, these differential equations are replaced with memoryless Markovian models, where discrete individuals can be members of a compartment and transition stochastically.","Classic stochastic simulation algorithms, such as Gillespie's algorithm and the next reaction method, can be employed to solve these Markovian models exactly.","The intricate coupling between models at different scales underscores the importance of multiscale modelling in infectious diseases.","However, several computational challenges arise when the multiscale model becomes non-Markovian.","In this paper, we address these challenges by developing a novel exact stochastic simulation algorithm.","We apply it to a showcase multiscale system where all individuals share the same deterministic within-host model while the population-level dynamics are governed by a stochastic formulation.","We demonstrate that as long as the within-host information is harvested at a reasonable resolution, the novel algorithm we develop will always be accurate.","Moreover, the novel algorithm we develop is general and can be easily applied to other multiscale models in (or outside) the realm of infectious diseases."],"url":"http://arxiv.org/abs/2406.05058v1","category":"q-bio.PE"}
{"created":"2024-06-07 16:26:22","title":"Planar chemical reaction systems with algebraic and non-algebraic limit cycles","abstract":"The Hilbert number $H(n)$ is defined as the maximum number of limit cycles of a planar autonomous system of ordinary differential equations (ODEs) with right-hand sides containing polynomials of degree at most $n \\in {\\mathbb N}$. The dynamics of chemical reaction systems with two chemical species can be (under mass-action kinetics) described by such planar autonomous ODEs, where $n$ is equal to the maximum order of the chemical reactions in the system. Generalizations of the Hilbert number $H(n)$ to three different classes of chemical reaction networks are investigated: (i) chemical systems with reactions up to the $n$-th order; (ii) systems with up to $n$-molecular chemical reactions; and (iii) weakly reversible chemical reaction networks. In each case (i), (ii) and (iii), the question on the number of limit cycles is considered. Lower bounds on the generalized Hilbert numbers are provided for both algebraic and non-algebraic limit cycles. Furthermore, given a general algebraic curve $h(x,y)=0$ of degree $n_h \\in {\\mathbb N}$ and containing one or more ovals in the positive quadrant, a chemical system is constructed which has the oval(s) as its stable algebraic limit cycle(s). The ODEs describing the dynamics of the constructed chemical system contain polynomials of degree at most $n=2\\,n_h+1.$ Considering $n_h \\ge 4,$ the algebraic curve $h(x,y)=0$ can contain multiple closed components with the maximum number of ovals given by Harnack's curve theorem as $1+(n_h-1)(n_h-2)/2$, which is equal to 4 for $n_h=4.$ Algebraic curve $h(x,y)=0$ with $n_h=4$ and the maximum number of four ovals is used to construct a chemical system which has four stable algebraic limit cycles.","sentences":["The Hilbert number $H(n)$ is defined as the maximum number of limit cycles of a planar autonomous system of ordinary differential equations (ODEs) with right-hand sides containing polynomials of degree at most $n \\in {\\mathbb N}$.","The dynamics of chemical reaction systems with two chemical species can be (under mass-action kinetics) described by such planar autonomous ODEs, where $n$ is equal to the maximum order of the chemical reactions in the system.","Generalizations of the Hilbert number $H(n)$ to three different classes of chemical reaction networks are investigated: (i) chemical systems with reactions up to the $n$-th order; (ii) systems with up to $n$-molecular chemical reactions; and (iii) weakly reversible chemical reaction networks.","In each case (i), (ii) and (iii), the question on the number of limit cycles is considered.","Lower bounds on the generalized Hilbert numbers are provided for both algebraic and non-algebraic limit cycles.","Furthermore, given a general algebraic curve $h(x,y)=0$ of degree $n_h \\in {\\mathbb N}$ and containing one or more ovals in the positive quadrant, a chemical system is constructed which has the oval(s) as its stable algebraic limit cycle(s).","The ODEs describing the dynamics of the constructed chemical system contain polynomials of degree at most $n=2\\,n_h+1.$ Considering $n_h \\ge 4,$ the algebraic curve $h(x,y)=0$ can contain multiple closed components with the maximum number of ovals given by Harnack's curve theorem as $1+(n_h-1)(n_h-2)/2$, which is equal to 4 for $n_h=4.$ Algebraic curve $h(x,y)=0$ with $n_h=4$ and the maximum number of four ovals is used to construct a chemical system which has four stable algebraic limit cycles."],"url":"http://arxiv.org/abs/2406.05057v1","category":"math.DS"}
{"created":"2024-06-07 16:24:12","title":"Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions","abstract":"Large language models (LLMs) have demonstrated impressive performance on reasoning tasks, which can be further improved through few-shot prompting techniques. However, the current evaluation primarily focuses on carefully constructed benchmarks and neglects the consideration of real-world reasoning problems that present missing and contradictory conditions, known as ill-defined problems. Our observations suggest that existing few-shot prompting techniques are ineffective in such scenarios, often providing overconfident answers or hallucination. To further study this problem, we develop a benchmark called Problems with Missing and Contradictory conditions (PMC) and introduce two novel metrics to evaluate the performance of few-shot prompting methods in these scenarios. Our analysis using the PMC benchmark reveals a trade-off dilemma between the performance of mathematical reasoning for well-defined problems and the ability to recognize ill-defined problems. To address the challenges posed by PMC, we propose a novel few-shot prompting method called SMT-LIB Prompting (SLP), which utilizes the SMT-LIB language to model the problems instead of solving them directly. Subsequently, a double-check solving strategy checks the satisfiability and uniqueness of the solution and provides final feedback. Extensive experiments demonstrate the superiority of our SLP approach compared to existing few-shot prompting methods when dealing with problems with missing and contradictory conditions. We will open-source our benchmark and code to facilitate future research.","sentences":["Large language models (LLMs) have demonstrated impressive performance on reasoning tasks, which can be further improved through few-shot prompting techniques.","However, the current evaluation primarily focuses on carefully constructed benchmarks and neglects the consideration of real-world reasoning problems that present missing and contradictory conditions, known as ill-defined problems.","Our observations suggest that existing few-shot prompting techniques are ineffective in such scenarios, often providing overconfident answers or hallucination.","To further study this problem, we develop a benchmark called Problems with Missing and Contradictory conditions (PMC) and introduce two novel metrics to evaluate the performance of few-shot prompting methods in these scenarios.","Our analysis using the PMC benchmark reveals a trade-off dilemma between the performance of mathematical reasoning for well-defined problems and the ability to recognize ill-defined problems.","To address the challenges posed by PMC, we propose a novel few-shot prompting method called SMT-LIB Prompting (SLP), which utilizes the SMT-LIB language to model the problems instead of solving them directly.","Subsequently, a double-check solving strategy checks the satisfiability and uniqueness of the solution and provides final feedback.","Extensive experiments demonstrate the superiority of our SLP approach compared to existing few-shot prompting methods when dealing with problems with missing and contradictory conditions.","We will open-source our benchmark and code to facilitate future research."],"url":"http://arxiv.org/abs/2406.05055v1","category":"cs.AI"}
{"created":"2024-06-07 16:23:36","title":"Prototype Correlation Matching and Class-Relation Reasoning for Few-Shot Medical Image Segmentation","abstract":"Few-shot medical image segmentation has achieved great progress in improving accuracy and efficiency of medical analysis in the biomedical imaging field. However, most existing methods cannot explore inter-class relations among base and novel medical classes to reason unseen novel classes. Moreover, the same kind of medical class has large intra-class variations brought by diverse appearances, shapes and scales, thus causing ambiguous visual characterization to degrade generalization performance of these existing methods on unseen novel classes. To address the above challenges, in this paper, we propose a \\underline{\\textbf{P}}rototype correlation \\underline{\\textbf{M}}atching and \\underline{\\textbf{C}}lass-relation \\underline{\\textbf{R}}easoning (i.e., \\textbf{PMCR}) model. The proposed model can effectively mitigate false pixel correlation matches caused by large intra-class variations while reasoning inter-class relations among different medical classes. Specifically, in order to address false pixel correlation match brought by large intra-class variations, we propose a prototype correlation matching module to mine representative prototypes that can characterize diverse visual information of different appearances well. We aim to explore prototype-level rather than pixel-level correlation matching between support and query features via optimal transport algorithm to tackle false matches caused by intra-class variations. Meanwhile, in order to explore inter-class relations, we design a class-relation reasoning module to segment unseen novel medical objects via reasoning inter-class relations between base and novel classes. Such inter-class relations can be well propagated to semantic encoding of local query features to improve few-shot segmentation performance. Quantitative comparisons illustrates the large performance improvement of our model over other baseline methods.","sentences":["Few-shot medical image segmentation has achieved great progress in improving accuracy and efficiency of medical analysis in the biomedical imaging field.","However, most existing methods cannot explore inter-class relations among base and novel medical classes to reason unseen novel classes.","Moreover, the same kind of medical class has large intra-class variations brought by diverse appearances, shapes and scales, thus causing ambiguous visual characterization to degrade generalization performance of these existing methods on unseen novel classes.","To address the above challenges, in this paper, we propose a \\underline{\\textbf{P}}rototype correlation \\underline{\\textbf{M}}atching and \\underline{\\textbf{C}}lass-relation \\underline{\\textbf{R}}easoning (i.e., \\textbf{PMCR}) model.","The proposed model can effectively mitigate false pixel correlation matches caused by large intra-class variations while reasoning inter-class relations among different medical classes.","Specifically, in order to address false pixel correlation match brought by large intra-class variations, we propose a prototype correlation matching module to mine representative prototypes that can characterize diverse visual information of different appearances well.","We aim to explore prototype-level rather than pixel-level correlation matching between support and query features via optimal transport algorithm to tackle false matches caused by intra-class variations.","Meanwhile, in order to explore inter-class relations, we design a class-relation reasoning module to segment unseen novel medical objects via reasoning inter-class relations between base and novel classes.","Such inter-class relations can be well propagated to semantic encoding of local query features to improve few-shot segmentation performance.","Quantitative comparisons illustrates the large performance improvement of our model over other baseline methods."],"url":"http://arxiv.org/abs/2406.05054v1","category":"cs.CV"}
{"created":"2024-06-07 16:22:51","title":"Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation","abstract":"Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors' quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser inference engine on three different Python programming datasets. We will release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.","sentences":["Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners.","Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors' quality.","While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments.","In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy.","The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy.","To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data.","We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser inference engine on three different Python programming datasets.","We will release the full implementation along with a web app and datasets to facilitate further research on in-browser language models."],"url":"http://arxiv.org/abs/2406.05053v1","category":"cs.LG"}
{"created":"2024-06-07 16:21:49","title":"Column generation for multistage stochastic mixed-integer nonlinear programs with discrete state variables","abstract":"Stochastic programming provides a natural framework for modeling sequential optimization problems under uncertainty; however, the efficient solution of large-scale multistage stochastic programs remains a challenge, especially in the presence of discrete decisions and nonlinearities. In this work, we consider multistage stochastic mixed-integer nonlinear programs (MINLPs) with discrete state variables, which exhibit a decomposable structure that allows its solution using a column generation approach. Following a Dantzig-Wolfe reformulation, we apply column generation such that each pricing subproblem is an MINLP of much smaller size, making it more amenable to global MINLP solvers. We further propose a method for generating additional columns that satisfy the nonanticipativity constraints, leading to significantly improved convergence and optimal or near-optimal solutions for many large-scale instances in a reasonable computation time. The effectiveness of the tailored column generation algorithm is demonstrated via computational case studies on a multistage blending problem and a problem involving the routing of mobile generators in a power distribution network.","sentences":["Stochastic programming provides a natural framework for modeling sequential optimization problems under uncertainty; however, the efficient solution of large-scale multistage stochastic programs remains a challenge, especially in the presence of discrete decisions and nonlinearities.","In this work, we consider multistage stochastic mixed-integer nonlinear programs (MINLPs) with discrete state variables, which exhibit a decomposable structure that allows its solution using a column generation approach.","Following a Dantzig-Wolfe reformulation, we apply column generation such that each pricing subproblem is an MINLP of much smaller size, making it more amenable to global MINLP solvers.","We further propose a method for generating additional columns that satisfy the nonanticipativity constraints, leading to significantly improved convergence and optimal or near-optimal solutions for many large-scale instances in a reasonable computation time.","The effectiveness of the tailored column generation algorithm is demonstrated via computational case studies on a multistage blending problem and a problem involving the routing of mobile generators in a power distribution network."],"url":"http://arxiv.org/abs/2406.05052v1","category":"math.OC"}
{"created":"2024-06-07 16:02:07","title":"Efficient 3D Shape Generation via Diffusion Mamba with Bidirectional SSMs","abstract":"Recent advancements in sequence modeling have led to the development of the Mamba architecture, noted for its selective state space approach, offering a promising avenue for efficient long sequence handling. However, its application in 3D shape generation, particularly at high resolutions, remains underexplored. Traditional diffusion transformers (DiT) with self-attention mechanisms, despite their potential, face scalability challenges due to the cubic complexity of attention operations as input length increases. This complexity becomes a significant hurdle when dealing with high-resolution voxel sizes. To address this challenge, we introduce a novel diffusion architecture tailored for 3D point clouds generation-Diffusion Mamba (DiM-3D). This architecture forgoes traditional attention mechanisms, instead utilizing the inherent efficiency of the Mamba architecture to maintain linear complexity with respect to sequence length. DiM-3D is characterized by fast inference times and substantially lower computational demands, quantified in reduced Gflops, thereby addressing the key scalability issues of prior models. Our empirical results on the ShapeNet benchmark demonstrate that DiM-3D achieves state-of-the-art performance in generating high-fidelity and diverse 3D shapes. Additionally, DiM-3D shows superior capabilities in tasks like 3D point cloud completion. This not only proves the model's scalability but also underscores its efficiency in generating detailed, high-resolution voxels necessary for advanced 3D shape modeling, particularly excelling in environments requiring high-resolution voxel sizes. Through these findings, we illustrate the exceptional scalability and efficiency of the Diffusion Mamba framework in 3D shape generation, setting a new standard for the field and paving the way for future explorations in high-resolution 3D modeling technologies.","sentences":["Recent advancements in sequence modeling have led to the development of the Mamba architecture, noted for its selective state space approach, offering a promising avenue for efficient long sequence handling.","However, its application in 3D shape generation, particularly at high resolutions, remains underexplored.","Traditional diffusion transformers (DiT) with self-attention mechanisms, despite their potential, face scalability challenges due to the cubic complexity of attention operations as input length increases.","This complexity becomes a significant hurdle when dealing with high-resolution voxel sizes.","To address this challenge, we introduce a novel diffusion architecture tailored for 3D point clouds generation-Diffusion Mamba (DiM-3D).","This architecture forgoes traditional attention mechanisms, instead utilizing the inherent efficiency of the Mamba architecture to maintain linear complexity with respect to sequence length.","DiM-3D is characterized by fast inference times and substantially lower computational demands, quantified in reduced Gflops, thereby addressing the key scalability issues of prior models.","Our empirical results on the ShapeNet benchmark demonstrate that DiM-3D achieves state-of-the-art performance in generating high-fidelity and diverse 3D shapes.","Additionally, DiM-3D shows superior capabilities in tasks like 3D point cloud completion.","This not only proves the model's scalability but also underscores its efficiency in generating detailed, high-resolution voxels necessary for advanced 3D shape modeling, particularly excelling in environments requiring high-resolution voxel sizes.","Through these findings, we illustrate the exceptional scalability and efficiency of the Diffusion Mamba framework in 3D shape generation, setting a new standard for the field and paving the way for future explorations in high-resolution 3D modeling technologies."],"url":"http://arxiv.org/abs/2406.05038v1","category":"cs.CV"}
{"created":"2024-06-07 15:58:12","title":"TimeSieve: Extracting Temporal Dynamics through Information Bottlenecks","abstract":"Time series forecasting has become an increasingly popular research area due to its critical applications in various real-world domains such as traffic management, weather prediction, and financial analysis. Despite significant advancements, existing models face notable challenges, including the necessity of manual hyperparameter tuning for different datasets, and difficulty in effectively distinguishing signal from redundant features in data characterized by strong seasonality. These issues hinder the generalization and practical application of time series forecasting models. To solve this issues, we propose an innovative time series forecasting model TimeSieve designed to address these challenges. Our approach employs wavelet transforms to preprocess time series data, effectively capturing multi-scale features without the need for additional parameters or manual hyperparameter tuning. Additionally, we introduce the information bottleneck theory that filters out redundant features from both detail and approximation coefficients, retaining only the most predictive information. This combination reduces significantly improves the model's accuracy. Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on 70\\% of the datasets, achieving higher predictive accuracy and better generalization across diverse datasets. Our results validate the effectiveness of our approach in addressing the key challenges in time series forecasting, paving the way for more reliable and efficient predictive models in practical applications. The code for our model is available at https://github.com/xll0328/TimeSieve.","sentences":["Time series forecasting has become an increasingly popular research area due to its critical applications in various real-world domains such as traffic management, weather prediction, and financial analysis.","Despite significant advancements, existing models face notable challenges, including the necessity of manual hyperparameter tuning for different datasets, and difficulty in effectively distinguishing signal from redundant features in data characterized by strong seasonality.","These issues hinder the generalization and practical application of time series forecasting models.","To solve this issues, we propose an innovative time series forecasting model TimeSieve designed to address these challenges.","Our approach employs wavelet transforms to preprocess time series data, effectively capturing multi-scale features without the need for additional parameters or manual hyperparameter tuning.","Additionally, we introduce the information bottleneck theory that filters out redundant features from both detail and approximation coefficients, retaining only the most predictive information.","This combination reduces significantly improves the model's accuracy.","Extensive experiments demonstrate that our model outperforms existing state-of-the-art methods on 70\\% of the datasets, achieving higher predictive accuracy and better generalization across diverse datasets.","Our results validate the effectiveness of our approach in addressing the key challenges in time series forecasting, paving the way for more reliable and efficient predictive models in practical applications.","The code for our model is available at https://github.com/xll0328/TimeSieve."],"url":"http://arxiv.org/abs/2406.05036v1","category":"cs.LG"}
{"created":"2024-06-07 15:56:32","title":"Scenarios and Approaches for Situated Natural Language Explanations","abstract":"Large language models (LLMs) can be used to generate natural language explanations (NLE) that are adapted to different users' situations. However, there is yet to be a quantitative evaluation of the extent of such adaptation. To bridge this gap, we collect a benchmarking dataset, Situation-Based Explanation. This dataset contains 100 explanandums. Each explanandum is paired with explanations targeted at three distinct audience types-such as educators, students, and professionals-enabling us to assess how well the explanations meet the specific informational needs and contexts of these diverse groups e.g. students, teachers, and parents. For each \"explanandum paired with an audience\" situation, we include a human-written explanation. These allow us to compute scores that quantify how the LLMs adapt the explanations to the situations. On an array of pretrained language models with varying sizes, we examine three categories of prompting methods: rule-based prompting, meta-prompting, and in-context learning prompting. We find that 1) language models can generate prompts that result in explanations more precisely aligned with the target situations, 2) explicitly modeling an \"assistant\" persona by prompting \"You are a helpful assistant...\" is not a necessary prompt technique for situated NLE tasks, and 3) the in-context learning prompts only can help LLMs learn the demonstration template but can't improve their inference performance. SBE and our analysis facilitate future research towards generating situated natural language explanations.","sentences":["Large language models (LLMs) can be used to generate natural language explanations (NLE) that are adapted to different users' situations.","However, there is yet to be a quantitative evaluation of the extent of such adaptation.","To bridge this gap, we collect a benchmarking dataset, Situation-Based Explanation.","This dataset contains 100 explanandums.","Each explanandum is paired with explanations targeted at three distinct audience types-such as educators, students, and professionals-enabling us to assess how well the explanations meet the specific informational needs and contexts of these diverse groups e.g. students, teachers, and parents.","For each \"explanandum paired with an audience\" situation, we include a human-written explanation.","These allow us to compute scores that quantify how the LLMs adapt the explanations to the situations.","On an array of pretrained language models with varying sizes, we examine three categories of prompting methods: rule-based prompting, meta-prompting, and in-context learning prompting.","We find that 1) language models can generate prompts that result in explanations more precisely aligned with the target situations, 2) explicitly modeling an \"assistant\" persona by prompting \"You are a helpful assistant...\" is not a necessary prompt technique for situated NLE tasks, and 3) the in-context learning prompts only can help LLMs learn the demonstration template but can't improve their inference performance.","SBE and our analysis facilitate future research towards generating situated natural language explanations."],"url":"http://arxiv.org/abs/2406.05035v1","category":"cs.CL"}
{"created":"2024-06-07 15:55:56","title":"Multiple peaks in gravitational waves induced from primordial curvature perturbations with non-Gaussianity","abstract":"First-order primordial curvature perturbations are known to induce gravitational waves at the second-order, which can in turn probe the small-scale curvature perturbations near the end of the inflation. In this work, we extend the previous analysis in the Gaussian case into the non-Gaussian case, with particular efforts to obtain some thumb rules of sandwiching the associated peaks in gravitational waves induced from multiple peaks of non-Gaussian curvature perturbations.","sentences":["First-order primordial curvature perturbations are known to induce gravitational waves at the second-order, which can in turn probe the small-scale curvature perturbations near the end of the inflation.","In this work, we extend the previous analysis in the Gaussian case into the non-Gaussian case, with particular efforts to obtain some thumb rules of sandwiching the associated peaks in gravitational waves induced from multiple peaks of non-Gaussian curvature perturbations."],"url":"http://arxiv.org/abs/2406.05034v1","category":"astro-ph.CO"}
{"created":"2024-06-07 15:50:50","title":"Unified view of scalar and vector dark matter solitons","abstract":"The existence of solitons -- stable, long-lived, and localized field configurations -- is a generic prediction for ultralight dark matter. These solitons, known by various names such as boson stars, axion stars, oscillons, and Q-balls depending on the context, are typically treated as distinct entities in the literature. This study aims to provide a unified perspective on these solitonic objects for real or complex, scalar or vector dark matter, considering self-interactions and nonminimal gravitational interactions. We demonstrate that these solitons share universal nonrelativistic properties, such as conserved charges, mass-radius relations, stability and profiles. Without accounting for alternative interactions or relativistic effects, distinguishing between real and complex scalar dark matter is challenging. However, self-interactions differentiate real and complex vector dark matter due to their different dependencies on the macroscopic spin density of dark matter waves. Furthermore, gradient-dependent nonminimal gravitational interactions impose an upper bound on soliton amplitudes, influencing their mass distribution and phenomenology in the present-day universe.","sentences":["The existence of solitons -- stable, long-lived, and localized field configurations -- is a generic prediction for ultralight dark matter.","These solitons, known by various names such as boson stars, axion stars, oscillons, and Q-balls depending on the context, are typically treated as distinct entities in the literature.","This study aims to provide a unified perspective on these solitonic objects for real or complex, scalar or vector dark matter, considering self-interactions and nonminimal gravitational interactions.","We demonstrate that these solitons share universal nonrelativistic properties, such as conserved charges, mass-radius relations, stability and profiles.","Without accounting for alternative interactions or relativistic effects, distinguishing between real and complex scalar dark matter is challenging.","However, self-interactions differentiate real and complex vector dark matter due to their different dependencies on the macroscopic spin density of dark matter waves.","Furthermore, gradient-dependent nonminimal gravitational interactions impose an upper bound on soliton amplitudes, influencing their mass distribution and phenomenology in the present-day universe."],"url":"http://arxiv.org/abs/2406.05031v1","category":"hep-ph"}
{"created":"2024-06-07 15:49:50","title":"Stochastic simulation of dissipative quantum oscillators","abstract":"Generic open quantum systems are notoriously difficult to simulate unless one looks at specific regimes. In contrast, classical dissipative systems can often be effectively described by stochastic processes, which are generally less computationally expensive. Here, we use the paradigmatic case of a dissipative quantum oscillator to give a pedagogic introduction into the modelling of open quantum systems using quasiclassical methods, i.e. classical stochastic methods that use a 'quantum' noise spectrum to capture the influence of the environment on the system. Such quasiclassical methods have the potential to offer insights into the impact of the quantum nature of the environment on the dynamics of the system of interest whilst still being computationally tractable.","sentences":["Generic open quantum systems are notoriously difficult to simulate unless one looks at specific regimes.","In contrast, classical dissipative systems can often be effectively described by stochastic processes, which are generally less computationally expensive.","Here, we use the paradigmatic case of a dissipative quantum oscillator to give a pedagogic introduction into the modelling of open quantum systems using quasiclassical methods, i.e. classical stochastic methods that use a 'quantum' noise spectrum to capture the influence of the environment on the system.","Such quasiclassical methods have the potential to offer insights into the impact of the quantum nature of the environment on the dynamics of the system of interest whilst still being computationally tractable."],"url":"http://arxiv.org/abs/2406.05030v1","category":"quant-ph"}
{"created":"2024-06-07 15:47:48","title":"Goal-Oriented Error Estimation and Adaptivity for Stochastic Collocation FEM","abstract":"We propose and analyze a general goal-oriented adaptive strategy for approximating quantities of interest (QoIs) associated with solutions to linear elliptic partial differential equations with random inputs. The QoIs are represented by bounded linear or continuously G\\^ateaux differentiable nonlinear goal functionals, and the approximations are computed using the sparse grid stochastic collocation finite element method (SC-FEM). The proposed adaptive strategy relies on novel reliable a posteriori estimates of the errors in approximating QoIs. One of the key features of our error estimation approach is the introduction of a correction term into the approximation of QoIs in order to compensate for the lack of (global) Galerkin orthogonality in the SC-FEM setting. Computational results generated using the proposed adaptive algorithm are presented in the paper for representative elliptic problems with affine and nonaffine parametric coefficient dependence and for a range of linear and nonlinear goal functionals.","sentences":["We propose and analyze a general goal-oriented adaptive strategy for approximating quantities of interest (QoIs) associated with solutions to linear elliptic partial differential equations with random inputs.","The QoIs are represented by bounded linear or continuously G\\^ateaux differentiable nonlinear goal functionals, and the approximations are computed using the sparse grid stochastic collocation finite element method (SC-FEM).","The proposed adaptive strategy relies on novel reliable a posteriori estimates of the errors in approximating QoIs.","One of the key features of our error estimation approach is the introduction of a correction term into the approximation of QoIs in order to compensate for the lack of (global) Galerkin orthogonality in the SC-FEM setting.","Computational results generated using the proposed adaptive algorithm are presented in the paper for representative elliptic problems with affine and nonaffine parametric coefficient dependence and for a range of linear and nonlinear goal functionals."],"url":"http://arxiv.org/abs/2406.05028v1","category":"math.NA"}
{"created":"2024-06-07 15:44:33","title":"Optimizing Automatic Differentiation with Deep Reinforcement Learning","abstract":"Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian. In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost. We formulate the search for the optimal elimination order that minimizes the number of necessary multiplications as a single player game which is played by an RL agent. We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from diverse domains. Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can efficiently execute the obtained elimination orders.","sentences":["Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance.","Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime.","While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.","In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian.","Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.","We formulate the search for the optimal elimination order that minimizes the number of necessary multiplications as a single player game which is played by an RL agent.","We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from diverse domains.","Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can efficiently execute the obtained elimination orders."],"url":"http://arxiv.org/abs/2406.05027v1","category":"cs.LG"}
{"created":"2024-06-07 15:44:31","title":"Unraveling Trace Anomaly of Supradense Matter via Neutron Star Compactness Scaling","abstract":"The trace anomaly $\\Delta\\equiv 1/3-P/\\varepsilon$ quantifies the possibly broken conformal symmetry in supradense matter under pressure $P$ at energy density $\\varepsilon$. Perturbative QCD (pQCD) predicts a vanishing $\\Delta$ at extremely high energy or baryon densities when the conformal symmetry is realized but its behavior at intermediate densities reachable in neutron stars (NSs) are still very uncertain. The extraction of $\\Delta$ from NS observations strongly depends on the employed model for nuclear Equation of State (EOS). Based on the analytical results from perturbatively analyzing the dimensionless Tolman-Oppenheimer-Volkoff (TOV) equations that are further verified numerically by using $10^5$ EOSs generated randomly with a meta-model in a very broad EOS parameter space constrained by terrestrial nuclear experiments and astrophysical observations, here we first show that the compactness $\\xi\\equiv M_{\\rm{NS}}/R$ of a NS with mass $M_{\\rm{NS}}$ and radius $R$ scales very accurately with $\\Pi_{\\rm{c}}\\equiv\\mathrm{X}/(1+3\\mathrm{X}^2+4\\mathrm{X})$ where $\\mathrm{X}\\equiv P_{\\rm{c}}/\\varepsilon_{\\rm{c}}$ is the ratio of pressure over energy density at NS centers. The scaling of NS compactness thus enables one to readily read off the central trace anomaly $\\Delta_{\\rm{c}}=1/3-\\mathrm{X}$ directly from the observational data of either the mass-radius or red-shift measurements. We then demonstrate indeed that the available NS data themselves from recent X-ray and gravitational wave observations can determine model-independently the trace anomaly as a function of energy density in NS cores, providing a stringent test of existing NS models and a clear guidance in a new direction for further understanding the nature and EOS of supradense matter.","sentences":["The trace anomaly $\\Delta\\equiv 1/3-P/\\varepsilon$ quantifies the possibly broken conformal symmetry in supradense matter under pressure $P$ at energy density $\\varepsilon$. Perturbative QCD (pQCD) predicts a vanishing $\\Delta$ at extremely high energy or baryon densities when the conformal symmetry is realized but its behavior at intermediate densities reachable in neutron stars (NSs) are still very uncertain.","The extraction of $\\Delta$ from NS observations strongly depends on the employed model for nuclear Equation of State (EOS).","Based on the analytical results from perturbatively analyzing the dimensionless Tolman-Oppenheimer-Volkoff (TOV) equations that are further verified numerically by using $10^5$ EOSs generated randomly with a meta-model in a very broad EOS parameter space constrained by terrestrial nuclear experiments and astrophysical observations, here we first show that the compactness $\\xi\\equiv M_{\\rm{NS}}/R$ of a NS with mass $M_{\\rm{NS}}$ and radius $R$ scales very accurately with $\\Pi_{\\rm{c}}\\equiv\\mathrm{X}/(1+3\\mathrm{X}^2+4\\mathrm{X})$ where $\\mathrm{X}\\equiv P_{\\rm{c}}/\\varepsilon_{\\rm{c}}$ is the ratio of pressure over energy density at NS centers.","The scaling of NS compactness thus enables one to readily read off the central trace anomaly $\\Delta_{\\rm{c}}=1/3-\\mathrm{X}$ directly from the observational data of either the mass-radius or red-shift measurements.","We then demonstrate indeed that the available NS data themselves from recent X-ray and gravitational wave observations can determine model-independently the trace anomaly as a function of energy density in NS cores, providing a stringent test of existing NS models and a clear guidance in a new direction for further understanding the nature and EOS of supradense matter."],"url":"http://arxiv.org/abs/2406.05025v1","category":"astro-ph.HE"}
{"created":"2024-06-07 15:43:56","title":"Wave functions of multiquark hadrons from representations of the symmetry groups $S_n$","abstract":"Construction of the wave functions of multiquark hadrons by traditional method based on tensor products of colors, flavors, spins (and orbital) parts becomes quite complex when quark numbers grow $n=5,6...12$, as it gets difficult to satisfy requirements of Fermi statistics. Our novel approach is focused directly on representations of the permutation symmetry generators. After showing how $C_3$ is manifested in the wave functions of (excited) baryons, we use it to construct the wave functions for a set of pentaquarks and hexaquarks (n=5,6). We also have some partial results for larger systems, with $n=9$ and 12, and even beyond that as far as $n=24$.","sentences":["Construction of the wave functions of multiquark hadrons by traditional method based on tensor products of colors, flavors, spins (and orbital) parts becomes quite complex when quark numbers grow $n=5,6...12$, as it gets difficult to satisfy requirements of Fermi statistics.","Our novel approach is focused directly on representations of the permutation symmetry generators.","After showing how $C_3$ is manifested in the wave functions of (excited) baryons, we use it to construct the wave functions for a set of pentaquarks and hexaquarks (n=5,6).","We also have some partial results for larger systems, with $n=9$ and 12, and even beyond that as far as $n=24$."],"url":"http://arxiv.org/abs/2406.05024v1","category":"hep-ph"}
{"created":"2024-06-07 15:43:29","title":"GANetic Loss for Generative Adversarial Networks with a Focus on Medical Applications","abstract":"Generative adversarial networks (GANs) are machine learning models that are used to estimate the underlying statistical structure of a given dataset and as a result can be used for a variety of tasks such as image generation or anomaly detection. Despite their initial simplicity, designing an effective loss function for training GANs remains challenging, and various loss functions have been proposed aiming to improve the performance and stability of the generative models. In this study, loss function design for GANs is presented as an optimization problem solved using the genetic programming (GP) approach. Initial experiments were carried out using small Deep Convolutional GAN (DCGAN) model and the MNIST dataset, in order to search experimentally for an improved loss function. The functions found were evaluated on CIFAR10, with the best function, named GANetic loss, showing exceptionally better performance and stability compared to the losses commonly used for GAN training. To further evalute its general applicability on more challenging problems, GANetic loss was applied for two medical applications: image generation and anomaly detection. Experiments were performed with histopathological, gastrointestinal or glaucoma images to evaluate the GANetic loss in medical image generation, resulting in improved image quality compared to the baseline models. The GANetic Loss used for polyp and glaucoma images showed a strong improvement in the detection of anomalies. In summary, the GANetic loss function was evaluated on multiple datasets and applications where it consistently outperforms alternative loss functions. Moreover, GANetic loss leads to stable training and reproducible results, a known weak spot of GANs.","sentences":["Generative adversarial networks (GANs) are machine learning models that are used to estimate the underlying statistical structure of a given dataset and as a result can be used for a variety of tasks such as image generation or anomaly detection.","Despite their initial simplicity, designing an effective loss function for training GANs remains challenging, and various loss functions have been proposed aiming to improve the performance and stability of the generative models.","In this study, loss function design for GANs is presented as an optimization problem solved using the genetic programming (GP) approach.","Initial experiments were carried out using small Deep Convolutional GAN (DCGAN) model and the MNIST dataset, in order to search experimentally for an improved loss function.","The functions found were evaluated on CIFAR10, with the best function, named GANetic loss, showing exceptionally better performance and stability compared to the losses commonly used for GAN training.","To further evalute its general applicability on more challenging problems, GANetic loss was applied for two medical applications: image generation and anomaly detection.","Experiments were performed with histopathological, gastrointestinal or glaucoma images to evaluate the GANetic loss in medical image generation, resulting in improved image quality compared to the baseline models.","The GANetic Loss used for polyp and glaucoma images showed a strong improvement in the detection of anomalies.","In summary, the GANetic loss function was evaluated on multiple datasets and applications where it consistently outperforms alternative loss functions.","Moreover, GANetic loss leads to stable training and reproducible results, a known weak spot of GANs."],"url":"http://arxiv.org/abs/2406.05023v1","category":"cs.CV"}
{"created":"2024-06-07 15:38:27","title":"Scaling up Probabilistic PDE Simulators with Structured Volumetric Information","abstract":"Modeling real-world problems with partial differential equations (PDEs) is a prominent topic in scientific machine learning. Classic solvers for this task continue to play a central role, e.g. to generate training data for deep learning analogues. Any such numerical solution is subject to multiple sources of uncertainty, both from limited computational resources and limited data (including unknown parameters). Gaussian process analogues to classic PDE simulation methods have recently emerged as a framework to construct fully probabilistic estimates of all these types of uncertainty. So far, much of this work focused on theoretical foundations, and as such is not particularly data efficient or scalable. Here we propose a framework combining a discretization scheme based on the popular Finite Volume Method with complementary numerical linear algebra techniques. Practical experiments, including a spatiotemporal tsunami simulation, demonstrate substantially improved scaling behavior of this approach over previous collocation-based techniques.","sentences":["Modeling real-world problems with partial differential equations (PDEs) is a prominent topic in scientific machine learning.","Classic solvers for this task continue to play a central role, e.g. to generate training data for deep learning analogues.","Any such numerical solution is subject to multiple sources of uncertainty, both from limited computational resources and limited data (including unknown parameters).","Gaussian process analogues to classic PDE simulation methods have recently emerged as a framework to construct fully probabilistic estimates of all these types of uncertainty.","So far, much of this work focused on theoretical foundations, and as such is not particularly data efficient or scalable.","Here we propose a framework combining a discretization scheme based on the popular Finite Volume Method with complementary numerical linear algebra techniques.","Practical experiments, including a spatiotemporal tsunami simulation, demonstrate substantially improved scaling behavior of this approach over previous collocation-based techniques."],"url":"http://arxiv.org/abs/2406.05020v1","category":"cs.LG"}
{"created":"2024-06-07 15:33:48","title":"Adaptively Learning to Select-Rank in Online Platforms","abstract":"Ranking algorithms are fundamental to various online platforms across e-commerce sites to content streaming services. Our research addresses the challenge of adaptively ranking items from a candidate pool for heterogeneous users, a key component in personalizing user experience. We develop a user response model that considers diverse user preferences and the varying effects of item positions, aiming to optimize overall user satisfaction with the ranked list. We frame this problem within a contextual bandits framework, with each ranked list as an action. Our approach incorporates an upper confidence bound to adjust predicted user satisfaction scores and selects the ranking action that maximizes these adjusted scores, efficiently solved via maximum weight imperfect matching. We demonstrate that our algorithm achieves a cumulative regret bound of $O(d\\sqrt{NKT})$ for ranking $K$ out of $N$ items in a $d$-dimensional context space over $T$ rounds, under the assumption that user responses follow a generalized linear model. This regret alleviates dependence on the ambient action space, whose cardinality grows exponentially with $N$ and $K$ (thus rendering direct application of existing adaptive learning algorithms -- such as UCB or Thompson sampling -- infeasible). Experiments conducted on both simulated and real-world datasets demonstrate our algorithm outperforms the baseline.","sentences":["Ranking algorithms are fundamental to various online platforms across e-commerce sites to content streaming services.","Our research addresses the challenge of adaptively ranking items from a candidate pool for heterogeneous users, a key component in personalizing user experience.","We develop a user response model that considers diverse user preferences and the varying effects of item positions, aiming to optimize overall user satisfaction with the ranked list.","We frame this problem within a contextual bandits framework, with each ranked list as an action.","Our approach incorporates an upper confidence bound to adjust predicted user satisfaction scores and selects the ranking action that maximizes these adjusted scores, efficiently solved via maximum weight imperfect matching.","We demonstrate that our algorithm achieves a cumulative regret bound of $O(d\\sqrt{NKT})$ for ranking $K$ out of $N$ items in a $d$-dimensional context space over $T$ rounds, under the assumption that user responses follow a generalized linear model.","This regret alleviates dependence on the ambient action space, whose cardinality grows exponentially with $N$ and $K$ (thus rendering direct application of existing adaptive learning algorithms -- such as UCB or Thompson sampling -- infeasible).","Experiments conducted on both simulated and real-world datasets demonstrate our algorithm outperforms the baseline."],"url":"http://arxiv.org/abs/2406.05017v1","category":"cs.LG"}
{"created":"2024-06-07 15:29:40","title":"Quantum Alternating Operator Ansatz for the Preparation and Detection of Long-Lived Singlet States in NMR","abstract":"Designing efficient and robust quantum control strategies is vital for developing quantum technologies. One recent strategy is the Quantum Alternating Operator Ansatz (QAOA) sequence that alternatively propagates under two noncommuting Hamiltonians, whose control parameters can be optimized to generate a gate or prepare a state. Here, we describe the design of the QAOA sequence and their variants to prepare long-lived singlet states (LLS) from the thermal state in NMR. With extraordinarily long lifetimes exceeding the spin-lattice relaxation time constant $T_1$, LLS have been of great interest for various applications, from spectroscopy to medical imaging. Accordingly, designing sequences for efficiently preparing LLS in a general spin system is crucial. Using numerical analysis, we study the efficiency and robustness of the QAOA sequences over a wide range of errors in the control parameters. Using a two-qubit NMR register, we conduct an experimental study to benchmark QAOA sequences against other prominent methods of LLS preparation and observe the significantly superior performance of the QAOA sequences.","sentences":["Designing efficient and robust quantum control strategies is vital for developing quantum technologies.","One recent strategy is the Quantum Alternating Operator Ansatz (QAOA) sequence that alternatively propagates under two noncommuting Hamiltonians, whose control parameters can be optimized to generate a gate or prepare a state.","Here, we describe the design of the QAOA sequence and their variants to prepare long-lived singlet states (LLS) from the thermal state in NMR.","With extraordinarily long lifetimes exceeding the spin-lattice relaxation time constant $T_1$, LLS have been of great interest for various applications, from spectroscopy to medical imaging.","Accordingly, designing sequences for efficiently preparing LLS in a general spin system is crucial.","Using numerical analysis, we study the efficiency and robustness of the QAOA sequences over a wide range of errors in the control parameters.","Using a two-qubit NMR register, we conduct an experimental study to benchmark QAOA sequences against other prominent methods of LLS preparation and observe the significantly superior performance of the QAOA sequences."],"url":"http://arxiv.org/abs/2406.05015v1","category":"quant-ph"}
{"created":"2024-06-07 15:23:53","title":"CHIQ: Contextual History Enhancement for Improving Query Rewriting in Conversational Search","abstract":"In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries. We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting. This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history. We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs. Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs. Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ.","sentences":["In this paper, we study how open-source large language models (LLMs) can be effectively deployed for improving query rewriting in conversational search, especially for ambiguous queries.","We introduce CHIQ, a two-step method that leverages the capabilities of LLMs to resolve ambiguities in the conversation history before query rewriting.","This approach contrasts with prior studies that predominantly use closed-source LLMs to directly generate search queries from conversation history.","We demonstrate on five well-established benchmarks that CHIQ leads to state-of-the-art results across most settings, showing highly competitive performances with systems leveraging closed-source LLMs.","Our study provides a first step towards leveraging open-source LLMs in conversational search, as a competitive alternative to the prevailing reliance on commercial LLMs.","Data, models, and source code will be publicly available upon acceptance at https://github.com/fengranMark/CHIQ."],"url":"http://arxiv.org/abs/2406.05013v1","category":"cs.IR"}
{"created":"2024-06-07 15:22:13","title":"Nonlinear effects on charge fractionalization in critical chains","abstract":"We investigate the generic transport in a one-dimensional strongly correlated fermionic chain beyond linear response. Starting from a Gaussian wave packet with positive momentum on top of the ground state, we find that the numerical time evolution splits the signal into at least three distinct fractional charges moving with different velocities. A fractional left-moving charge is expected from conventional Luttinger liquid theory, but for the prediction of the two separate right-moving packets the nonlinearity of the dispersion must also be taken into account. This out-of-equilibrium protocol therefore allows a direct measurement of nonlinear interaction parameters, which also govern threshold singularities of dynamic response functions. The nonlinear Luttinger Liquid theory also predicts the correct dynamics at low energies, where it agrees with the conventional Luttinger liquid. Moreover, at high energies, the wave packet dynamics reveals signatures of composite excitations containing two-particle bound states. Our results uncover a simple strategy to probe the nonlinear regime in time-resolved experiments in quantum wires and ultracold-atom platforms.","sentences":["We investigate the generic transport in a one-dimensional strongly correlated fermionic chain beyond linear response.","Starting from a Gaussian wave packet with positive momentum on top of the ground state, we find that the numerical time evolution splits the signal into at least three distinct fractional charges moving with different velocities.","A fractional left-moving charge is expected from conventional Luttinger liquid theory, but for the prediction of the two separate right-moving packets the nonlinearity of the dispersion must also be taken into account.","This out-of-equilibrium protocol therefore allows a direct measurement of nonlinear interaction parameters, which also govern threshold singularities of dynamic response functions.","The nonlinear Luttinger Liquid theory also predicts the correct dynamics at low energies, where it agrees with the conventional Luttinger liquid.","Moreover, at high energies, the wave packet dynamics reveals signatures of composite excitations containing two-particle bound states.","Our results uncover a simple strategy to probe the nonlinear regime in time-resolved experiments in quantum wires and ultracold-atom platforms."],"url":"http://arxiv.org/abs/2406.05009v1","category":"cond-mat.str-el"}
{"created":"2024-06-07 15:21:55","title":"Generative diffusion models for synthetic trajectories of heavy and light particles in turbulence","abstract":"Heavy and light particles are commonly found in many natural phenomena and industrial processes, such as suspensions of bubbles, dust, and droplets in incompressible turbulent flows. Based on a recent machine learning approach using a diffusion model that successfully generated single tracer trajectories in three-dimensional turbulence and passed most statistical benchmarks across time scales, we extend this model to include heavy and light particles. Given the particle type - tracer, light, or heavy - the model can generate synthetic, realistic trajectories with correct fat-tail distributions for acceleration, anomalous power laws, and scale dependent local slope properties. This work paves the way for future exploration of the use of diffusion models to produce high-quality synthetic datasets for different flow configurations, potentially allowing interpolation between different setups and adaptation to new conditions.","sentences":["Heavy and light particles are commonly found in many natural phenomena and industrial processes, such as suspensions of bubbles, dust, and droplets in incompressible turbulent flows.","Based on a recent machine learning approach using a diffusion model that successfully generated single tracer trajectories in three-dimensional turbulence and passed most statistical benchmarks across time scales, we extend this model to include heavy and light particles.","Given the particle type - tracer, light, or heavy - the model can generate synthetic, realistic trajectories with correct fat-tail distributions for acceleration, anomalous power laws, and scale dependent local slope properties.","This work paves the way for future exploration of the use of diffusion models to produce high-quality synthetic datasets for different flow configurations, potentially allowing interpolation between different setups and adaptation to new conditions."],"url":"http://arxiv.org/abs/2406.05008v1","category":"physics.flu-dyn"}
{"created":"2024-06-07 15:21:32","title":"Slow and Stored Light via Electromagnetically Induced Transparency Using A $\u039b$-type Superconducting Artificial Atom","abstract":"Recent progresses in Josephson-junction-based superconducting circuits have propelled quantum information processing forward. However, the lack of a metastable state in most superconducting artificial atoms hinders the development of photonic quantum memory in this platform. Here, we use a single superconducting qubit-resonator system to realize a desired $\\Lambda$-type artificial atom, and to demonstrate slow light with a group velocity of 3.6 km/s and the microwave storage with a memory time extending to several hundred nanoseconds via electromagnetically induced transparency. Our results highlight the potential of achieving microwave quantum memory, promising substantial advancements in quantum information processing within superconducting circuits.","sentences":["Recent progresses in Josephson-junction-based superconducting circuits have propelled quantum information processing forward.","However, the lack of a metastable state in most superconducting artificial atoms hinders the development of photonic quantum memory in this platform.","Here, we use a single superconducting qubit-resonator system to realize a desired $\\Lambda$-type artificial atom, and to demonstrate slow light with a group velocity of 3.6 km/s and the microwave storage with a memory time extending to several hundred nanoseconds via electromagnetically induced transparency.","Our results highlight the potential of achieving microwave quantum memory, promising substantial advancements in quantum information processing within superconducting circuits."],"url":"http://arxiv.org/abs/2406.05007v1","category":"quant-ph"}
{"created":"2024-06-07 15:17:06","title":"Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems","abstract":"Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human's actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.","sentences":["Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety.","Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human's actions to act independently rather than in a synergistic fashion.","To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates.","Our 50-subject study provides several findings that we summarize into guidelines.","While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training.","Together, these findings present three important directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration."],"url":"http://arxiv.org/abs/2406.05003v1","category":"cs.RO"}
{"created":"2024-06-07 15:16:46","title":"Benchmarking Deep Jansen-Rit Parameter Inference: An in Silico Study","abstract":"The study of effective connectivity (EC) is essential in understanding how the brain integrates and responds to various sensory inputs. Model-driven estimation of EC is a powerful approach that requires estimating global and local parameters of a generative model of neural activity. Insights gathered through this process can be used in various applications, such as studying neurodevelopmental disorders. However, accurately determining EC through generative models remains a significant challenge due to the complexity of brain dynamics and the inherent noise in neural recordings, e.g., in electroencephalography (EEG). Current model-driven methods to study EC are computationally complex and cannot scale to all brain regions as required by whole-brain analyses. To facilitate EC assessment, an inference algorithm must exhibit reliable prediction of parameters in the presence of noise. Further, the relationship between the model parameters and the neural recordings must be learnable. To progress toward these objectives, we benchmarked the performance of a Bi-LSTM model for parameter inference from the Jansen-Rit neural mass model (JR-NMM) simulated EEG under various noise conditions. Additionally, our study explores how the JR-NMM reacts to changes in key biological parameters (i.e., sensitivity analysis) like synaptic gains and time constants, a crucial step in understanding the connection between neural mechanisms and observed brain activity. Our results indicate that we can predict the local JR-NMM parameters from EEG, supporting the feasibility of our deep-learning-based inference approach. In future work, we plan to extend this framework to estimate local and global parameters from real EEG in clinically relevant applications.","sentences":["The study of effective connectivity (EC) is essential in understanding how the brain integrates and responds to various sensory inputs.","Model-driven estimation of EC is a powerful approach that requires estimating global and local parameters of a generative model of neural activity.","Insights gathered through this process can be used in various applications, such as studying neurodevelopmental disorders.","However, accurately determining EC through generative models remains a significant challenge due to the complexity of brain dynamics and the inherent noise in neural recordings, e.g., in electroencephalography (EEG).","Current model-driven methods to study EC are computationally complex and cannot scale to all brain regions as required by whole-brain analyses.","To facilitate EC assessment, an inference algorithm must exhibit reliable prediction of parameters in the presence of noise.","Further, the relationship between the model parameters and the neural recordings must be learnable.","To progress toward these objectives, we benchmarked the performance of a Bi-LSTM model for parameter inference from the Jansen-Rit neural mass model (JR-NMM) simulated EEG under various noise conditions.","Additionally, our study explores how the JR-NMM reacts to changes in key biological parameters (i.e., sensitivity analysis) like synaptic gains and time constants, a crucial step in understanding the connection between neural mechanisms and observed brain activity.","Our results indicate that we can predict the local JR-NMM parameters from EEG, supporting the feasibility of our deep-learning-based inference approach.","In future work, we plan to extend this framework to estimate local and global parameters from real EEG in clinically relevant applications."],"url":"http://arxiv.org/abs/2406.05002v1","category":"q-bio.NC"}
{"created":"2024-06-07 15:12:26","title":"AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation","abstract":"Recent advances in text-to-image models have enabled high-quality personalized image synthesis of user-provided concepts with flexible textual control. In this work, we analyze the limitations of two primary techniques in text-to-image personalization: Textual Inversion and DreamBooth. When integrating the learned concept into new prompts, Textual Inversion tends to overfit the concept, while DreamBooth often overlooks it. We attribute these issues to the incorrect learning of the embedding alignment for the concept. We introduce AttnDreamBooth, a novel approach that addresses these issues by separately learning the embedding alignment, the attention map, and the subject identity in different training stages. We also introduce a cross-attention map regularization term to enhance the learning of the attention map. Our method demonstrates significant improvements in identity preservation and text alignment compared to the baseline methods.","sentences":["Recent advances in text-to-image models have enabled high-quality personalized image synthesis of user-provided concepts with flexible textual control.","In this work, we analyze the limitations of two primary techniques in text-to-image personalization: Textual Inversion and DreamBooth.","When integrating the learned concept into new prompts, Textual Inversion tends to overfit the concept, while DreamBooth often overlooks it.","We attribute these issues to the incorrect learning of the embedding alignment for the concept.","We introduce AttnDreamBooth, a novel approach that addresses these issues by separately learning the embedding alignment, the attention map, and the subject identity in different training stages.","We also introduce a cross-attention map regularization term to enhance the learning of the attention map.","Our method demonstrates significant improvements in identity preservation and text alignment compared to the baseline methods."],"url":"http://arxiv.org/abs/2406.05000v1","category":"cs.CV"}
{"created":"2024-06-07 15:09:25","title":"ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial Attacks","abstract":"Many machine learning models are susceptible to adversarial attacks, with decision-based black-box attacks representing the most critical threat in real-world applications. These attacks are extremely stealthy, generating adversarial examples using hard labels obtained from the target machine learning model. This is typically realized by optimizing perturbation directions, guided by decision boundaries identified through query-intensive exact search, significantly limiting the attack success rate. This paper introduces a novel approach using the Approximation Decision Boundary (ADB) to efficiently and accurately compare perturbation directions without precisely determining decision boundaries. The effectiveness of our ADB approach (ADBA) hinges on promptly identifying suitable ADB, ensuring reliable differentiation of all perturbation directions. For this purpose, we analyze the probability distribution of decision boundaries, confirming that using the distribution's median value as ADB can effectively distinguish different perturbation directions, giving rise to the development of the ADBA-md algorithm. ADBA-md only requires four queries on average to differentiate any pair of perturbation directions, which is highly query-efficient. Extensive experiments on six well-known image classifiers clearly demonstrate the superiority of ADBA and ADBA-md over multiple state-of-the-art black-box attacks.","sentences":["Many machine learning models are susceptible to adversarial attacks, with decision-based black-box attacks representing the most critical threat in real-world applications.","These attacks are extremely stealthy, generating adversarial examples using hard labels obtained from the target machine learning model.","This is typically realized by optimizing perturbation directions, guided by decision boundaries identified through query-intensive exact search, significantly limiting the attack success rate.","This paper introduces a novel approach using the Approximation Decision Boundary (ADB) to efficiently and accurately compare perturbation directions without precisely determining decision boundaries.","The effectiveness of our ADB approach (ADBA) hinges on promptly identifying suitable ADB, ensuring reliable differentiation of all perturbation directions.","For this purpose, we analyze the probability distribution of decision boundaries, confirming that using the distribution's median value as ADB can effectively distinguish different perturbation directions, giving rise to the development of the ADBA-md algorithm.","ADBA-md only requires four queries on average to differentiate any pair of perturbation directions, which is highly query-efficient.","Extensive experiments on six well-known image classifiers clearly demonstrate the superiority of ADBA and ADBA-md over multiple state-of-the-art black-box attacks."],"url":"http://arxiv.org/abs/2406.04998v1","category":"cs.LG"}
{"created":"2024-06-07 15:04:59","title":"Development and Validation of a Deep-Learning Model for Differential Treatment Benefit Prediction for Adults with Major Depressive Disorder Deployed in the Artificial Intelligence in Depression Medication Enhancement (AIDME) Study","abstract":"INTRODUCTION: The pharmacological treatment of Major Depressive Disorder (MDD) relies on a trial-and-error approach. We introduce an artificial intelligence (AI) model aiming to personalize treatment and improve outcomes, which was deployed in the Artificial Intelligence in Depression Medication Enhancement (AIDME) Study. OBJECTIVES: 1) Develop a model capable of predicting probabilities of remission across multiple pharmacological treatments for adults with at least moderate major depression. 2) Validate model predictions and examine them for amplification of harmful biases. METHODS: Data from previous clinical trials of antidepressant medications were standardized into a common framework and included 9,042 adults with moderate to severe major depression. Feature selection retained 25 clinical and demographic variables. Using Bayesian optimization, a deep learning model was trained on the training set, refined using the validation set, and tested once on the held-out test set. RESULTS: In the evaluation on the held-out test set, the model demonstrated achieved an AUC of 0.65. The model outperformed a null model on the test set (p = 0.01). The model demonstrated clinical utility, achieving an absolute improvement in population remission rate in hypothetical and actual improvement testing. While the model did identify one drug (escitalopram) as generally outperforming the other drugs (consistent with the input data), there was otherwise significant variation in drug rankings. On bias testing, the model did not amplify potentially harmful biases. CONCLUSIONS: We demonstrate the first model capable of predicting outcomes for 10 different treatment options for patients with MDD, intended to be used at or near the start of treatment to personalize treatment. The model was put into clinical practice during the AIDME randomized controlled trial whose results are reported separately.","sentences":["INTRODUCTION:","The pharmacological treatment of Major Depressive Disorder (MDD) relies on a trial-and-error approach.","We introduce an artificial intelligence (AI) model aiming to personalize treatment and improve outcomes, which was deployed in the Artificial Intelligence in Depression Medication Enhancement (AIDME) Study.","OBJECTIVES:","1) Develop a model capable of predicting probabilities of remission across multiple pharmacological treatments for adults with at least moderate major depression.","2) Validate model predictions and examine them for amplification of harmful biases.","METHODS:","Data from previous clinical trials of antidepressant medications were standardized into a common framework and included 9,042 adults with moderate to severe major depression.","Feature selection retained 25 clinical and demographic variables.","Using Bayesian optimization, a deep learning model was trained on the training set, refined using the validation set, and tested once on the held-out test set.","RESULTS:","In the evaluation on the held-out test set, the model demonstrated achieved an AUC of 0.65.","The model outperformed a null model on the test set (p = 0.01).","The model demonstrated clinical utility, achieving an absolute improvement in population remission rate in hypothetical and actual improvement testing.","While the model did identify one drug (escitalopram) as generally outperforming the other drugs (consistent with the input data), there was otherwise significant variation in drug rankings.","On bias testing, the model did not amplify potentially harmful biases.","CONCLUSIONS:","We demonstrate the first model capable of predicting outcomes for 10 different treatment options for patients with MDD, intended to be used at or near the start of treatment to personalize treatment.","The model was put into clinical practice during the AIDME randomized controlled trial whose results are reported separately."],"url":"http://arxiv.org/abs/2406.04993v1","category":"q-bio.NC"}
{"created":"2024-06-07 15:01:48","title":"Campana rational connectedness and weak approximation","abstract":"Campana introduced a notion of Campana rational connectedness for Campana orbifolds. Given a Campana fibration over a complex curve, we prove that a version of weak approximation for Campana sections holds at places of good reduction when the general fiber satisfies a slightly stronger version of Campana rational connectedness. Campana also conjectured that any Fano orbifold is Campana rationally connected; we verify a stronger statement for toric Campana orbifolds. A key tool in our study is log geometry and moduli stacks of stable log maps.","sentences":["Campana introduced a notion of Campana rational connectedness for Campana orbifolds.","Given a Campana fibration over a complex curve, we prove that a version of weak approximation for Campana sections holds at places of good reduction when the general fiber satisfies a slightly stronger version of Campana rational connectedness.","Campana also conjectured that any Fano orbifold is Campana rationally connected; we verify a stronger statement for toric Campana orbifolds.","A key tool in our study is log geometry and moduli stacks of stable log maps."],"url":"http://arxiv.org/abs/2406.04991v1","category":"math.AG"}
{"created":"2024-06-07 14:57:24","title":"Caloric properties of materials near a tricritical point","abstract":"We present a mean-field study of caloric effects driven by primary and secondary fields near tricritical points, which are fields thermodynamically conjugated to the main and secondary order parameters, respectively. General features, such as critical exponents and their crossover from critical to tricritical behaviours, are studied by means of a generic free energy Landau expansion. To deal with specific materials, we propose a model that combines the Blume-Emery-Griffiths prototype to study tricritical points with the Bean-Rodbell approach to include magnetovolume effects. In this model the primary field is the magnetic field, while chemical and mechanical pressures are secondary fields. In spite of the scarcity of experimental data, we have shown that results for the La(Fe$_{x}$Si$_{1-x}$)$_{13}$ and MnSi compounds are in good agreement with our predictions. We expect that our results will motivate and guide new experimental research aiming at optimizing caloric materials.","sentences":["We present a mean-field study of caloric effects driven by primary and secondary fields near tricritical points, which are fields thermodynamically conjugated to the main and secondary order parameters, respectively.","General features, such as critical exponents and their crossover from critical to tricritical behaviours, are studied by means of a generic free energy Landau expansion.","To deal with specific materials, we propose a model that combines the Blume-Emery-Griffiths prototype to study tricritical points with the Bean-Rodbell approach to include magnetovolume effects.","In this model the primary field is the magnetic field, while chemical and mechanical pressures are secondary fields.","In spite of the scarcity of experimental data, we have shown that results for the La(Fe$_{x}$Si$_{1-x}$)$_{13}$ and MnSi compounds are in good agreement with our predictions.","We expect that our results will motivate and guide new experimental research aiming at optimizing caloric materials."],"url":"http://arxiv.org/abs/2406.04990v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 14:56:51","title":"Compositional Generalization with Grounded Language Models","abstract":"Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training. By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs. We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights. We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components. While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations.","sentences":["Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training.","By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs.","We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights.","We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components.","While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations."],"url":"http://arxiv.org/abs/2406.04989v1","category":"cs.CL"}
{"created":"2024-06-07 14:54:56","title":"Language models emulate certain cognitive profiles: An investigation of how predictability measures interact with individual differences","abstract":"To date, most investigations on surprisal and entropy effects in reading have been conducted on the group level, disregarding individual differences. In this work, we revisit the predictive power of surprisal and entropy measures estimated from a range of language models (LMs) on data of human reading times as a measure of processing effort by incorporating information of language users' cognitive capacities. To do so, we assess the predictive power of surprisal and entropy estimated from generative LMs on reading data obtained from individuals who also completed a wide range of psychometric tests. Specifically, we investigate if modulating surprisal and entropy relative to cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, revealing what type of psycholinguistic subject a given LM emulates. Our study finds that in most cases, incorporating cognitive capacities increases predictive power of surprisal and entropy on reading times, and that generally, high performance in the psychometric tests is associated with lower sensitivity to predictability effects. Finally, our results suggest that the analyzed LMs emulate readers with lower verbal intelligence, suggesting that for a given target group (i.e., individuals with high verbal intelligence), these LMs provide less accurate predictability estimates.","sentences":["To date, most investigations on surprisal and entropy effects in reading have been conducted on the group level, disregarding individual differences.","In this work, we revisit the predictive power of surprisal and entropy measures estimated from a range of language models (LMs) on data of human reading times as a measure of processing effort by incorporating information of language users' cognitive capacities.","To do so, we assess the predictive power of surprisal and entropy estimated from generative LMs on reading data obtained from individuals who also completed a wide range of psychometric tests.","Specifically, we investigate if modulating surprisal and entropy relative to cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, revealing what type of psycholinguistic subject a given LM emulates.","Our study finds that in most cases, incorporating cognitive capacities increases predictive power of surprisal and entropy on reading times, and that generally, high performance in the psychometric tests is associated with lower sensitivity to predictability effects.","Finally, our results suggest that the analyzed LMs emulate readers with lower verbal intelligence, suggesting that for a given target group (i.e., individuals with high verbal intelligence), these LMs provide less accurate predictability estimates."],"url":"http://arxiv.org/abs/2406.04988v1","category":"cs.CL"}
{"created":"2024-06-07 14:53:41","title":"CWR sequence of invariants of alternating links and its properties","abstract":"We present the $CWR$ invariant, a new invariant for alternating links, which builds upon and generalizes the $WRP$ invariant. The $CWR$ invariant is an array of two-variable polynomials that provides a stronger invariant compared to the $WRP$ invariant. We compare the strength of our invariant with the classical HOMFLYPT, Kauffman $3$-variable, and Kauffman $2$-variable polynomials on specific knot examples. Additionally, we derive general recursive \"skein\" relations, and also specific formulas for the initial components of the $CWR$ invariant ($CWR_2$ and $CWR_3$) using weighted adjacency matrices of modified Tait graphs.","sentences":["We present the $CWR$ invariant, a new invariant for alternating links, which builds upon and generalizes the $WRP$ invariant.","The $CWR$ invariant is an array of two-variable polynomials that provides a stronger invariant compared to the $WRP$ invariant.","We compare the strength of our invariant with the classical HOMFLYPT, Kauffman $3$-variable, and Kauffman $2$-variable polynomials on specific knot examples.","Additionally, we derive general recursive \"skein\" relations, and also specific formulas for the initial components of the $CWR$ invariant ($CWR_2$ and $CWR_3$) using weighted adjacency matrices of modified Tait graphs."],"url":"http://arxiv.org/abs/2406.04987v1","category":"math.GT"}
{"created":"2024-06-07 14:49:29","title":"Hybrid Beamforming Design for RSMA-assisted mmWave Integrated Sensing and Communications","abstract":"Integrated sensing and communications (ISAC) has been considered one of the new paradigms for sixth-generation (6G) wireless networks. In the millimeter-wave (mmWave) ISAC system, hybrid beamforming (HBF) is considered an emerging technology to exploit the limited number of radio frequency (RF) chains in order to reduce the system hardware cost and power consumption. However, the HBF structure reduces the spatial degrees of freedom for the ISAC system, which further leads to increased interference between multiple users and between users and radar sensing. To solve the above problem, rate split multiple access (RSMA), which is a flexible and robust interference management strategy, is considered. We investigate the joint common rate allocation and HBF design problem for the HBF-based RSMA-assisted mmWave ISAC scheme. We propose the penalty dual decomposition (PDD) method coupled with the weighted mean squared error (WMMSE) minimization method to solve this high-dimensional non-convex problem, which converges to the Karush-Kuhn-Tucker (KKT) point of the original problem. Then, we extend the proposed algorithm to the HBF design based on finite-resolution phase shifters (PSs) to further improve the energy efficiency of the system. Simulation results demonstrate the effectiveness of the proposed algorithm and show that the RSMA-ISAC scheme outperforms other benchmark schemes.","sentences":["Integrated sensing and communications (ISAC) has been considered one of the new paradigms for sixth-generation (6G) wireless networks.","In the millimeter-wave (mmWave) ISAC system, hybrid beamforming (HBF) is considered an emerging technology to exploit the limited number of radio frequency (RF) chains in order to reduce the system hardware cost and power consumption.","However, the HBF structure reduces the spatial degrees of freedom for the ISAC system, which further leads to increased interference between multiple users and between users and radar sensing.","To solve the above problem, rate split multiple access (RSMA), which is a flexible and robust interference management strategy, is considered.","We investigate the joint common rate allocation and HBF design problem for the HBF-based RSMA-assisted mmWave ISAC scheme.","We propose the penalty dual decomposition (PDD) method coupled with the weighted mean squared error (WMMSE) minimization method to solve this high-dimensional non-convex problem, which converges to the Karush-Kuhn-Tucker (KKT) point of the original problem.","Then, we extend the proposed algorithm to the HBF design based on finite-resolution phase shifters (PSs) to further improve the energy efficiency of the system.","Simulation results demonstrate the effectiveness of the proposed algorithm and show that the RSMA-ISAC scheme outperforms other benchmark schemes."],"url":"http://arxiv.org/abs/2406.04985v1","category":"eess.SP"}
{"created":"2024-06-07 14:49:00","title":"CityCraft: A Real Crafter for 3D City Generation","abstract":"City scene generation has gained significant attention in autonomous driving, smart city development, and traffic simulation. It helps enhance infrastructure planning and monitoring solutions. Existing methods have employed a two-stage process involving city layout generation, typically using Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), or Transformers, followed by neural rendering. These techniques often exhibit limited diversity and noticeable artifacts in the rendered city scenes. The rendered scenes lack variety, resembling the training images, resulting in monotonous styles. Additionally, these methods lack planning capabilities, leading to less realistic generated scenes. In this paper, we introduce CityCraft, an innovative framework designed to enhance both the diversity and quality of urban scene generation. Our approach integrates three key stages: initially, a diffusion transformer (DiT) model is deployed to generate diverse and controllable 2D city layouts. Subsequently, a Large Language Model(LLM) is utilized to strategically make land-use plans within these layouts based on user prompts and language guidelines. Based on the generated layout and city plan, we utilize the asset retrieval module and Blender for precise asset placement and scene construction. Furthermore, we contribute two new datasets to the field: 1)CityCraft-OSM dataset including 2D semantic layouts of urban areas, corresponding satellite images, and detailed annotations. 2) CityCraft-Buildings dataset, featuring thousands of diverse, high-quality 3D building assets. CityCraft achieves state-of-the-art performance in generating realistic 3D cities.","sentences":["City scene generation has gained significant attention in autonomous driving, smart city development, and traffic simulation.","It helps enhance infrastructure planning and monitoring solutions.","Existing methods have employed a two-stage process involving city layout generation, typically using Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), or Transformers, followed by neural rendering.","These techniques often exhibit limited diversity and noticeable artifacts in the rendered city scenes.","The rendered scenes lack variety, resembling the training images, resulting in monotonous styles.","Additionally, these methods lack planning capabilities, leading to less realistic generated scenes.","In this paper, we introduce CityCraft, an innovative framework designed to enhance both the diversity and quality of urban scene generation.","Our approach integrates three key stages: initially, a diffusion transformer (DiT) model is deployed to generate diverse and controllable 2D city layouts.","Subsequently, a Large Language Model(LLM) is utilized to strategically make land-use plans within these layouts based on user prompts and language guidelines.","Based on the generated layout and city plan, we utilize the asset retrieval module and Blender for precise asset placement and scene construction.","Furthermore, we contribute two new datasets to the field: 1)CityCraft-OSM dataset including 2D semantic layouts of urban areas, corresponding satellite images, and detailed annotations.","2) CityCraft-Buildings dataset, featuring thousands of diverse, high-quality 3D building assets.","CityCraft achieves state-of-the-art performance in generating realistic 3D cities."],"url":"http://arxiv.org/abs/2406.04983v1","category":"cs.CV"}
{"created":"2024-06-07 14:44:37","title":"The Price of Implicit Bias in Adversarially Robust Generalization","abstract":"We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization. In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.","sentences":["We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization.","In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization.","We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture.","We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks."],"url":"http://arxiv.org/abs/2406.04981v1","category":"cs.LG"}
{"created":"2024-06-07 14:41:05","title":"Path-integral treatment of charged Bose polarons","abstract":"The system of a charged impurity in an interacting Bose gas has gained significant attention due to the long-range ion-atom interactions and the study of transport properties. Here, the ground state energy of a charged Bose polaron is calculated within the Bogoliubov approximation for both the Fr\\\"ohlich and beyond-Fr\\\"ohlich Hamiltonians using a generalized Feynman variational path-integral approach, which obtained accurate results for other polaron problems. The generalized approach, which was used to improve the energy result for the neutral polaron, has resulted in a minor improvement, indicating that Feynman's approach is sufficient when the impurity-boson interaction is long-range. Beyond-Fr\\\"ohlich corrections results in the emergence of a divergence in the polaronic energy indicating a transition between the repulsive and attractive polaron regime. The path-integral approach with the beyond-Fr\\\"ohlich Hamiltonian is also compared to a field-theory calculation from Christensen et al, 2021. The validity of the Bogoliubov approximation is investigated. The optical absorption has also been calculated within the Bogoliubov approximation for weak ion-atom interactions, and the effect of finite temperature has been studied. We show that the coupling of the ion to an oscillating external electric field offers a straigtforward experimental probe for the charged polaron in a Bose gas, different from but complementary to existing spectroscopic techniques.","sentences":["The system of a charged impurity in an interacting Bose gas has gained significant attention due to the long-range ion-atom interactions and the study of transport properties.","Here, the ground state energy of a charged Bose polaron is calculated within the Bogoliubov approximation for both the Fr\\\"ohlich and beyond-Fr\\\"ohlich Hamiltonians using a generalized Feynman variational path-integral approach, which obtained accurate results for other polaron problems.","The generalized approach, which was used to improve the energy result for the neutral polaron, has resulted in a minor improvement, indicating that Feynman's approach is sufficient when the impurity-boson interaction is long-range.","Beyond-Fr\\\"ohlich corrections results in the emergence of a divergence in the polaronic energy indicating a transition between the repulsive and attractive polaron regime.","The path-integral approach with the beyond-Fr\\\"ohlich Hamiltonian is also compared to a field-theory calculation from Christensen et al, 2021.","The validity of the Bogoliubov approximation is investigated.","The optical absorption has also been calculated within the Bogoliubov approximation for weak ion-atom interactions, and the effect of finite temperature has been studied.","We show that the coupling of the ion to an oscillating external electric field offers a straigtforward experimental probe for the charged polaron in a Bose gas, different from but complementary to existing spectroscopic techniques."],"url":"http://arxiv.org/abs/2406.04976v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-07 14:39:28","title":"UniTST: Effectively Modeling Inter-Series and Intra-Series Dependencies for Multivariate Time Series Forecasting","abstract":"Transformer-based models have emerged as powerful tools for multivariate time series forecasting (MTSF). However, existing Transformer models often fall short of capturing both intricate dependencies across variate and temporal dimensions in MTS data. Some recent models are proposed to separately capture variate and temporal dependencies through either two sequential or parallel attention mechanisms. However, these methods cannot directly and explicitly learn the intricate inter-series and intra-series dependencies. In this work, we first demonstrate that these dependencies are very important as they usually exist in real-world data. To directly model these dependencies, we propose a transformer-based model UniTST containing a unified attention mechanism on the flattened patch tokens. Additionally, we add a dispatcher module which reduces the complexity and makes the model feasible for a potentially large number of variates. Although our proposed model employs a simple architecture, it offers compelling performance as shown in our extensive experiments on several datasets for time series forecasting.","sentences":["Transformer-based models have emerged as powerful tools for multivariate time series forecasting (MTSF).","However, existing Transformer models often fall short of capturing both intricate dependencies across variate and temporal dimensions in MTS data.","Some recent models are proposed to separately capture variate and temporal dependencies through either two sequential or parallel attention mechanisms.","However, these methods cannot directly and explicitly learn the intricate inter-series and intra-series dependencies.","In this work, we first demonstrate that these dependencies are very important as they usually exist in real-world data.","To directly model these dependencies, we propose a transformer-based model UniTST containing a unified attention mechanism on the flattened patch tokens.","Additionally, we add a dispatcher module which reduces the complexity and makes the model feasible for a potentially large number of variates.","Although our proposed model employs a simple architecture, it offers compelling performance as shown in our extensive experiments on several datasets for time series forecasting."],"url":"http://arxiv.org/abs/2406.04975v1","category":"cs.LG"}
{"created":"2024-06-07 14:36:18","title":"Time-correlation Transduction in Strong-field Quantum Electrodynamics","abstract":"Recent developments in high-power ultrafast optical technology and emerging theoretical frameworks in strong-field quantum electrodynamics (SF-QED) are unveiling nuanced differentiations between the semi-classical and full quantum mechanical descriptions of physical systems. Here we present a computational investigation of a novel technique for attosecond optical sensing through time correlation transduction (TCT) by investigating high-harmonic generation (HHG) as a representative SF-QED process. TCT is an experimental method to capture photon-electron interactions at higher harmonic orders by temporarily correlating the emitted and driving photon fields. This approach enables resolving the dynamical behavior of optically-driven strong-field phenomena in quantum materials such as Two-dimensional Materials and Dirac Semimetals down to 10 attosecond temporal resolution to discover a full quantum explanation. Predicting and measuring the transition between perturbative and non-perturbative regimes with attosecond resolution can deepen the understanding of SF-QED such as HHG. As such, we find that TCT is a powerful method to pave the way toward the functional characterization of quantum matter.","sentences":["Recent developments in high-power ultrafast optical technology and emerging theoretical frameworks in strong-field quantum electrodynamics (SF-QED) are unveiling nuanced differentiations between the semi-classical and full quantum mechanical descriptions of physical systems.","Here we present a computational investigation of a novel technique for attosecond optical sensing through time correlation transduction (TCT) by investigating high-harmonic generation (HHG) as a representative SF-QED process.","TCT is an experimental method to capture photon-electron interactions at higher harmonic orders by temporarily correlating the emitted and driving photon fields.","This approach enables resolving the dynamical behavior of optically-driven strong-field phenomena in quantum materials such as Two-dimensional Materials and Dirac Semimetals down to 10 attosecond temporal resolution to discover a full quantum explanation.","Predicting and measuring the transition between perturbative and non-perturbative regimes with attosecond resolution can deepen the understanding of SF-QED such as HHG.","As such, we find that TCT is a powerful method to pave the way toward the functional characterization of quantum matter."],"url":"http://arxiv.org/abs/2406.04971v1","category":"physics.optics"}
{"created":"2024-06-07 14:33:57","title":"An Algebraic Framework for the Modeling of Limit Order Books","abstract":"Introducing an algebraic framework for modeling limit order books (LOBs) with tools from physics and stochastic processes, our proposed framework captures the creation and annihilation of orders, order matching, and the time evolution of the LOB state. It also enables compositional settings, accommodating the interaction of heterogeneous traders and different market structures. We employ Dirac notation and generalized generating functions to describe the state space and dynamics of LOBs. The utility of this framework is shown through simulations of simplified market scenarios, illustrating how variations in trader behavior impact key market observables such as spread, return volatility, and liquidity. The algebraic representation allows for exact simulations using the Gillespie algorithm, providing a robust tool for exploring the implications of market design and policy changes on LOB dynamics. Future research can expand this framework to incorporate more complex order types, adaptive event rates, and multi-asset trading environments, offering deeper insights into market microstructure and trader behavior and estimation of key drivers for market microstructure dynamics.","sentences":["Introducing an algebraic framework for modeling limit order books (LOBs) with tools from physics and stochastic processes, our proposed framework captures the creation and annihilation of orders, order matching, and the time evolution of the LOB state.","It also enables compositional settings, accommodating the interaction of heterogeneous traders and different market structures.","We employ Dirac notation and generalized generating functions to describe the state space and dynamics of LOBs.","The utility of this framework is shown through simulations of simplified market scenarios, illustrating how variations in trader behavior impact key market observables such as spread, return volatility, and liquidity.","The algebraic representation allows for exact simulations using the Gillespie algorithm, providing a robust tool for exploring the implications of market design and policy changes on LOB dynamics.","Future research can expand this framework to incorporate more complex order types, adaptive event rates, and multi-asset trading environments, offering deeper insights into market microstructure and trader behavior and estimation of key drivers for market microstructure dynamics."],"url":"http://arxiv.org/abs/2406.04969v1","category":"q-fin.TR"}
{"created":"2024-06-07 14:29:30","title":"Neural Laplace for learning Stochastic Differential Equations","abstract":"Neural Laplace is a unified framework for learning diverse classes of differential equations (DE). For different classes of DE, this framework outperforms other approaches relying on neural networks that aim to learn classes of ordinary differential equations (ODE). However, many systems can't be modelled using ODEs. Stochastic differential equations (SDE) are the mathematical tool of choice when modelling spatiotemporal DE dynamics under the influence of randomness. In this work, we review the potential applications of Neural Laplace to learn diverse classes of SDE, both from a theoretical and a practical point of view.","sentences":["Neural Laplace is a unified framework for learning diverse classes of differential equations (DE).","For different classes of DE, this framework outperforms other approaches relying on neural networks that aim to learn classes of ordinary differential equations (ODE).","However, many systems can't be modelled using ODEs.","Stochastic differential equations (SDE) are the mathematical tool of choice when modelling spatiotemporal DE dynamics under the influence of randomness.","In this work, we review the potential applications of Neural Laplace to learn diverse classes of SDE, both from a theoretical and a practical point of view."],"url":"http://arxiv.org/abs/2406.04964v1","category":"cs.LG"}
{"created":"2024-06-07 14:29:21","title":"Learning Divergence Fields for Shift-Robust Graph Representations","abstract":"Real-world data generation often involves certain geometries (e.g., graphs) that induce instance-level interdependence. This characteristic makes the generalization of learning models more difficult due to the intricate interdependent patterns that impact data-generative distributions and can vary from training to testing. In this work, we propose a geometric diffusion model with learnable divergence fields for the challenging generalization problem with interdependent data. We generalize the diffusion equation with stochastic diffusivity at each time step, which aims to capture the multi-faceted information flows among interdependent data. Furthermore, we derive a new learning objective through causal inference, which can guide the model to learn generalizable patterns of interdependence that are insensitive across domains. Regarding practical implementation, we introduce three model instantiations that can be considered as the generalized versions of GCN, GAT, and Transformers, respectively, which possess advanced robustness against distribution shifts. We demonstrate their promising efficacy for out-of-distribution generalization on diverse real-world datasets.","sentences":["Real-world data generation often involves certain geometries (e.g., graphs) that induce instance-level interdependence.","This characteristic makes the generalization of learning models more difficult due to the intricate interdependent patterns that impact data-generative distributions and can vary from training to testing.","In this work, we propose a geometric diffusion model with learnable divergence fields for the challenging generalization problem with interdependent data.","We generalize the diffusion equation with stochastic diffusivity at each time step, which aims to capture the multi-faceted information flows among interdependent data.","Furthermore, we derive a new learning objective through causal inference, which can guide the model to learn generalizable patterns of interdependence that are insensitive across domains.","Regarding practical implementation, we introduce three model instantiations that can be considered as the generalized versions of GCN, GAT, and Transformers, respectively, which possess advanced robustness against distribution shifts.","We demonstrate their promising efficacy for out-of-distribution generalization on diverse real-world datasets."],"url":"http://arxiv.org/abs/2406.04963v1","category":"cs.LG"}
{"created":"2024-06-07 14:24:05","title":"Leveraging Generative AI for Extracting Process Models from Multimodal Documents","abstract":"This paper presents an investigation of the capabilities of Generative Pre-trained Transformers (GPTs) to auto-generate graphical process models from multi-modal (i.e., text- and image-based) inputs. More precisely, we first introduce a small dataset as well as a set of evaluation metrics that allow for a ground truth-based evaluation of multi-modal process model generation capabilities. We then conduct an initial evaluation of commercial GPT capabilities using zero-, one-, and few-shot prompting strategies. Our results indicate that GPTs can be useful tools for semi-automated process modeling based on multi-modal inputs. More importantly, the dataset and evaluation metrics as well as the open-source evaluation code provide a structured framework for continued systematic evaluations moving forward.","sentences":["This paper presents an investigation of the capabilities of Generative Pre-trained Transformers (GPTs) to auto-generate graphical process models from multi-modal (i.e., text- and image-based) inputs.","More precisely, we first introduce a small dataset as well as a set of evaluation metrics that allow for a ground truth-based evaluation of multi-modal process model generation capabilities.","We then conduct an initial evaluation of commercial GPT capabilities using zero-, one-, and few-shot prompting strategies.","Our results indicate that GPTs can be useful tools for semi-automated process modeling based on multi-modal inputs.","More importantly, the dataset and evaluation metrics as well as the open-source evaluation code provide a structured framework for continued systematic evaluations moving forward."],"url":"http://arxiv.org/abs/2406.04959v1","category":"cs.SE"}
{"created":"2024-06-07 14:22:49","title":"Meeting times of Markov chains via singular value decomposition","abstract":"We suggest a non-asymptotic matrix perturbation-theoretic approach to get sharp bounds on the expected meeting time of random walks on large (possibly random) graphs. We provide a formula for the expected meeting time in terms of the singular value decomposition of the diagonally killed generator of a pair of independent random walks, which we view as a perturbation of the generator. Employing a rank-one approximation of the diagonally killed generator as the proof of concept, we work out sharp bounds on the expected meeting time of simple random walks on sufficiently dense Erd\\H{o}s-R\\'enyi random graphs.","sentences":["We suggest a non-asymptotic matrix perturbation-theoretic approach to get sharp bounds on the expected meeting time of random walks on large (possibly random) graphs.","We provide a formula for the expected meeting time in terms of the singular value decomposition of the diagonally killed generator of a pair of independent random walks, which we view as a perturbation of the generator.","Employing a rank-one approximation of the diagonally killed generator as the proof of concept, we work out sharp bounds on the expected meeting time of simple random walks on sufficiently dense Erd\\H{o}s-R\\'enyi random graphs."],"url":"http://arxiv.org/abs/2406.04958v1","category":"math.PR"}
{"created":"2024-06-07 14:21:37","title":"Measuring the circular polarization of gravitational waves with pulsar timing arrays","abstract":"The circular polarization of the stochastic gravitational wave background (SGWB) is a key observable for characterising the origin of the signal detected by Pulsar Timing Array (PTA) collaborations. Both the astrophysical and the cosmological SGWB can have a sizeable amount of circular polarization, due to Poisson fluctuations in the source properties for the former, and to parity violating processes in the early universe for the latter. Its measurement is challenging since PTA are blind to the circular polarization monopole, forcing us to turn to anisotropies for detection. We study the sensitivity of current and future PTA datasets to circular polarization anisotropies, focusing on realistic modelling of intrinsic and kinematic anisotropies for astrophysical and cosmological scenarios respectively. Our results indicate that the expected level of circular polarization for the astrophysical SGWB should be within the reach of near future datasets, while for cosmological SGWB circular polarization is a viable target for more advanced SKA-type experiments.","sentences":["The circular polarization of the stochastic gravitational wave background (SGWB) is a key observable for characterising the origin of the signal detected by Pulsar Timing Array (PTA) collaborations.","Both the astrophysical and the cosmological SGWB can have a sizeable amount of circular polarization, due to Poisson fluctuations in the source properties for the former, and to parity violating processes in the early universe for the latter.","Its measurement is challenging since PTA are blind to the circular polarization monopole, forcing us to turn to anisotropies for detection.","We study the sensitivity of current and future PTA datasets to circular polarization anisotropies, focusing on realistic modelling of intrinsic and kinematic anisotropies for astrophysical and cosmological scenarios respectively.","Our results indicate that the expected level of circular polarization for the astrophysical SGWB should be within the reach of near future datasets, while for cosmological SGWB circular polarization is a viable target for more advanced SKA-type experiments."],"url":"http://arxiv.org/abs/2406.04957v1","category":"astro-ph.CO"}
{"created":"2024-06-07 14:21:01","title":"Expansion of situations theory for exploring shared awareness in human-intelligent autonomous systems","abstract":"Intelligent autonomous systems are part of a system of systems that interact with other agents to accomplish tasks in complex environments. However, intelligent autonomous systems integrated system of systems add additional layers of complexity based on their limited cognitive processes, specifically shared situation awareness that allows a team to respond to novel tasks. Intelligent autonomous systems' lack of shared situation awareness adversely influences team effectiveness in complex task environments, such as military command-and-control. A complementary approach of shared situation awareness, called situations theory, is beneficial for understanding the relationship between system of systems shared situation awareness and effectiveness. The current study elucidates a conceptual discussion on situations theory to investigate the development of an system of systems shared situational awareness when humans team with intelligent autonomous system agents. To ground the discussion, the reviewed studies expanded situations theory within the context of a system of systems that result in three major conjectures that can be beneficial to the design and development of future systems of systems.","sentences":["Intelligent autonomous systems are part of a system of systems that interact with other agents to accomplish tasks in complex environments.","However, intelligent autonomous systems integrated system of systems add additional layers of complexity based on their limited cognitive processes, specifically shared situation awareness that allows a team to respond to novel tasks.","Intelligent autonomous systems' lack of shared situation awareness adversely influences team effectiveness in complex task environments, such as military command-and-control.","A complementary approach of shared situation awareness, called situations theory, is beneficial for understanding the relationship between system of systems shared situation awareness and effectiveness.","The current study elucidates a conceptual discussion on situations theory to investigate the development of an system of systems shared situational awareness when humans team with intelligent autonomous system agents.","To ground the discussion, the reviewed studies expanded situations theory within the context of a system of systems that result in three major conjectures that can be beneficial to the design and development of future systems of systems."],"url":"http://arxiv.org/abs/2406.04956v1","category":"cs.HC"}
{"created":"2024-06-07 14:20:30","title":"Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial Interaction Scenarios","abstract":"Deploying robots in human-shared environments requires a deep understanding of how nearby agents and objects interact. Employing causal inference to model cause-and-effect relationships facilitates the prediction of human behaviours and enables the anticipation of robot interventions. However, a significant challenge arises due to the absence of implementation of existing causal discovery methods within the ROS ecosystem, the standard de-facto framework in robotics, hindering effective utilisation on real robots. To bridge this gap, in our previous work we proposed ROS-Causal, a ROS-based framework designed for onboard data collection and causal discovery in human-robot spatial interactions. In this work, we present an experimental evaluation of ROS-Causal both in simulation and on a new dataset of human-robot spatial interactions in a lab scenario, to assess its performance and effectiveness. Our analysis demonstrates the efficacy of this approach, showcasing how causal models can be extracted directly onboard by robots during data collection. The online causal models generated from the simulation are consistent with those from lab experiments. These findings can help researchers to enhance the performance of robotic systems in shared environments, firstly by studying the causal relations between variables in simulation without real people, and then facilitating the actual robot deployment in real human environments. ROS-Causal: https://lcastri.github.io/roscausal","sentences":["Deploying robots in human-shared environments requires a deep understanding of how nearby agents and objects interact.","Employing causal inference to model cause-and-effect relationships facilitates the prediction of human behaviours and enables the anticipation of robot interventions.","However, a significant challenge arises due to the absence of implementation of existing causal discovery methods within the ROS ecosystem, the standard de-facto framework in robotics, hindering effective utilisation on real robots.","To bridge this gap, in our previous work we proposed ROS-Causal, a ROS-based framework designed for onboard data collection and causal discovery in human-robot spatial interactions.","In this work, we present an experimental evaluation of ROS-Causal both in simulation and on a new dataset of human-robot spatial interactions in a lab scenario, to assess its performance and effectiveness.","Our analysis demonstrates the efficacy of this approach, showcasing how causal models can be extracted directly onboard by robots during data collection.","The online causal models generated from the simulation are consistent with those from lab experiments.","These findings can help researchers to enhance the performance of robotic systems in shared environments, firstly by studying the causal relations between variables in simulation without real people, and then facilitating the actual robot deployment in real human environments.","ROS-Causal: https://lcastri.github.io/roscausal"],"url":"http://arxiv.org/abs/2406.04955v1","category":"cs.RO"}
{"created":"2024-06-07 14:19:34","title":"Scalar Dark Energy Models and Scalar-Tensor Gravity: Theoretical Explanations for the Accelerated Expansion of Present Universe","abstract":"The reason for the present accelerated expansion of the Universe stands as one of the most profound questions in the realm of science, with deep connections to both cosmology and fundamental physics. From a cosmological point of view, physical models aimed at elucidating the observed expansion can be categorized into two major classes: dark energy and modified gravity. We review various major approaches that employ a single scalar field to account for the accelerating phase of our present Universe. Dynamical system analysis is employed in several important models to seek for cosmological solutions that exhibit an accelerating phase as an attractor. For scalar field models of dark energy, we consistently focus on addressing challenges related to the fine-tuning and coincidence problems in cosmology, as well as exploring potential solutions to them. For scalar-tensor theories and their generalizations, we emphasize the importance of constraints on theoretical parameters to ensure overall consistency with experimental tests. Models or theories that could potentially explain the Hubble tension are also emphasized throughout this review.","sentences":["The reason for the present accelerated expansion of the Universe stands as one of the most profound questions in the realm of science, with deep connections to both cosmology and fundamental physics.","From a cosmological point of view, physical models aimed at elucidating the observed expansion can be categorized into two major classes: dark energy and modified gravity.","We review various major approaches that employ a single scalar field to account for the accelerating phase of our present Universe.","Dynamical system analysis is employed in several important models to seek for cosmological solutions that exhibit an accelerating phase as an attractor.","For scalar field models of dark energy, we consistently focus on addressing challenges related to the fine-tuning and coincidence problems in cosmology, as well as exploring potential solutions to them.","For scalar-tensor theories and their generalizations, we emphasize the importance of constraints on theoretical parameters to ensure overall consistency with experimental tests.","Models or theories that could potentially explain the Hubble tension are also emphasized throughout this review."],"url":"http://arxiv.org/abs/2406.04954v1","category":"gr-qc"}
{"created":"2024-06-07 14:16:37","title":"Quantifying Geospatial in the Common Crawl Corpus","abstract":"Large language models (LLMs) exhibit emerging geospatial capabilities, stemming from their pre-training on vast unlabelled text datasets that are often derived from the Common Crawl corpus. However, the geospatial content within CC remains largely unexplored, impacting our understanding of LLMs' spatial reasoning. This paper investigates the prevalence of geospatial data in recent Common Crawl releases using Gemini, a powerful language model. By analyzing a sample of documents and manually revising the results, we estimate that between 1 in 5 and 1 in 6 documents contain geospatial information such as coordinates and street addresses. Our findings provide quantitative insights into the nature and extent of geospatial data within Common Crawl, and web crawl data in general. Furthermore, we formulate questions to guide future investigations into the geospatial content of available web crawl datasets and its influence on LLMs.","sentences":["Large language models (LLMs) exhibit emerging geospatial capabilities, stemming from their pre-training on vast unlabelled text datasets that are often derived from the Common Crawl corpus.","However, the geospatial content within CC remains largely unexplored, impacting our understanding of LLMs' spatial reasoning.","This paper investigates the prevalence of geospatial data in recent Common Crawl releases using Gemini, a powerful language model.","By analyzing a sample of documents and manually revising the results, we estimate that between 1 in 5 and 1 in 6 documents contain geospatial information such as coordinates and street addresses.","Our findings provide quantitative insights into the nature and extent of geospatial data within Common Crawl, and web crawl data in general.","Furthermore, we formulate questions to guide future investigations into the geospatial content of available web crawl datasets and its influence on LLMs."],"url":"http://arxiv.org/abs/2406.04952v1","category":"cs.CL"}
{"created":"2024-06-07 14:13:20","title":"The Database and Benchmark for Source Speaker Verification Against Voice Conversion","abstract":"Voice conversion systems can transform audio to mimic another speaker's voice, thereby attacking speaker verification systems. However, ongoing studies on source speaker verification are hindered by limited data availability and methodological constraints. In this paper, we generate a large-scale converted speech database and train a batch of baseline systems based on the MFA-Conformer architecture to promote the source speaker verification task. In addition, we introduce a related task called conversion method recognition. An adapter-based multi-task learning approach is employed to achieve effective conversion method recognition without compromising source speaker verification performance. Additionally, we investigate and effectively address the open-set conversion method recognition problem through the implementation of an open-set nearest neighbor approach.","sentences":["Voice conversion systems can transform audio to mimic another speaker's voice, thereby attacking speaker verification systems.","However, ongoing studies on source speaker verification are hindered by limited data availability and methodological constraints.","In this paper, we generate a large-scale converted speech database and train a batch of baseline systems based on the MFA-Conformer architecture to promote the source speaker verification task.","In addition, we introduce a related task called conversion method recognition.","An adapter-based multi-task learning approach is employed to achieve effective conversion method recognition without compromising source speaker verification performance.","Additionally, we investigate and effectively address the open-set conversion method recognition problem through the implementation of an open-set nearest neighbor approach."],"url":"http://arxiv.org/abs/2406.04951v1","category":"eess.AS"}
{"created":"2024-06-07 14:08:54","title":"Robotic in-hand manipulation with relaxed optimization","abstract":"Dexterous in-hand manipulation is a unique and valuable human skill requiring sophisticated sensorimotor interaction with the environment while respecting stability constraints. Satisfying these constraints with generated motions is essential for a robotic platform to achieve reliable in-hand manipulation skills. Explicitly modelling these constraints can be challenging, but they can be implicitly modelled and learned through experience or human demonstrations. We propose a learning and control approach based on dictionaries of motion primitives generated from human demonstrations. To achieve this, we defined an optimization process that combines motion primitives to generate robot fingertip trajectories for moving an object from an initial to a desired final pose. Based on our experiments, our approach allows a robotic hand to handle objects like humans, adhering to stability constraints without requiring explicit formalization. In other words, the proposed motion primitive dictionaries learn and implicitly embed the constraints crucial to the in-hand manipulation task.","sentences":["Dexterous in-hand manipulation is a unique and valuable human skill requiring sophisticated sensorimotor interaction with the environment while respecting stability constraints.","Satisfying these constraints with generated motions is essential for a robotic platform to achieve reliable in-hand manipulation skills.","Explicitly modelling these constraints can be challenging, but they can be implicitly modelled and learned through experience or human demonstrations.","We propose a learning and control approach based on dictionaries of motion primitives generated from human demonstrations.","To achieve this, we defined an optimization process that combines motion primitives to generate robot fingertip trajectories for moving an object from an initial to a desired final pose.","Based on our experiments, our approach allows a robotic hand to handle objects like humans, adhering to stability constraints without requiring explicit formalization.","In other words, the proposed motion primitive dictionaries learn and implicitly embed the constraints crucial to the in-hand manipulation task."],"url":"http://arxiv.org/abs/2406.04950v1","category":"cs.RO"}
{"created":"2024-06-07 14:07:23","title":"Nacala-Roof-Material: Drone Imagery for Roof Detection, Classification, and Segmentation to Support Mosquito-borne Disease Risk Assessment","abstract":"As low-quality housing and in particular certain roof characteristics are associated with an increased risk of malaria, classification of roof types based on remote sensing imagery can support the assessment of malaria risk and thereby help prevent the disease. To support research in this area, we release the Nacala-Roof-Material dataset, which contains high-resolution drone images from Mozambique with corresponding labels delineating houses and specifying their roof types. The dataset defines a multi-task computer vision problem, comprising object detection, classification, and segmentation. In addition, we benchmarked various state-of-the-art approaches on the dataset. Canonical U-Nets, YOLOv8, and a custom decoder on pretrained DINOv2 served as baselines. We show that each of the methods has its advantages but none is superior on all tasks, which highlights the potential of our dataset for future research in multi-task learning. While the tasks are closely related, accurate segmentation of objects does not necessarily imply accurate instance separation, and vice versa. We address this general issue by introducing a variant of the deep ordinal watershed (DOW) approach that additionally separates the interior of objects, allowing for improved object delineation and separation. We show that our DOW variant is a generic approach that improves the performance of both U-Net and DINOv2 backbones, leading to a better trade-off between semantic segmentation and instance segmentation.","sentences":["As low-quality housing and in particular certain roof characteristics are associated with an increased risk of malaria, classification of roof types based on remote sensing imagery can support the assessment of malaria risk and thereby help prevent the disease.","To support research in this area, we release the Nacala-Roof-Material dataset, which contains high-resolution drone images from Mozambique with corresponding labels delineating houses and specifying their roof types.","The dataset defines a multi-task computer vision problem, comprising object detection, classification, and segmentation.","In addition, we benchmarked various state-of-the-art approaches on the dataset.","Canonical U-Nets, YOLOv8, and a custom decoder on pretrained DINOv2 served as baselines.","We show that each of the methods has its advantages but none is superior on all tasks, which highlights the potential of our dataset for future research in multi-task learning.","While the tasks are closely related, accurate segmentation of objects does not necessarily imply accurate instance separation, and vice versa.","We address this general issue by introducing a variant of the deep ordinal watershed (DOW) approach that additionally separates the interior of objects, allowing for improved object delineation and separation.","We show that our DOW variant is a generic approach that improves the performance of both U-Net and DINOv2 backbones, leading to a better trade-off between semantic segmentation and instance segmentation."],"url":"http://arxiv.org/abs/2406.04949v1","category":"cs.CV"}
{"created":"2024-06-07 14:06:24","title":"Origin of the yield stress anomaly in L12 intermetallics unveiled with physically-informed machine-learning potentials","abstract":"The yield stress anomaly of L12 intermetallics such as Ni3Al or Ni3Ga is controlled by the so-called Kear-Wilsdorf lock (KWL), of which the formation and unlocking are governed by dislocation cross-slip. Despite the importance of L12 intermetallics for strengthening Ni-based superalloys, microscopic understanding of the KWL is limited. Here, molecular dynamics simulations are conducted by employing a dedicated machine-learning interatomic potential derived via physically-informed active-learning. The potential facilitates modelling of the dislocation behavior in Ni3Al with near ab initio accuracy. KWL formation and unlocking are observed and analyzed. The unlocking stress demonstrates a pronounced temperature dependence, contradicting the assumptions of existing analytical models. A phenomenological model is proposed to effectively describe the atomistic unlocking stresses and extrapolate them to the macroscopic scale. The model is general and applicable to other L12 intermetallics. The acquired knowledge of KWLs provides a deeper understanding on the origin of the yield stress anomaly.","sentences":["The yield stress anomaly of L12 intermetallics such as Ni3Al or Ni3Ga is controlled by the so-called Kear-Wilsdorf lock (KWL), of which the formation and unlocking are governed by dislocation cross-slip.","Despite the importance of L12 intermetallics for strengthening Ni-based superalloys, microscopic understanding of the KWL is limited.","Here, molecular dynamics simulations are conducted by employing a dedicated machine-learning interatomic potential derived via physically-informed active-learning.","The potential facilitates modelling of the dislocation behavior in Ni3Al with near ab initio accuracy.","KWL formation and unlocking are observed and analyzed.","The unlocking stress demonstrates a pronounced temperature dependence, contradicting the assumptions of existing analytical models.","A phenomenological model is proposed to effectively describe the atomistic unlocking stresses and extrapolate them to the macroscopic scale.","The model is general and applicable to other L12 intermetallics.","The acquired knowledge of KWLs provides a deeper understanding on the origin of the yield stress anomaly."],"url":"http://arxiv.org/abs/2406.04948v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 14:01:56","title":"BAMO at SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense","abstract":"This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense. The task aims to evaluate the ability of language models to think creatively. The dataset comprises multi-choice questions that challenge models to think \"outside of the box\". We fine-tune 2 models, BERT and RoBERTa Large. Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2. Finally, we utilize ReConcile, a technique that employs a \"round table conference\" approach with multiple agents for zero-shot learning, to generate consensus answers among 3 selected language models. Our best method achieves an overall accuracy of 85 percent on the sentence puzzles subtask.","sentences":["This paper outlines our approach to SemEval 2024 Task 9, BRAINTEASER: A Novel Task Defying Common Sense.","The task aims to evaluate the ability of language models to think creatively.","The dataset comprises multi-choice questions that challenge models to think \"outside of the box\".","We fine-tune 2 models, BERT and RoBERTa Large.","Next, we employ a Chain of Thought (CoT) zero-shot prompting approach with 6 large language models, such as GPT-3.5, Mixtral, and Llama2.","Finally, we utilize ReConcile, a technique that employs a \"round table conference\" approach with multiple agents for zero-shot learning, to generate consensus answers among 3 selected language models.","Our best method achieves an overall accuracy of 85 percent on the sentence puzzles subtask."],"url":"http://arxiv.org/abs/2406.04947v1","category":"cs.CL"}
{"created":"2024-06-07 13:53:02","title":"Joint Spatial-Temporal Modeling and Contrastive Learning for Self-supervised Heart Rate Measurement","abstract":"This paper briefly introduces the solutions developed by our team, HFUT-VUT, for Track 1 of self-supervised heart rate measurement in the 3rd Vision-based Remote Physiological Signal Sensing (RePSS) Challenge hosted at IJCAI 2024. The goal is to develop a self-supervised learning algorithm for heart rate (HR) estimation using unlabeled facial videos. To tackle this task, we present two self-supervised HR estimation solutions that integrate spatial-temporal modeling and contrastive learning, respectively. Specifically, we first propose a non-end-to-end self-supervised HR measurement framework based on spatial-temporal modeling, which can effectively capture subtle rPPG clues and leverage the inherent bandwidth and periodicity characteristics of rPPG to constrain the model. Meanwhile, we employ an excellent end-to-end solution based on contrastive learning, aiming to generalize across different scenarios from complementary perspectives. Finally, we combine the strengths of the above solutions through an ensemble strategy to generate the final predictions, leading to a more accurate HR estimation. As a result, our solutions achieved a remarkable RMSE score of 8.85277 on the test dataset, securing \\textbf{2nd place} in Track 1 of the challenge.","sentences":["This paper briefly introduces the solutions developed by our team, HFUT-VUT, for Track 1 of self-supervised heart rate measurement in the 3rd Vision-based Remote Physiological Signal Sensing (RePSS) Challenge hosted at IJCAI 2024.","The goal is to develop a self-supervised learning algorithm for heart rate (HR) estimation using unlabeled facial videos.","To tackle this task, we present two self-supervised HR estimation solutions that integrate spatial-temporal modeling and contrastive learning, respectively.","Specifically, we first propose a non-end-to-end self-supervised HR measurement framework based on spatial-temporal modeling, which can effectively capture subtle rPPG clues and leverage the inherent bandwidth and periodicity characteristics of rPPG to constrain the model.","Meanwhile, we employ an excellent end-to-end solution based on contrastive learning, aiming to generalize across different scenarios from complementary perspectives.","Finally, we combine the strengths of the above solutions through an ensemble strategy to generate the final predictions, leading to a more accurate HR estimation.","As a result, our solutions achieved a remarkable RMSE score of 8.85277 on the test dataset, securing \\textbf{2nd place} in Track 1 of the challenge."],"url":"http://arxiv.org/abs/2406.04942v1","category":"cs.CV"}
{"created":"2024-06-07 13:48:15","title":"TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models","abstract":"The recently unprecedented advancements in Large Language Models (LLMs) have propelled the medical community by establishing advanced medical-domain models. However, due to the limited collection of medical datasets, there are only a few comprehensive benchmarks available to gauge progress in this area. In this paper, we introduce a new medical question-answering (QA) dataset that contains massive manual instruction for solving Traditional Chinese Medicine examination tasks, called TCMD. Specifically, our TCMD collects massive questions across diverse domains with their annotated medical subjects and thus supports us in comprehensively assessing the capability of LLMs in the TCM domain. Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted. Moreover, we also analyze the robustness of current LLMs in solving TCM QA tasks by introducing randomness. The inconsistency of the experimental results also reveals the shortcomings of current LLMs in solving QA tasks. We also expect that our dataset can further facilitate the development of LLMs in the TCM area.","sentences":["The recently unprecedented advancements in Large Language Models (LLMs) have propelled the medical community by establishing advanced medical-domain models.","However, due to the limited collection of medical datasets, there are only a few comprehensive benchmarks available to gauge progress in this area.","In this paper, we introduce a new medical question-answering (QA) dataset that contains massive manual instruction for solving Traditional Chinese Medicine examination tasks, called TCMD.","Specifically, our TCMD collects massive questions across diverse domains with their annotated medical subjects and thus supports us in comprehensively assessing the capability of LLMs in the TCM domain.","Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted.","Moreover, we also analyze the robustness of current LLMs in solving TCM QA tasks by introducing randomness.","The inconsistency of the experimental results also reveals the shortcomings of current LLMs in solving QA tasks.","We also expect that our dataset can further facilitate the development of LLMs in the TCM area."],"url":"http://arxiv.org/abs/2406.04941v1","category":"cs.CL"}
{"created":"2024-06-07 13:47:40","title":"CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling","abstract":"Terrestrial carbon fluxes provide vital information about our biosphere's health and its capacity to absorb anthropogenic CO$_2$ emissions. The importance of predicting carbon fluxes has led to the emerging field of data-driven carbon flux modelling (DDCFM), which uses statistical techniques to predict carbon fluxes from biophysical data. However, the field lacks a standardized dataset to promote comparisons between models. To address this gap, we present CarbonSense, the first machine learning-ready dataset for DDCFM. CarbonSense integrates measured carbon fluxes, meteorological predictors, and satellite imagery from 385 locations across the globe, offering comprehensive coverage and facilitating robust model training. Additionally, we provide a baseline model using a current state-of-the-art DDCFM approach and a novel transformer based model. Our experiments illustrate the potential gains that multimodal deep learning techniques can bring to this domain. By providing these resources, we aim to lower the barrier to entry for other deep learning researchers to develop new models and drive new advances in carbon flux modelling.","sentences":["Terrestrial carbon fluxes provide vital information about our biosphere's health and its capacity to absorb anthropogenic CO$_2$ emissions.","The importance of predicting carbon fluxes has led to the emerging field of data-driven carbon flux modelling (DDCFM), which uses statistical techniques to predict carbon fluxes from biophysical data.","However, the field lacks a standardized dataset to promote comparisons between models.","To address this gap, we present CarbonSense, the first machine learning-ready dataset for DDCFM.","CarbonSense integrates measured carbon fluxes, meteorological predictors, and satellite imagery from 385 locations across the globe, offering comprehensive coverage and facilitating robust model training.","Additionally, we provide a baseline model using a current state-of-the-art DDCFM approach and a novel transformer based model.","Our experiments illustrate the potential gains that multimodal deep learning techniques can bring to this domain.","By providing these resources, we aim to lower the barrier to entry for other deep learning researchers to develop new models and drive new advances in carbon flux modelling."],"url":"http://arxiv.org/abs/2406.04940v1","category":"cs.LG"}
{"created":"2024-06-07 13:46:23","title":"SpanGNN: Towards Memory-Efficient Graph Neural Networks via Spanning Subgraph Training","abstract":"Graph Neural Networks (GNNs) have superior capability in learning graph data. Full-graph GNN training generally has high accuracy, however, it suffers from large peak memory usage and encounters the Out-of-Memory problem when handling large graphs. To address this memory problem, a popular solution is mini-batch GNN training. However, mini-batch GNN training increases the training variance and sacrifices the model accuracy. In this paper, we propose a new memory-efficient GNN training method using spanning subgraph, called SpanGNN. SpanGNN trains GNN models over a sequence of spanning subgraphs, which are constructed from empty structure. To overcome the excessive peak memory consumption problem, SpanGNN selects a set of edges from the original graph to incrementally update the spanning subgraph between every epoch. To ensure the model accuracy, we introduce two types of edge sampling strategies (i.e., variance-reduced and noise-reduced), and help SpanGNN select high-quality edges for the GNN learning. We conduct experiments with SpanGNN on widely used datasets, demonstrating SpanGNN's advantages in the model performance and low peak memory usage.","sentences":["Graph Neural Networks (GNNs) have superior capability in learning graph data.","Full-graph GNN training generally has high accuracy, however, it suffers from large peak memory usage and encounters the Out-of-Memory problem when handling large graphs.","To address this memory problem, a popular solution is mini-batch GNN training.","However, mini-batch GNN training increases the training variance and sacrifices the model accuracy.","In this paper, we propose a new memory-efficient GNN training method using spanning subgraph, called SpanGNN.","SpanGNN trains GNN models over a sequence of spanning subgraphs, which are constructed from empty structure.","To overcome the excessive peak memory consumption problem, SpanGNN selects a set of edges from the original graph to incrementally update the spanning subgraph between every epoch.","To ensure the model accuracy, we introduce two types of edge sampling strategies (i.e., variance-reduced and noise-reduced), and help SpanGNN select high-quality edges for the GNN learning.","We conduct experiments with SpanGNN on widely used datasets, demonstrating SpanGNN's advantages in the model performance and low peak memory usage."],"url":"http://arxiv.org/abs/2406.04938v1","category":"cs.LG"}
{"created":"2024-06-07 13:44:55","title":"On Quantifiers for Quantitative Reasoning","abstract":"We explore a kind of first-order predicate logic with intended semantics in the reals. Compared to other approaches in the literature, we work predominantly in the multiplicative reals [0,\\infty], showing they support three generations of connectives, that we call non-linear, linear additive, and linear multiplicative. Means and harmonic means emerge as natural candidates for bounded existential and universal quantifiers, and in fact we see they behave as expected in relation to the other logical connectives. We explain this fact through the well-known fact that min/max and arithmetic mean/harmonic mean sit at opposite ends of a spectrum, that of p-means. We give syntax and semantics for this quantitative predicate logic, and as example applications, we show how softmax is the quantitative semantics of argmax, and R\\'enyi entropy/Hill numbers are additive/multiplicative semantics of the same formula. Indeed, the additive reals also fit into the story by exploiting the Napierian duality -log \\dashv 1/exp, which highlights a formal distinction between 'additive' and 'multiplicative' quantities. Finally, we describe two attempts at a categorical semantics via enriched hyperdoctrines. We discuss why hyperdoctrines are in fact probably inadequate for this kind of logic.","sentences":["We explore a kind of first-order predicate logic with intended semantics in the reals.","Compared to other approaches in the literature, we work predominantly in the multiplicative reals [0,\\infty], showing they support three generations of connectives, that we call non-linear, linear additive, and linear multiplicative.","Means and harmonic means emerge as natural candidates for bounded existential and universal quantifiers, and in fact we see they behave as expected in relation to the other logical connectives.","We explain this fact through the well-known fact that min/max and arithmetic mean/harmonic mean sit at opposite ends of a spectrum, that of p-means.","We give syntax and semantics for this quantitative predicate logic, and as example applications, we show how softmax is the quantitative semantics of argmax, and R\\'enyi entropy/Hill numbers are additive/multiplicative semantics of the same formula.","Indeed, the additive reals also fit into the story by exploiting the Napierian duality -log","\\dashv 1/exp, which highlights a formal distinction between 'additive' and 'multiplicative' quantities.","Finally, we describe two attempts at a categorical semantics via enriched hyperdoctrines.","We discuss why hyperdoctrines are in fact probably inadequate for this kind of logic."],"url":"http://arxiv.org/abs/2406.04936v1","category":"math.LO"}
{"created":"2024-06-07 13:42:15","title":"SLOPE: Search with Learned Optimal Pruning-based Expansion","abstract":"Heuristic search is often used for motion planning and pathfinding problems, for finding the shortest path in a graph while also promising completeness and optimal efficiency. The drawback is it's space complexity, specifically storing all expanded child nodes in memory and sorting large lists of active nodes, which can be a problem in real-time scenarios with limited on-board computation. To combat this, we present the Search with Learned Optimal Pruning-based Expansion (SLOPE), which, learns the distance of a node from a possible optimal path, unlike other approaches that learn a cost-to-go value. The unfavored nodes are then pruned according to the said distance, which in turn reduces the size of the open list. This ensures that the search explores only the region close to optimal paths while lowering memory and computational costs. Unlike traditional learning methods, our approach is orthogonal to estimating cost-to-go heuristics, offering a complementary strategy for improving search efficiency. We demonstrate the effectiveness of our approach evaluating it as a standalone search method and in conjunction with learned heuristic functions, achieving comparable-or-better node expansion metrics, while lowering the number of child nodes in the open list. Our code is available at https://github.com/dbokan1/SLOPE.","sentences":["Heuristic search is often used for motion planning and pathfinding problems, for finding the shortest path in a graph while also promising completeness and optimal efficiency.","The drawback is it's space complexity, specifically storing all expanded child nodes in memory and sorting large lists of active nodes, which can be a problem in real-time scenarios with limited on-board computation.","To combat this, we present the Search with Learned Optimal Pruning-based Expansion (SLOPE), which, learns the distance of a node from a possible optimal path, unlike other approaches that learn a cost-to-go value.","The unfavored nodes are then pruned according to the said distance, which in turn reduces the size of the open list.","This ensures that the search explores only the region close to optimal paths while lowering memory and computational costs.","Unlike traditional learning methods, our approach is orthogonal to estimating cost-to-go heuristics, offering a complementary strategy for improving search efficiency.","We demonstrate the effectiveness of our approach evaluating it as a standalone search method and in conjunction with learned heuristic functions, achieving comparable-or-better node expansion metrics, while lowering the number of child nodes in the open list.","Our code is available at https://github.com/dbokan1/SLOPE."],"url":"http://arxiv.org/abs/2406.04935v1","category":"cs.AI"}
{"created":"2024-06-07 13:41:17","title":"Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction","abstract":"In dynamical systems reconstruction (DSR) we seek to infer from time series measurements a generative model of the underlying dynamical process. This is a prime objective in any scientific discipline, where we are particularly interested in parsimonious models with a low parameter load. A common strategy here is parameter pruning, removing all parameters with small weights. However, here we find this strategy does not work for DSR, where even low magnitude parameters can contribute considerably to the system dynamics. On the other hand, it is well known that many natural systems which generate complex dynamics, like the brain or ecological networks, have a sparse topology with comparatively few links. Inspired by this, we show that geometric pruning, where in contrast to magnitude-based pruning weights with a low contribution to an attractor's geometrical structure are removed, indeed manages to reduce parameter load substantially without significantly hampering DSR quality. We further find that the networks resulting from geometric pruning have a specific type of topology, and that this topology, and not the magnitude of weights, is what is most crucial to performance. We provide an algorithm that automatically generates such topologies which can be used as priors for generative modeling of dynamical systems by RNNs, and compare it to other well studied topologies like small-world or scale-free networks.","sentences":["In dynamical systems reconstruction (DSR) we seek to infer from time series measurements a generative model of the underlying dynamical process.","This is a prime objective in any scientific discipline, where we are particularly interested in parsimonious models with a low parameter load.","A common strategy here is parameter pruning, removing all parameters with small weights.","However, here we find this strategy does not work for DSR, where even low magnitude parameters can contribute considerably to the system dynamics.","On the other hand, it is well known that many natural systems which generate complex dynamics, like the brain or ecological networks, have a sparse topology with comparatively few links.","Inspired by this, we show that geometric pruning, where in contrast to magnitude-based pruning weights with a low contribution to an attractor's geometrical structure are removed, indeed manages to reduce parameter load substantially without significantly hampering DSR quality.","We further find that the networks resulting from geometric pruning have a specific type of topology, and that this topology, and not the magnitude of weights, is what is most crucial to performance.","We provide an algorithm that automatically generates such topologies which can be used as priors for generative modeling of dynamical systems by RNNs, and compare it to other well studied topologies like small-world or scale-free networks."],"url":"http://arxiv.org/abs/2406.04934v1","category":"cs.LG"}
{"created":"2024-06-07 13:37:36","title":"Faster Than Lies: Real-time Deepfake Detection using Binary Neural Networks","abstract":"Deepfake detection aims to contrast the spread of deep-generated media that undermines trust in online content. While existing methods focus on large and complex models, the need for real-time detection demands greater efficiency. With this in mind, unlike previous work, we introduce a novel deepfake detection approach on images using Binary Neural Networks (BNNs) for fast inference with minimal accuracy loss. Moreover, our method incorporates Fast Fourier Transform (FFT) and Local Binary Pattern (LBP) as additional channel features to uncover manipulation traces in frequency and texture domains. Evaluations on COCOFake, DFFD, and CIFAKE datasets demonstrate our method's state-of-the-art performance in most scenarios with a significant efficiency gain of up to a $20\\times$ reduction in FLOPs during inference. Finally, by exploring BNNs in deepfake detection to balance accuracy and efficiency, this work paves the way for future research on efficient deepfake detection.","sentences":["Deepfake detection aims to contrast the spread of deep-generated media that undermines trust in online content.","While existing methods focus on large and complex models, the need for real-time detection demands greater efficiency.","With this in mind, unlike previous work, we introduce a novel deepfake detection approach on images using Binary Neural Networks (BNNs) for fast inference with minimal accuracy loss.","Moreover, our method incorporates Fast Fourier Transform (FFT) and Local Binary Pattern (LBP) as additional channel features to uncover manipulation traces in frequency and texture domains.","Evaluations on COCOFake, DFFD, and CIFAKE datasets demonstrate our method's state-of-the-art performance in most scenarios with a significant efficiency gain of up to a $20\\times$ reduction in FLOPs during inference.","Finally, by exploring BNNs in deepfake detection to balance accuracy and efficiency, this work paves the way for future research on efficient deepfake detection."],"url":"http://arxiv.org/abs/2406.04932v1","category":"cs.CV"}
{"created":"2024-06-07 13:35:08","title":"Protein pathways as a catalyst to directed evolution of the topology of artificial neural networks","abstract":"In the present article, we propose a paradigm shift on evolving Artificial Neural Networks (ANNs) towards a new bio-inspired design that is grounded on the structural properties, interactions, and dynamics of protein networks (PNs): the Artificial Protein Network (APN). This introduces several advantages previously unrealized by state-of-the-art approaches in NE: (1) We can draw inspiration from how nature, thanks to millions of years of evolution, efficiently encodes protein interactions in the DNA to translate our APN to silicon DNA. This helps bridge the gap between syntax and semantics observed in current NE approaches. (2) We can learn from how nature builds networks in our genes, allowing us to design new and smarter networks through EA evolution. (3) We can perform EA crossover/mutation operations and evolution steps, replicating the operations observed in nature directly on the genotype of networks, thus exploring and exploiting the phenotypic space, such that we avoid getting trapped in sub-optimal solutions. (4) Our novel definition of APN opens new ways to leverage our knowledge about different living things and processes from biology. (5) Using biologically inspired encodings, we can model more complex demographic and ecological relationships (e.g., virus-host or predator-prey interactions), allowing us to optimise for multiple, often conflicting objectives.","sentences":["In the present article, we propose a paradigm shift on evolving Artificial Neural Networks (ANNs) towards a new bio-inspired design that is grounded on the structural properties, interactions, and dynamics of protein networks (PNs): the Artificial Protein Network (APN).","This introduces several advantages previously unrealized by state-of-the-art approaches in NE: (1) We can draw inspiration from how nature, thanks to millions of years of evolution, efficiently encodes protein interactions in the DNA to translate our APN to silicon DNA.","This helps bridge the gap between syntax and semantics observed in current NE approaches.","(2) We can learn from how nature builds networks in our genes, allowing us to design new and smarter networks through EA evolution.","(3) We can perform EA crossover/mutation operations and evolution steps, replicating the operations observed in nature directly on the genotype of networks, thus exploring and exploiting the phenotypic space, such that we avoid getting trapped in sub-optimal solutions.","(4) Our novel definition of APN opens new ways to leverage our knowledge about different living things and processes from biology.","(5) Using biologically inspired encodings, we can model more complex demographic and ecological relationships (e.g., virus-host or predator-prey interactions), allowing us to optimise for multiple, often conflicting objectives."],"url":"http://arxiv.org/abs/2406.04929v1","category":"cs.NE"}
{"created":"2024-06-07 13:31:51","title":"Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models","abstract":"Large Language Models (LLMs) have shown exceptional performance in text processing. Notably, LLMs can synthesize information from large datasets and explain their decisions similarly to human reasoning through a chain of thought (CoT). An emerging application of LLMs is the handling and interpreting of numerical data, where fine-tuning enhances their performance over basic inference methods. This paper proposes a novel approach to training LLMs using knowledge transfer from a random forest (RF) ensemble, leveraging its efficiency and accuracy. By converting RF decision paths into natural language statements, we generate outputs for LLM fine-tuning, enhancing the model's ability to classify and explain its decisions. Our method includes verifying these rules through established classification metrics, ensuring their correctness. We also examine the impact of preprocessing techniques on the representation of numerical data and their influence on classification accuracy and rule correctness","sentences":["Large Language Models (LLMs) have shown exceptional performance in text processing.","Notably, LLMs can synthesize information from large datasets and explain their decisions similarly to human reasoning through a chain of thought (CoT).","An emerging application of LLMs is the handling and interpreting of numerical data, where fine-tuning enhances their performance over basic inference methods.","This paper proposes a novel approach to training LLMs using knowledge transfer from a random forest (RF) ensemble, leveraging its efficiency and accuracy.","By converting RF decision paths into natural language statements, we generate outputs for LLM fine-tuning, enhancing the model's ability to classify and explain its decisions.","Our method includes verifying these rules through established classification metrics, ensuring their correctness.","We also examine the impact of preprocessing techniques on the representation of numerical data and their influence on classification accuracy and rule correctness"],"url":"http://arxiv.org/abs/2406.04926v1","category":"cs.CL"}
{"created":"2024-06-07 13:29:54","title":"Hamiltonian formulation and aspects of integrability of generalised hydrodynamics","abstract":"Generalised Hydrodynamics (GHD) describes the large-scale inhomogeneous dynamics of integrable (or close to integrable) systems in one dimension of space, based on a central equation for the fluid density or quasi-particle density: the GHD equation. We consider a new, general form of the GHD equation: we allow for spatially extended interaction kernels, generalising previous constructions. We show that the GHD equation, in our general form and hence also in its conventional form, is Hamiltonian. This holds also including force terms representing inhomogeneous external potentials coupled to conserved densities. To this end, we introduce a new Poisson bracket on functionals of the fluid density, which is seen as our dynamical field variable. The total energy is the Hamiltonian whose flow under this Poisson bracket generates the GHD equation. The fluid density depends on two (real and spectral) variables so the GHD equation can be seen as a $2+1$-dimensional classical field theory. We further show the system admits an infinite set of conserved quantities that are in involution for our Poisson bracket, hinting at integrability of this field theory.","sentences":["Generalised Hydrodynamics (GHD) describes the large-scale inhomogeneous dynamics of integrable (or close to integrable) systems in one dimension of space, based on a central equation for the fluid density or quasi-particle density: the GHD equation.","We consider a new, general form of the GHD equation: we allow for spatially extended interaction kernels, generalising previous constructions.","We show that the GHD equation, in our general form and hence also in its conventional form, is Hamiltonian.","This holds also including force terms representing inhomogeneous external potentials coupled to conserved densities.","To this end, we introduce a new Poisson bracket on functionals of the fluid density, which is seen as our dynamical field variable.","The total energy is the Hamiltonian whose flow under this Poisson bracket generates the GHD equation.","The fluid density depends on two (real and spectral) variables so the GHD equation can be seen as a $2+1$-dimensional classical field theory.","We further show the system admits an infinite set of conserved quantities that are in involution for our Poisson bracket, hinting at integrability of this field theory."],"url":"http://arxiv.org/abs/2406.04924v1","category":"nlin.PS"}
{"created":"2024-06-07 13:28:10","title":"Hausdorff dimension of the Apollonian gasket","abstract":"The Apollonian gasket is a well-studied circle packing. Important properties of the packing, including the distribution of the circle radii, are governed by its Hausdorff dimension. No closed form is currently known for the Hausdorff dimension, and its computation is a special case of a more general and hard problem: effective, rigorous estimates of dimension of a parabolic limit set. In this paper we develop an efficient method for solving this problem which allows us to compute the dimension of the gasket to 128 decimal places and rigorously justify the error bounds. We expect our approach to generalise easily to other parabolic fractals.","sentences":["The Apollonian gasket is a well-studied circle packing.","Important properties of the packing, including the distribution of the circle radii, are governed by its Hausdorff dimension.","No closed form is currently known for the Hausdorff dimension, and its computation is a special case of a more general and hard problem: effective, rigorous estimates of dimension of a parabolic limit set.","In this paper we develop an efficient method for solving this problem which allows us to compute the dimension of the gasket to 128 decimal places and rigorously justify the error bounds.","We expect our approach to generalise easily to other parabolic fractals."],"url":"http://arxiv.org/abs/2406.04922v1","category":"math.DS"}
{"created":"2024-06-07 13:27:38","title":"Freezing, melting and the onset of glassiness in binary mixtures","abstract":"We clarify the relationship between freezing, melting and the onset of glassy dynamics in the prototypical glass-forming Kob-Andersen mixture. Our starting point is a precise definition of the onset of glassiness, as expressed by the emergence of an inflection in the time-dependent correlation functions. By scanning the temperature-composition phase diagram of the model, we find that the onset temperature barely varies with concentration, unlike the freezing temperature, which shows a marked drop at the eutectic point. We observe, however, a surprising correspondence between the onset of glassiness and the solidus line, along which the underlying crystalline phases melt. At fixed concentration, the freezing and onset temperatures display similar pressure dependencies, which are very well predicted by the isomorph theory and are encoded in the variation of the excess entropy. While our results rule out a general connection between thermodynamic metastability and glassiness, they call for a careful assessment of the role of crystalline precursors in glass-forming liquids.","sentences":["We clarify the relationship between freezing, melting and the onset of glassy dynamics in the prototypical glass-forming Kob-Andersen mixture.","Our starting point is a precise definition of the onset of glassiness, as expressed by the emergence of an inflection in the time-dependent correlation functions.","By scanning the temperature-composition phase diagram of the model, we find that the onset temperature barely varies with concentration, unlike the freezing temperature, which shows a marked drop at the eutectic point.","We observe, however, a surprising correspondence between the onset of glassiness and the solidus line, along which the underlying crystalline phases melt.","At fixed concentration, the freezing and onset temperatures display similar pressure dependencies, which are very well predicted by the isomorph theory and are encoded in the variation of the excess entropy.","While our results rule out a general connection between thermodynamic metastability and glassiness, they call for a careful assessment of the role of crystalline precursors in glass-forming liquids."],"url":"http://arxiv.org/abs/2406.04921v1","category":"cond-mat.soft"}
{"created":"2024-06-07 13:19:02","title":"Spatio-spectral optical fission in time-varying subwavelength layers","abstract":"Transparent conducting oxides are highly doped semiconductors that exhibit favourable characteristics when compared to metals, including reduced material losses, tuneable electronic and optical properties, and enhanced damage thresholds. Recently, the photonic community has renewed its attention towards these materials, recognizing their remarkable nonlinear optical properties in the near-infrared spectrum, a feature previously overlooked despite their long-standing application in photovoltaics and touchscreens. The exceptionally large and ultra-fast change of the refractive index, which can be optically induced in these compounds, extends beyond the boundaries of conventional perturbative analysis and makes this class of materials the closest approximation to a time-varying system, and a unique playground for studying a variety of novel phenomena within the domain of photon acceleration. Here we report the spatio-spectral fission of an ultra-fast pulse trespassing a thin film of aluminium zinc oxide with a non-stationary refractive index. By applying phase conservation to this time-varying layer, our model can account for both space and time refraction and explain in quantitative terms, the spatial separation of both the spectrum and energy. Our findings represent an example of extreme nonlinear phenomena on subwavelength propagation distances and shed light on the nature of several nonlinear effects recently reported not accounting for the full optical field distribution. Our work also provides new important insights into transparent conducting oxides transient optical properties which are critical for the ongoing research in photonic time crystals, on-chip generation of nonclassical states of light, integrated optical neural networks as well as ultra-fast beam steering and frequency division multiplexing","sentences":["Transparent conducting oxides are highly doped semiconductors that exhibit favourable characteristics when compared to metals, including reduced material losses, tuneable electronic and optical properties, and enhanced damage thresholds.","Recently, the photonic community has renewed its attention towards these materials, recognizing their remarkable nonlinear optical properties in the near-infrared spectrum, a feature previously overlooked despite their long-standing application in photovoltaics and touchscreens.","The exceptionally large and ultra-fast change of the refractive index, which can be optically induced in these compounds, extends beyond the boundaries of conventional perturbative analysis and makes this class of materials the closest approximation to a time-varying system, and a unique playground for studying a variety of novel phenomena within the domain of photon acceleration.","Here we report the spatio-spectral fission of an ultra-fast pulse trespassing a thin film of aluminium zinc oxide with a non-stationary refractive index.","By applying phase conservation to this time-varying layer, our model can account for both space and time refraction and explain in quantitative terms, the spatial separation of both the spectrum and energy.","Our findings represent an example of extreme nonlinear phenomena on subwavelength propagation distances and shed light on the nature of several nonlinear effects recently reported not accounting for the full optical field distribution.","Our work also provides new important insights into transparent conducting oxides transient optical properties which are critical for the ongoing research in photonic time crystals, on-chip generation of nonclassical states of light, integrated optical neural networks as well as ultra-fast beam steering and frequency division multiplexing"],"url":"http://arxiv.org/abs/2406.04917v1","category":"physics.optics"}
{"created":"2024-06-07 13:16:10","title":"Combinatorial Complex Score-based Diffusion Modelling through Stochastic Differential Equations","abstract":"Graph structures offer a versatile framework for representing diverse patterns in nature and complex systems, applicable across domains like molecular chemistry, social networks, and transportation systems. While diffusion models have excelled in generating various objects, generating graphs remains challenging. This thesis explores the potential of score-based generative models in generating such objects through a modelization as combinatorial complexes, which are powerful topological structures that encompass higher-order relationships.   In this thesis, we propose a unified framework by employing stochastic differential equations. We not only generalize the generation of complex objects such as graphs and hypergraphs, but we also unify existing generative modelling approaches such as Score Matching with Langevin dynamics and Denoising Diffusion Probabilistic Models. This innovation overcomes limitations in existing frameworks that focus solely on graph generation, opening up new possibilities in generative AI.   The experiment results showed that our framework could generate these complex objects, and could also compete against state-of-the-art approaches for mere graph and molecule generation tasks.","sentences":["Graph structures offer a versatile framework for representing diverse patterns in nature and complex systems, applicable across domains like molecular chemistry, social networks, and transportation systems.","While diffusion models have excelled in generating various objects, generating graphs remains challenging.","This thesis explores the potential of score-based generative models in generating such objects through a modelization as combinatorial complexes, which are powerful topological structures that encompass higher-order relationships.   ","In this thesis, we propose a unified framework by employing stochastic differential equations.","We not only generalize the generation of complex objects such as graphs and hypergraphs, but we also unify existing generative modelling approaches such as Score Matching with Langevin dynamics and Denoising Diffusion Probabilistic Models.","This innovation overcomes limitations in existing frameworks that focus solely on graph generation, opening up new possibilities in generative AI.   ","The experiment results showed that our framework could generate these complex objects, and could also compete against state-of-the-art approaches for mere graph and molecule generation tasks."],"url":"http://arxiv.org/abs/2406.04916v1","category":"cs.LG"}
{"created":"2024-06-07 13:11:04","title":"Submodular Framework for Structured-Sparse Optimal Transport","abstract":"Unbalanced optimal transport (UOT) has recently gained much attention due to its flexible framework for handling un-normalized measures and its robustness properties. In this work, we explore learning (structured) sparse transport plans in the UOT setting, i.e., transport plans have an upper bound on the number of non-sparse entries in each column (structured sparse pattern) or in the whole plan (general sparse pattern). We propose novel sparsity-constrained UOT formulations building on the recently explored maximum mean discrepancy based UOT. We show that the proposed optimization problem is equivalent to the maximization of a weakly submodular function over a uniform matroid or a partition matroid. We develop efficient gradient-based discrete greedy algorithms and provide the corresponding theoretical guarantees. Empirically, we observe that our proposed greedy algorithms select a diverse support set and we illustrate the efficacy of the proposed approach in various applications.","sentences":["Unbalanced optimal transport (UOT) has recently gained much attention due to its flexible framework for handling un-normalized measures and its robustness properties.","In this work, we explore learning (structured) sparse transport plans in the UOT setting, i.e., transport plans have an upper bound on the number of non-sparse entries in each column (structured sparse pattern) or in the whole plan (general sparse pattern).","We propose novel sparsity-constrained UOT formulations building on the recently explored maximum mean discrepancy based UOT.","We show that the proposed optimization problem is equivalent to the maximization of a weakly submodular function over a uniform matroid or a partition matroid.","We develop efficient gradient-based discrete greedy algorithms and provide the corresponding theoretical guarantees.","Empirically, we observe that our proposed greedy algorithms select a diverse support set and we illustrate the efficacy of the proposed approach in various applications."],"url":"http://arxiv.org/abs/2406.04914v1","category":"cs.LG"}
{"created":"2024-06-07 13:09:48","title":"Online Adaptation for Enhancing Imitation Learning Policies","abstract":"Imitation learning enables autonomous agents to learn from human examples, without the need for a reward signal. Still, if the provided dataset does not encapsulate the task correctly, or when the task is too complex to be modeled, such agents fail to reproduce the expert policy. We propose to recover from these failures through online adaptation. Our approach combines the action proposal coming from a pre-trained policy with relevant experience recorded by an expert. The combination results in an adapted action that closely follows the expert. Our experiments show that an adapted agent performs better than its pure imitation learning counterpart. Notably, adapted agents can achieve reasonable performance even when the base, non-adapted policy catastrophically fails.","sentences":["Imitation learning enables autonomous agents to learn from human examples, without the need for a reward signal.","Still, if the provided dataset does not encapsulate the task correctly, or when the task is too complex to be modeled, such agents fail to reproduce the expert policy.","We propose to recover from these failures through online adaptation.","Our approach combines the action proposal coming from a pre-trained policy with relevant experience recorded by an expert.","The combination results in an adapted action that closely follows the expert.","Our experiments show that an adapted agent performs better than its pure imitation learning counterpart.","Notably, adapted agents can achieve reasonable performance even when the base, non-adapted policy catastrophically fails."],"url":"http://arxiv.org/abs/2406.04913v1","category":"cs.AI"}
{"created":"2024-06-07 13:00:57","title":"PolyLUT-Add: FPGA-based LUT Inference with Wide Inputs","abstract":"FPGAs have distinct advantages as a technology for deploying deep neural networks (DNNs) at the edge. Lookup Table (LUT) based networks, where neurons are directly modelled using LUTs, help maximize this promise of offering ultra-low latency and high area efficiency on FPGAs. Unfortunately, LUT resource usage scales exponentially with the number of inputs to the LUT, restricting PolyLUT to small LUT sizes. This work introduces PolyLUT-Add, a technique that enhances neuron connectivity by combining $A$ PolyLUT sub-neurons via addition to improve accuracy. Moreover, we describe a novel architecture to improve its scalability. We evaluated our implementation over the MNIST, Jet Substructure classification and Network Intrusion Detection benchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT reduction of $1.3-7.7\\times$ with a $1.2-2.2\\times$ decrease in latency.","sentences":["FPGAs have distinct advantages as a technology for deploying deep neural networks (DNNs) at the edge.","Lookup Table (LUT) based networks, where neurons are directly modelled using LUTs, help maximize this promise of offering ultra-low latency and high area efficiency on FPGAs.","Unfortunately, LUT resource usage scales exponentially with the number of inputs to the LUT, restricting PolyLUT to small LUT sizes.","This work introduces PolyLUT-Add, a technique that enhances neuron connectivity by combining $A$ PolyLUT sub-neurons via addition to improve accuracy.","Moreover, we describe a novel architecture to improve its scalability.","We evaluated our implementation over the MNIST, Jet Substructure classification and Network Intrusion Detection benchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT reduction of $1.3-7.7\\times$ with a $1.2-2.2\\times$ decrease in latency."],"url":"http://arxiv.org/abs/2406.04910v1","category":"cs.LG"}
{"created":"2024-06-07 13:00:49","title":"Orchestrating time and color: a programmable source of high-dimensional entanglement","abstract":"High-dimensional encodings based on temporal modes (TMs) of photonic quantum states provide the foundations for a highly versatile and efficient quantum information science (QIS) framework. Here, we demonstrate a crucial building block for any QIS applications based on TMs: a programmable source of maximally entangled high-dimensional TM states. Our source is based on a parametric down-conversion process driven by a spectrally shaped pump pulse, which facilitates the generation of maximally entangled TM states with a well-defined dimensionality that can be chosen programmatically. We characterize the effective dimensionality of the generated states via measurements of second-order correlation functions and joint spectral intensities, demonstrating the generation of bi-photon TM states with a controlled dimensionality in up to 20 dimensions.","sentences":["High-dimensional encodings based on temporal modes (TMs) of photonic quantum states provide the foundations for a highly versatile and efficient quantum information science (QIS) framework.","Here, we demonstrate a crucial building block for any QIS applications based on TMs: a programmable source of maximally entangled high-dimensional TM states.","Our source is based on a parametric down-conversion process driven by a spectrally shaped pump pulse, which facilitates the generation of maximally entangled TM states with a well-defined dimensionality that can be chosen programmatically.","We characterize the effective dimensionality of the generated states via measurements of second-order correlation functions and joint spectral intensities, demonstrating the generation of bi-photon TM states with a controlled dimensionality in up to 20 dimensions."],"url":"http://arxiv.org/abs/2406.04909v1","category":"quant-ph"}
{"created":"2024-06-07 12:58:14","title":"RU-AI: A Large Multimodal Dataset for Machine Generated Content Detection","abstract":"The recent advancements in generative AI models, which can create realistic and human-like content, are significantly transforming how people communicate, create, and work. While the appropriate use of generative AI models can benefit the society, their misuse poses significant threats to data reliability and authentication. However, due to a lack of aligned multimodal datasets, effective and robust methods for detecting machine-generated content are still in the early stages of development. In this paper, we introduce RU-AI, a new large-scale multimodal dataset designed for the robust and efficient detection of machine-generated content in text, image, and voice. Our dataset is constructed from three large publicly available datasets: Flickr8K, COCO, and Places205, by combining the original datasets and their corresponding machine-generated pairs. Additionally, experimental results show that our proposed unified model, which incorporates a multimodal embedding module with a multilayer perceptron network, can effectively determine the origin of the data (i.e., original data samples or machine-generated ones) from RU-AI. However, future work is still required to address the remaining challenges posed by RU-AI. The source code and dataset are available at https://github.com/ZhihaoZhang97/RU-AI.","sentences":["The recent advancements in generative AI models, which can create realistic and human-like content, are significantly transforming how people communicate, create, and work.","While the appropriate use of generative AI models can benefit the society, their misuse poses significant threats to data reliability and authentication.","However, due to a lack of aligned multimodal datasets, effective and robust methods for detecting machine-generated content are still in the early stages of development.","In this paper, we introduce RU-AI, a new large-scale multimodal dataset designed for the robust and efficient detection of machine-generated content in text, image, and voice.","Our dataset is constructed from three large publicly available datasets: Flickr8K, COCO, and Places205, by combining the original datasets and their corresponding machine-generated pairs.","Additionally, experimental results show that our proposed unified model, which incorporates a multimodal embedding module with a multilayer perceptron network, can effectively determine the origin of the data (i.e., original data samples or machine-generated ones) from RU-AI.","However, future work is still required to address the remaining challenges posed by RU-AI.","The source code and dataset are available at https://github.com/ZhihaoZhang97/RU-AI."],"url":"http://arxiv.org/abs/2406.04906v1","category":"cs.CV"}
{"created":"2024-06-07 12:51:40","title":"Exotic compact objects: a recent numerical-relativity perspective","abstract":"Beyond black holes and neutron stars, new hypothetical compact objects have been proposed as potential astrophysical entities. In general, their properties have not yet been fully explored or understood, nor has it been proven whether or not they exist in nature. They are the so-called $\\textit{exotic compact objects}$, theoretical equilibrium configurations in the strong regime of gravity that involve new exotic physical phenomena deeply related to fundamental questions of theoretical physics (e.g., the nature of dark matter, the formation of singularities, or the presence of horizons). Among these exotic objects, there are those that require the existence of new fields and particles beyond the Standard Model, such as boson stars and ultralight bosons; those that seek to describe the dense equation of state of neutron stars as an even more extreme state of matter made up of free quarks; or those that exhibit additional properties related to extensions of General Relativity or even quantum gravity. However, in order to move from theoretical objects to astrophysical objects, their dynamics and stability must first be assessed by performing numerical-relativity simulations under the premise that solutions that are unstable on dynamical timescales will never completely form, being, at most, a transient state that will not be able to play any astrophysical role. Furthermore, numerical simulations also make it possible to extract the gravitational radiation from relevant astrophysical scenarios, such as the collapse of exotic stars or binary mergers, which could then be compared with current and upcoming LIGO-Virgo-KAGRA gravitational-wave detections.","sentences":["Beyond black holes and neutron stars, new hypothetical compact objects have been proposed as potential astrophysical entities.","In general, their properties have not yet been fully explored or understood, nor has it been proven whether or not they exist in nature.","They are the so-called $\\textit{exotic compact objects}$, theoretical equilibrium configurations in the strong regime of gravity that involve new exotic physical phenomena deeply related to fundamental questions of theoretical physics (e.g., the nature of dark matter, the formation of singularities, or the presence of horizons).","Among these exotic objects, there are those that require the existence of new fields and particles beyond the Standard Model, such as boson stars and ultralight bosons; those that seek to describe the dense equation of state of neutron stars as an even more extreme state of matter made up of free quarks; or those that exhibit additional properties related to extensions of General Relativity or even quantum gravity.","However, in order to move from theoretical objects to astrophysical objects, their dynamics and stability must first be assessed by performing numerical-relativity simulations under the premise that solutions that are unstable on dynamical timescales will never completely form, being, at most, a transient state that will not be able to play any astrophysical role.","Furthermore, numerical simulations also make it possible to extract the gravitational radiation from relevant astrophysical scenarios, such as the collapse of exotic stars or binary mergers, which could then be compared with current and upcoming LIGO-Virgo-KAGRA gravitational-wave detections."],"url":"http://arxiv.org/abs/2406.04901v1","category":"gr-qc"}
{"created":"2024-06-07 12:45:32","title":"Sliding Window 3-Objective Pareto Optimization for Problems with Chance Constraints","abstract":"Constrained single-objective problems have been frequently tackled by evolutionary multi-objective algorithms where the constraint is relaxed into an additional objective. Recently, it has been shown that Pareto optimization approaches using bi-objective models can be significantly sped up using sliding windows (Neumann and Witt, ECAI 2023). In this paper, we extend the sliding window approach to $3$-objective formulations for tackling chance constrained problems. On the theoretical side, we show that our new sliding window approach improves previous runtime bounds obtained in (Neumann and Witt, GECCO 2023) while maintaining the same approximation guarantees. Our experimental investigations for the chance constrained dominating set problem show that our new sliding window approach allows one to solve much larger instances in a much more efficient way than the 3-objective approach presented in (Neumann and Witt, GECCO 2023).","sentences":["Constrained single-objective problems have been frequently tackled by evolutionary multi-objective algorithms where the constraint is relaxed into an additional objective.","Recently, it has been shown that Pareto optimization approaches using bi-objective models can be significantly sped up using sliding windows (Neumann and Witt, ECAI 2023).","In this paper, we extend the sliding window approach to $3$-objective formulations for tackling chance constrained problems.","On the theoretical side, we show that our new sliding window approach improves previous runtime bounds obtained in (Neumann and Witt, GECCO 2023) while maintaining the same approximation guarantees.","Our experimental investigations for the chance constrained dominating set problem show that our new sliding window approach allows one to solve much larger instances in a much more efficient way than the 3-objective approach presented in (Neumann and Witt, GECCO 2023)."],"url":"http://arxiv.org/abs/2406.04899v1","category":"cs.NE"}
{"created":"2024-06-07 12:43:17","title":"Stabilizing Extreme Q-learning by Maclaurin Expansion","abstract":"In Extreme Q-learning (XQL), Gumbel Regression is performed with an assumed Gumbel distribution for the error distribution. This allows learning of the value function without sampling out-of-distribution actions and has shown excellent performance mainly in Offline RL. However, issues remained, including the exponential term in the loss function causing instability and the potential for an error distribution diverging from the Gumbel distribution. Therefore, we propose Maclaurin Expanded Extreme Q-learning to enhance stability. In this method, applying Maclaurin expansion to the loss function in XQL enhances stability against large errors. It also allows adjusting the error distribution assumption from normal to Gumbel based on the expansion order. Our method significantly stabilizes learning in Online RL tasks from DM Control, where XQL was previously unstable. Additionally, it improves performance in several Offline RL tasks from D4RL, where XQL already showed excellent results.","sentences":["In Extreme Q-learning (XQL), Gumbel Regression is performed with an assumed Gumbel distribution for the error distribution.","This allows learning of the value function without sampling out-of-distribution actions and has shown excellent performance mainly in Offline RL.","However, issues remained, including the exponential term in the loss function causing instability and the potential for an error distribution diverging from the Gumbel distribution.","Therefore, we propose Maclaurin Expanded Extreme Q-learning to enhance stability.","In this method, applying Maclaurin expansion to the loss function in XQL enhances stability against large errors.","It also allows adjusting the error distribution assumption from normal to Gumbel based on the expansion order.","Our method significantly stabilizes learning in Online RL tasks from DM Control, where XQL was previously unstable.","Additionally, it improves performance in several Offline RL tasks from D4RL, where XQL already showed excellent results."],"url":"http://arxiv.org/abs/2406.04896v1","category":"cs.LG"}
{"created":"2024-06-07 12:41:28","title":"Recovering a phase transition signal in simulated LISA data with a modulated galactic foreground","abstract":"Stochastic backgrounds of gravitational waves from primordial first-order phase transitions are a key probe of physics beyond the Standard Model. They represent one of the best prospects for observing or constraining new physics with the LISA gravitational wave observatory. However, the large foreground population of galactic binaries in the same frequency range represents a challenge, and will hinder the recovery of a stochastic background. To test the recoverability of a stochastic gravitational wave background, we use the LISA Simulation Suite to generate data incorporating both a stochastic background and an annually modulated foreground modelling the galactic binary population, and the Bayesian analysis code Cobaya to attempt to recover the model parameters. By applying the Deviance Information Criterion to compare models with and without a stochastic background we place bounds on the detectability of gravitational waves from first-order phase transitions. By further comparing models with and without the annual modulation, we show that exploiting the modulation improves the goodness-of-fit and gives a modest improvement to the bounds on detectable models.","sentences":["Stochastic backgrounds of gravitational waves from primordial first-order phase transitions are a key probe of physics beyond the Standard Model.","They represent one of the best prospects for observing or constraining new physics with the LISA gravitational wave observatory.","However, the large foreground population of galactic binaries in the same frequency range represents a challenge, and will hinder the recovery of a stochastic background.","To test the recoverability of a stochastic gravitational wave background, we use the LISA Simulation Suite to generate data incorporating both a stochastic background and an annually modulated foreground modelling the galactic binary population, and the Bayesian analysis code Cobaya to attempt to recover the model parameters.","By applying the Deviance Information Criterion to compare models with and without a stochastic background we place bounds on the detectability of gravitational waves from first-order phase transitions.","By further comparing models with and without the annual modulation, we show that exploiting the modulation improves the goodness-of-fit and gives a modest improvement to the bounds on detectable models."],"url":"http://arxiv.org/abs/2406.04894v1","category":"astro-ph.CO"}
{"created":"2024-06-07 12:38:18","title":"Dispersive Qubit Readout with Intrinsic Resonator Reset","abstract":"A key challenge in quantum computing is speeding up measurement and initialization. Here, we experimentally demonstrate a dispersive measurement method for superconducting qubits that simultaneously measures the qubit and returns the readout resonator to its initial state. The approach is based on universal analytical pulses and requires knowledge of the qubit and resonator parameters, but needs no direct optimization of the pulse shape, even when accounting for the nonlinearity of the system. Moreover, the method generalizes to measuring an arbitrary number of modes and states. For the qubit readout, we can drive the resonator to $\\sim 10^2$ photons and back to $\\sim 10^{-3}$ photons in less than $3 \\kappa^{-1}$, while still achieving a $T_1$-limited assignment error below 1\\%. We also present universal pulse shapes and experimental results for qutrit readout.","sentences":["A key challenge in quantum computing is speeding up measurement and initialization.","Here, we experimentally demonstrate a dispersive measurement method for superconducting qubits that simultaneously measures the qubit and returns the readout resonator to its initial state.","The approach is based on universal analytical pulses and requires knowledge of the qubit and resonator parameters, but needs no direct optimization of the pulse shape, even when accounting for the nonlinearity of the system.","Moreover, the method generalizes to measuring an arbitrary number of modes and states.","For the qubit readout, we can drive the resonator to $\\sim 10^2$ photons and back to $\\sim 10^{-3}$ photons in less than $3 \\kappa^{-1}$, while still achieving a $T_1$-limited assignment error below 1\\%.","We also present universal pulse shapes and experimental results for qutrit readout."],"url":"http://arxiv.org/abs/2406.04891v1","category":"quant-ph"}
{"created":"2024-06-07 12:36:31","title":"Enhancing Indoor Temperature Forecasting through Synthetic Data in Low-Data Environments","abstract":"Forecasting indoor temperatures is important to achieve efficient control of HVAC systems. In this task, the limited data availability presents a challenge as most of the available data is acquired during standard operation where extreme scenarios and transitory regimes such as major temperature increases or decreases are de-facto excluded. Acquisition of such data requires significant energy consumption and a dedicated facility, hindering the quantity and diversity of available data. Cost related constraints however do not allow for continuous year-around acquisition. To address this, we investigate the efficacy of data augmentation techniques leveraging SoTA AI-based methods for synthetic data generation. Inspired by practical and experimental motivations, we explore fusion strategies of real and synthetic data to improve forecasting models. This approach alleviates the need for continuously acquiring extensive time series data, especially in contexts involving repetitive heating and cooling cycles in buildings. In our evaluation 1) we assess the performance of synthetic data generators independently, particularly focusing on SoTA AI-based methods; 2) we measure the utility of incorporating synthetically augmented data in a subsequent forecasting tasks where we employ a simple model in two distinct scenarios: 1) we first examine an augmentation technique that combines real and synthetically generated data to expand the training dataset, 2) we delve into utilizing synthetic data to tackle dataset imbalances. Our results highlight the potential of synthetic data augmentation in enhancing forecasting accuracy while mitigating training variance. Through empirical experiments, we show significant improvements achievable by integrating synthetic data, thereby paving the way for more robust forecasting models in low-data regime.","sentences":["Forecasting indoor temperatures is important to achieve efficient control of HVAC systems.","In this task, the limited data availability presents a challenge as most of the available data is acquired during standard operation where extreme scenarios and transitory regimes such as major temperature increases or decreases are de-facto excluded.","Acquisition of such data requires significant energy consumption and a dedicated facility, hindering the quantity and diversity of available data.","Cost related constraints however do not allow for continuous year-around acquisition.","To address this, we investigate the efficacy of data augmentation techniques leveraging SoTA AI-based methods for synthetic data generation.","Inspired by practical and experimental motivations, we explore fusion strategies of real and synthetic data to improve forecasting models.","This approach alleviates the need for continuously acquiring extensive time series data, especially in contexts involving repetitive heating and cooling cycles in buildings.","In our evaluation 1) we assess the performance of synthetic data generators independently, particularly focusing on SoTA AI-based methods; 2) we measure the utility of incorporating synthetically augmented data in a subsequent forecasting tasks where we employ a simple model in two distinct scenarios: 1) we first examine an augmentation technique that combines real and synthetically generated data to expand the training dataset, 2) we delve into utilizing synthetic data to tackle dataset imbalances.","Our results highlight the potential of synthetic data augmentation in enhancing forecasting accuracy while mitigating training variance.","Through empirical experiments, we show significant improvements achievable by integrating synthetic data, thereby paving the way for more robust forecasting models in low-data regime."],"url":"http://arxiv.org/abs/2406.04890v1","category":"cs.LG"}
{"created":"2024-06-07 12:33:59","title":"Zero-Shot Video Editing through Adaptive Sliding Score Distillation","abstract":"The burgeoning field of text-based video generation (T2V) has reignited significant interest in the research of controllable video editing. Although pre-trained T2V-based editing models have achieved efficient editing capabilities, current works are still plagued by two major challenges. Firstly, the inherent limitations of T2V models lead to content inconsistencies and motion discontinuities between frames. Secondly, the notorious issue of over-editing significantly disrupts areas that are intended to remain unaltered. To address these challenges, our work aims to explore a robust video-based editing paradigm based on score distillation. Specifically, we propose an Adaptive Sliding Score Distillation strategy, which not only enhances the stability of T2V supervision but also incorporates both global and local video guidance to mitigate the impact of generation errors. Additionally, we modify the self-attention layers during the editing process to further preserve the key features of the original video. Extensive experiments demonstrate that these strategies enable us to effectively address the aforementioned challenges, achieving superior editing performance compared to existing state-of-the-art methods.","sentences":["The burgeoning field of text-based video generation (T2V) has reignited significant interest in the research of controllable video editing.","Although pre-trained T2V-based editing models have achieved efficient editing capabilities, current works are still plagued by two major challenges.","Firstly, the inherent limitations of T2V models lead to content inconsistencies and motion discontinuities between frames.","Secondly, the notorious issue of over-editing significantly disrupts areas that are intended to remain unaltered.","To address these challenges, our work aims to explore a robust video-based editing paradigm based on score distillation.","Specifically, we propose an Adaptive Sliding Score Distillation strategy, which not only enhances the stability of T2V supervision but also incorporates both global and local video guidance to mitigate the impact of generation errors.","Additionally, we modify the self-attention layers during the editing process to further preserve the key features of the original video.","Extensive experiments demonstrate that these strategies enable us to effectively address the aforementioned challenges, achieving superior editing performance compared to existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.04888v1","category":"cs.CV"}
{"created":"2024-06-07 12:32:48","title":"A variable version of the quasi-kernel conjecture","abstract":"A quasi-kernel of a digraph $D$ is an independent set $Q$ such that every vertex can reach $Q$ in at most two steps. A 48-year conjecture made by Erd\\H{o}s and Sz\\'ekely, denoted the \\textit{small QK conjecture}, says that every sink-free digraph contains a quasi-kernel of size at most $n/2$.   Recently, Spiro posed the \\textit{large QK conjecture}, that every sink-free digraph contains a quasi-kernel~$Q$ such that $|N^-[Q]|\\geq n/2$, and showed that it follows from the small QK conjecture.   In this paper, we establish that the large QK conjecture implies the small QK conjecture with a weaker constant. We also show that the large QK conjecture is equivalent to a sharp version of it, answering affirmatively a question of Spiro. We formulate variable versions of these conjectures, which are still open in general.   Not many digraphs are known to have quasi-kernels of size $(1-\\alpha)n$ or less. We show this for digraphs with bounded dichromatic number, by proving that every sink-free digraph contains a quasi-kernel of size at most $\\frac{kn}{k+1}$, where $k$ is the dichromatic number of $D$.","sentences":["A quasi-kernel of a digraph $D$ is an independent set $Q$ such that every vertex can reach $Q$ in at most two steps.","A 48-year conjecture made by Erd\\H{o}s and Sz\\'ekely, denoted the \\textit{small QK conjecture}, says that every sink-free digraph contains a quasi-kernel of size at most $n/2$.   Recently, Spiro posed the \\textit{large QK conjecture}, that every sink-free digraph contains a quasi-kernel~$Q$ such that $|N^-[Q]|\\geq n/2$, and showed that it follows from the small QK conjecture.   ","In this paper, we establish that the large QK conjecture implies the small QK conjecture with a weaker constant.","We also show that the large QK conjecture is equivalent to a sharp version of it, answering affirmatively a question of Spiro.","We formulate variable versions of these conjectures, which are still open in general.   ","Not many digraphs are known to have quasi-kernels of size $(1-\\alpha)n$ or less.","We show this for digraphs with bounded dichromatic number, by proving that every sink-free digraph contains a quasi-kernel of size at most $\\frac{kn}{k+1}$, where $k$ is the dichromatic number of $D$."],"url":"http://arxiv.org/abs/2406.04887v1","category":"math.CO"}
{"created":"2024-06-07 12:32:44","title":"Seeing the Unseen: Visual Metaphor Captioning for Videos","abstract":"Metaphors are a common communication tool used in our day-to-day life. The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored. Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts. As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos. Hence, we introduce a new VL task of describing the metaphors present in the videos in our work. To facilitate this novel task, we construct and release a manually created dataset with 705 videos and 2115 human-written captions, along with a new metric called Average Concept Distance (ACD), to automatically evaluate the creativity of the metaphors generated. We also propose a novel low-resource video metaphor captioning system: GIT-LLaVA, which obtains comparable performance to SoTA video language models on the proposed task. We perform a comprehensive analysis of existing video language models on this task and publish our dataset, models, and benchmark results to enable further research.","sentences":["Metaphors are a common communication tool used in our day-to-day life.","The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored.","Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts.","As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos.","Hence, we introduce a new VL task of describing the metaphors present in the videos in our work.","To facilitate this novel task, we construct and release a manually created dataset with 705 videos and 2115 human-written captions, along with a new metric called Average Concept Distance (ACD), to automatically evaluate the creativity of the metaphors generated.","We also propose a novel low-resource video metaphor captioning system: GIT-LLaVA, which obtains comparable performance to SoTA video language models on the proposed task.","We perform a comprehensive analysis of existing video language models on this task and publish our dataset, models, and benchmark results to enable further research."],"url":"http://arxiv.org/abs/2406.04886v1","category":"cs.CV"}
{"created":"2024-06-07 12:30:33","title":"Characters for extended affine Lie algebras; a combinatorial approach","abstract":"The behavior of objects associated with general extended affine Lie algebras is typically distinct from their counterparts in affine Lie algebras. Our research focuses on studying characters and Cartan automorphisms, which appear in the study of Chevalley involutions and Chevalley bases for extended affine Lie algebras. We show that for almost all extended affine Lie algebras, any finite order Cartan automorphism is diagonal, and its corresponding combinatorial map is a character.","sentences":["The behavior of objects associated with general extended affine Lie algebras is typically distinct from their counterparts in affine Lie algebras.","Our research focuses on studying characters and Cartan automorphisms, which appear in the study of Chevalley involutions and Chevalley bases for extended affine Lie algebras.","We show that for almost all extended affine Lie algebras, any finite order Cartan automorphism is diagonal, and its corresponding combinatorial map is a character."],"url":"http://arxiv.org/abs/2406.04885v1","category":"math.RT"}
{"created":"2024-06-07 12:26:34","title":"InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment","abstract":"Enabling robots to navigate following diverse language instructions in unexplored environments is an attractive goal for human-robot interaction. However, this goal is challenging because different navigation tasks require different strategies. The scarcity of instruction navigation data hinders training an instruction navigation model with varied strategies. Therefore, previous methods are all constrained to one specific type of navigation instruction. In this work, we propose InstructNav, a generic instruction navigation system. InstructNav makes the first endeavor to handle various instruction navigation tasks without any navigation training or pre-built maps. To reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify the planning process for different types of navigation instructions. Furthermore, we propose Multi-sourced Value Maps to model key elements in instruction navigation so that linguistic DCoN planning can be converted into robot actionable trajectories. With InstructNav, we complete the R2R-CE task in a zero-shot way for the first time and outperform many task-training methods. Besides, InstructNav also surpasses the previous SOTA method by 10.48% on the zero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN. Real robot experiments on diverse indoor scenes further demonstrate our method's robustness in coping with the environment and instruction variations.","sentences":["Enabling robots to navigate following diverse language instructions in unexplored environments is an attractive goal for human-robot interaction.","However, this goal is challenging because different navigation tasks require different strategies.","The scarcity of instruction navigation data hinders training an instruction navigation model with varied strategies.","Therefore, previous methods are all constrained to one specific type of navigation instruction.","In this work, we propose InstructNav, a generic instruction navigation system.","InstructNav makes the first endeavor to handle various instruction navigation tasks without any navigation training or pre-built maps.","To reach this goal, we introduce Dynamic Chain-of-Navigation (DCoN) to unify the planning process for different types of navigation instructions.","Furthermore, we propose Multi-sourced Value Maps to model key elements in instruction navigation so that linguistic DCoN planning can be converted into robot actionable trajectories.","With InstructNav, we complete the R2R-CE task in a zero-shot way for the first time and outperform many task-training methods.","Besides, InstructNav also surpasses the previous SOTA method by 10.48% on the zero-shot Habitat ObjNav and by 86.34% on demand-driven navigation DDN.","Real robot experiments on diverse indoor scenes further demonstrate our method's robustness in coping with the environment and instruction variations."],"url":"http://arxiv.org/abs/2406.04882v1","category":"cs.RO"}
{"created":"2024-06-07 12:21:25","title":"Fourth Post-Minkowskian Local-in-Time Conservative Dynamics of Binary Systems","abstract":"We compute the purely local-in-time (scale-free and logarithm-free) part of the conservative dynamics of gravitationally interacting two-body systems at the fourth post-Minkowskian order, and at the thirtiest order in velocity. The gauge-invariant content of this fourth post-Minkowskian local dynamics is given in two ways: (i) its contribution to the on-shell action (for both hyperboliclike and ellipticlike motions); and (ii) its contribution to the Effective One Body Hamiltonian (in energy gauge). Our computation capitalizes on the Tutti Frutti approach [Phys. Rev. Lett. \\textbf{123}, no.23, 231104 (2019)], and on recent post-Minkowskian advances [Phys. Rev. Lett. \\textbf{128}, no.16, 161103 (2022)], [Phys. Rev. Lett. \\textbf{128}, no.16, 161104 (2022)], and [Phys. Rev. Lett. \\textbf{132}, no.22, 221401 (2024)].","sentences":["We compute the purely local-in-time (scale-free and logarithm-free) part of the conservative dynamics of gravitationally interacting two-body systems at the fourth post-Minkowskian order, and at the thirtiest order in velocity.","The gauge-invariant content of this fourth post-Minkowskian local dynamics is given in two ways: (i) its contribution to the on-shell action (for both hyperboliclike and ellipticlike motions); and (ii) its contribution to the Effective One Body Hamiltonian (in energy gauge).","Our computation capitalizes on the Tutti Frutti approach [Phys. Rev. Lett.","\\textbf{123}, no.23, 231104 (2019)], and on recent post-Minkowskian advances [Phys. Rev. Lett.","\\textbf{128}, no.16, 161103 (2022)], [Phys. Rev. Lett.","\\textbf{128}, no.16, 161104 (2022)], and [Phys. Rev. Lett.","\\textbf{132}, no.22, 221401 (2024)]."],"url":"http://arxiv.org/abs/2406.04878v1","category":"gr-qc"}
{"created":"2024-06-07 12:14:24","title":"Approximate Bayesian Computation with Deep Learning and Conformal prediction","abstract":"Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these \"user-choices\". In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).   Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.","sentences":["Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods.","Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold.","Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these \"user-choices\".","In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold.","Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).   ","Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets.","Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature."],"url":"http://arxiv.org/abs/2406.04874v1","category":"stat.ME"}
{"created":"2024-06-07 12:12:25","title":"Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion Prior","abstract":"Video-to-video synthesis models face significant challenges, such as ensuring consistent character generation across frames, maintaining smooth temporal transitions, and preserving quality during fast motion. The introduction of joint fully cross-frame self-attention mechanisms has improved character consistency, but this comes at the cost of increased computational complexity. This full cross-frame self-attention mechanism also incorporates redundant details and limits the number of frames that can be jointly edited due to its computational cost. Moreover, the lack of frames in cross-frame attention adversely affects temporal consistency and visual quality. To address these limitations, we propose a new adaptive motion-guided cross-frame attention mechanism that drastically reduces complexity while preserving semantic details and temporal consistency. Specifically, we selectively incorporate the moving regions of successive frames in cross-frame attention and sparsely include stationary regions based on optical flow sampling. This technique allows for an increased number of jointly edited frames without additional computational overhead. For longer duration of video editing, existing methods primarily focus on frame interpolation or flow-warping from jointly edited keyframes, which often results in blurry frames or reduced temporal consistency. To improve this, we introduce KV-caching of jointly edited frames and reuse the same KV across all intermediate frames, significantly enhancing both intermediate frame quality and temporal consistency. Overall, our motion-sampling method enables the use of around three times more keyframes than existing joint editing methods while maintaining superior prediction quality. Ada-VE achieves up to 4x speed-up when using fully-extended self-attention across 40 frames for joint editing, without compromising visual quality or temporal consistency.","sentences":["Video-to-video synthesis models face significant challenges, such as ensuring consistent character generation across frames, maintaining smooth temporal transitions, and preserving quality during fast motion.","The introduction of joint fully cross-frame self-attention mechanisms has improved character consistency, but this comes at the cost of increased computational complexity.","This full cross-frame self-attention mechanism also incorporates redundant details and limits the number of frames that can be jointly edited due to its computational cost.","Moreover, the lack of frames in cross-frame attention adversely affects temporal consistency and visual quality.","To address these limitations, we propose a new adaptive motion-guided cross-frame attention mechanism that drastically reduces complexity while preserving semantic details and temporal consistency.","Specifically, we selectively incorporate the moving regions of successive frames in cross-frame attention and sparsely include stationary regions based on optical flow sampling.","This technique allows for an increased number of jointly edited frames without additional computational overhead.","For longer duration of video editing, existing methods primarily focus on frame interpolation or flow-warping from jointly edited keyframes, which often results in blurry frames or reduced temporal consistency.","To improve this, we introduce KV-caching of jointly edited frames and reuse the same KV across all intermediate frames, significantly enhancing both intermediate frame quality and temporal consistency.","Overall, our motion-sampling method enables the use of around three times more keyframes than existing joint editing methods while maintaining superior prediction quality.","Ada-VE achieves up to 4x speed-up when using fully-extended self-attention across 40 frames for joint editing, without compromising visual quality or temporal consistency."],"url":"http://arxiv.org/abs/2406.04873v1","category":"cs.CV"}
{"created":"2024-06-07 12:12:20","title":"Diversified Batch Selection for Training Acceleration","abstract":"The remarkable success of modern machine learning models on large datasets often demands extensive training time and resource consumption. To save cost, a prevalent research line, known as online batch selection, explores selecting informative subsets during the training process. Although recent efforts achieve advancements by measuring the impact of each sample on generalization, their reliance on additional reference models inherently limits their practical applications, when there are no such ideal models available. On the other hand, the vanilla reference-model-free methods involve independently scoring and selecting data in a sample-wise manner, which sacrifices the diversity and induces the redundancy. To tackle this dilemma, we propose Diversified Batch Selection (DivBS), which is reference-model-free and can efficiently select diverse and representative samples. Specifically, we define a novel selection objective that measures the group-wise orthogonalized representativeness to combat the redundancy issue of previous sample-wise criteria, and provide a principled selection-efficient realization. Extensive experiments across various tasks demonstrate the significant superiority of DivBS in the performance-speedup trade-off. The code is publicly available.","sentences":["The remarkable success of modern machine learning models on large datasets often demands extensive training time and resource consumption.","To save cost, a prevalent research line, known as online batch selection, explores selecting informative subsets during the training process.","Although recent efforts achieve advancements by measuring the impact of each sample on generalization, their reliance on additional reference models inherently limits their practical applications, when there are no such ideal models available.","On the other hand, the vanilla reference-model-free methods involve independently scoring and selecting data in a sample-wise manner, which sacrifices the diversity and induces the redundancy.","To tackle this dilemma, we propose Diversified Batch Selection (DivBS), which is reference-model-free and can efficiently select diverse and representative samples.","Specifically, we define a novel selection objective that measures the group-wise orthogonalized representativeness to combat the redundancy issue of previous sample-wise criteria, and provide a principled selection-efficient realization.","Extensive experiments across various tasks demonstrate the significant superiority of DivBS in the performance-speedup trade-off.","The code is publicly available."],"url":"http://arxiv.org/abs/2406.04872v1","category":"cs.LG"}
{"created":"2024-06-07 12:07:09","title":"Deep learning for precipitation nowcasting: A survey from the perspective of time series forecasting","abstract":"Deep learning-based time series forecasting has dominated the short-term precipitation forecasting field with the help of its ability to estimate motion flow in high-resolution datasets. The growing interest in precipitation nowcasting offers substantial opportunities for the advancement of current forecasting technologies. Nevertheless, there has been a scarcity of in-depth surveys of time series precipitation forecasting using deep learning. Thus, this paper systemically reviews recent progress in time series precipitation forecasting models. Specifically, we investigate the following key points within background components, covering: i) preprocessing, ii) objective functions, and iii) evaluation metrics. We then categorize forecasting models into \\textit{recursive} and \\textit{multiple} strategies based on their approaches to predict future frames, investigate the impacts of models using the strategies, and performance assessments. Finally, we evaluate current deep learning-based models for precipitation forecasting on a public benchmark, discuss their limitations and challenges, and present some promising research directions. Our contribution lies in providing insights for a better understanding of time series precipitation forecasting and in aiding the development of robust AI solutions for the future.","sentences":["Deep learning-based time series forecasting has dominated the short-term precipitation forecasting field with the help of its ability to estimate motion flow in high-resolution datasets.","The growing interest in precipitation nowcasting offers substantial opportunities for the advancement of current forecasting technologies.","Nevertheless, there has been a scarcity of in-depth surveys of time series precipitation forecasting using deep learning.","Thus, this paper systemically reviews recent progress in time series precipitation forecasting models.","Specifically, we investigate the following key points within background components, covering: i) preprocessing, ii) objective functions, and iii) evaluation metrics.","We then categorize forecasting models into \\textit{recursive} and \\textit{multiple} strategies based on their approaches to predict future frames, investigate the impacts of models using the strategies, and performance assessments.","Finally, we evaluate current deep learning-based models for precipitation forecasting on a public benchmark, discuss their limitations and challenges, and present some promising research directions.","Our contribution lies in providing insights for a better understanding of time series precipitation forecasting and in aiding the development of robust AI solutions for the future."],"url":"http://arxiv.org/abs/2406.04867v1","category":"cs.LG"}
{"created":"2024-06-07 12:00:56","title":"Entangled magnon-pair generation in a driven synthetic antiferromagnet","abstract":"Understanding, manipulating, and using magnons - the quanta of spin waves - for energy-efficient applications is one of the primary goals of magnonics. In this paper, we consider a synthetic antiferromagnet in which one of the ferromagnetic layers is driven by spin-orbit torque. We find that under specific conditions for the magnitude of the spin-orbit torque and field, magnon pairs are spontaneously produced by quantum fluctuations in a way that is similar to Hawking pair production near black-hole horizons. One of the magnons is generated near the interface with the spacer layer in one of the magnetic layers of the synthetic antiferromagnet, while the other magnon is produced in the other magnetic layer. We compute the magnon current due to these spontaneously generated magnon pairs and estimate the temperature below which they should become observable. Additionally, we find that the magnons are entangled, which makes them interesting for future applications in quantum magnonics.","sentences":["Understanding, manipulating, and using magnons - the quanta of spin waves - for energy-efficient applications is one of the primary goals of magnonics.","In this paper, we consider a synthetic antiferromagnet in which one of the ferromagnetic layers is driven by spin-orbit torque.","We find that under specific conditions for the magnitude of the spin-orbit torque and field, magnon pairs are spontaneously produced by quantum fluctuations in a way that is similar to Hawking pair production near black-hole horizons.","One of the magnons is generated near the interface with the spacer layer in one of the magnetic layers of the synthetic antiferromagnet, while the other magnon is produced in the other magnetic layer.","We compute the magnon current due to these spontaneously generated magnon pairs and estimate the temperature below which they should become observable.","Additionally, we find that the magnons are entangled, which makes them interesting for future applications in quantum magnonics."],"url":"http://arxiv.org/abs/2406.04865v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-07 11:53:04","title":"Cactus-like Metamaterial Structures for Electromagnetically Induced Transparency at THz frequencies","abstract":"THz metamaterials present unique opportunities for next generation technologies and applications, as they can fill the ``THz gap'' originating from the weak response of natural materials in this regime, providing a variety of novel or advanced electromagnetic wave control components and systems. Here, we propose a novel metamaterial design, made of three-dimensional, metallic, \"cactus like\" meta-atoms, showing electromagnetically induced transparency (EIT) and enhanced refractive index sensing performance at low THz frequencies. Following a detailed theoretical analysis, the structure is realized experimentally using multi-photon polymerization and electroless silver plating. The experimental characterization results obtained through THz time domain spectroscopy validate the corresponding numerical data, verifying the high potential of the proposed structure in slow light and sensing applications.","sentences":["THz metamaterials present unique opportunities for next generation technologies and applications, as they can fill the ``THz gap'' originating from the weak response of natural materials in this regime, providing a variety of novel or advanced electromagnetic wave control components and systems.","Here, we propose a novel metamaterial design, made of three-dimensional, metallic, \"cactus like\" meta-atoms, showing electromagnetically induced transparency (EIT) and enhanced refractive index sensing performance at low THz frequencies.","Following a detailed theoretical analysis, the structure is realized experimentally using multi-photon polymerization and electroless silver plating.","The experimental characterization results obtained through THz time domain spectroscopy validate the corresponding numerical data, verifying the high potential of the proposed structure in slow light and sensing applications."],"url":"http://arxiv.org/abs/2406.04862v1","category":"physics.optics"}
{"created":"2024-06-07 11:44:50","title":"Stochastic full waveform inversion with deep generative prior for uncertainty quantification","abstract":"To obtain high-resolution images of subsurface structures from seismic data, seismic imaging techniques such as Full Waveform Inversion (FWI) serve as crucial tools. However, FWI involves solving a nonlinear and often non-unique inverse problem, presenting challenges such as local minima trapping and inadequate handling of inherent uncertainties. In addressing these challenges, we propose leveraging deep generative models as the prior distribution of geophysical parameters for stochastic Bayesian inversion. This approach integrates the adjoint state gradient for efficient back-propagation from the numerical solution of partial differential equations. Additionally, we introduce explicit and implicit variational Bayesian inference methods. The explicit method computes variational distribution density using a normalizing flow-based neural network, enabling computation of the Bayesian posterior of parameters. Conversely, the implicit method employs an inference network attached to a pretrained generative model to estimate density, incorporating an entropy estimator. Furthermore, we also experimented with the Stein Variational Gradient Descent (SVGD) method as another variational inference technique, using particles. We compare these variational Bayesian inference methods with conventional Markov chain Monte Carlo (McMC) sampling. Each method is able to quantify uncertainties and to generate seismic data-conditioned realizations of subsurface geophysical parameters. This framework provides insights into subsurface structures while accounting for inherent uncertainties.","sentences":["To obtain high-resolution images of subsurface structures from seismic data, seismic imaging techniques such as Full Waveform Inversion (FWI) serve as crucial tools.","However, FWI involves solving a nonlinear and often non-unique inverse problem, presenting challenges such as local minima trapping and inadequate handling of inherent uncertainties.","In addressing these challenges, we propose leveraging deep generative models as the prior distribution of geophysical parameters for stochastic Bayesian inversion.","This approach integrates the adjoint state gradient for efficient back-propagation from the numerical solution of partial differential equations.","Additionally, we introduce explicit and implicit variational Bayesian inference methods.","The explicit method computes variational distribution density using a normalizing flow-based neural network, enabling computation of the Bayesian posterior of parameters.","Conversely, the implicit method employs an inference network attached to a pretrained generative model to estimate density, incorporating an entropy estimator.","Furthermore, we also experimented with the Stein Variational Gradient Descent (SVGD) method as another variational inference technique, using particles.","We compare these variational Bayesian inference methods with conventional Markov chain Monte Carlo (McMC) sampling.","Each method is able to quantify uncertainties and to generate seismic data-conditioned realizations of subsurface geophysical parameters.","This framework provides insights into subsurface structures while accounting for inherent uncertainties."],"url":"http://arxiv.org/abs/2406.04859v1","category":"physics.geo-ph"}
{"created":"2024-06-07 11:33:21","title":"Digital assistant in a point of sales","abstract":"This article investigates the deployment of a Voice User Interface (VUI)-powered digital assistant in a retail setting and assesses its impact on customer engagement and service efficiency. The study explores how digital assistants can enhance user interactions through advanced conversational capabilities with multilingual support. By integrating a digital assistant into a high-traffic retail environment, we evaluate its effectiveness in improving the quality of customer service and operational efficiency. Data collected during the experiment demonstrate varied impacts on customer interaction, revealing insights into the future optimizations of digital assistant technologies in customer-facing roles. This study contributes to the understanding of digital transformation strategies within the customer relations domain emphasizing the need for service flexibility and user-centric design in modern retail stores.","sentences":["This article investigates the deployment of a Voice User Interface (VUI)-powered digital assistant in a retail setting and assesses its impact on customer engagement and service efficiency.","The study explores how digital assistants can enhance user interactions through advanced conversational capabilities with multilingual support.","By integrating a digital assistant into a high-traffic retail environment, we evaluate its effectiveness in improving the quality of customer service and operational efficiency.","Data collected during the experiment demonstrate varied impacts on customer interaction, revealing insights into the future optimizations of digital assistant technologies in customer-facing roles.","This study contributes to the understanding of digital transformation strategies within the customer relations domain emphasizing the need for service flexibility and user-centric design in modern retail stores."],"url":"http://arxiv.org/abs/2406.04851v1","category":"cs.HC"}
{"created":"2024-06-07 11:32:24","title":"Expected Lipschitz-Killing curvatures for spin random fields and other non-isotropic fields","abstract":"Spherical spin random fields are used to model the Cosmic Microwave Background polarization, the study of which is at the heart of modern Cosmology and will be the subject of the LITEBIRD mission, in the 2030s. Its scope is to collect datas to test the theoretical predictions of the Cosmic Inflation model. In particular, the Minkowski functionals, or the Lipschitz-Killing curvatures, of excursion sets can be used to detect deviations from Gaussianity and anisotropies of random fields, being fine descriptors of their geometry and topology.   In this paper we give an explicit, non-asymptotic, formula for the expectation of the Lipshitz-Killing curvatures of the excursion set of the real part of an arbitrary left-invariant Gaussian spin spherical random field, seen as a field on $SO(3)$. Our findings are coherent with the asymptotic ones presented in Carr\\'on Duque, J. et al. \"Minkowski Functionals in $SO(3)$ for the spin-2 CMB polarisation field\", Journal of Cosmology and Astroparticle Physics (2024). We also give explicit expressions for the Adler-Taylor metric, and its curvature. We obtain such result as an application of a general formula that applies to any nondegenerate Gaussian random field defined on an arbitrary three dimensional compact Riemannian manifold. The novelty is that the Lipschitz-Killing curvatures are computed with respect to an arbitrary metric, possibly different than the Adler-Taylor metric of the field.","sentences":["Spherical spin random fields are used to model the Cosmic Microwave Background polarization, the study of which is at the heart of modern Cosmology and will be the subject of the LITEBIRD mission, in the 2030s.","Its scope is to collect datas to test the theoretical predictions of the Cosmic Inflation model.","In particular, the Minkowski functionals, or the Lipschitz-Killing curvatures, of excursion sets can be used to detect deviations from Gaussianity and anisotropies of random fields, being fine descriptors of their geometry and topology.   ","In this paper we give an explicit, non-asymptotic, formula for the expectation of the Lipshitz-Killing curvatures of the excursion set of the real part of an arbitrary left-invariant Gaussian spin spherical random field, seen as a field on $SO(3)$. Our findings are coherent with the asymptotic ones presented in Carr\\'on Duque, J. et al.","\"Minkowski Functionals in $SO(3)$ for the spin-2 CMB polarisation field\", Journal of Cosmology and Astroparticle Physics (2024).","We also give explicit expressions for the Adler-Taylor metric, and its curvature.","We obtain such result as an application of a general formula that applies to any nondegenerate Gaussian random field defined on an arbitrary three dimensional compact Riemannian manifold.","The novelty is that the Lipschitz-Killing curvatures are computed with respect to an arbitrary metric, possibly different than the Adler-Taylor metric of the field."],"url":"http://arxiv.org/abs/2406.04850v1","category":"math.PR"}
{"created":"2024-06-07 11:32:00","title":"Dynamic prediction of death risk given a renewal hospitalization process","abstract":"Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making. This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians. Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors. The framework accommodates various submodels for the hospitalization process. In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling. In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death. This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters. We apply our methodology to a prospective, observational cohort study of 401 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.16 years. We find that more concentrated hospitalizations increase the risk of death.","sentences":["Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making.","This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians.","Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors.","The framework accommodates various submodels for the hospitalization process.","In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling.","In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death.","This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters.","We apply our methodology to a prospective, observational cohort study of 401 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.16 years.","We find that more concentrated hospitalizations increase the risk of death."],"url":"http://arxiv.org/abs/2406.04849v1","category":"stat.ME"}
{"created":"2024-06-07 11:27:18","title":"CTBENCH: A Library and Benchmark for Certified Training","abstract":"Training certifiably robust neural networks is an important but challenging task. While many algorithms for (deterministic) certified training have been proposed, they are often evaluated on different training schedules, certification methods, and systematically under-tuned hyperparameters, making it difficult to compare their performance. To address this challenge, we introduce CTBENCH, a unified library and a high-quality benchmark for certified training that evaluates all algorithms under fair settings and systematically tuned hyperparameters. We show that (1) almost all algorithms in CTBENCH surpass the corresponding reported performance in literature in the magnitude of algorithmic improvements, thus establishing new state-of-the-art, and (2) the claimed advantage of recent algorithms drops significantly when we enhance the outdated baselines with a fair training schedule, a fair certification method and well-tuned hyperparameters. Based on CTBENCH, we provide new insights into the current state of certified training and suggest future research directions. We are confident that CTBENCH will serve as a benchmark and testbed for future research in certified training.","sentences":["Training certifiably robust neural networks is an important but challenging task.","While many algorithms for (deterministic) certified training have been proposed, they are often evaluated on different training schedules, certification methods, and systematically under-tuned hyperparameters, making it difficult to compare their performance.","To address this challenge, we introduce CTBENCH, a unified library and a high-quality benchmark for certified training that evaluates all algorithms under fair settings and systematically tuned hyperparameters.","We show that (1) almost all algorithms in CTBENCH surpass the corresponding reported performance in literature in the magnitude of algorithmic improvements, thus establishing new state-of-the-art, and (2) the claimed advantage of recent algorithms drops significantly when we enhance the outdated baselines with a fair training schedule, a fair certification method and well-tuned hyperparameters.","Based on CTBENCH, we provide new insights into the current state of certified training and suggest future research directions.","We are confident that CTBENCH will serve as a benchmark and testbed for future research in certified training."],"url":"http://arxiv.org/abs/2406.04848v1","category":"cs.LG"}
{"created":"2024-06-07 11:19:30","title":"FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models","abstract":"Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.","sentences":["Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM).","Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy.","However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios.","Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community.","FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747.","Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios.","Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration).","We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons.","Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench."],"url":"http://arxiv.org/abs/2406.04845v1","category":"cs.CL"}
{"created":"2024-06-07 11:16:17","title":"Variational Flow Matching for Graph Generation","abstract":"We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.","sentences":["We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM).","Based on this formulation we develop CatFlow, a flow matching method for categorical data.","CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks.","In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory.","We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases.","We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective.","We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks.","In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models."],"url":"http://arxiv.org/abs/2406.04843v1","category":"cs.LG"}
{"created":"2024-06-07 11:15:03","title":"3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation","abstract":"Referring video object segmentation (RVOS) relies on natural language expressions to segment target objects in video, emphasizing modeling dense text-video relations. The current RVOS methods typically use independently pre-trained vision and language models as backbones, resulting in a significant domain gap between video and text. In cross-modal feature interaction, text features are only used as query initialization and do not fully utilize important information in the text. In this work, we propose using frozen pre-trained vision-language models (VLM) as backbones, with a specific emphasis on enhancing cross-modal feature interaction. Firstly, we use frozen convolutional CLIP backbone to generate feature-aligned vision and text features, alleviating the issue of domain gap and reducing training costs. Secondly, we add more cross-modal feature fusion in the pipeline to enhance the utilization of multi-modal information. Furthermore, we propose a novel video query initialization method to generate higher quality video queries. Without bells and whistles, our method achieved 51.5 J&F on the MeViS test set and ranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation.","sentences":["Referring video object segmentation (RVOS) relies on natural language expressions to segment target objects in video, emphasizing modeling dense text-video relations.","The current RVOS methods typically use independently pre-trained vision and language models as backbones, resulting in a significant domain gap between video and text.","In cross-modal feature interaction, text features are only used as query initialization and do not fully utilize important information in the text.","In this work, we propose using frozen pre-trained vision-language models (VLM) as backbones, with a specific emphasis on enhancing cross-modal feature interaction.","Firstly, we use frozen convolutional CLIP backbone to generate feature-aligned vision and text features, alleviating the issue of domain gap and reducing training costs.","Secondly, we add more cross-modal feature fusion in the pipeline to enhance the utilization of multi-modal information.","Furthermore, we propose a novel video query initialization method to generate higher quality video queries.","Without bells and whistles, our method achieved 51.5 J&F on the MeViS test set and ranked 3rd place for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation."],"url":"http://arxiv.org/abs/2406.04842v1","category":"cs.CV"}
{"created":"2024-06-07 11:13:38","title":"Primitive Agentic First-Order Optimization","abstract":"Efficient numerical optimization methods can improve performance and reduce the environmental impact of computing in many applications. This work presents a proof-of-concept study combining primitive state representations and agent-environment interactions as first-order optimizers in the setting of budget-limited optimization. Through reinforcement learning (RL) over a set of training instances of an optimization problem class, optimal policies for sequential update selection of algorithmic iteration steps are approximated in generally formulated low-dimensional partial state representations that consider aspects of progress and resource use. For the investigated case studies, deployment of the trained agents to unseen instances of the quadratic optimization problem classes outperformed conventional optimal algorithms with optimized hyperparameters. The results show that elementary RL methods combined with succinct partial state representations can be used as heuristics to manage complexity in RL-based optimization, paving the way for agentic optimization approaches.","sentences":["Efficient numerical optimization methods can improve performance and reduce the environmental impact of computing in many applications.","This work presents a proof-of-concept study combining primitive state representations and agent-environment interactions as first-order optimizers in the setting of budget-limited optimization.","Through reinforcement learning (RL) over a set of training instances of an optimization problem class, optimal policies for sequential update selection of algorithmic iteration steps are approximated in generally formulated low-dimensional partial state representations that consider aspects of progress and resource use.","For the investigated case studies, deployment of the trained agents to unseen instances of the quadratic optimization problem classes outperformed conventional optimal algorithms with optimized hyperparameters.","The results show that elementary RL methods combined with succinct partial state representations can be used as heuristics to manage complexity in RL-based optimization, paving the way for agentic optimization approaches."],"url":"http://arxiv.org/abs/2406.04841v1","category":"cs.LG"}
{"created":"2024-06-07 11:13:03","title":"TraceableSpeech: Towards Proactively Traceable Text-to-Speech with Watermarking","abstract":"Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations.","sentences":["Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech.","However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility.","In addition, these approaches are limited in robustness and flexibility.","To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality.","Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation.","Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks.","It also can apply to speech of various durations."],"url":"http://arxiv.org/abs/2406.04840v1","category":"cs.SD"}
{"created":"2024-06-07 11:12:07","title":"Emergent Universe from an Unstable de Sitter Phase","abstract":"In the Emergent scenario, the Universe should evolve from a non-singular state replacing the typical singularity of General Relativity, for any initial condition. For the scalar field model in [1] we show that only a subset of initial conditions leads to emergence, either from a static state (an Einstein model), or from a de Sitter state.   Assuming a scenario based on CDM interacting with a Dark Energy fluid, we show that in general flat and open models expand from a non-singular unstable de Sitter state at high energies; for closed models this state is a transition phase with a bounce. A subset of these models are qualitatively in agreement with the observable Universe, accelerating at high energies, going through a matter-dominated decelerated era, then accelerating toward a de Sitter phase.","sentences":["In the Emergent scenario, the Universe should evolve from a non-singular state replacing the typical singularity of General Relativity, for any initial condition.","For the scalar field model in [1] we show that only a subset of initial conditions leads to emergence, either from a static state (an Einstein model), or from a de Sitter state.   ","Assuming a scenario based on CDM interacting with a Dark Energy fluid, we show that in general flat and open models expand from a non-singular unstable de Sitter state at high energies; for closed models this state is a transition phase with a bounce.","A subset of these models are qualitatively in agreement with the observable Universe, accelerating at high energies, going through a matter-dominated decelerated era, then accelerating toward a de Sitter phase."],"url":"http://arxiv.org/abs/2406.04839v1","category":"gr-qc"}
{"created":"2024-06-07 11:10:07","title":"Algorithms for learning value-aligned policies considering admissibility relaxation","abstract":"The emerging field of \\emph{value awareness engineering} claims that software agents and systems should be value-aware, i.e. they must make decisions in accordance with human values. In this context, such agents must be capable of explicitly reasoning as to how far different courses of action are aligned with these values. For this purpose, values are often modelled as preferences over states or actions, which are then aggregated to determine the sequences of actions that are maximally aligned with a certain value. Recently, additional value admissibility constraints at this level have been considered as well.   However, often relaxed versions of these constraints are needed, and this increases considerably the complexity of computing value-aligned policies. To obtain efficient algorithms that make value-aligned decisions considering admissibility relaxation, we propose the use of learning techniques, in particular, we have used constrained reinforcement learning algorithms. In this paper, we present two algorithms, $\\epsilon\\text{-}ADQL$ for strategies based on local alignment and its extension $\\epsilon\\text{-}CADQL$ for a sequence of decisions. We have validated their efficiency in a water distribution problem in a drought scenario.","sentences":["The emerging field of \\emph{value awareness engineering} claims that software agents and systems should be value-aware, i.e. they must make decisions in accordance with human values.","In this context, such agents must be capable of explicitly reasoning as to how far different courses of action are aligned with these values.","For this purpose, values are often modelled as preferences over states or actions, which are then aggregated to determine the sequences of actions that are maximally aligned with a certain value.","Recently, additional value admissibility constraints at this level have been considered as well.   ","However, often relaxed versions of these constraints are needed, and this increases considerably the complexity of computing value-aligned policies.","To obtain efficient algorithms that make value-aligned decisions considering admissibility relaxation, we propose the use of learning techniques, in particular, we have used constrained reinforcement learning algorithms.","In this paper, we present two algorithms, $\\epsilon\\text{-}ADQL$ for strategies based on local alignment and its extension $\\epsilon\\text{-}CADQL$ for a sequence of decisions.","We have validated their efficiency in a water distribution problem in a drought scenario."],"url":"http://arxiv.org/abs/2406.04838v1","category":"cs.AI"}
{"created":"2024-06-07 11:09:13","title":"Revisiting Catastrophic Forgetting in Large Language Model Tuning","abstract":"Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.","sentences":["Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data.","It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated.","This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs.","Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape.","Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF.","Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF."],"url":"http://arxiv.org/abs/2406.04836v1","category":"cs.CL"}
{"created":"2024-06-07 11:01:15","title":"Annotating FrameNet via Structure-Conditioned Language Generation","abstract":"Despite the remarkable generative capabilities of language models in producing naturalistic language, their effectiveness on explicit manipulation and generation of linguistic structures remain understudied. In this paper, we investigate the task of generating new sentences preserving a given semantic structure, following the FrameNet formalism. We propose a framework to produce novel frame-semantically annotated sentences following an overgenerate-and-filter approach. Our results show that conditioning on rich, explicit semantic information tends to produce generations with high human acceptance, under both prompting and finetuning. Our generated frame-semantic structured annotations are effective at training data augmentation for frame-semantic role labeling in low-resource settings; however, we do not see benefits under higher resource settings. Our study concludes that while generating high-quality, semantically rich data might be within reach, the downstream utility of such generations remains to be seen, highlighting the outstanding challenges with automating linguistic annotation tasks.","sentences":["Despite the remarkable generative capabilities of language models in producing naturalistic language, their effectiveness on explicit manipulation and generation of linguistic structures remain understudied.","In this paper, we investigate the task of generating new sentences preserving a given semantic structure, following the FrameNet formalism.","We propose a framework to produce novel frame-semantically annotated sentences following an overgenerate-and-filter approach.","Our results show that conditioning on rich, explicit semantic information tends to produce generations with high human acceptance, under both prompting and finetuning.","Our generated frame-semantic structured annotations are effective at training data augmentation for frame-semantic role labeling in low-resource settings; however, we do not see benefits under higher resource settings.","Our study concludes that while generating high-quality, semantically rich data might be within reach, the downstream utility of such generations remains to be seen, highlighting the outstanding challenges with automating linguistic annotation tasks."],"url":"http://arxiv.org/abs/2406.04834v1","category":"cs.CL"}
{"created":"2024-06-07 10:56:51","title":"Linearization and Homogenization of nonlinear elasticity close to stress-free joints","abstract":"In this paper, we study a hyperelastic composite material with a periodic microstructure and a prestrain close to a stress-free joint. We consider two limits associated with linearization and homogenization. Unlike previous studies that focus on composites with a stress-free reference configuration, the minimizers of the elastic energy functional in the prestrained case are not explicitly known. Consequently, it is initially unclear at which deformation to perform the linearization. Our main result shows that both the consecutive and simultaneous limits converge to a single homogenized model of linearized elasticity. This model features a homogenized prestrain and provides first-order information about the minimizers of the original nonlinear model. We find that the homogenization of the material and the homogenization of the prestrain are generally coupled and cannot be considered separately. Additionally, we establish an asymptotic quadratic expansion of the homogenized stored energy function and present a detailed analysis of the effective model for laminate composite materials. A key analytical contribution of our paper is a new mixed-growth version of the geometric rigidity estimate for Jones domains. The proof of this result relies on the construction of an extension operator for Jones domains adapted to geometric rigidity.","sentences":["In this paper, we study a hyperelastic composite material with a periodic microstructure and a prestrain close to a stress-free joint.","We consider two limits associated with linearization and homogenization.","Unlike previous studies that focus on composites with a stress-free reference configuration, the minimizers of the elastic energy functional in the prestrained case are not explicitly known.","Consequently, it is initially unclear at which deformation to perform the linearization.","Our main result shows that both the consecutive and simultaneous limits converge to a single homogenized model of linearized elasticity.","This model features a homogenized prestrain and provides first-order information about the minimizers of the original nonlinear model.","We find that the homogenization of the material and the homogenization of the prestrain are generally coupled and cannot be considered separately.","Additionally, we establish an asymptotic quadratic expansion of the homogenized stored energy function and present a detailed analysis of the effective model for laminate composite materials.","A key analytical contribution of our paper is a new mixed-growth version of the geometric rigidity estimate for Jones domains.","The proof of this result relies on the construction of an extension operator for Jones domains adapted to geometric rigidity."],"url":"http://arxiv.org/abs/2406.04831v1","category":"math.AP"}
{"created":"2024-06-07 10:54:40","title":"EGOR: Efficient Generated Objects Replay for incremental object detection","abstract":"Incremental object detection aims to simultaneously maintain old-class accuracy and detect emerging new-class objects in incremental data. Most existing distillation-based methods underperform when unlabeled old-class objects are absent in the incremental dataset. While the absence can be mitigated by generating old-class samples, it also incurs high computational costs. In this paper, we argue that the extra computational cost stems from the inconsistency between the detector and the generative model, along with redundant generation. To overcome this problem, we propose Efficient Generated Object Replay (EGOR). Specifically, we generate old-class samples by inversing the original detectors, thus eliminating the necessity of training and storing additional generative models. We also propose augmented replay to reuse the objects in generated samples, thereby reducing the redundant generation. In addition, we propose high-response knowledge distillation focusing on the knowledge related to the old class, which transfers the knowledge in generated objects to the incremental detector. With the addition of the generated objects and losses, we observe a bias towards old classes in the detector. We balance the losses for old and new classes to alleviate the bias, thereby increasing the overall detection accuracy. Extensive experiments conducted on MS COCO 2017 demonstrate that our method can efficiently improve detection performance in the absence of old-class objects.","sentences":["Incremental object detection aims to simultaneously maintain old-class accuracy and detect emerging new-class objects in incremental data.","Most existing distillation-based methods underperform when unlabeled old-class objects are absent in the incremental dataset.","While the absence can be mitigated by generating old-class samples, it also incurs high computational costs.","In this paper, we argue that the extra computational cost stems from the inconsistency between the detector and the generative model, along with redundant generation.","To overcome this problem, we propose Efficient Generated Object Replay (EGOR).","Specifically, we generate old-class samples by inversing the original detectors, thus eliminating the necessity of training and storing additional generative models.","We also propose augmented replay to reuse the objects in generated samples, thereby reducing the redundant generation.","In addition, we propose high-response knowledge distillation focusing on the knowledge related to the old class, which transfers the knowledge in generated objects to the incremental detector.","With the addition of the generated objects and losses, we observe a bias towards old classes in the detector.","We balance the losses for old and new classes to alleviate the bias, thereby increasing the overall detection accuracy.","Extensive experiments conducted on MS COCO 2017 demonstrate that our method can efficiently improve detection performance in the absence of old-class objects."],"url":"http://arxiv.org/abs/2406.04829v1","category":"cs.CV"}
{"created":"2024-06-07 10:50:03","title":"Graph Mining under Data scarcity","abstract":"Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting.   Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.","sentences":["Multitude of deep learning models have been proposed for node classification in graphs.","However, they tend to perform poorly under labeled-data scarcity.","Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs).","Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance.","A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values.","We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting.   ","Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture.","We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks.","Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN."],"url":"http://arxiv.org/abs/2406.04825v1","category":"cs.LG"}
{"created":"2024-06-07 10:49:59","title":"FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch","abstract":"The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.","sentences":["The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations.","The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices.","This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings.","Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions.","We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks.","We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms."],"url":"http://arxiv.org/abs/2406.04824v1","category":"cs.LG"}
{"created":"2024-06-07 10:48:45","title":"BERTs are Generative In-Context Learners","abstract":"This paper explores the in-context learning capabilities of masked language models, challenging the common view that this ability does not 'emerge' in them. We present an embarrassingly simple inference technique that enables DeBERTa to operate as a generative model without any additional training. Our findings demonstrate that DeBERTa can match and even surpass GPT-3, its contemporary that famously introduced the paradigm of in-context learning. The comparative analysis reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. This suggests that there is great potential for a hybrid training approach that takes advantage of the strengths of both training objectives.","sentences":["This paper explores the in-context learning capabilities of masked language models, challenging the common view that this ability does not 'emerge' in them.","We present an embarrassingly simple inference technique that enables DeBERTa to operate as a generative model without any additional training.","Our findings demonstrate that DeBERTa can match and even surpass GPT-3, its contemporary that famously introduced the paradigm of in-context learning.","The comparative analysis reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks.","This suggests that there is great potential for a hybrid training approach that takes advantage of the strengths of both training objectives."],"url":"http://arxiv.org/abs/2406.04823v1","category":"cs.CL"}
{"created":"2024-06-07 10:47:33","title":"Deep Learning Powered Estimate of The Extrinsic Parameters on Unmanned Surface Vehicles","abstract":"Unmanned Surface Vehicles (USVs) are pivotal in marine exploration, but their sensors' accuracy is compromised by the dynamic marine environment. Traditional calibration methods fall short in these conditions. This paper introduces a deep learning architecture that predicts changes in the USV's dynamic metacenter and refines sensors' extrinsic parameters in real time using a Time-Sequence General Regression Neural Network (GRNN) with Euler angles as input. Simulation data from Unity3D ensures robust training and testing. Experimental results show that the Time-Sequence GRNN achieves the lowest mean squared error (MSE) loss, outperforming traditional neural networks. This method significantly enhances sensor calibration for USVs, promising improved data accuracy in challenging maritime conditions. Future work will refine the network and validate results with real-world data.","sentences":["Unmanned Surface Vehicles (USVs) are pivotal in marine exploration, but their sensors' accuracy is compromised by the dynamic marine environment.","Traditional calibration methods fall short in these conditions.","This paper introduces a deep learning architecture that predicts changes in the USV's dynamic metacenter and refines sensors' extrinsic parameters in real time using a Time-Sequence General Regression Neural Network (GRNN) with Euler angles as input.","Simulation data from Unity3D ensures robust training and testing.","Experimental results show that the Time-Sequence GRNN achieves the lowest mean squared error (MSE) loss, outperforming traditional neural networks.","This method significantly enhances sensor calibration for USVs, promising improved data accuracy in challenging maritime conditions.","Future work will refine the network and validate results with real-world data."],"url":"http://arxiv.org/abs/2406.04821v1","category":"cs.RO"}
{"created":"2024-06-07 10:41:24","title":"Navigating Efficiency in MobileViT through Gaussian Process on Global Architecture Factors","abstract":"Numerous techniques have been meticulously designed to achieve optimal architectures for convolutional neural networks (CNNs), yet a comparable focus on vision transformers (ViTs) has been somewhat lacking. Despite the remarkable success of ViTs in various vision tasks, their heavyweight nature presents challenges of computational costs. In this paper, we leverage the Gaussian process to systematically explore the nonlinear and uncertain relationship between performance and global architecture factors of MobileViT, such as resolution, width, and depth including the depth of in-verted residual blocks and the depth of ViT blocks, and joint factors including resolution-depth and resolution-width. We present design principles twisting magic 4D cube of the global architecture factors that minimize model sizes and computational costs with higher model accuracy. We introduce a formula for downsizing architectures by iteratively deriving smaller MobileViT V2, all while adhering to a specified constraint of multiply-accumulate operations (MACs). Experiment results show that our formula significantly outperforms CNNs and mobile ViTs across diversified datasets","sentences":["Numerous techniques have been meticulously designed to achieve optimal architectures for convolutional neural networks (CNNs), yet a comparable focus on vision transformers (ViTs) has been somewhat lacking.","Despite the remarkable success of ViTs in various vision tasks, their heavyweight nature presents challenges of computational costs.","In this paper, we leverage the Gaussian process to systematically explore the nonlinear and uncertain relationship between performance and global architecture factors of MobileViT, such as resolution, width, and depth including the depth of in-verted residual blocks and the depth of ViT blocks, and joint factors including resolution-depth and resolution-width.","We present design principles twisting magic 4D cube of the global architecture factors that minimize model sizes and computational costs with higher model accuracy.","We introduce a formula for downsizing architectures by iteratively deriving smaller MobileViT V2, all while adhering to a specified constraint of multiply-accumulate operations (MACs).","Experiment results show that our formula significantly outperforms CNNs and mobile ViTs across diversified datasets"],"url":"http://arxiv.org/abs/2406.04820v1","category":"cs.CV"}
{"created":"2024-06-07 10:35:29","title":"Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning","abstract":"Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviours). Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\\log$-$K$ curse. To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks. We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective. We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks. Additionally, the context encoder equipped with SaNCE demonstrates greater robustness to reductions in the number of available samples, thus possessing the potential to overcome the $\\log$-$K$ curse.","sentences":["Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across tasks with varying environmental features that require different optimal skills (i.e., different modes of behaviours).","Using context encoders based on contrastive learning to enhance the generalisability of Meta-RL agents is now widely studied but faces challenges such as the requirement for a large sample size, also referred to as the $\\log$-$K$ curse.","To improve RL generalisation to different tasks, we first introduce Skill-aware Mutual Information (SaMI), an optimisation objective that aids in distinguishing context embeddings according to skills, thereby equipping RL agents with the ability to identify and execute different skills across tasks.","We then propose Skill-aware Noise Contrastive Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective.","We provide a framework for equipping an RL agent with SaNCE in practice and conduct experimental validation on modified MuJoCo and Panda-gym benchmarks.","We empirically find that RL agents that learn by maximising SaMI achieve substantially improved zero-shot generalisation to unseen tasks.","Additionally, the context encoder equipped with SaNCE demonstrates greater robustness to reductions in the number of available samples, thus possessing the potential to overcome the $\\log$-$K$ curse."],"url":"http://arxiv.org/abs/2406.04815v1","category":"cs.LG"}
{"created":"2024-06-07 10:32:23","title":"Online Continual Learning of Video Diffusion Models From a Single Video Stream","abstract":"Diffusion models have shown exceptional capabilities in generating realistic videos. Yet, their training has been predominantly confined to offline environments where models can repeatedly train on i.i.d. data to convergence. This work explores the feasibility of training diffusion models from a semantically continuous video stream, where correlated video frames sequentially arrive one at a time. To investigate this, we introduce two novel continual video generative modeling benchmarks, Lifelong Bouncing Balls and Windows 95 Maze Screensaver, each containing over a million video frames generated from navigating stationary environments. Surprisingly, our experiments show that diffusion models can be effectively trained online using experience replay, achieving performance comparable to models trained with i.i.d. samples given the same number of gradient steps.","sentences":["Diffusion models have shown exceptional capabilities in generating realistic videos.","Yet, their training has been predominantly confined to offline environments where models can repeatedly train on i.i.d. data to convergence.","This work explores the feasibility of training diffusion models from a semantically continuous video stream, where correlated video frames sequentially arrive one at a time.","To investigate this, we introduce two novel continual video generative modeling benchmarks, Lifelong Bouncing Balls and Windows 95 Maze Screensaver, each containing over a million video frames generated from navigating stationary environments.","Surprisingly, our experiments show that diffusion models can be effectively trained online using experience replay, achieving performance comparable to models trained with i.i.d. samples given the same number of gradient steps."],"url":"http://arxiv.org/abs/2406.04814v1","category":"cs.CV"}
{"created":"2024-06-07 10:28:11","title":"Observational properties of Coherent Quantum Black Holes","abstract":"We consider null and time-like geodesics around a spherically symmetric, non-rotating Coherent Quantum Black Hole (CQBH). The classical limit of the geometry of CQBH departs from that of the Schwarzschild spacetime at short scales and depends on one parameter $R_s$ which can be interpreted as the physical radius of the 'quantum' core. We study circular orbits, photon rings, and lensing effects and compare them with the Schwarzschild metric. Using the relativistic ray-tracing code GYOTO, we produce a simulation of the shadow and show that thin accretion disks around a CQBH possess unique ring structures that distinguish them from other theoretical models.","sentences":["We consider null and time-like geodesics around a spherically symmetric, non-rotating Coherent Quantum Black Hole (CQBH).","The classical limit of the geometry of CQBH departs from that of the Schwarzschild spacetime at short scales and depends on one parameter $R_s$ which can be interpreted as the physical radius of the 'quantum' core.","We study circular orbits, photon rings, and lensing effects and compare them with the Schwarzschild metric.","Using the relativistic ray-tracing code GYOTO, we produce a simulation of the shadow and show that thin accretion disks around a CQBH possess unique ring structures that distinguish them from other theoretical models."],"url":"http://arxiv.org/abs/2406.04813v1","category":"gr-qc"}
{"created":"2024-06-07 10:27:07","title":"Generating Piano Practice Policy with a Gaussian Process","abstract":"A typical process of learning to play a piece on a piano consists of a progression through a series of practice units that focus on individual dimensions of the skill, the so-called practice modes. Practice modes in learning to play music comprise a particularly large set of possibilities, such as hand coordination, posture, articulation, ability to read a music score, correct timing or pitch, etc. Self-guided practice is known to be suboptimal, and a model that schedules optimal practice to maximize a learner's progress still does not exist. Because we each learn differently and there are many choices for possible piano practice tasks and methods, the set of practice modes should be dynamically adapted to the human learner, a process typically guided by a teacher. However, having a human teacher guide individual practice is not always feasible since it is time-consuming, expensive, and often unavailable. In this work, we present a modeling framework to guide the human learner through the learning process by choosing the practice modes generated by a policy model. To this end, we present a computational architecture building on a Gaussian process that incorporates 1) the learner state, 2) a policy that selects a suitable practice mode, 3) performance evaluation, and 4) expert knowledge. The proposed policy model is trained to approximate the expert-learner interaction during a practice session. In our future work, we will test different Bayesian optimization techniques, e.g., different acquisition functions, and evaluate their effect on the learning progress.","sentences":["A typical process of learning to play a piece on a piano consists of a progression through a series of practice units that focus on individual dimensions of the skill, the so-called practice modes.","Practice modes in learning to play music comprise a particularly large set of possibilities, such as hand coordination, posture, articulation, ability to read a music score, correct timing or pitch, etc. Self-guided practice is known to be suboptimal, and a model that schedules optimal practice to maximize a learner's progress still does not exist.","Because we each learn differently and there are many choices for possible piano practice tasks and methods, the set of practice modes should be dynamically adapted to the human learner, a process typically guided by a teacher.","However, having a human teacher guide individual practice is not always feasible since it is time-consuming, expensive, and often unavailable.","In this work, we present a modeling framework to guide the human learner through the learning process by choosing the practice modes generated by a policy model.","To this end, we present a computational architecture building on a Gaussian process that incorporates 1) the learner state, 2) a policy that selects a suitable practice mode, 3) performance evaluation, and 4) expert knowledge.","The proposed policy model is trained to approximate the expert-learner interaction during a practice session.","In our future work, we will test different Bayesian optimization techniques, e.g., different acquisition functions, and evaluate their effect on the learning progress."],"url":"http://arxiv.org/abs/2406.04812v1","category":"cs.LG"}
{"created":"2024-06-07 10:26:50","title":"Experimental Evaluation of All-Optical Up- and Down-Conversion of 3GPP 5G NR Signals using an Optomechanical Crystal Cavity Frequency Comb","abstract":"Optomechanical crystal cavities (OMCCs) allow the interaction between localized optical and mechanical modes through the radiation-pressure force. Driving such cavities with blue-detuned lasers relative to the optical resonance can induce a phonon lasing regime where the OMCC supports self-sustained mechanical oscillations. This dynamic state results in a narrow and stable microwave tone that modulates the laser at integer multiples of the mechanical resonance frequency, ultimately creating an optomechanical (OM) frequency comb suitable for microwave photonics applications. OMCCs enable compact, low-cost power-efficient all-photonic processing of multiple microwave signals, crucial for current 5G and future beyond-5G systems, whilst being compatible with silicon integrated photonic circuits. This work reports the experimental demonstration of all-optical multi-frequency up- and down-conversion of 3GPP 5G new-radio (NR) signals from the low- to mid- and extended-mid bands using the first and second harmonics of the frequency comb generated in a silicon OMCC. The OM comb generates up to 6 harmonics in the K-band, which is suitable for microwave photonic applications. The experimental demonstration also evaluates the impact of the phase-noise and the signal-to-noise ratio (SNR) in the frequency-converted 5G NR signals when the first and second OMCC harmonics are employed for frequency conversion.","sentences":["Optomechanical crystal cavities (OMCCs) allow the interaction between localized optical and mechanical modes through the radiation-pressure force.","Driving such cavities with blue-detuned lasers relative to the optical resonance can induce a phonon lasing regime where the OMCC supports self-sustained mechanical oscillations.","This dynamic state results in a narrow and stable microwave tone that modulates the laser at integer multiples of the mechanical resonance frequency, ultimately creating an optomechanical (OM) frequency comb suitable for microwave photonics applications.","OMCCs enable compact, low-cost power-efficient all-photonic processing of multiple microwave signals, crucial for current 5G and future beyond-5G systems, whilst being compatible with silicon integrated photonic circuits.","This work reports the experimental demonstration of all-optical multi-frequency up-","and down-conversion of 3GPP 5G new-radio (NR) signals from the low- to mid- and extended-mid bands using the first and second harmonics of the frequency comb generated in a silicon OMCC.","The OM comb generates up to 6 harmonics in the K-band, which is suitable for microwave photonic applications.","The experimental demonstration also evaluates the impact of the phase-noise and the signal-to-noise ratio (SNR) in the frequency-converted 5G NR signals when the first and second OMCC harmonics are employed for frequency conversion."],"url":"http://arxiv.org/abs/2406.04811v1","category":"physics.optics"}
{"created":"2024-06-07 10:23:25","title":"Fragile Model Watermarking: A Comprehensive Survey of Evolution, Characteristics, and Classification","abstract":"Model fragile watermarking, inspired by both the field of adversarial attacks on neural networks and traditional multimedia fragile watermarking, has gradually emerged as a potent tool for detecting tampering, and has witnessed rapid development in recent years. Unlike robust watermarks, which are widely used for identifying model copyrights, fragile watermarks for models are designed to identify whether models have been subjected to unexpected alterations such as backdoors, poisoning, compression, among others. These alterations can pose unknown risks to model users, such as misidentifying stop signs as speed limit signs in classic autonomous driving scenarios. This paper provides an overview of the relevant work in the field of model fragile watermarking since its inception, categorizing them and revealing the developmental trajectory of the field, thus offering a comprehensive survey for future endeavors in model fragile watermarking.","sentences":["Model fragile watermarking, inspired by both the field of adversarial attacks on neural networks and traditional multimedia fragile watermarking, has gradually emerged as a potent tool for detecting tampering, and has witnessed rapid development in recent years.","Unlike robust watermarks, which are widely used for identifying model copyrights, fragile watermarks for models are designed to identify whether models have been subjected to unexpected alterations such as backdoors, poisoning, compression, among others.","These alterations can pose unknown risks to model users, such as misidentifying stop signs as speed limit signs in classic autonomous driving scenarios.","This paper provides an overview of the relevant work in the field of model fragile watermarking since its inception, categorizing them and revealing the developmental trajectory of the field, thus offering a comprehensive survey for future endeavors in model fragile watermarking."],"url":"http://arxiv.org/abs/2406.04809v1","category":"cs.CR"}
{"created":"2024-06-07 10:23:03","title":"VERA: Generating Visual Explanations of Two-Dimensional Embeddings via Region Annotation","abstract":"Two-dimensional embeddings obtained from dimensionality reduction techniques, such as MDS, t-SNE, and UMAP, are widely used across various disciplines to visualize high-dimensional data. These visualizations provide a valuable tool for exploratory data analysis, allowing researchers to visually identify clusters, outliers, and other interesting patterns in the data. However, interpreting the resulting visualizations can be challenging, as it often requires additional manual inspection to understand the differences between data points in different regions of the embedding space. To address this issue, we propose Visual Explanations via Region Annotation (VERA), an automatic embedding-annotation approach that generates visual explanations for any two-dimensional embedding. VERA produces informative explanations that characterize distinct regions in the embedding space, allowing users to gain an overview of the embedding landscape at a glance. Unlike most existing approaches, which typically require some degree of manual user intervention, VERA produces static explanations, automatically identifying and selecting the most informative visual explanations to show to the user. We illustrate the usage of VERA on a real-world data set and validate the utility of our approach with a comparative user study. Our results demonstrate that the explanations generated by VERA are as useful as fully-fledged interactive tools on typical exploratory data analysis tasks but require significantly less time and effort from the user.","sentences":["Two-dimensional embeddings obtained from dimensionality reduction techniques, such as MDS, t-SNE, and UMAP, are widely used across various disciplines to visualize high-dimensional data.","These visualizations provide a valuable tool for exploratory data analysis, allowing researchers to visually identify clusters, outliers, and other interesting patterns in the data.","However, interpreting the resulting visualizations can be challenging, as it often requires additional manual inspection to understand the differences between data points in different regions of the embedding space.","To address this issue, we propose Visual Explanations via Region Annotation (VERA), an automatic embedding-annotation approach that generates visual explanations for any two-dimensional embedding.","VERA produces informative explanations that characterize distinct regions in the embedding space, allowing users to gain an overview of the embedding landscape at a glance.","Unlike most existing approaches, which typically require some degree of manual user intervention, VERA produces static explanations, automatically identifying and selecting the most informative visual explanations to show to the user.","We illustrate the usage of VERA on a real-world data set and validate the utility of our approach with a comparative user study.","Our results demonstrate that the explanations generated by VERA are as useful as fully-fledged interactive tools on typical exploratory data analysis tasks but require significantly less time and effort from the user."],"url":"http://arxiv.org/abs/2406.04808v1","category":"cs.LG"}
{"created":"2024-06-07 10:13:44","title":"TEDi Policy: Temporally Entangled Diffusion for Robotic Control","abstract":"Diffusion models have been shown to excel in robotic imitation learning by mastering the challenge of modeling complex distributions. However, sampling speed has traditionally not been a priority due to their popularity for image generation, limiting their application to dynamical tasks. While recent work has improved the sampling speed of diffusion-based robotic policies, they are restricted to techniques from the image generation domain. We adapt Temporally Entangled Diffusion (TEDi), a framework specific for trajectory generation, to speed up diffusion-based policies for imitation learning. We introduce TEDi Policy, with novel regimes for training and sampling, and show that it drastically improves the sampling speed while remaining performant when applied to state-of-the-art diffusion-based imitation learning policies.","sentences":["Diffusion models have been shown to excel in robotic imitation learning by mastering the challenge of modeling complex distributions.","However, sampling speed has traditionally not been a priority due to their popularity for image generation, limiting their application to dynamical tasks.","While recent work has improved the sampling speed of diffusion-based robotic policies, they are restricted to techniques from the image generation domain.","We adapt Temporally Entangled Diffusion (TEDi), a framework specific for trajectory generation, to speed up diffusion-based policies for imitation learning.","We introduce TEDi Policy, with novel regimes for training and sampling, and show that it drastically improves the sampling speed while remaining performant when applied to state-of-the-art diffusion-based imitation learning policies."],"url":"http://arxiv.org/abs/2406.04806v1","category":"cs.RO"}
{"created":"2024-06-07 10:10:11","title":"Embracing Nonlinearity and Geometry: A dimensional analysis guided design of shock absorbing materials","abstract":"Protective applications require energy-absorbing materials that are soft and compressible enough to absorb kinetic energy from impacts, yet stiff enough to bear crushing loads. Achieving this balance requires careful consideration of both mechanical properties and geometric design. Conventional shock-absorbing pads are made of very thick foams that exhibit a plateau of constant stress in their stress-strain response. Contrary to this belief, we report that foams with a nonlinear stress-strain response can be useful to achieve simultaneously thin and lightweight protective pads. We introduce a new framework for the thickness or volume-constrained design of compact and lightweight protective foams while ensuring the desired structural integrity and mechanical performance. Our streamlined dimensional analysis approach provides geometric constraints on the dimensionless thickness and cross-sectional area of a protective foam with a given stress-strain response to limit the acceleration and compressive strain within desired critical limits. We also identify optimal mechanical properties that will result in the most compact and lightest protective foam layer for absorbing a given kinetic energy of impact. Guided by this design framework, we achieve optimal protective properties in hierarchically architected vertically aligned carbon nanotube (VACNT) foams, enabling next generation protective applications in extreme environments.","sentences":["Protective applications require energy-absorbing materials that are soft and compressible enough to absorb kinetic energy from impacts, yet stiff enough to bear crushing loads.","Achieving this balance requires careful consideration of both mechanical properties and geometric design.","Conventional shock-absorbing pads are made of very thick foams that exhibit a plateau of constant stress in their stress-strain response.","Contrary to this belief, we report that foams with a nonlinear stress-strain response can be useful to achieve simultaneously thin and lightweight protective pads.","We introduce a new framework for the thickness or volume-constrained design of compact and lightweight protective foams while ensuring the desired structural integrity and mechanical performance.","Our streamlined dimensional analysis approach provides geometric constraints on the dimensionless thickness and cross-sectional area of a protective foam with a given stress-strain response to limit the acceleration and compressive strain within desired critical limits.","We also identify optimal mechanical properties that will result in the most compact and lightest protective foam layer for absorbing a given kinetic energy of impact.","Guided by this design framework, we achieve optimal protective properties in hierarchically architected vertically aligned carbon nanotube (VACNT) foams, enabling next generation protective applications in extreme environments."],"url":"http://arxiv.org/abs/2406.04803v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 10:06:13","title":"Predictive Dynamic Fusion","abstract":"Multimodal fusion is crucial in joint decision-making systems for rendering holistic judgments. Since multimodal data changes in open environments, dynamic fusion has emerged and achieved remarkable progress in numerous applications. However, most existing dynamic multimodal fusion methods lack theoretical guarantees and easily fall into suboptimal problems, yielding unreliability and instability. To address this issue, we propose a Predictive Dynamic Fusion (PDF) framework for multimodal learning. We proceed to reveal the multimodal fusion from a generalization perspective and theoretically derive the predictable Collaborative Belief (Co-Belief) with Mono- and Holo-Confidence, which provably reduces the upper bound of generalization error. Accordingly, we further propose a relative calibration strategy to calibrate the predicted Co-Belief for potential uncertainty. Extensive experiments on multiple benchmarks confirm our superiority. Our code is available at https://github.com/Yinan-Xia/PDF.","sentences":["Multimodal fusion is crucial in joint decision-making systems for rendering holistic judgments.","Since multimodal data changes in open environments, dynamic fusion has emerged and achieved remarkable progress in numerous applications.","However, most existing dynamic multimodal fusion methods lack theoretical guarantees and easily fall into suboptimal problems, yielding unreliability and instability.","To address this issue, we propose a Predictive Dynamic Fusion (PDF) framework for multimodal learning.","We proceed to reveal the multimodal fusion from a generalization perspective and theoretically derive the predictable Collaborative Belief (Co-Belief) with Mono- and Holo-Confidence, which provably reduces the upper bound of generalization error.","Accordingly, we further propose a relative calibration strategy to calibrate the predicted Co-Belief for potential uncertainty.","Extensive experiments on multiple benchmarks confirm our superiority.","Our code is available at https://github.com/Yinan-Xia/PDF."],"url":"http://arxiv.org/abs/2406.04802v1","category":"cs.CV"}
{"created":"2024-06-07 10:04:39","title":"Zero, Finite, and Infinite Belief History of Theory of Mind Reasoning in Large Language Models","abstract":"Large Language Models (LLMs) have recently shown a promise and emergence of Theory of Mind (ToM) ability and even outperform humans in certain ToM tasks. To evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we propose a novel concept, taxonomy, and framework, the ToM reasoning with Zero, Finite, and Infinite Belief History and develop a multi-round text-based game, called $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six LLMs with this game and found their performance on Zero Belief History is consistently better than on Finite Belief History. In addition, we have found two of the models with small parameter sizes outperform all the evaluated models with large parameter sizes. We expect this work to pave the way for future ToM benchmark development and also for the promotion and development of more complex AI agents or systems which are required to be equipped with more complex ToM reasoning ability.","sentences":["Large Language Models (LLMs) have recently shown a promise and emergence of Theory of Mind (ToM) ability and even outperform humans in certain ToM tasks.","To evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we propose a novel concept, taxonomy, and framework, the ToM reasoning with Zero, Finite, and Infinite Belief History and develop a multi-round text-based game, called $\\textit{Pick the Right Stuff}$, as a benchmark.","We have evaluated six LLMs with this game and found their performance on Zero Belief History is consistently better than on Finite Belief History.","In addition, we have found two of the models with small parameter sizes outperform all the evaluated models with large parameter sizes.","We expect this work to pave the way for future ToM benchmark development and also for the promotion and development of more complex AI agents or systems which are required to be equipped with more complex ToM reasoning ability."],"url":"http://arxiv.org/abs/2406.04800v1","category":"cs.AI"}
{"created":"2024-06-07 09:40:09","title":"Learning-Augmented Priority Queues","abstract":"Priority queues are one of the most fundamental and widely used data structures in computer science. Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element. In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance. We examine three prediction models spanning different use cases, and show how the predictions can be leveraged to enhance the performance of priority queue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible applications.","sentences":["Priority queues are one of the most fundamental and widely used data structures in computer science.","Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element.","In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance.","We examine three prediction models spanning different use cases, and show how the predictions can be leveraged to enhance the performance of priority queue operations.","Moreover, we demonstrate the optimality of our solution and discuss some possible applications."],"url":"http://arxiv.org/abs/2406.04793v1","category":"cs.DS"}
{"created":"2024-06-07 09:39:46","title":"Star Formation by Supernova Implosion","abstract":"Recent hydrodynamical simulations of the late stages of supernova remnant (SNR) evolution have revealed that as they merge with the ambient medium, SNRs implode, leading to the formation of dense clouds in their center. While being highly chemically enriched by their host SNR, these clouds appear to have similar properties as giant molecular clouds, which are believed to be the main site of star formation. Here, we develop a simple model, in order to estimate the efficiency of the star formation that might be triggered by the implosion of SNRs. We separately consider two cases, cyclic star formation, maintained by the episodic driving of feedback from new generations of stars; and a single burst of star formation, triggered by a single explosion. We find that in the cyclic case, star formation is inefficient, with implosion-triggered star-formation contributing a few percent of the observed star-formation efficiency per free-fall timescale. In the single-burst case, higher star-formation efficiencies can be obtained. However, while the implosion-triggered process might not contribute much to the overall star-formation, due to the high chemical enrichment of the birth clouds, it can explain the formation of a significant fraction of metal-rich stars.","sentences":["Recent hydrodynamical simulations of the late stages of supernova remnant (SNR) evolution have revealed that as they merge with the ambient medium, SNRs implode, leading to the formation of dense clouds in their center.","While being highly chemically enriched by their host SNR, these clouds appear to have similar properties as giant molecular clouds, which are believed to be the main site of star formation.","Here, we develop a simple model, in order to estimate the efficiency of the star formation that might be triggered by the implosion of SNRs.","We separately consider two cases, cyclic star formation, maintained by the episodic driving of feedback from new generations of stars; and a single burst of star formation, triggered by a single explosion.","We find that in the cyclic case, star formation is inefficient, with implosion-triggered star-formation contributing a few percent of the observed star-formation efficiency per free-fall timescale.","In the single-burst case, higher star-formation efficiencies can be obtained.","However, while the implosion-triggered process might not contribute much to the overall star-formation, due to the high chemical enrichment of the birth clouds, it can explain the formation of a significant fraction of metal-rich stars."],"url":"http://arxiv.org/abs/2406.04792v1","category":"astro-ph.GA"}
{"created":"2024-06-07 09:37:50","title":"Particle production and hadronization temperature of the massive Schwinger model","abstract":"We study the pair production, string breaking, and hadronization of a receding electron-positron pair using the bosonized version of the massive Schwinger model in quantum electrodynamics in 1+1 space-time dimensions. Specifically, we study the dynamics of the electric field in Bjorken coordinates by splitting it into a coherent field and its Gaussian fluctuations. We find that the electric field shows damped oscillations, reflecting pair production. Interestingly, the computation of the asymptotic total particle density per rapidity interval for large masses can be fitted using a Boltzmann factor, where the temperature can be related to the hadronization temperature in QCD. Lastly, we discuss the possibility of an analog quantum simulation of the massive Schwinger model using ultracold atoms, explicitly matching the potential of the Schwinger model to the effective potential for the relative phase of two linearly coupled Bose-Einstein condensates.","sentences":["We study the pair production, string breaking, and hadronization of a receding electron-positron pair using the bosonized version of the massive Schwinger model in quantum electrodynamics in 1+1 space-time dimensions.","Specifically, we study the dynamics of the electric field in Bjorken coordinates by splitting it into a coherent field and its Gaussian fluctuations.","We find that the electric field shows damped oscillations, reflecting pair production.","Interestingly, the computation of the asymptotic total particle density per rapidity interval for large masses can be fitted using a Boltzmann factor, where the temperature can be related to the hadronization temperature in QCD.","Lastly, we discuss the possibility of an analog quantum simulation of the massive Schwinger model using ultracold atoms, explicitly matching the potential of the Schwinger model to the effective potential for the relative phase of two linearly coupled Bose-Einstein condensates."],"url":"http://arxiv.org/abs/2406.04789v1","category":"hep-th"}
{"created":"2024-06-07 09:37:03","title":"Quasinormal modes and ringdown waveform of the Frolov black hole","abstract":"In this paper, we investigate the scalar perturbation over the Frolov black hole (BH), which is a regular BH induced by the quantum gravity effect. The quasinormal frequencies (QNFs) of scalar field always consistently reside in the lower half-plane, and its time-domain evolution demonstrates a decaying behavior, with the late-time tail exhibiting a power-law pattern. These observations collectively suggest the stability of the Frolov BH against scalar perturbation. Additionally, our study reveals that quantum gravity effects lead to slower decay modes. For the case of the angular quantum number $l=0$, the oscillation exhibits non-monotonic behavior with the quantum gravity parameter $\\alpha_0$. However, once $l\\geq 1$, the angular quantum number surpasses the influence of the quantum gravity effect.","sentences":["In this paper, we investigate the scalar perturbation over the Frolov black hole (BH), which is a regular BH induced by the quantum gravity effect.","The quasinormal frequencies (QNFs) of scalar field always consistently reside in the lower half-plane, and its time-domain evolution demonstrates a decaying behavior, with the late-time tail exhibiting a power-law pattern.","These observations collectively suggest the stability of the Frolov BH against scalar perturbation.","Additionally, our study reveals that quantum gravity effects lead to slower decay modes.","For the case of the angular quantum number $l=0$, the oscillation exhibits non-monotonic behavior with the quantum gravity parameter $\\alpha_0$. However, once $l\\geq 1$, the angular quantum number surpasses the influence of the quantum gravity effect."],"url":"http://arxiv.org/abs/2406.04787v1","category":"gr-qc"}
{"created":"2024-06-07 09:33:01","title":"Enabling Efficient Batch Serving for LMaaS via Generation Length Prediction","abstract":"Nowadays, large language models (LLMs) are published as a service and can be accessed by various applications via APIs, also known as language-model-as-a-service (LMaaS). Without knowing the generation length of requests, existing serving systems serve requests in a first-come, first-served (FCFS) manner with a fixed batch size, which leads to two problems that affect batch serving efficiency. First, the generation lengths of requests in a batch vary, and requests with short generation lengths must wait for requests with long generation lengths to finish during the batch serving procedure. Second, requests with longer generation lengths consume more memory during serving. Without knowing the generation lengths of batched requests, the batch size is always set small to avoid the out-of-memory (OOM) error, thus preventing the GPU from being fully utilized. In this paper, we find that a significant number of popular applications in the LMaaS scenario have a positive correlation between the generation length and the length of raw user input. Based on this observation, we propose Magnus, which can accurately predict the request generation length with the user input length, application-level, and user-level semantic features. Accordingly, Magnus can achieve high request throughput by batching requests of similar generation lengths together with adaptive batch sizes. Besides, Magnus can also schedule batches with the highest response ratio next (HRRN) policy to reduce request response time. Experiments conducted on our testbed show that Magnus improves request throughput by up to 234\\% and reduces response time by up to 89.7\\% compared to baselines.","sentences":["Nowadays, large language models (LLMs) are published as a service and can be accessed by various applications via APIs, also known as language-model-as-a-service (LMaaS).","Without knowing the generation length of requests, existing serving systems serve requests in a first-come, first-served (FCFS) manner with a fixed batch size, which leads to two problems that affect batch serving efficiency.","First, the generation lengths of requests in a batch vary, and requests with short generation lengths must wait for requests with long generation lengths to finish during the batch serving procedure.","Second, requests with longer generation lengths consume more memory during serving.","Without knowing the generation lengths of batched requests, the batch size is always set small to avoid the out-of-memory (OOM) error, thus preventing the GPU from being fully utilized.","In this paper, we find that a significant number of popular applications in the LMaaS scenario have a positive correlation between the generation length and the length of raw user input.","Based on this observation, we propose Magnus, which can accurately predict the request generation length with the user input length, application-level, and user-level semantic features.","Accordingly, Magnus can achieve high request throughput by batching requests of similar generation lengths together with adaptive batch sizes.","Besides, Magnus can also schedule batches with the highest response ratio next (HRRN) policy to reduce request response time.","Experiments conducted on our testbed show that Magnus improves request throughput by up to 234\\% and reduces response time by up to 89.7\\% compared to baselines."],"url":"http://arxiv.org/abs/2406.04785v1","category":"cs.DC"}
{"created":"2024-06-07 09:32:03","title":"SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals","abstract":"Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents' capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. Project page: https://selfgoal-agent.github.io.","sentences":["Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming.","However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed.","In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents' capabilities to achieve high-level goals with limited human prior and environmental feedback.","The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure.","Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments.","Project page: https://selfgoal-agent.github.io."],"url":"http://arxiv.org/abs/2406.04784v1","category":"cs.CL"}
{"created":"2024-06-07 09:28:22","title":"Software Engineering for Collective Cyber-Physical Ecosystems","abstract":"Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people. While most research focusses on treating these systems as \"composites\" (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as \"collectives\" (i.e., uniform, collaborative, and self-organising groups of entities). This article explores the motivations, state of the art, and implications of this \"collective computing paradigm\" in software engineering, discusses its peculiar challenges, and outlines a path for future research, touching on aspects such as macroprogramming, collective intelligence, self-adaptive middleware, learning, synthesis, and experimentation of collective behaviour.","sentences":["Today's distributed and pervasive computing addresses large-scale cyber-physical ecosystems, characterised by dense and large networks of devices capable of computation, communication and interaction with the environment and people.","While most research focusses on treating these systems as \"composites\" (i.e., heterogeneous functional complexes), recent developments in fields such as self-organising systems and swarm robotics have opened up a complementary perspective: treating systems as \"collectives\" (i.e., uniform, collaborative, and self-organising groups of entities).","This article explores the motivations, state of the art, and implications of this \"collective computing paradigm\" in software engineering, discusses its peculiar challenges, and outlines a path for future research, touching on aspects such as macroprogramming, collective intelligence, self-adaptive middleware, learning, synthesis, and experimentation of collective behaviour."],"url":"http://arxiv.org/abs/2406.04780v1","category":"cs.SE"}
{"created":"2024-06-07 09:28:18","title":"Mobile Network Configuration Recommendation using Deep Generative Graph Neural Network","abstract":"There are vast number of configurable parameters in a Radio Access Telecom Network. A significant amount of these parameters is configured by Radio Node or cell based on their deployment setting. Traditional methods rely on domain knowledge for individual parameter configuration, often leading to sub-optimal results. To improve this, a framework using a Deep Generative Graph Neural Network (GNN) is proposed. It encodes the network into a graph, extracts subgraphs for each RAN node, and employs a Siamese GNN (S-GNN) to learn embeddings. The framework recommends configuration parameters for a multitude of parameters and detects misconfigurations, handling both network expansion and existing cell reconfiguration. Tested on real-world data, the model surpasses baselines, demonstrating accuracy, generalizability, and robustness against concept drift.","sentences":["There are vast number of configurable parameters in a Radio Access Telecom Network.","A significant amount of these parameters is configured by Radio Node or cell based on their deployment setting.","Traditional methods rely on domain knowledge for individual parameter configuration, often leading to sub-optimal results.","To improve this, a framework using a Deep Generative Graph Neural Network (GNN) is proposed.","It encodes the network into a graph, extracts subgraphs for each RAN node, and employs a Siamese GNN (S-GNN) to learn embeddings.","The framework recommends configuration parameters for a multitude of parameters and detects misconfigurations, handling both network expansion and existing cell reconfiguration.","Tested on real-world data, the model surpasses baselines, demonstrating accuracy, generalizability, and robustness against concept drift."],"url":"http://arxiv.org/abs/2406.04779v1","category":"cs.LG"}
{"created":"2024-06-07 09:20:30","title":"OFDM-Standard Compatible SC-NOFS Waveforms for Low-Latency and Jitter-Tolerance Industrial IoT Communications","abstract":"Traditional communications focus on regular and orthogonal signal waveforms for simplified signal processing and improved spectral efficiency. In contrast, the next-generation communications would aim for irregular and non-orthogonal signal waveforms to introduce new capabilities. This work proposes a spectrally efficient irregular Sinc (irSinc) shaping technique, revisiting the traditional Sinc back to 1924, with the aim of enhancing performance in industrial Internet of things (IIoT). In time-critical IIoT applications, low-latency and time-jitter tolerance are two critical factors that significantly impact the performance and reliability. Recognizing the inevitability of latency and jitter in practice, this work aims to propose a waveform technique to mitigate these effects via reducing latency and enhancing the system robustness under time jitter effects. The utilization of irSinc yields a signal with increased spectral efficiency without sacrificing error performance. Integrating the irSinc in a two-stage framework, a single-carrier non-orthogonal frequency shaping (SC-NOFS) waveform is developed, showcasing perfect compatibility with 5G standards, enabling the direct integration of irSinc in existing industrial IoT setups. Through 5G standard signal configuration, our signal achieves faster data transmission within the same spectral bandwidth. Hardware experiments validate an 18% saving in timing resources, leading to either reduced latency or enhanced jitter tolerance.","sentences":["Traditional communications focus on regular and orthogonal signal waveforms for simplified signal processing and improved spectral efficiency.","In contrast, the next-generation communications would aim for irregular and non-orthogonal signal waveforms to introduce new capabilities.","This work proposes a spectrally efficient irregular Sinc (irSinc) shaping technique, revisiting the traditional Sinc back to 1924, with the aim of enhancing performance in industrial Internet of things (IIoT).","In time-critical IIoT applications, low-latency and time-jitter tolerance are two critical factors that significantly impact the performance and reliability.","Recognizing the inevitability of latency and jitter in practice, this work aims to propose a waveform technique to mitigate these effects via reducing latency and enhancing the system robustness under time jitter effects.","The utilization of irSinc yields a signal with increased spectral efficiency without sacrificing error performance.","Integrating the irSinc in a two-stage framework, a single-carrier non-orthogonal frequency shaping (SC-NOFS) waveform is developed, showcasing perfect compatibility with 5G standards, enabling the direct integration of irSinc in existing industrial IoT setups.","Through 5G standard signal configuration, our signal achieves faster data transmission within the same spectral bandwidth.","Hardware experiments validate an 18% saving in timing resources, leading to either reduced latency or enhanced jitter tolerance."],"url":"http://arxiv.org/abs/2406.04776v1","category":"eess.SP"}
{"created":"2024-06-07 09:19:40","title":"Defects induce phase transition from dynamic to static rippling in graphene","abstract":"Many of graphene's remarkable properties are intrinsically linked to its inherent ripples. Defects, whether naturally present or artificially introduced, are known to have a strong impact on the rippling of graphene. However, how defects alter ripple dynamics in two-dimensional (2D) materials in general, and graphene in particular, remains largely unexplored. Here, using machine learning-driven molecular dynamics simulations, we reveal a fundamental connection between defect concentration and ripple dynamics in freestanding graphene sheets. Specifically, we find that at a critical concentration of approximately 0.1%, dynamic rippling undergoes a transition from freely propagating to static ripples. This is in quantitative alignment with the experimentally observed turning point in the non-monotonic scaling of the Young's modulus and emphasises the critical interplay between defects and material dynamics. Our work not only unveils the significant impact of defects on rippling dynamics in graphene but also paves the way to design two-dimensional devices with tailored properties.","sentences":["Many of graphene's remarkable properties are intrinsically linked to its inherent ripples.","Defects, whether naturally present or artificially introduced, are known to have a strong impact on the rippling of graphene.","However, how defects alter ripple dynamics in two-dimensional (2D) materials in general, and graphene in particular, remains largely unexplored.","Here, using machine learning-driven molecular dynamics simulations, we reveal a fundamental connection between defect concentration and ripple dynamics in freestanding graphene sheets.","Specifically, we find that at a critical concentration of approximately 0.1%, dynamic rippling undergoes a transition from freely propagating to static ripples.","This is in quantitative alignment with the experimentally observed turning point in the non-monotonic scaling of the Young's modulus and emphasises the critical interplay between defects and material dynamics.","Our work not only unveils the significant impact of defects on rippling dynamics in graphene but also paves the way to design two-dimensional devices with tailored properties."],"url":"http://arxiv.org/abs/2406.04775v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 09:19:23","title":"Solving the Zeh problem about the density operator with higher-order statistics","abstract":"Since a 1932 work from von Neumann, it is generally considered that if two statistical mixtures are represented by the same density operator \\r{ho}, they should in fact be considered as the same mixture. In a 1970 paper, Zeh, considering this result to be a consequence of what he called the measurement axiom, introduced a thought experiment with neutron spins and showed that in that experiment the density operator could not tell the whole story. Since then, no consensus has emerged yet, and controversies on the subject still presently develop. In this paper, stimulated by our previous works in the field of Quantum Information Processing, we show that the two mixtures imagined by Zeh, with the same \\r{ho}, should however be distinguished. We show that this result suppresses a restriction unduly installed on statistical mixtures, but does not affect the general use of \\r{ho}, e.g. in quantum statistical mechanics, and the von Neumann entropy keeps its own interest and even helps clarifying this confusing consequence of the measurement axiom. In order to avoid any ambiguity, the identification of the introduction of this postulate, which von Neumann rather suggested to be a general property, is given in an appendix where it is shown that Zeh was right when he spoke of a measurement axiom and identified his problem. The use and content of a density operator is also discussed in another physical case which we are led to call the Landau-Feynman situation, and which implies the concept of entanglement rather than the one of mixed states.","sentences":["Since a 1932 work from von Neumann, it is generally considered that if two statistical mixtures are represented by the same density operator \\r{ho}, they should in fact be considered as the same mixture.","In a 1970 paper, Zeh, considering this result to be a consequence of what he called the measurement axiom, introduced a thought experiment with neutron spins and showed that in that experiment the density operator could not tell the whole story.","Since then, no consensus has emerged yet, and controversies on the subject still presently develop.","In this paper, stimulated by our previous works in the field of Quantum Information Processing, we show that the two mixtures imagined by Zeh, with the same \\r{ho}, should however be distinguished.","We show that this result suppresses a restriction unduly installed on statistical mixtures, but does not affect the general use of \\r{ho}, e.g. in quantum statistical mechanics, and the von Neumann entropy keeps its own interest and even helps clarifying this confusing consequence of the measurement axiom.","In order to avoid any ambiguity, the identification of the introduction of this postulate, which von Neumann rather suggested to be a general property, is given in an appendix where it is shown that Zeh was right when he spoke of a measurement axiom and identified his problem.","The use and content of a density operator is also discussed in another physical case which we are led to call the Landau-Feynman situation, and which implies the concept of entanglement rather than the one of mixed states."],"url":"http://arxiv.org/abs/2406.04774v1","category":"quant-ph"}
{"created":"2024-06-07 09:17:33","title":"REP: Resource-Efficient Prompting for On-device Continual Learning","abstract":"On-device continual learning (CL) requires the co-optimization of model accuracy and resource efficiency to be practical. This is extremely challenging because it must preserve accuracy while learning new tasks with continuously drifting data and maintain both high energy and memory efficiency to be deployable on real-world devices. Typically, a CL method leverages one of two types of backbone networks: CNN or ViT. It is commonly believed that CNN-based CL excels in resource efficiency, whereas ViT-based CL is superior in model performance, making each option attractive only for a single aspect. In this paper, we revisit this comparison while embracing powerful pre-trained ViT models of various sizes, including ViT-Ti (5.8M parameters). Our detailed analysis reveals that many practical options exist today for making ViT-based methods more suitable for on-device CL, even when accuracy, energy, and memory are all considered. To further expand this impact, we introduce REP, which improves resource efficiency specifically targeting prompt-based rehearsal-free methods. Our key focus is on avoiding catastrophic trade-offs with accuracy while trimming computational and memory costs throughout the training process. We achieve this by exploiting swift prompt selection that enhances input data using a carefully provisioned model, and by developing two novel algorithms-adaptive token merging (AToM) and adaptive layer dropping (ALD)-that optimize the prompt updating stage. In particular, AToM and ALD perform selective skipping across the data and model-layer dimensions without compromising task-specific features in vision transformer models. Extensive experiments on three image classification datasets validate REP's superior resource efficiency over current state-of-the-art methods.","sentences":["On-device continual learning (CL) requires the co-optimization of model accuracy and resource efficiency to be practical.","This is extremely challenging because it must preserve accuracy while learning new tasks with continuously drifting data and maintain both high energy and memory efficiency to be deployable on real-world devices.","Typically, a CL method leverages one of two types of backbone networks: CNN or ViT.","It is commonly believed that CNN-based CL excels in resource efficiency, whereas ViT-based CL is superior in model performance, making each option attractive only for a single aspect.","In this paper, we revisit this comparison while embracing powerful pre-trained ViT models of various sizes, including ViT-Ti (5.8M parameters).","Our detailed analysis reveals that many practical options exist today for making ViT-based methods more suitable for on-device CL, even when accuracy, energy, and memory are all considered.","To further expand this impact, we introduce REP, which improves resource efficiency specifically targeting prompt-based rehearsal-free methods.","Our key focus is on avoiding catastrophic trade-offs with accuracy while trimming computational and memory costs throughout the training process.","We achieve this by exploiting swift prompt selection that enhances input data using a carefully provisioned model, and by developing two novel algorithms-adaptive token merging (AToM) and adaptive layer dropping (ALD)-that optimize the prompt updating stage.","In particular, AToM and ALD perform selective skipping across the data and model-layer dimensions without compromising task-specific features in vision transformer models.","Extensive experiments on three image classification datasets validate REP's superior resource efficiency over current state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.04772v1","category":"cs.LG"}
{"created":"2024-06-07 09:15:44","title":"WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild","abstract":"We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.","sentences":["We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries.","WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs.","For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo.","WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments.","WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie.","Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation.","Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse'' to ``tie'' if the winner response exceeds the loser one by more than $K$ characters.","WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric.","WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks.","Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models.","Additionally, WB-Score reaches 0.95, surpassing both ArenaHard's 0.91 and AlpacaEval2.0's 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates."],"url":"http://arxiv.org/abs/2406.04770v1","category":"cs.CL"}
{"created":"2024-06-07 09:15:29","title":"Diffusion-based Generative Image Outpainting for Recovery of FOV-Truncated CT Images","abstract":"Field-of-view (FOV) recovery of truncated chest CT scans is crucial for accurate body composition analysis, which involves quantifying skeletal muscle and subcutaneous adipose tissue (SAT) on CT slices. This, in turn, enables disease prognostication. Here, we present a method for recovering truncated CT slices using generative image outpainting. We train a diffusion model and apply it to truncated CT slices generated by simulating a small FOV. Our model reliably recovers the truncated anatomy and outperforms the previous state-of-the-art despite being trained on 87% less data.","sentences":["Field-of-view (FOV) recovery of truncated chest CT scans is crucial for accurate body composition analysis, which involves quantifying skeletal muscle and subcutaneous adipose tissue (SAT) on CT slices.","This, in turn, enables disease prognostication.","Here, we present a method for recovering truncated CT slices using generative image outpainting.","We train a diffusion model and apply it to truncated CT slices generated by simulating a small FOV.","Our model reliably recovers the truncated anatomy and outperforms the previous state-of-the-art despite being trained on 87% less data."],"url":"http://arxiv.org/abs/2406.04769v1","category":"eess.IV"}
{"created":"2024-06-07 09:09:14","title":"Reinforcement Learning and Regret Bounds for Admission Control","abstract":"The expected regret of any reinforcement learning algorithm is lower bounded by $\\Omega\\left(\\sqrt{DXAT}\\right)$ for undiscounted returns, where $D$ is the diameter of the Markov decision process, $X$ the size of the state space, $A$ the size of the action space and $T$ the number of time steps. However, this lower bound is general. A smaller regret can be obtained by taking into account some specific knowledge of the problem structure. In this article, we consider an admission control problem to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding costs. Queuing systems often have a diameter that is exponential in the buffer size $S$, making the previous lower bound prohibitive for any practical use. We propose an algorithm inspired by UCRL2, and use the structure of the problem to upper bound the expected total regret by $O(S\\log T + \\sqrt{mT \\log T})$ in the finite server case. In the infinite server case, we prove that the dependence of the regret on $S$ disappears.","sentences":["The expected regret of any reinforcement learning algorithm is lower bounded by $\\Omega\\left(\\sqrt{DXAT}\\right)$ for undiscounted returns, where $D$ is the diameter of the Markov decision process, $X$ the size of the state space, $A$ the size of the action space and $T$ the number of time steps.","However, this lower bound is general.","A smaller regret can be obtained by taking into account some specific knowledge of the problem structure.","In this article, we consider an admission control problem to an $M/M/c/S$ queue with $m$ job classes and class-dependent rewards and holding costs.","Queuing systems often have a diameter that is exponential in the buffer size $S$, making the previous lower bound prohibitive for any practical use.","We propose an algorithm inspired by UCRL2, and use the structure of the problem to upper bound the expected total regret by $O(S\\log T + \\sqrt{mT \\log T})$ in the finite server case.","In the infinite server case, we prove that the dependence of the regret on $S$ disappears."],"url":"http://arxiv.org/abs/2406.04766v1","category":"cs.LG"}
{"created":"2024-06-07 09:04:16","title":"Accelerated protons produced by magnetic Penrose process in Sgr A*","abstract":"Typical mechanisms to extract energies from a rotating black hole are the Blandford-Znajek process and the Penrose process. The Penrose process requires a special condition that is difficult to occur in common astrophysical situations. However, the magnetic Penrose process (MPP) does not require such a special condition, and can produce ultra-high energy cosmic rays. When neutrons decay near a rotating black hole, the MPP efficiency of the produced proton is maximized. The supermassive black hole in Sagittarius A* (Sgr A*) is likely to have a radiatively inefficient accretion flow that is hot enough to produce neutrons by nuclear reactions, which can be subsequently accelerated to high-energy by the MPP. We calculate the production rate of accelerated protons from the Sgr A* to estimated the gamma-ray flux at Earth produced by these accelerated protons and the flux of the accelerated protons themselves transported from Sgr A* to Earth. We find that these very high-energy gamma rays ($E_{\\gamma}\\gtrsim10\\,\\mathrm{TeV}$) amount to a significant fraction of the flux of the gamma-ray from the HESS J1745-290 and the central molecular zone around $100\\,\\mathrm{TeV}$. The accelerated proton flux, when the dimensionless spin parameter $a_{*}=0.5$ and the magnetic field strength in the vicinity of the black hole $B_{0}=100\\,\\mathrm{G}$, is about $1.6-4.1\\%$ of the cosmic ray proton flux from KASCADE experiment at about $1\\,\\mathrm{PeV}$. Due to the finite decay time of neutrons which need to be transported from the accretion flow to the acceleration zone, our acceleration model can operate only around black holes with mass not much greater than $\\sim10^8\\,M_\\odot$.","sentences":["Typical mechanisms to extract energies from a rotating black hole are the Blandford-Znajek process and the Penrose process.","The Penrose process requires a special condition that is difficult to occur in common astrophysical situations.","However, the magnetic Penrose process (MPP) does not require such a special condition, and can produce ultra-high energy cosmic rays.","When neutrons decay near a rotating black hole, the MPP efficiency of the produced proton is maximized.","The supermassive black hole in Sagittarius A* (Sgr A*) is likely to have a radiatively inefficient accretion flow that is hot enough to produce neutrons by nuclear reactions, which can be subsequently accelerated to high-energy by the MPP.","We calculate the production rate of accelerated protons from the Sgr A* to estimated the gamma-ray flux at Earth produced by these accelerated protons and the flux of the accelerated protons themselves transported from Sgr A* to Earth.","We find that these very high-energy gamma rays ($E_{\\gamma}\\gtrsim10\\,\\mathrm{TeV}$) amount to a significant fraction of the flux of the gamma-ray from the HESS J1745-290 and the central molecular zone around $100\\,\\mathrm{TeV}$.","The accelerated proton flux, when the dimensionless spin parameter $a_{*}=0.5$ and the magnetic field strength in the vicinity of the black hole $B_{0}=100\\,\\mathrm{G}$, is about $1.6-4.1\\%$ of the cosmic ray proton flux from KASCADE experiment at about $1\\,\\mathrm{PeV}$. Due to the finite decay time of neutrons which need to be transported from the accretion flow to the acceleration zone, our acceleration model can operate only around black holes with mass not much greater than $\\sim10^8\\,M_\\odot$."],"url":"http://arxiv.org/abs/2406.04764v1","category":"astro-ph.HE"}
{"created":"2024-06-07 09:03:00","title":"Holographic Intelligence Surface Assisted Integrated Sensing and Communication","abstract":"Traditional discrete-array-based systems fail to exploit interactions between closely spaced antennas, resulting in inadequate utilization of the aperture resource. In this paper, we propose a holographic intelligence surface (HIS) assisted integrated sensing and communication (HISAC) system, wherein both the transmitter and receiver are fabricated using a continuous-aperture array. A continuous-discrete transformation of the HIS pattern based on the Fourier transform is proposed, converting the continuous pattern design into a discrete beamforming design. We formulate a joint transmit-receive beamforming optimization problem for the HISAC system, aiming to balance the performance of multi-target sensing while fulfilling the performance requirement of multi-user communication. To solve the non-convex problem with coupled variables, an alternating optimization-based algorithm is proposed to optimize the HISAC transmit-receive beamforming in an alternate manner. Specifically, the transmit beamforming design is solved by decoupling into a series of feasibility-checking sub-problems while the receive beamforming is determined by the Rayleigh quotient-based method. Simulation results demonstrate the superiority of the proposed HISAC system over traditional discrete-array-based ISAC systems, achieving significantly higher sensing performance while guaranteeing predetermined communication performance.","sentences":["Traditional discrete-array-based systems fail to exploit interactions between closely spaced antennas, resulting in inadequate utilization of the aperture resource.","In this paper, we propose a holographic intelligence surface (HIS) assisted integrated sensing and communication (HISAC) system, wherein both the transmitter and receiver are fabricated using a continuous-aperture array.","A continuous-discrete transformation of the HIS pattern based on the Fourier transform is proposed, converting the continuous pattern design into a discrete beamforming design.","We formulate a joint transmit-receive beamforming optimization problem for the HISAC system, aiming to balance the performance of multi-target sensing while fulfilling the performance requirement of multi-user communication.","To solve the non-convex problem with coupled variables, an alternating optimization-based algorithm is proposed to optimize the HISAC transmit-receive beamforming in an alternate manner.","Specifically, the transmit beamforming design is solved by decoupling into a series of feasibility-checking sub-problems while the receive beamforming is determined by the Rayleigh quotient-based method.","Simulation results demonstrate the superiority of the proposed HISAC system over traditional discrete-array-based ISAC systems, achieving significantly higher sensing performance while guaranteeing predetermined communication performance."],"url":"http://arxiv.org/abs/2406.04762v1","category":"eess.SP"}
{"created":"2024-06-07 09:01:26","title":"$L^2$-decomposition of the second fundamental form of a hypersurface in the study of the general relativistic vacuum constraint equations","abstract":"In present article, we consider a $L^2$-orthogonal decomposition of the second fundamental form of a closed spacelike hypersurface in a Lorentzian spacetime and its applications to the study of some algebraic-differential properties of the general relativistic vacuum constraint equations. For the study we will use the well-known Ahlfors Laplacian.","sentences":["In present article, we consider a $L^2$-orthogonal decomposition of the second fundamental form of a closed spacelike hypersurface in a Lorentzian spacetime and its applications to the study of some algebraic-differential properties of the general relativistic vacuum constraint equations.","For the study we will use the well-known Ahlfors Laplacian."],"url":"http://arxiv.org/abs/2406.04760v1","category":"math.DG"}
{"created":"2024-06-07 09:01:25","title":"Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks","abstract":"In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.","sentences":["In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting.","While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling.","We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework.","The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts.","Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles.","We experiment with the model on both global and limited area forecasting.","Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty."],"url":"http://arxiv.org/abs/2406.04759v1","category":"cs.LG"}
{"created":"2024-06-07 08:58:29","title":"Think out Loud: Emotion Deducing Explanation in Dialogues","abstract":"Humans convey emotions through daily dialogues, making emotion understanding a crucial step of affective intelligence. To understand emotions in dialogues, machines are asked to recognize the emotion for an utterance (Emotion Recognition in Dialogues, ERD); based on the emotion, then find causal utterances for the emotion (Emotion Cause Extraction in Dialogues, ECED). The setting of the two tasks requires first ERD and then ECED, ignoring the mutual complement between emotion and cause. To fix this, some new tasks are proposed to extract them simultaneously. Although the current research on these tasks has excellent achievements, simply identifying emotion-related factors by classification modeling lacks realizing the specific thinking process of causes stimulating the emotion in an explainable way. This thinking process especially reflected in the reasoning ability of Large Language Models (LLMs) is under-explored. To this end, we propose a new task \"Emotion Deducing Explanation in Dialogues\" (EDEN). EDEN recognizes emotion and causes in an explicitly thinking way. That is, models need to generate an explanation text, which first summarizes the causes; analyzes the inner activities of the speakers triggered by the causes using common sense; then guesses the emotion accordingly. To support the study of EDEN, based on the existing resources in ECED, we construct two EDEN datasets by human effort. We further evaluate different models on EDEN and find that LLMs are more competent than conventional PLMs. Besides, EDEN can help LLMs achieve better recognition of emotions and causes, which explores a new research direction of explainable emotion understanding in dialogues.","sentences":["Humans convey emotions through daily dialogues, making emotion understanding a crucial step of affective intelligence.","To understand emotions in dialogues, machines are asked to recognize the emotion for an utterance (Emotion Recognition in Dialogues, ERD); based on the emotion, then find causal utterances for the emotion (Emotion Cause Extraction in Dialogues, ECED).","The setting of the two tasks requires first ERD and then ECED, ignoring the mutual complement between emotion and cause.","To fix this, some new tasks are proposed to extract them simultaneously.","Although the current research on these tasks has excellent achievements, simply identifying emotion-related factors by classification modeling lacks realizing the specific thinking process of causes stimulating the emotion in an explainable way.","This thinking process especially reflected in the reasoning ability of Large Language Models (LLMs) is under-explored.","To this end, we propose a new task \"Emotion Deducing Explanation in Dialogues\" (EDEN).","EDEN recognizes emotion and causes in an explicitly thinking way.","That is, models need to generate an explanation text, which first summarizes the causes; analyzes the inner activities of the speakers triggered by the causes using common sense; then guesses the emotion accordingly.","To support the study of EDEN, based on the existing resources in ECED, we construct two EDEN datasets by human effort.","We further evaluate different models on EDEN and find that LLMs are more competent than conventional PLMs.","Besides, EDEN can help LLMs achieve better recognition of emotions and causes, which explores a new research direction of explainable emotion understanding in dialogues."],"url":"http://arxiv.org/abs/2406.04758v1","category":"cs.CL"}
{"created":"2024-06-07 08:54:55","title":"Sales Whisperer: A Human-Inconspicuous Attack on LLM Brand Recommendations","abstract":"Large language model (LLM) users might rely on others (e.g., prompting services), to write prompts. However, the risks of trusting prompts written by others remain unstudied. In this paper, we assess the risk of using such prompts on brand recommendation tasks when shopping. First, we found that paraphrasing prompts can result in LLMs mentioning given brands with drastically different probabilities, including a pair of prompts where the probability changes by 100%. Next, we developed an approach that can be used to perturb an original base prompt to increase the likelihood that an LLM mentions a given brand. We designed a human-inconspicuous algorithm that perturbs prompts, which empirically forces LLMs to mention strings related to a brand more often, by absolute improvements up to 78.3%. Our results suggest that our perturbed prompts, 1) are inconspicuous to humans, 2) force LLMs to recommend a target brand more often, and 3) increase the perceived chances of picking targeted brands.","sentences":["Large language model (LLM) users might rely on others (e.g., prompting services), to write prompts.","However, the risks of trusting prompts written by others remain unstudied.","In this paper, we assess the risk of using such prompts on brand recommendation tasks when shopping.","First, we found that paraphrasing prompts can result in LLMs mentioning given brands with drastically different probabilities, including a pair of prompts where the probability changes by 100%.","Next, we developed an approach that can be used to perturb an original base prompt to increase the likelihood that an LLM mentions a given brand.","We designed a human-inconspicuous algorithm that perturbs prompts, which empirically forces LLMs to mention strings related to a brand more often, by absolute improvements up to 78.3%.","Our results suggest that our perturbed prompts, 1) are inconspicuous to humans, 2) force LLMs to recommend a target brand more often, and 3) increase the perceived chances of picking targeted brands."],"url":"http://arxiv.org/abs/2406.04755v1","category":"cs.CR"}
{"created":"2024-06-07 08:53:04","title":"Differential equations satisfied by generating functions of 5-, 6-, and 7-regular labelled graphs: a reduction-based approach","abstract":"By a classic result of Gessel, the exponential generating functions for $k$-regular graphs are D-finite. Using Gr\\\"obner bases in Weyl algebras, we compute the linear differential equations satisfied by the generating function for 5-, 6-, and 7- regular graphs. The method is sufficiently robust to consider variants such as graphs with multiple edges, loops, and graphs whose degrees are limited to fixed sets of values.","sentences":["By a classic result of Gessel, the exponential generating functions for $k$-regular graphs are D-finite.","Using Gr\\\"obner bases in Weyl algebras, we compute the linear differential equations satisfied by the generating function for 5-, 6-, and 7- regular graphs.","The method is sufficiently robust to consider variants such as graphs with multiple edges, loops, and graphs whose degrees are limited to fixed sets of values."],"url":"http://arxiv.org/abs/2406.04753v1","category":"math.CO"}
{"created":"2024-06-07 08:52:12","title":"Asymptotically Optimal Policies for Weakly Coupled Markov Decision Processes","abstract":"We consider the problem of maximizing the expected average reward obtained over an infinite time horizon by $n$ weakly coupled Markov decision processes. Our setup is a substantial generalization of the multi-armed restless bandit problem that allows for multiple actions and constraints. We establish a connection with a deterministic and continuous-variable control problem where the objective is to maximize the average reward derived from an occupancy measure that represents the empirical distribution of the processes when $n \\to \\infty$. We show that a solution of this fluid problem can be used to construct policies for the weakly coupled processes that achieve the maximum expected average reward as $n \\to \\infty$, and we give sufficient conditions for the existence of solutions. Under certain assumptions on the constraints, we prove that these conditions are automatically satisfied if the unconstrained single-process problem admits a suitable unichain and aperiodic policy. In particular, the assumptions include multi-armed restless bandits and a broad class of problems with multiple actions and inequality constraints. Also, the policies can be constructed in an explicit way in these cases. Our theoretical results are complemented by several concrete examples and numerical experiments, which include multichain setups that are covered by the theoretical results.","sentences":["We consider the problem of maximizing the expected average reward obtained over an infinite time horizon by $n$ weakly coupled Markov decision processes.","Our setup is a substantial generalization of the multi-armed restless bandit problem that allows for multiple actions and constraints.","We establish a connection with a deterministic and continuous-variable control problem where the objective is to maximize the average reward derived from an occupancy measure that represents the empirical distribution of the processes when $n \\to \\infty$. We show that a solution of this fluid problem can be used to construct policies for the weakly coupled processes that achieve the maximum expected average reward as $n \\to \\infty$, and we give sufficient conditions for the existence of solutions.","Under certain assumptions on the constraints, we prove that these conditions are automatically satisfied if the unconstrained single-process problem admits a suitable unichain and aperiodic policy.","In particular, the assumptions include multi-armed restless bandits and a broad class of problems with multiple actions and inequality constraints.","Also, the policies can be constructed in an explicit way in these cases.","Our theoretical results are complemented by several concrete examples and numerical experiments, which include multichain setups that are covered by the theoretical results."],"url":"http://arxiv.org/abs/2406.04751v1","category":"math.OC"}
{"created":"2024-06-07 08:47:34","title":"Enhanced preprocessed multi-step splitting iterations for computing PageRank","abstract":"In recent years, the PageRank algorithm has garnered significant attention due to its crucial role in search engine technologies and its applications across various scientific fields. It is well-known that the power method is a classical method for computing PageRank. However, there is a pressing demand for alternative approaches that can address its limitations and enhance its efficiency. Specifically, the power method converges very slowly when the damping factor is close to 1. To address this challenge, this paper introduces a new multi-step splitting iteration approach for accelerating PageRank computations. Furthermore, we present two new approaches for computating PageRank, which are modifications of the new multi-step splitting iteration approach, specifically utilizing the thick restarted Arnoldi and generalized Arnoldi methods. We provide detailed discussions on the construction and theoretical convergence results of these two approaches. Extensive experiments using large test matrices demonstrate the significant performance improvements achieved by our proposed algorithms.","sentences":["In recent years, the PageRank algorithm has garnered significant attention due to its crucial role in search engine technologies and its applications across various scientific fields.","It is well-known that the power method is a classical method for computing PageRank.","However, there is a pressing demand for alternative approaches that can address its limitations and enhance its efficiency.","Specifically, the power method converges very slowly when the damping factor is close to 1.","To address this challenge, this paper introduces a new multi-step splitting iteration approach for accelerating PageRank computations.","Furthermore, we present two new approaches for computating PageRank, which are modifications of the new multi-step splitting iteration approach, specifically utilizing the thick restarted Arnoldi and generalized Arnoldi methods.","We provide detailed discussions on the construction and theoretical convergence results of these two approaches.","Extensive experiments using large test matrices demonstrate the significant performance improvements achieved by our proposed algorithms."],"url":"http://arxiv.org/abs/2406.04749v1","category":"math.NA"}
{"created":"2024-06-07 08:46:19","title":"PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance Prediction","abstract":"Text-to-image generation has recently emerged as a viable alternative to text-to-image retrieval, due to the visually impressive results of generative diffusion models. Although query performance prediction is an active research topic in information retrieval, to the best of our knowledge, there is no prior study that analyzes the difficulty of queries (prompts) in text-to-image generation, based on human judgments. To this end, we introduce the first dataset of prompts which are manually annotated in terms of image generation performance. In order to determine the difficulty of the same prompts in image retrieval, we also collect manual annotations that represent retrieval performance. We thus propose the first benchmark for joint text-to-image prompt and query performance prediction, comprising 10K queries. Our benchmark enables: (i) the comparative assessment of the difficulty of prompts/queries in image generation and image retrieval, and (ii) the evaluation of prompt/query performance predictors addressing both generation and retrieval. We present results with several pre-generation/retrieval and post-generation/retrieval performance predictors, thus providing competitive baselines for future research. Our benchmark and code is publicly available under the CC BY 4.0 license at https://github.com/Eduard6421/PQPP.","sentences":["Text-to-image generation has recently emerged as a viable alternative to text-to-image retrieval, due to the visually impressive results of generative diffusion models.","Although query performance prediction is an active research topic in information retrieval, to the best of our knowledge, there is no prior study that analyzes the difficulty of queries (prompts) in text-to-image generation, based on human judgments.","To this end, we introduce the first dataset of prompts which are manually annotated in terms of image generation performance.","In order to determine the difficulty of the same prompts in image retrieval, we also collect manual annotations that represent retrieval performance.","We thus propose the first benchmark for joint text-to-image prompt and query performance prediction, comprising 10K queries.","Our benchmark enables: (i) the comparative assessment of the difficulty of prompts/queries in image generation and image retrieval, and (ii) the evaluation of prompt/query performance predictors addressing both generation and retrieval.","We present results with several pre-generation/retrieval and post-generation/retrieval performance predictors, thus providing competitive baselines for future research.","Our benchmark and code is publicly available under the CC BY 4.0 license at https://github.com/Eduard6421/PQPP."],"url":"http://arxiv.org/abs/2406.04746v1","category":"cs.CV"}
{"created":"2024-06-07 08:43:53","title":"Confidence-aware Contrastive Learning for Selective Classification","abstract":"Selective classification enables models to make predictions only when they are sufficiently confident, aiming to enhance safety and reliability, which is important in high-stakes scenarios. Previous methods mainly use deep neural networks and focus on modifying the architecture of classification layers to enable the model to estimate the confidence of its prediction. This work provides a generalization bound for selective classification, disclosing that optimizing feature layers helps improve the performance of selective classification. Inspired by this theory, we propose to explicitly improve the selective classification model at the feature level for the first time, leading to a novel Confidence-aware Contrastive Learning method for Selective Classification, CCL-SC, which similarizes the features of homogeneous instances and differentiates the features of heterogeneous instances, with the strength controlled by the model's confidence. The experimental results on typical datasets, i.e., CIFAR-10, CIFAR-100, CelebA, and ImageNet, show that CCL-SC achieves significantly lower selective risk than state-of-the-art methods, across almost all coverage degrees. Moreover, it can be combined with existing methods to bring further improvement.","sentences":["Selective classification enables models to make predictions only when they are sufficiently confident, aiming to enhance safety and reliability, which is important in high-stakes scenarios.","Previous methods mainly use deep neural networks and focus on modifying the architecture of classification layers to enable the model to estimate the confidence of its prediction.","This work provides a generalization bound for selective classification, disclosing that optimizing feature layers helps improve the performance of selective classification.","Inspired by this theory, we propose to explicitly improve the selective classification model at the feature level for the first time, leading to a novel Confidence-aware Contrastive Learning method for Selective Classification, CCL-SC, which similarizes the features of homogeneous instances and differentiates the features of heterogeneous instances, with the strength controlled by the model's confidence.","The experimental results on typical datasets, i.e., CIFAR-10, CIFAR-100, CelebA, and ImageNet, show that CCL-SC achieves significantly lower selective risk than state-of-the-art methods, across almost all coverage degrees.","Moreover, it can be combined with existing methods to bring further improvement."],"url":"http://arxiv.org/abs/2406.04745v1","category":"cs.LG"}
{"created":"2024-06-07 08:43:07","title":"CRAG -- Comprehensive RAG Benchmark","abstract":"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.","sentences":["Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge.","Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks.","To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search.","CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds.","Our evaluation on this benchmark highlights the gap to fully trustworthy QA.","Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%.","State-of-the-art industry RAG solutions only answer 63% questions without any hallucination.","CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions.","The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition.","We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions."],"url":"http://arxiv.org/abs/2406.04744v1","category":"cs.CL"}
{"created":"2024-06-07 08:40:53","title":"Activation Map-based Vector Quantization for 360-degree Image Semantic Communication","abstract":"In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.","sentences":["In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE).","However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth.","To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission.","The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features.","Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation.","To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated.","Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols."],"url":"http://arxiv.org/abs/2406.04740v1","category":"eess.IV"}
{"created":"2024-06-07 08:36:13","title":"On the capability of high redshift kSZ measurement with galaxy surveys","abstract":"The kSZ effect has been detected at z<1 using various techniques and data sets. The ongoing and upcoming spectroscopic galaxy surveys such as DESI and PFS will push the detection beyond z = 1, and therefore map the baryon distribution at high redshifts. Such detection can be achieved by both the kSZ stacking and tomography methods. While the two methods are theoretically equivalent, they differ significantly in the probed physics and scales, and required data sets. Taking the combination of PFS and ACT as an example, we build mocks of kSZ and galaxies, quantify the kSZ detection S/N, and compare between the two methods. We segment the PFS galaxies into three redshift bins: 0.6 < z < 1.0, 1.0 < z < 1.6, and 1.6 < z < 2.4. For tomography method, our analysis reveals that the two higher redshift bins exhibit higher S/N, with values of 32 and 28, respectively, compared to the first redshift bin (S/N = 8). This is attributed to not only the increasing of electron density with redshifts, but also the larger survey volume and the reduced non-linearity, facilitating velocity reconstruction at higher redshifts. Therefore, the capability of the PFS survey to measure high redshift kSZ effect stands as a substantial advantage over other spectroscopic surveys at lower redshift. The S/N of kSZ stacking largely depends on the number of clusters/groups available from another photometric survey. But in general, its S/N is lower than that of kSZ tomography. Incorporating next-generation CMB surveys like CMB-S4, characterized by significantly reduced instrument noise and improved angular resolution, is expected to enhance tomographic detection by a factor of ten and stacking detection by five. This future high S/N detection holds the promise of not only providing precise constraints on the overall baryon abundance but also initiating a new insight into baryon distribution.","sentences":["The kSZ effect has been detected at z<1 using various techniques and data sets.","The ongoing and upcoming spectroscopic galaxy surveys such as DESI and PFS will push the detection beyond z = 1, and therefore map the baryon distribution at high redshifts.","Such detection can be achieved by both the kSZ stacking and tomography methods.","While the two methods are theoretically equivalent, they differ significantly in the probed physics and scales, and required data sets.","Taking the combination of PFS and ACT as an example, we build mocks of kSZ and galaxies, quantify the kSZ detection S/N, and compare between the two methods.","We segment the PFS galaxies into three redshift bins: 0.6 < z < 1.0, 1.0 < z < 1.6, and 1.6 < z < 2.4.","For tomography method, our analysis reveals that the two higher redshift bins exhibit higher S/N, with values of 32 and 28, respectively, compared to the first redshift bin (S/N = 8).","This is attributed to not only the increasing of electron density with redshifts, but also the larger survey volume and the reduced non-linearity, facilitating velocity reconstruction at higher redshifts.","Therefore, the capability of the PFS survey to measure high redshift kSZ effect stands as a substantial advantage over other spectroscopic surveys at lower redshift.","The S/N of kSZ stacking largely depends on the number of clusters/groups available from another photometric survey.","But in general, its S/N is lower than that of kSZ tomography.","Incorporating next-generation CMB surveys like CMB-S4, characterized by significantly reduced instrument noise and improved angular resolution, is expected to enhance tomographic detection by a factor of ten and stacking detection by five.","This future high S/N detection holds the promise of not only providing precise constraints on the overall baryon abundance but also initiating a new insight into baryon distribution."],"url":"http://arxiv.org/abs/2406.04735v1","category":"astro-ph.CO"}
{"created":"2024-06-07 08:34:30","title":"Generative AI Models: Opportunities and Risks for Industry and Authorities","abstract":"Generative AI models are capable of performing a wide range of tasks that traditionally require creativity and human understanding. They learn patterns from existing data during training and can subsequently generate new content such as texts, images, and music that follow these patterns. Due to their versatility and generally high-quality results, they, on the one hand, represent an opportunity for digitalization. On the other hand, the use of generative AI models introduces novel IT security risks that need to be considered for a comprehensive analysis of the threat landscape in relation to IT security. In response to this risk potential, companies or authorities using them should conduct an individual risk analysis before integrating generative AI into their workflows. The same applies to developers and operators, as many risks in the context of generative AI have to be taken into account at the time of development or can only be influenced by the operating company. Based on this, existing security measures can be adjusted, and additional measures can be taken.","sentences":["Generative AI models are capable of performing a wide range of tasks that traditionally require creativity and human understanding.","They learn patterns from existing data during training and can subsequently generate new content such as texts, images, and music that follow these patterns.","Due to their versatility and generally high-quality results, they, on the one hand, represent an opportunity for digitalization.","On the other hand, the use of generative AI models introduces novel IT security risks that need to be considered for a comprehensive analysis of the threat landscape in relation to IT security.","In response to this risk potential, companies or authorities using them should conduct an individual risk analysis before integrating generative AI into their workflows.","The same applies to developers and operators, as many risks in the context of generative AI have to be taken into account at the time of development or can only be influenced by the operating company.","Based on this, existing security measures can be adjusted, and additional measures can be taken."],"url":"http://arxiv.org/abs/2406.04734v1","category":"cs.AI"}
{"created":"2024-06-07 08:32:30","title":"Unsupervised representation learning with Hebbian synaptic and structural plasticity in brain-like feedforward neural networks","abstract":"Neural networks that can capture key principles underlying brain computation offer exciting new opportunities for developing artificial intelligence and brain-like computing algorithms. Such networks remain biologically plausible while leveraging localized forms of synaptic learning rules and modular network architecture found in the neocortex. Compared to backprop-driven deep learning approches, they provide more suitable models for deploying on neuromorphic hardware and have greater potential for scalability on large-scale computing clusters. The development of such brain-like neural networks depends on having a learning procedure that can build effective internal representations from data. In this work, we introduce and evaluate a brain-like neural network model capable of unsupervised representation learning. It builds on the Bayesian Confidence Propagation Neural Network (BCPNN), which has earlier been implemented as abstract as well as biophyscially detailed recurrent attractor neural networks explaining various cortical associative memory phenomena. Here we developed a feedforward BCPNN model to perform representation learning by incorporating a range of brain-like attributes derived from neocortical circuits such as cortical columns, divisive normalization, Hebbian synaptic plasticity, structural plasticity, sparse activity, and sparse patchy connectivity. The model was tested on a diverse set of popular machine learning benchmarks: grayscale images (MNIST, Fashion-MNIST), RGB natural images (SVHN, CIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER). The performance of the model when using a linear classifier to predict the class labels fared competitively with conventional multi-layer perceptrons and other state-of-the-art brain-like neural networks.","sentences":["Neural networks that can capture key principles underlying brain computation offer exciting new opportunities for developing artificial intelligence and brain-like computing algorithms.","Such networks remain biologically plausible while leveraging localized forms of synaptic learning rules and modular network architecture found in the neocortex.","Compared to backprop-driven deep learning approches, they provide more suitable models for deploying on neuromorphic hardware and have greater potential for scalability on large-scale computing clusters.","The development of such brain-like neural networks depends on having a learning procedure that can build effective internal representations from data.","In this work, we introduce and evaluate a brain-like neural network model capable of unsupervised representation learning.","It builds on the Bayesian Confidence Propagation Neural Network (BCPNN), which has earlier been implemented as abstract as well as biophyscially detailed recurrent attractor neural networks explaining various cortical associative memory phenomena.","Here we developed a feedforward BCPNN model to perform representation learning by incorporating a range of brain-like attributes derived from neocortical circuits such as cortical columns, divisive normalization, Hebbian synaptic plasticity, structural plasticity, sparse activity, and sparse patchy connectivity.","The model was tested on a diverse set of popular machine learning benchmarks: grayscale images (MNIST, Fashion-MNIST), RGB natural images (SVHN, CIFAR-10), QSAR (MUV, HIV), and malware detection (EMBER).","The performance of the model when using a linear classifier to predict the class labels fared competitively with conventional multi-layer perceptrons and other state-of-the-art brain-like neural networks."],"url":"http://arxiv.org/abs/2406.04733v1","category":"cs.NE"}
{"created":"2024-06-07 08:22:15","title":"Monotonic Decompositions of Submodular Set Functions","abstract":"Submodular set functions are undoubtedly among the most important building blocks of combinatorial optimization. Somewhat surprisingly, continuous counterparts of such functions have also appeared in an analytic line of research where they found applications in the theory of finitely additive measures, nonlinear integrals, and electric capacities. Recently, a number of connections between these two branches have been established, and the aim of this paper is to generalize further results on submodular set functions on finite sets to the analytic setting.   We first extend the notion of duality of matroids to submodular set functions, and characterize the uniquely determined decomposition of a submodular set function into the sum of a nonnegaive charge and an increasing submodular set function in which the charge is maximal. Then, we describe basic properties of infinite-alternating set functions, a subclass of submodular set functions that serves as an analytic counterpart of coverage functions. By relaxing the monotonicity assumption in the definition, we introduce a new class of submodular functions with distinguished structural properties that includes, among others, weighted cut functions of graphs. We prove that, unlike general submodular set functions over an infinite domain, any infinite-alternating set function can be written as the sum of an increasing and a decreasing submodular function or as the difference of two increasing submodular functions, thus giving extension of results on monotonic decompositions in the finite case. Finally, motivated by its connections to graph parameters such as the maximum size of a cut and the maximum size of a fractional triangle packing, we study the structure of such decompositions for weighted cut functions of undirected graphs.","sentences":["Submodular set functions are undoubtedly among the most important building blocks of combinatorial optimization.","Somewhat surprisingly, continuous counterparts of such functions have also appeared in an analytic line of research where they found applications in the theory of finitely additive measures, nonlinear integrals, and electric capacities.","Recently, a number of connections between these two branches have been established, and the aim of this paper is to generalize further results on submodular set functions on finite sets to the analytic setting.   ","We first extend the notion of duality of matroids to submodular set functions, and characterize the uniquely determined decomposition of a submodular set function into the sum of a nonnegaive charge and an increasing submodular set function in which the charge is maximal.","Then, we describe basic properties of infinite-alternating set functions, a subclass of submodular set functions that serves as an analytic counterpart of coverage functions.","By relaxing the monotonicity assumption in the definition, we introduce a new class of submodular functions with distinguished structural properties that includes, among others, weighted cut functions of graphs.","We prove that, unlike general submodular set functions over an infinite domain, any infinite-alternating set function can be written as the sum of an increasing and a decreasing submodular function or as the difference of two increasing submodular functions, thus giving extension of results on monotonic decompositions in the finite case.","Finally, motivated by its connections to graph parameters such as the maximum size of a cut and the maximum size of a fractional triangle packing, we study the structure of such decompositions for weighted cut functions of undirected graphs."],"url":"http://arxiv.org/abs/2406.04728v1","category":"math.CO"}
{"created":"2024-06-07 17:54:17","title":"Optimizing Exit Queues for Proof-of-Stake Blockchains: A Mechanism Design Approach","abstract":"Byzantine fault-tolerant consensus protocols have provable safety and liveness properties for static validator sets. In practice, however, the validator set changes over time, potentially eroding the protocol's security guarantees. For example, systems with accountable safety may lose some of that accountability over time as adversarial validators exit. As a result, protocols must rate limit entry and exit so that the set changes slowly enough to ensure security. Here, the system designer faces a fundamental trade-off. Slower exits increase friction, making it less attractive to stake in the first place. Faster exits provide more utility to stakers but weaken the protocol's security.   This paper provides the first systematic study of exit queues for Proof-of-Stake blockchains. Given a collection of validator-set consistency constraints imposed by the protocol, the social planner's goal is to provide a constrained-optimal mechanism that minimizes disutility for the participants. We introduce the MINSLACK mechanism, a dynamic capacity first-come-first-served queue in which the amount of stake that can exit in a period depends on the number of previous exits and the consistency constraints. We show that MINSLACK is optimal when stakers equally value the processing of their withdrawal. When stakers values are heterogeneous, the optimal mechanism resembles a priority queue with dynamic capacity. However, this mechanism must reserve exit capacity for the future in case a staker with a much higher need for liquidity arrives. We conclude with a survey of known consistency constraints and highlight the diversity of existing exit mechanisms.","sentences":["Byzantine fault-tolerant consensus protocols have provable safety and liveness properties for static validator sets.","In practice, however, the validator set changes over time, potentially eroding the protocol's security guarantees.","For example, systems with accountable safety may lose some of that accountability over time as adversarial validators exit.","As a result, protocols must rate limit entry and exit so that the set changes slowly enough to ensure security.","Here, the system designer faces a fundamental trade-off.","Slower exits increase friction, making it less attractive to stake in the first place.","Faster exits provide more utility to stakers but weaken the protocol's security.   ","This paper provides the first systematic study of exit queues for Proof-of-Stake blockchains.","Given a collection of validator-set consistency constraints imposed by the protocol, the social planner's goal is to provide a constrained-optimal mechanism that minimizes disutility for the participants.","We introduce the MINSLACK mechanism, a dynamic capacity first-come-first-served queue in which the amount of stake that can exit in a period depends on the number of previous exits and the consistency constraints.","We show that MINSLACK is optimal when stakers equally value the processing of their withdrawal.","When stakers values are heterogeneous, the optimal mechanism resembles a priority queue with dynamic capacity.","However, this mechanism must reserve exit capacity for the future in case a staker with a much higher need for liquidity arrives.","We conclude with a survey of known consistency constraints and highlight the diversity of existing exit mechanisms."],"url":"http://arxiv.org/abs/2406.05124v1","category":"econ.TH"}
{"created":"2024-06-07 17:31:09","title":"Measurement of inclusive and differential cross sections for W$^+$W$^-$ production in proton-proton collisions at $\\sqrt{s}$ = 13.6 TeV","abstract":"Measurements at $\\sqrt{s}$ = 13.6 TeV of the opposite-sign W boson pair production cross section in proton-proton collisions are presented. The data used in this study were collected with the CMS detector at the CERN LHC in 2022, and correspond to an integrated luminosity of 34.8 fb$^{-1}$. Events are selected by requiring one electron and one muon of opposite charge. A maximum likelihood fit is performed on signal- and background-enriched data categories defined by the flavour and charge of the leptons, the number of jets, and number of jets originating from b quarks. An inclusive W$^+$W$^-$ production cross section of 125.7 $\\pm$ 5.6 pb is measured, in agreement with standard model predictions. Cross sections are also reported in a fiducial region close to that of the detector acceptance, both inclusively and differentially, as a function of the jet multiplicity in the event. For first time in proton-proton collisions, WW events with at least two reconstructed jets are studied and compared with recent theoretical predictions.","sentences":["Measurements at $\\sqrt{s}$ = 13.6 TeV of the opposite-sign W boson pair production cross section in proton-proton collisions are presented.","The data used in this study were collected with the CMS detector at the CERN LHC in 2022, and correspond to an integrated luminosity of 34.8 fb$^{-1}$. Events are selected by requiring one electron and one muon of opposite charge.","A maximum likelihood fit is performed on signal- and background-enriched data categories defined by the flavour and charge of the leptons, the number of jets, and number of jets originating from b quarks.","An inclusive W$^+$W$^-$ production cross section of 125.7 $\\pm$ 5.6 pb is measured, in agreement with standard model predictions.","Cross sections are also reported in a fiducial region close to that of the detector acceptance, both inclusively and differentially, as a function of the jet multiplicity in the event.","For first time in proton-proton collisions, WW events with at least two reconstructed jets are studied and compared with recent theoretical predictions."],"url":"http://arxiv.org/abs/2406.05101v1","category":"hep-ex"}
{"created":"2024-06-07 15:40:23","title":"From cryptomarkets to the surface web: Scouting eBay for counterfeits","abstract":"Detecting counterfeits on online marketplaces is challenging, and current methods struggle with the volume of sales on platforms like eBay, while cryptomarkets openly sell counterfeits. Leveraging information from 453 cryptomarket counterfeits, we automated a search for corresponding products on eBay, utilizing image and text similarity metrics. We collected data twice over 4-months to analyze changes with an average of 159 eBay products per cryptomarket item, totaling 134k products. We found identical products, which would warrant further investigation as to whether they are counterfeits. Results indicate increasing difficulty finding similar products over time, moderated by product type and origin. Future improved versions of the current system could be used to examine possible connections between cryptomarket and surface web listings more closely and could hold practical value in supporting the detection of counterfeits on the surface web.","sentences":["Detecting counterfeits on online marketplaces is challenging, and current methods struggle with the volume of sales on platforms like eBay, while cryptomarkets openly sell counterfeits.","Leveraging information from 453 cryptomarket counterfeits, we automated a search for corresponding products on eBay, utilizing image and text similarity metrics.","We collected data twice over 4-months to analyze changes with an average of 159 eBay products per cryptomarket item, totaling 134k products.","We found identical products, which would warrant further investigation as to whether they are counterfeits.","Results indicate increasing difficulty finding similar products over time, moderated by product type and origin.","Future improved versions of the current system could be used to examine possible connections between cryptomarket and surface web listings more closely and could hold practical value in supporting the detection of counterfeits on the surface web."],"url":"http://arxiv.org/abs/2406.05021v1","category":"cs.SI"}
{"created":"2024-06-07 14:37:26","title":"Fast Brownian cluster dynamics","abstract":"We present an efficient method to perform overdamped Brownian dynamics simulations in external force fields and for particle interactions that include a hardcore part. The method applies to particle motion in one dimension, where it is possible to update particle positions by repositioning particle clusters as a whole. These clusters consist of several particles in contact. They form because particle collisions are treated as completely inelastic rather than elastic ones. Updating of cluster positions in time steps is carried out by cluster fragmentation and merging procedures. The presented method is particularly powerful at high collision rates in densely crowded systems, where collective movements of particle assemblies is governing the dynamics. As an application, we simulate the single-file diffusion of sticky hard spheres in a periodic potential.","sentences":["We present an efficient method to perform overdamped Brownian dynamics simulations in external force fields and for particle interactions that include a hardcore part.","The method applies to particle motion in one dimension, where it is possible to update particle positions by repositioning particle clusters as a whole.","These clusters consist of several particles in contact.","They form because particle collisions are treated as completely inelastic rather than elastic ones.","Updating of cluster positions in time steps is carried out by cluster fragmentation and merging procedures.","The presented method is particularly powerful at high collision rates in densely crowded systems, where collective movements of particle assemblies is governing the dynamics.","As an application, we simulate the single-file diffusion of sticky hard spheres in a periodic potential."],"url":"http://arxiv.org/abs/2406.04972v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-07 12:18:02","title":"HateDebias: On the Diversity and Variability of Hate Speech Debiasing","abstract":"Hate speech on social media is ubiquitous but urgently controlled. Without detecting and mitigating the biases brought by hate speech, different types of ethical problems. While a number of datasets have been proposed to address the problem of hate speech detection, these datasets seldom consider the diversity and variability of bias, making it far from real-world scenarios. To fill this gap, we propose a benchmark, named HateDebias, to analyze the model ability of hate speech detection under continuous, changing environments. Specifically, to meet the diversity of biases, we collect existing hate speech detection datasets with different types of biases. To further meet the variability (i.e., the changing of bias attributes in datasets), we reorganize datasets to follow the continuous learning setting. We evaluate the detection accuracy of models trained on the datasets with a single type of bias with the performance on the HateDebias, where a significant performance drop is observed. To provide a potential direction for debiasing, we further propose a debiasing framework based on continuous learning and bias information regularization, as well as the memory replay strategies to ensure the debiasing ability of the model. Experiment results on the proposed benchmark show that the aforementioned method can improve several baselines with a distinguished margin, highlighting its effectiveness in real-world applications.","sentences":["Hate speech on social media is ubiquitous but urgently controlled.","Without detecting and mitigating the biases brought by hate speech, different types of ethical problems.","While a number of datasets have been proposed to address the problem of hate speech detection, these datasets seldom consider the diversity and variability of bias, making it far from real-world scenarios.","To fill this gap, we propose a benchmark, named HateDebias, to analyze the model ability of hate speech detection under continuous, changing environments.","Specifically, to meet the diversity of biases, we collect existing hate speech detection datasets with different types of biases.","To further meet the variability (i.e., the changing of bias attributes in datasets), we reorganize datasets to follow the continuous learning setting.","We evaluate the detection accuracy of models trained on the datasets with a single type of bias with the performance on the HateDebias, where a significant performance drop is observed.","To provide a potential direction for debiasing, we further propose a debiasing framework based on continuous learning and bias information regularization, as well as the memory replay strategies to ensure the debiasing ability of the model.","Experiment results on the proposed benchmark show that the aforementioned method can improve several baselines with a distinguished margin, highlighting its effectiveness in real-world applications."],"url":"http://arxiv.org/abs/2406.04876v1","category":"cs.CL"}
{"created":"2024-06-07 12:14:27","title":"3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views","abstract":"3D cars are commonly used in self-driving systems, virtual/augmented reality, and games. However, existing 3D car datasets are either synthetic or low-quality, presenting a significant gap toward the high-quality real-world 3D car datasets and limiting their applications in practical scenarios. In this paper, we propose the first large-scale 3D real car dataset, termed 3DRealCar, offering three distinctive features. (1) \\textbf{High-Volume}: 2,500 cars are meticulously scanned by 3D scanners, obtaining car images and point clouds with real-world dimensions; (2) \\textbf{High-Quality}: Each car is captured in an average of 200 dense, high-resolution 360-degree RGB-D views, enabling high-fidelity 3D reconstruction; (3) \\textbf{High-Diversity}: The dataset contains various cars from over 100 brands, collected under three distinct lighting conditions, including reflective, standard, and dark. Additionally, we offer detailed car parsing maps for each instance to promote research in car parsing tasks. Moreover, we remove background point clouds and standardize the car orientation to a unified axis for the reconstruction only on cars without background and controllable rendering. We benchmark 3D reconstruction results with state-of-the-art methods across each lighting condition in 3DRealCar. Extensive experiments demonstrate that the standard lighting condition part of 3DRealCar can be used to produce a large number of high-quality 3D cars, improving various 2D and 3D tasks related to cars. Notably, our dataset brings insight into the fact that recent 3D reconstruction methods face challenges in reconstructing high-quality 3D cars under reflective and dark lighting conditions. \\textcolor{red}{\\href{https://xiaobiaodu.github.io/3drealcar/}{Our dataset is available here.}}","sentences":["3D cars are commonly used in self-driving systems, virtual/augmented reality, and games.","However, existing 3D car datasets are either synthetic or low-quality, presenting a significant gap toward the high-quality real-world 3D car datasets and limiting their applications in practical scenarios.","In this paper, we propose the first large-scale 3D real car dataset, termed 3DRealCar, offering three distinctive features.","(1) \\textbf{High-Volume}: 2,500 cars are meticulously scanned by 3D scanners, obtaining car images and point clouds with real-world dimensions; (2) \\textbf{High-Quality}: Each car is captured in an average of 200 dense, high-resolution 360-degree RGB-D views, enabling high-fidelity 3D reconstruction; (3) \\textbf{High-Diversity}:","The dataset contains various cars from over 100 brands, collected under three distinct lighting conditions, including reflective, standard, and dark.","Additionally, we offer detailed car parsing maps for each instance to promote research in car parsing tasks.","Moreover, we remove background point clouds and standardize the car orientation to a unified axis for the reconstruction only on cars without background and controllable rendering.","We benchmark 3D reconstruction results with state-of-the-art methods across each lighting condition in 3DRealCar.","Extensive experiments demonstrate that the standard lighting condition part of 3DRealCar can be used to produce a large number of high-quality 3D cars, improving various 2D and 3D tasks related to cars.","Notably, our dataset brings insight into the fact that recent 3D reconstruction methods face challenges in reconstructing high-quality 3D cars under reflective and dark lighting conditions.","\\textcolor{red}{\\href{https://xiaobiaodu.github.io/3drealcar/}{Our dataset is available here.}}"],"url":"http://arxiv.org/abs/2406.04875v1","category":"cs.CV"}
{"created":"2024-06-07 12:09:55","title":"The $\u03b2$ Pictoris b Hill sphere transit campaign. Paper II: Searching for the signatures of the $\u03b2$ Pictoris exoplanets through time delay analysis of the $\u03b4$ Scuti pulsations","abstract":"The $\\beta$ Pictoris system is the closest known stellar system with directly detected gas giant planets, an edge-on circumstellar disc, and evidence of falling sublimating bodies and transiting exocomets. The inner planet, $\\beta$ Pictoris c, has also been indirectly detected with radial velocity (RV) measurements. The star is a known $\\delta$ Scuti pulsator, and the long-term stability of these pulsations opens up the possibility of indirectly detecting the gas giant planets through time delays of the pulsations due to a varying light travel time. We search for phase shifts in the $\\delta$ Scuti pulsations consistent with the known planets $\\beta$ Pictoris b and c and carry out an analysis of the stellar pulsations of $\\beta$ Pictoris over a multi-year timescale. We used photometric data collected by the BRITE-Constellation, bRing, ASTEP, and TESS to derive a list of the strongest and most significant $\\delta$ Scuti pulsations. We carried out an analysis with the open-source python package maelstrom to study the stability of the pulsation modes of $\\beta$ Pictoris in order to determine the long-term trends in the observed pulsations. We did not detect the expected signal for $\\beta$ Pictoris b or $\\beta$ Pictoris c. The expected time delay is 6 seconds for $\\beta$ Pictoris c and 24 seconds for $\\beta$ Pictoris b. With simulations, we determined that the photometric noise in all the combined data sets cannot reach the sensitivity needed to detect the expected timing drifts. An analysis of the pulsational modes of $\\beta$ Pictoris using maelstrom showed that the modes themselves drift on the timescale of a year, fundamentally limiting our ability to detect exoplanets around $\\beta$ Pictoris via pulsation timing.","sentences":["The $\\beta$ Pictoris system is the closest known stellar system with directly detected gas giant planets, an edge-on circumstellar disc, and evidence of falling sublimating bodies and transiting exocomets.","The inner planet, $\\beta$ Pictoris c, has also been indirectly detected with radial velocity (RV) measurements.","The star is a known $\\delta$ Scuti pulsator, and the long-term stability of these pulsations opens up the possibility of indirectly detecting the gas giant planets through time delays of the pulsations due to a varying light travel time.","We search for phase shifts in the $\\delta$ Scuti pulsations consistent with the known planets $\\beta$ Pictoris b and c and carry out an analysis of the stellar pulsations of $\\beta$ Pictoris over a multi-year timescale.","We used photometric data collected by the BRITE-Constellation, bRing, ASTEP, and TESS to derive a list of the strongest and most significant $\\delta$ Scuti pulsations.","We carried out an analysis with the open-source python package maelstrom to study the stability of the pulsation modes of $\\beta$ Pictoris in order to determine the long-term trends in the observed pulsations.","We did not detect the expected signal for $\\beta$ Pictoris b or $\\beta$ Pictoris c.","The expected time delay is 6 seconds for $\\beta$ Pictoris c and 24 seconds for $\\beta$ Pictoris b.","With simulations, we determined that the photometric noise in all the combined data sets cannot reach the sensitivity needed to detect the expected timing drifts.","An analysis of the pulsational modes of $\\beta$ Pictoris using maelstrom showed that the modes themselves drift on the timescale of a year, fundamentally limiting our ability to detect exoplanets around $\\beta$ Pictoris via pulsation timing."],"url":"http://arxiv.org/abs/2406.04870v1","category":"astro-ph.EP"}
{"created":"2024-06-07 11:38:12","title":"The Russian Legislative Corpus","abstract":"We present the comprehensive Russian primary and secondary legislation corpus covering 1991 to 2023. The corpus collects all 281,413 texts (176,523,268 tokens) of non-secret federal regulations and acts, along with their metadata. The corpus has two versions the original text with minimal preprocessing and a version prepared for linguistic analysis with morphosyntactic markup.","sentences":["We present the comprehensive Russian primary and secondary legislation corpus covering 1991 to 2023.","The corpus collects all 281,413 texts (176,523,268 tokens) of non-secret federal regulations and acts, along with their metadata.","The corpus has two versions the original text with minimal preprocessing and a version prepared for linguistic analysis with morphosyntactic markup."],"url":"http://arxiv.org/abs/2406.04855v1","category":"cs.CL"}
{"created":"2024-06-07 09:44:56","title":"Children's expressed emotions during playful learning games","abstract":"Studies on software tutoring systems for complex learning have shown that confusion has a beneficial relationship with the learning experience and student engagement (Arguel et al., 2017). Causing confusion can prevent boredom while signs of confusion can serve as a signal of genuine learning and as a predecessor for frustration. There is little to no research on the role of confusion in early childhood education and playful learning, as these studies primarily focus on high school and university students during complex learning tasks. Despite that, the field acknowledges that confusion may be caused by inconsistency between information and a student's internal model referred to as cognitive disequilibrium known from the theory of cognitive development, which was originally theorized based on observational studies on young children (D'Mello and Graesser, 2012). Therefore, there is reason to expect that the virtues of confusion also apply to young children engaging in learning activities, such as playful learning. To investigate the role of confusion in playful learning, we conducted an observational study, in which the behavior and expressed emotions of young children were collected by familiar pedagogues, using a web app, while they engaged with playful learning games designed for kindergartens. The expressed emotions were analyzed using a likelihood metric to determine the likely transitions between emotions (D'Mello and Graesser, 2012). The preliminary results showed that during short play sessions, children express confusion, frustration, and boredom. Furthermore, the observed emotional transitions were matched with previously established models of affect dynamics during complex learning. We argue that games with a learning objective can benefit by purposely confusing the player and how the player's confusion may be managed to improve the learning experience.","sentences":["Studies on software tutoring systems for complex learning have shown that confusion has a beneficial relationship with the learning experience and student engagement (Arguel et al., 2017).","Causing confusion can prevent boredom while signs of confusion can serve as a signal of genuine learning and as a predecessor for frustration.","There is little to no research on the role of confusion in early childhood education and playful learning, as these studies primarily focus on high school and university students during complex learning tasks.","Despite that, the field acknowledges that confusion may be caused by inconsistency between information and a student's internal model referred to as cognitive disequilibrium known from the theory of cognitive development, which was originally theorized based on observational studies on young children (D'Mello and Graesser, 2012).","Therefore, there is reason to expect that the virtues of confusion also apply to young children engaging in learning activities, such as playful learning.","To investigate the role of confusion in playful learning, we conducted an observational study, in which the behavior and expressed emotions of young children were collected by familiar pedagogues, using a web app, while they engaged with playful learning games designed for kindergartens.","The expressed emotions were analyzed using a likelihood metric to determine the likely transitions between emotions (D'Mello and Graesser, 2012).","The preliminary results showed that during short play sessions, children express confusion, frustration, and boredom.","Furthermore, the observed emotional transitions were matched with previously established models of affect dynamics during complex learning.","We argue that games with a learning objective can benefit by purposely confusing the player and how the player's confusion may be managed to improve the learning experience."],"url":"http://arxiv.org/abs/2406.04794v1","category":"cs.HC"}
{"created":"2024-06-07 08:39:40","title":"A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences","abstract":"Optimizing discrete black-box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black-box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark","sentences":["Optimizing discrete black-box functions is key in several domains, e.g. protein engineering and drug design.","Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks.","Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently.","However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks.","To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black-box functions representing real-world application domains in chemistry and biology.","These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers.","Project website: https://machinelearninglifescience.github.io/hdbo_benchmark"],"url":"http://arxiv.org/abs/2406.04739v1","category":"cs.LG"}
{"created":"2024-06-07 17:57:40","title":"PatchSVD: A Non-uniform SVD-based Image Compression Algorithm","abstract":"Storing data is particularly a challenge when dealing with image data which often involves large file sizes due to the high resolution and complexity of images. Efficient image compression algorithms are crucial to better manage data storage costs. In this paper, we propose a novel region-based lossy image compression technique, called PatchSVD, based on the Singular Value Decomposition (SVD) algorithm. We show through experiments that PatchSVD outperforms SVD-based image compression with respect to three popular image compression metrics. Moreover, we compare PatchSVD compression artifacts with those of Joint Photographic Experts Group (JPEG) and SVD-based image compression and illustrate some cases where PatchSVD compression artifacts are preferable compared to JPEG and SVD artifacts.","sentences":["Storing data is particularly a challenge when dealing with image data which often involves large file sizes due to the high resolution and complexity of images.","Efficient image compression algorithms are crucial to better manage data storage costs.","In this paper, we propose a novel region-based lossy image compression technique, called PatchSVD, based on the Singular Value Decomposition (SVD) algorithm.","We show through experiments that PatchSVD outperforms SVD-based image compression with respect to three popular image compression metrics.","Moreover, we compare PatchSVD compression artifacts with those of Joint Photographic Experts Group (JPEG) and SVD-based image compression and illustrate some cases where PatchSVD compression artifacts are preferable compared to JPEG and SVD artifacts."],"url":"http://arxiv.org/abs/2406.05129v1","category":"cs.CV"}
{"created":"2024-06-07 17:55:43","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in processing vision-language tasks. One of the crux of MLLMs lies in vision tokenization, which involves efficiently transforming input visual signals into feature representations that are most beneficial for LLMs. However, existing vision tokenizers, essential for semantic alignment between vision and language, remain problematic. Existing methods aggressively fragment visual input, corrupting the visual semantic integrity. To address this, this paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok), which groups visual features into semantic units via a dynamic clustering algorithm, flexibly determining the number of tokens based on image complexity. The resulting vision tokens effectively preserve semantic integrity and capture both low-frequency and high-frequency visual features. The proposed MLLM (Setokim) equipped with SeTok significantly demonstrates superior performance across various tasks, as evidenced by our experimental results. The project page is at https://chocowu.github.io/SeTok-web/.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in processing vision-language tasks.","One of the crux of MLLMs lies in vision tokenization, which involves efficiently transforming input visual signals into feature representations that are most beneficial for LLMs.","However, existing vision tokenizers, essential for semantic alignment between vision and language, remain problematic.","Existing methods aggressively fragment visual input, corrupting the visual semantic integrity.","To address this, this paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok), which groups visual features into semantic units via a dynamic clustering algorithm, flexibly determining the number of tokens based on image complexity.","The resulting vision tokens effectively preserve semantic integrity and capture both low-frequency and high-frequency visual features.","The proposed MLLM (Setokim) equipped with SeTok significantly demonstrates superior performance across various tasks, as evidenced by our experimental results.","The project page is at https://chocowu.github.io/SeTok-web/."],"url":"http://arxiv.org/abs/2406.05127v1","category":"cs.CV"}
{"created":"2024-06-07 17:48:42","title":"Kru\u017ekov-type uniqueness theorem for the chemical flood conservation law system with local vanishing viscosity admissibility","abstract":"We study the uniqueness of solutions of the initial-boundary value problem in the quarter-plane for the chemical flood conservation law system in the class of piece-wise $\\mathcal C^1$-smooth functions under certain restrictions. The vanishing viscosity method is used locally on the discontinuities of the solution to determine admissible and inadmissible shocks. The Lagrange coordinate transformation is utilized in order to split the equations. The proof of uniqueness is based on an entropy inequality similar to the one used in the classical Kru\\v{z}kov's theorem.","sentences":["We study the uniqueness of solutions of the initial-boundary value problem in the quarter-plane for the chemical flood conservation law system in the class of piece-wise $\\mathcal C^1$-smooth functions under certain restrictions.","The vanishing viscosity method is used locally on the discontinuities of the solution to determine admissible and inadmissible shocks.","The Lagrange coordinate transformation is utilized in order to split the equations.","The proof of uniqueness is based on an entropy inequality similar to the one used in the classical Kru\\v{z}kov's theorem."],"url":"http://arxiv.org/abs/2406.05116v1","category":"math.AP"}
{"created":"2024-06-07 17:40:38","title":"Adapting Physics-Informed Neural Networks To Optimize ODEs in Mosquito Population Dynamics","abstract":"Physics informed neural networks have been gaining popularity due to their unique ability to incorporate physics laws into data-driven models, ensuring that the predictions are not only consistent with empirical data but also align with domain-specific knowledge in the form of physics equations. The integration of physics principles enables the method to require less data while maintaining the robustness of deep learning in modeling complex dynamical systems. However, current PINN frameworks are not sufficiently mature for real-world ODE systems, especially those with extreme multi-scale behavior such as mosquito population dynamical modelling. In this research, we propose a PINN framework with several improvements for forward and inverse problems for ODE systems with a case study application in modelling the dynamics of mosquito populations. The framework tackles the gradient imbalance and stiff problems posed by mosquito ordinary differential equations. The method offers a simple but effective way to resolve the time causality issue in PINNs by gradually expanding the training time domain until it covers entire domain of interest. As part of a robust evaluation, we conduct experiments using simulated data to evaluate the effectiveness of the approach. Preliminary results indicate that physics-informed machine learning holds significant potential for advancing the study of ecological systems.","sentences":["Physics informed neural networks have been gaining popularity due to their unique ability to incorporate physics laws into data-driven models, ensuring that the predictions are not only consistent with empirical data but also align with domain-specific knowledge in the form of physics equations.","The integration of physics principles enables the method to require less data while maintaining the robustness of deep learning in modeling complex dynamical systems.","However, current PINN frameworks are not sufficiently mature for real-world ODE systems, especially those with extreme multi-scale behavior such as mosquito population dynamical modelling.","In this research, we propose a PINN framework with several improvements for forward and inverse problems for ODE systems with a case study application in modelling the dynamics of mosquito populations.","The framework tackles the gradient imbalance and stiff problems posed by mosquito ordinary differential equations.","The method offers a simple but effective way to resolve the time causality issue in PINNs by gradually expanding the training time domain until it covers entire domain of interest.","As part of a robust evaluation, we conduct experiments using simulated data to evaluate the effectiveness of the approach.","Preliminary results indicate that physics-informed machine learning holds significant potential for advancing the study of ecological systems."],"url":"http://arxiv.org/abs/2406.05108v1","category":"q-bio.PE"}
{"created":"2024-06-07 17:29:33","title":"Dynamical correlation functions in the Ising field theory","abstract":"We study finite temperature dynamical correlation functions of the magnetization operator in the one-dimensional Ising quantum field theory. Our approach is based on a finite temperature form factor series and on a Fredholm determinant representation of the correlators. While for space-like separations the Fredholm determinant can be efficiently evaluated numerically, for the time-like region it has convergence issues inherited from the form factor series. We develop a method to compute the correlation functions at time-like separations based on the analytic continuation of the space-time coordinates to complex values. Using this numerical technique, we explore all space-time and temperature regimes in both the ordered and disordered phases including short, large, and near-light-cone separations at low and high temperatures. We confirm the existing analytic predictions for the asymptotic behavior of the correlations except in the case of space-like correlations in the paramagnetic phase. For this case we derive a new closed form expression for the correlation length that has some unusual properties: it is a non-analytic function of both the space-time direction and the temperature, and its temperature dependence is non-monotonic.","sentences":["We study finite temperature dynamical correlation functions of the magnetization operator in the one-dimensional Ising quantum field theory.","Our approach is based on a finite temperature form factor series and on a Fredholm determinant representation of the correlators.","While for space-like separations the Fredholm determinant can be efficiently evaluated numerically, for the time-like region it has convergence issues inherited from the form factor series.","We develop a method to compute the correlation functions at time-like separations based on the analytic continuation of the space-time coordinates to complex values.","Using this numerical technique, we explore all space-time and temperature regimes in both the ordered and disordered phases including short, large, and near-light-cone separations at low and high temperatures.","We confirm the existing analytic predictions for the asymptotic behavior of the correlations except in the case of space-like correlations in the paramagnetic phase.","For this case we derive a new closed form expression for the correlation length that has some unusual properties: it is a non-analytic function of both the space-time direction and the temperature, and its temperature dependence is non-monotonic."],"url":"http://arxiv.org/abs/2406.05100v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-07 17:15:07","title":"Investigating the price determinants of the European Emission Trading System: a non-parametric approach","abstract":"The European carbon market plays a pivotal role in the European Union's ambitious target of achieving carbon neutrality by 2050. Understanding the intricacies of factors influencing European Union Emission Trading System (EU ETS) market prices is paramount for effective policy making and strategy implementation. We propose the use of the Information Imbalance, a recently introduced non-parametric measure quantifying the degree to which a set of variables is informative with respect to another one, to study the relationships among macroeconomic, economic, uncertainty, and energy variables concerning EU ETS prices. Our analysis shows that in Phase 3 commodity related variables such as the ERIX index are the most informative to explain the behaviour of the EU ETS market price. Transitioning to Phase 4, financial fluctuations take centre stage, with the uncertainty in the EUR/CHF exchange rate emerging as a crucial determinant. These results reflect the disruptive impacts of the COVID-19 pandemic and the energy crisis in reshaping the importance of the different variables. Beyond variable analysis, we also propose to leverage the Information Imbalance to address the problem of mixed-frequency forecasting, and we identify the weekly time scale as the most informative for predicting the EU ETS price. Finally, we show how the Information Imbalance can be effectively combined with Gaussian Process regression for efficient nowcasting and forecasting using very small sets of highly informative predictors.","sentences":["The European carbon market plays a pivotal role in the European Union's ambitious target of achieving carbon neutrality by 2050.","Understanding the intricacies of factors influencing European Union Emission Trading System (EU ETS) market prices is paramount for effective policy making and strategy implementation.","We propose the use of the Information Imbalance, a recently introduced non-parametric measure quantifying the degree to which a set of variables is informative with respect to another one, to study the relationships among macroeconomic, economic, uncertainty, and energy variables concerning EU ETS prices.","Our analysis shows that in Phase 3 commodity related variables such as the ERIX index are the most informative to explain the behaviour of the EU ETS market price.","Transitioning to Phase 4, financial fluctuations take centre stage, with the uncertainty in the EUR/CHF exchange rate emerging as a crucial determinant.","These results reflect the disruptive impacts of the COVID-19 pandemic and the energy crisis in reshaping the importance of the different variables.","Beyond variable analysis, we also propose to leverage the Information Imbalance to address the problem of mixed-frequency forecasting, and we identify the weekly time scale as the most informative for predicting the EU ETS price.","Finally, we show how the Information Imbalance can be effectively combined with Gaussian Process regression for efficient nowcasting and forecasting using very small sets of highly informative predictors."],"url":"http://arxiv.org/abs/2406.05094v1","category":"q-fin.ST"}
{"created":"2024-06-07 16:43:54","title":"Linearization Turns Neural Operators into Function-Valued Gaussian Processes","abstract":"Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using function-valued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure--a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations.","sentences":["Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations.","Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data.","As for all statistical models, the predictions of these models are imperfect and exhibit errors.","Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems.","We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using function-valued Gaussian processes.","Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators.","In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure--a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points.","The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets.","We showcase the efficacy of our approach through applications to different types of partial differential equations."],"url":"http://arxiv.org/abs/2406.05072v1","category":"cs.LG"}
{"created":"2024-06-07 16:17:38","title":"The formation and survival of cold gas in a magnetized cool-core galaxy cluster","abstract":"Filaments of cold gas ($T\\leq 10^{4}$ K) are found in the inner regions of many cool-core clusters. These structures are thought to play a major role in the regulation of feedback from active galactic nuclei (AGN). We study the morphology of the filaments, their formation, and their impact on the propagation of the outflowing AGN jets. We present a set of GPU-accelerated 3D (magneto)hydrodynamical simulations of an idealized Perseus-like cluster using the performance portable code AthenaPK. We include radiative cooling, and a self-regulated AGN feedback model that redistributes accreted material through kinetic, thermal and magnetic feedback. We confirm that magnetic fields play an important role in both the formation and evolution of the cold material. These suppress the formation of massive cold disks and favor magnetically supported filaments over clumpy structures. Achieving resolutions of $25-50$ pc, we find that filaments are not monolithic as they contain numerous and complex magnetically--supported substructures. We find that the mass distribution of these clumps follows a power law for all investigated filaments, consistent with previous cloud-crushing simulations of individual clumps. Studying the evolution of individual filaments, we find that their formation pathways can be diverse. We find examples of filaments forming through a combination of cold gas uplifting and condensation, as well as systems of purely infalling clumps condensing out of the intracluster medium. The density contrast between the cold gas and the outflowing hot material leads to recurring deflections of the jets, favoring inflation of bubbles.","sentences":["Filaments of cold gas ($T\\leq 10^{4}$ K) are found in the inner regions of many cool-core clusters.","These structures are thought to play a major role in the regulation of feedback from active galactic nuclei (AGN).","We study the morphology of the filaments, their formation, and their impact on the propagation of the outflowing AGN jets.","We present a set of GPU-accelerated 3D (magneto)hydrodynamical simulations of an idealized Perseus-like cluster using the performance portable code AthenaPK.","We include radiative cooling, and a self-regulated AGN feedback model that redistributes accreted material through kinetic, thermal and magnetic feedback.","We confirm that magnetic fields play an important role in both the formation and evolution of the cold material.","These suppress the formation of massive cold disks and favor magnetically supported filaments over clumpy structures.","Achieving resolutions of $25-50$ pc, we find that filaments are not monolithic as they contain numerous and complex magnetically--supported substructures.","We find that the mass distribution of these clumps follows a power law for all investigated filaments, consistent with previous cloud-crushing simulations of individual clumps.","Studying the evolution of individual filaments, we find that their formation pathways can be diverse.","We find examples of filaments forming through a combination of cold gas uplifting and condensation, as well as systems of purely infalling clumps condensing out of the intracluster medium.","The density contrast between the cold gas and the outflowing hot material leads to recurring deflections of the jets, favoring inflation of bubbles."],"url":"http://arxiv.org/abs/2406.05044v1","category":"astro-ph.GA"}
{"created":"2024-06-07 16:15:46","title":"Quantitative convergence guarantees for the mean-field dispersion process","abstract":"We study the dispersion process on the complete graph introduced in the recent work \\cite{de_dispersion_2023} under the mean-field framework. In contrast to the probabilistic approach taken in \\cite{de_dispersion_2023} and many other related works, our focus is on the investigation of the large time behavior of solutions of the associated kinetic mean-field system of nonlinear ordinary differential equations (ODEs). We establish various analytical and quantitative convergence results for the long time behaviour of the mean-field system and related numerical illustrations are also provided.","sentences":["We study the dispersion process on the complete graph introduced in the recent work \\cite{de_dispersion_2023} under the mean-field framework.","In contrast to the probabilistic approach taken in \\cite{de_dispersion_2023} and many other related works, our focus is on the investigation of the large time behavior of solutions of the associated kinetic mean-field system of nonlinear ordinary differential equations (ODEs).","We establish various analytical and quantitative convergence results for the long time behaviour of the mean-field system and related numerical illustrations are also provided."],"url":"http://arxiv.org/abs/2406.05043v1","category":"math.PR"}
{"created":"2024-06-07 16:15:00","title":"Digital Twins of the EM Environment: Benchmark for Ray Launching Models","abstract":"Digital Twin has emerged as a promising paradigm for accurately representing the electromagnetic (EM) wireless environments. The resulting virtual representation of the reality facilitates comprehensive insights into the propagation environment, empowering multi-layer decision-making processes at the physical communication level. This paper investigates the digitization of wireless communication propagation, with particular emphasis on the indispensable aspect of ray-based propagation simulation for real-time Digital Twins. A benchmark for ray-based propagation simulations is presented to evaluate computational time, with two urban scenarios characterized by different mesh complexity, single and multiple wireless link configurations, and simulations with/without diffuse scattering. Exhaustive empirical analyses are performed showing and comparing the behavior of different ray-based solutions. By offering standardized simulations and scenarios, this work provides a technical benchmark for practitioners involved in the implementation of real-time Digital Twins and optimization of ray-based propagation models.","sentences":["Digital Twin has emerged as a promising paradigm for accurately representing the electromagnetic (EM) wireless environments.","The resulting virtual representation of the reality facilitates comprehensive insights into the propagation environment, empowering multi-layer decision-making processes at the physical communication level.","This paper investigates the digitization of wireless communication propagation, with particular emphasis on the indispensable aspect of ray-based propagation simulation for real-time Digital Twins.","A benchmark for ray-based propagation simulations is presented to evaluate computational time, with two urban scenarios characterized by different mesh complexity, single and multiple wireless link configurations, and simulations with/without diffuse scattering.","Exhaustive empirical analyses are performed showing and comparing the behavior of different ray-based solutions.","By offering standardized simulations and scenarios, this work provides a technical benchmark for practitioners involved in the implementation of real-time Digital Twins and optimization of ray-based propagation models."],"url":"http://arxiv.org/abs/2406.05042v1","category":"eess.SP"}
{"created":"2024-06-07 16:14:51","title":"Online Frequency Scheduling by Learning Parallel Actions","abstract":"Radio Resource Management is a challenging topic in future 6G networks where novel applications create strong competition among the users for the available resources. In this work we consider the frequency scheduling problem in a multi-user MIMO system. Frequency resources need to be assigned to a set of users while allowing for concurrent transmissions in the same sub-band. Traditional methods are insufficient to cope with all the involved constraints and uncertainties, whereas reinforcement learning can directly learn near-optimal solutions for such complex environments. However, the scheduling problem has an enormous action space accounting for all the combinations of users and sub-bands, so out-of-the-box algorithms cannot be used directly. In this work, we propose a scheduler based on action-branching over sub-bands, which is a deep Q-learning architecture with parallel decision capabilities. The sub-bands learn correlated but local decision policies and altogether they optimize a global reward. To improve the scaling of the architecture with the number of sub-bands, we propose variations (Unibranch, Graph Neural Network-based) that reduce the number of parameters to learn. The parallel decision making of the proposed architecture allows to meet short inference time requirements in real systems. Furthermore, the deep Q-learning approach permits online fine-tuning after deployment to bridge the sim-to-real gap. The proposed architectures are evaluated against relevant baselines from the literature showing competitive performance and possibilities of online adaptation to evolving environments.","sentences":["Radio Resource Management is a challenging topic in future 6G networks where novel applications create strong competition among the users for the available resources.","In this work we consider the frequency scheduling problem in a multi-user MIMO system.","Frequency resources need to be assigned to a set of users while allowing for concurrent transmissions in the same sub-band.","Traditional methods are insufficient to cope with all the involved constraints and uncertainties, whereas reinforcement learning can directly learn near-optimal solutions for such complex environments.","However, the scheduling problem has an enormous action space accounting for all the combinations of users and sub-bands, so out-of-the-box algorithms cannot be used directly.","In this work, we propose a scheduler based on action-branching over sub-bands, which is a deep Q-learning architecture with parallel decision capabilities.","The sub-bands learn correlated but local decision policies and altogether they optimize a global reward.","To improve the scaling of the architecture with the number of sub-bands, we propose variations (Unibranch, Graph Neural Network-based) that reduce the number of parameters to learn.","The parallel decision making of the proposed architecture allows to meet short inference time requirements in real systems.","Furthermore, the deep Q-learning approach permits online fine-tuning after deployment to bridge the sim-to-real gap.","The proposed architectures are evaluated against relevant baselines from the literature showing competitive performance and possibilities of online adaptation to evolving environments."],"url":"http://arxiv.org/abs/2406.05041v1","category":"cs.NI"}
{"created":"2024-06-07 16:09:37","title":"Structured physics-guided neural networks for electromagnetic commutation applied to industrial linear motors","abstract":"Mechatronic systems are described by an interconnection of the electromagnetic part, i.e., a static position-dependent nonlinear relation between currents and forces, and the mechanical part, i.e., a dynamic relation from forces to position. Commutation inverts a model of the electromagnetic part of the system, and thereby removes the electromagnetic part from the position control problem. Typical commutation algorithms rely on simplified models derived from physics-based knowledge, which do not take into account position dependent parasitic effects. In turn, these commutation related model errors translate into position tracking errors, which limit the system performance. Therefore, in this work, we develop a data-driven approach to commutation using physics-guided neural networks (PGNNs). A novel PGNN model is proposed which structures neural networks (NNs) to learn specific motor dependent parasitic effects. The PGNN is used to identify a model of the electromagnetic part using force measurements, after which it is analytically inverted to obtain a PGNN-based commutation algorithm. Motivated by industrial applications, we develop an input transformation to deal with systems with fixed commutation, i.e., when the currents cannot be controlled. Real-life experiments on an industrial coreless linear motor (CLM) demonstrate a factor 10 improvement in the commutation error in driving direction and a factor 4 improvement in the position error with respect to classical commutation in terms of the mean--squared error (MSE).","sentences":["Mechatronic systems are described by an interconnection of the electromagnetic part, i.e., a static position-dependent nonlinear relation between currents and forces, and the mechanical part, i.e., a dynamic relation from forces to position.","Commutation inverts a model of the electromagnetic part of the system, and thereby removes the electromagnetic part from the position control problem.","Typical commutation algorithms rely on simplified models derived from physics-based knowledge, which do not take into account position dependent parasitic effects.","In turn, these commutation related model errors translate into position tracking errors, which limit the system performance.","Therefore, in this work, we develop a data-driven approach to commutation using physics-guided neural networks (PGNNs).","A novel PGNN model is proposed which structures neural networks (NNs) to learn specific motor dependent parasitic effects.","The PGNN is used to identify a model of the electromagnetic part using force measurements, after which it is analytically inverted to obtain a PGNN-based commutation algorithm.","Motivated by industrial applications, we develop an input transformation to deal with systems with fixed commutation, i.e., when the currents cannot be controlled.","Real-life experiments on an industrial coreless linear motor (CLM) demonstrate a factor 10 improvement in the commutation error in driving direction and a factor 4 improvement in the position error with respect to classical commutation in terms of the mean--squared error (MSE)."],"url":"http://arxiv.org/abs/2406.05040v1","category":"eess.SY"}
{"created":"2024-06-07 15:58:23","title":"Linear stability analysis for a system of singular amplitude equations arising in biomorphology","abstract":"We study linear stability of exponential periodic solutions of a system of singular amplitude equations associated with convective Turing bifurcation in the presence of conservation laws, as arises in modern biomorphology models, binary fluids, and elsewhere. Consisting of a complex Ginzburg-Landau equation coupled with a singular convection-diffusion equation in \"mean modes\" associated with conservation laws, these were shown previously by the authors to admit a constant-coefficient linearized stability analysis as in the classical Ginzburg-Landau case -- albeit now singular in wave amplitude epsilon -- yielding useful necessary conditions for stability, both of the exponential functions as solutions of the amplitude equations, and of the associated periodic pattern solving the underlying PDE. Here, we show by a delicate two-parameter matrix perturbation analysis that (strict) satisfaction of these necessary conditions is also sufficient for diffusive stability in the sense of Schneider, yielding a corresponding result, and nonlinear stability, for the underlying PDE. Moreover, we show that they may be interpreted as stability along a non-normally hyperbolic slow manifold approximated by Darcy-type reduction, together with attraction along transverse mean modes, connecting with finite-time approximation theorems of Hacker-Schneider-Zimmerman.","sentences":["We study linear stability of exponential periodic solutions of a system of singular amplitude equations associated with convective Turing bifurcation in the presence of conservation laws, as arises in modern biomorphology models, binary fluids, and elsewhere.","Consisting of a complex Ginzburg-Landau equation coupled with a singular convection-diffusion equation in \"mean modes\" associated with conservation laws, these were shown previously by the authors to admit a constant-coefficient linearized stability analysis as in the classical Ginzburg-Landau case -- albeit now singular in wave amplitude epsilon -- yielding useful necessary conditions for stability, both of the exponential functions as solutions of the amplitude equations, and of the associated periodic pattern solving the underlying PDE.","Here, we show by a delicate two-parameter matrix perturbation analysis that (strict) satisfaction of these necessary conditions is also sufficient for diffusive stability in the sense of Schneider, yielding a corresponding result, and nonlinear stability, for the underlying PDE.","Moreover, we show that they may be interpreted as stability along a non-normally hyperbolic slow manifold approximated by Darcy-type reduction, together with attraction along transverse mean modes, connecting with finite-time approximation theorems of Hacker-Schneider-Zimmerman."],"url":"http://arxiv.org/abs/2406.05037v1","category":"math.AP"}
{"created":"2024-06-07 15:53:06","title":"Gradient Descent on Logistic Regression with Non-Separable Data and Large Step Sizes","abstract":"We study gradient descent (GD) dynamics on logistic regression problems with large, constant step sizes. For linearly-separable data, it is known that GD converges to the minimizer with arbitrarily large step sizes, a property which no longer holds when the problem is not separable. In fact, the behaviour can be much more complex -- a sequence of period-doubling bifurcations begins at the critical step size $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of the Hessian at the solution. Using a smaller-than-critical step size guarantees convergence if initialized nearby the solution: but does this suffice globally? In one dimension, we show that a step size less than $1/\\lambda$ suffices for global convergence. However, for all step sizes between $1/\\lambda$ and the critical step size $2/\\lambda$, one can construct a dataset such that GD converges to a stable cycle. In higher dimensions, this is actually possible even for step sizes less than $1/\\lambda$. Our results show that although local convergence is guaranteed for all step sizes less than the critical step size, global convergence is not, and GD may instead converge to a cycle depending on the initialization.","sentences":["We study gradient descent (GD) dynamics on logistic regression problems with large, constant step sizes.","For linearly-separable data, it is known that GD converges to the minimizer with arbitrarily large step sizes, a property which no longer holds when the problem is not separable.","In fact, the behaviour can be much more complex -- a sequence of period-doubling bifurcations begins at the critical step size $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of the Hessian at the solution.","Using a smaller-than-critical step size guarantees convergence if initialized nearby the solution: but does this suffice globally?","In one dimension, we show that a step size less than $1/\\lambda$ suffices for global convergence.","However, for all step sizes between $1/\\lambda$ and the critical step size $2/\\lambda$, one can construct a dataset such that GD converges to a stable cycle.","In higher dimensions, this is actually possible even for step sizes less than $1/\\lambda$. Our results show that although local convergence is guaranteed for all step sizes less than the critical step size, global convergence is not, and GD may instead converge to a cycle depending on the initialization."],"url":"http://arxiv.org/abs/2406.05033v1","category":"cs.LG"}
{"created":"2024-06-07 15:53:01","title":"Dynamical Schwinger Effect in Lead-Halide Perovskites","abstract":"We observe strong photoluminescence of a lead-halide perovskite driven by deep sub-gap irradiation. Using the quasi-adiabatic Landau-Dykhne approach, we interpret this observation in terms of the dynamical Schwinger effect -- tunneling ionization in strong fields. Further, the exponential sensitivity of the Schwinger effect to driving fields allows us to measure the local frozen-in fields in a nominally cubic MAPbBr$_3$ single crystal at room temperature. Finally, we demonstrate an AC analogue of biasing in our system -- the cooperation between two time-dependent fields simultaneously driving the sample. Our results (i) establish lead-halide perovskites as an excellent platform for simulating effects of strong fields on Dirac fields, (ii) contribute to the on-going discussion about inversion-breaking in MAPbBr$_3$ single crystal, and (iii) pave the way for a mid-infrared light detection with lead-halide perovskites.","sentences":["We observe strong photoluminescence of a lead-halide perovskite driven by deep sub-gap irradiation.","Using the quasi-adiabatic Landau-Dykhne approach, we interpret this observation in terms of the dynamical Schwinger effect -- tunneling ionization in strong fields.","Further, the exponential sensitivity of the Schwinger effect to driving fields allows us to measure the local frozen-in fields in a nominally cubic MAPbBr$_3$ single crystal at room temperature.","Finally, we demonstrate an AC analogue of biasing in our system -- the cooperation between two time-dependent fields simultaneously driving the sample.","Our results (i) establish lead-halide perovskites as an excellent platform for simulating effects of strong fields on Dirac fields, (ii) contribute to the on-going discussion about inversion-breaking in MAPbBr$_3$ single crystal, and (iii) pave the way for a mid-infrared light detection with lead-halide perovskites."],"url":"http://arxiv.org/abs/2406.05032v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-07 15:47:55","title":"Kondo Screening and Indirect Magnetic Exchange through a Conventional Superconductor Studied by the Density-Matrix Renormalization Group","abstract":"The competition between the Kondo screening and indirect magnetic exchange in systems with two magnetic impurities coupled to a conventional s-wave superconductor gives rise to a nontrivial ground-state phase diagram. Here, we utilize the density-matrix renormalization group (DMRG) method and exploit the non-abelian spin-SU(2) symmetry to study the phase diagram for two quantum-spin-$\\frac12$ impurities locally exchange coupled to large one-dimensional chains. The nonlocal inter-impurity exchange is treated as an emergent Ruderman-Kittel-Kasuya-Yosida (RKKY) coupling. We find qualitatively different phase diagrams for impurity spins coupled to sites with odd or even distances $d$ on the chain and a partial-Kondo-screened spin-doublet phase that extends over the whole range of local exchange couplings $J$ in the limit of weak superconducting pairing strength $\\Delta$. Our numerical studies are complemented by exact diagonalization of small (quantum-box) systems and by perturbative-in-$J$ computations of the $d$ and $\\Delta$ dependent RKKY interaction. It is thereby demonstrated that the specific system geometry is essential for our understanding of magnetic impurity interactions in superconducting hosts, and thus for insights into the control of quantum-state properties in nanoparticle systems and topological superconductivity.","sentences":["The competition between the Kondo screening and indirect magnetic exchange in systems with two magnetic impurities coupled to a conventional s-wave superconductor gives rise to a nontrivial ground-state phase diagram.","Here, we utilize the density-matrix renormalization group (DMRG) method and exploit the non-abelian spin-SU(2) symmetry to study the phase diagram for two quantum-spin-$\\frac12$ impurities locally exchange coupled to large one-dimensional chains.","The nonlocal inter-impurity exchange is treated as an emergent Ruderman-Kittel-Kasuya-Yosida (RKKY) coupling.","We find qualitatively different phase diagrams for impurity spins coupled to sites with odd or even distances $d$ on the chain and a partial-Kondo-screened spin-doublet phase that extends over the whole range of local exchange couplings $J$ in the limit of weak superconducting pairing strength $\\Delta$. Our numerical studies are complemented by exact diagonalization of small (quantum-box) systems and by perturbative-in-$J$ computations of the $d$ and $\\Delta$ dependent RKKY interaction.","It is thereby demonstrated that the specific system geometry is essential for our understanding of magnetic impurity interactions in superconducting hosts, and thus for insights into the control of quantum-state properties in nanoparticle systems and topological superconductivity."],"url":"http://arxiv.org/abs/2406.05029v1","category":"cond-mat.str-el"}
{"created":"2024-06-07 15:30:24","title":"Nano-Focusing of Vortex Beams with Hyperbolic Metamaterials","abstract":"The synergy of judiciously engineered nanostructures and complex topology of light creates unprecedented opportunities for tailoring light-matter interactions on the nanoscale. Electromagnetic waves can carry multiple units of angular momentum per photon, stemming from both spin and orbital angular momentum contributions, offering a potential route for modifying the optical transition selection rules. However, the size difference between a vortex beam and quantum objects limits the interaction strength and the angular momentum exchange. Here, we demonstrate the sub-diffraction-limited focusing of a vortex beam using the high in-plane wave number modes present in hyperbolic metamaterials. The spin-orbit interaction within the hyperbolic structure gives rise to the formation of an optical skyrmion with a deep subwavelength structure, which may enable the exploration of new light-matter interaction phenomena.","sentences":["The synergy of judiciously engineered nanostructures and complex topology of light creates unprecedented opportunities for tailoring light-matter interactions on the nanoscale.","Electromagnetic waves can carry multiple units of angular momentum per photon, stemming from both spin and orbital angular momentum contributions, offering a potential route for modifying the optical transition selection rules.","However, the size difference between a vortex beam and quantum objects limits the interaction strength and the angular momentum exchange.","Here, we demonstrate the sub-diffraction-limited focusing of a vortex beam using the high in-plane wave number modes present in hyperbolic metamaterials.","The spin-orbit interaction within the hyperbolic structure gives rise to the formation of an optical skyrmion with a deep subwavelength structure, which may enable the exploration of new light-matter interaction phenomena."],"url":"http://arxiv.org/abs/2406.05016v1","category":"physics.optics"}
{"created":"2024-06-07 15:22:45","title":"ComplexityMeasures.jl: scalable software to unify and accelerate entropy and complexity timeseries analysis","abstract":"In the nonlinear timeseries analysis literature, countless quantities have been presented as new \"entropy\" or \"complexity\" measures, often with similar roles. The ever-increasing pool of such measures makes creating a sustainable and all-encompassing software for them difficult both conceptually and pragmatically. Such a software however would be an important tool that can aid researchers make an informed decision of which measure to use and for which application, as well as accelerate novel research. Here we present ComplexityMeasures.jl, an easily extendable and highly performant open-source software that implements a vast selection of complexity measures. The software provides 1530 measures with 3,834 lines of source code, averaging only 2.5 lines of code per exported quantity (version 3.5). This is made possible by its mathematically rigorous composable design. In this paper we discuss the software design and demonstrate how it can accelerate complexity-related research in the future. We carefully compare it with alternative software and conclude that ComplexityMeasures.jl outclasses the alternatives in several objective aspects of comparison, such as computational performance, overall amount of measures, reliability, and extendability. ComplexityMeasures.jl is also a component of the DynamicalSystems.jl library for nonlinear dynamics and nonlinear timeseries analysis and follows open source development practices for creating a sustainable community of developers.","sentences":["In the nonlinear timeseries analysis literature, countless quantities have been presented as new \"entropy\" or \"complexity\" measures, often with similar roles.","The ever-increasing pool of such measures makes creating a sustainable and all-encompassing software for them difficult both conceptually and pragmatically.","Such a software however would be an important tool that can aid researchers make an informed decision of which measure to use and for which application, as well as accelerate novel research.","Here we present ComplexityMeasures.jl, an easily extendable and highly performant open-source software that implements a vast selection of complexity measures.","The software provides 1530 measures with 3,834 lines of source code, averaging only 2.5 lines of code per exported quantity (version 3.5).","This is made possible by its mathematically rigorous composable design.","In this paper we discuss the software design and demonstrate how it can accelerate complexity-related research in the future.","We carefully compare it with alternative software and conclude that ComplexityMeasures.jl outclasses the alternatives in several objective aspects of comparison, such as computational performance, overall amount of measures, reliability, and extendability.","ComplexityMeasures.jl is also a component of the DynamicalSystems.jl library for nonlinear dynamics and nonlinear timeseries analysis and follows open source development practices for creating a sustainable community of developers."],"url":"http://arxiv.org/abs/2406.05011v1","category":"cs.SE"}
{"created":"2024-06-07 15:22:16","title":"Testing common invariant subspace of multilayer networks","abstract":"Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common invariant subspace of the networks, because such common invariant subspace could capture the fundamental structural patterns and interactions across all layers. Many methods have been proposed to estimate the common invariant subspace. However, whether real-world multilayer networks share the same common subspace remains unknown. In this paper, we first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same subspace, and under the alternative hypothesis, there exist at least two networks that do not have the same subspace. We propose a Weighted Degree Difference Test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.","sentences":["Graph (or network) is a mathematical structure that has been widely used to model relational data.","As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems.","One active research problem in multilayer networks analysis is to study the common invariant subspace of the networks, because such common invariant subspace could capture the fundamental structural patterns and interactions across all layers.","Many methods have been proposed to estimate the common invariant subspace.","However, whether real-world multilayer networks share the same common subspace remains unknown.","In this paper, we first attempt to answer this question by means of hypothesis testing.","The null hypothesis states that the multilayer networks share the same subspace, and under the alternative hypothesis, there exist at least two networks that do not have the same subspace.","We propose a Weighted Degree Difference Test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power.","Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided."],"url":"http://arxiv.org/abs/2406.05010v1","category":"stat.ME"}
{"created":"2024-06-07 15:19:53","title":"Diversity in Fermi/GBM Gamma Ray Bursts: New insights from Machine Learning","abstract":"Classification of gamma-ray bursts (GRBs) has been a long-standing puzzle in high-energy astrophysics. Recent observations challenge the traditional short vs. long viewpoint, where long GRBs are thought to originate from the collapse of massive stars and short GRBs from compact binary mergers. Machine learning (ML) algorithms have been instrumental in addressing this problem, revealing five distinct GRB groups within the Swift/BAT light curve data, two of which are associated with kilonovae (KNe). We corroborate these five classes by extending this analysis to the Fermi/GBM data using unsupervised ML techniques. These five clusters are well separated in fluence-duration plane, hinting at a potential link between fluence, duration and complexities (or structures) in the light curves of GRBs. Further, we confirm two distinct classes of KN-associated GRBs. The presence of GRB 170817A in one of the two KNe-associated clusters lends evidence to the hypothesis that this class of GRBs could potentially be produced by binary neutron star (BNS) mergers. The second KN-associated GRB cluster could potentially originate from NS-BH mergers. Future multimessenger observations of compact binaries in gravitational waves (GWs) and electromagnetic waves can be paramount in understanding these clusters better.","sentences":["Classification of gamma-ray bursts (GRBs) has been a long-standing puzzle in high-energy astrophysics.","Recent observations challenge the traditional short vs. long viewpoint, where long GRBs are thought to originate from the collapse of massive stars and short GRBs from compact binary mergers.","Machine learning (ML) algorithms have been instrumental in addressing this problem, revealing five distinct GRB groups within the Swift/BAT light curve data, two of which are associated with kilonovae (KNe).","We corroborate these five classes by extending this analysis to the Fermi/GBM data using unsupervised ML techniques.","These five clusters are well separated in fluence-duration plane, hinting at a potential link between fluence, duration and complexities (or structures) in the light curves of GRBs.","Further, we confirm two distinct classes of KN-associated GRBs.","The presence of GRB 170817A in one of the two KNe-associated clusters lends evidence to the hypothesis that this class of GRBs could potentially be produced by binary neutron star (BNS) mergers.","The second KN-associated GRB cluster could potentially originate from NS-BH mergers.","Future multimessenger observations of compact binaries in gravitational waves (GWs) and electromagnetic waves can be paramount in understanding these clusters better."],"url":"http://arxiv.org/abs/2406.05005v1","category":"astro-ph.HE"}
{"created":"2024-06-07 15:19:33","title":"The Choquet-Deny Property for Groupoids","abstract":"A countable discrete group is called Choquet-Deny if for any non-degenerate probability measure on the group, the corresponding space of bounded harmonic functions is trivial. Building on the previous work of Jaworski, a complete characterization of Choquet-Deny groups was recently achieved by Frisch, Hartman, Tamuz, and Ferdowski. In this article, we extend the study of the Choquet-Deny property to the framework of discrete measured groupoids. Our primary result offers a complete characterization of this property in terms of the isotropy groups and the equivalence relation associated with the given groupoid. Additionally, we use the implications derived from our main theorem to classify the Choquet-Deny property of transformation groupoids.","sentences":["A countable discrete group is called Choquet-Deny if for any non-degenerate probability measure on the group, the corresponding space of bounded harmonic functions is trivial.","Building on the previous work of Jaworski, a complete characterization of Choquet-Deny groups was recently achieved by Frisch, Hartman, Tamuz, and Ferdowski.","In this article, we extend the study of the Choquet-Deny property to the framework of discrete measured groupoids.","Our primary result offers a complete characterization of this property in terms of the isotropy groups and the equivalence relation associated with the given groupoid.","Additionally, we use the implications derived from our main theorem to classify the Choquet-Deny property of transformation groupoids."],"url":"http://arxiv.org/abs/2406.05004v1","category":"math.FA"}
{"created":"2024-06-07 15:07:07","title":"On the social bias of speech self-supervised models","abstract":"Self-supervised learning (SSL) speech models have achieved remarkable performance in various tasks, yet the biased outcomes, especially affecting marginalized groups, raise significant concerns. Social bias refers to the phenomenon where algorithms potentially amplify disparate properties between social groups present in the data used for training. Bias in SSL models can perpetuate injustice by automating discriminatory patterns and reinforcing inequitable systems. This work reveals that prevalent SSL models inadvertently acquire biased associations. We probe how various factors, such as model architecture, size, and training methodologies, influence the propagation of social bias within these models. Finally, we explore the efficacy of debiasing SSL models through regularization techniques, specifically via model compression. Our findings reveal that employing techniques such as row-pruning and training wider, shallower models can effectively mitigate social bias within SSL model.","sentences":["Self-supervised learning (SSL) speech models have achieved remarkable performance in various tasks, yet the biased outcomes, especially affecting marginalized groups, raise significant concerns.","Social bias refers to the phenomenon where algorithms potentially amplify disparate properties between social groups present in the data used for training.","Bias in SSL models can perpetuate injustice by automating discriminatory patterns and reinforcing inequitable systems.","This work reveals that prevalent SSL models inadvertently acquire biased associations.","We probe how various factors, such as model architecture, size, and training methodologies, influence the propagation of social bias within these models.","Finally, we explore the efficacy of debiasing SSL models through regularization techniques, specifically via model compression.","Our findings reveal that employing techniques such as row-pruning and training wider, shallower models can effectively mitigate social bias within SSL model."],"url":"http://arxiv.org/abs/2406.04997v1","category":"eess.AS"}
{"created":"2024-06-07 15:06:36","title":"Data2Neo -- A Tool for Complex Neo4j Data Integration","abstract":"This paper introduces Data2Neo, an open-source Python library for converting relational data into knowledge graphs stored in Neo4j databases. With extensive customization options and support for continuous online data integration from various data sources, Data2Neo is designed to be user-friendly, efficient, and scalable to large datasets. The tool significantly lowers the barrier to entry for creating and using knowledge graphs, making this increasingly popular form of data representation accessible to a wider audience. The code is available at https://github.com/jkminder/data2neo .","sentences":["This paper introduces Data2Neo, an open-source Python library for converting relational data into knowledge graphs stored in Neo4j databases.","With extensive customization options and support for continuous online data integration from various data sources, Data2Neo is designed to be user-friendly, efficient, and scalable to large datasets.","The tool significantly lowers the barrier to entry for creating and using knowledge graphs, making this increasingly popular form of data representation accessible to a wider audience.","The code is available at https://github.com/jkminder/data2neo ."],"url":"http://arxiv.org/abs/2406.04995v1","category":"cs.DB"}
{"created":"2024-06-07 15:04:28","title":"Quantum hardware demonstrations of relativistic calculations of molecular electric dipole moments: from light to heavy systems using Variational Quantum Eigensolver","abstract":"The quantum-classical hybrid Variational Quantum Eigensolver (VQE) algorithm is recognized to be the method of choice to obtain ground state energies of quantum many-body systems in the noisy intermediate scale quantum (NISQ) era. This study not only extends the VQE algorithm to the relativistic regime, but also calculates a property other than energy, namely the molecular permanent electric dipole moment (PDM). We carry out 18-qubit quantum simulations to obtain ground state energies as well as PDMs of single-valence diatomic molecules, ranging from the light BeH to the heavy radioactive RaH molecule. We investigate the correlation trends in these systems as well as access the precision in our results. Furthermore, we measure the PDM of the moderately heavy SrH and SrF molecules on the optimized unitary coupled cluster state, using the state-of-the-art IonQ Aria-I quantum computer in an active space of 6 qubits. The associated quantum circuits for these computations were extensively optimized in view of limitations imposed by NISQ hardware. To that end, we employ an array of techniques, including the use of point group symmetries, integrating ZX-Calculus into our pipeline-based circuit optimization, and energy sort VQE procedure. Through these methods, we compress our 6-qubit quantum circuit from 280 two-qubit gates to 37 two-qubit gates (with a marginal trade-off of 0.33 and 0.31 percent in the PDM for SrH and SrF in their respective 6-spin orbital active spaces). We anticipate that our proof-of-concept demonstration lays the groundwork for future quantum hardware calculations involving heavy atoms and molecules.","sentences":["The quantum-classical hybrid Variational Quantum Eigensolver (VQE) algorithm is recognized to be the method of choice to obtain ground state energies of quantum many-body systems in the noisy intermediate scale quantum (NISQ) era.","This study not only extends the VQE algorithm to the relativistic regime, but also calculates a property other than energy, namely the molecular permanent electric dipole moment (PDM).","We carry out 18-qubit quantum simulations to obtain ground state energies as well as PDMs of single-valence diatomic molecules, ranging from the light BeH to the heavy radioactive RaH molecule.","We investigate the correlation trends in these systems as well as access the precision in our results.","Furthermore, we measure the PDM of the moderately heavy SrH and SrF molecules on the optimized unitary coupled cluster state, using the state-of-the-art IonQ Aria-I quantum computer in an active space of 6 qubits.","The associated quantum circuits for these computations were extensively optimized in view of limitations imposed by NISQ hardware.","To that end, we employ an array of techniques, including the use of point group symmetries, integrating ZX-Calculus into our pipeline-based circuit optimization, and energy sort VQE procedure.","Through these methods, we compress our 6-qubit quantum circuit from 280 two-qubit gates to 37 two-qubit gates (with a marginal trade-off of 0.33 and 0.31 percent in the PDM for SrH and SrF in their respective 6-spin orbital active spaces).","We anticipate that our proof-of-concept demonstration lays the groundwork for future quantum hardware calculations involving heavy atoms and molecules."],"url":"http://arxiv.org/abs/2406.04992v1","category":"physics.atom-ph"}
{"created":"2024-06-07 14:49:22","title":"MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter","abstract":"Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.","sentences":["Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources.","However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters.","To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient.","This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU).","We store and update the parameters of larger adapters on the CPU.","Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU.","This is particularly beneficial over the limited bandwidth of PCI Express (PCIe).","Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency.","Our codes are available at https://github.com/CURRENTF/MEFT."],"url":"http://arxiv.org/abs/2406.04984v1","category":"cs.CL"}
{"created":"2024-06-07 14:41:05","title":"Interacting Fermi systems in the tracial state","abstract":"We argue that for Fermi systems on lattices or the continuum with interaction invariant under a kind of Galilei transformation the time evolution is either weakly asymptotically abelian or at least $\\eta$-abelian in the tracial state but not norm asymptotically abelian.","sentences":["We argue that for Fermi systems on lattices or the continuum with interaction invariant under a kind of Galilei transformation the time evolution is either weakly asymptotically abelian or at least $\\eta$-abelian in the tracial state but not norm asymptotically abelian."],"url":"http://arxiv.org/abs/2406.04977v1","category":"math-ph"}
{"created":"2024-06-07 14:37:50","title":"Light holographic dilatons near critical points","abstract":"We investigate the relation between the emergence of a dilaton in gapped (confining) field theories, and the presence of either complex fixed points or instabilities in the strongly coupled dynamics in two classes of bottom-up holographic models. We demonstrate that in one of the two classes there is a critical line of first-order phase transitions (at zero temperature) that terminates at a critical point. We calculate the mass spectrum of fluctuations of the associated regular gravity backgrounds, which we interpret as bound states in the dual field theories. In proximity to the second-order phase transition, we find a parametrically light scalar state, and its composition leads us to identify it as a dilaton.","sentences":["We investigate the relation between the emergence of a dilaton in gapped (confining) field theories, and the presence of either complex fixed points or instabilities in the strongly coupled dynamics in two classes of bottom-up holographic models.","We demonstrate that in one of the two classes there is a critical line of first-order phase transitions (at zero temperature) that terminates at a critical point.","We calculate the mass spectrum of fluctuations of the associated regular gravity backgrounds, which we interpret as bound states in the dual field theories.","In proximity to the second-order phase transition, we find a parametrically light scalar state, and its composition leads us to identify it as a dilaton."],"url":"http://arxiv.org/abs/2406.04974v1","category":"hep-th"}
{"created":"2024-06-07 14:00:14","title":"Effect of time-varying X-ray emission from stellar flares on the ionization of protoplanetary disks","abstract":"X-rays have significant impacts on cold, weakly ionized protoplanetary disks by increasing the ionization rate and driving chemical reactions. Stellar flares are explosions that emit intense X-rays and are the unique source of hard X-rays with an energy of $\\gtrsim10$ keV in the protoplanetary disk systems. Hard X-rays should be carefully taken into account in models as they can reach the disk midplane as a result of scattering in the disk atmospheres. However, previous models are insufficient to predict the hard X-ray spectra because of the simplification in flare models. We develop a model of X-ray spectra of stellar flares based on observations and flare theories. The flare temperature and nonthermal electron emissions are modeled as functions of flare energy, which allows us to better predict the hard X-ray photon flux than before. Using our X-ray model, we conduct radiative transfer calculations to investigate the impact of flare hard X-rays on disk ionization, with a particular focus on the protoplanetary disk around a T Tauri star. We demonstrate that for a flare with an energy of $\\geq 10^{33}$ erg, X-ray photons with $\\gtrsim 10$ keV penetrate down to the midplane to increase the ionization rates more than galactic cosmic rays. We also find that the 10-year-averaged X-rays from multiple flares could certainly contribute to the ionization at the disk midplane. These results emphasize the importance of stellar flares on the disk evolution.","sentences":["X-rays have significant impacts on cold, weakly ionized protoplanetary disks by increasing the ionization rate and driving chemical reactions.","Stellar flares are explosions that emit intense X-rays and are the unique source of hard X-rays with an energy of $\\gtrsim10$ keV in the protoplanetary disk systems.","Hard X-rays should be carefully taken into account in models as they can reach the disk midplane as a result of scattering in the disk atmospheres.","However, previous models are insufficient to predict the hard X-ray spectra because of the simplification in flare models.","We develop a model of X-ray spectra of stellar flares based on observations and flare theories.","The flare temperature and nonthermal electron emissions are modeled as functions of flare energy, which allows us to better predict the hard X-ray photon flux than before.","Using our X-ray model, we conduct radiative transfer calculations to investigate the impact of flare hard X-rays on disk ionization, with a particular focus on the protoplanetary disk around a T Tauri star.","We demonstrate that for a flare with an energy of $\\geq 10^{33}$ erg, X-ray photons with $\\gtrsim 10$ keV penetrate down to the midplane to increase the ionization rates more than galactic cosmic rays.","We also find that the 10-year-averaged X-rays from multiple flares could certainly contribute to the ionization at the disk midplane.","These results emphasize the importance of stellar flares on the disk evolution."],"url":"http://arxiv.org/abs/2406.04946v1","category":"astro-ph.SR"}
{"created":"2024-06-07 13:53:24","title":"Multiple-input, multiple-output modal testing of a Hawk T1A aircraft: A new full-scale dataset for structural health monitoring","abstract":"The use of measured vibration data from structures has a long history of enabling the development of methods for inference and monitoring. In particular, applications based on system identification and structural health monitoring have risen to prominence over recent decades and promise significant benefits when implemented in practice. However, significant challenges remain in the development of these methods. The introduction of realistic, full-scale datasets will be an important contribution to overcoming these challenges. This paper presents a new benchmark dataset capturing the dynamic response of a decommissioned BAE Systems Hawk T1A. The dataset reflects the behaviour of a complex structure with a history of service that can still be tested in controlled laboratory conditions, using a variety of known loading and damage simulation conditions. As such, it provides a key stepping stone between simple laboratory test structures and in-service structures. In this paper, the Hawk structure is described in detail, alongside a comprehensive summary of the experimental work undertaken. Following this, key descriptive highlights of the dataset are presented, before a discussion of the research challenges that the data present. Using the dataset, non-linearity in the structure is demonstrated, as well as the sensitivity of the structure to damage of different types. The dataset is highly applicable to many academic enquiries and additional analysis techniques which will enable further advancement of vibration-based engineering techniques.","sentences":["The use of measured vibration data from structures has a long history of enabling the development of methods for inference and monitoring.","In particular, applications based on system identification and structural health monitoring have risen to prominence over recent decades and promise significant benefits when implemented in practice.","However, significant challenges remain in the development of these methods.","The introduction of realistic, full-scale datasets will be an important contribution to overcoming these challenges.","This paper presents a new benchmark dataset capturing the dynamic response of a decommissioned BAE Systems Hawk T1A.","The dataset reflects the behaviour of a complex structure with a history of service that can still be tested in controlled laboratory conditions, using a variety of known loading and damage simulation conditions.","As such, it provides a key stepping stone between simple laboratory test structures and in-service structures.","In this paper, the Hawk structure is described in detail, alongside a comprehensive summary of the experimental work undertaken.","Following this, key descriptive highlights of the dataset are presented, before a discussion of the research challenges that the data present.","Using the dataset, non-linearity in the structure is demonstrated, as well as the sensitivity of the structure to damage of different types.","The dataset is highly applicable to many academic enquiries and additional analysis techniques which will enable further advancement of vibration-based engineering techniques."],"url":"http://arxiv.org/abs/2406.04943v1","category":"eess.SY"}
{"created":"2024-06-07 13:46:18","title":"The lens was fabricated by fluidic shaping","abstract":"As an important optical component, lens is widely used in scientific inquiry and production. At present, lens manufacturing mainly relies on grinding, polishing and other methods. However, these methods often require expensive equipment and complex processes. This paper presents a method of injecting liquid material into the frame structure and curing it quickly. At the same time, based on the principle of energy minimization, we give a set of theory that can accurately predict the lens face shape, and give the simulation results by software. In this paper, 3D printing technology was used to produce different shapes of borders, which were used to produce free-form surface and spherical lens samples. By characterizing their surface contours and optical properties, the practicability of the method was verified. This method has the advantages of low cost, fast forming, high surface smoothness, and can theoretically prepare any size aperture lens, which has great potential for development.","sentences":["As an important optical component, lens is widely used in scientific inquiry and production.","At present, lens manufacturing mainly relies on grinding, polishing and other methods.","However, these methods often require expensive equipment and complex processes.","This paper presents a method of injecting liquid material into the frame structure and curing it quickly.","At the same time, based on the principle of energy minimization, we give a set of theory that can accurately predict the lens face shape, and give the simulation results by software.","In this paper, 3D printing technology was used to produce different shapes of borders, which were used to produce free-form surface and spherical lens samples.","By characterizing their surface contours and optical properties, the practicability of the method was verified.","This method has the advantages of low cost, fast forming, high surface smoothness, and can theoretically prepare any size aperture lens, which has great potential for development."],"url":"http://arxiv.org/abs/2406.04937v1","category":"physics.optics"}
{"created":"2024-06-07 13:36:48","title":"Quantifying the quantum nature of high spin YSR excitations in transverse magnetic field","abstract":"Excitations of individual and coupled spins on superconductors provide a platform to study quantum spin impurity models as well as a pathway toward realizing topological quantum computing. Here, we characterize, using ultra-low temperature scanning tunneling microscopy/spectroscopy, the Yu-Shiba-Rusinov (YSR) states of individual manganese phthalocyanine molecules with high spin character on the surface of an ultra-thin lead film in variable transverse magnetic field. We observe two types of YSR excitations, depending on the adsorption geometry of the molecule. Using a zero-bandwidth model, we detail the role of the magnetic anisotropy, spin-spin exchange, and Kondo exchange. We illustrate that one molecular type can be treated as an individual spin akin to an isolated spin on the metal center, whereas the other molecular type invokes a coupled spin system represented by a spin on the center and the ligand. Using the field-dependent evolution of the YSR excitations and comparisons to modeling, we describe the quantum phase of each of the molecules. These results provide an insight into the quantum nature of YSR excitations in magnetic field, and a platform to study spin impurity models on superconductors in magnetic field.","sentences":["Excitations of individual and coupled spins on superconductors provide a platform to study quantum spin impurity models as well as a pathway toward realizing topological quantum computing.","Here, we characterize, using ultra-low temperature scanning tunneling microscopy/spectroscopy, the Yu-Shiba-Rusinov (YSR) states of individual manganese phthalocyanine molecules with high spin character on the surface of an ultra-thin lead film in variable transverse magnetic field.","We observe two types of YSR excitations, depending on the adsorption geometry of the molecule.","Using a zero-bandwidth model, we detail the role of the magnetic anisotropy, spin-spin exchange, and Kondo exchange.","We illustrate that one molecular type can be treated as an individual spin akin to an isolated spin on the metal center, whereas the other molecular type invokes a coupled spin system represented by a spin on the center and the ligand.","Using the field-dependent evolution of the YSR excitations and comparisons to modeling, we describe the quantum phase of each of the molecules.","These results provide an insight into the quantum nature of YSR excitations in magnetic field, and a platform to study spin impurity models on superconductors in magnetic field."],"url":"http://arxiv.org/abs/2406.04931v1","category":"cond-mat.supr-con"}
{"created":"2024-06-07 13:24:19","title":"Sim-to-real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning","abstract":"Sim-to-real transfer presents a difficult challenge, where models trained in simulation are to be deployed in the real world. The distribution shift between the two settings leads to biased representations of the perceived real-world environment, and thus to suboptimal predictions. In this work, we tackle the challenge of sim-to-real transfer of reinforcement learning (RL) agents for coverage path planning (CPP). In CPP, the task is for a robot to find a path that visits every point of a confined area. Specifically, we consider the case where the environment is unknown, and the agent needs to plan the path online while mapping the environment. We bridge the sim-to-real gap through a semi-virtual environment with a simulated sensor and obstacles, while including real robot kinematics and real-time aspects. We investigate what level of fine-tuning is needed for adapting to a realistic setting, comparing to an agent trained solely in simulation. We find that a high model inference frequency is sufficient for reducing the sim-to-real gap, while fine-tuning degrades performance initially. By training the model in simulation and deploying it at a high inference frequency, we transfer state-of-the-art results from simulation to the real domain, where direct learning would take in the order of weeks with manual interaction, i.e., would be completely infeasible.","sentences":["Sim-to-real transfer presents a difficult challenge, where models trained in simulation are to be deployed in the real world.","The distribution shift between the two settings leads to biased representations of the perceived real-world environment, and thus to suboptimal predictions.","In this work, we tackle the challenge of sim-to-real transfer of reinforcement learning (RL) agents for coverage path planning (CPP).","In CPP, the task is for a robot to find a path that visits every point of a confined area.","Specifically, we consider the case where the environment is unknown, and the agent needs to plan the path online while mapping the environment.","We bridge the sim-to-real gap through a semi-virtual environment with a simulated sensor and obstacles, while including real robot kinematics and real-time aspects.","We investigate what level of fine-tuning is needed for adapting to a realistic setting, comparing to an agent trained solely in simulation.","We find that a high model inference frequency is sufficient for reducing the sim-to-real gap, while fine-tuning degrades performance initially.","By training the model in simulation and deploying it at a high inference frequency, we transfer state-of-the-art results from simulation to the real domain, where direct learning would take in the order of weeks with manual interaction, i.e., would be completely infeasible."],"url":"http://arxiv.org/abs/2406.04920v1","category":"cs.RO"}
{"created":"2024-06-07 13:22:40","title":"Anomaly inflow for dipole symmetry and higher form foliated field theories","abstract":"In accordance with recent progress of fracton topological phases, unusual topological phases of matter hosting fractionalized quasiparticle excitations with mobility constraints, new type of symmetry is studied -- multipole symmetry, associated with conservation of multipoles. Based on algebraic relation between dipole and global charges, we introduce a series of $(d+1)$-dimensional BF theories with $p$-form gauge fields, which admit dipole of spatially extended excitations, and study their physical properties. We elucidate that gauge invariant loops have unusual form, containing linear function of the spatial coordinate, which leads to the position dependent braiding statistics and unusual ground state degeneracy dependence on the system size. We also show that the theories exhibit a mixed 't Hooft anomaly between $p$-form and $(d-p)$-form dipole symmetries, which is canceled by an invertible theory defined in one dimensional higher via anomaly inflow mechanism.","sentences":["In accordance with recent progress of fracton topological phases, unusual topological phases of matter hosting fractionalized quasiparticle excitations with mobility constraints, new type of symmetry is studied -- multipole symmetry, associated with conservation of multipoles.","Based on algebraic relation between dipole and global charges, we introduce a series of $(d+1)$-dimensional BF theories with $p$-form gauge fields, which admit dipole of spatially extended excitations, and study their physical properties.","We elucidate that gauge invariant loops have unusual form, containing linear function of the spatial coordinate, which leads to the position dependent braiding statistics and unusual ground state degeneracy dependence on the system size.","We also show that the theories exhibit a mixed 't Hooft anomaly between $p$-form and $(d-p)$-form dipole symmetries, which is canceled by an invertible theory defined in one dimensional higher via anomaly inflow mechanism."],"url":"http://arxiv.org/abs/2406.04919v1","category":"cond-mat.str-el"}
{"created":"2024-06-07 13:06:09","title":"Mexican Computers: A Brief Technical and Historical Overview","abstract":"The emergence of the microprocessor in the early 1970s allowed the design of computers that did not require the substantial economic resources possessed by large computer companies of that era. Shortly after this event, a variety of computers based on microprocessors appeared in the United States and other developed countries. Unlike in those countries, where small and large companies developed most personal computers, in Mexico, the first microprocessor-based computers were designed within academic institutions. It is little known that Mexican computers of that era include a variety of systems ranging from purpose-specific research and teaching-oriented computers to high-performance personal computers. The goal of this article is to describe in detail some of these Mexican computers designed between the late 1970s and mid-1980s.","sentences":["The emergence of the microprocessor in the early 1970s allowed the design of computers that did not require the substantial economic resources possessed by large computer companies of that era.","Shortly after this event, a variety of computers based on microprocessors appeared in the United States and other developed countries.","Unlike in those countries, where small and large companies developed most personal computers, in Mexico, the first microprocessor-based computers were designed within academic institutions.","It is little known that Mexican computers of that era include a variety of systems ranging from purpose-specific research and teaching-oriented computers to high-performance personal computers.","The goal of this article is to describe in detail some of these Mexican computers designed between the late 1970s and mid-1980s."],"url":"http://arxiv.org/abs/2406.04912v1","category":"cs.AR"}
{"created":"2024-06-07 12:58:38","title":"A Modular Framework for Flexible Planning in Human-Robot Collaboration","abstract":"This paper presents a comprehensive framework to enhance Human-Robot Collaboration (HRC) in real-world scenarios. It introduces a formalism to model articulated tasks, requiring cooperation between two agents, through a smaller set of primitives. Our implementation leverages Hierarchical Task Networks (HTN) planning and a modular multisensory perception pipeline, which includes vision, human activity recognition, and tactile sensing. To showcase the system's scalability, we present an experimental scenario where two humans alternate in collaborating with a Baxter robot to assemble four pieces of furniture with variable components. This integration highlights promising advancements in HRC, suggesting a scalable approach for complex, cooperative tasks across diverse applications.","sentences":["This paper presents a comprehensive framework to enhance Human-Robot Collaboration (HRC) in real-world scenarios.","It introduces a formalism to model articulated tasks, requiring cooperation between two agents, through a smaller set of primitives.","Our implementation leverages Hierarchical Task Networks (HTN) planning and a modular multisensory perception pipeline, which includes vision, human activity recognition, and tactile sensing.","To showcase the system's scalability, we present an experimental scenario where two humans alternate in collaborating with a Baxter robot to assemble four pieces of furniture with variable components.","This integration highlights promising advancements in HRC, suggesting a scalable approach for complex, cooperative tasks across diverse applications."],"url":"http://arxiv.org/abs/2406.04907v1","category":"cs.RO"}
{"created":"2024-06-07 12:58:07","title":"On a higher-dimensional worm domain and its geometric properties","abstract":"We construct new $3$-dimensional variants of the classical Diederich-Fornaess worm domain. We show that they are smoothly bounded, pseudoconvex, and have nontrivial Nebenh\\\"{u}lle. We also show that their Bergman projections do not preserve the Sobolev space for sufficiently large Sobolev indices.","sentences":["We construct new $3$-dimensional variants of the classical Diederich-Fornaess worm domain.","We show that they are smoothly bounded, pseudoconvex, and have nontrivial Nebenh\\\"{u}lle.","We also show that their Bergman projections do not preserve the Sobolev space for sufficiently large Sobolev indices."],"url":"http://arxiv.org/abs/2406.04905v1","category":"math.CV"}
{"created":"2024-06-07 12:56:11","title":"XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model","abstract":"Most Zero-shot Multi-speaker TTS (ZS-TTS) systems support only a single language. Although models like YourTTS, VALL-E X, Mega-TTS 2, and Voicebox explored Multilingual ZS-TTS they are limited to just a few high/medium resource languages, limiting the applications of these models in most of the low/medium resource languages. In this paper, we aim to alleviate this issue by proposing and making publicly available the XTTS system. Our method builds upon the Tortoise model and adds several novel modifications to enable multilingual training, improve voice cloning, and enable faster training and inference. XTTS was trained in 16 languages and achieved state-of-the-art (SOTA) results in most of them.","sentences":["Most Zero-shot Multi-speaker TTS (ZS-TTS) systems support only a single language.","Although models like YourTTS, VALL-E X, Mega-TTS 2, and Voicebox explored Multilingual ZS-TTS they are limited to just a few high/medium resource languages, limiting the applications of these models in most of the low/medium resource languages.","In this paper, we aim to alleviate this issue by proposing and making publicly available the XTTS system.","Our method builds upon the Tortoise model and adds several novel modifications to enable multilingual training, improve voice cloning, and enable faster training and inference.","XTTS was trained in 16 languages and achieved state-of-the-art (SOTA) results in most of them."],"url":"http://arxiv.org/abs/2406.04904v1","category":"eess.AS"}
{"created":"2024-06-07 12:43:01","title":"Operator entanglement growth quantifies complexity of cellular automata","abstract":"Cellular automata (CA) exemplify systems where simple local interaction rules can lead to intricate and complex emergent phenomena at large scales. The various types of dynamical behavior of CA are usually categorized empirically into Wolfram's complexity classes. Here, we propose a quantitative measure, rooted in quantum information theory, to categorize the complexity of classical deterministic cellular automata. Specifically, we construct a Matrix Product Operator (MPO) of the transition matrix on the space of all possible CA configurations. We find that the growth of entropy of the singular value spectrum of the MPO reveals the complexity of the CA and can be used to characterize its dynamical behavior. This measure defines the concept of operator entanglement entropy for CA, demonstrating that quantum information measures can be meaningfully applied to classical deterministic systems.","sentences":["Cellular automata (CA) exemplify systems where simple local interaction rules can lead to intricate and complex emergent phenomena at large scales.","The various types of dynamical behavior of CA are usually categorized empirically into Wolfram's complexity classes.","Here, we propose a quantitative measure, rooted in quantum information theory, to categorize the complexity of classical deterministic cellular automata.","Specifically, we construct a Matrix Product Operator (MPO) of the transition matrix on the space of all possible CA configurations.","We find that the growth of entropy of the singular value spectrum of the MPO reveals the complexity of the CA and can be used to characterize its dynamical behavior.","This measure defines the concept of operator entanglement entropy for CA, demonstrating that quantum information measures can be meaningfully applied to classical deterministic systems."],"url":"http://arxiv.org/abs/2406.04895v1","category":"nlin.CG"}
{"created":"2024-06-07 12:34:46","title":"Ensuring Grid-Safe Forwarding of Distributed Flexibility in Sequential DSO-TSO Markets","abstract":"This paper investigates sequential flexibility markets consisting of a first market layer for distribution system operators (DSOs) to procure local flexibility to resolve their own needs (e.g., congestion management) followed by a second layer, in which the transmission system operator (TSO) procures remaining flexibility forwarded from the distribution system layer as well as flexibility from its own system for providing system services. As the TSO does not necessarily have full knowledge of the distribution grid constraints, this bid forwarding can cause an infeasibility problem for distribution systems, i.e., cleared distribution-level bids in the TSO layer might not satisfy local network constraints. To address this challenge, we introduce and examine three methods aiming to enable the grid-safe use of distribution-located resources in markets for system services, namely: a corrective three-layer market scheme, a bid prequalification/filtering method, and a bid aggregation method. Technically, we provide conditions under which these methods can produce a grid-safe use of distributed flexibility. We also characterize the efficiency of the market outcome under these methods. Finally, we carry out a representative case study to evaluate the performances of the three methods, focusing on economic efficiency, grid-safety, and computational load.","sentences":["This paper investigates sequential flexibility markets consisting of a first market layer for distribution system operators (DSOs) to procure local flexibility to resolve their own needs (e.g., congestion management) followed by a second layer, in which the transmission system operator (TSO) procures remaining flexibility forwarded from the distribution system layer as well as flexibility from its own system for providing system services.","As the TSO does not necessarily have full knowledge of the distribution grid constraints, this bid forwarding can cause an infeasibility problem for distribution systems, i.e., cleared distribution-level bids in the TSO layer might not satisfy local network constraints.","To address this challenge, we introduce and examine three methods aiming to enable the grid-safe use of distribution-located resources in markets for system services, namely: a corrective three-layer market scheme, a bid prequalification/filtering method, and a bid aggregation method.","Technically, we provide conditions under which these methods can produce a grid-safe use of distributed flexibility.","We also characterize the efficiency of the market outcome under these methods.","Finally, we carry out a representative case study to evaluate the performances of the three methods, focusing on economic efficiency, grid-safety, and computational load."],"url":"http://arxiv.org/abs/2406.04889v1","category":"math.OC"}
{"created":"2024-06-07 12:30:09","title":"Stability of stationary states for mean field models with multichromatic interaction potentials","abstract":"We consider weakly interacting diffusions on the torus, for multichromatic interaction potentials. We consider interaction potentials that are not H-stable, leading to phase transitions in the mean field limit. We show that the mean field dynamics can exhibit multipeak stationary states, where the number of peaks is related to the number of nonzero Fourier modes of the interaction. We also consider the effect of a confining potential on the structure of non-uniform steady states. We approach the problem by means of analysis, perturbation theory and numerical simulations for the interacting particle systems and the PDEs.","sentences":["We consider weakly interacting diffusions on the torus, for multichromatic interaction potentials.","We consider interaction potentials that are not H-stable, leading to phase transitions in the mean field limit.","We show that the mean field dynamics can exhibit multipeak stationary states, where the number of peaks is related to the number of nonzero Fourier modes of the interaction.","We also consider the effect of a confining potential on the structure of non-uniform steady states.","We approach the problem by means of analysis, perturbation theory and numerical simulations for the interacting particle systems and the PDEs."],"url":"http://arxiv.org/abs/2406.04884v1","category":"math-ph"}
{"created":"2024-06-07 12:28:05","title":"Upright to supine image registration and contour propagation for thoracic patients","abstract":"A renewed interest in upright therapy is currently driven by the availability of upright positioning and imaging systems. Aside from reduced cost, upright positioning possibly provides clinical advantages. The comparison between upright and supine particle therapy treatments can be biased through multiple variables, such as differences in the target contouring on the two CTs. We present a method for upright and supine CT registration and structures propagation, and the investigation of an AI-based contouring tool for upright images. Six paired 4DCTs from Proton Therapy Collaboration Group registry were available from the Northwestern Medicine Proton Centre. Deformable image registration (DIR) is challenged by the different patient anatomy between postures, causing artefacts in the warped images. To achieve high quality contour propagation, we propose the construction of a region of interest covering the ribcage volume to overcome this problem. As no target contour ground truth was available, the registration quality analysis (QA) was performed on lung structures, for which dice score coefficient (DSC) and average Hausdorff distance (AHD) is reported. The TotalSegmentator tool, trained on supine dataset, was applied on upright images, verified against lung structures and used as additional comparison for contour propagation. The TotalSegmentator QA results in a maximum AHD of 2mm and a minimum DSC of 0.94. An average AHD of 1.5mm and 1.6mm, and an average DSC of 0.95 and 0.94 were obtained comparing the propagated volumes to manually contoured and AI structures, respectively. All AHD values are smaller than the CT slice distances. The developed framework allows for target propagation between upright and supine images, defining the first step to compare upright and supine therapy of thoracic patients and enabling the application of image fusion techniques in the upright therapy field.","sentences":["A renewed interest in upright therapy is currently driven by the availability of upright positioning and imaging systems.","Aside from reduced cost, upright positioning possibly provides clinical advantages.","The comparison between upright and supine particle therapy treatments can be biased through multiple variables, such as differences in the target contouring on the two CTs.","We present a method for upright and supine CT registration and structures propagation, and the investigation of an AI-based contouring tool for upright images.","Six paired 4DCTs from Proton Therapy Collaboration Group registry were available from the Northwestern Medicine Proton Centre.","Deformable image registration (DIR) is challenged by the different patient anatomy between postures, causing artefacts in the warped images.","To achieve high quality contour propagation, we propose the construction of a region of interest covering the ribcage volume to overcome this problem.","As no target contour ground truth was available, the registration quality analysis (QA) was performed on lung structures, for which dice score coefficient (DSC) and average Hausdorff distance (AHD) is reported.","The TotalSegmentator tool, trained on supine dataset, was applied on upright images, verified against lung structures and used as additional comparison for contour propagation.","The TotalSegmentator QA results in a maximum AHD of 2mm and a minimum DSC of 0.94.","An average AHD of 1.5mm and 1.6mm, and an average DSC of 0.95 and 0.94 were obtained comparing the propagated volumes to manually contoured and AI structures, respectively.","All AHD values are smaller than the CT slice distances.","The developed framework allows for target propagation between upright and supine images, defining the first step to compare upright and supine therapy of thoracic patients and enabling the application of image fusion techniques in the upright therapy field."],"url":"http://arxiv.org/abs/2406.04883v1","category":"physics.med-ph"}
{"created":"2024-06-07 12:26:09","title":"MIMO Capacity Analysis and Channel Estimation for Electromagnetic Information Theory","abstract":"Electromagnetic information theory (EIT) is an interdisciplinary subject that serves to integrate deterministic electromagnetic theory with stochastic Shannon's information theory. Existing EIT analysis operates in the continuous space domain, which is not aligned with the practical algorithms working in the discrete space domain. This mismatch leads to a significant difficulty in application of EIT methodologies to practical discrete space systems, which is called as the discrete-continuous gap in this paper. To bridge this gap, we establish the discrete-continuous correspondence with a prolate spheroidal wave function (PSWF)-based ergodic capacity analysis framework. Specifically, we state and prove some discrete-continuous correspondence lemmas to establish a firm theoretical connection between discrete information-theoretic quantities to their continuous counterparts. With these lemmas, we apply the PSWF ergodic capacity bound to advanced MIMO architectures such as continuous-aperture MIMO (CAP-MIMO) and extremely large-scale MIMO (XL-MIMO). From this PSWF capacity bound, we discover the capacity saturation phenomenon both theoretically and empirically. Although the growth of MIMO performance is fundamentally limited in this EIT-based analysis framework, we reveal new opportunities in MIMO channel estimation by exploiting the EIT knowledge about the channel. Inspired by the PSWF capacity bound, we utilize continuous PSWFs to improve the pilot design of discrete MIMO channel estimators, which is called as the PSWF channel estimator (PSWF-CE). Simulation results demonstrate improved performances of the proposed PSWF-CE, compared to traditional minimum mean squared error (MMSE) and compressed sensing-based estimators.","sentences":["Electromagnetic information theory (EIT) is an interdisciplinary subject that serves to integrate deterministic electromagnetic theory with stochastic Shannon's information theory.","Existing EIT analysis operates in the continuous space domain, which is not aligned with the practical algorithms working in the discrete space domain.","This mismatch leads to a significant difficulty in application of EIT methodologies to practical discrete space systems, which is called as the discrete-continuous gap in this paper.","To bridge this gap, we establish the discrete-continuous correspondence with a prolate spheroidal wave function (PSWF)-based ergodic capacity analysis framework.","Specifically, we state and prove some discrete-continuous correspondence lemmas to establish a firm theoretical connection between discrete information-theoretic quantities to their continuous counterparts.","With these lemmas, we apply the PSWF ergodic capacity bound to advanced MIMO architectures such as continuous-aperture MIMO (CAP-MIMO) and extremely large-scale MIMO (XL-MIMO).","From this PSWF capacity bound, we discover the capacity saturation phenomenon both theoretically and empirically.","Although the growth of MIMO performance is fundamentally limited in this EIT-based analysis framework, we reveal new opportunities in MIMO channel estimation by exploiting the EIT knowledge about the channel.","Inspired by the PSWF capacity bound, we utilize continuous PSWFs to improve the pilot design of discrete MIMO channel estimators, which is called as the PSWF channel estimator (PSWF-CE).","Simulation results demonstrate improved performances of the proposed PSWF-CE, compared to traditional minimum mean squared error (MMSE) and compressed sensing-based estimators."],"url":"http://arxiv.org/abs/2406.04881v1","category":"cs.IT"}
{"created":"2024-06-07 12:10:42","title":"Mind Mansion: Exploring Metaphorical Interactions to Engage with Negative Thoughts in Virtual Reality","abstract":"Recurrent negative thoughts can significantly disrupt daily life and contribute to negative emotional states. Facing, confronting, and noticing such thoughts without support can be challenging. To provide a playful setting and leverage the technical maturation of Virtual Reality (VR), our VR experience, Mind Mansion, places the user in an initially cluttered virtual apartment. Here we utilize established concepts from traditional therapy and metaphors identified in prior works to let users engage metaphorically with representations of thoughts, gradually sorting the space, fostering awareness of thoughts, and supporting mental self-care. The results of our user study (n = 30) reveal that Mind Mansion encourages the exploration of alternative perspectives, fosters acceptance, and potentially offers new coping mechanisms. Our findings suggest that this VR intervention can reduce negative affect and improve overall emotional awareness.","sentences":["Recurrent negative thoughts can significantly disrupt daily life and contribute to negative emotional states.","Facing, confronting, and noticing such thoughts without support can be challenging.","To provide a playful setting and leverage the technical maturation of Virtual Reality (VR), our VR experience, Mind Mansion, places the user in an initially cluttered virtual apartment.","Here we utilize established concepts from traditional therapy and metaphors identified in prior works to let users engage metaphorically with representations of thoughts, gradually sorting the space, fostering awareness of thoughts, and supporting mental self-care.","The results of our user study (n = 30) reveal that Mind Mansion encourages the exploration of alternative perspectives, fosters acceptance, and potentially offers new coping mechanisms.","Our findings suggest that this VR intervention can reduce negative affect and improve overall emotional awareness."],"url":"http://arxiv.org/abs/2406.04871v1","category":"cs.HC"}
{"created":"2024-06-07 12:08:07","title":"Unraveling-induced entanglement phase transition in diffusive trajectories of continuously monitored noninteracting fermionic systems","abstract":"The competition between unitary quantum dynamics and dissipative stochastic effects, as emerging from continuous-monitoring processes, can culminate in measurement-induced phase transitions. Here, a many-body system abruptly passes, when exceeding a critical measurement rate, from a highly entangled phase to a low-entanglement one. We consider a different perspective on entanglement phase transitions and explore whether these can emerge when the measurement process itself is modified, while keeping the measurement rate fixed. To illustrate this idea, we consider a noninteracting fermionic system and focus on diffusive detection processes. Through extensive numerical simulations, we show that, upon varying a suitable \\textit{unraveling parameter} -- interpolating between measurements of different quadrature operators -- the system displays a transition from a phase with area-law entanglement to one where entanglement scales logarithmically with the system size. Our findings may be relevant for tailoring quantum correlations in noisy quantum devices and for conceiving optimal classical simulation strategies.","sentences":["The competition between unitary quantum dynamics and dissipative stochastic effects, as emerging from continuous-monitoring processes, can culminate in measurement-induced phase transitions.","Here, a many-body system abruptly passes, when exceeding a critical measurement rate, from a highly entangled phase to a low-entanglement one.","We consider a different perspective on entanglement phase transitions and explore whether these can emerge when the measurement process itself is modified, while keeping the measurement rate fixed.","To illustrate this idea, we consider a noninteracting fermionic system and focus on diffusive detection processes.","Through extensive numerical simulations, we show that, upon varying a suitable \\textit{unraveling parameter} -- interpolating between measurements of different quadrature operators -- the system displays a transition from a phase with area-law entanglement to one where entanglement scales logarithmically with the system size.","Our findings may be relevant for tailoring quantum correlations in noisy quantum devices and for conceiving optimal classical simulation strategies."],"url":"http://arxiv.org/abs/2406.04869v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-07 12:07:16","title":"Perturb-and-Project: Differentially Private Similarities and Marginals","abstract":"We revisit the input perturbations framework for differential privacy where noise is added to the input $A\\in \\mathcal{S}$ and the result is then projected back to the space of admissible datasets $\\mathcal{S}$. Through this framework, we first design novel efficient algorithms to privately release pair-wise cosine similarities. Second, we derive a novel algorithm to compute $k$-way marginal queries over $n$ features. Prior work could achieve comparable guarantees only for $k$ even. Furthermore, we extend our results to $t$-sparse datasets, where our efficient algorithms yields novel, stronger guarantees whenever $t\\le n^{5/6}/\\log n\\,.$ Finally, we provide a theoretical perspective on why \\textit{fast} input perturbation algorithms works well in practice. The key technical ingredients behind our results are tight sum-of-squares certificates upper bounding the Gaussian complexity of sets of solutions.","sentences":["We revisit the input perturbations framework for differential privacy where noise is added to the input $A\\in \\mathcal{S}$ and the result is then projected back to the space of admissible datasets $\\mathcal{S}$. Through this framework, we first design novel efficient algorithms to privately release pair-wise cosine similarities.","Second, we derive a novel algorithm to compute $k$-way marginal queries over $n$ features.","Prior work could achieve comparable guarantees only for $k$ even.","Furthermore, we extend our results to $t$-sparse datasets, where our efficient algorithms yields novel, stronger guarantees whenever $t\\le n^{5/6}/\\log n\\,.$ Finally, we provide a theoretical perspective on why \\textit{fast} input perturbation algorithms works well in practice.","The key technical ingredients behind our results are tight sum-of-squares certificates upper bounding the Gaussian complexity of sets of solutions."],"url":"http://arxiv.org/abs/2406.04868v1","category":"cs.LG"}
{"created":"2024-06-07 12:01:59","title":"ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering","abstract":"We introduce ComplexTempQA,a large-scale dataset consisting of over 100 million question-answer pairs designed to tackle the challenges in temporal question answering. ComplexTempQA significantly surpasses existing benchmarks like HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data from Wikipedia and Wikidata, the dataset covers questions spanning over two decades and offers an unmatched breadth of topics. We introduce a unique taxonomy that categorizes questions as attributes, comparisons, and counting questions, each revolving around events, entities, and time periods. One standout feature of ComplexTempQA is the high complexity of its questions, which demand effective capabilities for answering such as across-time comparison, temporal aggregation, and multi-hop reasoning involving temporal event ordering and entity recognition. Additionally, each question is accompanied by detailed metadata, including specific time scopes, allowing for comprehensive evaluation and enhancement of the temporal reasoning abilities of large language models. ComplexTempQA serves both as a testing ground for developing sophisticated AI models and as a foundation for advancing research in question answering, information retrieval, and language understanding. Dataset and code are freely available at: https://github.com/DataScienceUIBK/ComplexTempQA.","sentences":["We introduce ComplexTempQA,a large-scale dataset consisting of over 100 million question-answer pairs designed to tackle the challenges in temporal question answering.","ComplexTempQA significantly surpasses existing benchmarks like HOTPOTQA, TORQUE, and TEQUILA in scale and scope.","Utilizing data from Wikipedia and Wikidata, the dataset covers questions spanning over two decades and offers an unmatched breadth of topics.","We introduce a unique taxonomy that categorizes questions as attributes, comparisons, and counting questions, each revolving around events, entities, and time periods.","One standout feature of ComplexTempQA is the high complexity of its questions, which demand effective capabilities for answering such as across-time comparison, temporal aggregation, and multi-hop reasoning involving temporal event ordering and entity recognition.","Additionally, each question is accompanied by detailed metadata, including specific time scopes, allowing for comprehensive evaluation and enhancement of the temporal reasoning abilities of large language models.","ComplexTempQA serves both as a testing ground for developing sophisticated AI models and as a foundation for advancing research in question answering, information retrieval, and language understanding.","Dataset and code are freely available at: https://github.com/DataScienceUIBK/ComplexTempQA."],"url":"http://arxiv.org/abs/2406.04866v1","category":"cs.CL"}
{"created":"2024-06-07 11:41:28","title":"Auto-Multilift: Distributed Learning and Control for Cooperative Load Transportation With Quadrotors","abstract":"Designing motion control and planning algorithms for multilift systems remains challenging due to the complexities of dynamics, collision avoidance, actuator limits, and scalability. Existing methods that use optimization and distributed techniques effectively address these constraints and scalability issues. However, they often require substantial manual tuning, leading to suboptimal performance. This paper proposes Auto-Multilift, a novel framework that automates the tuning of model predictive controllers (MPCs) for multilift systems. We model the MPC cost functions with deep neural networks (DNNs), enabling fast online adaptation to various scenarios. We develop a distributed policy gradient algorithm to train these DNNs efficiently in a closed-loop manner. Central to our algorithm is distributed sensitivity propagation, which parallelizes gradient computation across quadrotors, focusing on actual system state sensitivities relative to key MPC parameters. We also provide theoretical guarantees for the convergence of this algorithm. Extensive simulations show rapid convergence and favorable scalability to a large number of quadrotors. Our method outperforms a state-of-the-art open-loop MPC tuning approach by effectively learning adaptive MPCs from trajectory tracking errors and handling the unique dynamics couplings within the multilift system. Additionally, our framework can learn an adaptive reference for reconfigurating the system when traversing through multiple narrow slots.","sentences":["Designing motion control and planning algorithms for multilift systems remains challenging due to the complexities of dynamics, collision avoidance, actuator limits, and scalability.","Existing methods that use optimization and distributed techniques effectively address these constraints and scalability issues.","However, they often require substantial manual tuning, leading to suboptimal performance.","This paper proposes Auto-Multilift, a novel framework that automates the tuning of model predictive controllers (MPCs) for multilift systems.","We model the MPC cost functions with deep neural networks (DNNs), enabling fast online adaptation to various scenarios.","We develop a distributed policy gradient algorithm to train these DNNs efficiently in a closed-loop manner.","Central to our algorithm is distributed sensitivity propagation, which parallelizes gradient computation across quadrotors, focusing on actual system state sensitivities relative to key MPC parameters.","We also provide theoretical guarantees for the convergence of this algorithm.","Extensive simulations show rapid convergence and favorable scalability to a large number of quadrotors.","Our method outperforms a state-of-the-art open-loop MPC tuning approach by effectively learning adaptive MPCs from trajectory tracking errors and handling the unique dynamics couplings within the multilift system.","Additionally, our framework can learn an adaptive reference for reconfigurating the system when traversing through multiple narrow slots."],"url":"http://arxiv.org/abs/2406.04858v1","category":"cs.RO"}
{"created":"2024-06-07 11:40:54","title":"A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering","abstract":"We consider the semi-random graph model of [Makarychev, Makarychev and Vijayaraghavan, STOC'12], where, given a random bipartite graph with $\\alpha$ edges and an unknown bipartition $(A, B)$ of the vertex set, an adversary can add arbitrary edges inside each community and remove arbitrary edges from the cut $(A, B)$ (i.e. all adversarial changes are \\textit{monotone} with respect to the bipartition). For this model, a polynomial time algorithm is known to approximate the Balanced Cut problem up to value $O(\\alpha)$ [MMV'12] as long as the cut $(A, B)$ has size $\\Omega(\\alpha)$. However, it consists of slow subroutines requiring optimal solutions for logarithmically many semidefinite programs. We study the fine-grained complexity of the problem and present the first near-linear time algorithm that achieves similar performances to that of [MMV'12]. Our algorithm runs in time $O(|V(G)|^{1+o(1)} + |E(G)|^{1+o(1)})$ and finds a balanced cut of value $O(\\alpha)$. Our approach appears easily extendible to related problem, such as Sparsest Cut, and also yields an near-linear time $O(1)$-approximation to Dagupta's objective function for hierarchical clustering [Dasgupta, STOC'16] for the semi-random hierarchical stochastic block model inputs of [Cohen-Addad, Kanade, Mallmann-Trenn, Mathieu, JACM'19].","sentences":["We consider the semi-random graph model of [Makarychev, Makarychev and Vijayaraghavan, STOC'12], where, given a random bipartite graph with $\\alpha$ edges and an unknown bipartition $(A, B)$ of the vertex set, an adversary can add arbitrary edges inside each community and remove arbitrary edges from the cut $(A, B)$ (i.e. all adversarial changes are \\textit{monotone} with respect to the bipartition).","For this model, a polynomial time algorithm is known to approximate the Balanced Cut problem up to value $O(\\alpha)$","[MMV'12] as long as the cut $(A, B)$ has size $\\Omega(\\alpha)$. However, it consists of slow subroutines requiring optimal solutions for logarithmically many semidefinite programs.","We study the fine-grained complexity of the problem and present the first near-linear time algorithm that achieves similar performances to that of [MMV'12].","Our algorithm runs in time $O(|V(G)|^{1+o(1)} + |E(G)|^{1+o(1)})$ and finds a balanced cut of value $O(\\alpha)$. Our approach appears easily extendible to related problem, such as Sparsest Cut, and also yields an near-linear time $O(1)$-approximation to Dagupta's objective function for hierarchical clustering [Dasgupta, STOC'16] for the semi-random hierarchical stochastic block model inputs of [Cohen-Addad, Kanade, Mallmann-Trenn, Mathieu, JACM'19]."],"url":"http://arxiv.org/abs/2406.04857v1","category":"cs.DS"}
{"created":"2024-06-07 11:37:45","title":"Uncertainty Aware Learning for Language Model Alignment","abstract":"As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL in a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\\% on complex low-entropy tasks (i.e., MetaMath and GSM8K).","sentences":["As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges.","Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally.","This may lead to suboptimal data efficiency and model performance.","In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs).","We implement UAL in a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples.","Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis.","Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning.","Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\\% on complex low-entropy tasks (i.e., MetaMath and GSM8K)."],"url":"http://arxiv.org/abs/2406.04854v1","category":"cs.CL"}
{"created":"2024-06-07 11:35:15","title":"Time-Series JEPA for Predictive Remote Control under Capacity-Limited Networks","abstract":"In remote control systems, transmitting large data volumes (e.g. video feeds) from wireless sensors to faraway controllers is challenging when the uplink channel capacity is limited (e.g. RedCap devices or massive wireless sensor networks). Furthermore, the controllers often only need the information-rich components of the original data. To address this, we propose a Time-Series Joint Embedding Predictive Architecture (TS-JEPA) and a semantic actor trained through self-supervised learning. This approach harnesses TS-JEPA's semantic representation power and predictive capabilities by capturing spatio-temporal correlations in the source data. We leverage this to optimize uplink channel utilization, while the semantic actor calculates control commands directly from the encoded representations, rather than from the original data. We test our model through multiple parallel instances of the well-known inverted cart-pole scenario, where the approach is validated through the maximization of stability under constrained uplink channel capacity.","sentences":["In remote control systems, transmitting large data volumes (e.g. video feeds) from wireless sensors to faraway controllers is challenging when the uplink channel capacity is limited (e.g. RedCap devices or massive wireless sensor networks).","Furthermore, the controllers often only need the information-rich components of the original data.","To address this, we propose a Time-Series Joint Embedding Predictive Architecture (TS-JEPA) and a semantic actor trained through self-supervised learning.","This approach harnesses TS-JEPA's semantic representation power and predictive capabilities by capturing spatio-temporal correlations in the source data.","We leverage this to optimize uplink channel utilization, while the semantic actor calculates control commands directly from the encoded representations, rather than from the original data.","We test our model through multiple parallel instances of the well-known inverted cart-pole scenario, where the approach is validated through the maximization of stability under constrained uplink channel capacity."],"url":"http://arxiv.org/abs/2406.04853v1","category":"cs.IT"}
{"created":"2024-06-07 11:00:02","title":"A Remark on the Expressivity of Asynchronous TeamLTL and HyperLTL","abstract":"Linear temporal logic (LTL) is used in system verification to write formal specifications for reactive systems. However, some relevant properties, e.g. non-inference in information flow security, cannot be expressed in LTL. A class of such properties that has recently received ample attention is known as hyperproperties. There are two major streams in the research regarding capturing hyperproperties, namely hyperlogics, which extend LTL with trace quantifiers (HyperLTL), and logics that employ team semantics, extending truth to sets of traces. In this article we explore the relation between asynchronous LTL under set-based team semantics (TeamLTL) and HyperLTL. In particular we consider the extensions of TeamLTL with the Boolean disjunction and a fragment of the extension of TeamLTL with the Boolean negation, where the negation cannot occur in the left-hand side of the Until-operator or within the Global-operator. We show that TeamLTL extended with the Boolean disjunction is equi-expressive with the positive Boolean closure of HyperLTL restricted to one universal quantifier, while the left-downward closed fragment of TeamLTL extended with the Boolean negation is expressively equivalent with the Boolean closure of HyperLTL restricted to one universal quantifier.","sentences":["Linear temporal logic (LTL) is used in system verification to write formal specifications for reactive systems.","However, some relevant properties, e.g. non-inference in information flow security, cannot be expressed in LTL.","A class of such properties that has recently received ample attention is known as hyperproperties.","There are two major streams in the research regarding capturing hyperproperties, namely hyperlogics, which extend LTL with trace quantifiers (HyperLTL), and logics that employ team semantics, extending truth to sets of traces.","In this article we explore the relation between asynchronous LTL under set-based team semantics (TeamLTL) and HyperLTL.","In particular we consider the extensions of TeamLTL with the Boolean disjunction and a fragment of the extension of TeamLTL with the Boolean negation, where the negation cannot occur in the left-hand side of the Until-operator or within the Global-operator.","We show that TeamLTL extended with the Boolean disjunction is equi-expressive with the positive Boolean closure of HyperLTL restricted to one universal quantifier, while the left-downward closed fragment of TeamLTL extended with the Boolean negation is expressively equivalent with the Boolean closure of HyperLTL restricted to one universal quantifier."],"url":"http://arxiv.org/abs/2406.04833v1","category":"cs.LO"}
{"created":"2024-06-07 10:52:37","title":"QAGCF: Graph Collaborative Filtering for Q&A Recommendation","abstract":"Question and answer (Q&A) platforms usually recommend question-answer pairs to meet users' knowledge acquisition needs, unlike traditional recommendations that recommend only one item. This makes user behaviors more complex, and presents two challenges for Q&A recommendation, including: the collaborative information entanglement, which means user feedback is influenced by either the question or the answer; and the semantic information entanglement, where questions are correlated with their corresponding answers, and correlations also exist among different question-answer pairs. Traditional recommendation methods treat the question-answer pair as a whole or only consider the answer as a single item, which overlooks the two challenges and cannot effectively model user interests. To address these challenges, we introduce Question & Answer Graph Collaborative Filtering (QAGCF), a graph neural network model that creates separate graphs for collaborative and semantic views to disentangle the information in question-answer pairs. The collaborative view disentangles questions and answers to individually model collaborative information, while the semantic view captures the semantic information both within and between question-answer pairs. These views are further merged into a global graph to integrate the collaborative and semantic information. Polynomial-based graph filters are used to address the high heterophily issues of the global graph. Additionally, contrastive learning is utilized to obtain robust embeddings during training. Extensive experiments on industrial and public datasets demonstrate that QAGCF consistently outperforms baselines and achieves state-of-the-art results.","sentences":["Question and answer (Q&A) platforms usually recommend question-answer pairs to meet users' knowledge acquisition needs, unlike traditional recommendations that recommend only one item.","This makes user behaviors more complex, and presents two challenges for Q&A recommendation, including: the collaborative information entanglement, which means user feedback is influenced by either the question or the answer; and the semantic information entanglement, where questions are correlated with their corresponding answers, and correlations also exist among different question-answer pairs.","Traditional recommendation methods treat the question-answer pair as a whole or only consider the answer as a single item, which overlooks the two challenges and cannot effectively model user interests.","To address these challenges, we introduce Question & Answer Graph Collaborative Filtering (QAGCF), a graph neural network model that creates separate graphs for collaborative and semantic views to disentangle the information in question-answer pairs.","The collaborative view disentangles questions and answers to individually model collaborative information, while the semantic view captures the semantic information both within and between question-answer pairs.","These views are further merged into a global graph to integrate the collaborative and semantic information.","Polynomial-based graph filters are used to address the high heterophily issues of the global graph.","Additionally, contrastive learning is utilized to obtain robust embeddings during training.","Extensive experiments on industrial and public datasets demonstrate that QAGCF consistently outperforms baselines and achieves state-of-the-art results."],"url":"http://arxiv.org/abs/2406.04828v1","category":"cs.IR"}
{"created":"2024-06-07 10:52:08","title":"Quantum Computing for nonlinear differential equations and turbulence","abstract":"A large spectrum of problems in classical physics and engineering, such as turbulence, is governed by nonlinear differential equations, which typically require high-performance computing to be solved. Over the past decade, however, the growth of classical computing power has slowed down because the miniaturisation of chips has been approaching the atomic scale. This is marking an end to Moore's law, which calls for a new computing paradigm: Quantum computing is a prime candidate. In this paper, we offer a perspective on the current challenges that need to be overcome in order to use quantum computing for the simulation of nonlinear dynamics. We review and discuss progress in the development of both quantum algorithms for nonlinear equations and quantum hardware. We propose pairings between quantum algorithms for nonlinear equations and quantum hardware concepts. These avenues open new opportunities for the simulation of nonlinear systems and turbulence.","sentences":["A large spectrum of problems in classical physics and engineering, such as turbulence, is governed by nonlinear differential equations, which typically require high-performance computing to be solved.","Over the past decade, however, the growth of classical computing power has slowed down because the miniaturisation of chips has been approaching the atomic scale.","This is marking an end to Moore's law, which calls for a new computing paradigm:","Quantum computing is a prime candidate.","In this paper, we offer a perspective on the current challenges that need to be overcome in order to use quantum computing for the simulation of nonlinear dynamics.","We review and discuss progress in the development of both quantum algorithms for nonlinear equations and quantum hardware.","We propose pairings between quantum algorithms for nonlinear equations and quantum hardware concepts.","These avenues open new opportunities for the simulation of nonlinear systems and turbulence."],"url":"http://arxiv.org/abs/2406.04826v1","category":"physics.flu-dyn"}
{"created":"2024-06-07 10:47:40","title":"M2NO: Multiresolution Operator Learning with Multiwavelet-based Algebraic Multigrid Method","abstract":"Solving partial differential equations (PDEs) effectively necessitates a multi-scale approach, particularly critical in high-dimensional scenarios characterized by increasing grid points or resolution. Traditional methods often fail to capture the detailed features necessary for accurate modeling, presenting a significant challenge in scientific computing. In response, we introduce the Multiwavelet-based Algebraic Multigrid Neural Operator (M2NO), a novel deep learning framework that synergistically combines multiwavelet transformations and algebraic multigrid (AMG) techniques. By exploiting the inherent similarities between these two approaches, M2NO overcomes their individual limitations and enhances precision and flexibility across various PDE benchmarks. Employing Multiresolution Analysis (MRA) with high-pass and low-pass filters, the model executes hierarchical decomposition to accurately delineate both global trends and localized details within PDE solutions, supporting adaptive data representation at multiple scales. M2NO also automates node selection and adeptly manages complex boundary conditions through its multiwavelet-based operators. Extensive evaluations on a diverse array of PDE datasets with different boundary conditions confirm M2NO's superior performance. Furthermore, M2NO excels in handling high-resolution and super-resolution tasks, consistently outperforming competing models and demonstrating robust adaptability in complex computational scenarios.","sentences":["Solving partial differential equations (PDEs) effectively necessitates a multi-scale approach, particularly critical in high-dimensional scenarios characterized by increasing grid points or resolution.","Traditional methods often fail to capture the detailed features necessary for accurate modeling, presenting a significant challenge in scientific computing.","In response, we introduce the Multiwavelet-based Algebraic Multigrid Neural Operator (M2NO), a novel deep learning framework that synergistically combines multiwavelet transformations and algebraic multigrid (AMG) techniques.","By exploiting the inherent similarities between these two approaches, M2NO overcomes their individual limitations and enhances precision and flexibility across various PDE benchmarks.","Employing Multiresolution Analysis (MRA) with high-pass and low-pass filters, the model executes hierarchical decomposition to accurately delineate both global trends and localized details within PDE solutions, supporting adaptive data representation at multiple scales.","M2NO also automates node selection and adeptly manages complex boundary conditions through its multiwavelet-based operators.","Extensive evaluations on a diverse array of PDE datasets with different boundary conditions confirm M2NO's superior performance.","Furthermore, M2NO excels in handling high-resolution and super-resolution tasks, consistently outperforming competing models and demonstrating robust adaptability in complex computational scenarios."],"url":"http://arxiv.org/abs/2406.04822v1","category":"cs.LG"}
{"created":"2024-06-07 10:38:32","title":"A short review on graphonometric evaluation tools in children","abstract":"Handwriting is a complex task that involves the coordination of motor, perceptual and cognitive skills. It is a fundamental skill for the cognitive and academic development of children. However, the technological, and educational changes in recent decades have affected both the teaching and assessment of handwriting. This paper presents a literature review of handwriting analysis in children, including a bibliometric analysis of published articles, the study participants, and the methods of evaluating the graphonometric state of children. The aim is to synthesize the state of the art and provide an overview of the main study trends over the last decade. The review concludes that handwriting remains a fundamental tool for early estimation of cognitive problems and early intervention. The article analyzes graphonometric evaluation tools. Likewise, it reflects on the importance of graphonometric evaluation as a means to detect possible difficulties or disorders in learning to write. The article concludes by highlighting the need to agree on an evaluation methodology and to combine databases.","sentences":["Handwriting is a complex task that involves the coordination of motor, perceptual and cognitive skills.","It is a fundamental skill for the cognitive and academic development of children.","However, the technological, and educational changes in recent decades have affected both the teaching and assessment of handwriting.","This paper presents a literature review of handwriting analysis in children, including a bibliometric analysis of published articles, the study participants, and the methods of evaluating the graphonometric state of children.","The aim is to synthesize the state of the art and provide an overview of the main study trends over the last decade.","The review concludes that handwriting remains a fundamental tool for early estimation of cognitive problems and early intervention.","The article analyzes graphonometric evaluation tools.","Likewise, it reflects on the importance of graphonometric evaluation as a means to detect possible difficulties or disorders in learning to write.","The article concludes by highlighting the need to agree on an evaluation methodology and to combine databases."],"url":"http://arxiv.org/abs/2406.04818v1","category":"cs.CV"}
{"created":"2024-06-07 10:36:05","title":"Optimal path planning and weighted control of a four-arm robot in on-orbit servicing","abstract":"This paper presents a trajectory optimization and control approach for the guidance of an orbital four-arm robot in extravehicular activities. The robot operates near the target spacecraft, enabling its arm's end-effectors to reach the spacecraft's surface. Connections to the target spacecraft can be established by the arms through specific footholds (docking devices). The trajectory optimization allows the robot path planning by computing the docking positions on the target spacecraft surface, along with their timing, the arm trajectories, the six degrees of freedom body motion, and the necessary contact forces during docking. In addition, the paper introduces a controller designed to track the planned trajectories derived from the solution of the nonlinear programming problem. A weighted controller formulated as a convex optimization problem is proposed. The controller is defined as the optimization of an objective function that allows the system to perform a set of tasks simultaneously. Simulation results show the application of the trajectory optimization and control approaches to an on-orbit servicing scenario.","sentences":["This paper presents a trajectory optimization and control approach for the guidance of an orbital four-arm robot in extravehicular activities.","The robot operates near the target spacecraft, enabling its arm's end-effectors to reach the spacecraft's surface.","Connections to the target spacecraft can be established by the arms through specific footholds (docking devices).","The trajectory optimization allows the robot path planning by computing the docking positions on the target spacecraft surface, along with their timing, the arm trajectories, the six degrees of freedom body motion, and the necessary contact forces during docking.","In addition, the paper introduces a controller designed to track the planned trajectories derived from the solution of the nonlinear programming problem.","A weighted controller formulated as a convex optimization problem is proposed.","The controller is defined as the optimization of an objective function that allows the system to perform a set of tasks simultaneously.","Simulation results show the application of the trajectory optimization and control approaches to an on-orbit servicing scenario."],"url":"http://arxiv.org/abs/2406.04816v1","category":"cs.RO"}
{"created":"2024-06-07 10:22:31","title":"Fully screened two-dimensional magnetoplasmons and rotational gravity shallow water waves in a rectangle","abstract":"We study plasmons in a rectangular two-dimensional (2D) electron system in the vicinity of a planar metal electrode (gate) and in the presence of a perpendicular uniform magnetic field, using Maxwell's equations and neglecting retardation effects. The conductivity of the 2D system is characterized by the dynamical Drude model without taking collisional relaxation into account, which well describes both high mobility graphene and other field effect transistor structures, including quantum wells like Ga(Al)As, in the terahertz and in some cases sub-terahertz frequency ranges. Without a magnetic field, we analytically find the current distribution and frequency of plasma eigenmodes when the plasmon wavelength is much larger than the distance to the gate, i.e. in the fully screened limit. To find an approximate solution in a magnetic field, we expand current in the complete set of eigenmodes without magnetic fields. Analytical asymptotic expressions in weak and strong magnetic fields were obtained for the lower modes. Unlike the disk and stripe, the frequencies of these modes tend to zero as the magnetic field tends to infinity. We also discuss a direct analogy to rotational gravity shallow water waves, where size-quantized Poincare waves correspond to size-quantized magnetoplasmons, while Kelvin waves correspond to edge magnetoplasmons.","sentences":["We study plasmons in a rectangular two-dimensional (2D) electron system in the vicinity of a planar metal electrode (gate) and in the presence of a perpendicular uniform magnetic field, using Maxwell's equations and neglecting retardation effects.","The conductivity of the 2D system is characterized by the dynamical Drude model without taking collisional relaxation into account, which well describes both high mobility graphene and other field effect transistor structures, including quantum wells like Ga(Al)As, in the terahertz and in some cases sub-terahertz frequency ranges.","Without a magnetic field, we analytically find the current distribution and frequency of plasma eigenmodes when the plasmon wavelength is much larger than the distance to the gate, i.e. in the fully screened limit.","To find an approximate solution in a magnetic field, we expand current in the complete set of eigenmodes without magnetic fields.","Analytical asymptotic expressions in weak and strong magnetic fields were obtained for the lower modes.","Unlike the disk and stripe, the frequencies of these modes tend to zero as the magnetic field tends to infinity.","We also discuss a direct analogy to rotational gravity shallow water waves, where size-quantized Poincare waves correspond to size-quantized magnetoplasmons, while Kelvin waves correspond to edge magnetoplasmons."],"url":"http://arxiv.org/abs/2406.04807v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-07 09:56:11","title":"Lewy curves in para-CR geometry","abstract":"We define a class of curves, referred to as Lewy curves, in para-CR geometry, following H. Lewy's original definition in CR geometry. We give a characterization of path geometries defined by para-CR Lewy curves. In dimension 3 our characterization is given by a set of necessary and sufficient conditions which, with the exception of one condition, are easily computationally verifiable. Furthermore, we show that Lewy curves of a para-CR 3-manifold coincide with chains of some (para-)CR 3-manifold if and only if it is flat. Subsequently, it follows that Lewy curves determine the para-CR structure up to the sign of the almost para-complex structure. In higher dimensions we show that para-CR Lewy curves define a path geometry if and only if the para-CR structure is flat, in which case chains and Lewy curves coincide.","sentences":["We define a class of curves, referred to as Lewy curves, in para-CR geometry, following H. Lewy's original definition in CR geometry.","We give a characterization of path geometries defined by para-CR Lewy curves.","In dimension 3 our characterization is given by a set of necessary and sufficient conditions which, with the exception of one condition, are easily computationally verifiable.","Furthermore, we show that Lewy curves of a para-CR 3-manifold coincide with chains of some (para-)CR 3-manifold if and only if it is flat.","Subsequently, it follows that Lewy curves determine the para-CR structure up to the sign of the almost para-complex structure.","In higher dimensions we show that para-CR Lewy curves define a path geometry","if and only if the para-CR structure is flat, in which case chains and Lewy curves coincide."],"url":"http://arxiv.org/abs/2406.04798v1","category":"math.DG"}
{"created":"2024-06-07 09:38:38","title":"Speaker-Smoothed kNN Speaker Adaptation for End-to-End ASR","abstract":"Despite recent improvements in End-to-End Automatic Speech Recognition (E2E ASR) systems, the performance can degrade due to vocal characteristic mismatches between training and testing data, particularly with limited target speaker adaptation data. We propose a novel speaker adaptation approach Speaker-Smoothed kNN that leverages k-Nearest Neighbors (kNN) retrieval techniques to improve model output by finding correctly pronounced tokens from its pre-built datastore during the decoding phase. Moreover, we utilize x-vector to dynamically adjust kNN interpolation parameters for data sparsity issue. This approach was validated using KeSpeech and MagicData corpora under in-domain and all-domain settings. Our method consistently performs comparably to fine-tuning without the associated performance degradation during speaker changes. Furthermore, in the all-domain setting, our method achieves state-of-the-art results, reducing the CER in both single speaker and multi-speaker test scenarios.","sentences":["Despite recent improvements in End-to-End Automatic Speech Recognition (E2E ASR) systems, the performance can degrade due to vocal characteristic mismatches between training and testing data, particularly with limited target speaker adaptation data.","We propose a novel speaker adaptation approach Speaker-Smoothed kNN that leverages k-Nearest Neighbors (kNN) retrieval techniques to improve model output by finding correctly pronounced tokens from its pre-built datastore during the decoding phase.","Moreover, we utilize x-vector to dynamically adjust kNN interpolation parameters for data sparsity issue.","This approach was validated using KeSpeech and MagicData corpora under in-domain and all-domain settings.","Our method consistently performs comparably to fine-tuning without the associated performance degradation during speaker changes.","Furthermore, in the all-domain setting, our method achieves state-of-the-art results, reducing the CER in both single speaker and multi-speaker test scenarios."],"url":"http://arxiv.org/abs/2406.04791v1","category":"cs.SD"}
{"created":"2024-06-07 09:34:18","title":"Asymptotic Analysis of Near-Field Coupling in Massive MISO and Massive SIMO Systems","abstract":"This paper studies the receiver to transmitter antenna coupling in near-field communications with massive arrays. Although most works in the literature consider that it is negligible and approximate it by zero, there is no rigorous analysis on its relevance for practical systems. In this work, we leverage multiport communication theory to obtain conditions for the aforementioned approximation to be valid in MISO and SIMO systems. These conditions are then particularized for arrays with fixed inter-element spacing and arrays with fixed size.","sentences":["This paper studies the receiver to transmitter antenna coupling in near-field communications with massive arrays.","Although most works in the literature consider that it is negligible and approximate it by zero, there is no rigorous analysis on its relevance for practical systems.","In this work, we leverage multiport communication theory to obtain conditions for the aforementioned approximation to be valid in MISO and SIMO systems.","These conditions are then particularized for arrays with fixed inter-element spacing and arrays with fixed size."],"url":"http://arxiv.org/abs/2406.04786v1","category":"eess.SP"}
{"created":"2024-06-07 09:30:40","title":"Effects of nonlinearity on Anderson localization of surface gravity waves","abstract":"Anderson localization is a multiple-scattering phenomenon of linear waves propagating within a disordered medium. Discovered in the late 50s for electrons, it has since been observed experimentally with cold atoms and with classical waves (optics, microwaves, and acoustics), but whether wave localization is enhanced or weakened for nonlinear waves is a long-standing debate. Here, we show that the nonlinearity strengthens the localization of surface-gravity waves propagating in a canal with a random bottom. We also show experimentally how the localization length depends on the nonlinearity, which has never been reported previously with any type of wave. To do so, we use a full space-and-time-resolved wavefield measurement as well as numerical simulations. The effects of the disorder level and the system's finite size on localization are also reported. We also highlight the first experimental evidence of the macroscopic analog of Bloch's dispersion relation of linear hydrodynamic surface waves over periodic bathymetry.","sentences":["Anderson localization is a multiple-scattering phenomenon of linear waves propagating within a disordered medium.","Discovered in the late 50s for electrons, it has since been observed experimentally with cold atoms and with classical waves (optics, microwaves, and acoustics), but whether wave localization is enhanced or weakened for nonlinear waves is a long-standing debate.","Here, we show that the nonlinearity strengthens the localization of surface-gravity waves propagating in a canal with a random bottom.","We also show experimentally how the localization length depends on the nonlinearity, which has never been reported previously with any type of wave.","To do so, we use a full space-and-time-resolved wavefield measurement as well as numerical simulations.","The effects of the disorder level and the system's finite size on localization are also reported.","We also highlight the first experimental evidence of the macroscopic analog of Bloch's dispersion relation of linear hydrodynamic surface waves over periodic bathymetry."],"url":"http://arxiv.org/abs/2406.04782v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-07 09:28:58","title":"The RGB tip in the SDSS, PS1, JWST, NGRST and Euclid photometric systems. Calibration in optical passbands using Gaia DR3 synthetic photometry","abstract":"We use synthetic photometry from Gaia DR3 BP and RP spectra for a large selected sample of stars in the Large Magellanic Cloud (LMC) and Small Magellanic Cloud (SMC) to derive the magnitude of the Red Giant Branch (RGB) tip for these two galaxies in several passbands in various widely used optical photometric systems, including those of space missions that have not yet started operations. The RGB tip is estimated by fitting a well-motivated model to the RGB luminosity function (LF) within a fully Bayesian framework, allowing for a proper representation of the uncertainties of all the involved parameters and their correlations. Adopting the best available distance and interstellar extinction estimates we provide a calibration of the RGB tip as a standard candle for the following passbands: Johnson-Kron-Cousins I (mainly used for validation purposes), Hubble Space Telescope F814W, Sloan Digital Sky Survey i and z, PanSTARRS1 y, James Webb Space Telescope F090W, Nancy Grace Roman Space Telescope Z087, and Euclid I$_E$, with an accuracy of a few per cent, depending on the case. The trend of the absolute magnitude of the tip as a function of colour in the different passbands, beyond the range spanned by the LMC and SMC, as well as its dependency on age, is explored by means of theoretical models. These calibrations can be very helpful to obtain state-of-the-art RGB tip distance estimates to stellar systems in a very large range of distances directly from data in the natural photometric system of these surveys and/or missions, without recurring to photometric transformations. [abridged]","sentences":["We use synthetic photometry from Gaia DR3 BP and RP spectra for a large selected sample of stars in the Large Magellanic Cloud (LMC) and Small Magellanic Cloud (SMC) to derive the magnitude of the Red Giant Branch (RGB) tip for these two galaxies in several passbands in various widely used optical photometric systems, including those of space missions that have not yet started operations.","The RGB tip is estimated by fitting a well-motivated model to the RGB luminosity function (LF) within a fully Bayesian framework, allowing for a proper representation of the uncertainties of all the involved parameters and their correlations.","Adopting the best available distance and interstellar extinction estimates we provide a calibration of the RGB tip as a standard candle for the following passbands: Johnson-Kron-Cousins I (mainly used for validation purposes), Hubble Space Telescope F814W, Sloan Digital Sky Survey i and z, PanSTARRS1 y, James Webb Space Telescope F090W, Nancy Grace Roman Space Telescope Z087, and Euclid I$_E$, with an accuracy of a few per cent, depending on the case.","The trend of the absolute magnitude of the tip as a function of colour in the different passbands, beyond the range spanned by the LMC and SMC, as well as its dependency on age, is explored by means of theoretical models.","These calibrations can be very helpful to obtain state-of-the-art RGB tip distance estimates to stellar systems in a very large range of distances directly from data in the natural photometric system of these surveys and/or missions, without recurring to photometric transformations.","[abridged]"],"url":"http://arxiv.org/abs/2406.04781v1","category":"astro-ph.GA"}
{"created":"2024-06-07 09:28:05","title":"Compilation Quotient (CQ): A Metric for the Compilation Hardness of Programming Languages","abstract":"Today's programmers can choose from an exceptional range of programming languages, each with its own traits, purpose, and complexity. A key aspect of a language's complexity is how hard it is to compile programs in the language. While most programmers have an intuition about compilation hardness for different programming languages, no metric exists to quantify it. We introduce the compilation quotient (CQ), a metric to quantify the compilation hardness of compiled programming languages. The key idea is to measure the compilation success rates of programs sampled from context-free grammars. To this end, we fairly sample over 12 million programs in total. CQ ranges between 0 and 100, where 0 indicates that no programs compile, and 100 means that all programs compile. Our findings on 12 popular compiled programming languages show high variation in CQ. C has a CQ of 48.11, C++ has 0.60, Java has 0.27 and Haskell has 0.13. Strikingly, Rust's CQ is nearly 0, and for C, even a large fraction of very sizable programs compile. We believe CQ can help understand the differences of compiled programming languages better and help language designers.","sentences":["Today's programmers can choose from an exceptional range of programming languages, each with its own traits, purpose, and complexity.","A key aspect of a language's complexity is how hard it is to compile programs in the language.","While most programmers have an intuition about compilation hardness for different programming languages, no metric exists to quantify it.","We introduce the compilation quotient (CQ), a metric to quantify the compilation hardness of compiled programming languages.","The key idea is to measure the compilation success rates of programs sampled from context-free grammars.","To this end, we fairly sample over 12 million programs in total.","CQ ranges between 0 and 100, where 0 indicates that no programs compile, and 100 means that all programs compile.","Our findings on 12 popular compiled programming languages show high variation in CQ.","C has a CQ of 48.11, C++ has 0.60, Java has 0.27 and Haskell has 0.13.","Strikingly, Rust's CQ is nearly 0, and for C, even a large fraction of very sizable programs compile.","We believe CQ can help understand the differences of compiled programming languages better and help language designers."],"url":"http://arxiv.org/abs/2406.04778v1","category":"cs.PL"}
{"created":"2024-06-07 09:13:23","title":"Isotopies of complete minimal surfaces of finite total curvature","abstract":"Let $M$ be a Riemann surface biholomorphic to an affine algebraic curve. We show that the inclusion of the space $\\Re \\mathrm{NC}_*(M,\\mathbb{C}^n)$ of real parts of nonflat proper algebraic null immersions $M\\to\\mathbb{C}^n$, $n\\ge 3$, into the space $\\mathrm{CMI}_*(M,\\mathbb{R}^n)$ of complete nonflat conformal minimal immersions $M\\to\\mathbb{R}^n$ of finite total curvature is a weak homotopy equivalence. We also show that the $(1,0)$-differential $\\partial$, mapping $\\mathrm{CMI}_*(M,\\mathbb{R}^n)$ or $\\Re \\mathrm{NC}_*(M,\\mathbb{C}^n)$ to the space $\\mathscr{A}^1(M,\\mathbf{A})$ of algebraic $1$-forms on $M$ with values in the punctured null quadric $\\mathbf{A} \\subset \\mathbb{C}^n\\setminus\\{0\\}$, is a weak homotopy equivalence. Analogous results are obtained for proper algebraic immersions $M\\to\\mathbb{C}^n$, $n\\ge 2$, directed by a flexible or algebraically elliptic punctured cone in $\\mathbb{C}^n\\setminus\\{0\\}$.","sentences":["Let $M$ be a Riemann surface biholomorphic to an affine algebraic curve.","We show that the inclusion of the space $\\Re \\mathrm{NC}_*(M,\\mathbb{C}^n)$ of real parts of nonflat proper algebraic null immersions $M\\to\\mathbb{C}^n$, $n\\ge 3$, into the space $\\mathrm{CMI}_*(M,\\mathbb{R}^n)$ of complete nonflat conformal minimal immersions $M\\to\\mathbb{R}^n$ of finite total curvature is a weak homotopy equivalence.","We also show that the $(1,0)$-differential $\\partial$, mapping $\\mathrm{CMI}_*(M,\\mathbb{R}^n)$ or $\\Re \\mathrm{NC}_*(M,\\mathbb{C}^n)$ to the space $\\mathscr{A}^1(M,\\mathbf{A})$ of algebraic $1$-forms on $M$ with values in the punctured null quadric $\\mathbf{A} \\subset \\mathbb{C}^n\\setminus\\{0\\}$, is a weak homotopy equivalence.","Analogous results are obtained for proper algebraic immersions $M\\to\\mathbb{C}^n$, $n\\ge 2$, directed by a flexible or algebraically elliptic punctured cone in $\\mathbb{C}^n\\setminus\\{0\\}$."],"url":"http://arxiv.org/abs/2406.04767v1","category":"math.DG"}
{"created":"2024-06-07 09:06:40","title":"SMC++: Masked Learning of Unsupervised Video Semantic Compression","abstract":"Most video compression methods focus on human visual perception, neglecting semantic preservation. This leads to severe semantic loss during the compression, hampering downstream video analysis tasks. In this paper, we propose a Masked Video Modeling (MVM)-powered compression framework that particularly preserves video semantics, by jointly mining and compressing the semantics in a self-supervised manner. While MVM is proficient at learning generalizable semantics through the masked patch prediction task, it may also encode non-semantic information like trivial textural details, wasting bitcost and bringing semantic noises. To suppress this, we explicitly regularize the non-semantic entropy of the compressed video in the MVM token space. The proposed framework is instantiated as a simple Semantic-Mining-then-Compression (SMC) model. Furthermore, we extend SMC as an advanced SMC++ model from several aspects. First, we equip it with a masked motion prediction objective, leading to better temporal semantic learning ability. Second, we introduce a Transformer-based compression module, to improve the semantic compression efficacy. Considering that directly mining the complex redundancy among heterogeneous features in different coding stages is non-trivial, we introduce a compact blueprint semantic representation to align these features into a similar form, fully unleashing the power of the Transformer-based compression module. Extensive results demonstrate the proposed SMC and SMC++ models show remarkable superiority over previous traditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets. \\textit{Codes and model are available at: \\url{https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch}.","sentences":["Most video compression methods focus on human visual perception, neglecting semantic preservation.","This leads to severe semantic loss during the compression, hampering downstream video analysis tasks.","In this paper, we propose a Masked Video Modeling (MVM)-powered compression framework that particularly preserves video semantics, by jointly mining and compressing the semantics in a self-supervised manner.","While MVM is proficient at learning generalizable semantics through the masked patch prediction task, it may also encode non-semantic information like trivial textural details, wasting bitcost and bringing semantic noises.","To suppress this, we explicitly regularize the non-semantic entropy of the compressed video in the MVM token space.","The proposed framework is instantiated as a simple Semantic-Mining-then-Compression (SMC) model.","Furthermore, we extend SMC as an advanced SMC++ model from several aspects.","First, we equip it with a masked motion prediction objective, leading to better temporal semantic learning ability.","Second, we introduce a Transformer-based compression module, to improve the semantic compression efficacy.","Considering that directly mining the complex redundancy among heterogeneous features in different coding stages is non-trivial, we introduce a compact blueprint semantic representation to align these features into a similar form, fully unleashing the power of the Transformer-based compression module.","Extensive results demonstrate the proposed SMC and SMC++ models show remarkable superiority over previous traditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets.","\\textit{Codes and model are available at: \\url{https://github.com/tianyuan168326/VideoSemanticCompression-Pytorch}."],"url":"http://arxiv.org/abs/2406.04765v1","category":"cs.CV"}
{"created":"2024-06-07 08:54:01","title":"Global well-posedness and large time behavior for the Oldroyd-B model","abstract":"This paper studies the global well-posedness and optimal decay estimates to the Oldroyd-B model in $\\mathbb R^d$ ($d\\geq2$). By utilizing the special structure of this system, we give a simplified proof to the global existence of solutions for the case of initial data small in critical Besov spaces and non-small coupling parameters. Moreover, the optimal decay rates of the solutions under minimal small assumption on the initial data are established by fully making use of the effect of velocity dissipation and damping mechanism.","sentences":["This paper studies the global well-posedness and optimal decay estimates to the Oldroyd-B model in $\\mathbb R^d$ ($d\\geq2$).","By utilizing the special structure of this system, we give a simplified proof to the global existence of solutions for the case of initial data small in critical Besov spaces and non-small coupling parameters.","Moreover, the optimal decay rates of the solutions under minimal small assumption on the initial data are established by fully making use of the effect of velocity dissipation and damping mechanism."],"url":"http://arxiv.org/abs/2406.04754v1","category":"math.AP"}
{"created":"2024-06-07 08:47:59","title":"Throughput and Fairness Trade-off Balancing for UAV-Enabled Wireless Communication Systems","abstract":"Given the imperative of 6G networks' ubiquitous connectivity, along with the inherent mobility and cost-effectiveness of unmanned aerial vehicles (UAVs), UAVs play a critical role within 6G wireless networks. Despite advancements in enhancing the UAV-enabled communication systems' throughput in existing studies, there remains a notable gap in addressing issues concerning user fairness and quality-of-service (QoS) provisioning and lacks an effective scheme to depict the trade-off between system throughput and user fairness. To solve the above challenges, in this paper we introduce a novel fairness control scheme for UAV-enabled wireless communication systems based on a new weighted function. First, we propose a throughput combining model based on a new weighted function with fairness considering. Second, we formulate the optimization problem to maximize the weighted sum of all users' throughput. Third, we decompose the optimization problem and propose an efficient iterative algorithm to solve it. Finally, simulation results are provided to demonstrate the considerable potential of our proposed scheme in fairness and QoS provisioning.","sentences":["Given the imperative of 6G networks' ubiquitous connectivity, along with the inherent mobility and cost-effectiveness of unmanned aerial vehicles (UAVs), UAVs play a critical role within 6G wireless networks.","Despite advancements in enhancing the UAV-enabled communication systems' throughput in existing studies, there remains a notable gap in addressing issues concerning user fairness and quality-of-service (QoS) provisioning and lacks an effective scheme to depict the trade-off between system throughput and user fairness.","To solve the above challenges, in this paper we introduce a novel fairness control scheme for UAV-enabled wireless communication systems based on a new weighted function.","First, we propose a throughput combining model based on a new weighted function with fairness considering.","Second, we formulate the optimization problem to maximize the weighted sum of all users' throughput.","Third, we decompose the optimization problem and propose an efficient iterative algorithm to solve it.","Finally, simulation results are provided to demonstrate the considerable potential of our proposed scheme in fairness and QoS provisioning."],"url":"http://arxiv.org/abs/2406.04750v1","category":"cs.IT"}
{"created":"2024-06-07 08:46:32","title":"Approximated Coded Computing: Towards Fast, Private and Secure Distributed Machine Learning","abstract":"In a large-scale distributed machine learning system, coded computing has attracted wide-spread attention since it can effectively alleviate the impact of stragglers. However, several emerging problems greatly limit the performance of coded distributed systems. Firstly, an existence of colluding workers who collude results with each other leads to serious privacy leakage issues. Secondly, there are few existing works considering security issues in data transmission of distributed computing systems. Thirdly, the number of required results for which need to wait increases with the degree of decoding functions. In this paper, we design a secure and private approximated coded distributed computing (SPACDC) scheme that deals with the above-mentioned problems simultaneously. Our SPACDC scheme guarantees data security during the transmission process using a new encryption algorithm based on elliptic curve cryptography. Especially, the SPACDC scheme does not impose strict constraints on the minimum number of results required to be waited for. An extensive performance analysis is conducted to demonstrate the effectiveness of our SPACDC scheme. Furthermore, we present a secure and private distributed learning algorithm based on the SPACDC scheme, which can provide information-theoretic privacy protection for training data. Our experiments show that the SPACDC-based deep learning algorithm achieves a significant speedup over the baseline approaches.","sentences":["In a large-scale distributed machine learning system, coded computing has attracted wide-spread attention since it can effectively alleviate the impact of stragglers.","However, several emerging problems greatly limit the performance of coded distributed systems.","Firstly, an existence of colluding workers who collude results with each other leads to serious privacy leakage issues.","Secondly, there are few existing works considering security issues in data transmission of distributed computing systems.","Thirdly, the number of required results for which need to wait increases with the degree of decoding functions.","In this paper, we design a secure and private approximated coded distributed computing (SPACDC) scheme that deals with the above-mentioned problems simultaneously.","Our SPACDC scheme guarantees data security during the transmission process using a new encryption algorithm based on elliptic curve cryptography.","Especially, the SPACDC scheme does not impose strict constraints on the minimum number of results required to be waited for.","An extensive performance analysis is conducted to demonstrate the effectiveness of our SPACDC scheme.","Furthermore, we present a secure and private distributed learning algorithm based on the SPACDC scheme, which can provide information-theoretic privacy protection for training data.","Our experiments show that the SPACDC-based deep learning algorithm achieves a significant speedup over the baseline approaches."],"url":"http://arxiv.org/abs/2406.04747v1","category":"cs.DC"}
{"created":"2024-06-07 08:32:30","title":"Built-in Bernal gap in large-angle-twisted monolayer-bilayer graphene","abstract":"Atomically thin materials offer multiple opportunities for layer-by-layer control of their electronic properties. While monolayer graphene (MLG) is a zero-gap system, Bernal-stacked bilayer graphene (BLG) acquires a finite band gap when the symmetry between the layers' potential energy is broken, usually, via a large electric field applied in double-gate devices. Here, we introduce an asymmetric twistronic stack comprising both MLG and BLG, synthesized via low-pressure chemical vapor deposition (LP-CVD) on Cu. Although a large ($\\sim30^{\\circ}$) twist angle decouples the MLG and BLG electronic bands near Fermi level, we find that the layer degeneracy in the BLG subsystem is lifted, producing a gap in the absence of external fields. The built-in interlayer asymmetry originates from proximity-induced energy shifts in the outermost layers and requires a displacement field of $0.14$ V/nm to be compensated. The latter corresponds to a $\\sim10$ meV intrinsic BLG gap, a value confirmed by our thermal-activation measurements. The present results highlight the role of structural asymmetry and encapsulating environment, expanding the engineering toolbox for monolithically-grown graphene multilayers.","sentences":["Atomically thin materials offer multiple opportunities for layer-by-layer control of their electronic properties.","While monolayer graphene (MLG) is a zero-gap system, Bernal-stacked bilayer graphene (BLG) acquires a finite band gap when the symmetry between the layers' potential energy is broken, usually, via a large electric field applied in double-gate devices.","Here, we introduce an asymmetric twistronic stack comprising both MLG and BLG, synthesized via low-pressure chemical vapor deposition (LP-CVD) on Cu.","Although a large ($\\sim30^{\\circ}$) twist angle decouples the MLG and BLG electronic bands near Fermi level, we find that the layer degeneracy in the BLG subsystem is lifted, producing a gap in the absence of external fields.","The built-in interlayer asymmetry originates from proximity-induced energy shifts in the outermost layers and requires a displacement field of $0.14$ V/nm to be compensated.","The latter corresponds to a $\\sim10$ meV intrinsic BLG gap, a value confirmed by our thermal-activation measurements.","The present results highlight the role of structural asymmetry and encapsulating environment, expanding the engineering toolbox for monolithically-grown graphene multilayers."],"url":"http://arxiv.org/abs/2406.04732v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-07 08:26:31","title":"Efficient Continual Finite-Sum Minimization","abstract":"Given a sequence of functions $f_1,\\ldots,f_n$ with $f_i:\\mathcal{D}\\mapsto \\mathbb{R}$, finite-sum minimization seeks a point ${x}^\\star \\in \\mathcal{D}$ minimizing $\\sum_{j=1}^n f_j(x)/n$. In this work, we propose a key twist into the finite-sum minimization, dubbed as continual finite-sum minimization, that asks for a sequence of points ${x}_1^\\star,\\ldots,{x}_n^\\star \\in \\mathcal{D}$ such that each ${x}^\\star_i \\in \\mathcal{D}$ minimizes the prefix-sum $\\sum_{j=1}^if_j(x)/i$. Assuming that each prefix-sum is strongly convex, we develop a first-order continual stochastic variance reduction gradient method ($\\mathrm{CSVRG}$) producing an $\\epsilon$-optimal sequence with $\\mathcal{\\tilde{O}}(n/\\epsilon^{1/3} + 1/\\sqrt{\\epsilon})$ overall first-order oracles (FO). An FO corresponds to the computation of a single gradient $\\nabla f_j(x)$ at a given $x \\in \\mathcal{D}$ for some $j \\in [n]$. Our approach significantly improves upon the $\\mathcal{O}(n/\\epsilon)$ FOs that $\\mathrm{StochasticGradientDescent}$ requires and the $\\mathcal{O}(n^2 \\log (1/\\epsilon))$ FOs that state-of-the-art variance reduction methods such as $\\mathrm{Katyusha}$ require. We also prove that there is no natural first-order method with $\\mathcal{O}\\left(n/\\epsilon^\\alpha\\right)$ gradient complexity for $\\alpha < 1/4$, establishing that the first-order complexity of our method is nearly tight.","sentences":["Given a sequence of functions $f_1,\\ldots,f_n$ with $f_i:\\mathcal{D}\\mapsto \\mathbb{R}$, finite-sum minimization seeks a point ${x}^\\star \\in \\mathcal{D}$ minimizing $\\sum_{j=1}^n f_j(x)/n$.","In this work, we propose a key twist into the finite-sum minimization, dubbed as continual finite-sum minimization, that asks for a sequence of points ${x}_1^\\star,\\ldots,{x}_n^\\star \\in \\mathcal{D}$ such that each ${x}^\\star_i \\in \\mathcal{D}$ minimizes the prefix-sum $\\sum_{j=1}^if_j(x)/i$. Assuming that each prefix-sum is strongly convex, we develop a first-order continual stochastic variance reduction gradient method ($\\mathrm{CSVRG}$) producing an $\\epsilon$-optimal sequence with $\\mathcal{\\tilde{O}}(n/\\epsilon^{1/3} + 1/\\sqrt{\\epsilon})$ overall first-order oracles (FO).","An FO corresponds to the computation of a single gradient $\\nabla f_j(x)$ at a given $x \\in \\mathcal{D}$ for some $j \\in","[n]$. Our approach significantly improves upon the $\\mathcal{O}(n/\\epsilon)$ FOs that $\\mathrm{StochasticGradientDescent}$ requires and the $\\mathcal{O}(n^2 \\log (1/\\epsilon))$ FOs that state-of-the-art variance reduction methods such as $\\mathrm{Katyusha}$ require.","We also prove that there is no natural first-order method with $\\mathcal{O}\\left(n/\\epsilon^\\alpha\\right)$ gradient complexity for $\\alpha < 1/4$, establishing that the first-order complexity of our method is nearly tight."],"url":"http://arxiv.org/abs/2406.04731v1","category":"math.OC"}
{"created":"2024-06-07 17:49:07","title":"The SRG/eROSITA All-Sky Survey: X-ray beacons at late cosmic dawn","abstract":"The SRG/eROSITA All-Sky Survey (eRASS) is expected to contain ~100 quasars that emitted their light when the universe was less than a billion years old, i.e. at z>5.6. By selection, these quasars populate the bright end of the AGN X-ray luminosity function and their count offers a powerful demographic diagnostic of the parent super-massive black hole population. Of the >~ 400 quasars that have been discovered at z>5.6 to date, less than 15 % have been X-ray detected. We present a pilot survey to uncover the elusive X-ray luminous end of the distant quasar population. We have designed a quasar selection pipeline based on optical, infrared and X-ray imaging data from DES DR2, VHS DR5, CatWISE2020 and the eRASS. The core selection method relies on SED template fitting. We performed optical follow-up spectroscopy with the Magellan/LDSS3 instrument for the redshift confirmation of a subset of candidates. We have further obtained a deeper X-ray image of one of our candidates with Chandra ACIS-S. We report the discovery of five new quasars in the redshift range 5.6 < z < 6.1. Two of these quasars are detected in eRASS and are by selection X-ray ultra-luminous. These quasars are also detected at radio frequencies. The first one is a broad absorption line quasar which shows significant X-ray dimming over 3.5 years, i.e. about 6 months in the quasar rest frame. The second radio-detected quasar is a jetted source with compact morphology. We show that a blazar configuration is likely for this source, making it the second most distant blazar known to date. With our pilot study, we demonstrate the power of eROSITA as a discovery machine for luminous quasars in the epoch of reionization. The X-ray emission of the two eROSITA detected quasars are likely to be driven by different high-energetic emission mechanisms a diversity which will be further explored in a future systematic full-hemisphere survey.","sentences":["The SRG/eROSITA All-Sky Survey (eRASS) is expected to contain ~100 quasars that emitted their light when the universe was less than a billion years old, i.e. at z>5.6.","By selection, these quasars populate the bright end of the AGN X-ray luminosity function and their count offers a powerful demographic diagnostic of the parent super-massive black hole population.","Of the >~ 400 quasars that have been discovered at z>5.6 to date, less than 15 % have been X-ray detected.","We present a pilot survey to uncover the elusive X-ray luminous end of the distant quasar population.","We have designed a quasar selection pipeline based on optical, infrared and X-ray imaging data from DES DR2, VHS DR5, CatWISE2020 and the eRASS.","The core selection method relies on SED template fitting.","We performed optical follow-up spectroscopy with the Magellan/LDSS3 instrument for the redshift confirmation of a subset of candidates.","We have further obtained a deeper X-ray image of one of our candidates with Chandra ACIS-S. We report the discovery of five new quasars in the redshift range 5.6 < z < 6.1.","Two of these quasars are detected in eRASS and are by selection X-ray ultra-luminous.","These quasars are also detected at radio frequencies.","The first one is a broad absorption line quasar which shows significant X-ray dimming over 3.5 years, i.e. about 6 months in the quasar rest frame.","The second radio-detected quasar is a jetted source with compact morphology.","We show that a blazar configuration is likely for this source, making it the second most distant blazar known to date.","With our pilot study, we demonstrate the power of eROSITA as a discovery machine for luminous quasars in the epoch of reionization.","The X-ray emission of the two eROSITA detected quasars are likely to be driven by different high-energetic emission mechanisms a diversity which will be further explored in a future systematic full-hemisphere survey."],"url":"http://arxiv.org/abs/2406.05118v1","category":"astro-ph.GA"}
{"created":"2024-06-07 17:23:37","title":"Cosmological parameters from the joint analysis of Density Split and Second Order Statistics: an Emulator based on the Halo Occupation Distribution","abstract":"In this work, we develop a simulation-based model to predict the density split (DSS) and second-order shear and clustering statistics. A simulation-based model has the potential to model highly non-linear scales where current DSS models fail. To build this model, we use the \\texttt{AbacusSummit} N-body simulation suite from which we measure all necessary statistics and train an emulator based on \\texttt{CosmoPower}. In that context, we discuss possible improvements for future emulators to make the measurement less noisy and biased, resulting in more accurate and precise model predictions.   Regarding the emulator's accuracy, we find that the most important aspect is the average of the summary statistics over multiple-shot noise realizations of the foreground galaxies. However, these results probably depend on the chosen number density of the foreground galaxies. Regarding the parameter forecast based on preliminary LOWZxUNIONS data, we find that DSS has more constraining power to derive cosmological parameters and that the joint analysis with second-order statistics is particularly useful for extracting parameters of the galaxy-halo connection.","sentences":["In this work, we develop a simulation-based model to predict the density split (DSS) and second-order shear and clustering statistics.","A simulation-based model has the potential to model highly non-linear scales where current DSS models fail.","To build this model, we use the \\texttt{AbacusSummit} N-body simulation suite from which we measure all necessary statistics and train an emulator based on \\texttt{CosmoPower}.","In that context, we discuss possible improvements for future emulators to make the measurement less noisy and biased, resulting in more accurate and precise model predictions.   ","Regarding the emulator's accuracy, we find that the most important aspect is the average of the summary statistics over multiple-shot noise realizations of the foreground galaxies.","However, these results probably depend on the chosen number density of the foreground galaxies.","Regarding the parameter forecast based on preliminary LOWZxUNIONS data, we find that DSS has more constraining power to derive cosmological parameters and that the joint analysis with second-order statistics is particularly useful for extracting parameters of the galaxy-halo connection."],"url":"http://arxiv.org/abs/2406.05098v1","category":"astro-ph.CO"}
{"created":"2024-06-07 17:12:40","title":"Comprehensive description of color centers by wave function theory: a CASSCF-NEVPT2 study of the NV defect in diamond","abstract":"Paramagnetic point defects in wide-bandgap semiconductors, characterized by atomic-like in-gap defect states, constitute a unique challenge for ab initio modeling. In this theoretical study, we aim to devise a wave-function only computational protocol, exemplified on the prominent nitrogen-vacancy (NV) center in diamond, which enables the full characterization of future quantum bit candidates implemented in color centers. We propose the application of the second order $n$-electron valence state perturbation theory on top of the complete active space self-consistent field approximation (CASSCF-NEVPT2) to provide a balanced ab initio level description of the correlation effects yielded from the defect orbitals and the embedding nanodiamond. By relaxing the molecular cluster under the compression of the surrounding bulk material, we manage to model both the vertical and relaxed experimental electronic spectra within an average error margin of 0.1 eV. Furthermore, the experimentally observed Jahn-Teller behavior of $^3E$ and $^1E$ states, the measured fine structure of the triplet electronic states, as well as the expected spin-selectivity are quantitatively reproduced by the presented methodology. Our findings showcase that using conventional wave-function-based quantum chemical approaches on carefully crafted cluster models can be a competing alternative for discussing the energetics of point defects in solids.","sentences":["Paramagnetic point defects in wide-bandgap semiconductors, characterized by atomic-like in-gap defect states, constitute a unique challenge for ab initio modeling.","In this theoretical study, we aim to devise a wave-function only computational protocol, exemplified on the prominent nitrogen-vacancy (NV) center in diamond, which enables the full characterization of future quantum bit candidates implemented in color centers.","We propose the application of the second order $n$-electron valence state perturbation theory on top of the complete active space self-consistent field approximation (CASSCF-NEVPT2) to provide a balanced ab initio level description of the correlation effects yielded from the defect orbitals and the embedding nanodiamond.","By relaxing the molecular cluster under the compression of the surrounding bulk material, we manage to model both the vertical and relaxed experimental electronic spectra within an average error margin of 0.1 eV. Furthermore, the experimentally observed Jahn-Teller behavior of $^3E$ and $^1E$ states, the measured fine structure of the triplet electronic states, as well as the expected spin-selectivity are quantitatively reproduced by the presented methodology.","Our findings showcase that using conventional wave-function-based quantum chemical approaches on carefully crafted cluster models can be a competing alternative for discussing the energetics of point defects in solids."],"url":"http://arxiv.org/abs/2406.05092v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 16:57:13","title":"On the $abc$ and the $abcd$ conjectures","abstract":"We revisit a subexponential bound for the $abc$ conjecture due to the first author, and we establish a variation of it using linear forms in logarithms. As an application, we prove an unconditional subexponential bound towards the $4$-terms $abc$ conjecture under a suitable hypothesis on the size of the variables.","sentences":["We revisit a subexponential bound for the $abc$ conjecture due to the first author, and we establish a variation of it using linear forms in logarithms.","As an application, we prove an unconditional subexponential bound towards the $4$-terms $abc$ conjecture under a suitable hypothesis on the size of the variables."],"url":"http://arxiv.org/abs/2406.05083v1","category":"math.NT"}
{"created":"2024-06-07 16:37:13","title":"Efficient Centroid-Linkage Clustering","abstract":"We give an efficient algorithm for Centroid-Linkage Hierarchical Agglomerative Clustering (HAC), which computes a $c$-approximate clustering in roughly $n^{1+O(1/c^2)}$ time. We obtain our result by combining a new Centroid-Linkage HAC algorithm with a novel fully dynamic data structure for nearest neighbor search which works under adaptive updates.   We also evaluate our algorithm empirically. By leveraging a state-of-the-art nearest-neighbor search library, we obtain a fast and accurate Centroid-Linkage HAC algorithm. Compared to an existing state-of-the-art exact baseline, our implementation maintains the clustering quality while delivering up to a $36\\times$ speedup due to performing fewer distance comparisons.","sentences":["We give an efficient algorithm for Centroid-Linkage Hierarchical Agglomerative Clustering (HAC), which computes a $c$-approximate clustering in roughly $n^{1+O(1/c^2)}$ time.","We obtain our result by combining a new Centroid-Linkage HAC algorithm with a novel fully dynamic data structure for nearest neighbor search which works under adaptive updates.   ","We also evaluate our algorithm empirically.","By leveraging a state-of-the-art nearest-neighbor search library, we obtain a fast and accurate Centroid-Linkage HAC algorithm.","Compared to an existing state-of-the-art exact baseline, our implementation maintains the clustering quality while delivering up to a $36\\times$ speedup due to performing fewer distance comparisons."],"url":"http://arxiv.org/abs/2406.05066v1","category":"cs.DS"}
{"created":"2024-06-07 16:33:30","title":"The wind-shade roughness model for turbulent wall-bounded flows","abstract":"To aid in prediction of turbulent boundary layer flows over rough surfaces, a new model is proposed to estimate hydrodynamic roughness based solely on geometric surface information. The model is based on a fluid-mechanics motivated geometric parameter called the wind-shade factor. Sheltering is included using a rapid algorithm adapted from the landscape shadow literature, while local pressure drag is estimated using a piecewise potential flow approximation. Similarly to evaluating traditional surface parameters such as skewness or average slope magnitude, the wind-shade factor is purely geometric and can be evaluated efficiently from knowing the surface elevation map and the mean flow direction. The wind-shade roughness model is applied to over 100 different surfaces available in a public roughness database and some others, and the predicted sandgrain-roughness heights are compared to measured values. Effects of various model ingredients are analyzed, and transitionally rough surfaces are treated by adding a term representing the viscous stress component.","sentences":["To aid in prediction of turbulent boundary layer flows over rough surfaces, a new model is proposed to estimate hydrodynamic roughness based solely on geometric surface information.","The model is based on a fluid-mechanics motivated geometric parameter called the wind-shade factor.","Sheltering is included using a rapid algorithm adapted from the landscape shadow literature, while local pressure drag is estimated using a piecewise potential flow approximation.","Similarly to evaluating traditional surface parameters such as skewness or average slope magnitude, the wind-shade factor is purely geometric and can be evaluated efficiently from knowing the surface elevation map and the mean flow direction.","The wind-shade roughness model is applied to over 100 different surfaces available in a public roughness database and some others, and the predicted sandgrain-roughness heights are compared to measured values.","Effects of various model ingredients are analyzed, and transitionally rough surfaces are treated by adding a term representing the viscous stress component."],"url":"http://arxiv.org/abs/2406.05062v1","category":"physics.flu-dyn"}
{"created":"2024-06-07 16:18:59","title":"The Dark Energy Survey Supernova Program: Investigating Beyond-$\u039b$CDM","abstract":"We report constraints on a variety of non-standard cosmological models using the full 5-year photometrically-classified type Ia supernova sample from the Dark Energy Survey (DES-SN5YR). Both Akaike Information Criterion (AIC) and Suspiciousness calculations find no strong evidence for or against any of the non-standard models we explore. When combined with external probes, the AIC and Suspiciousness agree that 11 of the 15 models are moderately preferred over Flat-$\\Lambda$CDM suggesting additional flexibility in our cosmological models may be required beyond the cosmological constant. We also provide a detailed discussion of all cosmological assumptions that appear in the DES supernova cosmology analyses, evaluate their impact, and provide guidance on using the DES Hubble diagram to test non-standard models. An approximate cosmological model, used to perform bias corrections to the data holds the biggest potential for harbouring cosmological assumptions. We show that even if the approximate cosmological model is constructed with a matter density shifted by $\\Delta\\Omega_m\\sim0.2$ from the true matter density of a simulated data set the bias that arises is sub-dominant to statistical uncertainties. Nevertheless, we present and validate a methodology to reduce this bias.","sentences":["We report constraints on a variety of non-standard cosmological models using the full 5-year photometrically-classified type Ia supernova sample from the Dark Energy Survey (DES-SN5YR).","Both Akaike Information Criterion (AIC) and Suspiciousness calculations find no strong evidence for or against any of the non-standard models we explore.","When combined with external probes, the AIC and Suspiciousness agree that 11 of the 15 models are moderately preferred over Flat-$\\Lambda$CDM suggesting additional flexibility in our cosmological models may be required beyond the cosmological constant.","We also provide a detailed discussion of all cosmological assumptions that appear in the DES supernova cosmology analyses, evaluate their impact, and provide guidance on using the DES Hubble diagram to test non-standard models.","An approximate cosmological model, used to perform bias corrections to the data holds the biggest potential for harbouring cosmological assumptions.","We show that even if the approximate cosmological model is constructed with a matter density shifted by $\\Delta\\Omega_m\\sim0.2$ from the true matter density of a simulated data set the bias that arises is sub-dominant to statistical uncertainties.","Nevertheless, we present and validate a methodology to reduce this bias."],"url":"http://arxiv.org/abs/2406.05048v1","category":"astro-ph.CO"}
{"created":"2024-06-07 16:18:58","title":"The Dark Energy Survey : Detection of weak lensing magnification of supernovae and constraints on dark matter haloes","abstract":"The residuals of the distance moduli of Type Ia supernovae (SN Ia) relative to a Hubble diagram fit contain information about the inhomogeneity of the universe, due to weak lensing magnification by foreground matter. By correlating the residuals of the Dark Energy Survey Year 5 SN Ia sample (DES-SN5YR) with extra-galactic foregrounds from the DES Y3 Gold catalog, we detect the presence of lensing at $6.0 \\sigma$ significance. This is the first detection with a significance level above $5\\sigma$. Constraints on the effective mass-to-light ratios and radial profiles of dark-matter haloes surrounding individual galaxies are also obtained. We show that the scatter of SNe Ia around the Hubble diagram is reduced by modifying the standardisation of the distance moduli to include an easily calculable de-lensing (i.e., environmental) term. We use the de-lensed distance moduli to recompute cosmological parameters derived from SN Ia, finding in Flat $w$CDM a difference of $\\Delta \\Omega_{\\rm M} = +0.036$ and $\\Delta w = -0.056$ compared to the unmodified distance moduli, a change of $\\sim 0.3\\sigma$. We argue that our modelling of SN Ia lensing will lower systematics on future surveys with higher statistical power. We use the observed dispersion of lensing in DES-SN5YR to constrain $\\sigma_8$, but caution that the fit is sensitive to uncertainties at small scales. Nevertheless, our detection of SN Ia lensing opens a new pathway to study matter inhomogeneity that complements galaxy-galaxy lensing surveys and has unrelated systematics.","sentences":["The residuals of the distance moduli of Type Ia supernovae (SN Ia) relative to a Hubble diagram fit contain information about the inhomogeneity of the universe, due to weak lensing magnification by foreground matter.","By correlating the residuals of the Dark Energy Survey Year 5 SN Ia sample (DES-SN5YR) with extra-galactic foregrounds from the DES Y3 Gold catalog, we detect the presence of lensing at $6.0 \\sigma$ significance.","This is the first detection with a significance level above $5\\sigma$. Constraints on the effective mass-to-light ratios and radial profiles of dark-matter haloes surrounding individual galaxies are also obtained.","We show that the scatter of SNe Ia around the Hubble diagram is reduced by modifying the standardisation of the distance moduli to include an easily calculable de-lensing (i.e., environmental) term.","We use the de-lensed distance moduli to recompute cosmological parameters derived from SN Ia, finding in Flat $w$CDM a difference of $\\Delta \\Omega_{\\rm M} = +0.036$ and $\\Delta w = -0.056$ compared to the unmodified distance moduli, a change of $\\sim 0.3\\sigma$.","We argue that our modelling of SN Ia lensing will lower systematics on future surveys with higher statistical power.","We use the observed dispersion of lensing in DES-SN5YR to constrain $\\sigma_8$, but caution that the fit is sensitive to uncertainties at small scales.","Nevertheless, our detection of SN Ia lensing opens a new pathway to study matter inhomogeneity that complements galaxy-galaxy lensing surveys and has unrelated systematics."],"url":"http://arxiv.org/abs/2406.05047v1","category":"astro-ph.CO"}
{"created":"2024-06-07 16:18:32","title":"A Tensor Decomposition Perspective on Second-order RNNs","abstract":"Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging second-order interactions for sequence modelling. These models are provably more expressive than their first-order counterparts and have connections to well-studied models from formal language theory. However, their large parameter tensor makes computations intractable. To circumvent this issue, one approach known as MIRNN consists in limiting the type of interactions used by the model. Another is to leverage tensor decomposition to diminish the parameter count. In this work, we study the model resulting from parameterizing 2RNNs using the CP decomposition, which we call CPRNN. Intuitively, the rank of the decomposition should reduce expressivity. We analyze how rank and hidden size affect model capacity and show the relationships between RNNs, 2RNNs, MIRNNs, and CPRNNs based on these parameters. We support these results empirically with experiments on the Penn Treebank dataset which demonstrate that, with a fixed parameter budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs with the right choice of rank and hidden size.","sentences":["Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging second-order interactions for sequence modelling.","These models are provably more expressive than their first-order counterparts and have connections to well-studied models from formal language theory.","However, their large parameter tensor makes computations intractable.","To circumvent this issue, one approach known as MIRNN consists in limiting the type of interactions used by the model.","Another is to leverage tensor decomposition to diminish the parameter count.","In this work, we study the model resulting from parameterizing 2RNNs using the CP decomposition, which we call CPRNN.","Intuitively, the rank of the decomposition should reduce expressivity.","We analyze how rank and hidden size affect model capacity and show the relationships between RNNs, 2RNNs, MIRNNs, and CPRNNs based on these parameters.","We support these results empirically with experiments on the Penn Treebank dataset which demonstrate that, with a fixed parameter budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs with the right choice of rank and hidden size."],"url":"http://arxiv.org/abs/2406.05045v1","category":"cs.LG"}
{"created":"2024-06-07 16:02:10","title":"Bootstrapping Referring Multi-Object Tracking","abstract":"Referring multi-object tracking (RMOT) aims at detecting and tracking multiple objects following human instruction represented by a natural language expression. Existing RMOT benchmarks are usually formulated through manual annotations, integrated with static regulations. This approach results in a dearth of notable diversity and a constrained scope of implementation. In this work, our key idea is to bootstrap the task of referring multi-object tracking by introducing discriminative language words as much as possible. In specific, we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2. It starts with 2,719 manual annotations, addressing the issue of class imbalance and introducing more keywords to make it closer to real-world scenarios compared to Refer-KITTI. They are further expanded to a total of 9,758 annotations by prompting large language models, which create 617 different words, surpassing previous RMOT benchmarks. In addition, the end-to-end framework in RMOT is also bootstrapped by a simple yet elegant temporal advancement strategy, which achieves better performance than previous approaches. The source code and dataset is available at https://github.com/zyn213/TempRMOT.","sentences":["Referring multi-object tracking (RMOT) aims at detecting and tracking multiple objects following human instruction represented by a natural language expression.","Existing RMOT benchmarks are usually formulated through manual annotations, integrated with static regulations.","This approach results in a dearth of notable diversity and a constrained scope of implementation.","In this work, our key idea is to bootstrap the task of referring multi-object tracking by introducing discriminative language words as much as possible.","In specific, we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2.","It starts with 2,719 manual annotations, addressing the issue of class imbalance and introducing more keywords to make it closer to real-world scenarios compared to Refer-KITTI.","They are further expanded to a total of 9,758 annotations by prompting large language models, which create 617 different words, surpassing previous RMOT benchmarks.","In addition, the end-to-end framework in RMOT is also bootstrapped by a simple yet elegant temporal advancement strategy, which achieves better performance than previous approaches.","The source code and dataset is available at https://github.com/zyn213/TempRMOT."],"url":"http://arxiv.org/abs/2406.05039v1","category":"cs.CV"}
{"created":"2024-06-07 15:21:00","title":"Clarifying Myths About the Relationship Between Shape Bias, Accuracy, and Robustness","abstract":"Deep learning models can perform well when evaluated on images from the same distribution as the training set. However, applying small perturbations in the forms of noise, artifacts, occlusions, blurring, etc. to a model's input image and feeding the model with out-of-distribution (OOD) data can significantly drop the model's accuracy, making it not applicable to real-world scenarios. Data augmentation is one of the well-practiced methods to improve model robustness against OOD data; however, examining which augmentation type to choose and how it affects the OOD robustness remains understudied. There is a growing belief that augmenting datasets using data augmentations that improve a model's bias to shape-based features rather than texture-based features results in increased OOD robustness for Convolutional Neural Networks trained on the ImageNet-1K dataset. This is usually stated as ``an increase in the model's shape bias results in an increase in its OOD robustness\". Based on this hypothesis, some works in the literature aim to find augmentations with higher effects on model shape bias and use those for data augmentation. By evaluating 39 types of data augmentations on a widely used OOD dataset, we demonstrate the impact of each data augmentation on the model's robustness to OOD data and further show that the mentioned hypothesis is not true; an increase in shape bias does not necessarily result in higher OOD robustness. By analyzing the results, we also find some biases in the ImageNet-1K dataset that can easily be reduced using proper data augmentation. Our evaluation results further show that there is not necessarily a trade-off between in-domain accuracy and OOD robustness, and choosing the proper augmentations can help increase both in-domain accuracy and OOD robustness simultaneously.","sentences":["Deep learning models can perform well when evaluated on images from the same distribution as the training set.","However, applying small perturbations in the forms of noise, artifacts, occlusions, blurring, etc. to a model's input image and feeding the model with out-of-distribution (OOD) data can significantly drop the model's accuracy, making it not applicable to real-world scenarios.","Data augmentation is one of the well-practiced methods to improve model robustness against OOD data; however, examining which augmentation type to choose and how it affects the OOD robustness remains understudied.","There is a growing belief that augmenting datasets using data augmentations that improve a model's bias to shape-based features rather than texture-based features results in increased OOD robustness for Convolutional Neural Networks trained on the ImageNet-1K dataset.","This is usually stated as ``an increase in the model's shape bias results in an increase in its OOD robustness\".","Based on this hypothesis, some works in the literature aim to find augmentations with higher effects on model shape bias and use those for data augmentation.","By evaluating 39 types of data augmentations on a widely used OOD dataset, we demonstrate the impact of each data augmentation on the model's robustness to OOD data and further show that the mentioned hypothesis is not true; an increase in shape bias does not necessarily result in higher OOD robustness.","By analyzing the results, we also find some biases in the ImageNet-1K dataset that can easily be reduced using proper data augmentation.","Our evaluation results further show that there is not necessarily a trade-off between in-domain accuracy and OOD robustness, and choosing the proper augmentations can help increase both in-domain accuracy and OOD robustness simultaneously."],"url":"http://arxiv.org/abs/2406.05006v1","category":"cs.CV"}
{"created":"2024-06-07 15:05:33","title":"Unguided structure learning of DAGs for count data","abstract":"Mainly motivated by the problem of modelling directional dependence relationships for multivariate count data in high-dimensional settings, we present a new algorithm, called learnDAG, for learning the structure of directed acyclic graphs (DAGs). In particular, the proposed algorithm tackled the problem of learning DAGs from observational data in two main steps: (i) estimation of candidate parent sets; and (ii) feature selection. We experimentally compare learnDAG to several popular competitors in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available. Furthermore, to make our algorithm is stronger, a validation of the algorithm is presented through the analysis of real datasets.","sentences":["Mainly motivated by the problem of modelling directional dependence relationships for multivariate count data in high-dimensional settings, we present a new algorithm, called learnDAG, for learning the structure of directed acyclic graphs (DAGs).","In particular, the proposed algorithm tackled the problem of learning DAGs from observational data in two main steps: (i) estimation of candidate parent sets; and (ii) feature selection.","We experimentally compare learnDAG to several popular competitors in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available.","Furthermore, to make our algorithm is stronger, a validation of the algorithm is presented through the analysis of real datasets."],"url":"http://arxiv.org/abs/2406.04994v1","category":"stat.ME"}
{"created":"2024-06-07 14:49:40","title":"Self-testing in the compiled setting via tilted-CHSH inequalities","abstract":"In a Bell scenario, a classical verifier interacts with two non-communicating provers, producing a correlation. Certain correlations allow the verifier to certify, or self-test, the underlying quantum state and measurements. Notably, the family of tilted-CHSH inequalities has been used to self-test any two-qubit state. Self-tests underpin numerous device-independent protocols, however, a significant drawback of such applications is the non-communicating assumption. To address the non-communication assumption Kalai et al. (STOC'23) give a procedure which compiles a bipartite Bell scenario into a 2-round interaction between a verifier and a single computationally bounded prover. In this work, we formalize self-testing for compiled Bell scenarios. We prove the maximal quantum value is preserved under compilation for the family of tilted-CHSH inequalities, and that any maximal violation constitutes a compiled self-test. More specifically, we establish the existence of an efficient isometry recovering the state and measurements in the second round.","sentences":["In a Bell scenario, a classical verifier interacts with two non-communicating provers, producing a correlation.","Certain correlations allow the verifier to certify, or self-test, the underlying quantum state and measurements.","Notably, the family of tilted-CHSH inequalities has been used to self-test any two-qubit state.","Self-tests underpin numerous device-independent protocols, however, a significant drawback of such applications is the non-communicating assumption.","To address the non-communication assumption Kalai et al.","(STOC'23) give a procedure which compiles a bipartite Bell scenario into a 2-round interaction between a verifier and a single computationally bounded prover.","In this work, we formalize self-testing for compiled Bell scenarios.","We prove the maximal quantum value is preserved under compilation for the family of tilted-CHSH inequalities, and that any maximal violation constitutes a compiled self-test.","More specifically, we establish the existence of an efficient isometry recovering the state and measurements in the second round."],"url":"http://arxiv.org/abs/2406.04986v1","category":"quant-ph"}
{"created":"2024-06-07 14:25:51","title":"Multi-style Neural Radiance Field with AdaIN","abstract":"In this work, we propose a novel pipeline that combines AdaIN and NeRF for the task of stylized Novel View Synthesis. Compared to previous works, we make the following contributions: 1) We simplify the pipeline. 2) We extend the capabilities of model to handle the multi-style task. 3) We modify the model architecture to perform well on styles with strong brush strokes. 4) We implement style interpolation on the multi-style model, allowing us to control the style between any two styles and the style intensity between the stylized output and the original scene, providing better control over the stylization strength.","sentences":["In this work, we propose a novel pipeline that combines AdaIN and NeRF for the task of stylized Novel View Synthesis.","Compared to previous works, we make the following contributions: 1) We simplify the pipeline.","2) We extend the capabilities of model to handle the multi-style task.","3) We modify the model architecture to perform well on styles with strong brush strokes.","4) We implement style interpolation on the multi-style model, allowing us to control the style between any two styles and the style intensity between the stylized output and the original scene, providing better control over the stylization strength."],"url":"http://arxiv.org/abs/2406.04960v1","category":"cs.CV"}
{"created":"2024-06-07 13:57:22","title":"Disentangling Quantum Classifiers: Simplex Edge Mapping for Few-Sample Confidence","abstract":"Quantum machine learning aims to use quantum computers to enhance machine learning, but it is often limited by the required number of samples due to quantum noise and statistical limits on expectation value estimates. While efforts are made to reduce quantum noise, less attention is given to boosting the confidence of Variational Quantum Classifiers (VQCs) and reducing their sampling needs. This paper focuses on multiclass classification, introducing a parameter-free post-processing technique that treats circuit outputs as edges of an n-dimensional simplex, representing independent binary decisions between each pair of classes. We prove and show in our experiments that this method improves few-sample accuracy by a factor of two by disentangling the wire outputs and compelling the VQC to avoid uncertain outputs. We describe this method and provide comparisons of accuracy, confidence, and entanglement, advocating for few-sample accuracy as a primary goal for effective VQCs.","sentences":["Quantum machine learning aims to use quantum computers to enhance machine learning, but it is often limited by the required number of samples due to quantum noise and statistical limits on expectation value estimates.","While efforts are made to reduce quantum noise, less attention is given to boosting the confidence of Variational Quantum Classifiers (VQCs) and reducing their sampling needs.","This paper focuses on multiclass classification, introducing a parameter-free post-processing technique that treats circuit outputs as edges of an n-dimensional simplex, representing independent binary decisions between each pair of classes.","We prove and show in our experiments that this method improves few-sample accuracy by a factor of two by disentangling the wire outputs and compelling the VQC to avoid uncertain outputs.","We describe this method and provide comparisons of accuracy, confidence, and entanglement, advocating for few-sample accuracy as a primary goal for effective VQCs."],"url":"http://arxiv.org/abs/2406.04944v1","category":"quant-ph"}
{"created":"2024-06-07 13:37:45","title":"Leveraging Activations for Superpixel Explanations","abstract":"Saliency methods have become standard in the explanation toolkit of deep neural networks. Recent developments specific to image classifiers have investigated region-based explanations with either new methods or by adapting well-established ones using ad-hoc superpixel algorithms. In this paper, we aim to avoid relying on these segmenters by extracting a segmentation from the activations of a deep neural network image classifier without fine-tuning the network. Our so-called Neuro-Activated Superpixels (NAS) can isolate the regions of interest in the input relevant to the model's prediction, which boosts high-threshold weakly supervised object localization performance. This property enables the semi-supervised semantic evaluation of saliency methods. The aggregation of NAS with existing saliency methods eases their interpretation and reveals the inconsistencies of the widely used area under the relevance curve metric.","sentences":["Saliency methods have become standard in the explanation toolkit of deep neural networks.","Recent developments specific to image classifiers have investigated region-based explanations with either new methods or by adapting well-established ones using ad-hoc superpixel algorithms.","In this paper, we aim to avoid relying on these segmenters by extracting a segmentation from the activations of a deep neural network image classifier without fine-tuning the network.","Our so-called Neuro-Activated Superpixels (NAS) can isolate the regions of interest in the input relevant to the model's prediction, which boosts high-threshold weakly supervised object localization performance.","This property enables the semi-supervised semantic evaluation of saliency methods.","The aggregation of NAS with existing saliency methods eases their interpretation and reveals the inconsistencies of the widely used area under the relevance curve metric."],"url":"http://arxiv.org/abs/2406.04933v1","category":"cs.CV"}
{"created":"2024-06-07 13:33:22","title":"LLM-based speaker diarization correction: A generalizable approach","abstract":"Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools. Despite significant developments in diarization methods, diarization accuracy remains an issue. Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step. LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations. The ability of the models to improve diarization accuracy in a holdout dataset was measured. We report that fine-tuned LLMs can markedly improve diarization accuracy. However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability. To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable. We hope to make these models accessible through public-facing APIs for use by third-party applications.","sentences":["Speaker diarization is necessary for interpreting conversations transcribed using automated speech recognition (ASR) tools.","Despite significant developments in diarization methods, diarization accuracy remains an issue.","Here, we investigate the use of large language models (LLMs) for diarization correction as a post-processing step.","LLMs were fine-tuned using the Fisher corpus, a large dataset of transcribed conversations.","The ability of the models to improve diarization accuracy in a holdout dataset was measured.","We report that fine-tuned LLMs can markedly improve diarization accuracy.","However, model performance is constrained to transcripts produced using the same ASR tool as the transcripts used for fine-tuning, limiting generalizability.","To address this constraint, an ensemble model was developed by combining weights from three separate models, each fine-tuned using transcripts from a different ASR tool.","The ensemble model demonstrated better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable.","We hope to make these models accessible through public-facing APIs for use by third-party applications."],"url":"http://arxiv.org/abs/2406.04927v1","category":"eess.AS"}
{"created":"2024-06-07 12:53:32","title":"Beyond Data, Towards Sustainability: A Sydney Case Study on Urban Digital Twins","abstract":"As urban areas grapple with unprecedented challenges stemming from population growth and climate change, the emergence of urban digital twins offers a promising solution. This paper presents a case study focusing on Sydney's urban digital twin, a virtual replica integrating diverse real-time and historical data, including weather, crime, emissions, and traffic. Through advanced visualization and data analysis techniques, the study explores some applications of this digital twin in urban sustainability, such as spatial ranking of suburbs and automatic identification of correlations between variables. Additionally, the research delves into predictive modeling, employing machine learning to forecast traffic crash risks using environmental data, showcasing the potential for proactive interventions. The contributions of this work lie in the comprehensive exploration of a city-scale digital twin for sustainable urban planning, offering a multifaceted approach to data-driven decision-making.","sentences":["As urban areas grapple with unprecedented challenges stemming from population growth and climate change, the emergence of urban digital twins offers a promising solution.","This paper presents a case study focusing on Sydney's urban digital twin, a virtual replica integrating diverse real-time and historical data, including weather, crime, emissions, and traffic.","Through advanced visualization and data analysis techniques, the study explores some applications of this digital twin in urban sustainability, such as spatial ranking of suburbs and automatic identification of correlations between variables.","Additionally, the research delves into predictive modeling, employing machine learning to forecast traffic crash risks using environmental data, showcasing the potential for proactive interventions.","The contributions of this work lie in the comprehensive exploration of a city-scale digital twin for sustainable urban planning, offering a multifaceted approach to data-driven decision-making."],"url":"http://arxiv.org/abs/2406.04902v1","category":"cs.ET"}
{"created":"2024-06-07 11:21:52","title":"Do Language Models Exhibit Human-like Structural Priming Effects?","abstract":"We explore which linguistic factors -- at the sentence and token level -- play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017). We make use of the structural priming paradigm, where recent exposure to a structure facilitates processing of the same structure. We don't only investigate whether, but also where priming effects occur, and what factors predict them. We show that these effects can be explained via the inverse frequency effect, known in human priming, where rarer elements within a prime increase priming effects, as well as lexical dependence between prime and target. Our results provide an important piece in the puzzle of understanding how properties within their context affect structural prediction in language models.","sentences":["We explore which linguistic factors -- at the sentence and token level -- play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017).","We make use of the structural priming paradigm, where recent exposure to a structure facilitates processing of the same structure.","We don't only investigate whether, but also where priming effects occur, and what factors predict them.","We show that these effects can be explained via the inverse frequency effect, known in human priming, where rarer elements within a prime increase priming effects, as well as lexical dependence between prime and target.","Our results provide an important piece in the puzzle of understanding how properties within their context affect structural prediction in language models."],"url":"http://arxiv.org/abs/2406.04847v1","category":"cs.CL"}
{"created":"2024-06-07 11:09:50","title":"Normal and superconducting properties of La$_3$Ni$_2$O$_7$","abstract":"This review provides a comprehensive overview of current research on the structural, electronic, and magnetic characteristics of the recently discovered high-temperature superconductor La$_3$Ni$_2$O$_7$ under high pressures. We present the experimental results for synthesizing and characterizing this material, derived from measurements of transport, thermodynamics, and various spectroscopic techniques, and discuss their physical implications. We also explore theoretical models proposed to describe the electronic structures and superconducting pairing symmetry in La$_3$Ni$_2$O$_7$, highlighting the intricate interplay between electronic correlations and magnetic interactions. Despite these advances, challenges remain in growing high-quality samples free of extrinsic phases and oxygen deficiencies and in developing reliable measurement tools for determining diamagnetism and other physical quantities under high pressures. Further investigations in these areas are essential to deepening our understanding of the physical properties of La$_3$Ni$_2$O$_7$ and unlocking its superconducting pairing mechanism.","sentences":["This review provides a comprehensive overview of current research on the structural, electronic, and magnetic characteristics of the recently discovered high-temperature superconductor La$_3$Ni$_2$O$_7$ under high pressures.","We present the experimental results for synthesizing and characterizing this material, derived from measurements of transport, thermodynamics, and various spectroscopic techniques, and discuss their physical implications.","We also explore theoretical models proposed to describe the electronic structures and superconducting pairing symmetry in La$_3$Ni$_2$O$_7$, highlighting the intricate interplay between electronic correlations and magnetic interactions.","Despite these advances, challenges remain in growing high-quality samples free of extrinsic phases and oxygen deficiencies and in developing reliable measurement tools for determining diamagnetism and other physical quantities under high pressures.","Further investigations in these areas are essential to deepening our understanding of the physical properties of La$_3$Ni$_2$O$_7$ and unlocking its superconducting pairing mechanism."],"url":"http://arxiv.org/abs/2406.04837v1","category":"cond-mat.supr-con"}
{"created":"2024-06-07 10:12:01","title":"GENIE: Watermarking Graph Neural Networks for Link Prediction","abstract":"Graph Neural Networks (GNNs) have advanced the field of machine learning by utilizing graph-structured data, which is ubiquitous in the real world. GNNs have applications in various fields, ranging from social network analysis to drug discovery. GNN training is strenuous, requiring significant computational resources and human expertise. It makes a trained GNN an indispensable Intellectual Property (IP) for its owner. Recent studies have shown GNNs to be vulnerable to model-stealing attacks, which raises concerns over IP rights protection. Watermarking has been shown to be effective at protecting the IP of a GNN model. Existing efforts to develop a watermarking scheme for GNNs have only focused on the node classification and the graph classification tasks.   To the best of our knowledge, we introduce the first-ever watermarking scheme for GNNs tailored to the Link Prediction (LP) task. We call our proposed watermarking scheme GENIE (watermarking Graph nEural Networks for lInk prEdiction). We design GENIE using a novel backdoor attack to create a trigger set for two key methods of LP: (1) node representation-based and (2) subgraph-based. In GENIE, the watermark is embedded into the GNN model by training it on both the trigger set and a modified training set, resulting in a watermarked GNN model. To assess a suspect model, we verify the watermark against the trigger set. We extensively evaluate GENIE across 3 model architectures (i.e., SEAL, GCN, and GraphSAGE) and 7 real-world datasets. Furthermore, we validate the robustness of GENIE against 11 state-of-the-art watermark removal techniques and 3 model extraction attacks. We also demonstrate that GENIE is robust against ownership piracy attack. Our ownership demonstration scheme statistically guarantees both False Positive Rate (FPR) and False Negative Rate (FNR) to be less than $10^{-6}$.","sentences":["Graph Neural Networks (GNNs) have advanced the field of machine learning by utilizing graph-structured data, which is ubiquitous in the real world.","GNNs have applications in various fields, ranging from social network analysis to drug discovery.","GNN training is strenuous, requiring significant computational resources and human expertise.","It makes a trained GNN an indispensable Intellectual Property (IP) for its owner.","Recent studies have shown GNNs to be vulnerable to model-stealing attacks, which raises concerns over IP rights protection.","Watermarking has been shown to be effective at protecting the IP of a GNN model.","Existing efforts to develop a watermarking scheme for GNNs have only focused on the node classification and the graph classification tasks.   ","To the best of our knowledge, we introduce the first-ever watermarking scheme for GNNs tailored to the Link Prediction (LP) task.","We call our proposed watermarking scheme GENIE (watermarking Graph nEural Networks for lInk prEdiction).","We design GENIE using a novel backdoor attack to create a trigger set for two key methods of LP: (1) node representation-based and (2) subgraph-based.","In GENIE, the watermark is embedded into the GNN model by training it on both the trigger set and a modified training set, resulting in a watermarked GNN model.","To assess a suspect model, we verify the watermark against the trigger set.","We extensively evaluate GENIE across 3 model architectures (i.e., SEAL, GCN, and GraphSAGE) and 7 real-world datasets.","Furthermore, we validate the robustness of GENIE against 11 state-of-the-art watermark removal techniques and 3 model extraction attacks.","We also demonstrate that GENIE is robust against ownership piracy attack.","Our ownership demonstration scheme statistically guarantees both False Positive Rate (FPR) and False Negative Rate (FNR) to be less than $10^{-6}$."],"url":"http://arxiv.org/abs/2406.04805v1","category":"cs.CR"}
{"created":"2024-06-07 09:31:46","title":"Entropy stable finite difference schemes for Chew, Goldberger & Low anisotropic plasma flow equations","abstract":"In this article, we consider the Chew, Goldberger \\& Low (CGL) plasma flow equations, which is a set of nonlinear, non-conservative hyperbolic PDEs modelling anisotropic plasma flows. These equations incorporate the double adiabatic approximation for the evolution of the pressure, making them very valuable for plasma physics, space physics and astrophysical applications. We first present the entropy analysis for the weak solutions. We then propose entropy-stable finite-difference schemes for the CGL equations. The key idea is to rewrite the CGL equations such that the non-conservative terms do not contribute to the entropy equations. The conservative part of the rewritten equations is very similar to the magnetohydrodynamics (MHD) equations. We then symmetrize the conservative part by following Godunov's symmetrization process for MHD. The resulting equations are then discretized by designing entropy conservative numerical flux and entropy diffusion operator based on the entropy scaled eigenvectors of the conservative part. We then prove the semi-discrete entropy stability of the schemes for CGL equations. The schemes are then tested using several test problems derived from the corresponding MHD test cases.","sentences":["In this article, we consider the Chew, Goldberger \\& Low (CGL) plasma flow equations, which is a set of nonlinear, non-conservative hyperbolic PDEs modelling anisotropic plasma flows.","These equations incorporate the double adiabatic approximation for the evolution of the pressure, making them very valuable for plasma physics, space physics and astrophysical applications.","We first present the entropy analysis for the weak solutions.","We then propose entropy-stable finite-difference schemes for the CGL equations.","The key idea is to rewrite the CGL equations such that the non-conservative terms do not contribute to the entropy equations.","The conservative part of the rewritten equations is very similar to the magnetohydrodynamics (MHD) equations.","We then symmetrize the conservative part by following Godunov's symmetrization process for MHD.","The resulting equations are then discretized by designing entropy conservative numerical flux and entropy diffusion operator based on the entropy scaled eigenvectors of the conservative part.","We then prove the semi-discrete entropy stability of the schemes for CGL equations.","The schemes are then tested using several test problems derived from the corresponding MHD test cases."],"url":"http://arxiv.org/abs/2406.04783v1","category":"math.NA"}
{"created":"2024-06-07 09:18:59","title":"Uniform estimates for a family of Poisson problems: `rounding up the corners'","abstract":"We prove \\emph{uniform solvability estimates} for certain families of elliptic problems posed in a family of domains that converge to another domain. We provide uniform estimates both in weighted and in usual Sobolev spaces. When the limit domain is a \\emph{polygon}, our results amount to ``rounding up'' the corners of the limit domain. The technique of proof is based on a suitable conformal modification of the metric, which makes the union of the domains a manifold with boundary and relative bounded geometry.","sentences":["We prove \\emph{uniform solvability estimates} for certain families of elliptic problems posed in a family of domains that converge to another domain.","We provide uniform estimates both in weighted and in usual Sobolev spaces.","When the limit domain is a \\emph{polygon}, our results amount to ``rounding up'' the corners of the limit domain.","The technique of proof is based on a suitable conformal modification of the metric, which makes the union of the domains a manifold with boundary and relative bounded geometry."],"url":"http://arxiv.org/abs/2406.04773v1","category":"math.AP"}
{"created":"2024-06-07 08:57:25","title":"Interpretable Multimodal Out-of-context Detection with Soft Logic Regularization","abstract":"The rapid spread of information through mobile devices and media has led to the widespread of false or deceptive news, causing significant concerns in society. Among different types of misinformation, image repurposing, also known as out-of-context misinformation, remains highly prevalent and effective. However, current approaches for detecting out-of-context misinformation often lack interpretability and offer limited explanations. In this study, we propose a logic regularization approach for out-of-context detection called LOGRAN (LOGic Regularization for out-of-context ANalysis). The primary objective of LOGRAN is to decompose the out-of-context detection at the phrase level. By employing latent variables for phrase-level predictions, the final prediction of the image-caption pair can be aggregated using logical rules. The latent variables also provide an explanation for how the final result is derived, making this fine-grained detection method inherently explanatory. We evaluate the performance of LOGRAN on the NewsCLIPpings dataset, showcasing competitive overall results. Visualized examples also reveal faithful phrase-level predictions of out-of-context images, accompanied by explanations. This highlights the effectiveness of our approach in addressing out-of-context detection and enhancing interpretability.","sentences":["The rapid spread of information through mobile devices and media has led to the widespread of false or deceptive news, causing significant concerns in society.","Among different types of misinformation, image repurposing, also known as out-of-context misinformation, remains highly prevalent and effective.","However, current approaches for detecting out-of-context misinformation often lack interpretability and offer limited explanations.","In this study, we propose a logic regularization approach for out-of-context detection called LOGRAN (LOGic Regularization for out-of-context ANalysis).","The primary objective of LOGRAN is to decompose the out-of-context detection at the phrase level.","By employing latent variables for phrase-level predictions, the final prediction of the image-caption pair can be aggregated using logical rules.","The latent variables also provide an explanation for how the final result is derived, making this fine-grained detection method inherently explanatory.","We evaluate the performance of LOGRAN on the NewsCLIPpings dataset, showcasing competitive overall results.","Visualized examples also reveal faithful phrase-level predictions of out-of-context images, accompanied by explanations.","This highlights the effectiveness of our approach in addressing out-of-context detection and enhancing interpretability."],"url":"http://arxiv.org/abs/2406.04756v1","category":"cs.CV"}
{"created":"2024-06-07 08:37:28","title":"In-depth Analysis of Densest Subgraph Discovery in a Unified Framework","abstract":"As a fundamental topic in graph mining, Densest Subgraph Discovery (DSD) has found a wide spectrum of real applications. Several DSD algorithms, including exact and approximation algorithms, have been proposed in the literature. However, these algorithms have not been systematically and comprehensively compared under the same experimental settings. In this paper, we first propose a unified framework to incorporate all DSD algorithms from a high-level perspective. We then extensively compare representative DSD algorithms over a range of graphs -- from small to billion-scale -- and examine the effectiveness of all methods. Moreover, we suggest new variants of the DSD algorithms by combining the existing techniques, which are up to 10 X faster than the state-of-the-art algorithm with the same accuracy guarantee. Finally, based on the findings, we offer promising research opportunities. We believe that a deeper understanding of the behavior of existing algorithms can provide new valuable insights for future research. The codes are released at https://anonymous.4open.science/r/DensestSubgraph-245A","sentences":["As a fundamental topic in graph mining, Densest Subgraph Discovery (DSD) has found a wide spectrum of real applications.","Several DSD algorithms, including exact and approximation algorithms, have been proposed in the literature.","However, these algorithms have not been systematically and comprehensively compared under the same experimental settings.","In this paper, we first propose a unified framework to incorporate all DSD algorithms from a high-level perspective.","We then extensively compare representative DSD algorithms over a range of graphs -- from small to billion-scale -- and examine the effectiveness of all methods.","Moreover, we suggest new variants of the DSD algorithms by combining the existing techniques, which are up to 10 X faster than the state-of-the-art algorithm with the same accuracy guarantee.","Finally, based on the findings, we offer promising research opportunities.","We believe that a deeper understanding of the behavior of existing algorithms can provide new valuable insights for future research.","The codes are released at https://anonymous.4open.science/r/DensestSubgraph-245A"],"url":"http://arxiv.org/abs/2406.04738v1","category":"cs.DB"}
{"created":"2024-06-07 14:37:45","title":"Characterizing Biphoton Spatial Wave Function Dynamics with Quantum Wavefront Sensing","abstract":"With an extremely high dimensionality, the spatial degree of freedom of entangled photons is a key tool for quantum foundation and applied quantum techniques. To fully utilize the feature, the essential task is to experimentally characterize the multiphoton spatial wave function including the entangled amplitude and phase information at different evolutionary stages. However, there is no effective method to measure it. Quantum state tomography is costly, and quantum holography requires additional references. Here we introduce quantum Shack-Hartmann wavefront sensing to perform efficient and reference-free measurement of the biphoton spatial wave function. The joint probability distribution of photon pairs at the back focal plane of a microlens array is measured and used for amplitude extraction and phase reconstruction. In the experiment, we observe that the biphoton amplitude correlation becomes weak while phase correlation shows up during free-space propagation. Our work is a crucial step in quantum physical and adaptive optics and paves the way for characterizing quantum optical fields with high-order correlations or topological patterns.","sentences":["With an extremely high dimensionality, the spatial degree of freedom of entangled photons is a key tool for quantum foundation and applied quantum techniques.","To fully utilize the feature, the essential task is to experimentally characterize the multiphoton spatial wave function including the entangled amplitude and phase information at different evolutionary stages.","However, there is no effective method to measure it.","Quantum state tomography is costly, and quantum holography requires additional references.","Here we introduce quantum Shack-Hartmann wavefront sensing to perform efficient and reference-free measurement of the biphoton spatial wave function.","The joint probability distribution of photon pairs at the back focal plane of a microlens array is measured and used for amplitude extraction and phase reconstruction.","In the experiment, we observe that the biphoton amplitude correlation becomes weak while phase correlation shows up during free-space propagation.","Our work is a crucial step in quantum physical and adaptive optics and paves the way for characterizing quantum optical fields with high-order correlations or topological patterns."],"url":"http://arxiv.org/abs/2406.04973v1","category":"quant-ph"}
{"created":"2024-06-07 14:34:19","title":"Programmable Multi-responsive Nanocellulose-based Hydrogels with Embodied Logic","abstract":"Programmable materials are desirable for a variety of functional applications that range from biomedical devices, actuators and soft robots to adaptive surfaces and deployable structures. However, current smart materials are often designed to respond to single stimuli (like temperature, humidity, or light). Here, a novel multi-stimuli-responsive composite is fabricated using direct ink writing (DIW) to enable programmability in both space and time and computation of logic operations. The composite hydrogels consist of double-network matrices of poly(N-isopropylacrylamide) (PNIPAM) or poly(acrylic acid) (PAA) and sodium alginate (SA) and are reinforced by a high content of cellulose nanocrystals (CNC) (14 wt%) and nanofibers (CNF) (1 wt%). These composites exhibit a simultaneously tunable response to external stimuli, such as temperature, pH, and ion concentration, enabling precise control over their swelling and shrinking behavior, shape, and mechanical properties over time. Bilayer hydrogel actuators are designed to display bidirectional bending in response to various stimuli scenarios. Finally, to leverage the multi-responsiveness and programmability of this new composite, Boolean algebra concepts are used to design and execute NOT, YES, OR, and AND logic gates, paving the way for self-actuating materials with embodied logic.","sentences":["Programmable materials are desirable for a variety of functional applications that range from biomedical devices, actuators and soft robots to adaptive surfaces and deployable structures.","However, current smart materials are often designed to respond to single stimuli (like temperature, humidity, or light).","Here, a novel multi-stimuli-responsive composite is fabricated using direct ink writing (DIW) to enable programmability in both space and time and computation of logic operations.","The composite hydrogels consist of double-network matrices of poly(N-isopropylacrylamide) (PNIPAM) or poly(acrylic acid)","(PAA) and sodium alginate (SA) and are reinforced by a high content of cellulose nanocrystals (CNC) (14 wt%) and nanofibers (CNF) (1 wt%).","These composites exhibit a simultaneously tunable response to external stimuli, such as temperature, pH, and ion concentration, enabling precise control over their swelling and shrinking behavior, shape, and mechanical properties over time.","Bilayer hydrogel actuators are designed to display bidirectional bending in response to various stimuli scenarios.","Finally, to leverage the multi-responsiveness and programmability of this new composite, Boolean algebra concepts are used to design and execute NOT, YES, OR, and AND logic gates, paving the way for self-actuating materials with embodied logic."],"url":"http://arxiv.org/abs/2406.04970v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 12:25:51","title":"A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques","abstract":"Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.","sentences":["Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences.","While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA.","Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank.","However, there has not yet been an extensive study of their effect on downstream performance.","To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct).","Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings.","We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks.","Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment."],"url":"http://arxiv.org/abs/2406.04879v1","category":"cs.CL"}
{"created":"2024-06-07 10:05:42","title":"MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks","abstract":"The sparsely activated mixture of experts (MoE) model presents a promising alternative to traditional densely activated (dense) models, enhancing both quality and computational efficiency. However, training MoE models from scratch demands extensive data and computational resources. Moreover, public repositories like timm mainly provide pre-trained dense checkpoints, lacking similar resources for MoE models, hindering their adoption. To bridge this gap, we introduce MoE Jetpack, an effective method for fine-tuning dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which repurposes dense checkpoints as initial weights for MoE models, thereby accelerating convergence, enhancing accuracy, and alleviating the computational burden of pre-training; (2) hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture for better integration of dense checkpoints, enhancing fine-tuning performance. Our experiments on vision tasks demonstrate that MoE Jetpack significantly improves convergence speed and accuracy when fine-tuning dense checkpoints into MoE models. Our code will be publicly available at https://github.com/Adlith/MoE-Jetpack.","sentences":["The sparsely activated mixture of experts (MoE) model presents a promising alternative to traditional densely activated (dense) models, enhancing both quality and computational efficiency.","However, training MoE models from scratch demands extensive data and computational resources.","Moreover, public repositories like timm mainly provide pre-trained dense checkpoints, lacking similar resources for MoE models, hindering their adoption.","To bridge this gap, we introduce MoE Jetpack, an effective method for fine-tuning dense checkpoints into MoE models.","MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which repurposes dense checkpoints as initial weights for MoE models, thereby accelerating convergence, enhancing accuracy, and alleviating the computational burden of pre-training; (2) hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture for better integration of dense checkpoints, enhancing fine-tuning performance.","Our experiments on vision tasks demonstrate that MoE Jetpack significantly improves convergence speed and accuracy when fine-tuning dense checkpoints into MoE models.","Our code will be publicly available at https://github.com/Adlith/MoE-Jetpack."],"url":"http://arxiv.org/abs/2406.04801v1","category":"cs.CV"}
{"created":"2024-06-07 09:21:06","title":"TDT Loss Takes It All: Integrating Temporal Dependencies among Targets into Non-Autoregressive Time Series Forecasting","abstract":"Learning temporal dependencies among targets (TDT) benefits better time series forecasting, where targets refer to the predicted sequence. Although autoregressive methods model TDT recursively, they suffer from inefficient inference and error accumulation. We argue that integrating TDT learning into non-autoregressive methods is essential for pursuing effective and efficient time series forecasting. In this study, we introduce the differencing approach to represent TDT and propose a parameter-free and plug-and-play solution through an optimization objective, namely TDT Loss. It leverages the proportion of inconsistent signs between predicted and ground truth TDT as an adaptive weight, dynamically balancing target prediction and fine-grained TDT fitting. Importantly, TDT Loss incurs negligible additional cost, with only $\\mathcal{O}(n)$ increased computation and $\\mathcal{O}(1)$ memory requirements, while significantly enhancing the predictive performance of non-autoregressive models. To assess the effectiveness of TDT loss, we conduct extensive experiments on 7 widely used datasets. The experimental results of plugging TDT loss into 6 state-of-the-art methods show that out of the 168 experiments, 75.00\\% and 94.05\\% exhibit improvements in terms of MSE and MAE with the maximum 24.56\\% and 16.31\\%, respectively.","sentences":["Learning temporal dependencies among targets (TDT) benefits better time series forecasting, where targets refer to the predicted sequence.","Although autoregressive methods model TDT recursively, they suffer from inefficient inference and error accumulation.","We argue that integrating TDT learning into non-autoregressive methods is essential for pursuing effective and efficient time series forecasting.","In this study, we introduce the differencing approach to represent TDT and propose a parameter-free and plug-and-play solution through an optimization objective, namely TDT Loss.","It leverages the proportion of inconsistent signs between predicted and ground truth TDT as an adaptive weight, dynamically balancing target prediction and fine-grained TDT fitting.","Importantly, TDT Loss incurs negligible additional cost, with only $\\mathcal{O}(n)$ increased computation and $\\mathcal{O}(1)$ memory requirements, while significantly enhancing the predictive performance of non-autoregressive models.","To assess the effectiveness of TDT loss, we conduct extensive experiments on 7 widely used datasets.","The experimental results of plugging TDT loss into 6 state-of-the-art methods show that out of the 168 experiments, 75.00\\% and 94.05\\% exhibit improvements in terms of MSE and MAE with the maximum 24.56\\% and 16.31\\%, respectively."],"url":"http://arxiv.org/abs/2406.04777v1","category":"cs.LG"}
{"created":"2024-06-07 17:57:29","title":"Differentiable Time-Varying Linear Prediction in the Context of End-to-End Analysis-by-Synthesis","abstract":"Training the linear prediction (LP) operator end-to-end for audio synthesis in modern deep learning frameworks is slow due to its recursive formulation. In addition, frame-wise approximation as an acceleration method cannot generalise well to test time conditions where the LP is computed sample-wise. Efficient differentiable sample-wise LP for end-to-end training is the key to removing this barrier. We generalise the efficient time-invariant LP implementation from the GOLF vocoder to time-varying cases. Combining this with the classic source-filter model, we show that the improved GOLF learns LP coefficients and reconstructs the voice better than its frame-wise counterparts. Moreover, in our listening test, synthesised outputs from GOLF scored higher in quality ratings than the state-of-the-art differentiable WORLD vocoder.","sentences":["Training the linear prediction (LP) operator end-to-end for audio synthesis in modern deep learning frameworks is slow due to its recursive formulation.","In addition, frame-wise approximation as an acceleration method cannot generalise well to test time conditions where the LP is computed sample-wise.","Efficient differentiable sample-wise LP for end-to-end training is the key to removing this barrier.","We generalise the efficient time-invariant LP implementation from the GOLF vocoder to time-varying cases.","Combining this with the classic source-filter model, we show that the improved GOLF learns LP coefficients and reconstructs the voice better than its frame-wise counterparts.","Moreover, in our listening test, synthesised outputs from GOLF scored higher in quality ratings than the state-of-the-art differentiable WORLD vocoder."],"url":"http://arxiv.org/abs/2406.05128v1","category":"eess.AS"}
{"created":"2024-06-07 17:50:15","title":"Compositional Curvature Bounds for Deep Neural Networks","abstract":"A key challenge that threatens the widespread use of neural networks in safety-critical applications is their vulnerability to adversarial attacks. In this paper, we study the second-order behavior of continuously differentiable deep neural networks, focusing on robustness against adversarial perturbations. First, we provide a theoretical analysis of robustness and attack certificates for deep classifiers by leveraging local gradients and upper bounds on the second derivative (curvature constant). Next, we introduce a novel algorithm to analytically compute provable upper bounds on the second derivative of neural networks. This algorithm leverages the compositional structure of the model to propagate the curvature bound layer-by-layer, giving rise to a scalable and modular approach. The proposed bound can serve as a differentiable regularizer to control the curvature of neural networks during training, thereby enhancing robustness. Finally, we demonstrate the efficacy of our method on classification tasks using the MNIST and CIFAR-10 datasets.","sentences":["A key challenge that threatens the widespread use of neural networks in safety-critical applications is their vulnerability to adversarial attacks.","In this paper, we study the second-order behavior of continuously differentiable deep neural networks, focusing on robustness against adversarial perturbations.","First, we provide a theoretical analysis of robustness and attack certificates for deep classifiers by leveraging local gradients and upper bounds on the second derivative (curvature constant).","Next, we introduce a novel algorithm to analytically compute provable upper bounds on the second derivative of neural networks.","This algorithm leverages the compositional structure of the model to propagate the curvature bound layer-by-layer, giving rise to a scalable and modular approach.","The proposed bound can serve as a differentiable regularizer to control the curvature of neural networks during training, thereby enhancing robustness.","Finally, we demonstrate the efficacy of our method on classification tasks using the MNIST and CIFAR-10 datasets."],"url":"http://arxiv.org/abs/2406.05119v1","category":"cs.LG"}
{"created":"2024-06-07 17:34:38","title":"New results on biorthogonal families in cylindrical domains and controllability consequences","abstract":"In this article we consider moment problems equivalent to null controllability of some linear parabolic partial differential equations in space dimension higher than one. For these moment problems, we prove existence of an associated biorthogonal family and estimate its norm. The considered setting requires the space domain to be a cylinder and the evolution operator to be tensorized. Roughly speaking, we assume that the so-called Lebeau-Robbiano spectral inequality holds but only for the eigenvectors of the transverse operator. In the one dimensional tangent variable we assume the solvability of block moment problem as introduced in [Benabdallah, Boyer and Morancey - \\textit{Ann. H. Lebesgue.} 3 (2020)]. We apply this abstract construction of biorthogonal families to the characterization of the minimal time for simultaneous null controllability of two heat-like equations in a cylindrical domain. To the best of our knowledge, this result is unattainable with other known techniques.","sentences":["In this article we consider moment problems equivalent to null controllability of some linear parabolic partial differential equations in space dimension higher than one.","For these moment problems, we prove existence of an associated biorthogonal family and estimate its norm.","The considered setting requires the space domain to be a cylinder and the evolution operator to be tensorized.","Roughly speaking, we assume that the so-called Lebeau-Robbiano spectral inequality holds but only for the eigenvectors of the transverse operator.","In the one dimensional tangent variable we assume the solvability of block moment problem as introduced in [Benabdallah, Boyer and Morancey - \\textit{Ann. H. Lebesgue.} 3 (2020)].","We apply this abstract construction of biorthogonal families to the characterization of the minimal time for simultaneous null controllability of two heat-like equations in a cylindrical domain.","To the best of our knowledge, this result is unattainable with other known techniques."],"url":"http://arxiv.org/abs/2406.05104v1","category":"math.AP"}
{"created":"2024-06-07 17:02:37","title":"Optimizing Time Series Forecasting Architectures: A Hierarchical Neural Architecture Search Approach","abstract":"The rapid development of time series forecasting research has brought many deep learning-based modules in this field. However, despite the increasing amount of new forecasting architectures, it is still unclear if we have leveraged the full potential of these existing modules within a properly designed architecture. In this work, we propose a novel hierarchical neural architecture search approach for time series forecasting tasks. With the design of a hierarchical search space, we incorporate many architecture types designed for forecasting tasks and allow for the efficient combination of different forecasting architecture modules. Results on long-term-time-series-forecasting tasks show that our approach can search for lightweight high-performing forecasting architectures across different forecasting tasks.","sentences":["The rapid development of time series forecasting research has brought many deep learning-based modules in this field.","However, despite the increasing amount of new forecasting architectures, it is still unclear if we have leveraged the full potential of these existing modules within a properly designed architecture.","In this work, we propose a novel hierarchical neural architecture search approach for time series forecasting tasks.","With the design of a hierarchical search space, we incorporate many architecture types designed for forecasting tasks and allow for the efficient combination of different forecasting architecture modules.","Results on long-term-time-series-forecasting tasks show that our approach can search for lightweight high-performing forecasting architectures across different forecasting tasks."],"url":"http://arxiv.org/abs/2406.05088v1","category":"cs.LG"}
{"created":"2024-06-07 14:33:07","title":"The role of magnetar transient activity in time-domain and multimessenger astronomy","abstract":"Time-domain and multimessenger astronomy (TDAMM) involves the study of transient and time-variable phenomena across various wavelengths and messengers. The Astro2020 Decadal Survey has identified TDAMM as the top priority for NASA in this decade, emphasizing its crucial role in advancing our understanding of the universe and driving new discoveries in astrophysics. The TDAMM community has come together to provide further guidance to funding agencies, aiming to define a clear path toward optimizing scientific returns in this research domain. This encompasses not only astronomy but also fundamental physics, offering insights into gravity properties, the formation of heavy elements, the equation of state of dense matter, and quantum effects associated with extreme magnetic fields. Magnetars, neutron stars with the strongest magnetic fields known in the universe, play a critical role in this context. In this manuscript, we aim to underscore the significance of magnetars in TDAMM, highlighting the necessity of ensuring observational continuity, addressing current limitations, and outlining essential requirements to expand our knowledge in this field.","sentences":["Time-domain and multimessenger astronomy (TDAMM) involves the study of transient and time-variable phenomena across various wavelengths and messengers.","The Astro2020 Decadal Survey has identified TDAMM as the top priority for NASA in this decade, emphasizing its crucial role in advancing our understanding of the universe and driving new discoveries in astrophysics.","The TDAMM community has come together to provide further guidance to funding agencies, aiming to define a clear path toward optimizing scientific returns in this research domain.","This encompasses not only astronomy but also fundamental physics, offering insights into gravity properties, the formation of heavy elements, the equation of state of dense matter, and quantum effects associated with extreme magnetic fields.","Magnetars, neutron stars with the strongest magnetic fields known in the universe, play a critical role in this context.","In this manuscript, we aim to underscore the significance of magnetars in TDAMM, highlighting the necessity of ensuring observational continuity, addressing current limitations, and outlining essential requirements to expand our knowledge in this field."],"url":"http://arxiv.org/abs/2406.04967v1","category":"astro-ph.HE"}
{"created":"2024-06-07 14:29:17","title":"Mapping the Global Election Landscape on Social Media in 2024","abstract":"In 2024, half of the global population is expected to participate in elections, offering researchers a unique opportunity to study online information diffusion and user behavior. This study investigates the media landscape on social media by analyzing Facebook posts from national political parties and major news agencies across Europe, Mexico, and India. Our methodology identifies key topics and evaluates public interaction, reflecting broader trends in political engagement. Using Principal Component Analysis, we distil these topics to uncover patterns of correlation and differentiation. This approach reveals dominant themes that engage global audiences, providing critical insights into the interplay between public opinion and digital narratives during a major electoral cycle. Our findings highlight how different topics resonate across political spectrums, shaping political debate and offering a comprehensive view of the interaction between media content, political ideology, and audience engagement.","sentences":["In 2024, half of the global population is expected to participate in elections, offering researchers a unique opportunity to study online information diffusion and user behavior.","This study investigates the media landscape on social media by analyzing Facebook posts from national political parties and major news agencies across Europe, Mexico, and India.","Our methodology identifies key topics and evaluates public interaction, reflecting broader trends in political engagement.","Using Principal Component Analysis, we distil these topics to uncover patterns of correlation and differentiation.","This approach reveals dominant themes that engage global audiences, providing critical insights into the interplay between public opinion and digital narratives during a major electoral cycle.","Our findings highlight how different topics resonate across political spectrums, shaping political debate and offering a comprehensive view of the interaction between media content, political ideology, and audience engagement."],"url":"http://arxiv.org/abs/2406.04962v1","category":"cs.SI"}
{"created":"2024-06-07 14:25:53","title":"Multiplane Prior Guided Few-Shot Aerial Scene Rendering","abstract":"Neural Radiance Fields (NeRF) have been successfully applied in various aerial scenes, yet they face challenges with sparse views due to limited supervision. The acquisition of dense aerial views is often prohibitive, as unmanned aerial vehicles (UAVs) may encounter constraints in perspective range and energy constraints. In this work, we introduce Multiplane Prior guided NeRF (MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking a pioneering effort in this domain. Our key insight is that the intrinsic geometric regularities specific to aerial imagery could be leveraged to enhance NeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image (MPI)'s behavior, we propose to guide the training process of NeRF with a Multiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and incorporates advanced image comprehension through a SwinV2 Transformer, pre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF outperforms existing state-of-the-art methods applied in non-aerial contexts, by tripling the performance in SSIM and LPIPS even with three views available. We hope our work offers insights into the development of NeRF-based applications in aerial scenes with limited data.","sentences":["Neural Radiance Fields (NeRF) have been successfully applied in various aerial scenes, yet they face challenges with sparse views due to limited supervision.","The acquisition of dense aerial views is often prohibitive, as unmanned aerial vehicles (UAVs) may encounter constraints in perspective range and energy constraints.","In this work, we introduce Multiplane Prior guided NeRF (MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking a pioneering effort in this domain.","Our key insight is that the intrinsic geometric regularities specific to aerial imagery could be leveraged to enhance NeRF in sparse aerial scenes.","By investigating NeRF's and Multiplane Image (MPI)'s behavior, we propose to guide the training process of NeRF with a Multiplane Prior.","The proposed Multiplane Prior draws upon MPI's benefits and incorporates advanced image comprehension through a SwinV2 Transformer, pre-trained via SimMIM.","Our extensive experiments demonstrate that MPNeRF outperforms existing state-of-the-art methods applied in non-aerial contexts, by tripling the performance in SSIM and LPIPS even with three views available.","We hope our work offers insights into the development of NeRF-based applications in aerial scenes with limited data."],"url":"http://arxiv.org/abs/2406.04961v1","category":"cs.CV"}
{"created":"2024-06-07 12:59:05","title":"Dosimetric comparison of the BNCT treatment planning performances when using a nnU-NET to automatically segment Glioblastoma Multiforme","abstract":"This work presents a preliminary evaluation of the use of the convolutional neural network nnU-NET to automatically contour the volume of Glioblastoma Multiforme in medical images of patients. The goal is to assist the preparation of the Treatment Planning of patients who undergo Boron Neutron Capture Therapy (BNCT). BNCT is a binary form of radiotherapy based on the selective loading of a suitable 10-boron concentration into the tumour and on subsequent low-energy neutron irradiation. The selectivity of the therapeutic effect is based on the capacity of the boron drug to target preferentially cancer cells, thus triggering the neutron capture only in the tumour and depositing there a lethal dose. Even if the tailoring of the beam to the tumour volume is less crucial for BNCT than for other radiation therapies, a proper delimitation of the tumour volume is needed to assess a safe and effective dosimetry. In clinical application the contour must be manually decided by the physician, however, a tool to automatically define important structures such as the Gross Tumour Volume (GTV) and the Organs At Risk (OAR) would be beneficial to enable medical physicists assessing preliminary positioning and simulated dosimetry before the approval or possible changes introduced by the radiotherapist. Moreover, an initial contouring may speed up the work of the physician. The nnU-NET was trained and tested and its performance was evaluated through different parameters such as the Dice Coefficient. To assess a more meaningful evaluation for BNCT, for the first time, this work analyzed the difference of the clinical dosimetry in 16 patients using the manual and the automatic contoured images.","sentences":["This work presents a preliminary evaluation of the use of the convolutional neural network nnU-NET to automatically contour the volume of Glioblastoma Multiforme in medical images of patients.","The goal is to assist the preparation of the Treatment Planning of patients who undergo Boron Neutron Capture Therapy (BNCT).","BNCT is a binary form of radiotherapy based on the selective loading of a suitable 10-boron concentration into the tumour and on subsequent low-energy neutron irradiation.","The selectivity of the therapeutic effect is based on the capacity of the boron drug to target preferentially cancer cells, thus triggering the neutron capture only in the tumour and depositing there a lethal dose.","Even if the tailoring of the beam to the tumour volume is less crucial for BNCT than for other radiation therapies, a proper delimitation of the tumour volume is needed to assess a safe and effective dosimetry.","In clinical application the contour must be manually decided by the physician, however, a tool to automatically define important structures such as the Gross Tumour Volume (GTV) and the Organs At Risk (OAR) would be beneficial to enable medical physicists assessing preliminary positioning and simulated dosimetry before the approval or possible changes introduced by the radiotherapist.","Moreover, an initial contouring may speed up the work of the physician.","The nnU-NET was trained and tested and its performance was evaluated through different parameters such as the Dice Coefficient.","To assess a more meaningful evaluation for BNCT, for the first time, this work analyzed the difference of the clinical dosimetry in 16 patients using the manual and the automatic contoured images."],"url":"http://arxiv.org/abs/2406.04908v1","category":"physics.med-ph"}
{"created":"2024-06-07 12:54:50","title":"Concept Drift Detection using Ensemble of Integrally Private Models","abstract":"Deep neural networks (DNNs) are one of the most widely used machine learning algorithm. DNNs requires the training data to be available beforehand with true labels. This is not feasible for many real-world problems where data arrives in the streaming form and acquisition of true labels are scarce and expensive. In the literature, not much focus has been given to the privacy prospect of the streaming data, where data may change its distribution frequently. These concept drifts must be detected privately in order to avoid any disclosure risk from DNNs. Existing privacy models use concept drift detection schemes such ADWIN, KSWIN to detect the drifts. In this paper, we focus on the notion of integrally private DNNs to detect concept drifts. Integrally private DNNs are the models which recur frequently from different datasets. Based on this, we introduce an ensemble methodology which we call 'Integrally Private Drift Detection' (IPDD) method to detect concept drift from private models. Our IPDD method does not require labels to detect drift but assumes true labels are available once the drift has been detected. We have experimented with binary and multi-class synthetic and real-world data. Our experimental results show that our methodology can privately detect concept drift, has comparable utility (even better in some cases) with ADWIN and outperforms utility from different levels of differentially private models. The source code for the paper is available \\hyperlink{https://github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models}{here}.","sentences":["Deep neural networks (DNNs) are one of the most widely used machine learning algorithm.","DNNs requires the training data to be available beforehand with true labels.","This is not feasible for many real-world problems where data arrives in the streaming form and acquisition of true labels are scarce and expensive.","In the literature, not much focus has been given to the privacy prospect of the streaming data, where data may change its distribution frequently.","These concept drifts must be detected privately in order to avoid any disclosure risk from DNNs.","Existing privacy models use concept drift detection schemes such ADWIN, KSWIN to detect the drifts.","In this paper, we focus on the notion of integrally private DNNs to detect concept drifts.","Integrally private DNNs are the models which recur frequently from different datasets.","Based on this, we introduce an ensemble methodology which we call 'Integrally Private Drift Detection' (IPDD) method to detect concept drift from private models.","Our IPDD method does not require labels to detect drift but assumes true labels are available once the drift has been detected.","We have experimented with binary and multi-class synthetic and real-world data.","Our experimental results show that our methodology can privately detect concept drift, has comparable utility (even better in some cases) with ADWIN and outperforms utility from different levels of differentially private models.","The source code for the paper is available \\hyperlink{https://github.com/Ayush-Umu/Concept-drift-detection-Using-Integrally-private-models}{here}."],"url":"http://arxiv.org/abs/2406.04903v1","category":"cs.LG"}
{"created":"2024-06-07 11:57:56","title":"Shaking and Tumbling: Short- and Long-Timescale Mechanisms for Resurfacing of Near-Earth Asteroid Surfaces from Planetary Tides and Predictions for the 2029 Earth Encounter by (99942) Apophis","abstract":"Spectral characterization of near-Earth asteroids (NEAs) has revealed a continuum of space-weathered states for the surfaces of S-complex NEAs, with Q-class NEAs, an S-complex subclass, most closely matching the un-weathered surfaces of ordinary chondrite meteorites. Dynamical calculations of the orbital evolution of S-complex NEAs revealed that Q-class NEAs tend to have close encounters with terrestrial planets, suggesting that planetary tides may play a role in refreshing NEA surfaces. However, the exact physical mechanism(s) that drive resurfacing through tidal encounters and the encounter distance at which these mechanisms are effective, has remained unclear. Through the lens of the upcoming (99942) Apophis encounter with Earth in 2029, we investigate the potential for surface mobilization through tidally-driven seismic shaking over short-timescales during encounter and subsequent surface slope evolution over longer-timescales driven by tumbling. We perform multi-scale numerical modeling and find that the 2029 encounter will induce short-term tidally-driven discrete seismic events that lead to high-frequency (greater than 0.1 Hz) surface accelerations that reach magnitudes similar to Apophis' gravity, and that may be detectable by modern seismometers. It is still unclear if the shaking we model translates to widespread particle mobilization and/or lofting. We also find there will be a significant change in Apophis' tumbling spin state that could lead to longer-term surface refreshing in response to tumbling-induced surface slope changes. We propose that through these mechanisms, space-weathered S-class asteroid surfaces may become refreshed through the exposure of unweathered underlying material. These results will be tested by the future exploration of Apophis by NASA's OSIRIS-APEX.","sentences":["Spectral characterization of near-Earth asteroids (NEAs) has revealed a continuum of space-weathered states for the surfaces of S-complex NEAs, with Q-class NEAs, an S-complex subclass, most closely matching the un-weathered surfaces of ordinary chondrite meteorites.","Dynamical calculations of the orbital evolution of S-complex NEAs revealed that Q-class NEAs tend to have close encounters with terrestrial planets, suggesting that planetary tides may play a role in refreshing NEA surfaces.","However, the exact physical mechanism(s) that drive resurfacing through tidal encounters and the encounter distance at which these mechanisms are effective, has remained unclear.","Through the lens of the upcoming (99942)","Apophis encounter with Earth in 2029, we investigate the potential for surface mobilization through tidally-driven seismic shaking over short-timescales during encounter and subsequent surface slope evolution over longer-timescales driven by tumbling.","We perform multi-scale numerical modeling and find that the 2029 encounter will induce short-term tidally-driven discrete seismic events that lead to high-frequency (greater than 0.1 Hz) surface accelerations that reach magnitudes similar to Apophis' gravity, and that may be detectable by modern seismometers.","It is still unclear if the shaking we model translates to widespread particle mobilization and/or lofting.","We also find there will be a significant change in Apophis' tumbling spin state that could lead to longer-term surface refreshing in response to tumbling-induced surface slope changes.","We propose that through these mechanisms, space-weathered S-class asteroid surfaces may become refreshed through the exposure of unweathered underlying material.","These results will be tested by the future exploration of Apophis by NASA's OSIRIS-APEX."],"url":"http://arxiv.org/abs/2406.04864v1","category":"astro-ph.EP"}
{"created":"2024-06-07 11:48:47","title":"Normal-guided Detail-Preserving Neural Implicit Functions for High-Fidelity 3D Surface Reconstruction","abstract":"Neural implicit representations have emerged as a powerful paradigm for 3D reconstruction. However, despite their success, existing methods fail to capture fine geometric details and thin structures, especially in scenarios where only sparse RGB views of the objects of interest are available. We hypothesize that current methods for learning neural implicit representations from RGB or RGBD images produce 3D surfaces with missing parts and details because they only rely on 0-order differential properties, i.e. the 3D surface points and their projections, as supervisory signals. Such properties, however, do not capture the local 3D geometry around the points and also ignore the interactions between points. This paper demonstrates that training neural representations with first-order differential properties, i.e. surface normals, leads to highly accurate 3D surface reconstruction even in situations where only as few as two RGB (front and back) images are available. Given multiview RGB images of an object of interest, we first compute the approximate surface normals in the image space using the gradient of the depth maps produced using an off-the-shelf monocular depth estimator such as Depth Anything model. An implicit surface regressor is then trained using a loss function that enforces the first-order differential properties of the regressed surface to match those estimated from Depth Anything. Our extensive experiments on a wide range of real and synthetic datasets show that the proposed method achieves an unprecedented level of reconstruction accuracy even when using as few as two RGB views. The detailed ablation study also demonstrates that normal-based supervision plays a key role in this significant improvement in performance, enabling the 3D reconstruction of intricate geometric details and thin structures that were previously challenging to capture.","sentences":["Neural implicit representations have emerged as a powerful paradigm for 3D reconstruction.","However, despite their success, existing methods fail to capture fine geometric details and thin structures, especially in scenarios where only sparse RGB views of the objects of interest are available.","We hypothesize that current methods for learning neural implicit representations from RGB or RGBD images produce 3D surfaces with missing parts and details because they only rely on 0-order differential properties, i.e. the 3D surface points and their projections, as supervisory signals.","Such properties, however, do not capture the local 3D geometry around the points and also ignore the interactions between points.","This paper demonstrates that training neural representations with first-order differential properties, i.e. surface normals, leads to highly accurate 3D surface reconstruction even in situations where only as few as two RGB (front and back) images are available.","Given multiview RGB images of an object of interest, we first compute the approximate surface normals in the image space using the gradient of the depth maps produced using an off-the-shelf monocular depth estimator such as Depth Anything model.","An implicit surface regressor is then trained using a loss function that enforces the first-order differential properties of the regressed surface to match those estimated from Depth Anything.","Our extensive experiments on a wide range of real and synthetic datasets show that the proposed method achieves an unprecedented level of reconstruction accuracy even when using as few as two RGB views.","The detailed ablation study also demonstrates that normal-based supervision plays a key role in this significant improvement in performance, enabling the 3D reconstruction of intricate geometric details and thin structures that were previously challenging to capture."],"url":"http://arxiv.org/abs/2406.04861v1","category":"cs.CV"}
{"created":"2024-06-07 11:34:01","title":"Destabilization of Alzheimers Amyloid-beta Protofibrils by Bai-calein: Mechanistic Insights from All-atom Molecular Dynamics Simulations","abstract":"Alzheimer's disease (AD) is a neurodegenerative disorder; it is the most common form of de-mentia and the fifth leading cause of death globally. Aggregation and deposition of neurotoxic A-beta fibrils in the neural tissues of the brain is a key hallmark in the pathogenesis of AD. Desta-bilisation studies of the amyloid-peptide by various natural molecules are of the utmost rele-vance due to their enormous potential as neuroprotective and therapeutic agents for AD. We performed molecular dynamics (MD) simulation on the U-shaped pentamers of amyloidogenic protofilament intermediates to investigate the destabilisation mechanism in the presence of Bai-calein (BCL), a naturally occurring flavonoid. We found that the BCL molecule formed strong hydrophobic contacts with non-polar residues of the protofibril. Upon binding, it competed with the native hydrophobic contacts of the A-beta protein. BCL loosened the tight packing of the hydrophobic core of the protofibril by disrupting the inter-chain salt bridges and hydrogen bonds. The decrease in the structural stability of A-beta protofibrils was confirmed through the en-hanced root mean square deviation (RMSD), radius of gyration and solvent accessible surface area (SASA), and reduced beta-sheet content. PCA indicated that the presence of the BCL mole-cule intensified protofibril motions, particularly affecting residues in Chain A and B regions. Our findings propose that BCL would be a potent destabiliser of A-beta protofilament, and may be considered as a therapeutic agent in treating AD.","sentences":["Alzheimer's disease (AD) is a neurodegenerative disorder; it is the most common form of de-mentia and the fifth leading cause of death globally.","Aggregation and deposition of neurotoxic A-beta fibrils in the neural tissues of the brain is a key hallmark in the pathogenesis of AD.","Desta-bilisation studies of the amyloid-peptide by various natural molecules are of the utmost rele-vance due to their enormous potential as neuroprotective and therapeutic agents for AD.","We performed molecular dynamics (MD) simulation on the U-shaped pentamers of amyloidogenic protofilament intermediates to investigate the destabilisation mechanism in the presence of Bai-calein (BCL), a naturally occurring flavonoid.","We found that the BCL molecule formed strong hydrophobic contacts with non-polar residues of the protofibril.","Upon binding, it competed with the native hydrophobic contacts of the A-beta protein.","BCL loosened the tight packing of the hydrophobic core of the protofibril by disrupting the inter-chain salt bridges and hydrogen bonds.","The decrease in the structural stability of A-beta protofibrils was confirmed through the en-hanced root mean square deviation (RMSD), radius of gyration and solvent accessible surface area (SASA), and reduced beta-sheet content.","PCA indicated that the presence of the BCL mole-cule intensified protofibril motions, particularly affecting residues in Chain A and B regions.","Our findings propose that BCL would be a potent destabiliser of A-beta protofilament, and may be considered as a therapeutic agent in treating AD."],"url":"http://arxiv.org/abs/2406.04852v1","category":"cond-mat.soft"}
{"created":"2024-06-07 10:52:15","title":"Black Box Differential Privacy Auditing Using Total Variation Distance","abstract":"We present a practical method to audit the differential privacy (DP) guarantees of a machine learning model using a small hold-out dataset that is not exposed to the model during the training. Having a score function such as the loss function employed during the training, our method estimates the total variation (TV) distance between scores obtained with a subset of the training data and the hold-out dataset. With some meta information about the underlying DP training algorithm, these TV distance values can be converted to $(\\varepsilon,\\delta)$-guarantees for any $\\delta$. We show that these score distributions asymptotically give lower bounds for the DP guarantees of the underlying training algorithm, however, we perform a one-shot estimation for practicality reasons. We specify conditions that lead to lower bounds for the DP guarantees with high probability. To estimate the TV distance between the score distributions, we use a simple density estimation method based on histograms. We show that the TV distance gives a very close to optimally robust estimator and has an error rate $\\mathcal{O}(k^{-1/3})$, where $k$ is the total number of samples. Numerical experiments on benchmark datasets illustrate the effectiveness of our approach and show improvements over baseline methods for black-box auditing.","sentences":["We present a practical method to audit the differential privacy (DP) guarantees of a machine learning model using a small hold-out dataset that is not exposed to the model during the training.","Having a score function such as the loss function employed during the training, our method estimates the total variation (TV) distance between scores obtained with a subset of the training data and the hold-out dataset.","With some meta information about the underlying DP training algorithm, these TV distance values can be converted to $(\\varepsilon,\\delta)$-guarantees for any $\\delta$. We show that these score distributions asymptotically give lower bounds for the DP guarantees of the underlying training algorithm, however, we perform a one-shot estimation for practicality reasons.","We specify conditions that lead to lower bounds for the DP guarantees with high probability.","To estimate the TV distance between the score distributions, we use a simple density estimation method based on histograms.","We show that the TV distance gives a very close to optimally robust estimator and has an error rate $\\mathcal{O}(k^{-1/3})$, where $k$ is the total number of samples.","Numerical experiments on benchmark datasets illustrate the effectiveness of our approach and show improvements over baseline methods for black-box auditing."],"url":"http://arxiv.org/abs/2406.04827v1","category":"cs.LG"}
{"created":"2024-06-07 17:44:48","title":"The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint Incremental Learning of Homogeneous Tasks","abstract":"Recent research identified a temporary performance drop on previously learned tasks when transitioning to a new one. This drop is called the stability gap and has great consequences for continual learning: it complicates the direct employment of continually learning since the worse-case performance at task-boundaries is dramatic, it limits its potential as an energy-efficient training paradigm, and finally, the stability drop could result in a reduced final performance of the algorithm. In this paper, we show that the stability gap also occurs when applying joint incremental training of homogeneous tasks. In this scenario, the learner continues training on the same data distribution and has access to all data from previous tasks. In addition, we show that in this scenario, there exists a low-loss linear path to the next minima, but that SGD optimization does not choose this path. We perform further analysis including a finer batch-wise analysis which could provide insights towards potential solution directions.","sentences":["Recent research identified a temporary performance drop on previously learned tasks when transitioning to a new one.","This drop is called the stability gap and has great consequences for continual learning: it complicates the direct employment of continually learning since the worse-case performance at task-boundaries is dramatic, it limits its potential as an energy-efficient training paradigm, and finally, the stability drop could result in a reduced final performance of the algorithm.","In this paper, we show that the stability gap also occurs when applying joint incremental training of homogeneous tasks.","In this scenario, the learner continues training on the same data distribution and has access to all data from previous tasks.","In addition, we show that in this scenario, there exists a low-loss linear path to the next minima, but that SGD optimization does not choose this path.","We perform further analysis including a finer batch-wise analysis which could provide insights towards potential solution directions."],"url":"http://arxiv.org/abs/2406.05114v1","category":"cs.LG"}
{"created":"2024-06-07 16:45:53","title":"Hibou: A Family of Foundational Vision Transformers for Pathology","abstract":"Pathology, the microscopic examination of diseased tissue, is critical for diagnosing various medical conditions, particularly cancers. Traditional methods are labor-intensive and prone to human error. Digital pathology, which converts glass slides into high-resolution digital images for analysis by computer algorithms, revolutionizes the field by enhancing diagnostic accuracy, consistency, and efficiency through automated image analysis and large-scale data processing. Foundational transformer pretraining is crucial for developing robust, generalizable models as it enables learning from vast amounts of unannotated data.   This paper introduces the Hibou family of foundational vision transformers for pathology, leveraging the DINOv2 framework to pretrain two model variants, Hibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide images (WSIs) representing diverse tissue types and staining techniques. Our pretrained models demonstrate superior performance on both patch-level and slide-level benchmarks, surpassing existing state-of-the-art methods. Notably, Hibou-L achieves the highest average accuracy across multiple benchmark datasets. To support further research and application in the field, we have open-sourced the Hibou-B model, which can be accessed at https://github.com/HistAI/hibou","sentences":["Pathology, the microscopic examination of diseased tissue, is critical for diagnosing various medical conditions, particularly cancers.","Traditional methods are labor-intensive and prone to human error.","Digital pathology, which converts glass slides into high-resolution digital images for analysis by computer algorithms, revolutionizes the field by enhancing diagnostic accuracy, consistency, and efficiency through automated image analysis and large-scale data processing.","Foundational transformer pretraining is crucial for developing robust, generalizable models as it enables learning from vast amounts of unannotated data.   ","This paper introduces the Hibou family of foundational vision transformers for pathology, leveraging the DINOv2 framework to pretrain two model variants, Hibou-B and Hibou-L, on a proprietary dataset of over 1 million whole slide images (WSIs) representing diverse tissue types and staining techniques.","Our pretrained models demonstrate superior performance on both patch-level and slide-level benchmarks, surpassing existing state-of-the-art methods.","Notably, Hibou-L achieves the highest average accuracy across multiple benchmark datasets.","To support further research and application in the field, we have open-sourced the Hibou-B model, which can be accessed at https://github.com/HistAI/hibou"],"url":"http://arxiv.org/abs/2406.05074v1","category":"eess.IV"}
{"created":"2024-06-07 16:36:50","title":"Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition","abstract":"The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics. However, SER models might contain social bias toward gender, leading to unfair outcomes. This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it. SSL-based SER models are chosen for their cutting-edge performance. Our research pioneering research gender bias in SER from both upstream model and data perspectives. Our findings reveal that females exhibit slightly higher overall SER performance than males. Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias. Moreover, models trained with Mandarin datasets display a pronounced bias toward valence. Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact.","sentences":["The rapid growth of Speech Emotion Recognition (SER) has diverse global applications, from improving human-computer interactions to aiding mental health diagnostics.","However, SER models might contain social bias toward gender, leading to unfair outcomes.","This study analyzes gender bias in SER models trained with Self-Supervised Learning (SSL) at scale, exploring factors influencing it.","SSL-based SER models are chosen for their cutting-edge performance.","Our research pioneering research gender bias in SER from both upstream model and data perspectives.","Our findings reveal that females exhibit slightly higher overall SER performance than males.","Modified CPC and XLS-R, two well-known SSL models, notably exhibit significant bias.","Moreover, models trained with Mandarin datasets display a pronounced bias toward valence.","Lastly, we find that gender-wise emotion distribution differences in training data significantly affect gender bias, while upstream model representation has a limited impact."],"url":"http://arxiv.org/abs/2406.05065v1","category":"eess.AS"}
{"created":"2024-06-07 15:38:23","title":"Predictability Limit of the 2021 Pacific Northwest Heatwave from Deep-Learning Sensitivity Analysis","abstract":"The traditional method for estimating weather forecast sensitivity to initial conditions uses adjoint models, which are limited to short lead times due to linearization around a control forecast. The advent of deep-learning frameworks enables a new approach using backpropagation and gradient descent to iteratively optimize initial conditions to minimize forecast errors. We apply this approach to forecasts of the June 2021 Pacific Northwest heatwave using the GraphCast model, yielding over 90% reduction in 10-day forecast errors over the Pacific Northwest. Similar improvements are found for Pangu-Weather model forecasts initialized with the GraphCast-derived optimal, suggesting that model error is not an important part of the initial perturbations. Eliminating small scales from the initial perturbations also yields similar forecast improvements. Extending the length of the optimization window, we find forecast improvement to about 23 days, suggesting atmospheric predictability at the upper end of recent estimates.","sentences":["The traditional method for estimating weather forecast sensitivity to initial conditions uses adjoint models, which are limited to short lead times due to linearization around a control forecast.","The advent of deep-learning frameworks enables a new approach using backpropagation and gradient descent to iteratively optimize initial conditions to minimize forecast errors.","We apply this approach to forecasts of the June 2021 Pacific Northwest heatwave using the GraphCast model, yielding over 90% reduction in 10-day forecast errors over the Pacific Northwest.","Similar improvements are found for Pangu-Weather model forecasts initialized with the GraphCast-derived optimal, suggesting that model error is not an important part of the initial perturbations.","Eliminating small scales from the initial perturbations also yields similar forecast improvements.","Extending the length of the optimization window, we find forecast improvement to about 23 days, suggesting atmospheric predictability at the upper end of recent estimates."],"url":"http://arxiv.org/abs/2406.05019v1","category":"physics.ao-ph"}
{"created":"2024-06-07 15:24:38","title":"Root Cause Analysis of Outliers with Missing Structural Knowledge","abstract":"Recent work conceptualized root cause analysis (RCA) of anomalies via quantitative contribution analysis using causal counterfactuals in structural causal models (SCMs). The framework comes with three practical challenges: (1) it requires the causal directed acyclic graph (DAG), together with an SCM, (2) it is statistically ill-posed since it probes regression models in regions of low probability density, (3) it relies on Shapley values which are computationally expensive to find.   In this paper, we propose simplified, efficient methods of root cause analysis when the task is to identify a unique root cause instead of quantitative contribution analysis. Our proposed methods run in linear order of SCM nodes and they require only the causal DAG without counterfactuals. Furthermore, for those use cases where the causal DAG is unknown, we justify the heuristic of identifying root causes as the variables with the highest anomaly score.","sentences":["Recent work conceptualized root cause analysis (RCA) of anomalies via quantitative contribution analysis using causal counterfactuals in structural causal models (SCMs).","The framework comes with three practical challenges: (1) it requires the causal directed acyclic graph (DAG), together with an SCM, (2) it is statistically ill-posed since it probes regression models in regions of low probability density, (3) it relies on Shapley values which are computationally expensive to find.   ","In this paper, we propose simplified, efficient methods of root cause analysis when the task is to identify a unique root cause instead of quantitative contribution analysis.","Our proposed methods run in linear order of SCM nodes and they require only the causal DAG without counterfactuals.","Furthermore, for those use cases where the causal DAG is unknown, we justify the heuristic of identifying root causes as the variables with the highest anomaly score."],"url":"http://arxiv.org/abs/2406.05014v1","category":"stat.ML"}
{"created":"2024-06-07 15:10:33","title":"ProMotion: Prototypes As Motion Learners","abstract":"In this work, we introduce ProMotion, a unified prototypical framework engineered to model fundamental motion tasks. ProMotion offers a range of compelling attributes that set it apart from current task-specific paradigms. We adopt a prototypical perspective, establishing a unified paradigm that harmonizes disparate motion learning approaches. This novel paradigm streamlines the architectural design, enabling the simultaneous assimilation of diverse motion information. We capitalize on a dual mechanism involving the feature denoiser and the prototypical learner to decipher the intricacies of motion. This approach effectively circumvents the pitfalls of ambiguity in pixel-wise feature matching, significantly bolstering the robustness of motion representation. We demonstrate a profound degree of transferability across distinct motion patterns. This inherent versatility reverberates robustly across a comprehensive spectrum of both 2D and 3D downstream tasks. Empirical results demonstrate that ProMotion outperforms various well-known specialized architectures, achieving 0.54 and 0.054 Abs Rel error on the Sintel and KITTI depth datasets, 1.04 and 2.01 average endpoint error on the clean and final pass of Sintel flow benchmark, and 4.30 F1-all error on the KITTI flow benchmark. For its efficacy, we hope our work can catalyze a paradigm shift in universal models in computer vision.","sentences":["In this work, we introduce ProMotion, a unified prototypical framework engineered to model fundamental motion tasks.","ProMotion offers a range of compelling attributes that set it apart from current task-specific paradigms.","We adopt a prototypical perspective, establishing a unified paradigm that harmonizes disparate motion learning approaches.","This novel paradigm streamlines the architectural design, enabling the simultaneous assimilation of diverse motion information.","We capitalize on a dual mechanism involving the feature denoiser and the prototypical learner to decipher the intricacies of motion.","This approach effectively circumvents the pitfalls of ambiguity in pixel-wise feature matching, significantly bolstering the robustness of motion representation.","We demonstrate a profound degree of transferability across distinct motion patterns.","This inherent versatility reverberates robustly across a comprehensive spectrum of both 2D and 3D downstream tasks.","Empirical results demonstrate that ProMotion outperforms various well-known specialized architectures, achieving 0.54 and 0.054 Abs Rel error on the Sintel and KITTI depth datasets, 1.04 and 2.01 average endpoint error on the clean and final pass of Sintel flow benchmark, and 4.30 F1-all error on the KITTI flow benchmark.","For its efficacy, we hope our work can catalyze a paradigm shift in universal models in computer vision."],"url":"http://arxiv.org/abs/2406.04999v1","category":"cs.CV"}
{"created":"2024-06-07 14:41:24","title":"Semantic Segmentation on VSPW Dataset through Masked Video Consistency","abstract":"Pixel-level Video Understanding requires effectively integrating three-dimensional data in both spatial and temporal dimensions to learn accurate and stable semantic information from continuous frames. However, existing advanced models on the VSPW dataset have not fully modeled spatiotemporal relationships. In this paper, we present our solution for the PVUW competition, where we introduce masked video consistency (MVC) based on existing models. MVC enforces the consistency between predictions of masked frames where random patches are withheld. The model needs to learn the segmentation results of the masked parts through the context of images and the relationship between preceding and succeeding frames of the video. Additionally, we employed test-time augmentation, model aggeregation and a multimodal model-based post-processing method. Our approach achieves 67.27% mIoU performance on the VSPW dataset, ranking 2nd place in the PVUW2024 challenge VSS track.","sentences":["Pixel-level Video Understanding requires effectively integrating three-dimensional data in both spatial and temporal dimensions to learn accurate and stable semantic information from continuous frames.","However, existing advanced models on the VSPW dataset have not fully modeled spatiotemporal relationships.","In this paper, we present our solution for the PVUW competition, where we introduce masked video consistency (MVC) based on existing models.","MVC enforces the consistency between predictions of masked frames where random patches are withheld.","The model needs to learn the segmentation results of the masked parts through the context of images and the relationship between preceding and succeeding frames of the video.","Additionally, we employed test-time augmentation, model aggeregation and a multimodal model-based post-processing method.","Our approach achieves 67.27% mIoU performance on the VSPW dataset, ranking 2nd place in the PVUW2024 challenge VSS track."],"url":"http://arxiv.org/abs/2406.04979v1","category":"cs.CV"}
{"created":"2024-06-07 13:35:44","title":"MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers","abstract":"Recent advances in pre-trained vision transformers have shown promise in parameter-efficient audio-visual learning without audio pre-training. However, few studies have investigated effective methods for aligning multimodal features in parameter-efficient audio-visual transformers. In this paper, we propose MA-AVT, a new parameter-efficient audio-visual transformer employing deep modality alignment for corresponding multimodal semantic features. Specifically, we introduce joint unimodal and multimodal token learning for aligning the two modalities with a frozen modality-shared transformer. This allows the model to learn separate representations for each modality, while also attending to the cross-modal relationships between them. In addition, unlike prior work that only aligns coarse features from the output of unimodal encoders, we introduce blockwise contrastive learning to align coarse-to-fine-grain hierarchical features throughout the encoding phase. Furthermore, to suppress the background features in each modality from foreground matched audio-visual features, we introduce a robust discriminative foreground mining scheme. Through extensive experiments on benchmark AVE, VGGSound, and CREMA-D datasets, we achieve considerable performance improvements over SOTA methods.","sentences":["Recent advances in pre-trained vision transformers have shown promise in parameter-efficient audio-visual learning without audio pre-training.","However, few studies have investigated effective methods for aligning multimodal features in parameter-efficient audio-visual transformers.","In this paper, we propose MA-AVT, a new parameter-efficient audio-visual transformer employing deep modality alignment for corresponding multimodal semantic features.","Specifically, we introduce joint unimodal and multimodal token learning for aligning the two modalities with a frozen modality-shared transformer.","This allows the model to learn separate representations for each modality, while also attending to the cross-modal relationships between them.","In addition, unlike prior work that only aligns coarse features from the output of unimodal encoders, we introduce blockwise contrastive learning to align coarse-to-fine-grain hierarchical features throughout the encoding phase.","Furthermore, to suppress the background features in each modality from foreground matched audio-visual features, we introduce a robust discriminative foreground mining scheme.","Through extensive experiments on benchmark AVE, VGGSound, and CREMA-D datasets, we achieve considerable performance improvements over SOTA methods."],"url":"http://arxiv.org/abs/2406.04930v1","category":"cs.CV"}
{"created":"2024-06-07 13:34:17","title":"AGBD: A Global-scale Biomass Dataset","abstract":"Accurate estimates of Above Ground Biomass (AGB) are essential in addressing two of humanity's biggest challenges, climate change and biodiversity loss. Existing datasets for AGB estimation from satellite imagery are limited. Either they focus on specific, local regions at high resolution, or they offer global coverage at low resolution. There is a need for a machine learning-ready, globally representative, high-resolution benchmark. Our findings indicate significant variability in biomass estimates across different vegetation types, emphasizing the necessity for a dataset that accurately captures global diversity. To address these gaps, we introduce a comprehensive new dataset that is globally distributed, covers a range of vegetation types, and spans several years. This dataset combines AGB reference data from the GEDI mission with data from Sentinel-2 and PALSAR-2 imagery. Additionally, it includes pre-processed high-level features such as a dense canopy height map, an elevation map, and a land-cover classification map. We also produce a dense, high-resolution (10m) map of AGB predictions for the entire area covered by the dataset. Rigorously tested, our dataset is accompanied by several benchmark models and is publicly available. It can be easily accessed using a single line of code, offering a solid basis for efforts towards global AGB estimation. The GitHub repository github.com/ghjuliasialelli/AGBD serves as a one-stop shop for all code and data.","sentences":["Accurate estimates of Above Ground Biomass (AGB) are essential in addressing two of humanity's biggest challenges, climate change and biodiversity loss.","Existing datasets for AGB estimation from satellite imagery are limited.","Either they focus on specific, local regions at high resolution, or they offer global coverage at low resolution.","There is a need for a machine learning-ready, globally representative, high-resolution benchmark.","Our findings indicate significant variability in biomass estimates across different vegetation types, emphasizing the necessity for a dataset that accurately captures global diversity.","To address these gaps, we introduce a comprehensive new dataset that is globally distributed, covers a range of vegetation types, and spans several years.","This dataset combines AGB reference data from the GEDI mission with data from Sentinel-2 and PALSAR-2 imagery.","Additionally, it includes pre-processed high-level features such as a dense canopy height map, an elevation map, and a land-cover classification map.","We also produce a dense, high-resolution (10m) map of AGB predictions for the entire area covered by the dataset.","Rigorously tested, our dataset is accompanied by several benchmark models and is publicly available.","It can be easily accessed using a single line of code, offering a solid basis for efforts towards global AGB estimation.","The GitHub repository github.com/ghjuliasialelli/AGBD serves as a one-stop shop for all code and data."],"url":"http://arxiv.org/abs/2406.04928v1","category":"cs.CV"}
{"created":"2024-06-07 12:45:30","title":"Labeled Data Selection for Category Discovery","abstract":"Category discovery methods aim to find novel categories in unlabeled visual data. At training time, a set of labeled and unlabeled images are provided, where the labels correspond to the categories present in the images. The labeled data provides guidance during training by indicating what types of visual properties and features are relevant for performing discovery in the unlabeled data. As a result, changing the categories present in the labeled set can have a large impact on what is ultimately discovered in the unlabeled set. Despite its importance, the impact of labeled data selection has not been explored in the category discovery literature to date. We show that changing the labeled data can significantly impact discovery performance. Motivated by this, we propose two new approaches for automatically selecting the most suitable labeled data based on the similarity between the labeled and unlabeled data. Our observation is that, unlike in conventional supervised transfer learning, the best labeled is neither too similar, nor too dissimilar, to the unlabeled categories. Our resulting approaches obtains state-of-the-art discovery performance across a range of challenging fine-grained benchmark datasets.","sentences":["Category discovery methods aim to find novel categories in unlabeled visual data.","At training time, a set of labeled and unlabeled images are provided, where the labels correspond to the categories present in the images.","The labeled data provides guidance during training by indicating what types of visual properties and features are relevant for performing discovery in the unlabeled data.","As a result, changing the categories present in the labeled set can have a large impact on what is ultimately discovered in the unlabeled set.","Despite its importance, the impact of labeled data selection has not been explored in the category discovery literature to date.","We show that changing the labeled data can significantly impact discovery performance.","Motivated by this, we propose two new approaches for automatically selecting the most suitable labeled data based on the similarity between the labeled and unlabeled data.","Our observation is that, unlike in conventional supervised transfer learning, the best labeled is neither too similar, nor too dissimilar, to the unlabeled categories.","Our resulting approaches obtains state-of-the-art discovery performance across a range of challenging fine-grained benchmark datasets."],"url":"http://arxiv.org/abs/2406.04898v1","category":"cs.CV"}
{"created":"2024-06-07 12:45:12","title":"From Link Prediction to Forecasting: Information Loss in Batch-based Temporal Graph Learning","abstract":"Dynamic link prediction is an important problem considered by many recent works proposing various approaches for learning temporal edge patterns. To assess their efficacy, models are evaluated on publicly available benchmark datasets involving continuous-time and discrete-time temporal graphs. However, as we show in this work, the suitability of common batch-oriented evaluation depends on the datasets' characteristics, which can cause two issues: First, for continuous-time temporal graphs, fixed-size batches create time windows with different durations, resulting in an inconsistent dynamic link prediction task. Second, for discrete-time temporal graphs, the sequence of batches can additionally introduce temporal dependencies that are not present in the data. In this work, we empirically show that this common evaluation approach leads to skewed model performance and hinders the fair comparison of methods. We mitigate this problem by reformulating dynamic link prediction as a link forecasting task that better accounts for temporal information present in the data. We provide implementations of our new evaluation method for commonly used graph learning frameworks.","sentences":["Dynamic link prediction is an important problem considered by many recent works proposing various approaches for learning temporal edge patterns.","To assess their efficacy, models are evaluated on publicly available benchmark datasets involving continuous-time and discrete-time temporal graphs.","However, as we show in this work, the suitability of common batch-oriented evaluation depends on the datasets' characteristics, which can cause two issues: First, for continuous-time temporal graphs, fixed-size batches create time windows with different durations, resulting in an inconsistent dynamic link prediction task.","Second, for discrete-time temporal graphs, the sequence of batches can additionally introduce temporal dependencies that are not present in the data.","In this work, we empirically show that this common evaluation approach leads to skewed model performance and hinders the fair comparison of methods.","We mitigate this problem by reformulating dynamic link prediction as a link forecasting task that better accounts for temporal information present in the data.","We provide implementations of our new evaluation method for commonly used graph learning frameworks."],"url":"http://arxiv.org/abs/2406.04897v1","category":"cs.LG"}
{"created":"2024-06-07 12:39:54","title":"Sexism Detection on a Data Diet","abstract":"There is an increase in the proliferation of online hate commensurate with the rise in the usage of social media. In response, there is also a significant advancement in the creation of automated tools aimed at identifying harmful text content using approaches grounded in Natural Language Processing and Deep Learning. Although it is known that training Deep Learning models require a substantial amount of annotated data, recent line of work suggests that models trained on specific subsets of the data still retain performance comparable to the model that was trained on the full dataset. In this work, we show how we can leverage influence scores to estimate the importance of a data point while training a model and designing a pruning strategy applied to the case of sexism detection. We evaluate the model performance trained on data pruned with different pruning strategies on three out-of-domain datasets and find, that in accordance with other work a large fraction of instances can be removed without significant performance drop. However, we also discover that the strategies for pruning data, previously successful in Natural Language Inference tasks, do not readily apply to the detection of harmful content and instead amplify the already prevalent class imbalance even more, leading in the worst-case to a complete absence of the hateful class.","sentences":["There is an increase in the proliferation of online hate commensurate with the rise in the usage of social media.","In response, there is also a significant advancement in the creation of automated tools aimed at identifying harmful text content using approaches grounded in Natural Language Processing and Deep Learning.","Although it is known that training Deep Learning models require a substantial amount of annotated data, recent line of work suggests that models trained on specific subsets of the data still retain performance comparable to the model that was trained on the full dataset.","In this work, we show how we can leverage influence scores to estimate the importance of a data point while training a model and designing a pruning strategy applied to the case of sexism detection.","We evaluate the model performance trained on data pruned with different pruning strategies on three out-of-domain datasets and find, that in accordance with other work a large fraction of instances can be removed without significant performance drop.","However, we also discover that the strategies for pruning data, previously successful in Natural Language Inference tasks, do not readily apply to the detection of harmful content and instead amplify the already prevalent class imbalance even more, leading in the worst-case to a complete absence of the hateful class."],"url":"http://arxiv.org/abs/2406.04892v1","category":"cs.CL"}
{"created":"2024-06-07 11:45:31","title":"Multi-View Stochastic Block Models","abstract":"Graph clustering is a central topic in unsupervised learning with a multitude of practical applications. In recent years, multi-view graph clustering has gained a lot of attention for its applicability to real-world instances where one has access to multiple data sources. In this paper we formalize a new family of models, called \\textit{multi-view stochastic block models} that captures this setting.   For this model, we first study efficient algorithms that naively work on the union of multiple graphs. Then, we introduce a new efficient algorithm that provably outperforms previous approaches by analyzing the structure of each graph separately. Furthermore, we complement our results with an information-theoretic lower bound studying the limits of what can be done in this model. Finally, we corroborate our results with experimental evaluations.","sentences":["Graph clustering is a central topic in unsupervised learning with a multitude of practical applications.","In recent years, multi-view graph clustering has gained a lot of attention for its applicability to real-world instances where one has access to multiple data sources.","In this paper we formalize a new family of models, called \\textit{multi-view stochastic block models} that captures this setting.   ","For this model, we first study efficient algorithms that naively work on the union of multiple graphs.","Then, we introduce a new efficient algorithm that provably outperforms previous approaches by analyzing the structure of each graph separately.","Furthermore, we complement our results with an information-theoretic lower bound studying the limits of what can be done in this model.","Finally, we corroborate our results with experimental evaluations."],"url":"http://arxiv.org/abs/2406.04860v1","category":"cs.LG"}
{"created":"2024-06-07 11:18:40","title":"Multi-Granularity Language-Guided Multi-Object Tracking","abstract":"Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance. While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance. In this work, we argue that multi-modal language-driven features provide complementary information to classical visual features, thereby aiding in improving the robustness to such environmental interference. To this end, we propose a new multi-object tracking framework, named LG-MOT, that explicitly leverages language information at different levels of granularity (scene-and instance-level) and combines it with standard visual features to obtain discriminative representations. To develop LG-MOT, we annotate existing MOT datasets with scene-and instance-level language descriptions. We then encode both instance-and scene-level language information into high-dimensional embeddings, which are utilized to guide the visual features during training. At inference, our LG-MOT uses the standard visual features without relying on annotated language descriptions. Extensive experiments on three benchmarks, MOT17, DanceTrack and SportsMOT, reveal the merits of the proposed contributions leading to state-of-the-art performance. On the DanceTrack test set, our LG-MOT achieves an absolute gain of 2.2\\% in terms of target object association (IDF1 score), compared to the baseline using only visual features. Further, our LG-MOT exhibits strong cross-domain generalizability. The dataset and code will be available at ~\\url{https://github.com/WesLee88524/LG-MOT}.","sentences":["Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance.","While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance.","In this work, we argue that multi-modal language-driven features provide complementary information to classical visual features, thereby aiding in improving the robustness to such environmental interference.","To this end, we propose a new multi-object tracking framework, named LG-MOT, that explicitly leverages language information at different levels of granularity (scene-and instance-level) and combines it with standard visual features to obtain discriminative representations.","To develop LG-MOT, we annotate existing MOT datasets with scene-and instance-level language descriptions.","We then encode both instance-and scene-level language information into high-dimensional embeddings, which are utilized to guide the visual features during training.","At inference, our LG-MOT uses the standard visual features without relying on annotated language descriptions.","Extensive experiments on three benchmarks, MOT17, DanceTrack and SportsMOT, reveal the merits of the proposed contributions leading to state-of-the-art performance.","On the DanceTrack test set, our LG-MOT achieves an absolute gain of 2.2\\% in terms of target object association (IDF1 score), compared to the baseline using only visual features.","Further, our LG-MOT exhibits strong cross-domain generalizability.","The dataset and code will be available at ~\\url{https://github.com/WesLee88524/LG-MOT}."],"url":"http://arxiv.org/abs/2406.04844v1","category":"cs.CV"}
{"created":"2024-06-07 11:06:26","title":"SLR: Learning Quadruped Locomotion without Privileged Information","abstract":"Traditional reinforcement learning control for quadruped robots often relies on privileged information, demanding meticulous selection and precise estimation, thereby imposing constraints on the development process. This work proposes a Self-learning Latent Representation (SLR) method, which achieves high-performance control policy learning without the need for privileged information. To enhance the credibility of our proposed method's evaluation, SLR is compared with open-source code repositories of state-of-the-art algorithms, retaining the original authors' configuration parameters. Across four repositories, SLR consistently outperforms the reference results. Ultimately, the trained policy and encoder empower the quadruped robot to navigate steps, climb stairs, ascend rocks, and traverse various challenging terrains. Robot experiment videos are at https://11chens.github.io/SLR/","sentences":["Traditional reinforcement learning control for quadruped robots often relies on privileged information, demanding meticulous selection and precise estimation, thereby imposing constraints on the development process.","This work proposes a Self-learning Latent Representation (SLR) method, which achieves high-performance control policy learning without the need for privileged information.","To enhance the credibility of our proposed method's evaluation, SLR is compared with open-source code repositories of state-of-the-art algorithms, retaining the original authors' configuration parameters.","Across four repositories, SLR consistently outperforms the reference results.","Ultimately, the trained policy and encoder empower the quadruped robot to navigate steps, climb stairs, ascend rocks, and traverse various challenging terrains.","Robot experiment videos are at https://11chens.github.io/SLR/"],"url":"http://arxiv.org/abs/2406.04835v1","category":"cs.RO"}
{"created":"2024-06-07 10:37:14","title":"Experiences from Integrating Large Language Model Chatbots into the Classroom","abstract":"In the present study, we provided students an unfiltered access to a state-of-the-art large language model (LLM) chatbot. The chatbot was intentionally designed to mimic proprietary commercial chatbots such as ChatGPT where the chatbot has not been tailored for the educational context; the underlying engine was OpenAI GPT-4. The chatbot was integrated into online learning materials of three courses. One of the courses focused on software engineering with LLMs, while the two other courses were not directly related to LLMs. Our results suggest that only a minority of students engage with the chatbot in the courses that do not relate to LLMs. At the same time, unsurprisingly, nearly all students in the LLM-focused course leveraged the chatbot. In all courses, the majority of the LLM usage came from a few superusers, whereas the majority of the students did not heavily use the chatbot even though it was readily available and effectively provided a free access to the OpenAI GPT-4 model. We also observe that in addition to students using the chatbot for course-specific purposes, many use the chatbot for their own purposes. These results suggest that the worst fears of educators -- all students overrelying on LLMs -- did not materialize even when the chatbot access was unfiltered. We finally discuss potential reasons for the low usage, suggesting the need for more tailored and scaffolded LLM experiences targeted for specific types of student use cases.","sentences":["In the present study, we provided students an unfiltered access to a state-of-the-art large language model (LLM) chatbot.","The chatbot was intentionally designed to mimic proprietary commercial chatbots such as ChatGPT where the chatbot has not been tailored for the educational context; the underlying engine was OpenAI GPT-4.","The chatbot was integrated into online learning materials of three courses.","One of the courses focused on software engineering with LLMs, while the two other courses were not directly related to LLMs.","Our results suggest that only a minority of students engage with the chatbot in the courses that do not relate to LLMs.","At the same time, unsurprisingly, nearly all students in the LLM-focused course leveraged the chatbot.","In all courses, the majority of the LLM usage came from a few superusers, whereas the majority of the students did not heavily use the chatbot even though it was readily available and effectively provided a free access to the OpenAI GPT-4 model.","We also observe that in addition to students using the chatbot for course-specific purposes, many use the chatbot for their own purposes.","These results suggest that the worst fears of educators -- all students overrelying on LLMs -- did not materialize even when the chatbot access was unfiltered.","We finally discuss potential reasons for the low usage, suggesting the need for more tailored and scaffolded LLM experiences targeted for specific types of student use cases."],"url":"http://arxiv.org/abs/2406.04817v1","category":"cs.CY"}
{"created":"2024-06-07 09:53:55","title":"Multi-Label Requirements Classification with Large Taxonomies","abstract":"Classification aids software development activities by organizing requirements in classes for easier access and retrieval. The majority of requirements classification research has, so far, focused on binary or multi-class classification. Multi-label classification with large taxonomies could aid requirements traceability but is prohibitively costly with supervised training. Hence, we investigate zero-short learning to evaluate the feasibility of multi-label requirements classification with large taxonomies. We associated, together with domain experts from the industry, 129 requirements with 769 labels from taxonomies ranging between 250 and 1183 classes. Then, we conducted a controlled experiment to study the impact of the type of classifier, the hierarchy, and the structural characteristics of taxonomies on the classification performance. The results show that: (1) The sentence-based classifier had a significantly higher recall compared to the word-based classifier; however, the precision and F1-score did not improve significantly. (2) The hierarchical classification strategy did not always improve the performance of requirements classification. (3) The total and leaf nodes of the taxonomies have a strong negative correlation with the recall of the hierarchical sentence-based classifier. We investigate the problem of multi-label requirements classification with large taxonomies, illustrate a systematic process to create a ground truth involving industry participants, and provide an analysis of different classification pipelines using zero-shot learning.","sentences":["Classification aids software development activities by organizing requirements in classes for easier access and retrieval.","The majority of requirements classification research has, so far, focused on binary or multi-class classification.","Multi-label classification with large taxonomies could aid requirements traceability but is prohibitively costly with supervised training.","Hence, we investigate zero-short learning to evaluate the feasibility of multi-label requirements classification with large taxonomies.","We associated, together with domain experts from the industry, 129 requirements with 769 labels from taxonomies ranging between 250 and 1183 classes.","Then, we conducted a controlled experiment to study the impact of the type of classifier, the hierarchy, and the structural characteristics of taxonomies on the classification performance.","The results show that: (1) The sentence-based classifier had a significantly higher recall compared to the word-based classifier; however, the precision and F1-score did not improve significantly.","(2) The hierarchical classification strategy did not always improve the performance of requirements classification.","(3) The total and leaf nodes of the taxonomies have a strong negative correlation with the recall of the hierarchical sentence-based classifier.","We investigate the problem of multi-label requirements classification with large taxonomies, illustrate a systematic process to create a ground truth involving industry participants, and provide an analysis of different classification pipelines using zero-shot learning."],"url":"http://arxiv.org/abs/2406.04797v1","category":"cs.SE"}
{"created":"2024-06-07 09:48:11","title":"Robust Inference of Dynamic Covariance Using Wishart Processes and Sequential Monte Carlo","abstract":"Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time. A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging. In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference. Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance. SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters. We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance","sentences":["Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time.","A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging.","In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference.","Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance.","SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters.","We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance"],"url":"http://arxiv.org/abs/2406.04796v1","category":"stat.ME"}
{"created":"2024-06-07 08:42:26","title":"When Swarm Learning meets energy series data: A decentralized collaborative learning design based on blockchain","abstract":"Machine learning models offer the capability to forecast future energy production or consumption and infer essential unknown variables from existing data. However, legal and policy constraints within specific energy sectors render the data sensitive, presenting technical hurdles in utilizing data from diverse sources. Therefore, we propose adopting a Swarm Learning (SL) scheme, which replaces the centralized server with a blockchain-based distributed network to address the security and privacy issues inherent in Federated Learning (FL)'s centralized architecture. Within this distributed Collaborative Learning framework, each participating organization governs nodes for inter-organizational communication. Devices from various organizations utilize smart contracts for parameter uploading and retrieval. Consensus mechanism ensures distributed consistency throughout the learning process, guarantees the transparent trustworthiness and immutability of parameters on-chain. The efficacy of the proposed framework is substantiated across three real-world energy series modeling scenarios with superior performance compared to Local Learning approaches, simultaneously emphasizing enhanced data security and privacy over Centralized Learning and FL method. Notably, as the number of data volume and the count of local epochs increases within a threshold, there is an improvement in model performance accompanied by a reduction in the variance of performance errors. Consequently, this leads to an increased stability and reliability in the outcomes produced by the model.","sentences":["Machine learning models offer the capability to forecast future energy production or consumption and infer essential unknown variables from existing data.","However, legal and policy constraints within specific energy sectors render the data sensitive, presenting technical hurdles in utilizing data from diverse sources.","Therefore, we propose adopting a Swarm Learning (SL) scheme, which replaces the centralized server with a blockchain-based distributed network to address the security and privacy issues inherent in Federated Learning (FL)'s centralized architecture.","Within this distributed Collaborative Learning framework, each participating organization governs nodes for inter-organizational communication.","Devices from various organizations utilize smart contracts for parameter uploading and retrieval.","Consensus mechanism ensures distributed consistency throughout the learning process, guarantees the transparent trustworthiness and immutability of parameters on-chain.","The efficacy of the proposed framework is substantiated across three real-world energy series modeling scenarios with superior performance compared to Local Learning approaches, simultaneously emphasizing enhanced data security and privacy over Centralized Learning and FL method.","Notably, as the number of data volume and the count of local epochs increases within a threshold, there is an improvement in model performance accompanied by a reduction in the variance of performance errors.","Consequently, this leads to an increased stability and reliability in the outcomes produced by the model."],"url":"http://arxiv.org/abs/2406.04743v1","category":"cs.LG"}
{"created":"2024-06-07 11:57:40","title":"Constructing $3$-Dimensional Monogenic Homogeneous Functions","abstract":"This paper is dedicated to the construction of multidimensional spherical monogenics. Firstly, we investigate the construction of monogenic functions in dimension $3$ by applying the Dirac operator to the orthonormal bases of spherical harmonics, resulting in orthogonal spherical monogenics. Additionally, we employ the reproducing kernel for monogenic functions and a specialized optimization method to derive various types of $3$-dimensional spherical harmonics and spherical monogenics.","sentences":["This paper is dedicated to the construction of multidimensional spherical monogenics.","Firstly, we investigate the construction of monogenic functions in dimension $3$ by applying the Dirac operator to the orthonormal bases of spherical harmonics, resulting in orthogonal spherical monogenics.","Additionally, we employ the reproducing kernel for monogenic functions and a specialized optimization method to derive various types of $3$-dimensional spherical harmonics and spherical monogenics."],"url":"http://arxiv.org/abs/2406.04863v1","category":"math.AP"}
{"created":"2024-06-07 09:16:16","title":"Observation and spectroscopy of proton-unbound nucleus $^{21}$Al","abstract":"We report on the observation of previously-unknown isotope $^{21}$Al, the first unbound aluminum isotope located beyond the proton dripline. The $^{21}$Al nucleus decays by one-proton (1p) emission, and its in-flight decays were detected by tracking trajectories of all decay products with micro-strip silicon detectors. The 1p-emission processes were studied by analyses of the measured angular correlations of decay products $^{20}$Mg+p. The 1p-decay energies of ground and low-lying excited states of $^{21}$Al, its mass excess and proton separation energy value $S_p$=$-1.1(1)$ MeV were determined.","sentences":["We report on the observation of previously-unknown isotope $^{21}$Al, the first unbound aluminum isotope located beyond the proton dripline.","The $^{21}$Al nucleus decays by one-proton (1p) emission, and its in-flight decays were detected by tracking trajectories of all decay products with micro-strip silicon detectors.","The 1p-emission processes were studied by analyses of the measured angular correlations of decay products $^{20}$Mg+p.","The 1p-decay energies of ground and low-lying excited states of $^{21}$Al, its mass excess and proton separation energy value $S_p$=$-1.1(1)$ MeV were determined."],"url":"http://arxiv.org/abs/2406.04771v1","category":"nucl-ex"}
{"created":"2024-06-07 08:36:47","title":"Fast-Fading Channel and Power Optimization of the Magnetic Inductive Cellular Network","abstract":"The cellular network of magnetic Induction (MI) communication holds promise in long-distance underground environments. In the traditional MI communication, there is no fast-fading channel since the MI channel is treated as a quasi-static channel. However, for the vehicle (mobile) MI (VMI) communication, the unpredictable antenna vibration brings the remarkable fast-fading. As such fast-fading cannot be modeled by the central limit theorem, it differs radically from other wireless fast-fading channels. Unfortunately, few studies focus on this phenomenon. In this paper, using a novel space modeling based on the electromagnetic field theorem, we propose a 3-dimension model of the VMI antenna vibration. By proposing ``conjugate pseudo-piecewise functions'' and boundary $p(x)$ distribution, we derive the cumulative distribution function (CDF), probability density function (PDF) and the expectation of the VMI fast-fading channel. We also theoretically analyze the effects of the VMI fast-fading on the network throughput, including the VMI outage probability which can be ignored in the traditional MI channel study. We draw several intriguing conclusions different from those in wireless fast-fading studies. For instance, the fast-fading brings more uniformly distributed channel coefficients. Finally, we propose the power control algorithm using the non-cooperative game and multiagent Q-learning methods to optimize the throughput of the cellular VMI network. Simulations validate the derivation and the proposed algorithm.","sentences":["The cellular network of magnetic Induction (MI) communication holds promise in long-distance underground environments.","In the traditional MI communication, there is no fast-fading channel since the MI channel is treated as a quasi-static channel.","However, for the vehicle (mobile) MI (VMI) communication, the unpredictable antenna vibration brings the remarkable fast-fading.","As such fast-fading cannot be modeled by the central limit theorem, it differs radically from other wireless fast-fading channels.","Unfortunately, few studies focus on this phenomenon.","In this paper, using a novel space modeling based on the electromagnetic field theorem, we propose a 3-dimension model of the VMI antenna vibration.","By proposing ``conjugate pseudo-piecewise functions'' and boundary $p(x)$ distribution, we derive the cumulative distribution function (CDF), probability density function (PDF) and the expectation of the VMI fast-fading channel.","We also theoretically analyze the effects of the VMI fast-fading on the network throughput, including the VMI outage probability which can be ignored in the traditional MI channel study.","We draw several intriguing conclusions different from those in wireless fast-fading studies.","For instance, the fast-fading brings more uniformly distributed channel coefficients.","Finally, we propose the power control algorithm using the non-cooperative game and multiagent Q-learning methods to optimize the throughput of the cellular VMI network.","Simulations validate the derivation and the proposed algorithm."],"url":"http://arxiv.org/abs/2406.04737v1","category":"eess.SP"}
{"created":"2024-06-07 08:24:41","title":"Iterative composition optimization in Fe$_2$VAl-based thin-film thermoelectrics using single-target sputtering","abstract":"Magnetron sputtering inherently exhibits the advantage of dislodging particles from the target in a ratio equivalent to the target stoichiometry. Nevertheless, film compositions often deviate due to element-dependent scattering with the working gas, necessitating the adjustment of the sputtering process. In this work, we explore an unconventional approach of addressing this issue, involving the employment of an off-stoichiometric target. The required composition is obtained through an iterative process, which is demonstrated by Fe$_2$VAl and Fe$_2$V$_{0.9}$Ti$_{0.1}$Al films as case studies. Ultimately, the correct stoichiometry is obtained from Fe$_{1.86}$V$_{1.15}$Al$_{0.99}$ and Fe$_{1.88}$V$_{1.02}$Ti$_{0.13}$Al$_{0.97}$ targets, respectively. Despite the thermoelectric properties falling below expectations, mainly due to imperfect film crystallization, the strategy successfully achieved the desired stoichiometry, enabling accurate film synthesis without the need of advanced sputtering setups.","sentences":["Magnetron sputtering inherently exhibits the advantage of dislodging particles from the target in a ratio equivalent to the target stoichiometry.","Nevertheless, film compositions often deviate due to element-dependent scattering with the working gas, necessitating the adjustment of the sputtering process.","In this work, we explore an unconventional approach of addressing this issue, involving the employment of an off-stoichiometric target.","The required composition is obtained through an iterative process, which is demonstrated by Fe$_2$VAl and Fe$_2$V$_{0.9}$Ti$_{0.1}$Al films as case studies.","Ultimately, the correct stoichiometry is obtained from Fe$_{1.86}$V$_{1.15}$Al$_{0.99}$ and Fe$_{1.88}$V$_{1.02}$Ti$_{0.13}$Al$_{0.97}$ targets, respectively.","Despite the thermoelectric properties falling below expectations, mainly due to imperfect film crystallization, the strategy successfully achieved the desired stoichiometry, enabling accurate film synthesis without the need of advanced sputtering setups."],"url":"http://arxiv.org/abs/2406.04729v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 17:55:08","title":"Eccentricity effects on the SMBH GW background","abstract":"We study how eccentricity affects the gravitational wave (GW) spectrum from supermassive black hole (SMBH) binaries. We develop a fast and accurate semi-analytic method for computing the GW spectra, the distribution for the spectral fluctuations and the correlations between different frequencies. As GW emission circularizes binaries, the suppression of the signal strength due to eccentricity will be relevant for signals from wider binaries emitting at lower frequencies. Such a feature is present in the signal observed at pulsar timing arrays. We find that, when orbital decay of the SMBH binaries is driven by GWs only, the shape of the observed signal prefers highly eccentric binaries $\\langle e \\rangle_{2\\,{\\rm nHz}} = 0.83^{+0.04}_{-0.05}$. However, when environmental effects are included, the initial eccentricity can be significantly lowered yet the scenario with purely circular binaries is mildly disfavored.","sentences":["We study how eccentricity affects the gravitational wave (GW) spectrum from supermassive black hole (SMBH) binaries.","We develop a fast and accurate semi-analytic method for computing the GW spectra, the distribution for the spectral fluctuations and the correlations between different frequencies.","As GW emission circularizes binaries, the suppression of the signal strength due to eccentricity will be relevant for signals from wider binaries emitting at lower frequencies.","Such a feature is present in the signal observed at pulsar timing arrays.","We find that, when orbital decay of the SMBH binaries is driven by GWs only, the shape of the observed signal prefers highly eccentric binaries $\\langle e \\rangle_{2\\,{\\rm nHz}} = 0.83^{+0.04}_{-0.05}$.","However, when environmental effects are included, the initial eccentricity can be significantly lowered yet the scenario with purely circular binaries is mildly disfavored."],"url":"http://arxiv.org/abs/2406.05125v1","category":"astro-ph.CO"}
{"created":"2024-06-07 17:53:37","title":"Born-Oppenheimer Potentials for $SU(3)$ Gauge Theory","abstract":"We develop parameterizations of 8 of the lowest Born-Oppenheimer potentials for quarkonium hybrid mesons as functions of the separation $r$ of the static quark and antiquark sources. The parameters are determined by fitting results calculated using pure $SU(3)$ lattice gauge theory. The parameterizations have the correct limiting behavior at small $r$, where the potentials form multiplets associated with gluelumps. They have the correct limiting behavior at large $r$, where the potentials form multiplets associated with excitations of a relativistic string. There is a narrow avoided crossing in the small-$r$ region between two potentials with the same Born-Oppenheimer quantum numbers.","sentences":["We develop parameterizations of 8 of the lowest Born-Oppenheimer potentials for quarkonium hybrid mesons as functions of the separation $r$ of the static quark and antiquark sources.","The parameters are determined by fitting results calculated using pure $SU(3)$ lattice gauge theory.","The parameterizations have the correct limiting behavior at small $r$, where the potentials form multiplets associated with gluelumps.","They have the correct limiting behavior at large $r$, where the potentials form multiplets associated with excitations of a relativistic string.","There is a narrow avoided crossing in the small-$r$ region between two potentials with the same Born-Oppenheimer quantum numbers."],"url":"http://arxiv.org/abs/2406.05123v1","category":"hep-ph"}
{"created":"2024-06-07 17:52:43","title":"Third-order intrinsic alignment of SDSS BOSS LOWZ galaxies","abstract":"Cosmic shear is a powerful probe of cosmology, but it is affected by the intrinsic alignment (IA) of galaxy shapes with the large-scale structure. Upcoming surveys like Euclid and Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) require an accurate understanding of IA, particularly for higher-order cosmic shear statistics that are vital for extracting the most cosmological information. In this paper, we report the first detection of third-order IA correlations using the LOWZ galaxy sample from the Sloan Digital Sky Survey (SDSS) Baryon Oscillation Spectroscopic Survey (BOSS). We compare our measurements with predictions from the MICE cosmological simulation and an analytical NLA-inspired model informed by second-order correlations. We also explore the dependence of the third-order correlation on the galaxies' luminosity. We find that the amplitude $A_\\mathrm{IA}$ of the IA signal is non-zero at the $4.7\\sigma$ ($7.6\\sigma$) level for scales between $6 h^{-1} \\mathrm{Mpc}$ ($1 h^{-1} \\mathrm{Mpc}$) and $20 h^{-1} \\mathrm{Mpc}$. For scales above $6 h^{-1}\\mathrm{Mpc}$ the inferred AIA agrees both with the prediction from the simulation and estimates from second-order statistics within $1\\sigma$ but deviations arise at smaller scales. Our results demonstrate the feasibility of measuring third-order IA correlations and using them for constraining IA models. The agreement between second- and third-order IA constraints also opens the opportunity for a consistent joint analysis and IA self-calibration, promising tighter parameter constraints for upcoming cosmological surveys.","sentences":["Cosmic shear is a powerful probe of cosmology, but it is affected by the intrinsic alignment (IA) of galaxy shapes with the large-scale structure.","Upcoming surveys like Euclid and Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) require an accurate understanding of IA, particularly for higher-order cosmic shear statistics that are vital for extracting the most cosmological information.","In this paper, we report the first detection of third-order IA correlations using the LOWZ galaxy sample from the Sloan Digital Sky Survey (SDSS) Baryon Oscillation Spectroscopic Survey (BOSS).","We compare our measurements with predictions from the MICE cosmological simulation and an analytical NLA-inspired model informed by second-order correlations.","We also explore the dependence of the third-order correlation on the galaxies' luminosity.","We find that the amplitude $A_\\mathrm{IA}$ of the IA signal is non-zero at the $4.7\\sigma$ ($7.6\\sigma$) level for scales between $6 h^{-1} \\mathrm{Mpc}$ ($1 h^{-1} \\mathrm{Mpc}$) and $20 h^{-1} \\mathrm{Mpc}$.","For scales above $6 h^{-1}\\mathrm{Mpc}$ the inferred AIA agrees both with the prediction from the simulation and estimates from second-order statistics within $1\\sigma$ but deviations arise at smaller scales.","Our results demonstrate the feasibility of measuring third-order IA correlations and using them for constraining IA models.","The agreement between second- and third-order IA constraints also opens the opportunity for a consistent joint analysis and IA self-calibration, promising tighter parameter constraints for upcoming cosmological surveys."],"url":"http://arxiv.org/abs/2406.05122v1","category":"astro-ph.CO"}
{"created":"2024-06-07 17:43:15","title":"Ohms law lost and regained: observation and impact of zeros and poles","abstract":"The quantum conductance and its classical wave analogue, the transmittance, are given by the sum of the eigenvalues of the transmission matrix. The lowest transmission eigenvalue in diffusive media might be expected to play a negligible role in the conductance, and, in any case, to be too small to be observed. Here, we observe the lowest transmission eigenchannel in microwave waveguides, though it is orders of magnitude below the nominal noise level, and show that the transmittance is pulled down by global correlation among transmission eigenvalues and among zeros and poles of the transmission matrix. Transmission vanishes either when the energy density on the sample output vanishes at topological transmission zeros or when the longitudinal velocity vanishes precisely at the crossover to a new channel. This lowers the conductance by an amount proportional to the modulation of the density of states. In accord with the correspondence principle, the conductance approaches Ohms law as the number of channels increases with sample width. The exploration of the transmission matrix opens the door to a new understanding of mesoscopic transport and ultrasensitive detection techniques.","sentences":["The quantum conductance and its classical wave analogue, the transmittance, are given by the sum of the eigenvalues of the transmission matrix.","The lowest transmission eigenvalue in diffusive media might be expected to play a negligible role in the conductance, and, in any case, to be too small to be observed.","Here, we observe the lowest transmission eigenchannel in microwave waveguides, though it is orders of magnitude below the nominal noise level, and show that the transmittance is pulled down by global correlation among transmission eigenvalues and among zeros and poles of the transmission matrix.","Transmission vanishes either when the energy density on the sample output vanishes at topological transmission zeros or when the longitudinal velocity vanishes precisely at the crossover to a new channel.","This lowers the conductance by an amount proportional to the modulation of the density of states.","In accord with the correspondence principle, the conductance approaches Ohms law as the number of channels increases with sample width.","The exploration of the transmission matrix opens the door to a new understanding of mesoscopic transport and ultrasensitive detection techniques."],"url":"http://arxiv.org/abs/2406.05112v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-07 17:35:03","title":"IRIS observational approach to the oscillatory and damping nature of network and internetwork chromosphere small-scale brightening (SSBs) and their unusual dynamical and morphological differences in different regions on the solar disk","abstract":"One of the most exciting benefits of solar small-scale brightening is their oscillations, this study investigated the properties of small-scale brightening (SSBs) in different regions of the Sun and found that there are differences and similarities in the properties of oscillated and non-oscillated SSBs in different regions of the Sun, including quiet Sun (QS), the adjacent to active regions (AAR), and coronal hole (CH). The damping per period (Q-factor) and maximum Doppler velocity of SSBs varied depending on the region, with the less bright internetwork SSBs in QS having lower damping time (120 seconds) and higher maximum Doppler velocities (47 km/s) compared to the brighter network SSBs (with 216 seconds & 37 km/s, respectively), while in AAR, internetwork SSBs tend to have higher damping time (about of 220 seconds) and wider maximum Doppler velocity (10 to 140 km/s) ranges compared to network SSBs (130 seconds & 10 to 85 km/s). In CH, both types of SSBs show similar damping time (120 seconds), but internetwork SSBs tend to have higher maximum Doppler velocities (100 km/s) compared to network SSBs (85 km/s). Also, it was pointed out that the majority of network SSBs in AARs are in the overdamping mode, while in QS, internetwork SSBs demonstrate overdamping behavior and oscillated network SSBs exhibit critical damping behavior. It is important to bear in mind, however, that the physical mechanisms underlying the damping of SSBs may vary depending on the local plasma conditions and magnetic environment.","sentences":["One of the most exciting benefits of solar small-scale brightening is their oscillations, this study investigated the properties of small-scale brightening (SSBs) in different regions of the Sun and found that there are differences and similarities in the properties of oscillated and non-oscillated SSBs in different regions of the Sun, including quiet Sun (QS), the adjacent to active regions (AAR), and coronal hole (CH).","The damping per period (Q-factor) and maximum Doppler velocity of SSBs varied depending on the region, with the less bright internetwork SSBs in QS having lower damping time (120 seconds) and higher maximum Doppler velocities (47 km/s) compared to the brighter network SSBs (with 216 seconds & 37 km/s, respectively), while in AAR, internetwork SSBs tend to have higher damping time (about of 220 seconds) and wider maximum Doppler velocity (10 to 140 km/s) ranges compared to network SSBs (130 seconds & 10 to 85 km/s).","In CH, both types of SSBs show similar damping time (120 seconds), but internetwork SSBs tend to have higher maximum Doppler velocities (100 km/s) compared to network SSBs (85 km/s).","Also, it was pointed out that the majority of network SSBs in AARs are in the overdamping mode, while in QS, internetwork SSBs demonstrate overdamping behavior and oscillated network SSBs exhibit critical damping behavior.","It is important to bear in mind, however, that the physical mechanisms underlying the damping of SSBs may vary depending on the local plasma conditions and magnetic environment."],"url":"http://arxiv.org/abs/2406.05105v1","category":"astro-ph.SR"}
{"created":"2024-06-07 17:33:37","title":"Curvature induced magnetization of altermagnetic films","abstract":"We consider a thin film of $d$-wave altermagnet bent in a stretching-free manner and demonstrate that gradients of the film curvature induce a local magnetization which is approximately tangential to the film. The magnetization amplitude directly reflects the altermagnetic symmetry and depends on the direction of bending. It is maximal for the bending along directions of the maximal altermagnetic splitting of the magnon bands. A periodically bent film of sinusoidal shape possesses a total magnetic moment per period $\\propto\\mathscr{A}^2q^4$ where $\\mathscr{A}$ and $q$ are the bending amplitude and wave vector, respectively. The total magnetic moment is perpendicular to the plane of the unbent film and its direction (up or down) is determined by the bending direction. A film roll up to a nanotube possesses a toroidal moment directed along the tube $\\propto \\delta_r/r^2$ per one coil, where $r$ and $\\delta_r$ are the coil radius and the pitch between coils. All these analytical predictions agree with numerical spin-lattice simulations.","sentences":["We consider a thin film of $d$-wave altermagnet bent in a stretching-free manner and demonstrate that gradients of the film curvature induce a local magnetization which is approximately tangential to the film.","The magnetization amplitude directly reflects the altermagnetic symmetry and depends on the direction of bending.","It is maximal for the bending along directions of the maximal altermagnetic splitting of the magnon bands.","A periodically bent film of sinusoidal shape possesses a total magnetic moment per period $\\propto\\mathscr{A}^2q^4$ where $\\mathscr{A}$ and $q$ are the bending amplitude and wave vector, respectively.","The total magnetic moment is perpendicular to the plane of the unbent film and its direction (up or down) is determined by the bending direction.","A film roll up to a nanotube possesses a toroidal moment directed along the tube $\\propto \\delta_r/r^2$ per one coil, where $r$ and $\\delta_r$ are the coil radius and the pitch between coils.","All these analytical predictions agree with numerical spin-lattice simulations."],"url":"http://arxiv.org/abs/2406.05103v1","category":"cond-mat.str-el"}
{"created":"2024-06-07 16:47:45","title":"Discovery of a Relativistic Stripped Envelope Type Ic-BL Supernova at z = 2.83 with JWST","abstract":"We present JWST NIRCam and NIRSpec observations of a Type Ic supernova (SN Ic) and its host galaxy (JADES-GS+53.13533-27.81457) at $z = 2.83$. This SN (named SN 2023adta) was identified in deep James Webb Space Telescope (JWST)/NIRCam imaging from the JWST Advanced Deep Extragalactic Survey (JADES) Program. Follow-up observations with JWST/NIRSpec provided a spectroscopic redshift of $z = 2.83$ and the classification as a SN Ic-BL. The light curve of SN 2023adta matches well with other stripped envelope supernovae and we find a high peak luminosity, $M_V = -19.0 \\pm 0.2$ mag, based on the distribution of best-fit SNe. The broad absorption features in its spectrum are consistent with other SNe Ic-BL 1-3 weeks after peak brightness. We measure a Ca II NIR triplet expansion velocity of $29{,}000 \\pm 2{,}000$ km s$^{-1}$. The host galaxy of SN 2023adta is irregular, and modeling of its spectral energy distribution (SED) indicates a metallicity of $Z = 0.35^{+0.16}_{-0.08} Z_{\\odot}$. This environment is consistent with the population of low-$z$ SNe Ic-BL which prefer lower metallicities relative to other stripped envelope supernovae, and track long duration $\\gamma$-ray burst (LGRB) environments. We do not identify any GRBs that are coincident with SN 2023adta. Given the rarity of SNe Ic-BL in the local universe, the detection of a SN Ic-BL at $z = 2.83$ could indicate that their rates are enhanced at high redshift.","sentences":["We present JWST NIRCam and NIRSpec observations of a Type Ic supernova (SN Ic) and its host galaxy (JADES-GS+53.13533-27.81457) at $z = 2.83$.","This SN (named SN 2023adta) was identified in deep James Webb Space Telescope (JWST)/NIRCam imaging from the JWST Advanced Deep Extragalactic Survey (JADES) Program.","Follow-up observations with JWST/NIRSpec provided a spectroscopic redshift of $z = 2.83$ and the classification as a SN Ic-BL.","The light curve of SN 2023adta matches well with other stripped envelope supernovae and we find a high peak luminosity, $M_V =","-19.0 \\pm 0.2$ mag, based on the distribution of best-fit SNe.","The broad absorption features in its spectrum are consistent with other SNe Ic-BL 1-3 weeks after peak brightness.","We measure a Ca II NIR triplet expansion velocity of $29{,}000 \\pm 2{,}000$ km s$^{-1}$.","The host galaxy of SN 2023adta is irregular, and modeling of its spectral energy distribution (SED) indicates a metallicity of $Z = 0.35^{+0.16}_{-0.08} Z_{\\odot}$.","This environment is consistent with the population of low-$z$ SNe Ic-BL which prefer lower metallicities relative to other stripped envelope supernovae, and track long duration $\\gamma$-ray burst (LGRB) environments.","We do not identify any GRBs that are coincident with SN 2023adta.","Given the rarity of SNe Ic-BL in the local universe, the detection of a SN Ic-BL at $z = 2.83$ could indicate that their rates are enhanced at high redshift."],"url":"http://arxiv.org/abs/2406.05076v1","category":"astro-ph.HE"}
{"created":"2024-06-07 16:32:24","title":"The JADES Transient Survey: Discovery and Classification of Supernovae in the JADES Deep Field","abstract":"The JWST Advanced Deep Extragalactic Survey (JADES) is a multi-cycle JWST program that has taken among the deepest near-infrared images to date (down to $\\sim$30.5 ABmag) over $\\sim$25 arcmin$^2$ in the GOODS-S field in two sets of observations with one year of separation. This presented the first opportunity to systematically search for transients, mostly supernovae (SNe), out to $z>$2. We found 79 SNe: 38 at $z<$2, 23 at 2$<z<$3, 8 at 3$<z<$4, 7 at 4$<z<$5, and 3 with undetermined redshifts, where the redshifts are predominantly based on spectroscopic or highly reliable JADES photometric redshifts of the host galaxies. At this depth, the detection rate is $\\sim$1-2 per arcmin$^2$ per year, demonstrating the power of JWST as a supernova discovery machine. We also conducted multi-band follow-up NIRCam observations of a subset of the SNe to better constrain their light curves and classify their types. Here, we present the survey, sample, search parameters, spectral energy distributions (SEDs), light curves, and classifications. Even at $z\\geq$2, the NIRCam data quality is such that we can perform multi-epoch light-curve fitting to classify supernovae with a reasonable degree of confidence. The multi-epoch SN sample includes a Type Ia SN at $z_{spec}= $ 2.91, Type IIP SN at $z_{spec}= $ 3.61, and a Type Ic-BL SN at $z_{spec}= $ 2.845. We also found that two $z\\sim$16 galaxy candidates from the first imaging epoch were actually transients that faded in the second epoch, illustrating the possibility that moderate/high-redshift SNe could mimic high-redshift dropout galaxies.","sentences":["The JWST Advanced Deep Extragalactic Survey (JADES) is a multi-cycle JWST program that has taken among the deepest near-infrared images to date (down to $\\sim$30.5 ABmag) over $\\sim$25 arcmin$^2$ in the GOODS-S field in two sets of observations with one year of separation.","This presented the first opportunity to systematically search for transients, mostly supernovae (SNe), out to $z>$2.","We found 79 SNe: 38 at $z<$2, 23 at 2$<z<$3, 8 at 3$<z<$4, 7 at 4$<z<$5, and 3 with undetermined redshifts, where the redshifts are predominantly based on spectroscopic or highly reliable JADES photometric redshifts of the host galaxies.","At this depth, the detection rate is $\\sim$1-2 per arcmin$^2$ per year, demonstrating the power of JWST as a supernova discovery machine.","We also conducted multi-band follow-up NIRCam observations of a subset of the SNe to better constrain their light curves and classify their types.","Here, we present the survey, sample, search parameters, spectral energy distributions (SEDs), light curves, and classifications.","Even at $z\\geq$2, the NIRCam data quality is such that we can perform multi-epoch light-curve fitting to classify supernovae with a reasonable degree of confidence.","The multi-epoch SN sample includes a Type Ia SN at $z_{spec}= $ 2.91, Type IIP SN at $z_{spec}= $ 3.61, and a Type Ic-BL SN at $z_{spec}= $ 2.845.","We also found that two $z\\sim$16 galaxy candidates from the first imaging epoch were actually transients that faded in the second epoch, illustrating the possibility that moderate/high-redshift SNe could mimic high-redshift dropout galaxies."],"url":"http://arxiv.org/abs/2406.05060v1","category":"astro-ph.HE"}
{"created":"2024-06-07 16:19:02","title":"Modelling the impact of host galaxy dust on type Ia supernova distance measurements","abstract":"Type Ia Supernovae (SNe Ia) are a critical tool in measuring the accelerating expansion of the universe. Recent efforts to improve these standard candles have focused on incorporating the effects of dust on distance measurements with SNe Ia. In this paper, we use the state-of-the-art Dark Energy Survey 5 year sample to evaluate two different families of dust models: empirical extinction models derived from SNe Ia data, and physical attenuation models from the spectra of galaxies. Among the SNe Ia-derived models, we find that a logistic function of the total-to-selective extinction RV best recreates the correlations between supernova distance measurements and host galaxy properties, though an additional 0.02 magnitudes of grey scatter are needed to fully explain the scatter in SNIa brightness in all cases. These empirically-derived extinction distributions are highly incompatible with the physical attenuation models from galactic spectral measurements. From these results, we conclude that SNe Ia must either preferentially select extreme ends of galactic dust distributions, or that the characterisation of dust along the SNe Ia line-of-sight is incompatible with that of galactic dust distributions.","sentences":["Type Ia Supernovae (SNe Ia) are a critical tool in measuring the accelerating expansion of the universe.","Recent efforts to improve these standard candles have focused on incorporating the effects of dust on distance measurements with SNe Ia.","In this paper, we use the state-of-the-art Dark Energy Survey 5 year sample to evaluate two different families of dust models: empirical extinction models derived from SNe Ia data, and physical attenuation models from the spectra of galaxies.","Among the SNe Ia-derived models, we find that a logistic function of the total-to-selective extinction RV best recreates the correlations between supernova distance measurements and host galaxy properties, though an additional 0.02 magnitudes of grey scatter are needed to fully explain the scatter in SNIa brightness in all cases.","These empirically-derived extinction distributions are highly incompatible with the physical attenuation models from galactic spectral measurements.","From these results, we conclude that SNe Ia must either preferentially select extreme ends of galactic dust distributions, or that the characterisation of dust along the SNe Ia line-of-sight is incompatible with that of galactic dust distributions."],"url":"http://arxiv.org/abs/2406.05051v1","category":"astro-ph.CO"}
{"created":"2024-06-07 16:19:01","title":"The Dark Energy Survey Supernova Program: Slow supernovae show cosmological time dilation out to $z \\sim 1$","abstract":"We present a precise measurement of cosmological time dilation using the light curves of 1504 type Ia supernovae from the Dark Energy Survey spanning a redshift range $0.1\\lesssim z\\lesssim 1.2$. We find that the width of supernova light curves is proportional to $(1+z)$, as expected for time dilation due to the expansion of the Universe. Assuming type Ia supernovae light curves are emitted with a consistent duration $\\Delta t_{\\rm em}$, and parameterising the observed duration as $\\Delta t_{\\rm obs}=\\Delta t_{\\rm em}(1+z)^b$, we fit for the form of time dilation using two methods. Firstly, we find that a power of $b \\approx 1$ minimises the flux scatter in stacked subsamples of light curves across different redshifts. Secondly, we fit each target supernova to a stacked light curve (stacking all supernovae with observed bandpasses matching that of the target light curve) and find $b=1.003\\pm0.005$ (stat) $\\pm\\,0.010$ (sys). Thanks to the large number of supernovae and large redshift-range of the sample, this analysis gives the most precise measurement of cosmological time dilation to date, ruling out any non-time-dilating cosmological models at very high significance.","sentences":["We present a precise measurement of cosmological time dilation using the light curves of 1504 type Ia supernovae from the Dark Energy Survey spanning a redshift range $0.1\\lesssim z\\lesssim","1.2$. We find that the width of supernova light curves is proportional to $(1+z)$, as expected for time dilation due to the expansion of the Universe.","Assuming type Ia supernovae light curves are emitted with a consistent duration $\\Delta t_{\\rm em}$, and parameterising the observed duration as $\\Delta t_{\\rm obs}=\\Delta t_{\\rm em}(1+z)^b$, we fit for the form of time dilation using two methods.","Firstly, we find that a power of $b \\approx 1$ minimises the flux scatter in stacked subsamples of light curves across different redshifts.","Secondly, we fit each target supernova to a stacked light curve (stacking all supernovae with observed bandpasses matching that of the target light curve) and find $b=1.003\\pm0.005$ (stat) $\\pm\\,0.010$ (sys).","Thanks to the large number of supernovae and large redshift-range of the sample, this analysis gives the most precise measurement of cosmological time dilation to date, ruling out any non-time-dilating cosmological models at very high significance."],"url":"http://arxiv.org/abs/2406.05050v1","category":"astro-ph.CO"}
{"created":"2024-06-07 16:19:00","title":"The Dark Energy Survey Supernova Program: An updated measurement of the Hubble constant using the Inverse Distance Ladder","abstract":"We measure the current expansion rate of the Universe, Hubble's constant $H_0$, by calibrating the absolute magnitudes of supernovae to distances measured by Baryon Acoustic Oscillations. This `inverse distance ladder' technique provides an alternative to calibrating supernovae using nearby absolute distance measurements, replacing the calibration with a high-redshift anchor. We use the recent release of 1829 supernovae from the Dark Energy Survey spanning $0.01\\lt z \\lt1.13$ anchored to the recent Baryon Acoustic Oscillation measurements from DESI spanning $0.30 \\lt z_{\\mathrm{eff}} \\lt 2.33$. To trace cosmology to $z=0$, we use the third-, fourth- and fifth-order cosmographic models, which, by design, are agnostic about the energy content and expansion history of the universe. With the inclusion of the higher-redshift DESI-BAO data, the third-order model is a poor fit to both data sets, with the fourth-order model being preferred by the Akaike Information Criterion. Using the fourth-order cosmographic model, we find $H_0=67.19^{+0.66}_{-0.64}\\mathrm{~km} \\mathrm{~s}^{-1} \\mathrm{~Mpc}^{-1}$, in agreement with the value found by Planck without the need to assume Flat-$\\Lambda$CDM. However the best-fitting expansion history differs from that of Planck, providing continued motivation to investigate these tensions.","sentences":["We measure the current expansion rate of the Universe, Hubble's constant $H_0$, by calibrating the absolute magnitudes of supernovae to distances measured by Baryon Acoustic Oscillations.","This `inverse distance ladder' technique provides an alternative to calibrating supernovae using nearby absolute distance measurements, replacing the calibration with a high-redshift anchor.","We use the recent release of 1829 supernovae from the Dark Energy Survey spanning $0.01\\lt z \\lt1.13$ anchored to the recent Baryon Acoustic Oscillation measurements from DESI spanning $0.30 \\lt z_{\\mathrm{eff}} \\lt 2.33$. To trace cosmology to $z=0$, we use the third-, fourth- and fifth-order cosmographic models, which, by design, are agnostic about the energy content and expansion history of the universe.","With the inclusion of the higher-redshift DESI-BAO data, the third-order model is a poor fit to both data sets, with the fourth-order model being preferred by the Akaike Information Criterion.","Using the fourth-order cosmographic model, we find $H_0=67.19^{+0.66}_{-0.64}\\mathrm{~km} \\mathrm{~s}^{-1} \\mathrm{~Mpc}^{-1}$, in agreement with the value found by Planck without the need to assume Flat-$\\Lambda$CDM.","However the best-fitting expansion history differs from that of Planck, providing continued motivation to investigate these tensions."],"url":"http://arxiv.org/abs/2406.05049v1","category":"astro-ph.CO"}
{"created":"2024-06-07 16:18:57","title":"The Dark Energy Survey Supernova Program: Light curves and 5-Year data release","abstract":"We present $griz$ photometric light curves for the full 5 years of the Dark Energy Survey Supernova program (DES-SN), obtained with both forced Point Spread Function (PSF) photometry on Difference Images (DIFFIMG) performed during survey operations, and Scene Modelling Photometry (SMP) on search images processed after the survey. This release contains $31,636$ DIFFIMG and $19,706$ high-quality SMP light curves, the latter of which contains $1635$ photometrically-classified supernovae that pass cosmology quality cuts. This sample spans the largest redshift ($z$) range ever covered by a single SN survey ($0.1<z<1.13$) and is the largest single sample from a single instrument of SNe ever used for cosmological constraints. We describe in detail the improvements made to obtain the final DES-SN photometry and provide a comparison to what was used in the DES-SN3YR spectroscopically-confirmed SN Ia sample. We also include a comparative analysis of the performance of the SMP photometry with respect to the real-time DIFFIMG forced photometry and find that SMP photometry is more precise, more accurate, and less sensitive to the host-galaxy surface brightness anomaly. The public release of the light curves and ancillary data can be found at https://github.com/des-science/DES-SN5YR. Finally, we discuss implications for future transient surveys, such as the forthcoming Vera Rubin Observatory Legacy Survey of Space and Time (LSST).","sentences":["We present $griz$ photometric light curves for the full 5 years of the Dark Energy Survey Supernova program (DES-SN), obtained with both forced Point Spread Function (PSF) photometry on Difference Images (DIFFIMG) performed during survey operations, and Scene Modelling Photometry (SMP) on search images processed after the survey.","This release contains $31,636$ DIFFIMG and $19,706$ high-quality SMP light curves, the latter of which contains $1635$ photometrically-classified supernovae that pass cosmology quality cuts.","This sample spans the largest redshift ($z$) range ever covered by a single SN survey ($0.1<z<1.13$) and is the largest single sample from a single instrument of SNe ever used for cosmological constraints.","We describe in detail the improvements made to obtain the final DES-SN photometry and provide a comparison to what was used in the DES-SN3YR spectroscopically-confirmed SN Ia sample.","We also include a comparative analysis of the performance of the SMP photometry with respect to the real-time DIFFIMG forced photometry and find that SMP photometry is more precise, more accurate, and less sensitive to the host-galaxy surface brightness anomaly.","The public release of the light curves and ancillary data can be found at https://github.com/des-science/DES-SN5YR.","Finally, we discuss implications for future transient surveys, such as the forthcoming Vera Rubin Observatory Legacy Survey of Space and Time (LSST)."],"url":"http://arxiv.org/abs/2406.05046v1","category":"astro-ph.CO"}
{"created":"2024-06-07 15:34:02","title":"An update on supersphere non linear sigma model on the lattice","abstract":"We consider the discretized version of the sigma-model with supersphere target space $OSp(N+2m|2m)/OSp(N+2m-1|2m)$ and present a preliminary numerical study of bosonic and fermionic two-point functions for the cases $OSp(3|2)$ and $OSp(5|2)$. We observe consistency with the expectations of this supersymmetric setup and discuss the sign problem.","sentences":["We consider the discretized version of the sigma-model with supersphere target space $OSp(N+2m|2m)/OSp(N+2m-1|2m)$ and present a preliminary numerical study of bosonic and fermionic two-point functions for the cases $OSp(3|2)$ and $OSp(5|2)$. We observe consistency with the expectations of this supersymmetric setup and discuss the sign problem."],"url":"http://arxiv.org/abs/2406.05018v1","category":"hep-lat"}
{"created":"2024-06-07 15:23:38","title":"TrendLSW: Trend and Spectral Estimation of Nonstationary Time Series in R","abstract":"The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series. The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals. Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary. The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring.","sentences":["The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series.","The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals.","Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary.","The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring."],"url":"http://arxiv.org/abs/2406.05012v1","category":"stat.ME"}
{"created":"2024-06-07 15:15:25","title":"Universal Polar Instability In Highly Orthorhombic Perovskites","abstract":"The design of novel multiferroic ABO$_3$ perovskites is complicated by the presence of necessary magnetic cations and ubiquitous antiferrodistortive modes, both of which suppress polar distortions. Using first-principles simulations, we observe that the existence of quadlinear and trilinear invariants in the free energy, coupling tilts and antipolar motions of the A and B sites to the polar mode, drives an avalanche-like transition to a non-centrosymmetric $Pna2_1$ symmetry in a wide range of magnetic perovskites with small tolerance factors - overcoming the above restrictions. We find that the $Pna2_1$ phase is especially favoured with tensile epitaxial strain, leading to an unexpected but technologically useful out-of-plane polarization. We use this mechanism to predict various novel multiferroics displaying interesting magnetoelectric properties with small polarization switching barriers.","sentences":["The design of novel multiferroic ABO$_3$ perovskites is complicated by the presence of necessary magnetic cations and ubiquitous antiferrodistortive modes, both of which suppress polar distortions.","Using first-principles simulations, we observe that the existence of quadlinear and trilinear invariants in the free energy, coupling tilts and antipolar motions of the A and B sites to the polar mode, drives an avalanche-like transition to a non-centrosymmetric $Pna2_1$ symmetry in a wide range of magnetic perovskites with small tolerance factors - overcoming the above restrictions.","We find that the $Pna2_1$ phase is especially favoured with tensile epitaxial strain, leading to an unexpected but technologically useful out-of-plane polarization.","We use this mechanism to predict various novel multiferroics displaying interesting magnetoelectric properties with small polarization switching barriers."],"url":"http://arxiv.org/abs/2406.05001v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 14:41:17","title":"Impact of pressure anisotropy on the cascade rate of Hall-MHD turbulence with bi-adiabatic ions","abstract":"The impact of ion pressure anisotropy on the energy cascade rate of Hall-MHD turbulence with bi-adiabatic ions and isothermal electrons is evaluated in three-dimensional direct numerical simulations, using the exact law derived in Simon and Sahraoui (2022). It is shown that pressure anisotropy can enhance or reduce the cascade rate, depending on the scales, in comparison with the prediction of the exact law with isotropic pressure, by an amount that correlates well with pressure anisotropy $a_p=\\frac{p_\\perp}{p_\\parallel}\\neq1$ developing in simulations initialized with an isotropic pressure (${a_p}_0=1$). A simulation with an initial pressure anisotropy, ${a_p}_0=4$, confirms this trend, yielding a stronger impact on the cascade rate, both in the inertial range and at larger scales, close to the forcing. Furthermore, a Fourier-based numerical method to compute the exact laws in numerical simulations in the full $(\\ell_\\perp,\\ell_\\parallel)$ scale separation plane is presented.","sentences":["The impact of ion pressure anisotropy on the energy cascade rate of Hall-MHD turbulence with bi-adiabatic ions and isothermal electrons is evaluated in three-dimensional direct numerical simulations, using the exact law derived in Simon and Sahraoui (2022).","It is shown that pressure anisotropy can enhance or reduce the cascade rate, depending on the scales, in comparison with the prediction of the exact law with isotropic pressure, by an amount that correlates well with pressure anisotropy $a_p=\\frac{p_\\perp}{p_\\parallel}\\neq1$ developing in simulations initialized with an isotropic pressure (${a_p}_0=1$).","A simulation with an initial pressure anisotropy, ${a_p}_0=4$, confirms this trend, yielding a stronger impact on the cascade rate, both in the inertial range and at larger scales, close to the forcing.","Furthermore, a Fourier-based numerical method to compute the exact laws in numerical simulations in the full $(\\ell_\\perp,\\ell_\\parallel)$ scale separation plane is presented."],"url":"http://arxiv.org/abs/2406.04978v1","category":"physics.plasm-ph"}
{"created":"2024-06-07 14:33:17","title":"Shear viscosity from perturbative Quantum Chromodynamics to the hadron resonance gas at finite baryon, strangeness, and electric charge densities","abstract":"Through model-to-data comparisons from heavy-ion collisions, it has been shown that the Quark Gluon Plasma has an extremely small shear viscosity at vanishing densities. At large baryon densities, significantly less is known about the nature of the shear viscosity from Quantum Chromodynamics (QCD). Within heavy-ion collisions, there are three conserved charges: baryon number (B), strangeness (S), and electric charge (Q). Here we calculate the shear viscosity in two limits using perturbative QCD and an excluded-volume hadron resonance gas at finite BSQ densities. We then develop a framework that interpolates between these two limits such that shear viscosity is possible to calculate across a wide range of finite BSQ densities. We find that the pQCD and hadron resonance gas calculations have different BSQ densities dependence such that a rather non-trivial shear viscosity appears at finite densities.","sentences":["Through model-to-data comparisons from heavy-ion collisions, it has been shown that the Quark Gluon Plasma has an extremely small shear viscosity at vanishing densities.","At large baryon densities, significantly less is known about the nature of the shear viscosity from Quantum Chromodynamics (QCD).","Within heavy-ion collisions, there are three conserved charges: baryon number (B), strangeness (S), and electric charge (Q).","Here we calculate the shear viscosity in two limits using perturbative QCD and an excluded-volume hadron resonance gas at finite BSQ densities.","We then develop a framework that interpolates between these two limits such that shear viscosity is possible to calculate across a wide range of finite BSQ densities.","We find that the pQCD and hadron resonance gas calculations have different BSQ densities dependence such that a rather non-trivial shear viscosity appears at finite densities."],"url":"http://arxiv.org/abs/2406.04968v1","category":"hep-ph"}
{"created":"2024-06-07 14:31:02","title":"Crystal Structure Prediction and Phase Stability in Highly Anharmonic Silver-Based Chalcohalide Anti-Perovskites","abstract":"Silver-based chalcohalide anti-perovskites (CAP), Ag$_{3}$BC (B = S, Se; C = Cl, Br, I), represent an emerging family of energy materials with intriguing optoelectronic, vibrational and ionic transport properties. However, the structural features and phase stability of CAP remain poorly investigated to date, hindering their fundamental understanding and potential integration into technological applications. Here we employ theoretical first-principles methods based on density functional theory to fill this knowledge gap. Through crystal structure prediction techniques, ab initio molecular dynamics simulations, and quasi-harmonic free energy calculations, we unveil a series of previously overlooked energetically competitive phases and temperature-induced phase transitions for all CAP. Specifically, we identify a new cubic $P2_{1}3$ structure as the stable phase of all CAP containing S both at zero temperature and $T \\neq 0$ K conditions. Consequently, our calculations suggest that the cubic $Pm\\overline{3}m$ phase identified in room-temperature X-ray diffraction experiments is likely to be metastable. Furthermore, for CAP containing Se, we propose different orthorhombic ($Pca2_{1}$ and $P2_{1}2_{1}2_{1}$) and cubic ($I2_{1}3$) structures as the ground-state phases and reveal several phase transformations induced by temperature. This theoretical investigation not only identifies new candidate ground-state phases and solid-solid phase transformations for all CAP but also provides insights into potential stability issues affecting these highly anharmonic superionic materials.","sentences":["Silver-based chalcohalide anti-perovskites (CAP), Ag$_{3}$BC (B = S, Se; C = Cl, Br, I), represent an emerging family of energy materials with intriguing optoelectronic, vibrational and ionic transport properties.","However, the structural features and phase stability of CAP remain poorly investigated to date, hindering their fundamental understanding and potential integration into technological applications.","Here we employ theoretical first-principles methods based on density functional theory to fill this knowledge gap.","Through crystal structure prediction techniques, ab initio molecular dynamics simulations, and quasi-harmonic free energy calculations, we unveil a series of previously overlooked energetically competitive phases and temperature-induced phase transitions for all CAP.","Specifically, we identify a new cubic $P2_{1}3$ structure as the stable phase of all CAP containing S both at zero temperature and $T \\neq 0$ K conditions.","Consequently, our calculations suggest that the cubic $Pm\\overline{3}m$ phase identified in room-temperature X-ray diffraction experiments is likely to be metastable.","Furthermore, for CAP containing Se, we propose different orthorhombic ($Pca2_{1}$ and $P2_{1}2_{1}2_{1}$) and cubic ($I2_{1}3$) structures as the ground-state phases and reveal several phase transformations induced by temperature.","This theoretical investigation not only identifies new candidate ground-state phases and solid-solid phase transformations for all CAP but also provides insights into potential stability issues affecting these highly anharmonic superionic materials."],"url":"http://arxiv.org/abs/2406.04966v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-07 13:30:03","title":"A classification of module braces over the ring of $\\mathbf{p}$-adic integers","abstract":"In this paper we study the $R$-braces $(M,+,\\circ)$ such that $M\\cdot M$ is cyclic, where $R$ is the ring of $p$-adic and $\\cdot$ is the product of the radical $R$-algebra associated to $M$. In particular, we give a classification up to isomorphism in the torsion-free case and up to isoclinism in the torsion case. More precisely, the isomorphism classes and the isoclinism classes of such radical algebras are in correspondence with particular equivalence classes of the bilinear forms defined starting from the products of the algebras.","sentences":["In this paper we study the $R$-braces $(M,+,\\circ)$ such that $M\\cdot M$ is cyclic, where $R$ is the ring of $p$-adic and $\\cdot$ is the product of the radical $R$-algebra associated to $M$. In particular, we give a classification up to isomorphism in the torsion-free case and up to isoclinism in the torsion case.","More precisely, the isomorphism classes and the isoclinism classes of such radical algebras are in correspondence with particular equivalence classes of the bilinear forms defined starting from the products of the algebras."],"url":"http://arxiv.org/abs/2406.04925v1","category":"math.GR"}
{"created":"2024-06-07 13:22:04","title":"The 3d-index of the 3d-skein module via the quantum trace map","abstract":"We define a map from the skein module of a cusped hyperbolic 3-manifold to the ring of Laurent series in one variable with integer coefficients that satisfies two properties: its evaluation at peripheral curves coincides with the Dimofte--Gaiotto--Gukov 3d-index, and it factors through the 3d-quantum trace map associated to a suitable ideal triangulation of the manifold. The map fulfills a supersymmetry prediction of mathematical physics and is part of a conjectural 3+1 dimensional topological quantum field theory.","sentences":["We define a map from the skein module of a cusped hyperbolic 3-manifold to the ring of Laurent series in one variable with integer coefficients that satisfies two properties: its evaluation at peripheral curves coincides with the Dimofte--Gaiotto--Gukov 3d-index, and it factors through the 3d-quantum trace map associated to a suitable ideal triangulation of the manifold.","The map fulfills a supersymmetry prediction of mathematical physics and is part of a conjectural 3+1 dimensional topological quantum field theory."],"url":"http://arxiv.org/abs/2406.04918v1","category":"math.GT"}
{"created":"2024-06-07 13:15:04","title":"Bayesian inference of Latent Spectral Shapes","abstract":"This paper proposes a hierarchical spatial-temporal model for modelling the spectrograms of animal calls. The motivation stems from analyzing recordings of the so-called grunt calls emitted by various lemur species. Our goal is to identify a latent spectral shape that characterizes each species and facilitates measuring dissimilarities between them. The model addresses the synchronization of animal vocalizations, due to varying time-lengths and speeds, with non-stationary temporal patterns and accounts for periodic sampling artifacts produced by the time discretization of analog signals. The former is achieved through a synchronization function, and the latter is modeled using a circular representation of time. To overcome the curse of dimensionality inherent in the model's implementation, we employ the Nearest Neighbor Gaussian Process, and posterior samples are obtained using the Markov Chain Monte Carlo method. We apply the model to a real dataset comprising sounds from 8 different species. We define a representative sound for each species and compare them using a simple distance measure. Cross-validation is used to evaluate the predictive capability of our proposal and explore special cases. Additionally, a simulation example is provided to demonstrate that the algorithm is capable of retrieving the true parameters.","sentences":["This paper proposes a hierarchical spatial-temporal model for modelling the spectrograms of animal calls.","The motivation stems from analyzing recordings of the so-called grunt calls emitted by various lemur species.","Our goal is to identify a latent spectral shape that characterizes each species and facilitates measuring dissimilarities between them.","The model addresses the synchronization of animal vocalizations, due to varying time-lengths and speeds, with non-stationary temporal patterns and accounts for periodic sampling artifacts produced by the time discretization of analog signals.","The former is achieved through a synchronization function, and the latter is modeled using a circular representation of time.","To overcome the curse of dimensionality inherent in the model's implementation, we employ the Nearest Neighbor Gaussian Process, and posterior samples are obtained using the Markov Chain Monte Carlo method.","We apply the model to a real dataset comprising sounds from 8 different species.","We define a representative sound for each species and compare them using a simple distance measure.","Cross-validation is used to evaluate the predictive capability of our proposal and explore special cases.","Additionally, a simulation example is provided to demonstrate that the algorithm is capable of retrieving the true parameters."],"url":"http://arxiv.org/abs/2406.04915v1","category":"stat.AP"}
{"created":"2024-06-07 12:49:28","title":"Deconfinement and chiral phase transitions in quark matter with chiral imbalance","abstract":"We study the thermodynamics of the Polyakov--Nambu--Jona-Lasinio model considering the effects of an effective chiral chemical potential. We offer a new parametrization of the Polyakov loop potential depending on temperature and the chemical potential which, when used together with a proper regularization scheme of vacuum contributions, predicts results consistent with those from lattice simulations.","sentences":["We study the thermodynamics of the Polyakov--Nambu--Jona-Lasinio model considering the effects of an effective chiral chemical potential.","We offer a new parametrization of the Polyakov loop potential depending on temperature and the chemical potential which, when used together with a proper regularization scheme of vacuum contributions, predicts results consistent with those from lattice simulations."],"url":"http://arxiv.org/abs/2406.04900v1","category":"hep-ph"}
{"created":"2024-06-07 12:39:54","title":"Proton 3D reconstruction with T-odd TMD gluon densities","abstract":"We present studies aimed at probing the 3D gluon content of the proton through spin-dependent TMD gluon densities, computed by means of the spectator-model approach. Our formalism incorporates a fit-based modulation function for the spectator mass, designed to capture longitudinal-momentum effects across a broad kinematic range. Special emphasis is placed on the time-reversal even Boer-Mulders and the time-reversal odd Sivers functions. Accurate understanding of these functions is crucial for conducting precise 3D analyses of nucleons, highlighting the importance of collaborative efforts between the LHC and EIC Communities.","sentences":["We present studies aimed at probing the 3D gluon content of the proton through spin-dependent TMD gluon densities, computed by means of the spectator-model approach.","Our formalism incorporates a fit-based modulation function for the spectator mass, designed to capture longitudinal-momentum effects across a broad kinematic range.","Special emphasis is placed on the time-reversal even Boer-Mulders and the time-reversal odd Sivers functions.","Accurate understanding of these functions is crucial for conducting precise 3D analyses of nucleons, highlighting the importance of collaborative efforts between the LHC and EIC Communities."],"url":"http://arxiv.org/abs/2406.04893v1","category":"hep-ph"}
{"created":"2024-06-07 12:25:51","title":"The free boundary problem of an epidemic model with nonlocal diffusions and nonlocal reactions: spreading-vanishing dichotomy","abstract":"This paper concerns the free boundary problem of an epidemic model. The spatial movements of the infectious agents and the infective humans are approximated by nonlocal diffusion operators. Especially, both the growth rate of the agents and the infective rate of humans are represented by nonlocal reaction terms. Thus our model has four integral terms which bring some diffculties for the study of the corresponding principal eigenvalue problem. Firstly, using some elementray analysis instead of Krein-Rutman theorem and the variational characteristic, we obtain the existence and asymptotic behaviors of principal eigenvalue. Then a spreading-vanishing dichotomy is proved to hold, and the criteria for spreading and vanishing are derived. Lastly, comparing our results with those in the existing works, we discuss the effect of nonlocal reaction term on spreading and vanishing, finding that the more nonlocal reaction terms a model has, the harder spreading happens.","sentences":["This paper concerns the free boundary problem of an epidemic model.","The spatial movements of the infectious agents and the infective humans are approximated by nonlocal diffusion operators.","Especially, both the growth rate of the agents and the infective rate of humans are represented by nonlocal reaction terms.","Thus our model has four integral terms which bring some diffculties for the study of the corresponding principal eigenvalue problem.","Firstly, using some elementray analysis instead of Krein-Rutman theorem and the variational characteristic, we obtain the existence and asymptotic behaviors of principal eigenvalue.","Then a spreading-vanishing dichotomy is proved to hold, and the criteria for spreading and vanishing are derived.","Lastly, comparing our results with those in the existing works, we discuss the effect of nonlocal reaction term on spreading and vanishing, finding that the more nonlocal reaction terms a model has, the harder spreading happens."],"url":"http://arxiv.org/abs/2406.04880v1","category":"math.AP"}
{"created":"2024-06-07 11:39:38","title":"Emergence of KNO scaling in multiplicity distributions in jets produced at the LHC","abstract":"In this work we study the multiplicity distributions (MDs) of charged particles within jets in proton-proton collisions, which were measured by the ATLAS collaboration in 2011, 2016 and 2019. The first data set refers to jets with lower transverse momenta ($4 < p_T < 40$ GeV ) whereas the other two refer to higher $p_T$ jets ($0.1 < p_T < 2.5$ TeV). We find that the first set shows no sign of KNO scaling whereas the higher $p_T$ sets gradually approach the scaling limit. In the first set the mean multiplicity as a function of $p_T$ can be well described by expressions derived from QCD with different approximation schemes. For higher ($> 500$ GeV) values of $p_T$ these expressions significantly overshoot the data. We show that the behavior of the MDs can be well represented by a Sub-Poisson distribution with energy dependent parameters. In the range $40 < p_T < 100$ GeV there is a transition from sub to super poissonian behavior and the MD evolves to a geometric distribution, which shows KNO scaling. In this way we fit the MDs in all transverse momentum intervals with one single expression. We discuss the implications of this phenomenological finding.","sentences":["In this work we study the multiplicity distributions (MDs) of charged particles within jets in proton-proton collisions, which were measured by the ATLAS collaboration in 2011, 2016 and 2019.","The first data set refers to jets with lower transverse momenta ($4 < p_T < 40$ GeV ) whereas the other two refer to higher $p_T$ jets ($0.1 < p_T < 2.5$ TeV).","We find that the first set shows no sign of KNO scaling whereas the higher $p_T$ sets gradually approach the scaling limit.","In the first set the mean multiplicity as a function of $p_T$ can be well described by expressions derived from QCD with different approximation schemes.","For higher ($> 500$ GeV) values of $p_T$ these expressions significantly overshoot the data.","We show that the behavior of the MDs can be well represented by a Sub-Poisson distribution with energy dependent parameters.","In the range $40 < p_T < 100$ GeV there is a transition from sub to super poissonian behavior and the MD evolves to a geometric distribution, which shows KNO scaling.","In this way we fit the MDs in all transverse momentum intervals with one single expression.","We discuss the implications of this phenomenological finding."],"url":"http://arxiv.org/abs/2406.04856v1","category":"hep-ph"}
{"created":"2024-06-07 10:55:00","title":"Cosmological Forecasts from the Baryon Acoustic Oscillations in 21cm Intensity Mapping","abstract":"We use the baryon acoustic oscillation (BAO) feature in the angular power spectrum (APS) of the 21cm line of neutral hydrogen (HI) to constrain cosmological parameters. As the BAO shift parameter can only constrain the product of the Hubble constant and the sound horizon $H_0r_s$, we combine our Fisher matrices with cosmic microwave background (CMB) Fisher matrices from the covariance of $Planck$ telescope to break this degeneracy. In particular, we find that the best constraints we can get with this method are on the Hubble parameter $h$, and the dark energy parameters $w_0$ and $w_a$. Assuming a noise level as for the BINGO telescope, we find $\\sigma_h = 0.0055\\;(0.8\\%)$ in the $\\Lambda$CDM model. For the $w$CDM model, we find $\\sigma_h = 0.020\\;(2.9\\%)$ and $\\sigma_{w_0} = 0.075\\;(7.5\\%)$. In the CPL parameterization, we find $\\sigma_h = 0.029\\;(4.4\\%)$, $\\sigma_{w_0} = 0.40\\;(40\\%)$, and $\\sigma_{w_a} = 1.7$. Although these constraints improve those from $Planck$ alone, we observe the BINGO data will be more significant for the $w$CDM model. We also observe the constraints from the BAO only is not as strong as using the whole 21cm angular power spectrum, but it is less susceptible to systematic effects.","sentences":["We use the baryon acoustic oscillation (BAO) feature in the angular power spectrum (APS) of the 21cm line of neutral hydrogen (HI) to constrain cosmological parameters.","As the BAO shift parameter can only constrain the product of the Hubble constant and the sound horizon $H_0r_s$, we combine our Fisher matrices with cosmic microwave background (CMB)","Fisher matrices from the covariance of $Planck$ telescope to break this degeneracy.","In particular, we find that the best constraints we can get with this method are on the Hubble parameter $h$, and the dark energy parameters $w_0$ and $w_a$. Assuming a noise level as for the BINGO telescope, we find $\\sigma_h = 0.0055\\;(0.8\\%)$ in the $\\Lambda$CDM model.","For the $w$CDM model, we find $\\sigma_h = 0.020\\;(2.9\\%)$ and $\\sigma_{w_0} = 0.075\\;(7.5\\%)$.","In the CPL parameterization, we find $\\sigma_h = 0.029\\;(4.4\\%)$, $\\sigma_{w_0} = 0.40\\;(40\\%)$, and $\\sigma_{w_a} = 1.7$.","Although these constraints improve those from $Planck$ alone, we observe the BINGO data will be more significant for the $w$CDM model.","We also observe the constraints from the BAO only is not as strong as using the whole 21cm angular power spectrum, but it is less susceptible to systematic effects."],"url":"http://arxiv.org/abs/2406.04830v1","category":"astro-ph.CO"}
{"created":"2024-06-07 10:39:07","title":"Magnetism of $\\mathrm{NaYbS_2}$: From finite temperatures to ground state","abstract":"Rare-earth chalcogenide compounds $\\mathrm{ARECh_2}$ (A = alkali or monovalent metal, RE = rare earth, Ch = O, S, Se, Te) are a large family of quantum spin liquid (QSL) candidate materials. $\\mathrm{NaYbS_2}$ is a representative member of the family. Several key issues on $\\mathrm{NaYbS_2}$, particularly how to determine the highly anisotropic spin Hamiltonian and describe the magnetism at finite temperatures and the ground state, remain to be addressed. In this paper, we conducted an in-depth and comprehensive study on the magnetism of $\\mathrm{NaYbS_2}$ from finite temperatures to the ground state. Firstly, we successfully detected three crystalline electric field (CEF) excitation energy levels using low-temperature Raman scattering technique. Combining them with the CEF theory and magnetization data, we worked out the CEF parameters, CEF energy levels, and CEF wavefunctions. We further determined a characteristic temperature of $\\sim$40 K, above which the magnetism is dominated by CEF excitations while below which the spin-exchange interactions play a main role. The characteristic temperature has been confirmed by the temperature-dependent electron spin resonance (ESR) linewidth. Low-temperature ESR experiments on the dilute magnetic doped crystal of $\\mathrm{NaYb_{0.1}Lu_{0.9}S_2}$ further helped us to determine the accurate $g$-factor. Next, we quantitatively obtained the spin-exchange interactions in the spin Hamiltonian by consistently simulating the magnetization and specific heat data. Finally, the above studies allow us to explore the ground state magnetism of $\\mathrm{NaYbS_2}$ by using the density matrix renormalization group. We combined numerical calculations and experimental results to demonstrate that the ground state of $\\mathrm{NaYbS_2}$ is a Dirac-like QSL.","sentences":["Rare-earth chalcogenide compounds $\\mathrm{ARECh_2}$ (A = alkali or monovalent metal, RE = rare earth, Ch = O, S, Se, Te) are a large family of quantum spin liquid (QSL) candidate materials.","$\\mathrm{NaYbS_2}$ is a representative member of the family.","Several key issues on $\\mathrm{NaYbS_2}$, particularly how to determine the highly anisotropic spin Hamiltonian and describe the magnetism at finite temperatures and the ground state, remain to be addressed.","In this paper, we conducted an in-depth and comprehensive study on the magnetism of $\\mathrm{NaYbS_2}$ from finite temperatures to the ground state.","Firstly, we successfully detected three crystalline electric field (CEF) excitation energy levels using low-temperature Raman scattering technique.","Combining them with the CEF theory and magnetization data, we worked out the CEF parameters, CEF energy levels, and CEF wavefunctions.","We further determined a characteristic temperature of $\\sim$40 K, above which the magnetism is dominated by CEF excitations while below which the spin-exchange interactions play a main role.","The characteristic temperature has been confirmed by the temperature-dependent electron spin resonance (ESR) linewidth.","Low-temperature ESR experiments on the dilute magnetic doped crystal of $\\mathrm{NaYb_{0.1}Lu_{0.9}S_2}$ further helped us to determine the accurate $g$-factor.","Next, we quantitatively obtained the spin-exchange interactions in the spin Hamiltonian by consistently simulating the magnetization and specific heat data.","Finally, the above studies allow us to explore the ground state magnetism of $\\mathrm{NaYbS_2}$ by using the density matrix renormalization group.","We combined numerical calculations and experimental results to demonstrate that the ground state of $\\mathrm{NaYbS_2}$ is a Dirac-like QSL."],"url":"http://arxiv.org/abs/2406.04819v1","category":"cond-mat.str-el"}
{"created":"2024-06-07 10:10:32","title":"Mitigation of DESI fiber assignment incompleteness effect on two-point clustering with small angular scale truncated estimators","abstract":"We present a method to mitigate the effects of fiber assignment incompleteness in two-point power spectrum and correlation function measurements from galaxy spectroscopic surveys, by truncating small angular scales from estimators. We derive the corresponding modified correlation function and power spectrum windows to account for the small angular scale truncation in the theory prediction. We validate this approach on simulations reproducing the Dark Energy Spectroscopic Instrument (DESI) Data Release 1 (DR1) with and without fiber assignment. We show that we recover unbiased cosmological constraints using small angular scale truncated estimators from simulations with fiber assignment incompleteness, with respect to standard estimators from complete simulations. Additionally, we present an approach to remove the sensitivity of the fits to high $k$ modes in the theoretical power spectrum, by applying a transformation to the data vector and window matrix. We find that our method efficiently mitigates the effect of fiber assignment incompleteness in two-point correlation function and power spectrum measurements, at low computational cost and with little statistical loss.","sentences":["We present a method to mitigate the effects of fiber assignment incompleteness in two-point power spectrum and correlation function measurements from galaxy spectroscopic surveys, by truncating small angular scales from estimators.","We derive the corresponding modified correlation function and power spectrum windows to account for the small angular scale truncation in the theory prediction.","We validate this approach on simulations reproducing the Dark Energy Spectroscopic Instrument (DESI) Data Release 1 (DR1) with and without fiber assignment.","We show that we recover unbiased cosmological constraints using small angular scale truncated estimators from simulations with fiber assignment incompleteness, with respect to standard estimators from complete simulations.","Additionally, we present an approach to remove the sensitivity of the fits to high $k$ modes in the theoretical power spectrum, by applying a transformation to the data vector and window matrix.","We find that our method efficiently mitigates the effect of fiber assignment incompleteness in two-point correlation function and power spectrum measurements, at low computational cost and with little statistical loss."],"url":"http://arxiv.org/abs/2406.04804v1","category":"astro-ph.CO"}
{"created":"2024-06-07 09:37:05","title":"Detection Prospects of Millicharged Dark Matter in Unconventional Interferometer","abstract":"We propose a novel idea to discover millicharged particles (mCPs) captured by the earth during its existence. It has been demonstrated that the mCPs accumulation inside the earth leads to the enhancement of its number density much larger than the corresponding virial density. We propose to utilize an unconventional laser interferometer to probe these earth-bound mCPs through the detection of the photons's phase shift. We demonstrate that, for mCPs mass in the range between $1$ GeV to $10^{12}$ GeV, the sensitivity of probing their fractional electric charge $\\epsilon$ could reach as low as $10^{-12}$ to $10^{-6}$ provided that mCPs number density is greater than $1~\\text{cm}^{-3}$.","sentences":["We propose a novel idea to discover millicharged particles (mCPs) captured by the earth during its existence.","It has been demonstrated that the mCPs accumulation inside the earth leads to the enhancement of its number density much larger than the corresponding virial density.","We propose to utilize an unconventional laser interferometer to probe these earth-bound mCPs through the detection of the photons's phase shift.","We demonstrate that, for mCPs mass in the range between $1$ GeV to $10^{12}$ GeV, the sensitivity of probing their fractional electric charge $\\epsilon$ could reach as low as $10^{-12}$ to $10^{-6}$ provided that mCPs number density is greater than $1~\\text{cm}^{-3}$."],"url":"http://arxiv.org/abs/2406.04788v1","category":"hep-ph"}
{"created":"2024-06-07 08:36:17","title":"CHSH Bell Tests For Optical Hybrid Entanglement","abstract":"Optical hybrid entanglement can be created between two qubits, one encoded in a single photon and another one in coherent states with opposite phases. It opens the path to a variety of quantum technologies, such as heterogeneous quantum networks, merging continuous and discrete variable encoding, and enabling the transport and interconversion of information. However, reliable characterization of the nature of this entanglement is limited so far to full quantum state tomography. Here, we perform a thorough study of Clauser-Horne-Shimony-Holt (CHSH) Bell inequality tests, enabling practical verification of quantum correlations for optical hybrid entanglement. We show that a practical violation of this inequality is possible with simple photon number on/off measurements if detection efficiencies stay above 82%. Another approach, based on photon-number parity measurements, requires 94% efficiency but works well in the limit of higher photon populations. Both tests use no postselection of the measurement outcomes and they are free of the fair-sampling hypothesis. Our proposal paves the way to performing loophole-free tests using feasible experimental tasks such as coherent state interference and photon counting, and to verification of hybrid entanglement in real-world applications.","sentences":["Optical hybrid entanglement can be created between two qubits, one encoded in a single photon and another one in coherent states with opposite phases.","It opens the path to a variety of quantum technologies, such as heterogeneous quantum networks, merging continuous and discrete variable encoding, and enabling the transport and interconversion of information.","However, reliable characterization of the nature of this entanglement is limited so far to full quantum state tomography.","Here, we perform a thorough study of Clauser-Horne-Shimony-Holt (CHSH)","Bell inequality tests, enabling practical verification of quantum correlations for optical hybrid entanglement.","We show that a practical violation of this inequality is possible with simple photon number on/off measurements if detection efficiencies stay above 82%.","Another approach, based on photon-number parity measurements, requires 94% efficiency but works well in the limit of higher photon populations.","Both tests use no postselection of the measurement outcomes and they are free of the fair-sampling hypothesis.","Our proposal paves the way to performing loophole-free tests using feasible experimental tasks such as coherent state interference and photon counting, and to verification of hybrid entanglement in real-world applications."],"url":"http://arxiv.org/abs/2406.04736v1","category":"quant-ph"}
{"created":"2024-06-07 08:25:10","title":"Reconstruction of 400 GeV/c proton interactions with the SHiP-charm project","abstract":"The SHiP-charm project was proposed to measure the associated charm production induced by 400 GeV/c protons in a thick target, including the contribution from cascade production. An optimisation run was performed in July 2018 at CERN SPS using a hybrid setup. The high resolution of nuclear emulsions acting as vertex detector was complemented by electronic detectors for kinematic measurements and muon identification. Here we present first results on the analysis of nuclear emulsions exposed in the 2018 run, which prove the capability of reconstructing proton interaction vertices in a harsh environment, where the signal is largely dominated by secondary particles produced in hadronic and electromagnetic showers within the lead target.","sentences":["The SHiP-charm project was proposed to measure the associated charm production induced by 400 GeV/c protons in a thick target, including the contribution from cascade production.","An optimisation run was performed in July 2018 at CERN SPS using a hybrid setup.","The high resolution of nuclear emulsions acting as vertex detector was complemented by electronic detectors for kinematic measurements and muon identification.","Here we present first results on the analysis of nuclear emulsions exposed in the 2018 run, which prove the capability of reconstructing proton interaction vertices in a harsh environment, where the signal is largely dominated by secondary particles produced in hadronic and electromagnetic showers within the lead target."],"url":"http://arxiv.org/abs/2406.04730v1","category":"hep-ex"}
