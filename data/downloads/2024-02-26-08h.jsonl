{"created":"2024-02-23 18:59:40","title":"Seamless Human Motion Composition with Blended Positional Encodings","abstract":"Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.","sentences":["Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics.","While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations.","Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions.","In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps.","For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain.","More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage.","As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets.","FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time.","Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions."],"url":"http://arxiv.org/abs/2402.15509v1","category":"cs.CV"}
{"created":"2024-02-23 18:58:08","title":"The \u03b1-element enrichment of gas in distant galaxies","abstract":"The chemical evolution of distant galaxies cannot be assessed from observations of individual stars, in contrast to the case of nearby galaxies. On the other hand, the study of the interstellar medium (ISM) offers an alternative way to reveal important properties of the chemical evolution of distant galaxies. The chemical enrichment of the ISM is produced by all the previous generations of stars and it is possible to precisely determine the metal abundances in the neutral ISM in galaxies. The chemical abundance patterns in the neutral ISM are determined by the gas metallicity, presence of dust (the depletion of metals into dust grains), and possible deviations due to specific nucleosynthesis, for example, $\\alpha$-element enhancements. We aim to derive the metallicities, dust depletion, and $\\alpha$-element enhancements in the neutral ISM of gas-rich mostly-metal-poor distant galaxies (Damped Lyman-$\\alpha$ absorbers, DLAs). Furthermore, we aim to constrain the distribution of $\\alpha$-element enhancements with metallicity in these galaxies. We have constrained, for the first time, the distribution of the $\\alpha$-element enhancement with metallicity in the neutral ISM in distant galaxies. Less massive galaxies show an $\\alpha$-element knee at lower metallicities than more massive galaxies. This can be explained by a lower star formation rate in less massive galaxies. If this collective behaviour can be interpreted in the same way as it is for individual systems, this would suggest that more massive and metal-rich systems evolve to higher metallicities before the contribution of SN-Ia to [$\\alpha$/Fe] levels out that of core-collapse SNe. This finding may plausibly be supported by different SFRs in galaxies of different masses. Overall, our results offer important clues to the study of chemical evolution in distant galaxies.","sentences":["The chemical evolution of distant galaxies cannot be assessed from observations of individual stars, in contrast to the case of nearby galaxies.","On the other hand, the study of the interstellar medium (ISM) offers an alternative way to reveal important properties of the chemical evolution of distant galaxies.","The chemical enrichment of the ISM is produced by all the previous generations of stars and it is possible to precisely determine the metal abundances in the neutral ISM in galaxies.","The chemical abundance patterns in the neutral ISM are determined by the gas metallicity, presence of dust (the depletion of metals into dust grains), and possible deviations due to specific nucleosynthesis, for example, $\\alpha$-element enhancements.","We aim to derive the metallicities, dust depletion, and $\\alpha$-element enhancements in the neutral ISM of gas-rich mostly-metal-poor distant galaxies (Damped Lyman-$\\alpha$ absorbers, DLAs).","Furthermore, we aim to constrain the distribution of $\\alpha$-element enhancements with metallicity in these galaxies.","We have constrained, for the first time, the distribution of the $\\alpha$-element enhancement with metallicity in the neutral ISM in distant galaxies.","Less massive galaxies show an $\\alpha$-element knee at lower metallicities than more massive galaxies.","This can be explained by a lower star formation rate in less massive galaxies.","If this collective behaviour can be interpreted in the same way as it is for individual systems, this would suggest that more massive and metal-rich systems evolve to higher metallicities before the contribution of SN-Ia to [$\\alpha$/Fe] levels out that of core-collapse SNe.","This finding may plausibly be supported by different SFRs in galaxies of different masses.","Overall, our results offer important clues to the study of chemical evolution in distant galaxies."],"url":"http://arxiv.org/abs/2402.15508v1","category":"astro-ph.GA"}
{"created":"2024-02-23 18:56:26","title":"AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning","abstract":"Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.","sentences":["Autonomous agents powered by large language models (LLMs) have garnered significant research attention.","However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories.","In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges.","\\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios.","It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training.","Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training.","Additionally, we present \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks."],"url":"http://arxiv.org/abs/2402.15506v1","category":"cs.AI"}
{"created":"2024-02-23 18:56:11","title":"Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts","abstract":"Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consistency, leveraging their dependencies to reject potential annotation noises. We validate the proposed method through visual recognition tasks on the OpenAI weak-to-strong benchmark and additional multi-domain datasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.","sentences":["Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors.","Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives.","Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps.","In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student.","Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consistency, leveraging their dependencies to reject potential annotation noises.","We validate the proposed method through visual recognition tasks on the OpenAI weak-to-strong benchmark and additional multi-domain datasets.","Our code is available at \\url{https://github.com/yuejiangliu/csl}."],"url":"http://arxiv.org/abs/2402.15505v1","category":"cs.LG"}
{"created":"2024-02-23 18:55:09","title":"Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition","abstract":"Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions. Using this, we create a dataset called MyCanvas, that can be used to benchmark the task of multi-concept personalization. In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized text-to-image diffusion methods. We provide a simple baseline built on top of Custom Diffusion with empirical prompting strategies for future researchers to evaluate on MyCanvas. We show that by improving data quality and prompting strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms.","sentences":["Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training.","This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models.","First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION).","Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description.","To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions.","Using this, we create a dataset called MyCanvas, that can be used to benchmark the task of multi-concept personalization.","In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized text-to-image diffusion methods.","We provide a simple baseline built on top of Custom Diffusion with empirical prompting strategies for future researchers to evaluate on MyCanvas.","We show that by improving data quality and prompting strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms."],"url":"http://arxiv.org/abs/2402.15504v1","category":"cs.CV"}
{"created":"2024-02-23 18:54:09","title":"Generative invariance: causal extrapolation without exogeneity","abstract":"We present a new estimator for predicting outcomes in different distributional settings under hidden confounding without relying on instruments or exogenous variables. The population definition of our estimator identifies causal parameters, whose empirical version is plugged into a generative model capable of replicating the conditional law within a test environment. We check that the probabilistic affinity between our proposal and test distributions is invariant across interventions. This work enhances the current statistical comprehension of causality by demonstrating that predictions in a test environment can be made without the need for exogenous variables and without specific assumptions regarding the strength of perturbations or the overlap of distributions.","sentences":["We present a new estimator for predicting outcomes in different distributional settings under hidden confounding without relying on instruments or exogenous variables.","The population definition of our estimator identifies causal parameters, whose empirical version is plugged into a generative model capable of replicating the conditional law within a test environment.","We check that the probabilistic affinity between our proposal and test distributions is invariant across interventions.","This work enhances the current statistical comprehension of causality by demonstrating that predictions in a test environment can be made without the need for exogenous variables and without specific assumptions regarding the strength of perturbations or the overlap of distributions."],"url":"http://arxiv.org/abs/2402.15502v1","category":"math.ST"}
{"created":"2024-02-23 18:45:18","title":"$t\\bar{t}b\\bar{b}$ at NLO precision in a variable flavor number scheme","abstract":"Top-quark pair production in association with two b-jets is computed at next-to-leading order QCD precision, including effects of the b-quark mass, and matched to a $t\\bar{t}$+jets simulation in a variable flavor number scheme. The Monte Carlo realization of this method, called fusing, consistently embeds the four-flavor calculation in a particle-level event generator. As a first phenomenological application, we present observables relevant to the data-driven estimation of irreducible backgrounds to $t\\bar{t}H$-production.","sentences":["Top-quark pair production in association with two b-jets is computed at next-to-leading order QCD precision, including effects of the b-quark mass, and matched to a $t\\bar{t}$+jets simulation in a variable flavor number scheme.","The Monte Carlo realization of this method, called fusing, consistently embeds the four-flavor calculation in a particle-level event generator.","As a first phenomenological application, we present observables relevant to the data-driven estimation of irreducible backgrounds to $t\\bar{t}H$-production."],"url":"http://arxiv.org/abs/2402.15497v1","category":"hep-ph"}
{"created":"2024-02-23 18:39:59","title":"On the structure of finitely generated subgroups of branch groups","abstract":"We describe the block structure of finitely generated subgroups of branch groups with the so-called subgroup induction property, including the first Grigorchuk group $\\mathcal{G}$ and the torsion GGS groups.","sentences":["We describe the block structure of finitely generated subgroups of branch groups with the so-called subgroup induction property, including the first Grigorchuk group $\\mathcal{G}$ and the torsion GGS groups."],"url":"http://arxiv.org/abs/2402.15496v1","category":"math.GR"}
{"created":"2024-02-23 18:35:28","title":"Super Caldero--Chapoton map for type $A$","abstract":"One can explicitly compute the generators of a surface cluster algebra either combinatorially, through dimer covers of snake graphs, or homologically, through the CC-map applied to indecomposable modules over the appropriate algebra. Recent work by Musiker, Ovenhouse and Zhang used Penner and Zeitlin's decorated super Teichm{\\\"u}ller theory to define a super version of the cluster algebra of type $A$ and gave a combinatorial formula to compute the even generators. We extend this theory by giving a homological way of explicitly computing these generators by defining a super CC-map for type $A$.","sentences":["One can explicitly compute the generators of a surface cluster algebra either combinatorially, through dimer covers of snake graphs, or homologically, through the CC-map applied to indecomposable modules over the appropriate algebra.","Recent work by Musiker, Ovenhouse and Zhang used Penner and Zeitlin's decorated super Teichm{\\\"u}ller theory to define a super version of the cluster algebra of type $A$ and gave a combinatorial formula to compute the even generators.","We extend this theory by giving a homological way of explicitly computing these generators by defining a super CC-map for type $A$."],"url":"http://arxiv.org/abs/2402.15495v1","category":"math.RT"}
{"created":"2024-02-23 18:30:49","title":"API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs","abstract":"There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes.","sentences":["There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks.","As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs.","Two lines of research have emerged as the predominant strategies for addressing this challenge.","The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks.","In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs.","The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs.","We demonstrate the utility of the API-BLEND dataset for both training and benchmarking purposes."],"url":"http://arxiv.org/abs/2402.15491v1","category":"cs.CL"}
{"created":"2024-02-23 18:28:57","title":"A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends","abstract":"In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.","sentences":["In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation.","There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others.","Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks.","It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses.","Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future.","We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives.","Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning.","This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends."],"url":"http://arxiv.org/abs/2402.15490v1","category":"cs.LG"}
{"created":"2024-02-23 18:27:23","title":"Perturbative criteria for the ergodicity of interacting dissipative quantum lattice systems","abstract":"We introduce a class of quantum Markov semigroups describing the evolution of interacting quantum lattice systems, specified either as generic qudits or as fermions. The corresponding generators, which include both conservative and dissipative evolutions, are given by the superposition of local generators in the Lindblad form. Under general conditions, we show that the associated infinite volume dynamics is well defined and can be obtained as the strong limit of the finite volume dynamics. By regarding the interacting evolution as a perturbation of a non-interacting dissipative dynamics, we further obtain a quantitative criterion that yields the ergodicity of the quantum Markov semigroup together with the exponential convergence of local observables. The analysis is based on suitable a priori bounds on the resolvent equation which yield quantitive estimates on the evolution of local observables.","sentences":["We introduce a class of quantum Markov semigroups describing the evolution of interacting quantum lattice systems, specified either as generic qudits or as fermions.","The corresponding generators, which include both conservative and dissipative evolutions, are given by the superposition of local generators in the Lindblad form.","Under general conditions, we show that the associated infinite volume dynamics is well defined and can be obtained as the strong limit of the finite volume dynamics.","By regarding the interacting evolution as a perturbation of a non-interacting dissipative dynamics, we further obtain a quantitative criterion that yields the ergodicity of the quantum Markov semigroup together with the exponential convergence of local observables.","The analysis is based on suitable a priori bounds on the resolvent equation which yield quantitive estimates on the evolution of local observables."],"url":"http://arxiv.org/abs/2402.15488v1","category":"math-ph"}
{"created":"2024-02-23 18:27:17","title":"RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation","abstract":"Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities. The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG. We apply our system across various real-world settings in a zero-shot manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before. Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth.","sentences":["Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments.","Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions.","This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments.","In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment.","The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene.","To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large Multimodal Model (LMM) and an explicit memory design to enhance our system's capabilities.","The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG.","We apply our system across various real-world settings in a zero-shot manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before.","Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth."],"url":"http://arxiv.org/abs/2402.15487v1","category":"cs.RO"}
{"created":"2024-02-23 18:26:16","title":"The Sample Average Approximation Method for Solving Two-Stage Stochastic Programs with Endogenous Uncertainty","abstract":"Real-world decision-making problems involve Type 1 decision-dependent uncertainty, where the probability distribution of the stochastic process depends on the model decisions. However, few studies focus on two-stage stochastic programs with this type of endogenous uncertainty, and those that do lack general methodologies. We thus propose herein a general method for solving a class of these programs based on the transformation of random variables, a technique widely employed in probability and statistics. The proposed method is tailored to large-scale problems with discrete or continuous endogenous random variables. The random variable transformation allows the use of the sample average approximation (SAA) method, which provides optimality convergence guarantees under certain conditions. We show that, for some classical distributions, the proposed method reduces to solving mixed-integer linear or convex programs. Finally, we validate this method by applying it to a network design and facility-protection problem, considering distinct decision-dependent distributions for the random variables. Whereas most distributions result in a nonlinear nonconvex deterministic equivalent program, the proposed method solves mixed-integer linear programs in all cases. In addition, it produces attractive performance estimators for the SAA method in a reasonable computational time and outperforms the case in which the endogenous distribution defines a mixed-integer deterministic equivalent.","sentences":["Real-world decision-making problems involve Type 1 decision-dependent uncertainty, where the probability distribution of the stochastic process depends on the model decisions.","However, few studies focus on two-stage stochastic programs with this type of endogenous uncertainty, and those that do lack general methodologies.","We thus propose herein a general method for solving a class of these programs based on the transformation of random variables, a technique widely employed in probability and statistics.","The proposed method is tailored to large-scale problems with discrete or continuous endogenous random variables.","The random variable transformation allows the use of the sample average approximation (SAA) method, which provides optimality convergence guarantees under certain conditions.","We show that, for some classical distributions, the proposed method reduces to solving mixed-integer linear or convex programs.","Finally, we validate this method by applying it to a network design and facility-protection problem, considering distinct decision-dependent distributions for the random variables.","Whereas most distributions result in a nonlinear nonconvex deterministic equivalent program, the proposed method solves mixed-integer linear programs in all cases.","In addition, it produces attractive performance estimators for the SAA method in a reasonable computational time and outperforms the case in which the endogenous distribution defines a mixed-integer deterministic equivalent."],"url":"http://arxiv.org/abs/2402.15486v1","category":"math.OC"}
{"created":"2024-02-23 18:25:19","title":"On Distinct Angles in the Plane","abstract":"We prove that if $N$ points lie in convex position in the plane then they determine $N^{1+ 3/23-o(1)}$ distinct angles, provided the points do not lie on a common circle. This is the first super-linear bound on the distinct angles problem that has received recent attention. This is derived from a more general claim that if $N$ points in the convex position in the real plane determine $KN$ distinct angles, then $K=\\Omega(N^{1/4})$ or $\\Omega(N/K)$ points are co-circular. The proof makes use of the implicit order one can give to points in convex position, recently used by Solymosi. Convex position also allows us to divide our set into two large ordered pieces. We use the squeezing lemma and the level sets of repeated angles to estimate the number of angles formed between these pieces. We obtain the main theorem using Stevens and Warren's convexity versus sum-set bound.","sentences":["We prove that if $N$ points lie in convex position in the plane then they determine $N^{1+ 3/23-o(1)}$ distinct angles, provided the points do not lie on a common circle.","This is the first super-linear bound on the distinct angles problem that has received recent attention.","This is derived from a more general claim that if $N$ points in the convex position in the real plane determine $KN$ distinct angles, then $K=\\Omega(N^{1/4})$ or $\\Omega(N/K)$ points are co-circular.","The proof makes use of the implicit order one can give to points in convex position, recently used by Solymosi.","Convex position also allows us to divide our set into two large ordered pieces.","We use the squeezing lemma and the level sets of repeated angles to estimate the number of angles formed between these pieces.","We obtain the main theorem using Stevens and Warren's convexity versus sum-set bound."],"url":"http://arxiv.org/abs/2402.15484v1","category":"math.CO"}
{"created":"2024-02-23 18:15:56","title":"Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models","abstract":"The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemming from their generation inconsistency. In addition, we utilize a data-mining approach to gather preference-detecting probes from sentence skeletons, devoid of attribute indications, to approximate LLMs' applied contexts. While initially intended for assessing discrimination in LLMs, our proposed PCF facilitates the comprehensive and flexible measurement of any inductive biases, including knowledge alongside prejudice, across various modality models. We apply our discrimination-measuring framework to 12 common LLMs, yielding intriguing findings: i) modern LLMs demonstrate significant pro-male stereotypes, ii) LLMs' exhibited discrimination correlates with several social and economic factors, iii) prejudice risk dominates the overall discrimination risk and follows a normal distribution, and iv) caprice risk contributes minimally to the overall risk but follows a fat-tailed distribution, suggesting that it is wild risk requiring enhanced surveillance.","sentences":["The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability.","However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts.","In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts.","Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemming from their generation inconsistency.","In addition, we utilize a data-mining approach to gather preference-detecting probes from sentence skeletons, devoid of attribute indications, to approximate LLMs' applied contexts.","While initially intended for assessing discrimination in LLMs, our proposed PCF facilitates the comprehensive and flexible measurement of any inductive biases, including knowledge alongside prejudice, across various modality models.","We apply our discrimination-measuring framework to 12 common LLMs, yielding intriguing findings: i) modern LLMs demonstrate significant pro-male stereotypes, ii) LLMs' exhibited discrimination correlates with several social and economic factors, iii) prejudice risk dominates the overall discrimination risk and follows a normal distribution, and iv) caprice risk contributes minimally to the overall risk but follows a fat-tailed distribution, suggesting that it is wild risk requiring enhanced surveillance."],"url":"http://arxiv.org/abs/2402.15481v1","category":"cs.CL"}
{"created":"2024-02-23 18:15:30","title":"Dynamical systems of an infinite-dimensional non-linear operator","abstract":"We investigate discrete-time dynamical systems generated by an infinite-dimensional non-linear operator that maps the Banach space $l_1$ to itself. It is demonstrated that this operator possesses up to seven fixed points. By leveraging the specific form of our operator, we illustrate that analyzing the operator can be simplified to a two-dimensional approach. Subsequently, we provide a detailed description of all fixed points, invariant sets for the two-dimensional operator and determine the set of limit points for its trajectories. These results are then applied to find the set of limit points for trajectories generated by the infinite-dimensional operator.","sentences":["We investigate discrete-time dynamical systems generated by an infinite-dimensional non-linear operator that maps the Banach space $l_1$ to itself.","It is demonstrated that this operator possesses up to seven fixed points.","By leveraging the specific form of our operator, we illustrate that analyzing the operator can be simplified to a two-dimensional approach.","Subsequently, we provide a detailed description of all fixed points, invariant sets for the two-dimensional operator and determine the set of limit points for its trajectories.","These results are then applied to find the set of limit points for trajectories generated by the infinite-dimensional operator."],"url":"http://arxiv.org/abs/2402.15479v1","category":"math.DS"}
{"created":"2024-02-23 18:07:29","title":"Length and Velocity Scales in Protoplanetary Disk Turbulence","abstract":"In the theory of protoplanetary disk turbulence, a widely adopted \\emph{ansatz}, or assumption, is that the turnover frequency of the largest turbulent eddy, $\\Omega_L$, is the local Keplerian frequency $\\Omega_K$. In terms of the standard dimensionless Shakura-Sunyaev $\\alpha$ parameter that quantifies turbulent viscosity or diffusivity, this assumption leads to characteristic length and velocity scales given respectively by $\\sqrt{\\alpha}H$ and $\\sqrt{\\alpha}c$, in which $H$ and $c$ are the local gas scale height and sound speed. However, this assumption is not applicable in cases when turbulence is forced numerically or driven by some natural processes such as Vertical Shear Instability. Here we explore the more general case where $\\Omega_L\\ge\\Omega_K$ and show that under these conditions, the characteristic length and velocity scales are respectively $\\sqrt{\\alpha/R'}H$ and $\\sqrt{\\alpha R'}c$, where $R'\\equiv \\Omega_L/\\Omega_K$ is twice the Rossby number. It follows that $\\alpha=\\alphat/R'$, where $\\sqrt{\\alphat} c$ is the root-mean-square average of the turbulent velocities. Properly allowing for this effect naturally explains the reduced particle scale heights produced in shearing box simulations of particles in forced turbulence, and may help with interpreting recent edge-on disk observations; more general implications for observations are also presented. For $R'>1$ the effective particle Stokes numbers are increased, which has implications for particle collision dynamics and growth, as well as for planetesimal formation.","sentences":["In the theory of protoplanetary disk turbulence, a widely adopted \\emph{ansatz}, or assumption, is that the turnover frequency of the largest turbulent eddy, $\\Omega_L$, is the local Keplerian frequency $\\Omega_K$. In terms of the standard dimensionless Shakura-Sunyaev $\\alpha$ parameter that quantifies turbulent viscosity or diffusivity, this assumption leads to characteristic length and velocity scales given respectively by $\\sqrt{\\alpha}H$ and $\\sqrt{\\alpha}c$, in which $H$ and $c$ are the local gas scale height and sound speed.","However, this assumption is not applicable in cases when turbulence is forced numerically or driven by some natural processes such as Vertical Shear Instability.","Here we explore the more general case where $\\Omega_L\\ge\\Omega_K$ and show that under these conditions, the characteristic length and velocity scales are respectively $\\sqrt{\\alpha/R'}H$ and $\\sqrt{\\alpha R'}c$, where $R'\\equiv \\Omega_L/\\Omega_K$ is twice the Rossby number.","It follows that $\\alpha=\\alphat/R'$, where $\\sqrt{\\alphat} c$ is the root-mean-square average of the turbulent velocities.","Properly allowing for this effect naturally explains the reduced particle scale heights produced in shearing box simulations of particles in forced turbulence, and may help with interpreting recent edge-on disk observations; more general implications for observations are also presented.","For $R'>1$ the effective particle Stokes numbers are increased, which has implications for particle collision dynamics and growth, as well as for planetesimal formation."],"url":"http://arxiv.org/abs/2402.15475v1","category":"astro-ph.EP"}
{"created":"2024-02-23 18:02:51","title":"Modeling Phonon-mediated Quasiparticle Poisoning in Superconducting Qubit Arrays","abstract":"Correlated errors caused by ionizing radiation impacting superconducting qubit chips are problematic for quantum error correction. Such impacts generate quasiparticle (QP) excitations in the qubit electrodes, which temporarily reduce qubit coherence significantly. The many energetic phonons produced by a particle impact travel efficiently throughout the device substrate and generate quasiparticles with high probability, thus causing errors on a large fraction of the qubits in an array simultaneously. We describe a comprehensive strategy for the numerical simulation of the phonon and quasiparticle dynamics in the aftermath of an impact. We compare the simulations with experimental measurements of phonon-mediated QP poisoning and demonstrate that our modeling captures the spatial and temporal footprint of the QP poisoning for various configurations of phonon downconversion structures. We thus present a path forward for the operation of superconducting quantum processors in the presence of ionizing radiation.","sentences":["Correlated errors caused by ionizing radiation impacting superconducting qubit chips are problematic for quantum error correction.","Such impacts generate quasiparticle (QP) excitations in the qubit electrodes, which temporarily reduce qubit coherence significantly.","The many energetic phonons produced by a particle impact travel efficiently throughout the device substrate and generate quasiparticles with high probability, thus causing errors on a large fraction of the qubits in an array simultaneously.","We describe a comprehensive strategy for the numerical simulation of the phonon and quasiparticle dynamics in the aftermath of an impact.","We compare the simulations with experimental measurements of phonon-mediated QP poisoning and demonstrate that our modeling captures the spatial and temporal footprint of the QP poisoning for various configurations of phonon downconversion structures.","We thus present a path forward for the operation of superconducting quantum processors in the presence of ionizing radiation."],"url":"http://arxiv.org/abs/2402.15471v1","category":"quant-ph"}
{"created":"2024-02-23 18:00:06","title":"Benchmarking the Robustness of Panoptic Segmentation for Automated Driving","abstract":"Precise situational awareness is required for the safe decision-making of assisted and automated driving (AAD) functions. Panoptic segmentation is a promising perception technique to identify and categorise objects, impending hazards, and driveable space at a pixel level. While segmentation quality is generally associated with the quality of the camera data, a comprehensive understanding and modelling of this relationship are paramount for AAD system designers. Motivated by such a need, this work proposes a unifying pipeline to assess the robustness of panoptic segmentation models for AAD, correlating it with traditional image quality. The first step of the proposed pipeline involves generating degraded camera data that reflects real-world noise factors. To this end, 19 noise factors have been identified and implemented with 3 severity levels. Of these factors, this work proposes novel models for unfavourable light and snow. After applying the degradation models, three state-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation networks are used to analyse their robustness. The variations of the segmentation performance are then correlated to 8 selected image quality metrics. This research reveals that: 1) certain specific noise factors produce the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian noise; 2) the ViT-based panoptic segmentation backbones show better robustness to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and CW-SSIM) correlate strongly with panoptic segmentation performance and therefore they can be used as predictive metrics for network performance.","sentences":["Precise situational awareness is required for the safe decision-making of assisted and automated driving (AAD) functions.","Panoptic segmentation is a promising perception technique to identify and categorise objects, impending hazards, and driveable space at a pixel level.","While segmentation quality is generally associated with the quality of the camera data, a comprehensive understanding and modelling of this relationship are paramount for AAD system designers.","Motivated by such a need, this work proposes a unifying pipeline to assess the robustness of panoptic segmentation models for AAD, correlating it with traditional image quality.","The first step of the proposed pipeline involves generating degraded camera data that reflects real-world noise factors.","To this end, 19 noise factors have been identified and implemented with 3 severity levels.","Of these factors, this work proposes novel models for unfavourable light and snow.","After applying the degradation models, three state-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation networks are used to analyse their robustness.","The variations of the segmentation performance are then correlated to 8 selected image quality metrics.","This research reveals that: 1) certain specific noise factors produce the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian noise; 2) the ViT-based panoptic segmentation backbones show better robustness to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and CW-SSIM) correlate strongly with panoptic segmentation performance and therefore they can be used as predictive metrics for network performance."],"url":"http://arxiv.org/abs/2402.15469v1","category":"cs.CV"}
{"created":"2024-02-23 17:58:05","title":"Spin-2 Green's Functions on Kerr in Radiation Gauge","abstract":"We construct retarded and advanced Green's functions for gravitational perturbations in Kerr in an ingoing radiation gauge. Our Green's functions have a frequency domain piece that has previously been obtained by Ori [Phys. Rev. D 67 (2003)] based on the Chrzanowski-Cohen-Kegeles metric reconstruction method. As is well known, this piece by itself is not sufficient to obtain an actual Green's function. We show how to complete it with a piece based on a method by Green et al. [Class. Quant. Grav. 37 (2020)]. The completion piece has a completely explicit form in the time-domain and is supported on pairs of points on the same outgoing principal null geodesic which are in the appropriate causal order. We expect our Green's functions to be useful for gravitational self-force calculations and other perturbation problems on Kerr spacetime.","sentences":["We construct retarded and advanced Green's functions for gravitational perturbations in Kerr in an ingoing radiation gauge.","Our Green's functions have a frequency domain piece that has previously been obtained by Ori [Phys.","Rev. D 67 (2003)] based on the Chrzanowski-Cohen-Kegeles metric reconstruction method.","As is well known, this piece by itself is not sufficient to obtain an actual Green's function.","We show how to complete it with a piece based on a method by Green et al.","[Class. Quant.","Grav.","37 (2020)].","The completion piece has a completely explicit form in the time-domain and is supported on pairs of points on the same outgoing principal null geodesic which are in the appropriate causal order.","We expect our Green's functions to be useful for gravitational self-force calculations and other perturbation problems on Kerr spacetime."],"url":"http://arxiv.org/abs/2402.15468v1","category":"gr-qc"}
{"created":"2024-02-23 17:56:41","title":"Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?","abstract":"The advent of generative AI (GenAI) technology produces transformative impact on the content creation landscape, offering alternative approaches to produce diverse, high-quality content across media, thereby reshaping online ecosystems but also raising concerns about market over-saturation and the potential marginalization of human creativity. Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and GenAI. Our theory and simulations suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in GenAI.","sentences":["The advent of generative AI (GenAI) technology produces transformative impact on the content creation landscape, offering alternative approaches to produce diverse, high-quality content across media, thereby reshaping online ecosystems but also raising concerns about market over-saturation and the potential marginalization of human creativity.","Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and GenAI.","Our theory and simulations suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible.","Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in GenAI."],"url":"http://arxiv.org/abs/2402.15467v1","category":"cs.GT"}
{"created":"2024-02-23 17:55:43","title":"Automatic treatment planning for radiotherapy: a cross-modality and protocol study","abstract":"This study investigates the applicability of 3D dose predictions from a model trained on one modality to a cross-modality automated planning workflow. Additionally, we explore the impact of integrating a multi-criteria optimizer on adapting predictions to different clinical preferences. Using a previously created three-stage UNet in-house model trained on the 2020 AAPM OpenKBP challenge dataset (340 head and neck plans, planned using 9-field static IMRT), we retrospectively generated dose predictions for 20 patients. These dose predictions were used to generate deliverable IMRT, VMAT, and Tomotherapy plans using the fallback plan functionality in Raystation. The deliverable plans were evaluated against the dose predictions based on primary clinical goals. A new set of plans was also generated using MCO-based optimization with predicted dose values as constraints. The mimicking approach accurately replicated the predicted dose distributions across different modalities, with slight deviations in spinal cord and external contour maximum doses. MCO customization significantly reduced doses to OARs prioritized by our institution while maintaining target coverage. All tested plans met clinical deliverability standards, evidenced by a delivery QA gamma analysis passing rate above 98%. Our findings show that a model trained only on IMRT plans can contribute to planning across various modalities. Additionally, integrating predictions as constraints in an MCO-based workflow, rather than direct dose mimicking, enables a flexible, warm-start approach for treatment planning. Together, these approaches have the potential to significantly decrease plan turnaround time and quality variance, both at high resource medical centers that can train in-house models, and smaller centers that can adapt a model from another institution with minimal effort.","sentences":["This study investigates the applicability of 3D dose predictions from a model trained on one modality to a cross-modality automated planning workflow.","Additionally, we explore the impact of integrating a multi-criteria optimizer on adapting predictions to different clinical preferences.","Using a previously created three-stage UNet in-house model trained on the 2020 AAPM OpenKBP challenge dataset (340 head and neck plans, planned using 9-field static IMRT), we retrospectively generated dose predictions for 20 patients.","These dose predictions were used to generate deliverable IMRT, VMAT, and Tomotherapy plans using the fallback plan functionality in Raystation.","The deliverable plans were evaluated against the dose predictions based on primary clinical goals.","A new set of plans was also generated using MCO-based optimization with predicted dose values as constraints.","The mimicking approach accurately replicated the predicted dose distributions across different modalities, with slight deviations in spinal cord and external contour maximum doses.","MCO customization significantly reduced doses to OARs prioritized by our institution while maintaining target coverage.","All tested plans met clinical deliverability standards, evidenced by a delivery QA gamma analysis passing rate above 98%.","Our findings show that a model trained only on IMRT plans can contribute to planning across various modalities.","Additionally, integrating predictions as constraints in an MCO-based workflow, rather than direct dose mimicking, enables a flexible, warm-start approach for treatment planning.","Together, these approaches have the potential to significantly decrease plan turnaround time and quality variance, both at high resource medical centers that can train in-house models, and smaller centers that can adapt a model from another institution with minimal effort."],"url":"http://arxiv.org/abs/2402.15466v1","category":"physics.med-ph"}
{"created":"2024-02-23 17:55:28","title":"Order-detection, representation-detection, and applications to cable knots","abstract":"Given a $3$-manifold $M$ with multiple incompressible torus boundary components, we develop a general definition of order-detection of tuples of slopes on the boundary components of $M$. In parallel, we arrive at a general definition of representation-detection of tuples of slopes, and show that these two kinds of slope detection are equivalent -- in the sense that a tuple of slopes on the boundary of $M$ is order-detected if and only if it is representation-detected. We use these results, together with new \"relative gluing theorems,\" to show how the work of Eisenbud-Hirsch-Neumann, Jankins-Neumann and Naimi can be used to determine tuples of representation-detected slopes and, in turn, the behaviour of order-detected slopes on the boundary of a knot manifold with respect to cabling. Our cabling results improve upon work of the first author and Watson, and in particular, this new approach shows how one can use the equivalence between representation-detection and order-detection to derive orderability results that parallel known behaviour of L-spaces with respect to Dehn filling.","sentences":["Given a $3$-manifold $M$ with multiple incompressible torus boundary components, we develop a general definition of order-detection of tuples of slopes on the boundary components of $M$. In parallel, we arrive at a general definition of representation-detection of tuples of slopes, and show that these two kinds of slope detection are equivalent -- in the sense that a tuple of slopes on the boundary of $M$ is order-detected if and only if it is representation-detected.","We use these results, together with new \"relative gluing theorems,\" to show how the work of Eisenbud-Hirsch-Neumann, Jankins-Neumann and Naimi can be used to determine tuples of representation-detected slopes and, in turn, the behaviour of order-detected slopes on the boundary of a knot manifold with respect to cabling.","Our cabling results improve upon work of the first author and Watson, and in particular, this new approach shows how one can use the equivalence between representation-detection and order-detection to derive orderability results that parallel known behaviour of L-spaces with respect to Dehn filling."],"url":"http://arxiv.org/abs/2402.15465v1","category":"math.GT"}
{"created":"2024-02-23 17:49:46","title":"Pattern-restricted permutations of small order","abstract":"We enumerate 132-avoiding permutations of order 3 in terms of the Catalan and Motzkin generating functions, answering a question of B\\'{o}na and Smith from 2019. We also enumerate 231-avoiding permutations that are composed only of 3-cycles, 2-cycles, and fixed points.","sentences":["We enumerate 132-avoiding permutations of order 3 in terms of the Catalan and Motzkin generating functions, answering a question of B\\'{o}na and Smith from 2019.","We also enumerate 231-avoiding permutations that are composed only of 3-cycles, 2-cycles, and fixed points."],"url":"http://arxiv.org/abs/2402.15463v1","category":"math.CO"}
{"created":"2024-02-23 17:33:07","title":"Design and Optimization of Functionally-graded Triangular Lattices for Multiple Loading Conditions","abstract":"Aligning lattices based on local stress distribution is crucial for achieving exceptional structural stiffness. However, this aspect has primarily been investigated under a single load condition, where stress in 2D can be described by two orthogonal principal stress directions. In this paper, we introduce a novel approach for designing and optimizing triangular lattice structures to accommodate multiple loading conditions, which means multiple stress fields. Our method comprises two main steps: homogenization-based topology optimization and geometry-based de-homogenization. To ensure the geometric regularity of triangular lattices, we propose a simplified version of the general rank-$3$ laminate and parameterize the design domain using equilateral triangles with unique thickness per edge. During optimization, the thicknesses and orientation of each equilateral triangle are adjusted based on the homogenized properties of triangular lattices. Our numerical findings demonstrate that this proposed simplification results in only a slight decrease in stiffness, while achieving triangular lattice structures with a compelling geometric regularity. In geometry-based de-homogenization, we adopt a field-aligned triangulation approach to generate a globally consistent triangle mesh, with each triangle oriented according to the optimized orientation field. Our approach for handling multiple loading conditions, akin to de-homogenization techniques for single loading conditions, yields highly detailed, optimized, spatially varying lattice structures. The method is computationally efficient, as simulations and optimizations are conducted at a low-resolution discretization of the design domain. Furthermore, since our approach is geometry-based, obtained structures are encoded into a compact geometric format that facilitates downstream operations such as editing and fabrication.","sentences":["Aligning lattices based on local stress distribution is crucial for achieving exceptional structural stiffness.","However, this aspect has primarily been investigated under a single load condition, where stress in 2D can be described by two orthogonal principal stress directions.","In this paper, we introduce a novel approach for designing and optimizing triangular lattice structures to accommodate multiple loading conditions, which means multiple stress fields.","Our method comprises two main steps: homogenization-based topology optimization and geometry-based de-homogenization.","To ensure the geometric regularity of triangular lattices, we propose a simplified version of the general rank-$3$ laminate and parameterize the design domain using equilateral triangles with unique thickness per edge.","During optimization, the thicknesses and orientation of each equilateral triangle are adjusted based on the homogenized properties of triangular lattices.","Our numerical findings demonstrate that this proposed simplification results in only a slight decrease in stiffness, while achieving triangular lattice structures with a compelling geometric regularity.","In geometry-based de-homogenization, we adopt a field-aligned triangulation approach to generate a globally consistent triangle mesh, with each triangle oriented according to the optimized orientation field.","Our approach for handling multiple loading conditions, akin to de-homogenization techniques for single loading conditions, yields highly detailed, optimized, spatially varying lattice structures.","The method is computationally efficient, as simulations and optimizations are conducted at a low-resolution discretization of the design domain.","Furthermore, since our approach is geometry-based, obtained structures are encoded into a compact geometric format that facilitates downstream operations such as editing and fabrication."],"url":"http://arxiv.org/abs/2402.15458v1","category":"cs.CE"}
{"created":"2024-02-23 17:30:53","title":"The dynamics of self-gravity wakes in the Mimas 5:3 bending wave: modifying the linear theory","abstract":"The satellite Mimas launches a bending wave -- a warping of the rings that propagates radially through self-gravity -- at the 5:3 inner vertical resonance with Saturn's rings. We present a modification of the linear bending wave theory which includes the effects of satellite self-gravity wakes on the particles in the wave. We show that, when treated as rigid, these wakes generate an extra layer of particles whose number density is proportional to the magnitude of the slope of the warped ring. Using a ray-tracing code we compare our predictions with those of linear bending wave theory and with 60 stellar occultations observed by the Cassini Ultraviolet Imaging Spectrograph (UVIS) and find that the extra layer of particles of our perturbed bending wave model has a considerable explanatory power for the UVIS dataset. Our best model explains the most discrepant and surprising features of the Mimas 5:3 bending wave; the enhancement of the signal for the cases of occultations with high ring opening angle and the bigger-than-expected viscosity, $\\nu = 576 \\, \\mathrm{cm^2/s}$, which is more than double the viscosity computed from density waves. This shows that self-gravity wakes can be effective at transporting angular momentum in a vertically perturbed disk. Relative to neighboring density waves, we find a lower-than-expected value for the surface mass density, $\\sigma = 36.7 \\, \\mathrm{g/cm^2}$, which suggests that the enhanced viscous interactions may be transporting material into the surrounding regions.","sentences":["The satellite Mimas launches a bending wave -- a warping of the rings that propagates radially through self-gravity -- at the 5:3 inner vertical resonance with Saturn's rings.","We present a modification of the linear bending wave theory which includes the effects of satellite self-gravity wakes on the particles in the wave.","We show that, when treated as rigid, these wakes generate an extra layer of particles whose number density is proportional to the magnitude of the slope of the warped ring.","Using a ray-tracing code we compare our predictions with those of linear bending wave theory and with 60 stellar occultations observed by the Cassini Ultraviolet Imaging Spectrograph (UVIS) and find that the extra layer of particles of our perturbed bending wave model has a considerable explanatory power for the UVIS dataset.","Our best model explains the most discrepant and surprising features of the Mimas 5:3 bending wave; the enhancement of the signal for the cases of occultations with high ring opening angle and the bigger-than-expected viscosity, $\\nu = 576 \\, \\mathrm{cm^2/s}$, which is more than double the viscosity computed from density waves.","This shows that self-gravity wakes can be effective at transporting angular momentum in a vertically perturbed disk.","Relative to neighboring density waves, we find a lower-than-expected value for the surface mass density, $\\sigma = 36.7 \\, \\mathrm{g/cm^2}$, which suggests that the enhanced viscous interactions may be transporting material into the surrounding regions."],"url":"http://arxiv.org/abs/2402.15456v1","category":"astro-ph.EP"}
{"created":"2024-02-23 17:29:44","title":"Sandpile groups for cones over trees","abstract":"Sandpile groups are a subtle graph isomorphism invariant, in the form of a finite abelian group, whose cardinality is the number of spanning trees in the graph. We study their group structure for graphs obtained by attaching a cone vertex to a tree. For example, it is shown that the number of generators of the sandpile group is at most one less than the number of leaves in the tree. For trees on a fixed number of vertices, the paths and stars are shown to provide extreme behavior, not only for the number of generators, but also for the number of spanning trees, and for Tutte polynomial evaluations that count the recurrent sandpile configurations by their numbers of chips.","sentences":["Sandpile groups are a subtle graph isomorphism invariant, in the form of a finite abelian group, whose cardinality is the number of spanning trees in the graph.","We study their group structure for graphs obtained by attaching a cone vertex to a tree.","For example, it is shown that the number of generators of the sandpile group is at most one less than the number of leaves in the tree.","For trees on a fixed number of vertices, the paths and stars are shown to provide extreme behavior, not only for the number of generators, but also for the number of spanning trees, and for Tutte polynomial evaluations that count the recurrent sandpile configurations by their numbers of chips."],"url":"http://arxiv.org/abs/2402.15453v1","category":"math.CO"}
{"created":"2024-02-23 17:27:41","title":"Characterizing BV- and BD-ellipticity for a class of positively 1-homogeneous surface energy densities","abstract":"Lower semicontinuity of surface energies in integral form is known to be equivalent to BV-ellipticity of the surface density. In this paper, we prove that BV-ellipticity coincides with the simpler notion of biconvexity for a class of densities that depend only on the jump height and jump normal, and are positively 1-homogeneous in the first argument. The second main result is the analogous statement in the setting of bounded deformations, where we show that BD-ellipticity reduces to symmetric biconvexity. Our techniques are primarily inspired by constructions from the analysis of structured deformations and the general theory of free discontinuity problems.","sentences":["Lower semicontinuity of surface energies in integral form is known to be equivalent to BV-ellipticity of the surface density.","In this paper, we prove that BV-ellipticity coincides with the simpler notion of biconvexity for a class of densities that depend only on the jump height and jump normal, and are positively 1-homogeneous in the first argument.","The second main result is the analogous statement in the setting of bounded deformations, where we show that BD-ellipticity reduces to symmetric biconvexity.","Our techniques are primarily inspired by constructions from the analysis of structured deformations and the general theory of free discontinuity problems."],"url":"http://arxiv.org/abs/2402.15450v1","category":"math.AP"}
{"created":"2024-02-23 17:23:06","title":"Computer Vision for Multimedia Geolocation in Human Trafficking Investigation: A Systematic Literature Review","abstract":"The task of multimedia geolocation is becoming an increasingly essential component of the digital forensics toolkit to effectively combat human trafficking, child sexual exploitation, and other illegal acts. Typically, metadata-based geolocation information is stripped when multimedia content is shared via instant messaging and social media. The intricacy of geolocating, geotagging, or finding geographical clues in this content is often overly burdensome for investigators. Recent research has shown that contemporary advancements in artificial intelligence, specifically computer vision and deep learning, show significant promise towards expediting the multimedia geolocation task. This systematic literature review thoroughly examines the state-of-the-art leveraging computer vision techniques for multimedia geolocation and assesses their potential to expedite human trafficking investigation. This includes a comprehensive overview of the application of computer vision-based approaches to multimedia geolocation, identifies their applicability in combating human trafficking, and highlights the potential implications of enhanced multimedia geolocation for prosecuting human trafficking. 123 articles inform this systematic literature review. The findings suggest numerous potential paths for future impactful research on the subject.","sentences":["The task of multimedia geolocation is becoming an increasingly essential component of the digital forensics toolkit to effectively combat human trafficking, child sexual exploitation, and other illegal acts.","Typically, metadata-based geolocation information is stripped when multimedia content is shared via instant messaging and social media.","The intricacy of geolocating, geotagging, or finding geographical clues in this content is often overly burdensome for investigators.","Recent research has shown that contemporary advancements in artificial intelligence, specifically computer vision and deep learning, show significant promise towards expediting the multimedia geolocation task.","This systematic literature review thoroughly examines the state-of-the-art leveraging computer vision techniques for multimedia geolocation and assesses their potential to expedite human trafficking investigation.","This includes a comprehensive overview of the application of computer vision-based approaches to multimedia geolocation, identifies their applicability in combating human trafficking, and highlights the potential implications of enhanced multimedia geolocation for prosecuting human trafficking.","123 articles inform this systematic literature review.","The findings suggest numerous potential paths for future impactful research on the subject."],"url":"http://arxiv.org/abs/2402.15448v1","category":"cs.CV"}
{"created":"2024-02-23 17:09:04","title":"Can we forget how we learned? Doxastic redundancy in iterated belief revision","abstract":"How information was acquired may become irrelevant. An obvious case is when something is confirmed many times. In terms of iterated belief revision, a specific revision may become irrelevant in presence of others. Simple repetitions are an example, but not the only case when this happens. Sometimes, a revision becomes redundant even in presence of none equal, or even no else implying it. A necessary and sufficient condition for the redundancy of the first of a sequence of lexicographic revisions is given. The problem is coNP-complete even with two propositional revisions only. Complexity is the same in the Horn case but only with an unbounded number of revisions: it becomes polynomial with two revisions. Lexicographic revisions are not only relevant by themselves, but also because sequences of them are the most compact of the common mechanisms used to represent the state of an iterated revision process. Shortening sequences of lexicographic revisions is shortening the most compact representations of iterated belief revision states.","sentences":["How information was acquired may become irrelevant.","An obvious case is when something is confirmed many times.","In terms of iterated belief revision, a specific revision may become irrelevant in presence of others.","Simple repetitions are an example, but not the only case when this happens.","Sometimes, a revision becomes redundant even in presence of none equal, or even no else implying it.","A necessary and sufficient condition for the redundancy of the first of a sequence of lexicographic revisions is given.","The problem is coNP-complete even with two propositional revisions only.","Complexity is the same in the Horn case but only with an unbounded number of revisions: it becomes polynomial with two revisions.","Lexicographic revisions are not only relevant by themselves, but also because sequences of them are the most compact of the common mechanisms used to represent the state of an iterated revision process.","Shortening sequences of lexicographic revisions is shortening the most compact representations of iterated belief revision states."],"url":"http://arxiv.org/abs/2402.15445v1","category":"cs.AI"}
{"created":"2024-02-23 17:07:51","title":"Origin of optical nonlinearity in plasmonic semiconductor nanostructures","abstract":"The development of nanoscale nonlinear elements in photonic integrated circuits is hindered by the physical limits to the nonlinear optical response of dielectrics, which requires that the interacting waves propagate in transparent volumes for distances much longer than their wavelength. Here we present experimental evidence that optical nonlinearities in doped semiconductors are due to free-electron and their efficiency could exceed by several orders of magnitude that of conventional dielectric nonlinearities. Our experimental findings are supported by comprehensive computational results based on the hydrodynamic modeling, which naturally includes nonlocal effects, of the free-electron dynamics in heavily doped semiconductors. By studying third-harmonic generation from plasmonic nanoantenna arrays made out of heavily n-doped InGaAs with increasing levels of free-carrier density, we discriminate between hydrodynamic and dielectric nonlinearities. As a result, the value of maximum nonlinear efficiency as well as its spectral location can now be controlled by tuning the doping level. Having employed the common material platform InGaAs/InP that supports integrated waveguides, our findings pave the way for future exploitation of plasmonic nonlinearities in all-semiconductor photonic integrated circuits.","sentences":["The development of nanoscale nonlinear elements in photonic integrated circuits is hindered by the physical limits to the nonlinear optical response of dielectrics, which requires that the interacting waves propagate in transparent volumes for distances much longer than their wavelength.","Here we present experimental evidence that optical nonlinearities in doped semiconductors are due to free-electron and their efficiency could exceed by several orders of magnitude that of conventional dielectric nonlinearities.","Our experimental findings are supported by comprehensive computational results based on the hydrodynamic modeling, which naturally includes nonlocal effects, of the free-electron dynamics in heavily doped semiconductors.","By studying third-harmonic generation from plasmonic nanoantenna arrays made out of heavily n-doped InGaAs with increasing levels of free-carrier density, we discriminate between hydrodynamic and dielectric nonlinearities.","As a result, the value of maximum nonlinear efficiency as well as its spectral location can now be controlled by tuning the doping level.","Having employed the common material platform InGaAs/InP that supports integrated waveguides, our findings pave the way for future exploitation of plasmonic nonlinearities in all-semiconductor photonic integrated circuits."],"url":"http://arxiv.org/abs/2402.15443v1","category":"physics.optics"}
{"created":"2024-02-23 17:00:32","title":"GROS: A General Robust Aggregation Strategy","abstract":"A new, very general, robust procedure for combining estimators in metric spaces is introduced GROS. The method is reminiscent of the well-known median of means, as described in \\cite{devroye2016sub}. Initially, the sample is divided into $K$ groups. Subsequently, an estimator is computed for each group. Finally, these $K$ estimators are combined using a robust procedure. We prove that this estimator is sub-Gaussian and we get its break-down point, in the sense of Donoho. The robust procedure involves a minimization problem on a general metric space, but we show that the same (up to a constant) sub-Gaussianity is obtained if the minimization is taken over the sample, making GROS feasible in practice. The performance of GROS is evaluated through five simulation studies: the first one focuses on classification using $k$-means, the second one on the multi-armed bandit problem, the third one on the regression problem. The fourth one is the set estimation problem under a noisy model. Lastly, we apply GROS to get a robust persistent diagram.","sentences":["A new, very general, robust procedure for combining estimators in metric spaces is introduced GROS.","The method is reminiscent of the well-known median of means, as described in \\cite{devroye2016sub}.","Initially, the sample is divided into $K$ groups.","Subsequently, an estimator is computed for each group.","Finally, these $K$ estimators are combined using a robust procedure.","We prove that this estimator is sub-Gaussian and we get its break-down point, in the sense of Donoho.","The robust procedure involves a minimization problem on a general metric space, but we show that the same (up to a constant) sub-Gaussianity is obtained if the minimization is taken over the sample, making GROS feasible in practice.","The performance of GROS is evaluated through five simulation studies: the first one focuses on classification using $k$-means, the second one on the multi-armed bandit problem, the third one on the regression problem.","The fourth one is the set estimation problem under a noisy model.","Lastly, we apply GROS to get a robust persistent diagram."],"url":"http://arxiv.org/abs/2402.15442v1","category":"math.ST"}
{"created":"2024-02-23 16:58:31","title":"Entanglement-assisted classical capacities of some channels acting as radial multipliers on fermion algebras","abstract":"We investigate a new class of unital quantum channels on $\\mathrm{M}_{2^k}$, acting as radial multipliers when we identify the matrix algebra $\\mathrm{M}_{2^k}$ with a finite-dimensional fermion algebra. Our primary contribution lies in the precise computation of the (optimal) rate at which classical information can be transmitted through these channels from a sender to receiver when they share an unlimited amount of noiseless entanglement. Our approach relies on new connections between fermions algebras with the $n$-dimensional discrete hypercube $\\{-1,1\\}^n$ and more generally the Cantor group. Significantly, our calculations yield exact values applicable to the operators of the fermionic Ornstein-Uhlenbeck semigroup. This advancement not only provides deeper insights into the structure and behaviour of these channels but also enhances our understanding of Quantum Information Theory in a dimension-independent context.","sentences":["We investigate a new class of unital quantum channels on $\\mathrm{M}_{2^k}$, acting as radial multipliers when we identify the matrix algebra $\\mathrm{M}_{2^k}$ with a finite-dimensional fermion algebra.","Our primary contribution lies in the precise computation of the (optimal) rate at which classical information can be transmitted through these channels from a sender to receiver when they share an unlimited amount of noiseless entanglement.","Our approach relies on new connections between fermions algebras with the $n$-dimensional discrete hypercube $\\{-1,1\\}^n$ and more generally the Cantor group.","Significantly, our calculations yield exact values applicable to the operators of the fermionic Ornstein-Uhlenbeck semigroup.","This advancement not only provides deeper insights into the structure and behaviour of these channels but also enhances our understanding of Quantum Information Theory in a dimension-independent context."],"url":"http://arxiv.org/abs/2402.15440v1","category":"quant-ph"}
{"created":"2024-02-23 16:56:01","title":"GQL-Based Bound-Preserving and Locally Divergence-Free Central Discontinuous Galerkin Schemes for Relativistic Magnetohydrodynamics","abstract":"This paper develops novel and robust central discontinuous Galerkin (CDG) schemes of arbitrarily high-order accuracy for special relativistic magnetohydrodynamics (RMHD) with a general equation of state (EOS). These schemes are provably bound-preserving (BP), i.e., consistently preserve the upper bound for subluminal fluid velocity and the positivity of density and pressure, while also (locally) maintaining the divergence-free (DF) constraint for the magnetic field. For 1D RMHD, the standard CDG method is exactly DF, and its BP property is proven under a condition achievable by BP limiter. For 2D RMHD, we design provably BP and locally DF CDG schemes based on the suitable discretization of a modified RMHD system. A key novelty in our schemes is the discretization of additional source terms in the modified RMHD equations, so as to precisely counteract the influence of divergence errors on the BP property across overlapping meshes. We provide rigorous proofs of the BP property for our CDG schemes and first establish the theoretical connection between BP and discrete DF properties on overlapping meshes for RMHD. Owing to the absence of explicit expressions for primitive variables in terms of conserved variables, the constraints of physical bounds are strongly nonlinear, making the BP proofs highly nontrivial. We overcome these challenges through technical estimates within the geometric quasilinearization (GQL) framework, which converts the nonlinear constraints into linear ones. Furthermore, we introduce a new 2D cell average decomposition on overlapping meshes, which relaxes the theoretical BP CFL constraint and reduces the number of internal nodes, thereby enhancing the efficiency of the 2D BP CDG method. We implement the proposed CDG schemes for extensive RMHD problems with various EOSs, demonstrating their robustness and effectiveness in challenging scenarios.","sentences":["This paper develops novel and robust central discontinuous Galerkin (CDG) schemes of arbitrarily high-order accuracy for special relativistic magnetohydrodynamics (RMHD) with a general equation of state (EOS).","These schemes are provably bound-preserving (BP), i.e., consistently preserve the upper bound for subluminal fluid velocity and the positivity of density and pressure, while also (locally) maintaining the divergence-free (DF) constraint for the magnetic field.","For 1D RMHD, the standard CDG method is exactly DF, and its BP property is proven under a condition achievable by BP limiter.","For 2D RMHD, we design provably BP and locally DF CDG schemes based on the suitable discretization of a modified RMHD system.","A key novelty in our schemes is the discretization of additional source terms in the modified RMHD equations, so as to precisely counteract the influence of divergence errors on the BP property across overlapping meshes.","We provide rigorous proofs of the BP property for our CDG schemes and first establish the theoretical connection between BP and discrete DF properties on overlapping meshes for RMHD.","Owing to the absence of explicit expressions for primitive variables in terms of conserved variables, the constraints of physical bounds are strongly nonlinear, making the BP proofs highly nontrivial.","We overcome these challenges through technical estimates within the geometric quasilinearization (GQL) framework, which converts the nonlinear constraints into linear ones.","Furthermore, we introduce a new 2D cell average decomposition on overlapping meshes, which relaxes the theoretical BP CFL constraint and reduces the number of internal nodes, thereby enhancing the efficiency of the 2D BP CDG method.","We implement the proposed CDG schemes for extensive RMHD problems with various EOSs, demonstrating their robustness and effectiveness in challenging scenarios."],"url":"http://arxiv.org/abs/2402.15437v1","category":"math.NA"}
{"created":"2024-02-23 16:50:07","title":"Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales","abstract":"Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize this theoretical framework into a given task. With the over-completeness, discriminative features w.r.t. the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We demonstrate the above arguments with accuracy, invariance, and efficiency results on texture, digit, and parasite classification experiments. Furthermore, at the application level, our representations are explored in real-world forensics tasks on adversarial perturbations and Artificial Intelligence Generated Content (AIGC). Such applications reveal that the proposed strategy not only realizes the theoretically promised invariance, but also exhibits competitive discriminability even in the era of deep learning. For robust and interpretable vision tasks at larger scales, hierarchical invariant representation can be considered as an effective alternative to traditional CNN and invariants.","sentences":["Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence.","In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation.","However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks.","For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives.","At the theoretical level, we show how to construct over-complete invariants with a Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a fully interpretable manner.","The general blueprint, specific definitions, invariant properties, and numerical implementations are provided.","At the practical level, we discuss how to customize this theoretical framework into a given task.","With the over-completeness, discriminative features w.r.t.","the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner.","We demonstrate the above arguments with accuracy, invariance, and efficiency results on texture, digit, and parasite classification experiments.","Furthermore, at the application level, our representations are explored in real-world forensics tasks on adversarial perturbations and Artificial Intelligence Generated Content (AIGC).","Such applications reveal that the proposed strategy not only realizes the theoretically promised invariance, but also exhibits competitive discriminability even in the era of deep learning.","For robust and interpretable vision tasks at larger scales, hierarchical invariant representation can be considered as an effective alternative to traditional CNN and invariants."],"url":"http://arxiv.org/abs/2402.15430v1","category":"cs.CV"}
{"created":"2024-02-23 16:48:56","title":"ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation","abstract":"Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the \"just-right\" number of stochastic perturbations whenever the verification target is met. Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we demonstrate an application of ProTIP to rank commonly used defence methods.","sentences":["Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions.","However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness.","While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found.","In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees.","The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels.","To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the \"just-right\" number of stochastic perturbations whenever the verification target is met.","Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs.","Finally, we demonstrate an application of ProTIP to rank commonly used defence methods."],"url":"http://arxiv.org/abs/2402.15429v1","category":"cs.CV"}
{"created":"2024-02-23 16:42:17","title":"Understanding Entrainment in Human Groups: Optimising Human-Robot Collaboration from Lessons Learned during Human-Human Collaboration","abstract":"Successful entrainment during collaboration positively affects trust, willingness to collaborate, and likeability towards collaborators. In this paper, we present a mixed-method study to investigate characteristics of successful entrainment leading to pair and group-based synchronisation. Drawing inspiration from industrial settings, we designed a fast-paced, short-cycle repetitive task. Using motion tracking, we investigated entrainment in both dyadic and triadic task completion. Furthermore, we utilise audio-video recordings and semi-structured interviews to contextualise participants' experiences. This paper contributes to the Human-Computer/Robot Interaction (HCI/HRI) literature using a human-centred approach to identify characteristics of entrainment during pair- and group-based collaboration. We present five characteristics related to successful entrainment. These are related to the occurrence of entrainment, leader-follower patterns, interpersonal communication, the importance of the point-of-assembly, and the value of acoustic feedback. Finally, we present three design considerations for future research and design on collaboration with robots.","sentences":["Successful entrainment during collaboration positively affects trust, willingness to collaborate, and likeability towards collaborators.","In this paper, we present a mixed-method study to investigate characteristics of successful entrainment leading to pair and group-based synchronisation.","Drawing inspiration from industrial settings, we designed a fast-paced, short-cycle repetitive task.","Using motion tracking, we investigated entrainment in both dyadic and triadic task completion.","Furthermore, we utilise audio-video recordings and semi-structured interviews to contextualise participants' experiences.","This paper contributes to the Human-Computer/Robot Interaction (HCI/HRI) literature using a human-centred approach to identify characteristics of entrainment during pair- and group-based collaboration.","We present five characteristics related to successful entrainment.","These are related to the occurrence of entrainment, leader-follower patterns, interpersonal communication, the importance of the point-of-assembly, and the value of acoustic feedback.","Finally, we present three design considerations for future research and design on collaboration with robots."],"url":"http://arxiv.org/abs/2402.15427v1","category":"cs.HC"}
{"created":"2024-02-23 16:32:28","title":"A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models","abstract":"Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in the zero-shot setting. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which yields promising results.","sentences":["Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations.","In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries.","To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries.","We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information.","Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40).","We also conduct a qualitative evaluation using hallucination-free and improved training data.","GPT-4 shows very good results even in the zero-shot setting.","We find that common quantitative metrics do not correlate well with faithfulness and quality.","Finally, we test GPT-4 for automatic hallucination detection, which yields promising results."],"url":"http://arxiv.org/abs/2402.15422v1","category":"cs.CL"}
{"created":"2024-02-23 16:28:55","title":"Reputational Algorithm Aversion","abstract":"People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called \"algorithm aversion\". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.","sentences":["People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called \"algorithm aversion\".","This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability.","I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal.","Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it.","However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill.","The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers."],"url":"http://arxiv.org/abs/2402.15418v1","category":"econ.TH"}
{"created":"2024-02-23 16:20:29","title":"Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?","abstract":"Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full fine-tuning and training a LoRA from scratch. Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters. Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters.","sentences":["Parameter-efficient fine-tuning stands as the standard for efficiently fine-tuning large language and vision models on downstream tasks.","Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks.","In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks.","Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging.","Our experimental results on both vision and language models reveal that in few-shot settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full fine-tuning and training a LoRA from scratch.","Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters.","Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters."],"url":"http://arxiv.org/abs/2402.15414v1","category":"cs.LG"}
{"created":"2024-02-23 16:19:49","title":"G-RepsNet: A Fast and General Construction of Equivariant Networks for Arbitrary Matrix Groups","abstract":"Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. (2021) directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key intuition for our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations can lead to expressive universal equivariant networks. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3) with scalars, vectors, and second-order tensors as data types. On image classification tasks, we find that G-RepsNet using second-order representations is competitive and often even outperforms sophisticated state-of-the-art equivariant models such as GCNNs (Cohen & Welling, 2016a) and E(2)-CNNs (Weiler & Cesa, 2019). To further illustrate the generality of our approach, we show that G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras et al., 2021) on N-body predictions and solving PDEs, respectively, while being efficient.","sentences":["Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks.","However, constructing efficient equivariant networks for general groups and domains is difficult.","Recent work by Finzi et al. (2021) directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs).","But this method does not scale well and scaling is crucial in deep learning.","Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials.","The key intuition for our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations can lead to expressive universal equivariant networks.","We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3) with scalars, vectors, and second-order tensors as data types.","On image classification tasks, we find that G-RepsNet using second-order representations is competitive and often even outperforms sophisticated state-of-the-art equivariant models such as GCNNs (Cohen & Welling, 2016a) and E(2)-CNNs (Weiler & Cesa, 2019).","To further illustrate the generality of our approach, we show that G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras et al., 2021) on N-body predictions and solving PDEs, respectively, while being efficient."],"url":"http://arxiv.org/abs/2402.15413v1","category":"cs.LG"}
{"created":"2024-02-23 16:19:33","title":"Ehrhart polynomials, Hecke series, and affine buildings","abstract":"Given a lattice polytope $P$ and a prime $p$, we define a function from the set of primitive symplectic $p$-adic lattices to the rationals that extracts the $\\ell$th coefficient of the Ehrhart polynomial of $P$ relative to the given lattice. Inspired by work of Gunnells and Rodriguez-Villegas in type $\\mathsf{A}$, we show that these functions are eigenfunctions of a suitably defined action of the spherical symplectic Hecke algebra. Although they depend significantly on the polytope $P$, their eigenvalues are independent of $P$ and expressed as polynomials in $p$. We define local zeta functions that enumerate the values of these Hecke eigenfunctions on the vertices of the affine Bruhat--Tits buildings associated with $p$-adic symplectic groups. We compute these zeta functions by enumerating $p$-adic lattices by their elementary divisors and, simultaneously, one Hermite parameter. We report on a general functional equation satisfied by these local zeta functions, confirming a conjecture of Vankov.","sentences":["Given a lattice polytope $P$ and a prime $p$, we define a function from the set of primitive symplectic $p$-adic lattices to the rationals that extracts the $\\ell$th coefficient of the Ehrhart polynomial of $P$ relative to the given lattice.","Inspired by work of Gunnells and Rodriguez-Villegas in type $\\mathsf{A}$, we show that these functions are eigenfunctions of a suitably defined action of the spherical symplectic Hecke algebra.","Although they depend significantly on the polytope $P$, their eigenvalues are independent of $P$ and expressed as polynomials in $p$. We define local zeta functions that enumerate the values of these Hecke eigenfunctions on the vertices of the affine Bruhat--Tits buildings associated with $p$-adic symplectic groups.","We compute these zeta functions by enumerating $p$-adic lattices by their elementary divisors and, simultaneously, one Hermite parameter.","We report on a general functional equation satisfied by these local zeta functions, confirming a conjecture of Vankov."],"url":"http://arxiv.org/abs/2402.15412v1","category":"math.CO"}
{"created":"2024-02-23 16:16:38","title":"Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps","abstract":"It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure for computing such a \"smart scaling.\"   The sample complexity of the resulting \"rescaled Lasso\" algorithm incurs (in the worst case) quadratic dependence on the sparsity of the underlying signal. While this dependence is not information-theoretically necessary, we give evidence that it is optimal among the class of polynomial-time algorithms, via the method of low-degree polynomials. This argument reveals a new connection between sparse linear regression and a special version of sparse PCA with a near-critical negative spike. The latter problem can be thought of as a real-valued analogue of learning a sparse parity. Using it, we also establish the first computational-statistical gap for the closely related problem of learning a Gaussian Graphical Model.","sentences":["It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations.","In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection.","Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general.   ","In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables.","In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix.","While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation.","Moreover, we design a simple, efficient procedure for computing such a \"smart scaling.\"   ","The sample complexity of the resulting \"rescaled Lasso\" algorithm incurs (in the worst case) quadratic dependence on the sparsity of the underlying signal.","While this dependence is not information-theoretically necessary, we give evidence that it is optimal among the class of polynomial-time algorithms, via the method of low-degree polynomials.","This argument reveals a new connection between sparse linear regression and a special version of sparse PCA with a near-critical negative spike.","The latter problem can be thought of as a real-valued analogue of learning a sparse parity.","Using it, we also establish the first computational-statistical gap for the closely related problem of learning a Gaussian Graphical Model."],"url":"http://arxiv.org/abs/2402.15409v1","category":"stat.ML"}
{"created":"2024-02-23 16:13:57","title":"Bubble-wall velocity in local thermal equilibrium: hydrodynamical simulations vs analytical treatment","abstract":"We perform real-time hydrodynamical simulations of the growth of bubbles formed during cosmological first-order phase transitions under the assumption of local thermal equilibrium. We confirm that pure hydrodynamic backreaction can lead to steady-state expansion and that bubble-wall velocity in such case agrees very well with the analytical estimates. However, this is not the generic outcome. Instead, it is much more common to observe runaways, as the early-stage dynamics right after the nucleation allow the bubble walls to achieve supersonic velocities before the heated fluid shell in front of the bubble is formed. This effect is not captured by other methods of calculation of the bubble-wall velocity which assume stationary solutions to exist at all times and would have a crucial impact on the possible generation of both baryon asymmetry and gravitational wave signals.","sentences":["We perform real-time hydrodynamical simulations of the growth of bubbles formed during cosmological first-order phase transitions under the assumption of local thermal equilibrium.","We confirm that pure hydrodynamic backreaction can lead to steady-state expansion and that bubble-wall velocity in such case agrees very well with the analytical estimates.","However, this is not the generic outcome.","Instead, it is much more common to observe runaways, as the early-stage dynamics right after the nucleation allow the bubble walls to achieve supersonic velocities before the heated fluid shell in front of the bubble is formed.","This effect is not captured by other methods of calculation of the bubble-wall velocity which assume stationary solutions to exist at all times and would have a crucial impact on the possible generation of both baryon asymmetry and gravitational wave signals."],"url":"http://arxiv.org/abs/2402.15408v1","category":"astro-ph.CO"}
{"created":"2024-02-23 16:10:43","title":"A Universal Method for Solar Filament Detection from H-alpha Observations using Semi-supervised Deep Learning","abstract":"Filaments are omnipresent features in the solar atmosphere. Their location, properties and time evolution can provide information about changes in solar activity and assist the operational space weather forecast. Therefore, filaments have to be identified in full disk images and their properties extracted from these images. Manual extraction is tedious and takes much time; extraction with morphological image processing tools produces a large number of false-positive detections. Automatic object detection, segmentation, and extraction in a reliable manner allows to process more data in a shorter time. The Chromospheric Telescope (ChroTel), Tenerife, Spain, the Global Oscillation Network Group (GONG), and the Kanzelh\\\"ohe Observatory (KSO), Austria, provide regular full-disk observations of the Sun in the core of the chromospheric H-alpha absorption line. We present a deep learning method that provides reliable extractions of filaments from H-alpha filtergrams. First, we train the object detection algorithm YOLOv5 with labeled filament data of ChroTel. We use the trained model to obtain bounding-boxes from the full GONG archive. In a second step, we apply a semi-supervised training approach, where we use the bounding boxes of filaments, to learn a pixel-wise classification of filaments with u-net. Here, we make use of the increased data set size to avoid overfitting of spurious artifacts from the generated training masks. Filaments are predicted with an accuracy of 92%. With the resulting filament segmentations, physical parameters such as the area or tilt angle can be easily determined and studied. This we demonstrate in one example, where we determine the rush-to-the pole for Solar Cycle 24 from the segmented GONG images. In a last step, we apply the filament detection to H-alpha observations from KSO which demonstrates the general applicability of our method to H-alpha filtergrams.","sentences":["Filaments are omnipresent features in the solar atmosphere.","Their location, properties and time evolution can provide information about changes in solar activity and assist the operational space weather forecast.","Therefore, filaments have to be identified in full disk images and their properties extracted from these images.","Manual extraction is tedious and takes much time; extraction with morphological image processing tools produces a large number of false-positive detections.","Automatic object detection, segmentation, and extraction in a reliable manner allows to process more data in a shorter time.","The Chromospheric Telescope (ChroTel), Tenerife, Spain, the Global Oscillation Network Group (GONG), and the Kanzelh\\\"ohe Observatory (KSO), Austria, provide regular full-disk observations of the Sun in the core of the chromospheric H-alpha absorption line.","We present a deep learning method that provides reliable extractions of filaments from H-alpha filtergrams.","First, we train the object detection algorithm YOLOv5 with labeled filament data of ChroTel.","We use the trained model to obtain bounding-boxes from the full GONG archive.","In a second step, we apply a semi-supervised training approach, where we use the bounding boxes of filaments, to learn a pixel-wise classification of filaments with u-net.","Here, we make use of the increased data set size to avoid overfitting of spurious artifacts from the generated training masks.","Filaments are predicted with an accuracy of 92%.","With the resulting filament segmentations, physical parameters such as the area or tilt angle can be easily determined and studied.","This we demonstrate in one example, where we determine the rush-to-the pole for Solar Cycle 24 from the segmented GONG images.","In a last step, we apply the filament detection to H-alpha observations from KSO which demonstrates the general applicability of our method to H-alpha filtergrams."],"url":"http://arxiv.org/abs/2402.15407v1","category":"astro-ph.SR"}
{"created":"2024-02-23 16:07:39","title":"Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty Quantification in Deep Operator Networks","abstract":"In this paper, we adopt conformal prediction, a distribution-free uncertainty quantification (UQ) framework, to obtain confidence prediction intervals with coverage guarantees for Deep Operator Network (DeepONet) regression. Initially, we enhance the uncertainty quantification frameworks (B-DeepONet and Prob-DeepONet) previously proposed by the authors by using split conformal prediction. By combining conformal prediction with our Prob- and B-DeepONets, we effectively quantify uncertainty by generating rigorous confidence intervals for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that allows for a more natural use of split conformal prediction. We refer to this distribution-free effective uncertainty quantification framework as split conformal Quantile-DeepONet regression. Finally, we demonstrate the effectiveness of the proposed methods using various ordinary, partial differential equation numerical examples, and multi-fidelity learning.","sentences":["In this paper, we adopt conformal prediction, a distribution-free uncertainty quantification (UQ) framework, to obtain confidence prediction intervals with coverage guarantees for Deep Operator Network (DeepONet) regression.","Initially, we enhance the uncertainty quantification frameworks (B-DeepONet and Prob-DeepONet) previously proposed by the authors by using split conformal prediction.","By combining conformal prediction with our Prob- and B-DeepONets, we effectively quantify uncertainty by generating rigorous confidence intervals for DeepONet prediction.","Additionally, we design a novel Quantile-DeepONet that allows for a more natural use of split conformal prediction.","We refer to this distribution-free effective uncertainty quantification framework as split conformal Quantile-DeepONet regression.","Finally, we demonstrate the effectiveness of the proposed methods using various ordinary, partial differential equation numerical examples, and multi-fidelity learning."],"url":"http://arxiv.org/abs/2402.15406v1","category":"cs.LG"}
{"created":"2024-02-23 16:03:28","title":"Simulation of Dissipative Dynamics Without Interferometers","abstract":"The development of techniques that reduce experimental complexity and minimize errors is an utmost importance for modeling quantum channels. In general, quantum simulators are focused on universal algorithms, whose practical implementation requires extra qubits necessary to control the quantum operations. In contrast, our technique is based on finding a way to optimally sum Kraus operators. These operators provide us with an experimentally simplified setup where only a degree of freedom is needed to implement any one-qubit quantum channel. Therefore, using entanglement polarized photon pairs and post-processing techniques, we experimentally built the Kraus maps, carrying out unitary and projection operations.","sentences":["The development of techniques that reduce experimental complexity and minimize errors is an utmost importance for modeling quantum channels.","In general, quantum simulators are focused on universal algorithms, whose practical implementation requires extra qubits necessary to control the quantum operations.","In contrast, our technique is based on finding a way to optimally sum Kraus operators.","These operators provide us with an experimentally simplified setup where only a degree of freedom is needed to implement any one-qubit quantum channel.","Therefore, using entanglement polarized photon pairs and post-processing techniques, we experimentally built the Kraus maps, carrying out unitary and projection operations."],"url":"http://arxiv.org/abs/2402.15401v1","category":"quant-ph"}
{"created":"2024-02-23 16:03:17","title":"Faithful Temporal Question Answering over Heterogeneous Sources","abstract":"Temporal question answering (QA) involves time constraints, with phrases such as \"... in 2019\" or \"... before COVID\". In the former, time is an explicit condition, in the latter it is implicit. State-of-the-art methods have limitations along three dimensions. First, with neural inference, time constraints are merely soft-matched, giving room to invalid or inexplicable answers. Second, questions with implicit time are poorly supported. Third, answers come from a single source: either a knowledge base (KB) or a text corpus. We propose a temporal QA system that addresses these shortcomings. First, it enforces temporal constraints for faithful answering with tangible evidence. Second, it properly handles implicit questions. Third, it operates over heterogeneous sources, covering KB, text and web tables in a unified manner. The method has three stages: (i) understanding the question and its temporal conditions, (ii) retrieving evidence from all sources, and (iii) faithfully answering the question. As implicit questions are sparse in prior benchmarks, we introduce a principled method for generating diverse questions. Experiments show superior performance over a suite of baselines.","sentences":["Temporal question answering (QA) involves time constraints, with phrases such as \"... in 2019\" or \"... before COVID\".","In the former, time is an explicit condition, in the latter it is implicit.","State-of-the-art methods have limitations along three dimensions.","First, with neural inference, time constraints are merely soft-matched, giving room to invalid or inexplicable answers.","Second, questions with implicit time are poorly supported.","Third, answers come from a single source: either a knowledge base (KB) or a text corpus.","We propose a temporal QA system that addresses these shortcomings.","First, it enforces temporal constraints for faithful answering with tangible evidence.","Second, it properly handles implicit questions.","Third, it operates over heterogeneous sources, covering KB, text and web tables in a unified manner.","The method has three stages: (i) understanding the question and its temporal conditions, (ii) retrieving evidence from all sources, and (iii) faithfully answering the question.","As implicit questions are sparse in prior benchmarks, we introduce a principled method for generating diverse questions.","Experiments show superior performance over a suite of baselines."],"url":"http://arxiv.org/abs/2402.15400v1","category":"cs.IR"}
{"created":"2024-02-23 16:01:44","title":"Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation","abstract":"We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics RL with function approximation, and establish a polynomial suboptimality bound that is independent of the state and action space sizes. Our work makes the first step towards a deeper understanding of the provable efficiency of online DRMDPs with linear function approximation. Finally, we substantiate the performance and robustness of DR-LSVI-UCB through different numerical experiments.","sentences":["We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain.","We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel.","We provide the first study on online DRMDPs with function approximation for off-dynamics RL.","We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation.","By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation.","We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics RL with function approximation, and establish a polynomial suboptimality bound that is independent of the state and action space sizes.","Our work makes the first step towards a deeper understanding of the provable efficiency of online DRMDPs with linear function approximation.","Finally, we substantiate the performance and robustness of DR-LSVI-UCB through different numerical experiments."],"url":"http://arxiv.org/abs/2402.15399v1","category":"cs.LG"}
{"created":"2024-02-23 16:00:04","title":"TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction","abstract":"Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative location encoder for nuanced flow representation. Following this, the transformer-based flow predictor enhances this by leveraging attention mechanisms to efficiently capture flow interactions. Our model outperforms existing methods by up to 30.8% Common Part of Commuters, offering insights into mobility dynamics crucial for urban planning and policy decisions.","sentences":["Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking.","This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses.","Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy.","While deep learning models offer improved accuracy, their black-box nature poses a trade-off between performance and explainability -- both vital for analyzing complex societal phenomena like commuting flows.","To address this, we introduce TransFlower, an explainable, transformer-based model employing flow-to-flow attention to predict urban commuting patterns.","It features a geospatial encoder with an anisotropy-aware relative location encoder for nuanced flow representation.","Following this, the transformer-based flow predictor enhances this by leveraging attention mechanisms to efficiently capture flow interactions.","Our model outperforms existing methods by up to 30.8% Common Part of Commuters, offering insights into mobility dynamics crucial for urban planning and policy decisions."],"url":"http://arxiv.org/abs/2402.15398v1","category":"cs.LG"}
{"created":"2024-02-23 15:54:49","title":"Q-balls and charged Q-balls in a two-scalar field theory with generalized Henon-Heiles potential","abstract":"We construct Q-ball solutions from a model consisting of one massive scalar field $\\xi$ and one massive complex scalar field $\\phi$ interacting via the cubic couplings $g_1 \\xi \\phi^{*} \\phi + g_2 \\xi^3$, typical of Henon-Heiles-like potentials. Although being formally simple, these couplings allow for Q-balls. In one spatial dimension, analytical solutions exist, either with vanishing or non vanishing $\\phi$. In three spatial dimensions, we numerically build Q-ball solutions and investigate their behaviours when changing the relatives values of $g_1$ and $g_2$. For $g_1<g_2$, two Q-balls with the same frequency exist, while $\\omega=0$ can be reached when $g_1>g_2$. We then extend the former solutions by gauging the U(1)-symmetry of $\\phi$ and show that charged Q-balls exist.","sentences":["We construct Q-ball solutions from a model consisting of one massive scalar field $\\xi$ and one massive complex scalar field $\\phi$ interacting via the cubic couplings $g_1 \\xi \\phi^{*} \\phi + g_2 \\xi^3$, typical of Henon-Heiles-like potentials.","Although being formally simple, these couplings allow for Q-balls.","In one spatial dimension, analytical solutions exist, either with vanishing or non vanishing $\\phi$. In three spatial dimensions, we numerically build Q-ball solutions and investigate their behaviours when changing the relatives values of $g_1$ and $g_2$. For $g_1<g_2$, two Q-balls with the same frequency exist, while $\\omega=0$ can be reached when","$g_1>g_2$.","We then extend the former solutions by gauging the U(1)-symmetry of $\\phi$ and show that charged Q-balls exist."],"url":"http://arxiv.org/abs/2402.15396v1","category":"hep-th"}
{"created":"2024-02-23 15:54:27","title":"Hydrostatic equilibrium configurations of neutron stars in the $f(R,\\mathcal{L},T)$ gravity theory","abstract":"In the present work, we obtain the hydrostatic equilibrium configurations of neutron stars in the recently proposed $f(R,\\mathcal{L},T)$ theory of gravity, for which $R$ is the Ricci scalar, $\\mathcal{L}$ is the matter lagrangian density, $T$ is the trace of the energy-momentum tensor and $f$ is a function of the argument. This theory emerges in the present literature as a generalized geometry-matter coupling theory of gravity. We derive the Tolman-Oppenheimer-Volkoff-like equation for a particular functional form of the $f(R,\\mathcal{L},T)$ function. Our solutions are obtained from realistic equations of state describing matter inside neutron stars. We obtain stable solutions for neutron stars and we show that for some values of the free parameter of the theory it is possible to be in agreement with both NICER and LIGO/Virgo observational data.","sentences":["In the present work, we obtain the hydrostatic equilibrium configurations of neutron stars in the recently proposed $f(R,\\mathcal{L},T)$ theory of gravity, for which $R$ is the Ricci scalar, $\\mathcal{L}$ is the matter lagrangian density, $T$ is the trace of the energy-momentum tensor and $f$ is a function of the argument.","This theory emerges in the present literature as a generalized geometry-matter coupling theory of gravity.","We derive the Tolman-Oppenheimer-Volkoff-like equation for a particular functional form of the $f(R,\\mathcal{L},T)$ function.","Our solutions are obtained from realistic equations of state describing matter inside neutron stars.","We obtain stable solutions for neutron stars and we show that for some values of the free parameter of the theory it is possible to be in agreement with both NICER and LIGO/Virgo observational data."],"url":"http://arxiv.org/abs/2402.15395v1","category":"gr-qc"}
{"created":"2024-02-23 15:51:45","title":"NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks","abstract":"While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.","sentences":["While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner.","Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments.","However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same.","To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different.","We contribute with a novel benchmark of asymmetrical tasks for extrapolation.","We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes."],"url":"http://arxiv.org/abs/2402.15393v1","category":"cs.LG"}
{"created":"2024-02-23 15:47:26","title":"Genie: Generative Interactive Environments","abstract":"We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.","sentences":["We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos.","The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches.","At 11B parameters, Genie can be considered a foundation world model.","It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model.","Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature.","Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future."],"url":"http://arxiv.org/abs/2402.15391v1","category":"cs.LG"}
{"created":"2024-02-23 15:42:12","title":"Explorations of Self-Repair in Language Models","abstract":"Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.","sentences":["Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate.","Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution.","We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect).","We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure.","We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair."],"url":"http://arxiv.org/abs/2402.15390v1","category":"cs.LG"}
{"created":"2024-02-23 15:34:43","title":"On the Usability of Next-Generation Authentication: A Study on Eye Movement and Brainwave-based Mechanisms","abstract":"Passwords remain a widely-used authentication mechanism, despite their well-known security and usability limitations. To improve on this situation, next-generation authentication mechanisms, based on behavioral biometric factors such as eye movement and brainwave have emerged. However, their usability remains relatively under-explored. To fill this gap, we conducted an empirical user study (n=32 participants) to evaluate three brain-based and three eye-based authentication mechanisms, using both qualitative and quantitative methods. Our findings show good overall usability according to the System Usability Scale for both categories of mechanisms, with average SUS scores in the range of 78.6-79.6 and the best mechanisms rated with an \"excellent\" score. Participants particularly identified brainwave authentication as more secure yet more privacy-invasive and effort-intensive compared to eye movement authentication. However, the significant number of neutral responses indicates participants' need for more detailed information about the security and privacy implications of these authentication methods. Building on the collected evidence, we identify three key areas for improvement: privacy, authentication interface design, and verification time. We offer recommendations for designers and developers to improve the usability and security of next-generation authentication mechanisms.","sentences":["Passwords remain a widely-used authentication mechanism, despite their well-known security and usability limitations.","To improve on this situation, next-generation authentication mechanisms, based on behavioral biometric factors such as eye movement and brainwave have emerged.","However, their usability remains relatively under-explored.","To fill this gap, we conducted an empirical user study (n=32 participants) to evaluate three brain-based and three eye-based authentication mechanisms, using both qualitative and quantitative methods.","Our findings show good overall usability according to the System Usability Scale for both categories of mechanisms, with average SUS scores in the range of 78.6-79.6 and the best mechanisms rated with an \"excellent\" score.","Participants particularly identified brainwave authentication as more secure yet more privacy-invasive and effort-intensive compared to eye movement authentication.","However, the significant number of neutral responses indicates participants' need for more detailed information about the security and privacy implications of these authentication methods.","Building on the collected evidence, we identify three key areas for improvement: privacy, authentication interface design, and verification time.","We offer recommendations for designers and developers to improve the usability and security of next-generation authentication mechanisms."],"url":"http://arxiv.org/abs/2402.15388v1","category":"cs.CR"}
{"created":"2024-02-23 15:32:57","title":"Low-Weight High-Distance Error Correcting Fermionic Encodings","abstract":"We perform an extended numerical search for practical fermion-to-qubit encodings with error correcting properties. Ideally, encodings should strike a balance between a number of the seemingly incompatible attributes, such as having a high minimum distance, low-weight fermionic logical operators, a small qubit to fermionic mode ratio and a simple qubit connectivity graph including ancilla qubits for the measurement of stabilizers. Our strategy consists of a three-step procedure in which we: first generate encodings with code distances up to $d\\leq4$ by a brute-force enumeration technique; subsequently, we use these encodings as starting points and apply Clifford deformations to them which allows us to identify higher-distance codes with $d\\leq7$; finally, we optimize the hardware connectivity graphs of resulting encodings in terms of the graph thickness and the number of connections per qubit. We report multiple promising high-distance encodings which significantly improve the weights of stabilizers and logical operators compared to previously reported alternatives.","sentences":["We perform an extended numerical search for practical fermion-to-qubit encodings with error correcting properties.","Ideally, encodings should strike a balance between a number of the seemingly incompatible attributes, such as having a high minimum distance, low-weight fermionic logical operators, a small qubit to fermionic mode ratio and a simple qubit connectivity graph including ancilla qubits for the measurement of stabilizers.","Our strategy consists of a three-step procedure in which we: first generate encodings with code distances up to $d\\leq4$ by a brute-force enumeration technique; subsequently, we use these encodings as starting points and apply Clifford deformations to them which allows us to identify higher-distance codes with $d\\leq7$; finally, we optimize the hardware connectivity graphs of resulting encodings in terms of the graph thickness and the number of connections per qubit.","We report multiple promising high-distance encodings which significantly improve the weights of stabilizers and logical operators compared to previously reported alternatives."],"url":"http://arxiv.org/abs/2402.15386v1","category":"quant-ph"}
{"created":"2024-02-23 15:32:30","title":"Zeptosecond-scale single-photon gyroscope","abstract":"This paper presents an all-fiber telecom-range optical gyroscope employing a spontaneous parametric down conversion crystal to produce ultra-low intensity thermal light by tracing-out one of the heralded photons. The prototype exhibits a detection limit on photon delay measurements of $249$ zs over a $72$ s averaging time and $26$ zs in differential delay measurements at $t=10^4$ s averaging. The detection scheme proves to be the most resource-efficient possible, saturating $>99.5\\%$ of the Cram\\'er-Rao bound. These results are groundbreaking in the context of low-photon regime quantum metrology, holding great promise for astrometry and paving the way to novel experimental configurations to bridge quantum optics with special or general relativity.","sentences":["This paper presents an all-fiber telecom-range optical gyroscope employing a spontaneous parametric down conversion crystal to produce ultra-low intensity thermal light by tracing-out one of the heralded photons.","The prototype exhibits a detection limit on photon delay measurements of $249$ zs over a $72$ s averaging time and $26$ zs in differential delay measurements at $t=10^4$ s averaging.","The detection scheme proves to be the most resource-efficient possible, saturating $>99.5\\%$ of the Cram\\'er-Rao bound.","These results are groundbreaking in the context of low-photon regime quantum metrology, holding great promise for astrometry and paving the way to novel experimental configurations to bridge quantum optics with special or general relativity."],"url":"http://arxiv.org/abs/2402.15385v1","category":"quant-ph"}
{"created":"2024-02-23 15:30:57","title":"Homeostatic motion planning with innate physics knowledge","abstract":"Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called \"tasks\", each representing a closed-loop behaviour. We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept.","sentences":["Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours.","Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control.","We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called \"tasks\", each representing a closed-loop behaviour.","We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment.","On the basis of this model, plans can be made by chaining temporary closed-loop controllers.","The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept."],"url":"http://arxiv.org/abs/2402.15384v1","category":"cs.RO"}
{"created":"2024-02-23 15:24:42","title":"Interacting electrons in a flat-band system within the Generalized Kadanoff-Baym Ansatz","abstract":"This work reports the study of the spectral properties of an open interacting system by solving the Generalized Kadanoff-Baym Ansatz (GKBA) master equation for the single-particle density matrix, namely the time-diagonal lesser Green function. To benchmark its validity, the solution obtained within the GKBA is compared with the solution of the Dyson equation at stationarity. In both approaches, the interaction is treated within the self-consistent second-order Born approximation, whereas the GKBA still retains the retarded propagator calculated at the Hartree-Fock and wide-band limit approximation level. The model chosen is that of two leads connected through a central correlated region where particles can interact and utilize the stationary particle current at the boundary of the junction as a probe of the spectral features of the system. The central region is chosen as the simplest model featuring a degenerate ground state with a flat band. The main result is that the solution of the GKBA master equation captures well the spectral feature of such system and specifically the transition from dispersionless to dispersive behavior of the flat-band as the interaction is increased. Therefore the GBKA solution retains the main spectral features of the self-energy used even when the propagator is at the Hartree-Fock level.","sentences":["This work reports the study of the spectral properties of an open interacting system by solving the Generalized Kadanoff-Baym Ansatz (GKBA) master equation for the single-particle density matrix, namely the time-diagonal lesser Green function.","To benchmark its validity, the solution obtained within the GKBA is compared with the solution of the Dyson equation at stationarity.","In both approaches, the interaction is treated within the self-consistent second-order Born approximation, whereas the GKBA still retains the retarded propagator calculated at the Hartree-Fock and wide-band limit approximation level.","The model chosen is that of two leads connected through a central correlated region where particles can interact and utilize the stationary particle current at the boundary of the junction as a probe of the spectral features of the system.","The central region is chosen as the simplest model featuring a degenerate ground state with a flat band.","The main result is that the solution of the GKBA master equation captures well the spectral feature of such system and specifically the transition from dispersionless to dispersive behavior of the flat-band as the interaction is increased.","Therefore the GBKA solution retains the main spectral features of the self-energy used even when the propagator is at the Hartree-Fock level."],"url":"http://arxiv.org/abs/2402.15378v1","category":"cond-mat.str-el"}
{"created":"2024-02-23 15:20:04","title":"Towards complete all-optical emission control of high-harmonic generation from solids","abstract":"Optical modulation of high-harmonics generation in solids enables the detection of material properties such as the band structure and promising new applications such as super-resolution imaging in semiconductors. Various recent studies have shown optical modulation of high-harmonics generation in solids, in particular, suppression of high-harmonics generation has been observed by synchronized or delayed multi-pulse sequences. Here we provide an overview of the underlying mechanisms attributed to this suppression and provide a perspective on the challenges and opportunities regarding these mechanisms. All-optical control of high-harmonic generation allows for femtosecond, and in the future possibly subfemtosecond, switching, which has numerous possible applications: These range from super-resolution microscopy, to nanoscale controlled chemistry, and highly tunable nonlinear light sources.","sentences":["Optical modulation of high-harmonics generation in solids enables the detection of material properties such as the band structure and promising new applications such as super-resolution imaging in semiconductors.","Various recent studies have shown optical modulation of high-harmonics generation in solids, in particular, suppression of high-harmonics generation has been observed by synchronized or delayed multi-pulse sequences.","Here we provide an overview of the underlying mechanisms attributed to this suppression and provide a perspective on the challenges and opportunities regarding these mechanisms.","All-optical control of high-harmonic generation allows for femtosecond, and in the future possibly subfemtosecond, switching, which has numerous possible applications: These range from super-resolution microscopy, to nanoscale controlled chemistry, and highly tunable nonlinear light sources."],"url":"http://arxiv.org/abs/2402.15375v1","category":"physics.optics"}
{"created":"2024-02-23 15:10:27","title":"The sandpile model on the complete split graph: $q,t$-Schr\u00f6der polynomials, sawtooth polyominoes, and a cyclic lemma","abstract":"This paper builds on work initiated in Dukes (2021) that considered the classification of recurrent configurations of the Abelian sandpile model on the complete split graph. We introduce two statistics, wtopple$_{CTI}$ and wtopple$_{ITC}$, on sorted recurrent configurations. These statistics arise from two natural but different toppling conventions, CTI and ITC, for Dhar's burning algorithm as it is applied to the complete split graph. In addition, we introduce the bivariate $q,t$-CTI and $q,t$-ITC polynomials that are the generating functions of the bistatistics (height,wtopple$_{ITC}$) and (height,wtopple$_{CTI}$) on the sorted recurrent configurations.   We prove that a modification of the bijection given in Dukes (2021) from sorted recurrent configurations to Schr\\\"oder paths maps the bistatistic (height,wtopple$_{ITC}$) to the bistatistic (area,bounce). The generating function of the bistatistic (area,bounce) on Schr\\\"oder paths is known in the literature as the $q,t$-Schr\\\"oder polynomial and was introduced by Egge, Haglund, Killpatrick and Kremer (2003). This connection allows us to relate the $q,t$-ITC polynomial to the theory of symmetric functions and also establishes symmetry of the $q,t$-ITC polynomials.   We also give a characterization of the sorted recurrent configurations as a new class of polyominoes that we call $sawtooth$ $polyominoes$. The CTI and ITC topplings processes on sorted recurrent configurations are proven to correspond to two bounce paths from one side of the sawtooth polyomino to the other. Moreover, and building on the results of Aval et al. (2016), we present a cyclic lemma for a slight extension of stable configurations that allows for an enumeration of sorted recurrent configurations within the framework of the sandpile model. Finally, we conjecture equality of the $q,t$-CTI and $q,t$-ITC polynomials.","sentences":["This paper builds on work initiated in Dukes (2021) that considered the classification of recurrent configurations of the Abelian sandpile model on the complete split graph.","We introduce two statistics, wtopple$_{CTI}$ and wtopple$_{ITC}$, on sorted recurrent configurations.","These statistics arise from two natural but different toppling conventions, CTI and ITC, for Dhar's burning algorithm as it is applied to the complete split graph.","In addition, we introduce the bivariate $q,t$-CTI and $q,t$-ITC polynomials that are the generating functions of the bistatistics (height,wtopple$_{ITC}$) and (height,wtopple$_{CTI}$) on the sorted recurrent configurations.   ","We prove that a modification of the bijection given in Dukes (2021) from sorted recurrent configurations to Schr\\\"oder paths maps the bistatistic (height,wtopple$_{ITC}$) to the bistatistic (area,bounce).","The generating function of the bistatistic (area,bounce) on Schr\\\"oder paths is known in the literature as the $q,t$-Schr\\\"oder polynomial and was introduced by Egge, Haglund, Killpatrick and Kremer (2003).","This connection allows us to relate the $q,t$-ITC polynomial to the theory of symmetric functions and also establishes symmetry of the $q,t$-ITC polynomials.   ","We also give a characterization of the sorted recurrent configurations as a new class of polyominoes that we call $sawtooth$ $polyominoes$. The CTI and ITC topplings processes on sorted recurrent configurations are proven to correspond to two bounce paths from one side of the sawtooth polyomino to the other.","Moreover, and building on the results of Aval et al. (2016), we present a cyclic lemma for a slight extension of stable configurations that allows for an enumeration of sorted recurrent configurations within the framework of the sandpile model.","Finally, we conjecture equality of the $q,t$-CTI and $q,t$-ITC polynomials."],"url":"http://arxiv.org/abs/2402.15372v1","category":"math.CO"}
{"created":"2024-02-23 15:07:13","title":"Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction","abstract":"Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \\emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks. Testing on public benchmarks, our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness.","sentences":["Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained sentiment analysis.","Recent studies have employed Graph Neural Networks (GNN) to model the syntax-semantic relationships inherent in triplet elements.","However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task.","In this work, we propose a \\emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words.","Specifically, our model utilizes a dual-channel encoder with a BERT channel to capture semantic information, and an enhanced LSTM channel for comprehensive syntactic information capture.","Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes.","We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks.","Testing on public benchmarks, our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2402.15370v1","category":"cs.CL"}
{"created":"2024-02-23 15:02:44","title":"Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction","abstract":"This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack mission performance and safety guarantees. To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates while minimizing the overall number of help requests. We demonstrate the performance of our approach on multi-robot home service applications. We also show through comparative experiments, that our method outperforms recent centralized and decentralized multi-robot LLM-based planners in terms of in terms of its ability to design correct plans. The advantage of our algorithm over baselines becomes more pronounced with increasing mission complexity and robot team size.","sentences":["This paper addresses task planning problems for language-instructed robot teams.","Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects.","Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans.","However, these approaches lack mission performance and safety guarantees.","To address this challenge, we introduce a new decentralized LLM-based planner that is capable of achieving high mission success rates.","This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models.","CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise.","We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates while minimizing the overall number of help requests.","We demonstrate the performance of our approach on multi-robot home service applications.","We also show through comparative experiments, that our method outperforms recent centralized and decentralized multi-robot LLM-based planners in terms of in terms of its ability to design correct plans.","The advantage of our algorithm over baselines becomes more pronounced with increasing mission complexity and robot team size."],"url":"http://arxiv.org/abs/2402.15368v1","category":"cs.RO"}
{"created":"2024-02-23 14:58:26","title":"A CWENO large time-step scheme for Hamilton--Jacobi equations","abstract":"We propose a high order numerical scheme for time-dependent first order Hamilton--Jacobi--Bellman equations. In particular we propose to combine a semi-Lagrangian scheme with a Central Weighted Non-Oscillatory reconstruction. We prove a convergence result in the case of state- and time-independent Hamiltonians.   Numerical simulations are presented in space dimensions one and two, also for more general state- and time-dependent Hamiltonians, demonstrating superior performance in terms of CPU time gain compared with a semi-Lagrangian scheme coupled with Weighted Non-Oscillatory reconstructions.","sentences":["We propose a high order numerical scheme for time-dependent first order Hamilton--Jacobi--Bellman equations.","In particular we propose to combine a semi-Lagrangian scheme with a Central Weighted Non-Oscillatory reconstruction.","We prove a convergence result in the case of state- and time-independent Hamiltonians.   ","Numerical simulations are presented in space dimensions one and two, also for more general state- and time-dependent Hamiltonians, demonstrating superior performance in terms of CPU time gain compared with a semi-Lagrangian scheme coupled with Weighted Non-Oscillatory reconstructions."],"url":"http://arxiv.org/abs/2402.15367v1","category":"math.NA"}
{"created":"2024-02-23 14:57:51","title":"Portable acceleration of CMS computing workflows with coprocessors as a service","abstract":"Computing demands for large scientific experiments, such as the CMS experiment at the CERN LHC, will increase dramatically in the next decades. To complement the future performance increases of software running on central processing units (CPUs), explorations of coprocessor usage in data processing hold great potential and interest. Coprocessors are a class of computer processors that supplement CPUs, often improving the execution of certain functions due to architectural design choices. We explore the approach of Services for Optimized Network Inference on Coprocessors (SONIC) and study the deployment of this as-a-service approach in large-scale data processing. In the studies, we take a data processing workflow of the CMS experiment and run the main workflow on CPUs, while offloading several machine learning (ML) inference tasks onto either remote or local coprocessors, specifically graphics processing units (GPUs). With experiments performed at Google Cloud, the Purdue Tier-2 computing center, and combinations of the two, we demonstrate the acceleration of these ML algorithms individually on coprocessors and the corresponding throughput improvement for the entire workflow. This approach can be easily generalized to different types of coprocessors and deployed on local CPUs without decreasing the throughput performance. We emphasize that the SONIC approach enables high coprocessor usage and enables the portability to run workflows on different types of coprocessors.","sentences":["Computing demands for large scientific experiments, such as the CMS experiment at the CERN LHC, will increase dramatically in the next decades.","To complement the future performance increases of software running on central processing units (CPUs), explorations of coprocessor usage in data processing hold great potential and interest.","Coprocessors are a class of computer processors that supplement CPUs, often improving the execution of certain functions due to architectural design choices.","We explore the approach of Services for Optimized Network Inference on Coprocessors (SONIC) and study the deployment of this as-a-service approach in large-scale data processing.","In the studies, we take a data processing workflow of the CMS experiment and run the main workflow on CPUs, while offloading several machine learning (ML) inference tasks onto either remote or local coprocessors, specifically graphics processing units (GPUs).","With experiments performed at Google Cloud, the Purdue Tier-2 computing center, and combinations of the two, we demonstrate the acceleration of these ML algorithms individually on coprocessors and the corresponding throughput improvement for the entire workflow.","This approach can be easily generalized to different types of coprocessors and deployed on local CPUs without decreasing the throughput performance.","We emphasize that the SONIC approach enables high coprocessor usage and enables the portability to run workflows on different types of coprocessors."],"url":"http://arxiv.org/abs/2402.15366v1","category":"physics.ins-det"}
{"created":"2024-02-23 14:54:40","title":"Is a model equivalent to its computer implementation?","abstract":"A recent trend in mathematical modeling is to publish the computer code together with the research findings. Here we explore the formal question, whether and in which sense a computer implementation is distinct from the mathematical model. We argue that, despite the convenience of implemented models, a set of implicit assumptions is perpetuated with the implementation to the extent that even in widely used models the causal link between the (formal) mathematical model and the set of results is no longer certain. Moreover, code publication is often seen as an important contributor to reproducible research, we suggest that in some cases the opposite may be true. A new perspective on this topic stems from the accelerating trend that in some branches of research only implemented models are used, e.g., in artificial intelligence (AI). With the advent of quantum computers we argue that completely novel challenges arise in the distinction between models and implementations.","sentences":["A recent trend in mathematical modeling is to publish the computer code together with the research findings.","Here we explore the formal question, whether and in which sense a computer implementation is distinct from the mathematical model.","We argue that, despite the convenience of implemented models, a set of implicit assumptions is perpetuated with the implementation to the extent that even in widely used models the causal link between the (formal) mathematical model and the set of results is no longer certain.","Moreover, code publication is often seen as an important contributor to reproducible research, we suggest that in some cases the opposite may be true.","A new perspective on this topic stems from the accelerating trend that in some branches of research only implemented models are used, e.g., in artificial intelligence (AI).","With the advent of quantum computers we argue that completely novel challenges arise in the distinction between models and implementations."],"url":"http://arxiv.org/abs/2402.15364v1","category":"cs.CY"}
{"created":"2024-02-23 14:52:44","title":"All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data","abstract":"Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more. Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool. In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count. The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset. In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score. Our approach targets a desirable ecological estimator and provides a more rigorous grounding for identifying the core problems caused by distribution shifts -- when the defining characteristics of the data distribution change -- and designing strategies to mitigate them. We propose a validation scheme for estimating call density in a body of data and obtain, through Bayesian reasoning, probability distributions of confidence scores for both the positive and negative classes. We use these distributions to predict site-level densities, which may be subject to distribution shifts. We test our proposed methods on a real-world study of Hawaiian birds and provide simulation results leveraging existing fully annotated datasets, demonstrating robustness to variations in call density and classifier model quality.","sentences":["Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more.","Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool.","In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count.","The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset.","In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score.","Our approach targets a desirable ecological estimator and provides a more rigorous grounding for identifying the core problems caused by distribution shifts -- when the defining characteristics of the data distribution change -- and designing strategies to mitigate them.","We propose a validation scheme for estimating call density in a body of data and obtain, through Bayesian reasoning, probability distributions of confidence scores for both the positive and negative classes.","We use these distributions to predict site-level densities, which may be subject to distribution shifts.","We test our proposed methods on a real-world study of Hawaiian birds and provide simulation results leveraging existing fully annotated datasets, demonstrating robustness to variations in call density and classifier model quality."],"url":"http://arxiv.org/abs/2402.15360v1","category":"q-bio.QM"}
{"created":"2024-02-23 14:45:06","title":"Rotational phase transitions in antifluorite-type osmate and iridate compounds","abstract":"We present temperature-dependent single-crystal diffraction results on seven antifluorite-type $A_2MeX_6$ compounds with $Me$=Os or Ir: K$_2$OsCl$_6$, $A_2$OsBr$_6$ with $A$=K, Rb, Cs and NH$_4$, and K$_2$Ir$X_6$ with $X$=Cl and Br. The structural transitions in this family arise from $MeX_6$ octahedron rotations that generate a rich variety of symmetries depending on the rotation axis and stacking schemes. In order to search for local distortions in the high-symmetry phase we perform refinements of anharmonic atomic displacement parameters with comprehensive data sets. Even at temperatures close to the onset of structural distortions, these refinements only yield a small improvement indicating only small anharmonic effects. The phase transitions in these antifluorites are essentially of displacive character. However, some harmonic displacement parameters are very large reflecting soft phonon modes with the softening covering large parts of the Brillouin zone. The occurrence of the rotational transitions in the antifluorite-type family can be remarkably well analyzed in terms of a tolerance factor of ionic radii.","sentences":["We present temperature-dependent single-crystal diffraction results on seven antifluorite-type $A_2MeX_6$ compounds with $Me$=Os or Ir: K$_2$OsCl$_6$, $A_2$OsBr$_6$ with $A$=K, Rb, Cs and NH$_4$, and K$_2$Ir$X_6$ with $X$=Cl and Br.","The structural transitions in this family arise from $MeX_6$ octahedron rotations that generate a rich variety of symmetries depending on the rotation axis and stacking schemes.","In order to search for local distortions in the high-symmetry phase we perform refinements of anharmonic atomic displacement parameters with comprehensive data sets.","Even at temperatures close to the onset of structural distortions, these refinements only yield a small improvement indicating only small anharmonic effects.","The phase transitions in these antifluorites are essentially of displacive character.","However, some harmonic displacement parameters are very large reflecting soft phonon modes with the softening covering large parts of the Brillouin zone.","The occurrence of the rotational transitions in the antifluorite-type family can be remarkably well analyzed in terms of a tolerance factor of ionic radii."],"url":"http://arxiv.org/abs/2402.15358v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 14:41:21","title":"Mixing cutoff for simple random walks on the Chung-Lu digraph","abstract":"In this paper, we are interested in the mixing behaviour of simple random walks on inhomogeneous directed graphs. We focus our study on the Chung-Lu digraph, which is an inhomogeneous network that generalizes the Erd\\H{o}s-R\\'enyi digraph. In particular, under the Chung-Lu model, edges are included in the graph independently and according to given Bernoulli laws, so that the average degrees are fixed. To guarantee the a.s. existence of a unique reversible measure, which is implied by the strong connectivity of the graph, we assume that the average degree grows logarithmically in the size $n$ of the graph. In this weakly dense regime, we prove that the total variation distance to equilibrium displays a cutoff behaviour at the entropic time of order $\\log(n)/\\log\\log(n)$. Moreover, we prove that on a precise window, the cutoff profile converges to the Gaussian tail function. This is qualitatively similar to what was proved in [6,7,8] for the directed configuration model, where degrees are deterministically fixed. In terms of statistical ensembles, our analysis provides an extension of these cutoff results from a hard to a soft-constrained model.","sentences":["In this paper, we are interested in the mixing behaviour of simple random walks on inhomogeneous directed graphs.","We focus our study on the Chung-Lu digraph, which is an inhomogeneous network that generalizes the Erd\\H{o}s-R\\'enyi digraph.","In particular, under the Chung-Lu model, edges are included in the graph independently and according to given Bernoulli laws, so that the average degrees are fixed.","To guarantee the a.s. existence of a unique reversible measure, which is implied by the strong connectivity of the graph, we assume that the average degree grows logarithmically in the size $n$ of the graph.","In this weakly dense regime, we prove that the total variation distance to equilibrium displays a cutoff behaviour at the entropic time of order $\\log(n)/\\log\\log(n)$.","Moreover, we prove that on a precise window, the cutoff profile converges to the Gaussian tail function.","This is qualitatively similar to what was proved in [6,7,8] for the directed configuration model, where degrees are deterministically fixed.","In terms of statistical ensembles, our analysis provides an extension of these cutoff results from a hard to a soft-constrained model."],"url":"http://arxiv.org/abs/2402.15356v1","category":"math.PR"}
{"created":"2024-02-23 14:40:05","title":"Boundaries and equivariant maps for ergodic groupoids","abstract":"We give a notion of boundary pair $(\\mathcal{B}_-,\\mathcal{B}_+)$ for measured groupoids which generalizes the one introduced by Bader and Furman for locally compact groups. In the case of a semidirect groupoid $\\mathcal{G}=\\Gamma \\ltimes X$ obtained by a probability measure preserving action $\\Gamma \\curvearrowright X$ of a locally compact group, we show that a boundary pair is exactly $(B_- \\times X, B_+ \\times X)$, where $(B_-,B_+)$ is a boundary pair for $\\Gamma$. For any measured groupoid $(\\mathcal{G},\\nu)$, we prove that the Poisson boundaries associated to the Markov operators generated by a probability measure equivalent to $\\nu$ provide other examples of our definition.   Following Bader and Furman, we define algebraic representability for an ergodic groupoid $(\\mathcal{G},\\nu)$. In this way, given any measurable representation $\\rho:\\mathcal{G} \\rightarrow H$ into the $\\kappa$-points of an algebraic $\\kappa$-group $\\mathbf{H}$, we obtain $\\rho$-equivariant maps $\\mathcal{B}_\\pm \\rightarrow H/L_\\pm$, where $L_\\pm=\\mathbf{L}_\\pm(\\kappa)$ for some $\\kappa$-subgroups $\\mathbf{L}_\\pm<\\mathbf{H}$. In the particular case when $\\kappa=\\mathbb{R}$ and $\\rho$ is Zariski dense, we show that $L_\\pm$ must be minimal parabolic subgroups.","sentences":["We give a notion of boundary pair $(\\mathcal{B}_-,\\mathcal{B}_+)$ for measured groupoids which generalizes the one introduced by Bader and Furman for locally compact groups.","In the case of a semidirect groupoid $\\mathcal{G}=\\Gamma \\ltimes X$ obtained by a probability measure preserving action $\\Gamma \\curvearrowright X$ of a locally compact group, we show that a boundary pair is exactly $(B_- \\times X, B_+ \\times X)$, where $(B_-,B_+)$ is a boundary pair for $\\Gamma$. For any measured groupoid $(\\mathcal{G},\\nu)$, we prove that the Poisson boundaries associated to the Markov operators generated by a probability measure equivalent to $\\nu$ provide other examples of our definition.   ","Following Bader and Furman, we define algebraic representability for an ergodic groupoid $(\\mathcal{G},\\nu)$.","In this way, given any measurable representation $\\rho:\\mathcal{G} \\rightarrow H$ into the $\\kappa$-points of an algebraic $\\kappa$-group $\\mathbf{H}$, we obtain $\\rho$-equivariant maps $\\mathcal{B}_\\pm \\rightarrow H/L_\\pm$, where $L_\\pm=\\mathbf{L}_\\pm(\\kappa)$ for some $\\kappa$-subgroups $\\mathbf{L}_\\pm<\\mathbf{H}$. In the particular case when $\\kappa=\\mathbb{R}$ and $\\rho$ is Zariski dense, we show that $L_\\pm$ must be minimal parabolic subgroups."],"url":"http://arxiv.org/abs/2402.15355v1","category":"math.DS"}
{"created":"2024-02-23 14:40:04","title":"Some curvature properties of perfect fluid spacetimes","abstract":"In this paper we assume that a perfect fluid is the source of the gravitational field while analyzing the solutions to the Einstein field equations.","sentences":["In this paper we assume that a perfect fluid is the source of the gravitational field while analyzing the solutions to the Einstein field equations."],"url":"http://arxiv.org/abs/2402.15354v1","category":"gr-qc"}
{"created":"2024-02-23 14:38:19","title":"AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks","abstract":"Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters. Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation. We further develop a new benchmark, called LAMP, for studying key components in the end-to-end prompt-based model training pipeline. Code, model, and data will be released.","sentences":["Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process.","While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow.","To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks.","The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface.","Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters.","Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation.","We further develop a new benchmark, called LAMP, for studying key components in the end-to-end prompt-based model training pipeline.","Code, model, and data will be released."],"url":"http://arxiv.org/abs/2402.15351v1","category":"cs.LG"}
{"created":"2024-02-23 14:38:05","title":"Farsight: Fostering Responsible AI Awareness During AI Application Prototyping","abstract":"Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. Their qualitative feedback also highlights that Farsight encourages them to focus on end-users and think beyond immediate harms. We discuss these findings and reflect on their implications for designing AI prototyping experiences that meaningfully engage with AI harms. Farsight is publicly accessible at: https://PAIR-code.github.io/farsight.","sentences":["Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before.","However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping.","To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping.","Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms.","We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers.","After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources.","Their qualitative feedback also highlights that Farsight encourages them to focus on end-users and think beyond immediate harms.","We discuss these findings and reflect on their implications for designing AI prototyping experiences that meaningfully engage with AI harms.","Farsight is publicly accessible at: https://PAIR-code.github.io/farsight."],"url":"http://arxiv.org/abs/2402.15350v1","category":"cs.HC"}
{"created":"2024-02-23 14:31:10","title":"Information-Theoretic Safe Bayesian Optimization","abstract":"We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters. We theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision. Empirical evaluations demonstrate improved data-efficiency and scalability.","sentences":["We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint.","A common approach is to place a Gaussian process prior on the unknown functions and allow evaluations only in regions that are safe with high probability.","Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case.","Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter.","In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate.","The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion.","Our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters.","We theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision.","Empirical evaluations demonstrate improved data-efficiency and scalability."],"url":"http://arxiv.org/abs/2402.15347v1","category":"cs.LG"}
{"created":"2024-02-23 14:26:12","title":"Fourier Basis Density Model","abstract":"We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis. We assess its performance at approximating a range of multi-modal 1D densities, which are generally difficult to fit. In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.","sentences":["We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis.","We assess its performance at approximating a range of multi-modal 1D densities, which are generally difficult to fit.","In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget.","In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression."],"url":"http://arxiv.org/abs/2402.15345v1","category":"cs.LG"}
{"created":"2024-02-23 14:23:51","title":"NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data","abstract":"Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.","sentences":["Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems.","In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task.","NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs.","We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance.","We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs."],"url":"http://arxiv.org/abs/2402.15343v1","category":"cs.CL"}
{"created":"2024-02-23 14:22:00","title":"The Behavioural House Indicator: A faster and real time small-area indicative deprivation measure for England","abstract":"Researchers have been long preoccupied with the measuring and monitoring of economic and social deprivation at small scales, neighbourhood, level in order to provide official government agencies and policy makers with more precise data insights. Whist valuable methodologies have been developed, the exercise of data collection associated with these methods tend to be expensive, time consuming, published infrequently with significant time delays, and subject to recurring changes to methodology. Here, we propose a novel method based on a straightforward methodology and data sources to generate a faster and real time indicator for deprivation at different scaling, small to larger areas. The results of our work show that our method provides a consistent view of deprivation across the regions of England and Wales, which are inline with the other indexes, but also highlight specific flash points of deep rural and highly dense urban deprivation areas that are not well captured by existing indexes. Our method is intended to aid researchers and policy makers by complementing existing but infrequent indexes.","sentences":["Researchers have been long preoccupied with the measuring and monitoring of economic and social deprivation at small scales, neighbourhood, level in order to provide official government agencies and policy makers with more precise data insights.","Whist valuable methodologies have been developed, the exercise of data collection associated with these methods tend to be expensive, time consuming, published infrequently with significant time delays, and subject to recurring changes to methodology.","Here, we propose a novel method based on a straightforward methodology and data sources to generate a faster and real time indicator for deprivation at different scaling, small to larger areas.","The results of our work show that our method provides a consistent view of deprivation across the regions of England and Wales, which are inline with the other indexes, but also highlight specific flash points of deep rural and highly dense urban deprivation areas that are not well captured by existing indexes.","Our method is intended to aid researchers and policy makers by complementing existing but infrequent indexes."],"url":"http://arxiv.org/abs/2402.15341v1","category":"physics.soc-ph"}
{"created":"2024-02-23 14:21:08","title":"Characterizations of generalized Robertson-Walker spacetimes concerning gradient solitons","abstract":"In this article, we examine gradient type Ricci solitons and $(m,\\tau)$-quasi Einstein solitons in generalized Robertson-Walker ($GRW$) spacetimes. Besides, we demonstrate that in this scenario the $GRW$ spacetime presents the Robertson-Walker ($RW$) spacetime and the perfect fluid ($PF$) spacetime presents the phantom era. Consequently, we show that if a $GRW$ spacetime permits a gradient $\\tau$- Einstein solitons, then it also represents a $PF$ spacetime under certain condition.","sentences":["In this article, we examine gradient type Ricci solitons and $(m,\\tau)$-quasi Einstein solitons in generalized Robertson-Walker ($GRW$) spacetimes.","Besides, we demonstrate that in this scenario the $GRW$ spacetime presents the Robertson-Walker ($RW$) spacetime and the perfect fluid ($PF$) spacetime presents the phantom era.","Consequently, we show that if a $GRW$ spacetime permits a gradient $\\tau$- Einstein solitons, then it also represents a $PF$ spacetime under certain condition."],"url":"http://arxiv.org/abs/2402.15339v1","category":"math.DG"}
{"created":"2024-02-23 14:19:10","title":"Effect of temperature and copper doping on the heterogeneous Fenton-like activity of Cu$_x$Fe$_{3-x}$O$_4$ nanoparticles","abstract":"Ferrite nanoparticles serve as potent heterogeneous Fenton-like catalysts, producing reactive oxygen species (ROS) for decomposing organic pollutants. We investigated the impact of temperature and copper content on the catalytic activity of nanoparticles with different oxidation states of iron. Via solvothermal synthesis, we fabricated copper-doped magnetite (Cu$_x$Fe$_{3-x}$O$_4$) with a Fe$^{2+}$/Fe ratio ~0.33 for the undoped system. Using a microwave-assisted method, we produced copper-doped oxidized ferrites, yielding a Fe$^{2+}$/Fe ratio of ~0.11 for the undoped nanoparticles. The ROS generated by the catalyst were identified and quantified by electron paramagnetic resonance, while optical spectroscopy allowed us to evaluate its effectiveness for the degradation of a model organic dye. At room temperature, the magnetite nanoparticles exhibited the most $\\cdot$OH radical production and achieved almost 90% dye discoloration in 2 hours. This efficiency decreased with increasing Cu concentration, concurrently with a decrease in $\\cdot$OH generation. Conversely, above room temperature, Cu-doped nanoparticles significantly enhance the dye degradation, reaching 100% discoloration at 90$^\\circ$C. This enhancement is accompanied by a systematic increase in the kinetic constants, obtained from reaction equations, with Cu doping. This study highlights the superior stability and high-temperature catalytic advantages of copper ferrite holding promise for enhancing the performance of nanocatalysts for decomposing organic contaminants.","sentences":["Ferrite nanoparticles serve as potent heterogeneous Fenton-like catalysts, producing reactive oxygen species (ROS) for decomposing organic pollutants.","We investigated the impact of temperature and copper content on the catalytic activity of nanoparticles with different oxidation states of iron.","Via solvothermal synthesis, we fabricated copper-doped magnetite (Cu$_x$Fe$_{3-x}$O$_4$) with a Fe$^{2+}$/Fe ratio ~0.33 for the undoped system.","Using a microwave-assisted method, we produced copper-doped oxidized ferrites, yielding a Fe$^{2+}$/Fe ratio of ~0.11 for the undoped nanoparticles.","The ROS generated by the catalyst were identified and quantified by electron paramagnetic resonance, while optical spectroscopy allowed us to evaluate its effectiveness for the degradation of a model organic dye.","At room temperature, the magnetite nanoparticles exhibited the most $\\cdot$OH radical production and achieved almost 90% dye discoloration in 2 hours.","This efficiency decreased with increasing Cu concentration, concurrently with a decrease in $\\cdot$OH generation.","Conversely, above room temperature, Cu-doped nanoparticles significantly enhance the dye degradation, reaching 100% discoloration at 90$^\\circ$C.","This enhancement is accompanied by a systematic increase in the kinetic constants, obtained from reaction equations, with Cu doping.","This study highlights the superior stability and high-temperature catalytic advantages of copper ferrite holding promise for enhancing the performance of nanocatalysts for decomposing organic contaminants."],"url":"http://arxiv.org/abs/2402.15338v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 14:15:59","title":"Probing black hole `charge' from the binary black hole inspiral","abstract":"Recent gravitational wave (GW) observations have enabled us to look beyond the standard paradigm of gravitational physics, namely general relativity (GR). Along with the mass and the angular momentum, which typical astrophysical black holes (BHs) are endowed with, theories beyond GR generically induce `charge' to these BHs. Notably, for BHs carrying the extra `charge' hair, we expect the BH absorption effects to modify accordingly and alter the tidal heating terms. Hence, the inclusion of the corrections in the GW waveform model, arising from the BH `charge', allows us to test the consistency of the observed binaries with Kerr BHs in GR. We compute the explicit dependence of the binary inspiral phase on the `charge' parameter arising from the tidal heating effect and study the measurability of the same from GW observations of binary mergers. Specifically, we employ the {\\tt TaylorF2} waveform model, which accurately models the inspiral evolution of an aligned-spin binary merger, and Bayesian analysis-based GW data inference to measure the `charge' parameter for a selected set of detected binaries. We also present a detailed simulation study to investigate the possibility of measuring the charge parameter from binaries with different masses, spins and source locations. The analysis of selected GW events from the third GW transient catalogue shows that the `charge' parameter constraints are poor from the observed signals with the current sensitivity. In contrast, the simulation studies indicate that the spinning binaries with significant mass asymmetry provide the best constraints on the BH `charge' parameter. Finally, we study the prospects of measuring the BH `charge' parameter from a future GW detector with improved sensitivity.","sentences":["Recent gravitational wave (GW) observations have enabled us to look beyond the standard paradigm of gravitational physics, namely general relativity (GR).","Along with the mass and the angular momentum, which typical astrophysical black holes (BHs) are endowed with, theories beyond GR generically induce `charge' to these BHs.","Notably, for BHs carrying the extra `charge' hair, we expect the BH absorption effects to modify accordingly and alter the tidal heating terms.","Hence, the inclusion of the corrections in the GW waveform model, arising from the BH `charge', allows us to test the consistency of the observed binaries with Kerr BHs in GR.","We compute the explicit dependence of the binary inspiral phase on the `charge' parameter arising from the tidal heating effect and study the measurability of the same from GW observations of binary mergers.","Specifically, we employ the {\\tt TaylorF2} waveform model, which accurately models the inspiral evolution of an aligned-spin binary merger, and Bayesian analysis-based GW data inference to measure the `charge' parameter for a selected set of detected binaries.","We also present a detailed simulation study to investigate the possibility of measuring the charge parameter from binaries with different masses, spins and source locations.","The analysis of selected GW events from the third GW transient catalogue shows that the `charge' parameter constraints are poor from the observed signals with the current sensitivity.","In contrast, the simulation studies indicate that the spinning binaries with significant mass asymmetry provide the best constraints on the BH `charge' parameter.","Finally, we study the prospects of measuring the BH `charge' parameter from a future GW detector with improved sensitivity."],"url":"http://arxiv.org/abs/2402.15336v1","category":"gr-qc"}
{"created":"2024-02-23 14:15:58","title":"Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection","abstract":"Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from low resolution, simple background, and small size of the detection data. These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection. To this end, we build a new set of HAD benchmark datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently. In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Method of Multipliers (ADMM) optimizer with the deep network, guiding its search process and imparting a level of interpretability to parameter optimization. Additionally, the integration of physical models with DL techniques eliminates the need for manual parameter tuning. The manually tuned parameters are seamlessly transformed into trainable parameters for deep neural networks, facilitating a more efficient and automated optimization process. Extensive experiments conducted on the AIR-HAD dataset show the superiority of our LRR-Net$^+$ in terms of detection performance and generalization ability, compared to top-performing rivals. Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this paper will be made available freely and openly at \\url{https://sites.google.com/view/danfeng-hong}.","sentences":["Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from low resolution, simple background, and small size of the detection data.","These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection.","To this end, we build a new set of HAD benchmark datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short.","Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently.","In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Method of Multipliers (ADMM) optimizer with the deep network, guiding its search process and imparting a level of interpretability to parameter optimization.","Additionally, the integration of physical models with DL techniques eliminates the need for manual parameter tuning.","The manually tuned parameters are seamlessly transformed into trainable parameters for deep neural networks, facilitating a more efficient and automated optimization process.","Extensive experiments conducted on the AIR-HAD dataset show the superiority of our LRR-Net$^+$ in terms of detection performance and generalization ability, compared to top-performing rivals.","Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this paper will be made available freely and openly at \\url{https://sites.google.com/view/danfeng-hong}."],"url":"http://arxiv.org/abs/2402.15335v1","category":"eess.IV"}
{"created":"2024-02-23 14:09:41","title":"A Quantum-Classical Collaborative Training Architecture Based on Quantum State Fidelity","abstract":"Recent advancements have highlighted the limitations of current quantum systems, particularly the restricted number of qubits available on near-term quantum devices. This constraint greatly inhibits the range of applications that can leverage quantum computers. Moreover, as the available qubits increase, the computational complexity grows exponentially, posing additional challenges. Consequently, there is an urgent need to use qubits efficiently and mitigate both present limitations and future complexities. To address this, existing quantum applications attempt to integrate classical and quantum systems in a hybrid framework. In this study, we concentrate on quantum deep learning and introduce a collaborative classical-quantum architecture called co-TenQu. The classical component employs a tensor network for compression and feature extraction, enabling higher-dimensional data to be encoded onto logical quantum circuits with limited qubits. On the quantum side, we propose a quantum-state-fidelity-based evaluation function to iteratively train the network through a feedback loop between the two sides. co-TenQu has been implemented and evaluated with both simulators and the IBM-Q platform. Compared to state-of-the-art approaches, co-TenQu enhances a classical deep neural network by up to 41.72% in a fair setting. Additionally, it outperforms other quantum-based methods by up to 1.9 times and achieves similar accuracy while utilizing 70.59% fewer qubits.","sentences":["Recent advancements have highlighted the limitations of current quantum systems, particularly the restricted number of qubits available on near-term quantum devices.","This constraint greatly inhibits the range of applications that can leverage quantum computers.","Moreover, as the available qubits increase, the computational complexity grows exponentially, posing additional challenges.","Consequently, there is an urgent need to use qubits efficiently and mitigate both present limitations and future complexities.","To address this, existing quantum applications attempt to integrate classical and quantum systems in a hybrid framework.","In this study, we concentrate on quantum deep learning and introduce a collaborative classical-quantum architecture called co-TenQu.","The classical component employs a tensor network for compression and feature extraction, enabling higher-dimensional data to be encoded onto logical quantum circuits with limited qubits.","On the quantum side, we propose a quantum-state-fidelity-based evaluation function to iteratively train the network through a feedback loop between the two sides.","co-TenQu has been implemented and evaluated with both simulators and the IBM-Q platform.","Compared to state-of-the-art approaches, co-TenQu enhances a classical deep neural network by up to 41.72% in a fair setting.","Additionally, it outperforms other quantum-based methods by up to 1.9 times and achieves similar accuracy while utilizing 70.59% fewer qubits."],"url":"http://arxiv.org/abs/2402.15333v1","category":"quant-ph"}
{"created":"2024-02-23 14:01:53","title":"Categorical Deep Learning: An Algebraic Theory of Architectures","abstract":"We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.","sentences":["We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures.","Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations.","Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design.","To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs.","We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory."],"url":"http://arxiv.org/abs/2402.15332v1","category":"cs.LG"}
{"created":"2024-02-23 13:51:20","title":"Towards Principled Task Grouping for Multi-Task Learning","abstract":"This paper presents a novel approach to task grouping in Multitask Learning (MTL), advancing beyond existing methods by addressing key theoretical and practical limitations. Unlike prior studies, our approach offers a more theoretically grounded method that does not rely on restrictive assumptions for constructing transfer gains. We also propose a flexible mathematical programming formulation which can accommodate a wide spectrum of resource constraints, thus enhancing its versatility. Experimental results across diverse domains, including computer vision datasets, combinatorial optimization benchmarks and time series tasks, demonstrate the superiority of our method over extensive baselines, validating its effectiveness and general applicability in MTL.","sentences":["This paper presents a novel approach to task grouping in Multitask Learning (MTL), advancing beyond existing methods by addressing key theoretical and practical limitations.","Unlike prior studies, our approach offers a more theoretically grounded method that does not rely on restrictive assumptions for constructing transfer gains.","We also propose a flexible mathematical programming formulation which can accommodate a wide spectrum of resource constraints, thus enhancing its versatility.","Experimental results across diverse domains, including computer vision datasets, combinatorial optimization benchmarks and time series tasks, demonstrate the superiority of our method over extensive baselines, validating its effectiveness and general applicability in MTL."],"url":"http://arxiv.org/abs/2402.15328v1","category":"cs.LG"}
{"created":"2024-02-23 13:46:52","title":"Perturbations of bimetric gravity on most general spherically symmetric spacetimes","abstract":"We present a formalism to study linear perturbations of bimetric gravity on any spherically symmetric background, including dynamical spacetimes. The setup is based on the Gerlach-Sengupta formalism for general relativity. Each of the two background metrics is written as a warped product between a two-dimensional Lorentzian metric and the round metric of the two-sphere. The different perturbations are then decomposed in terms of tensor spherical harmonics, which makes the two polarity (axial and polar) sectors decouple. In addition, a covariant notation on the Lorentzian manifold is used so that all expressions are valid for any coordinates. In this theory, there are seven physical propagating degrees of freedom, which, as compared to the two degrees of freedom of general relativity, makes the dynamics much more intricate. In particular, we discuss the amount of gauge and physical degrees of freedom for different polarities and multipoles. Finally, as an interesting application, we analyze static nonbidiagonal backgrounds and derive the corresponding perturbative equations.","sentences":["We present a formalism to study linear perturbations of bimetric gravity on any spherically symmetric background, including dynamical spacetimes.","The setup is based on the Gerlach-Sengupta formalism for general relativity.","Each of the two background metrics is written as a warped product between a two-dimensional Lorentzian metric and the round metric of the two-sphere.","The different perturbations are then decomposed in terms of tensor spherical harmonics, which makes the two polarity (axial and polar) sectors decouple.","In addition, a covariant notation on the Lorentzian manifold is used so that all expressions are valid for any coordinates.","In this theory, there are seven physical propagating degrees of freedom, which, as compared to the two degrees of freedom of general relativity, makes the dynamics much more intricate.","In particular, we discuss the amount of gauge and physical degrees of freedom for different polarities and multipoles.","Finally, as an interesting application, we analyze static nonbidiagonal backgrounds and derive the corresponding perturbative equations."],"url":"http://arxiv.org/abs/2402.15327v1","category":"gr-qc"}
{"created":"2024-02-23 13:44:57","title":"Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory","abstract":"This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in node classification tasks.","sentences":["This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs).","Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory.","This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator.","This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs.","Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon.","Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in node classification tasks."],"url":"http://arxiv.org/abs/2402.15326v1","category":"cs.LG"}
{"created":"2024-02-23 13:43:15","title":"Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network","abstract":"Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. One of the important questions to be answered is how to conduct credit assignment in a multi-agent system. There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms. Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate. In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory. We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov Shapley value respectively. We represent a global reward game as a Markov convex game under the grand coalition. As a result, Markov Shapley value can be reasonably used as a credit assignment scheme in the global reward game. Markov Shapley value possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment. Based on Markov Shapley value, we propose three multi-agent reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore, we extend Markov convex game to partial observability to deal with the partially observable problems, named as partially observable Markov convex game. In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks.","sentences":["Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning.","One of the important questions to be answered is how to conduct credit assignment in a multi-agent system.","There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms.","Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate.","In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory.","We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov Shapley value respectively.","We represent a global reward game as a Markov convex game under the grand coalition.","As a result, Markov Shapley value can be reasonably used as a credit assignment scheme in the global reward game.","Markov Shapley value possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment.","Based on Markov Shapley value, we propose three multi-agent reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO.","Furthermore, we extend Markov convex game to partial observability to deal with the partially observable problems, named as partially observable Markov convex game.","In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks."],"url":"http://arxiv.org/abs/2402.15324v1","category":"cs.MA"}
{"created":"2024-02-23 13:42:55","title":"Dynamical stability of bootstrapped Newtonian stars","abstract":"We investigate the dynamical stability of bootstrapped Newtonian stars following homologous adiabatic perturbations, focusing on objects of low or intermediate compactness. The results show that for stars with homogeneous densities these perturbations induce some oscillatory behaviour regardless of their compactness, density and adiabatic index, which makes them dynamically stable. In the case or polytropes with density profiles approximated by Gaussian distributions, both stable and unstable behaviours are possible. It was also shown that in the limit in which the density profile of the Gaussian density distribution flattens out, the parameter space for which the perturbations result in an oscillatory behaviour increases, which is in agreement with the case of stars with homogeneous densities.","sentences":["We investigate the dynamical stability of bootstrapped Newtonian stars following homologous adiabatic perturbations, focusing on objects of low or intermediate compactness.","The results show that for stars with homogeneous densities these perturbations induce some oscillatory behaviour regardless of their compactness, density and adiabatic index, which makes them dynamically stable.","In the case or polytropes with density profiles approximated by Gaussian distributions, both stable and unstable behaviours are possible.","It was also shown that in the limit in which the density profile of the Gaussian density distribution flattens out, the parameter space for which the perturbations result in an oscillatory behaviour increases, which is in agreement with the case of stars with homogeneous densities."],"url":"http://arxiv.org/abs/2402.15323v1","category":"gr-qc"}
{"created":"2024-02-23 13:39:59","title":"OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding","abstract":"This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023. The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping. We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods. For additional details, please see https://opensun3d.github.io/index_iccv23.html.","sentences":["This report provides an overview of the challenge hosted at the OpenSUN3D Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with ICCV 2023.","The goal of this workshop series is to provide a platform for exploration and discussion of open-vocabulary 3D scene understanding tasks, including but not limited to segmentation, detection and mapping.","We provide an overview of the challenge hosted at the workshop, present the challenge dataset, the evaluation methodology, and brief descriptions of the winning methods.","For additional details, please see https://opensun3d.github.io/index_iccv23.html."],"url":"http://arxiv.org/abs/2402.15321v1","category":"cs.CV"}
{"created":"2024-02-23 13:39:23","title":"$R_{\\infty}$-property for groups commensurable to nilpotent quotients of RAAGs","abstract":"Let $G$ be a group and $\\varphi$ an automorphism of $G$. Two elements $x,y \\in G$ are said to be $\\varphi$-conjugate if there exists a third element $z \\in G$ such that $z x \\varphi(z)^{-1} = y$. Being $\\varphi$-conjugate defines an equivalence relation on $G$. The group $G$ is said to have the $R_{\\infty}$-property if all its automorphisms $\\varphi$ have infinitely many $\\varphi$-conjugacy classes. For finitely generated torsion-free nilpotent groups, the so-called Mal'cev completion of the group is a useful tool in studying this property. Two groups have isomorphic Mal'cev completions if and only if they are abstractly commensurable. This raises the question whether the $R_{\\infty}$-property is invariant under abstract commensurability within the class of finitely generated torsion-free nilpotent groups. We show that the answer to this question is negative and provide counterexamples within a class of 2-step nilpotent groups associated to edge-weighted graphs. These groups are commensurable to 2-step nilpotent quotients of right-angled Artin groups.","sentences":["Let $G$ be a group and $\\varphi$ an automorphism of $G$. Two elements $x,y \\in G$ are said to be $\\varphi$-conjugate if there exists a third element $z \\in G$ such that $z x \\varphi(z)^{-1} = y$. Being $\\varphi$-conjugate defines an equivalence relation on $G$. The group $G$ is said to have the $R_{\\infty}$-property if all its automorphisms $\\varphi$ have infinitely many $\\varphi$-conjugacy classes.","For finitely generated torsion-free nilpotent groups, the so-called Mal'cev completion of the group is a useful tool in studying this property.","Two groups have isomorphic Mal'cev completions if and only if they are abstractly commensurable.","This raises the question whether the $R_{\\infty}$-property is invariant under abstract commensurability within the class of finitely generated torsion-free nilpotent groups.","We show that the answer to this question is negative and provide counterexamples within a class of 2-step nilpotent groups associated to edge-weighted graphs.","These groups are commensurable to 2-step nilpotent quotients of right-angled Artin groups."],"url":"http://arxiv.org/abs/2402.15320v1","category":"math.GR"}
{"created":"2024-02-23 13:34:35","title":"Logarithmic concavity of bimatroids","abstract":"A bimatroid is a matroid-like generalization of the collection of regular minors of a matrix. In this article, we use the theory of Lorentzian polynomials to study the logarithmic concavity of natural sequences associated to bimatroids. Bimatroids can be used to characterize morphisms of matroids and this observation (originally due to Kung) allows us to prove a weak version of logarithmic concavity of the number of bases of a morphism of matroids. This is weaker than the original result by Eur and Huh; it nevertheless provides us with a new perspective on Mason's log-concavity conjecture for independent sets of matroids. We finally show that for realizable bimatroids, the regular minor polynomial is a volume polynomial. Applied to morphisms of matroids, this shows that the weak basis generating polynomial of a morphism is a volume polynomial; this confirms a conjecture of Eur--Huh for morphisms of nullity $\\leq 1$ and gives an algebro-geometric explanation for Mason's log-concavity conjecture in the realizable case.","sentences":["A bimatroid is a matroid-like generalization of the collection of regular minors of a matrix.","In this article, we use the theory of Lorentzian polynomials to study the logarithmic concavity of natural sequences associated to bimatroids.","Bimatroids can be used to characterize morphisms of matroids and this observation (originally due to Kung) allows us to prove a weak version of logarithmic concavity of the number of bases of a morphism of matroids.","This is weaker than the original result by Eur and Huh; it nevertheless provides us with a new perspective on Mason's log-concavity conjecture for independent sets of matroids.","We finally show that for realizable bimatroids, the regular minor polynomial is a volume polynomial.","Applied to morphisms of matroids, this shows that the weak basis generating polynomial of a morphism is a volume polynomial; this confirms a conjecture of Eur--Huh for morphisms of nullity $\\leq 1$ and gives an algebro-geometric explanation for Mason's log-concavity conjecture in the realizable case."],"url":"http://arxiv.org/abs/2402.15317v1","category":"math.CO"}
{"created":"2024-02-23 13:34:03","title":"On Minimal Depth in Neural Networks","abstract":"A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural networks, we investigate several fundamental properties, deriving results equivalent to those of ReLU networks, such as depth inclusions and depth computation from vertices. Notably, we compute the minimal depth of simplices, which is strictly related to the minimal depth conjecture in ReLU networks.","sentences":["A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence.","This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL).","The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks.","For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation.","In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation.","The study also examine the minimal depth relationship between convex CPWL functions.","On polytope neural networks, we investigate several fundamental properties, deriving results equivalent to those of ReLU networks, such as depth inclusions and depth computation from vertices.","Notably, we compute the minimal depth of simplices, which is strictly related to the minimal depth conjecture in ReLU networks."],"url":"http://arxiv.org/abs/2402.15315v1","category":"cs.LG"}
{"created":"2024-02-23 13:32:47","title":"ArabianGPT: Native Arabic GPT-based Large Language","abstract":"The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning the models on tasks like sentiment analysis and summarization demonstrate significant improvements. For sentiment analysis, the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a substantial increase from the base model's 56%. Similarly, in summarization tasks, fine-tuned models showed enhanced F1 scores, indicating improved precision and recall in generating concise summaries. Comparative analysis of fine-tuned ArabianGPT models against their base versions across various benchmarks reveals nuanced differences in performance, with fine-tuning positively impacting specific tasks like question answering and summarization. These findings underscore the efficacy of fine-tuning in aligning ArabianGPT models more closely with specific NLP tasks, highlighting the potential of tailored transformer architectures in advancing Arabic NLP.","sentences":["The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs.","This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax.","Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements.","To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic.","These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic.","The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing.","Empirical results from fine-tuning the models on tasks like sentiment analysis and summarization demonstrate significant improvements.","For sentiment analysis, the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a substantial increase from the base model's 56%.","Similarly, in summarization tasks, fine-tuned models showed enhanced F1 scores, indicating improved precision and recall in generating concise summaries.","Comparative analysis of fine-tuned ArabianGPT models against their base versions across various benchmarks reveals nuanced differences in performance, with fine-tuning positively impacting specific tasks like question answering and summarization.","These findings underscore the efficacy of fine-tuning in aligning ArabianGPT models more closely with specific NLP tasks, highlighting the potential of tailored transformer architectures in advancing Arabic NLP."],"url":"http://arxiv.org/abs/2402.15313v1","category":"cs.CL"}
{"created":"2024-02-23 13:26:36","title":"Zero-dimensional affine Deligne--Lusztig varieties","abstract":"In this paper, we study the affine Deligne--Lusztig variety $X(\\mu,b)_K$ and classify all quadruples $(\\mathbf{G}, \\mu, b, K)$ with $\\dim X(\\mu, b)_K=0$. This question was first asked by Rapoport in 2005, who also made an explicit conjecture in the hyperspecial level. We prove that $\\dim X(\\mu,b)_K=0$ if and only if, up to certain Hodge-Newton decomposition condition, the pair $(\\mathbf{G}, \\{\\mu\\})$ is of extended Lubin-Tate type. We also give a combinatorial description of this condition by the essential gap function on $B(\\mathbf{G})$ and the $\\mu$-ordinary condition for the generic Newton stratum.","sentences":["In this paper, we study the affine Deligne--Lusztig variety $X(\\mu,b)_K$ and classify all quadruples $(\\mathbf{G}, \\mu, b, K)$ with $\\dim X(\\mu, b)_K=0$.","This question was first asked by Rapoport in 2005, who also made an explicit conjecture in the hyperspecial level.","We prove that $\\dim X(\\mu,b)_K=0$ if and only if, up to certain Hodge-Newton decomposition condition, the pair $(\\mathbf{G}, \\{\\mu\\})$ is of extended Lubin-Tate type.","We also give a combinatorial description of this condition by the essential gap function on $B(\\mathbf{G})$ and the $\\mu$-ordinary condition for the generic Newton stratum."],"url":"http://arxiv.org/abs/2402.15310v1","category":"math.AG"}
{"created":"2024-02-23 13:24:19","title":"Counterfactual Generation with Identifiability Guarantees","abstract":"Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation. This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly over domains. In this work, we tackle the domain-varying dependence between the content and the style variables inherent in the counterfactual generation task. We provide identification guarantees for such latent-variable models by leveraging the relative sparsity of the influences from different latent variables. Our theoretical insights enable the development of a doMain AdapTive counTerfactual gEneration model, called (MATTE). Our theoretically grounded framework achieves state-of-the-art performance in unsupervised style transfer tasks, where neither paired data nor style labels are utilized, across four large-scale datasets. Code is available at https://github.com/hanqi-qi/Matte.git","sentences":["Counterfactual generation lies at the core of various machine learning tasks, including image translation and controllable text generation.","This generation process usually requires the identification of the disentangled latent representations, such as content and style, that underlie the observed data.","However, it becomes more challenging when faced with a scarcity of paired data and labeling information.","Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and style variables, to identify the latent variables, even though such assumptions may not hold for complex data distributions.","For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment.","This problem is exacerbated when data are sampled from multiple domains since the dependence between content and style may vary significantly over domains.","In this work, we tackle the domain-varying dependence between the content and the style variables inherent in the counterfactual generation task.","We provide identification guarantees for such latent-variable models by leveraging the relative sparsity of the influences from different latent variables.","Our theoretical insights enable the development of a doMain AdapTive counTerfactual gEneration model, called (MATTE).","Our theoretically grounded framework achieves state-of-the-art performance in unsupervised style transfer tasks, where neither paired data nor style labels are utilized, across four large-scale datasets.","Code is available at https://github.com/hanqi-qi/Matte.git"],"url":"http://arxiv.org/abs/2402.15309v1","category":"cs.LG"}
{"created":"2024-02-23 13:11:10","title":"Representing Online Handwriting for Recognition in Large Vision-Language Models","abstract":"The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR). In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR. We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to or better than state-of-the-art online handwriting recognizers. Wide applicability is shown through results with two different VLM families, on multiple public datasets. Our approach can be applied to off-the-shelf VLMs, does not require any changes in their architecture, and can be used in both fine-tuning and parameter-efficient tuning. We perform a detailed ablation study to identify the key elements of the proposed representation.","sentences":["The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance.","Meanwhile, vision-language models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, fine-tuning, and inference.","While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing optical character recognition (OCR).","In this paper, we study online handwriting recognition with VLMs, going beyond naive OCR.","We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image.","We show that this representation yields results comparable to or better than state-of-the-art online handwriting recognizers.","Wide applicability is shown through results with two different VLM families, on multiple public datasets.","Our approach can be applied to off-the-shelf VLMs, does not require any changes in their architecture, and can be used in both fine-tuning and parameter-efficient tuning.","We perform a detailed ablation study to identify the key elements of the proposed representation."],"url":"http://arxiv.org/abs/2402.15307v1","category":"cs.CV"}
{"created":"2024-02-23 13:03:12","title":"How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries","abstract":"In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we report the harmfulness score metric as well as judgements from GPT-4 and humans. Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models. As an additional objective, we investigate the impact of model editing using the ROME technique, which further increases the propensity for generating undesirable content. In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.","sentences":["In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs).","Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation.","Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text.","To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses.","We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses.","For evaluation we report the harmfulness score metric as well as judgements from GPT-4 and humans.","Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models.","As an additional objective, we investigate the impact of model editing using the ROME technique, which further increases the propensity for generating undesirable content.","In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models."],"url":"http://arxiv.org/abs/2402.15302v1","category":"cs.CL"}
{"created":"2024-02-23 13:02:10","title":"Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models","abstract":"Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we give a method to aggregate the associational relationships to build a causal graph. We demonstrate our method is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature.","sentences":["Causal graph recovery is essential in the field of causal inference.","Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests.","The advance of large language models (LLMs) provides opportunities to address these problems.","We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks.","This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers.","Our method first retrieves relevant text chunks from the aggregated literature.","Then, the LLM is tasked with identifying and labelling potential associations between factors.","Finally, we give a method to aggregate the associational relationships to build a causal graph.","We demonstrate our method is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature."],"url":"http://arxiv.org/abs/2402.15301v1","category":"cs.CL"}
{"created":"2024-02-23 12:57:16","title":"Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding","abstract":"Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallucination across multiple LVLM families while preserving the utility of text generation.","sentences":["Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality.","Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools.","We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods.","Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time.","CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image.","Experiments demonstrate that CGD effectively mitigates object hallucination across multiple LVLM families while preserving the utility of text generation."],"url":"http://arxiv.org/abs/2402.15300v1","category":"cs.CV"}
{"created":"2024-02-23 12:45:55","title":"Vibronics of multi-material nanopillared membranes and impact on the thermal conductivity","abstract":"Atomic motion in nanopillars standing on the surface of a silicon membrane generates vibrons, which are wavenumber-independent phonons that act as local resonances. These vibrons couple with heat-carrying phonons traveling along the base membrane causing a reduction in the in-plane lattice thermal conductivity. In this work, we examine isolated silicon and gallium nitride nanopillars and for each compare the vibrons density of states (DOS) to those of phonons in an isolated uniform silicon membrane. We show that while the phonon-vibron DOS conformity across the full spectrum is a key factor in reducing the thermal conductivity of the assembled nanostructure, the presence of an intense vibron population at more dominant low frequencies plays a competing role. We report predictions from molecular dynamics simulations showing lower thermal conductivities for a silicon membrane with gallium-nitride nanopillars compared to a silicon membrane with silicon nanopillars.","sentences":["Atomic motion in nanopillars standing on the surface of a silicon membrane generates vibrons, which are wavenumber-independent phonons that act as local resonances.","These vibrons couple with heat-carrying phonons traveling along the base membrane causing a reduction in the in-plane lattice thermal conductivity.","In this work, we examine isolated silicon and gallium nitride nanopillars and for each compare the vibrons density of states (DOS) to those of phonons in an isolated uniform silicon membrane.","We show that while the phonon-vibron DOS conformity across the full spectrum is a key factor in reducing the thermal conductivity of the assembled nanostructure, the presence of an intense vibron population at more dominant low frequencies plays a competing role.","We report predictions from molecular dynamics simulations showing lower thermal conductivities for a silicon membrane with gallium-nitride nanopillars compared to a silicon membrane with silicon nanopillars."],"url":"http://arxiv.org/abs/2402.15295v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-23 12:41:44","title":"A Survey of Music Generation in the Context of Interaction","abstract":"In recent years, machine learning, and in particular generative adversarial neural networks (GANs) and attention-based neural networks (transformers), have been successfully used to compose and generate music, both melodies and polyphonic pieces. Current research focuses foremost on style replication (eg. generating a Bach-style chorale) or style transfer (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward \"performance\" evaluation. However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated. This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and automatic evaluation measures, along with a discussion of which approaches and models seem most suitable for live interaction.","sentences":["In recent years, machine learning, and in particular generative adversarial neural networks (GANs) and attention-based neural networks (transformers), have been successfully used to compose and generate music, both melodies and polyphonic pieces.","Current research focuses foremost on style replication (eg. generating a Bach-style chorale) or style transfer (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward \"performance\" evaluation.","However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated.","This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and automatic evaluation measures, along with a discussion of which approaches and models seem most suitable for live interaction."],"url":"http://arxiv.org/abs/2402.15294v1","category":"cs.SD"}
{"created":"2024-02-23 12:36:31","title":"Linear Dynamics-embedded Neural Network for Long-Sequence Modeling","abstract":"The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\\max\\{L, N\\})$ to $O(LN\\max \\{H, \\log L\\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance of LDNN.","sentences":["The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models.","Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN).","SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks.","Two efficient strategies, diagonalization and $'\\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\\max\\{L, N\\})$ to $O(LN\\max \\{H, \\log L\\})$.","We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications.","Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance of LDNN."],"url":"http://arxiv.org/abs/2402.15290v1","category":"cs.LG"}
{"created":"2024-02-23 12:33:27","title":"Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical Communications","abstract":"In this work, we present a high-throughput field programmable gate array (FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The equalization is performed and illustrated in real-time for a 30 GBd, two-level pulse amplitude modulation (PAM2) optical communication system.","sentences":["In this work, we present a high-throughput field programmable gate array (FPGA) demonstrator of an artificial neural network (ANN)-based equalizer.","The equalization is performed and illustrated in real-time for a 30 GBd, two-level pulse amplitude modulation (PAM2) optical communication system."],"url":"http://arxiv.org/abs/2402.15288v1","category":"eess.SP"}
{"created":"2024-02-23 12:30:20","title":"Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations","abstract":"Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \\textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionality due to the TT compression. We provide a complete derivation of the HJB equation's action on Tensor Train polynomials and demonstrate the performance of the proposed time-step-, rank- and degree-adaptive integration method on a nonlinear sampling task in 20 dimensions.","sentences":["Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM).","In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool.","In Berner et al.","[2022] the authors point out that these log-densities can be obtained by solution of a \\textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control.","While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization.","Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionality due to the TT compression.","We provide a complete derivation of the HJB equation's action on Tensor Train polynomials and demonstrate the performance of the proposed time-step-, rank- and degree-adaptive integration method on a nonlinear sampling task in 20 dimensions."],"url":"http://arxiv.org/abs/2402.15285v1","category":"stat.ML"}
{"created":"2024-02-23 12:28:31","title":"Spatiotemporal Observer Design for Predictive Learning of High-Dimensional Data","abstract":"Although deep learning-based methods have shown great success in spatiotemporal predictive learning, the framework of those models is designed mainly by intuition. How to make spatiotemporal forecasting with theoretical guarantees is still a challenging issue. In this work, we tackle this problem by applying domain knowledge from the dynamical system to the framework design of deep learning models. An observer theory-guided deep learning architecture, called Spatiotemporal Observer, is designed for predictive learning of high dimensional data. The characteristics of the proposed framework are twofold: firstly, it provides the generalization error bound and convergence guarantee for spatiotemporal prediction; secondly, dynamical regularization is introduced to enable the model to learn system dynamics better during training. Further experimental results show that this framework could capture the spatiotemporal dynamics and make accurate predictions in both one-step-ahead and multi-step-ahead forecasting scenarios.","sentences":["Although deep learning-based methods have shown great success in spatiotemporal predictive learning, the framework of those models is designed mainly by intuition.","How to make spatiotemporal forecasting with theoretical guarantees is still a challenging issue.","In this work, we tackle this problem by applying domain knowledge from the dynamical system to the framework design of deep learning models.","An observer theory-guided deep learning architecture, called Spatiotemporal Observer, is designed for predictive learning of high dimensional data.","The characteristics of the proposed framework are twofold: firstly, it provides the generalization error bound and convergence guarantee for spatiotemporal prediction; secondly, dynamical regularization is introduced to enable the model to learn system dynamics better during training.","Further experimental results show that this framework could capture the spatiotemporal dynamics and make accurate predictions in both one-step-ahead and multi-step-ahead forecasting scenarios."],"url":"http://arxiv.org/abs/2402.15284v1","category":"cs.LG"}
{"created":"2024-02-23 12:27:48","title":"When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination","abstract":"In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.","sentences":["In an unfamiliar setting, a model-based reinforcement learning agent can be limited by the accuracy of its world model.","In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning.","We do so by applying iterative inference at decision-time, to fine-tune the inferred agent states based on the coherence of future state representations.","Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks.","We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one.","Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach."],"url":"http://arxiv.org/abs/2402.15283v1","category":"cs.LG"}
{"created":"2024-02-23 12:22:03","title":"Dimension Independent Disentanglers from Unentanglement and Applications","abstract":"Quantum entanglement is a key enabling ingredient in diverse applications. However, the presence of unwanted adversarial entanglement also poses challenges in many applications.   In this paper, we explore methods to \"break\" quantum entanglement. Specifically, we construct a dimension-independent k-partite disentangler (like) channel from bipartite unentangled input. We show: For every $d,\\ell\\ge k$, there is an efficient channel $\\Lambda: \\mathbb{C}^{d\\ell} \\otimes \\mathbb{C}^{d\\ell} \\to \\mathbb{C}^{dk}$ such that for every bipartite separable state $\\rho_1\\otimes \\rho_2$, the output $\\Lambda(\\rho_1\\otimes\\rho_2)$ is close to a k-partite separable state. Concretely, for some distribution $\\mu$ on states from $\\mathbb{C}^d$, $$ \\left\\|\\Lambda(\\rho_1 \\otimes \\rho_2) - \\int | \\psi \\rangle \\langle \\psi |^{\\otimes k} d\\mu(\\psi)\\right\\|_1 \\le \\tilde O \\left(\\left(\\frac{k^{3}}{\\ell}\\right)^{1/4}\\right). $$ Moreover, $\\Lambda(| \\psi \\rangle \\langle \\psi |^{\\otimes \\ell}\\otimes | \\psi \\rangle \\langle \\psi |^{\\otimes \\ell}) = | \\psi \\rangle \\langle \\psi |^{\\otimes k}$. Without the bipartite unentanglement assumption, the above bound is conjectured to be impossible.   Leveraging our disentanglers, we show that unentangled quantum proofs of almost general real amplitudes capture NEXP, greatly relaxing the nonnegative amplitudes assumption in the recent work of QMA^+(2)=NEXP. Specifically, our findings show that to capture NEXP, it suffices to have unentangled proofs of the form $| \\psi \\rangle = \\sqrt{a} | \\psi_+ \\rangle + \\sqrt{1-a} | \\psi_- \\rangle$ where $| \\psi_+ \\rangle$ has non-negative amplitudes, $| \\psi_- \\rangle$ only has negative amplitudes and $| a-(1-a) | \\ge 1/poly(n)$ with $a \\in [0,1]$. Additionally, we present a protocol achieving an almost largest possible gap before obtaining QMA^R(k)=NEXP$, namely, a 1/poly(n) additive improvement to the gap results in this equality.","sentences":["Quantum entanglement is a key enabling ingredient in diverse applications.","However, the presence of unwanted adversarial entanglement also poses challenges in many applications.   ","In this paper, we explore methods to \"break\" quantum entanglement.","Specifically, we construct a dimension-independent k-partite disentangler (like) channel from bipartite unentangled input.","We show: For every $d,\\ell\\ge k$, there is an efficient channel $\\Lambda: \\mathbb{C}^{d\\ell} \\otimes \\mathbb{C}^{d\\ell} \\to \\mathbb{C}^{dk}$ such that for every bipartite separable state $\\rho_1\\otimes \\rho_2$, the output $\\Lambda(\\rho_1\\otimes\\rho_2)$ is close to a k-partite separable state.","Concretely, for some distribution $\\mu$ on states from $\\mathbb{C}^d$, $$ \\left\\|\\Lambda(\\rho_1 \\otimes \\rho_2) - \\int | \\psi \\rangle \\langle \\psi |^{\\otimes k} d\\mu(\\psi)\\right\\|_1 \\le \\tilde O \\left(\\left(\\frac{k^{3}}{\\ell}\\right)^{1/4}\\right).","$$ Moreover, $\\Lambda(| \\psi \\rangle \\langle \\psi |^{\\otimes \\ell}\\otimes | \\psi \\rangle \\langle \\psi |^{\\otimes \\ell})","= | \\psi \\rangle \\langle \\psi |^{\\otimes k}$. Without the bipartite unentanglement assumption, the above bound is conjectured to be impossible.   ","Leveraging our disentanglers, we show that unentangled quantum proofs of almost general real amplitudes capture NEXP, greatly relaxing the nonnegative amplitudes assumption in the recent work of QMA^+(2)=NEXP.","Specifically, our findings show that to capture NEXP, it suffices to have unentangled proofs of the form $| \\psi \\rangle = \\sqrt{a} | \\psi_+ \\rangle + \\sqrt{1-a} | \\psi_- \\rangle$ where $| \\psi_+ \\rangle$ has non-negative amplitudes, $| \\psi_- \\rangle$","only has negative amplitudes and $| a-(1-a) | \\ge 1/poly(n)$ with","$a \\in [0,1]$. Additionally, we present a protocol achieving an almost largest possible gap before obtaining QMA^R(k)=NEXP$, namely, a 1/poly(n) additive improvement to the gap results in this equality."],"url":"http://arxiv.org/abs/2402.15282v1","category":"quant-ph"}
{"created":"2024-02-23 12:06:39","title":"Whose Projection Postulate?","abstract":"The projection postulate is a description of the effect on a quantum system, assumed to be in a pure state, of a measurement of an observable with a discrete spectrum, in nonrelativistic quantum mechanics. It is often called \"von Neumann's projection postulate\" or \"the L\\\"uders rule\". This paper is an examination of the versions of this postulate due to Dirac, von Neumann and L\\\"uders. It is shown that Dirac, in 1930, proposed what is now generally known as the projection postulate. Von Neumann, in 1932, gave a different theory which only applies in special and rather unusual cases. L\\\"uders, in 1951, rejected this theory and presented one which is the same as Dirac's. Treatments of observables with continuous spectra by both Dirac and von Neumann are criticised, and the possibility of a generalised version of the projection postulate for this case is considered. The paper concludes with a discussion of the status of the projection postulate (in its various forms) as a separate postulate (independent of the other postulates of quantum mechanics) and as a separate form of time development (in addition to the time-dependent Schr\\\"odinger equation).","sentences":["The projection postulate is a description of the effect on a quantum system, assumed to be in a pure state, of a measurement of an observable with a discrete spectrum, in nonrelativistic quantum mechanics.","It is often called \"von Neumann's projection postulate\" or \"the L\\\"uders rule\".","This paper is an examination of the versions of this postulate due to Dirac, von Neumann and L\\\"uders.","It is shown that Dirac, in 1930, proposed what is now generally known as the projection postulate.","Von Neumann, in 1932, gave a different theory which only applies in special and rather unusual cases.","L\\\"uders, in 1951, rejected this theory and presented one which is the same as Dirac's.","Treatments of observables with continuous spectra by both Dirac and von Neumann are criticised, and the possibility of a generalised version of the projection postulate for this case is considered.","The paper concludes with a discussion of the status of the projection postulate (in its various forms) as a separate postulate (independent of the other postulates of quantum mechanics) and as a separate form of time development (in addition to the time-dependent Schr\\\"odinger equation)."],"url":"http://arxiv.org/abs/2402.15280v1","category":"quant-ph"}
{"created":"2024-02-23 11:55:43","title":"Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT","abstract":"In the evolving landscape of digital education, chatbots have emerged as potential game-changers, promising personalized and adaptive learning experiences. This research undertook an in-depth exploration of ChatGPT's potential as an educational tool, focusing on user perceptions, experiences and learning outcomes. Through a mixed-methods approach, a diverse group of 102 participants engaged with ChatGPT, providing insights pre- and postinteraction. The study reveals a notable positive shift in perceptions after exposure, underscoring the efficacy of ChatGPT. However, challenges such as prompting effectiveness and information accuracy emerged as pivotal concerns. Introducing the concept of 'AI-learning-competence', this study lays the groundwork for future research, emphasizing the need for formal training and pedagogical integration of AI tools.","sentences":["In the evolving landscape of digital education, chatbots have emerged as potential game-changers, promising personalized and adaptive learning experiences.","This research undertook an in-depth exploration of ChatGPT's potential as an educational tool, focusing on user perceptions, experiences and learning outcomes.","Through a mixed-methods approach, a diverse group of 102 participants engaged with ChatGPT, providing insights pre- and postinteraction.","The study reveals a notable positive shift in perceptions after exposure, underscoring the efficacy of ChatGPT.","However, challenges such as prompting effectiveness and information accuracy emerged as pivotal concerns.","Introducing the concept of 'AI-learning-competence', this study lays the groundwork for future research, emphasizing the need for formal training and pedagogical integration of AI tools."],"url":"http://arxiv.org/abs/2402.15278v1","category":"cs.HC"}
{"created":"2024-02-23 11:47:16","title":"Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries","abstract":"Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following this, the Summary-based Re-ranking (SR) stage further refines these selections based on concise query summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder, specifically designed to tackle the challenges of ambiguous queries and to facilitate both stages of the retrieval process, thereby significantly improving computational efficiency via vector-based similarity assessments. Our evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000, alongside reductions in training and retrieval durations by 68.75% and 99.79%, respectively.","sentences":["Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries.","Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings.","This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets.","The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis.","Following this, the Summary-based Re-ranking (SR) stage further refines these selections based on concise query summaries.","Additionally, we present a novel Decoupling-BEiT-3 encoder, specifically designed to tackle the challenges of ambiguous queries and to facilitate both stages of the retrieval process, thereby significantly improving computational efficiency via vector-based similarity assessments.","Our evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000, alongside reductions in training and retrieval durations by 68.75% and 99.79%, respectively."],"url":"http://arxiv.org/abs/2402.15276v1","category":"cs.IR"}
{"created":"2024-02-23 11:44:20","title":"Simulation Studies for the First Pathfinder of the CATCH Space Mission","abstract":"The Chasing All Transients Constellation Hunters (CATCH) space mission is an intelligent constellation consisting of 126 micro-satellites in three types (A, B, and C), designed for X-ray observation with the objective of studying the dynamic universe. Currently, we are actively developing the first Pathfinder (CATCH-1) for the CATCH mission, specifically for type-A satellites. CATCH-1 is equipped with Micro Pore Optics (MPO) and a 4-pixel Silicon Drift Detector (SDD) array. To assess its scientific performance, including the effective area of the optical system, on-orbit background, and telescope sensitivity, we employ the Monte Carlo software Geant4 for simulation in this study. The MPO optics exhibit an effective area of $41$ cm$^2$ at the focal spot for 1 keV X-rays, while the entire telescope system achieves an effective area of $29$ cm$^2$ at 1 keV when taking into account the SDD detector's detection efficiency. The primary contribution to the background is found to be from the Cosmic X-ray Background. Assuming a 625 km orbit with an inclination of $29^\\circ$, the total background for CATCH-1 is estimated to be $8.13\\times10^{-2}$ counts s$^{-1}$ in the energy range of 0.5--4 keV. Based on the background within the central detector and assuming a Crab-like source spectrum, the estimated ideal sensitivity could achieve $1.9\\times10^{-12}$ erg cm$^{-2}$ s$^{-1}$ for an exposure of 10$^4$ s in the energy band of 0.5--4 keV. Furthermore, after simulating the background caused by low-energy charged particles near the geomagnetic equator, we have determined that there is no need to install a magnetic deflector.","sentences":["The Chasing All Transients Constellation Hunters (CATCH) space mission is an intelligent constellation consisting of 126 micro-satellites in three types (A, B, and C), designed for X-ray observation with the objective of studying the dynamic universe.","Currently, we are actively developing the first Pathfinder (CATCH-1) for the CATCH mission, specifically for type-A satellites.","CATCH-1 is equipped with Micro Pore Optics (MPO) and a 4-pixel Silicon Drift Detector (SDD) array.","To assess its scientific performance, including the effective area of the optical system, on-orbit background, and telescope sensitivity, we employ the Monte Carlo software Geant4 for simulation in this study.","The MPO optics exhibit an effective area of $41$ cm$^2$ at the focal spot for 1 keV X-rays, while the entire telescope system achieves an effective area of $29$ cm$^2$ at 1 keV when taking into account the SDD detector's detection efficiency.","The primary contribution to the background is found to be from the Cosmic X-ray Background.","Assuming a 625 km orbit with an inclination of $29^\\circ$, the total background for CATCH-1 is estimated to be $8.13\\times10^{-2}$ counts s$^{-1}$ in the energy range of 0.5--4 keV.","Based on the background within the central detector and assuming a Crab-like source spectrum, the estimated ideal sensitivity could achieve $1.9\\times10^{-12}$ erg cm$^{-2}$ s$^{-1}$ for an exposure of 10$^4$ s in the energy band of 0.5--4 keV.","Furthermore, after simulating the background caused by low-energy charged particles near the geomagnetic equator, we have determined that there is no need to install a magnetic deflector."],"url":"http://arxiv.org/abs/2402.15275v1","category":"astro-ph.IM"}
{"created":"2024-02-23 11:35:57","title":"Optimized Deployment of Deep Neural Networks for Visual Pose Estimation on Nano-drones","abstract":"Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining popularity due to their small size, enabling new tasks such as indoor navigation or people monitoring. Nonetheless, their size and simple electronics pose severe challenges in implementing advanced onboard intelligence. This work proposes a new automatic optimization pipeline for visual pose estimation tasks using Deep Neural Networks (DNNs). The pipeline leverages two different Neural Architecture Search (NAS) algorithms to pursue a vast complexity-driven exploration in the DNNs' architectural space. The obtained networks are then deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low power System-on-Chip leveraging a set of novel software kernels for the efficient fused execution of critical DNN layer sequences. Our results improve the state-of-the-art reducing inference latency by up to 3.22x at iso-error.","sentences":["Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining popularity due to their small size, enabling new tasks such as indoor navigation or people monitoring.","Nonetheless, their size and simple electronics pose severe challenges in implementing advanced onboard intelligence.","This work proposes a new automatic optimization pipeline for visual pose estimation tasks using Deep Neural Networks (DNNs).","The pipeline leverages two different Neural Architecture Search (NAS) algorithms to pursue a vast complexity-driven exploration in the DNNs' architectural space.","The obtained networks are then deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low power System-on-Chip leveraging a set of novel software kernels for the efficient fused execution of critical DNN layer sequences.","Our results improve the state-of-the-art reducing inference latency by up to 3.22x at iso-error."],"url":"http://arxiv.org/abs/2402.15273v1","category":"cs.CV"}
{"created":"2024-02-23 11:35:48","title":"EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection","abstract":"In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.","sentences":["In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint.","Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth.","To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF).","To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony.","We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency.","Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs."],"url":"http://arxiv.org/abs/2402.15272v1","category":"cs.CV"}
{"created":"2024-02-23 11:32:46","title":"Smoothed Graph Contrastive Learning via Seamless Proximity Integration","abstract":"Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale graphs, the proposed framework incorporates a graph batch-generating strategy that partitions the given graphs into multiple subgraphs, facilitating efficient training in separate batches. Through extensive experimentation in the unsupervised setting on various benchmarks, particularly those of large scale, we demonstrate the superiority of our proposed framework against recent baselines.","sentences":["Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs.","The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive.","In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process.","The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives.","To enhance scalability for large-scale graphs, the proposed framework incorporates a graph batch-generating strategy that partitions the given graphs into multiple subgraphs, facilitating efficient training in separate batches.","Through extensive experimentation in the unsupervised setting on various benchmarks, particularly those of large scale, we demonstrate the superiority of our proposed framework against recent baselines."],"url":"http://arxiv.org/abs/2402.15270v1","category":"cs.LG"}
{"created":"2024-02-23 11:30:39","title":"MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models","abstract":"Transformer-based language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning. Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the underlying LM.","sentences":["Transformer-based language models (LMs) track contextual information through large, hard-coded input windows.","We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft prompts, without requiring LM finetuning.","Tested on a task designed to probe a LM's ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history.","We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history.","In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the underlying LM."],"url":"http://arxiv.org/abs/2402.15268v1","category":"cs.CL"}
{"created":"2024-02-23 11:30:12","title":"Adversarial Robustness of Deep Learning-based Malware Detectors via (De)Randomized Smoothing","abstract":"Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection. In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing. In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain. During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes. At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction. We propose two strategies to determine the location of the chunks used for classification: (1) randomly selecting the locations of the chunks and (2) selecting contiguous adjacent chunks. To showcase the effectiveness of our approach, we have trained two classifiers with our chunk-based ablation schemes on the BODMAS dataset. Our findings reveal that the chunk-based smoothing classifiers exhibit greater resilience against adversarial malware examples generated with state-of-the-are evasion attacks, outperforming a non-smoothed classifier and a randomized smoothing-based classifier by a great margin.","sentences":["Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection.","In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing.","In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain.","During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes.","At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction.","We propose two strategies to determine the location of the chunks used for classification: (1) randomly selecting the locations of the chunks and (2) selecting contiguous adjacent chunks.","To showcase the effectiveness of our approach, we have trained two classifiers with our chunk-based ablation schemes on the BODMAS dataset.","Our findings reveal that the chunk-based smoothing classifiers exhibit greater resilience against adversarial malware examples generated with state-of-the-are evasion attacks, outperforming a non-smoothed classifier and a randomized smoothing-based classifier by a great margin."],"url":"http://arxiv.org/abs/2402.15267v1","category":"cs.CR"}
{"created":"2024-02-23 11:27:10","title":"Calibration of Deep Learning Classification Models in fNIRS","abstract":"Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we summarize three practical tips. Through this letter, we hope to emphasize the critical role of calibration in fNIRS research and argue for enhancing the reliability of deep learning-based predictions in fNIRS classification tasks. All data from our experimental process are openly available on GitHub.","sentences":["Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity.","The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI).","Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness.","In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration.","However, many researchers overlook the important issue of calibration.","To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models.","Surprisingly, our results indicate poor calibration performance in many proposed models.","To advance calibration development in the fNIRS field, we summarize three practical tips.","Through this letter, we hope to emphasize the critical role of calibration in fNIRS research and argue for enhancing the reliability of deep learning-based predictions in fNIRS classification tasks.","All data from our experimental process are openly available on GitHub."],"url":"http://arxiv.org/abs/2402.15266v1","category":"cs.LG"}
{"created":"2024-02-23 11:25:17","title":"CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models","abstract":"Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.","sentences":["Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics.","However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs.","Creating personalized agent personas with distinct expertise or traits can address this issue.","Nonetheless, we lack knowledge of how people customize and interact with agent personas.","In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics.","To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs.","We conducted a study comparing how participants interact with CloChat and ChatGPT.","The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions.","These findings contribute to design implications for future systems with conversational agents using LLMs."],"url":"http://arxiv.org/abs/2402.15265v1","category":"cs.HC"}
{"created":"2024-02-23 11:24:00","title":"DEEM: Dynamic Experienced Expert Modeling for Stance Detection","abstract":"Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on three standard benchmarks, outperforms methods with self-consistency reasoning, and reduces the bias of LLMs.","sentences":["Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results.","However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis.","Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance.","In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable.","Experimental results demonstrate that DEEM consistently achieves the best results on three standard benchmarks, outperforms methods with self-consistency reasoning, and reduces the bias of LLMs."],"url":"http://arxiv.org/abs/2402.15264v1","category":"cs.CL"}
{"created":"2024-02-23 11:19:02","title":"Dynamic Memory Based Adaptive Optimization","abstract":"Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promising framework for boosting the performance of known optimizers by adding more memory units and by making them more adaptive.","sentences":["Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space.","Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units?","What information should be stored in them?","How to use them for the learning steps?","As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC.","This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily.","We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units).","Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers.","We conclude that RLLC is a promising framework for boosting the performance of known optimizers by adding more memory units and by making them more adaptive."],"url":"http://arxiv.org/abs/2402.15262v1","category":"cs.LG"}
{"created":"2024-02-23 10:55:18","title":"Towards Model-Driven Dashboard Generation for Systems-of-Systems","abstract":"Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.   This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.","sentences":["Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations.","Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.   ","This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology.","Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards."],"url":"http://arxiv.org/abs/2402.15257v1","category":"cs.SE"}
{"created":"2024-02-23 10:49:04","title":"Optimal Transport for Structure Learning Under Missing Data","abstract":"Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework is shown to recover the true causal graphs more effectively than the baselines in various simulations and real-data experiments. Empirical evidences also demonstrate the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data.","sentences":["Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma.","While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables.","Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal.","To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data.","This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM.","We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data.","Through extensive simulations and real-data experiments, our framework is shown to recover the true causal graphs more effectively than the baselines in various simulations and real-data experiments.","Empirical evidences also demonstrate the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data."],"url":"http://arxiv.org/abs/2402.15255v1","category":"cs.LG"}
{"created":"2024-02-23 10:35:40","title":"Remarks on the ($2+1$)-dimensional Duffin-Kemmer-Petiau oscillator in an external magnetic field","abstract":"This work re-examines the issue of spin-$1$ particles in a ($2+1$)-dimensional Duffin-Kemmer-Petiau oscillator (DKPO) in the presence of an external magnetic field. By following the appropriate procedure for the spin-$1$ sector of the Duffin-Kemmer-Petiau (DKP) theory, the previously used $6\\times 6$ representation in the literature is shown to be reducible to a $3\\times 3$ irreducible representation. This approach enabled us to find new aspects of the results recently disseminated in various studies, as well as other considerations overlooked and requiring revision. Finally, we present some applications of two-dimensional DKP theory in condensed matter systems, particularly in Lieb lattices.","sentences":["This work re-examines the issue of spin-$1$ particles in a ($2+1$)-dimensional Duffin-Kemmer-Petiau oscillator (DKPO) in the presence of an external magnetic field.","By following the appropriate procedure for the spin-$1$ sector of the Duffin-Kemmer-Petiau (DKP) theory, the previously used $6\\times 6$ representation in the literature is shown to be reducible to a $3\\times 3$ irreducible representation.","This approach enabled us to find new aspects of the results recently disseminated in various studies, as well as other considerations overlooked and requiring revision.","Finally, we present some applications of two-dimensional DKP theory in condensed matter systems, particularly in Lieb lattices."],"url":"http://arxiv.org/abs/2402.15252v1","category":"quant-ph"}
{"created":"2024-02-23 10:27:42","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues","abstract":"During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of generating novel chitchat-TOD scenarios to test TOD systems more thoroughly and improve their resilience to natural user interferences.","sentences":["During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation.","To address this issue without the need for expensive manual data creation, we use few-shot prompting with Llama-2-70B to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs.","We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction.","Our analysis reveals that our enriched dataset poses a significant challenge to these systems.","Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user's backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation.","These findings highlight the benefits of generating novel chitchat-TOD scenarios to test TOD systems more thoroughly and improve their resilience to natural user interferences."],"url":"http://arxiv.org/abs/2402.15248v1","category":"cs.CL"}
{"created":"2024-02-23 10:21:07","title":"A Bargaining-based Approach for Feature Trading in Vertical Federated Learning","abstract":"Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features. However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking into account the revenue-based optimization objectives of both parties. We analyze the proposed bargaining model under perfect and imperfect performance information settings, proving the existence of an equilibrium that optimizes the parties' objectives. Moreover, we develop performance gain estimation-based bargaining strategies for imperfect performance information scenarios and discuss potential security issues and solutions. Experiments on three real-world datasets demonstrate the effectiveness of the proposed bargaining model.","sentences":["Vertical Federated Learning (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy.","In production environment, VFL usually involves one task party and one data party.","Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party's features.","However, current VFL feature trading practices often price the data party's data as a whole and assume transactions occur prior to the performing VFL.","Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues.","In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions.","Our model incorporates performance gain-based pricing, taking into account the revenue-based optimization objectives of both parties.","We analyze the proposed bargaining model under perfect and imperfect performance information settings, proving the existence of an equilibrium that optimizes the parties' objectives.","Moreover, we develop performance gain estimation-based bargaining strategies for imperfect performance information scenarios and discuss potential security issues and solutions.","Experiments on three real-world datasets demonstrate the effectiveness of the proposed bargaining model."],"url":"http://arxiv.org/abs/2402.15247v1","category":"cs.LG"}
{"created":"2024-02-23 10:21:03","title":"Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging","abstract":"Most efforts in Computer Vision focus on natural images or artwork, which differ significantly both in size and contents from the kind of data biomedical image processing deals with. Thus, Transfer Learning models often prove themselves suboptimal for these tasks, even after manual finetuning. The development of architectures from scratch is oftentimes unfeasible due to the vastness of the hyperparameter space and a shortage of time, computational resources and Deep Learning experts in most biomedical research laboratories. An alternative to manually defining the models is the use of Neuroevolution, which employs metaheuristic techniques to optimize Deep Learning architectures. However, many algorithms proposed in the neuroevolutive literature are either too unreliable or limited to a small, predefined region of the hyperparameter space. To overcome these shortcomings, we propose the Chimera Algorithm, a novel, hybrid neuroevolutive algorithm that integrates the Artificial Bee Colony Algorithm with Evolutionary Computation tools to generate models from scratch, as well as to refine a given previous architecture to better fit the task at hand. The Chimera Algorithm has been validated with two datasets of natural and medical images, producing models that surpassed the performance of those coming from Transfer Learning.","sentences":["Most efforts in Computer Vision focus on natural images or artwork, which differ significantly both in size and contents from the kind of data biomedical image processing deals with.","Thus, Transfer Learning models often prove themselves suboptimal for these tasks, even after manual finetuning.","The development of architectures from scratch is oftentimes unfeasible due to the vastness of the hyperparameter space and a shortage of time, computational resources and Deep Learning experts in most biomedical research laboratories.","An alternative to manually defining the models is the use of Neuroevolution, which employs metaheuristic techniques to optimize Deep Learning architectures.","However, many algorithms proposed in the neuroevolutive literature are either too unreliable or limited to a small, predefined region of the hyperparameter space.","To overcome these shortcomings, we propose the Chimera Algorithm, a novel, hybrid neuroevolutive algorithm that integrates the Artificial Bee Colony Algorithm with Evolutionary Computation tools to generate models from scratch, as well as to refine a given previous architecture to better fit the task at hand.","The Chimera Algorithm has been validated with two datasets of natural and medical images, producing models that surpassed the performance of those coming from Transfer Learning."],"url":"http://arxiv.org/abs/2402.15246v1","category":"eess.IV"}
{"created":"2024-02-23 10:18:35","title":"Affleck-Dine Dirac Leptogenesis","abstract":"We present a minimal framework that realises successful Dirac Leptogenesis through the Affleck-Dine mechanism. A single right-handed neutrino and a neutrinophillic Higgs doublet are introduced to the Standard Model, which couple via a Yukawa interaction. The inflationary setting is induced by a combination of the two Higgs doublets, with their global symmetry violating interactions leading to a net charge generation via the Affleck-Dine mechanism. This simple Standard Model extension exhibits a unique and connected set of phenomenological implications including the resultant baryon asymmetry, inflationary predictions, cosmological implications, relic right-handed neutrinos, and its low energy phenomenology, while also being able to be embedded in various neutrino mass generating mechanisms.","sentences":["We present a minimal framework that realises successful Dirac Leptogenesis through the Affleck-Dine mechanism.","A single right-handed neutrino and a neutrinophillic Higgs doublet are introduced to the Standard Model, which couple via a Yukawa interaction.","The inflationary setting is induced by a combination of the two Higgs doublets, with their global symmetry violating interactions leading to a net charge generation via the Affleck-Dine mechanism.","This simple Standard Model extension exhibits a unique and connected set of phenomenological implications including the resultant baryon asymmetry, inflationary predictions, cosmological implications, relic right-handed neutrinos, and its low energy phenomenology, while also being able to be embedded in various neutrino mass generating mechanisms."],"url":"http://arxiv.org/abs/2402.15245v1","category":"hep-ph"}
{"created":"2024-02-23 10:13:08","title":"Safety-Conscious Pushing on Diverse Oriented Surfaces with Underactuated Aerial Vehicles","abstract":"Pushing tasks performed by aerial manipulators can be used for contact-based industrial inspections. Underactuated aerial vehicles are widely employed in aerial manipulation due to their widespread availability and relatively low cost. Industrial infrastructures often consist of diverse oriented work surfaces. When interacting with such surfaces, the coupled gravity compensation and interaction force generation of underactuated aerial vehicles can present the potential challenge of near-saturation operations. The blind utilization of these platforms for such tasks can lead to instability and accidents, creating unsafe operating conditions and potentially damaging the platform. In order to ensure safe pushing on these surfaces while managing platform saturation, this work establishes a safety assessment process. This process involves the prediction of the saturation level of each actuator during pushing across variable surface orientations. Furthermore, the assessment results are used to plan and execute physical experiments, ensuring safe operations and preventing platform damage.","sentences":["Pushing tasks performed by aerial manipulators can be used for contact-based industrial inspections.","Underactuated aerial vehicles are widely employed in aerial manipulation due to their widespread availability and relatively low cost.","Industrial infrastructures often consist of diverse oriented work surfaces.","When interacting with such surfaces, the coupled gravity compensation and interaction force generation of underactuated aerial vehicles can present the potential challenge of near-saturation operations.","The blind utilization of these platforms for such tasks can lead to instability and accidents, creating unsafe operating conditions and potentially damaging the platform.","In order to ensure safe pushing on these surfaces while managing platform saturation, this work establishes a safety assessment process.","This process involves the prediction of the saturation level of each actuator during pushing across variable surface orientations.","Furthermore, the assessment results are used to plan and execute physical experiments, ensuring safe operations and preventing platform damage."],"url":"http://arxiv.org/abs/2402.15243v1","category":"cs.RO"}
{"created":"2024-02-23 10:02:15","title":"GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation","abstract":"The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures.","sentences":["The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning.","Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging.","These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process.","To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware contrastive learning (BACL).","Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets.","The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures."],"url":"http://arxiv.org/abs/2402.15239v1","category":"cs.CV"}
{"created":"2024-02-23 10:02:01","title":"GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?","abstract":"Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind \"You are just a [slur] to me.\" However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.","sentences":["Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training.","Therefore, measuring the averaged performance over all examples in held-out test data is inadequate.","Instead, we must identify specific model weaknesses and be informed when it is more likely to fail.","A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind \"You are just a [slur] to me.\"","However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data.","To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs).","We employ an additional natural language inference (NLI) model to verify the generations.","Crowd-sourced annotation demonstrates that the generated test cases are of high quality.","Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset."],"url":"http://arxiv.org/abs/2402.15238v1","category":"cs.CL"}
{"created":"2024-02-23 09:57:20","title":"Multi-Agent Collaboration Framework for Recommender Systems","abstract":"LLM-based agents have gained considerable attention for their decision-making skills and ability to handle complex tasks. Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems, we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration. Unlike existing work on using agents for user/item simulation, we aim to deploy multi-agents to tackle recommendation tasks directly. In our framework, recommendation tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows. Furthermore, we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation of recommendation results. The framework and demonstration video are publicly available at https://github.com/wzf2000/MACRec.","sentences":["LLM-based agents have gained considerable attention for their decision-making skills and ability to handle complex tasks.","Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems, we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration.","Unlike existing work on using agents for user/item simulation, we aim to deploy multi-agents to tackle recommendation tasks directly.","In our framework, recommendation tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows.","Furthermore, we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation of recommendation results.","The framework and demonstration video are publicly available at https://github.com/wzf2000/MACRec."],"url":"http://arxiv.org/abs/2402.15235v1","category":"cs.IR"}
{"created":"2024-02-23 09:54:15","title":"Exact results for non-Newtonian transport properties in sheared granular suspensions: inelastic Maxwell models and BGK-type kinetic model","abstract":"The Boltzmann kinetic equation for dilute granular suspensions under simple (or uniform) shear flow (USF) is considered to determine the non-Newtonian transport properties of the system. In contrast to previous attempts based on a coarse-grained description, our suspension model accounts for the real collisions between grains and particles of the surrounding molecular gas. The latter is modeled as a bath (or thermostat) of elastic hard spheres at a given temperature. Two independent but complementary approaches are followed to reach exact expressions for the rheological properties. First, the Boltzmann equation for the so-called inelastic Maxwell models (IMM) is considered. The fact that the collision rate of IMM is independent of the relative velocity of the colliding spheres allows us to exactly compute the collisional moments of the Boltzmann operator without the knowledge of the distribution function. Thanks to this property the transport properties of the sheared granular suspension can be \\emph{exactly} determined. As a second approach, a Bhatnagar--Gross--Krook (BGK)-type kinetic model adapted to granular suspensions is solved to compute the velocity moments and the velocity distribution function of the system. The theoretical results show in general a good agreement with the approximate analytical results derived for inelastic hard spheres (IHS) by means of Grad's moment method and with computer simulations performed in the Brownian limiting case ($m/m_g\\to \\infty$, where $m_g$ and $m$ are the masses of the particles of the molecular and granular gases, respectively). In addition, as expected the IMM and BGK results show that the temperature and non-Newtonian viscosity exhibit and $S$ shape in a plane of stress-strain rate (discontinuous shear thickening, DST). The DST effect becomes more pronounced as the mass ratio $m/m_g$ increases.","sentences":["The Boltzmann kinetic equation for dilute granular suspensions under simple (or uniform) shear flow (USF) is considered to determine the non-Newtonian transport properties of the system.","In contrast to previous attempts based on a coarse-grained description, our suspension model accounts for the real collisions between grains and particles of the surrounding molecular gas.","The latter is modeled as a bath (or thermostat) of elastic hard spheres at a given temperature.","Two independent but complementary approaches are followed to reach exact expressions for the rheological properties.","First, the Boltzmann equation for the so-called inelastic Maxwell models (IMM) is considered.","The fact that the collision rate of IMM is independent of the relative velocity of the colliding spheres allows us to exactly compute the collisional moments of the Boltzmann operator without the knowledge of the distribution function.","Thanks to this property the transport properties of the sheared granular suspension can be \\emph{exactly} determined.","As a second approach, a Bhatnagar--Gross--Krook (BGK)-type kinetic model adapted to granular suspensions is solved to compute the velocity moments and the velocity distribution function of the system.","The theoretical results show in general a good agreement with the approximate analytical results derived for inelastic hard spheres (IHS) by means of Grad's moment method and with computer simulations performed in the Brownian limiting case ($m/m_g\\to \\infty$, where $m_g$ and $m$ are the masses of the particles of the molecular and granular gases, respectively).","In addition, as expected the IMM and BGK results show that the temperature and non-Newtonian viscosity exhibit and $S$ shape in a plane of stress-strain rate (discontinuous shear thickening, DST).","The DST effect becomes more pronounced as the mass ratio $m/m_g$ increases."],"url":"http://arxiv.org/abs/2402.15234v1","category":"cond-mat.soft"}
{"created":"2024-02-23 09:54:06","title":"On the composable security of weak coin flipping","abstract":"Weak coin flipping is a cryptographic primitive in which two mutually distrustful parties generate a shared random bit to agree on a winner via remote communication. While a stand-alone secure weak coin flipping protocol can be constructed from noiseless communication channels, its composability has not been explored. In this work, we demonstrate that no weak coin flipping protocol can be abstracted into a black box resource with composable security. Despite this, we also establish the overall stand-alone security of weak coin flipping protocols under sequential composition.","sentences":["Weak coin flipping is a cryptographic primitive in which two mutually distrustful parties generate a shared random bit to agree on a winner via remote communication.","While a stand-alone secure weak coin flipping protocol can be constructed from noiseless communication channels, its composability has not been explored.","In this work, we demonstrate that no weak coin flipping protocol can be abstracted into a black box resource with composable security.","Despite this, we also establish the overall stand-alone security of weak coin flipping protocols under sequential composition."],"url":"http://arxiv.org/abs/2402.15233v1","category":"quant-ph"}
{"created":"2024-02-23 09:47:42","title":"Classification of compact radio sources in the Galactic plane with supervised machine learning","abstract":"Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. Source extraction, characterization, and classification are the major stages involved in this process. In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs. To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP). Radio spectral index information was also obtained for a subset of the data. We then trained two different classifiers on the produced dataset. The first model uses gradient-boosted decision trees and is trained on a set of pre-computed features derived from the data, which include radio-infrared colour indices and the radio spectral index. The second model is trained directly on multi-channel images, employing convolutional neural networks. Using a completely supervised procedure, we obtained a high classification accuracy (F1-score>90%) for separating Galactic objects from the extragalactic background. Individual class discrimination performances, ranging from 60% to 75%, increased by 10% when adding far-infrared and spectral index information, with extragalactic objects, PNe and HII regions identified with higher accuracies. The implemented tools and trained models were publicly released, and made available to the radioastronomical community for future application on new radio data.","sentences":["Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing.","Source extraction, characterization, and classification are the major stages involved in this process.","In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs.","To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP).","Radio spectral index information was also obtained for a subset of the data.","We then trained two different classifiers on the produced dataset.","The first model uses gradient-boosted decision trees and is trained on a set of pre-computed features derived from the data, which include radio-infrared colour indices and the radio spectral index.","The second model is trained directly on multi-channel images, employing convolutional neural networks.","Using a completely supervised procedure, we obtained a high classification accuracy (F1-score>90%) for separating Galactic objects from the extragalactic background.","Individual class discrimination performances, ranging from 60% to 75%, increased by 10% when adding far-infrared and spectral index information, with extragalactic objects, PNe and HII regions identified with higher accuracies.","The implemented tools and trained models were publicly released, and made available to the radioastronomical community for future application on new radio data."],"url":"http://arxiv.org/abs/2402.15232v1","category":"astro-ph.IM"}
{"created":"2024-02-23 09:44:02","title":"Convolutions and Mixtures of Gamma, Stable and Mittag-Leffler Distributions","abstract":"This paper uses convolutions of the gamma density and the one-sided stable density to construct higher level densities. The approach is applied to constructing a 4-parameter Mittag-Leffler density, whose Laplace transform is a corresponding Mittag-Leffler function, which is completely monotone (CM) by construction. Laplace transforms of mixtures of the stable densities with respect to the 4-parameter Mittag-Leffler distribution are compositions of the Mittag-Leffler functions with Bernstein functions, thereby generating a rich family of CM variants of the base CM Mittag-Leffler functions, including known instances as special cases.","sentences":["This paper uses convolutions of the gamma density and the one-sided stable density to construct higher level densities.","The approach is applied to constructing a 4-parameter Mittag-Leffler density, whose Laplace transform is a corresponding Mittag-Leffler function, which is completely monotone (CM) by construction.","Laplace transforms of mixtures of the stable densities with respect to the 4-parameter Mittag-Leffler distribution are compositions of the Mittag-Leffler functions with Bernstein functions, thereby generating a rich family of CM variants of the base CM Mittag-Leffler functions, including known instances as special cases."],"url":"http://arxiv.org/abs/2402.15228v1","category":"math.PR"}
{"created":"2024-02-23 09:43:58","title":"Fixed Random Classifier Rearrangement for Continual Learning","abstract":"With the explosive growth of data, continual learning capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second stage, FRCR rearranges the entries of new classifiers to implicitly reduce the drift of old latent representations. The experimental results on multiple datasets show that FRCR significantly mitigates the model forgetting; subsequent experimental analyses further validate the effectiveness of the algorithm.","sentences":["With the explosive growth of data, continual learning capability is increasingly important for neural networks.","Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones.","In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone.","However, the impact of classifiers is underestimated.","In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level.","Based on this conclusion, we propose a two-stage continual learning algorithm named Fixed Random Classifier Rearrangement (FRCR).","In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network.","In second stage, FRCR rearranges the entries of new classifiers to implicitly reduce the drift of old latent representations.","The experimental results on multiple datasets show that FRCR significantly mitigates the model forgetting; subsequent experimental analyses further validate the effectiveness of the algorithm."],"url":"http://arxiv.org/abs/2402.15227v1","category":"cs.LG"}
{"created":"2024-02-23 09:36:57","title":"Attosecond vortex pulse trains","abstract":"The landscape of ultrafast structured light pulses has recently evolved driven by the capability of high-order harmonic generation (HHG) to up-convert orbital angular momentum (OAM) from the infrared to the extreme-ultraviolet (EUV) spectral regime. Accordingly, HHG has been proven to produce EUV vortex pulses at the femtosecond timescale. Here we demonstrate the generation of attosecond vortex pulse trains, i.e. a succession of attosecond pulses with a helical wavefront, resulting from the synthesis of a comb of EUV high-order harmonics with the same OAM. By driving HHG with a polarization tilt-angle fork grating, two spatially separated circularly polarized high-order harmonic beams with order-independent OAM are created. Our work opens the route towards attosecond-resolved OAM light-matter interactions.","sentences":["The landscape of ultrafast structured light pulses has recently evolved driven by the capability of high-order harmonic generation (HHG) to up-convert orbital angular momentum (OAM) from the infrared to the extreme-ultraviolet (EUV) spectral regime.","Accordingly, HHG has been proven to produce EUV vortex pulses at the femtosecond timescale.","Here we demonstrate the generation of attosecond vortex pulse trains, i.e. a succession of attosecond pulses with a helical wavefront, resulting from the synthesis of a comb of EUV high-order harmonics with the same OAM.","By driving HHG with a polarization tilt-angle fork grating, two spatially separated circularly polarized high-order harmonic beams with order-independent OAM are created.","Our work opens the route towards attosecond-resolved OAM light-matter interactions."],"url":"http://arxiv.org/abs/2402.15225v1","category":"physics.optics"}
{"created":"2024-02-23 09:28:16","title":"BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators","abstract":"Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future.","sentences":["Extremely large image generators offer significant transformative potential across diverse sectors.","It allows users to design specific prompts to generate realistic images through some black-box APIs.","However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers.","We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs.","Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions.","To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users.","It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators.","Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators.","Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval).","Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3.","Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future."],"url":"http://arxiv.org/abs/2402.15218v1","category":"cs.CR"}
{"created":"2024-02-23 09:25:57","title":"Label-efficient Multi-organ Segmentation Method with Diffusion Model","abstract":"Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. Various supervised-learning approaches have been proposed recently. However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice. In this study, we present a label-efficient learning approach using a pre-trained diffusion model for multi-organ segmentation tasks in CT images. First, a denoising diffusion model was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images. Then the pre-trained denoising diffusion network was transferred to the downstream multi-organ segmentation task, effectively creating a semi-supervised learning model that requires only a small amount of labeled data. Furthermore, linear classification and fine-tuning decoder strategies were employed to enhance the network's segmentation performance. Our generative model at 256x256 resolution achieves impressive performance in terms of Fr\\'echet inception distance, spatial Fr\\'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1\\%, respectively. These results affirm the diffusion model's ability to generate diverse and realistic 2D CT images. Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios. Remarkably, even with only 1\\% and 10\\% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56\\% and 78.51\\% after fine-tuning, respectively. The method achieves a DSC score of 51.81\\% using just four labeled CT scans. These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning heavily reliant on large-scale labeled data.","sentences":["Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems.","Various supervised-learning approaches have been proposed recently.","However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice.","In this study, we present a label-efficient learning approach using a pre-trained diffusion model for multi-organ segmentation tasks in CT images.","First, a denoising diffusion model was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images.","Then the pre-trained denoising diffusion network was transferred to the downstream multi-organ segmentation task, effectively creating a semi-supervised learning model that requires only a small amount of labeled data.","Furthermore, linear classification and fine-tuning decoder strategies were employed to enhance the network's segmentation performance.","Our generative model at 256x256 resolution achieves impressive performance in terms of Fr\\'echet inception distance, spatial Fr\\'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1\\%, respectively.","These results affirm the diffusion model's ability to generate diverse and realistic 2D CT images.","Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios.","Remarkably, even with only 1\\% and 10\\% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56\\% and 78.51\\% after fine-tuning, respectively.","The method achieves a DSC score of 51.81\\% using just four labeled CT scans.","These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning heavily reliant on large-scale labeled data."],"url":"http://arxiv.org/abs/2402.15216v1","category":"cs.CV"}
{"created":"2024-02-23 09:16:57","title":"Primordial Black Holes as a dark matter candidate -- a brief overview","abstract":"Historically the most popular dark matter candidates have been new elementary particles, such as Weakly Interacting Massive Particles and axions. However Primordial Black Holes (PBHs), black holes formed from overdensities in the early Universe, are another possibility. The discovery of gravitational waves from mergers of tens of Solar mass black hole binaries by LIGO-Virgo has generated a surge in interest in PBH dark matter. We overview the formation of PBHs, observational probes of their abundance, and some of the key open questions in the field.","sentences":["Historically the most popular dark matter candidates have been new elementary particles, such as Weakly Interacting Massive Particles and axions.","However Primordial Black Holes (PBHs), black holes formed from overdensities in the early Universe, are another possibility.","The discovery of gravitational waves from mergers of tens of Solar mass black hole binaries by LIGO-Virgo has generated a surge in interest in PBH dark matter.","We overview the formation of PBHs, observational probes of their abundance, and some of the key open questions in the field."],"url":"http://arxiv.org/abs/2402.15211v1","category":"astro-ph.CO"}
{"created":"2024-02-23 09:16:11","title":"An Analysis of the Evolution Equations for a Generalized Bioconvective Flow","abstract":"We prove results on existence and uniqueness of solutions of a system of equations modeling the evolution of a generalized bioconvective flow. The mathematical model considered in the present work describes the convective motion generated by the upward swimming of a culture of microorganisms under the influence of vertical gravitational forces, in an incompressible viscous fluid whose viscosity may depend on the concentration of microorganisms.","sentences":["We prove results on existence and uniqueness of solutions of a system of equations modeling the evolution of a generalized bioconvective flow.","The mathematical model considered in the present work describes the convective motion generated by the upward swimming of a culture of microorganisms under the influence of vertical gravitational forces, in an incompressible viscous fluid whose viscosity may depend on the concentration of microorganisms."],"url":"http://arxiv.org/abs/2402.15210v1","category":"math.AP"}
{"created":"2024-02-23 09:13:32","title":"Closed-loop design for scalable performance of vehicular formations","abstract":"This paper presents a novel control design for vehicular formations, which is an alternative to the conventional second-order consensus protocol. The design is motivated by the closed-loop system, which we construct as first-order systems connected in series, and is therefore called serial consensus. The serial consensus design will guarantee stability of the closed-loop system under the minimum requirement of the underlying communication graphs each containing a connected spanning tree -- something that is not true in general for the conventional consensus protocols. Here, we show that the serial consensus design also gives guarantees on the worst-case transient behavior of the formation, which are independent of the number of vehicles and the underlying graph structure. In particular this shows that the serial consensus design can be used to guarantee string stability of the formation, and is therefore suitable for directed formations. We show that it can be implemented through message passing or measurements to neighbors at most two hops away. The results are illustrated through numerical examples.","sentences":["This paper presents a novel control design for vehicular formations, which is an alternative to the conventional second-order consensus protocol.","The design is motivated by the closed-loop system, which we construct as first-order systems connected in series, and is therefore called serial consensus.","The serial consensus design will guarantee stability of the closed-loop system under the minimum requirement of the underlying communication graphs each containing a connected spanning tree -- something that is not true in general for the conventional consensus protocols.","Here, we show that the serial consensus design also gives guarantees on the worst-case transient behavior of the formation, which are independent of the number of vehicles and the underlying graph structure.","In particular this shows that the serial consensus design can be used to guarantee string stability of the formation, and is therefore suitable for directed formations.","We show that it can be implemented through message passing or measurements to neighbors at most two hops away.","The results are illustrated through numerical examples."],"url":"http://arxiv.org/abs/2402.15208v1","category":"math.OC"}
{"created":"2024-02-23 09:06:25","title":"Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing","abstract":"Intensive care unit (ICU) patients often develop new health-related problems in their long-term recovery. Health care professionals keeping a diary of a patient's stay is a proven strategy to tackle this but faces several adoption barriers, such as lack of time and difficulty in knowing what to write. Large language models (LLMs), with their ability to generate human-like text and adaptability, could solve these challenges. However, realizing this vision involves addressing several socio-technical and practical research challenges. This paper discusses these challenges and proposes future research directions to utilize the potential of LLMs in ICU diary writing, ultimately improving the long-term recovery outcomes for ICU patients.","sentences":["Intensive care unit (ICU) patients often develop new health-related problems in their long-term recovery.","Health care professionals keeping a diary of a patient's stay is a proven strategy to tackle this but faces several adoption barriers, such as lack of time and difficulty in knowing what to write.","Large language models (LLMs), with their ability to generate human-like text and adaptability, could solve these challenges.","However, realizing this vision involves addressing several socio-technical and practical research challenges.","This paper discusses these challenges and proposes future research directions to utilize the potential of LLMs in ICU diary writing, ultimately improving the long-term recovery outcomes for ICU patients."],"url":"http://arxiv.org/abs/2402.15205v1","category":"cs.HC"}
{"created":"2024-02-23 09:04:48","title":"Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models","abstract":"Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity.","sentences":["Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs).","However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility.","To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity.","However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models.","In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost.","Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level.","This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt.","We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels.","Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity."],"url":"http://arxiv.org/abs/2402.15202v1","category":"cs.CL"}
{"created":"2024-02-23 09:01:00","title":"DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators","abstract":"Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling.","sentences":["Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially.","This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts.","In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT.","First, DeMPT divides the context-aware NMT process into three separate phases.","During each phase, different continuous prompts are introduced to make LLMs discriminately model various information.","Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase.","Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling."],"url":"http://arxiv.org/abs/2402.15200v1","category":"cs.CL"}
{"created":"2024-02-23 08:58:38","title":"Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization","abstract":"Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a condition for SORL's converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff. The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods. The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications.","sentences":["Safe reinforcement learning (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error.","In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously.","The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic.","The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space.","This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints.","Through our theoretical analysis of SORL, we propose a condition for SORL's converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for fine-tuning the mentioned tradeoff.","The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods.","The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications."],"url":"http://arxiv.org/abs/2402.15197v1","category":"eess.SY"}
{"created":"2024-02-23 08:55:47","title":"The AffectToolbox: Affect Analysis for Everyone","abstract":"In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result. The entire system is open-sourced and will be publicly available to ensure easy integration into more complex applications through a well-structured, Python-based code base - therefore marking a substantial contribution toward advancing affective computing research and fostering a more collaborative and inclusive environment within this interdisciplinary field.","sentences":["In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent.","In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes.","The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers.","Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface.","The architecture encompasses a variety of models for emotion recognition on multiple affective channels and modalities, as well as an elaborate fusion system to merge multi-modal assessments into a unified result.","The entire system is open-sourced and will be publicly available to ensure easy integration into more complex applications through a well-structured, Python-based code base - therefore marking a substantial contribution toward advancing affective computing research and fostering a more collaborative and inclusive environment within this interdisciplinary field."],"url":"http://arxiv.org/abs/2402.15195v1","category":"cs.HC"}
{"created":"2024-02-23 08:54:42","title":"Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control","abstract":"Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth \"genuine\" reward, as is the case in many practical applications. These challenges, collectively termed \"reward collapse,\" pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.","sentences":["Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins.","While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins.","Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image).","However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function.","The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth \"genuine\" reward, as is the case in many practical applications.","These challenges, collectively termed \"reward collapse,\" pose a substantial obstacle.","To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs.","We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models."],"url":"http://arxiv.org/abs/2402.15194v1","category":"cs.LG"}
{"created":"2024-02-23 08:43:32","title":"Asymptotics of the Smallest Eigenvalue Distributions of Freud Unitary Ensembles","abstract":"We consider the smallest eigenvalue distributions of some Freud unitary ensembles, that is, the probabilities that all the eigenvalues of the Hermitian matrices from the ensembles lie in the interval $(t,\\infty)$. This problem is related to the Hankel determinants generated by the Freud weights with a jump discontinuity. By using Chen and Ismail's ladder operator approach, we obtain the discrete systems for the recurrence coefficients of the corresponding orthogonal polynomials. This enables us to derive the large $n$ asymptotics of the recurrence coefficients via Dyson's Coulomb fluid approach. We finally obtain the large $n$ asymptotics of the Hankel determinants and that of the probabilities from their relations to the recurrence coefficients and with the aid of some recent results in the literature.","sentences":["We consider the smallest eigenvalue distributions of some Freud unitary ensembles, that is, the probabilities that all the eigenvalues of the Hermitian matrices from the ensembles lie in the interval $(t,\\infty)$. This problem is related to the Hankel determinants generated by the Freud weights with a jump discontinuity.","By using Chen and Ismail's ladder operator approach, we obtain the discrete systems for the recurrence coefficients of the corresponding orthogonal polynomials.","This enables us to derive the large $n$ asymptotics of the recurrence coefficients via Dyson's Coulomb fluid approach.","We finally obtain the large $n$ asymptotics of the Hankel determinants and that of the probabilities from their relations to the recurrence coefficients and with the aid of some recent results in the literature."],"url":"http://arxiv.org/abs/2402.15190v1","category":"math-ph"}
{"created":"2024-02-23 08:40:38","title":"Biomedical Entity Linking as Multiple Choice Question Answering","abstract":"Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets.","sentences":["Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities.","To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering.","BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity.","This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves.","To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator.","Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets."],"url":"http://arxiv.org/abs/2402.15189v1","category":"cs.CL"}
{"created":"2024-02-23 08:36:28","title":"Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions","abstract":"This paper studies performative risk minimization, a formulation of stochastic optimization under decision-dependent distributions. We consider the general case where the performative risk can be non-convex, for which we develop efficient parameter-free optimistic optimization-based methods. Our algorithms significantly improve upon the existing Lipschitz bandit-based method in many aspects. In particular, our framework does not require knowledge about the sensitivity parameter of the distribution map and the Lipshitz constant of the loss function. This makes our framework practically favorable, together with the efficient optimistic optimization-based tree-search mechanism. We provide experimental results that demonstrate the numerical superiority of our algorithms over the existing method and other black-box optimistic optimization methods.","sentences":["This paper studies performative risk minimization, a formulation of stochastic optimization under decision-dependent distributions.","We consider the general case where the performative risk can be non-convex, for which we develop efficient parameter-free optimistic optimization-based methods.","Our algorithms significantly improve upon the existing Lipschitz bandit-based method in many aspects.","In particular, our framework does not require knowledge about the sensitivity parameter of the distribution map and the Lipshitz constant of the loss function.","This makes our framework practically favorable, together with the efficient optimistic optimization-based tree-search mechanism.","We provide experimental results that demonstrate the numerical superiority of our algorithms over the existing method and other black-box optimistic optimization methods."],"url":"http://arxiv.org/abs/2402.15188v1","category":"cs.LG"}
{"created":"2024-02-23 08:34:17","title":"Ultra-short lifetime isomer studies from photonuclear reactions using laser-driven ultra-intense \u03b3-ray","abstract":"Isomers, ubiquitous populations of relatively long-lived nuclear excited states, play a crucial role in nuclear physics. However, isomers with half-life times of several seconds or less barely had experimental cross section data due to the lack of a suitable measuring method. We report a method of online {\\gamma} spectroscopy for ultra-short-lived isomers from photonuclear reactions using laser-driven ultra-intense {\\gamma}-rays. The fastest time resolution can reach sub-ps level with {\\gamma}-ray intensities >10^{19}/s ({\\geqslant} 8 MeV). The ^{115}In({\\gamma}, n)^{114m2}In reaction (T_{1/2} = 43.1 ms) was first measured in the high-energy region which shed light on the nuclear structure studies of In element. Simulations showed it would be an efficient way to study ^{229m}Th (T_{1/2} = 7 {\\mu}s), which is believed to be the next generation of nuclear clock. This work offered a unique way of gaining insight into ultra-short lifetimes and promised an effective way to fill the gap in relevant experimental data.","sentences":["Isomers, ubiquitous populations of relatively long-lived nuclear excited states, play a crucial role in nuclear physics.","However, isomers with half-life times of several seconds or less barely had experimental cross section data due to the lack of a suitable measuring method.","We report a method of online {\\gamma} spectroscopy for ultra-short-lived isomers from photonuclear reactions using laser-driven ultra-intense {\\gamma}-rays.","The fastest time resolution can reach sub-ps level with {\\gamma}-ray intensities >10^{19}/s ({\\geqslant} 8 MeV).","The ^{115}In({\\gamma}, n)^{114m2}In reaction (T_{1/2} = 43.1 ms) was first measured in the high-energy region which shed light on the nuclear structure studies of In element.","Simulations showed it would be an efficient way to study ^{229m}Th (T_{1/2} = 7 {\\mu}s), which is believed to be the next generation of nuclear clock.","This work offered a unique way of gaining insight into ultra-short lifetimes and promised an effective way to fill the gap in relevant experimental data."],"url":"http://arxiv.org/abs/2402.15187v1","category":"nucl-ex"}
{"created":"2024-02-23 08:29:58","title":"A Linear Inverse Model for Colored-Gaussian Noise","abstract":"We propose a novel data-driven linear inverse model, called Colored-LIM, to extract the linear dynamics and diffusion matrix that define a linear stochastic process driven by an Ornstein-Uhlenbeck colored-noise. The Colored-LIM is a new variant of the classical linear inverse model (LIM) which relies on the white noise assumption. Similar to LIM, the Colored-LIM approximates the linear dynamics from a finite realization of a stochastic process and then solves the diffusion matrix based on, for instance, a generalized fluctuation-dissipation relation, which can be done by solving a system of linear equations. The main difficulty is that in practice, the colored-noise process can be hardly observed while it is correlated to the stochastic process of interest. Nevertheless, we show that the local behavior of the correlation function of the observable encodes the dynamics of the stochastic process and the diffusive behavior of the colored-noise.   In this article, we review the classical LIM and develop Colored-LIM with a mathematical background and rigorous derivations. In the numerical experiments, we examine the performance of both LIM and Colored-LIM. Finally, we discuss some false attempts to build a linear inverse model for colored-noise driven processes, and investigate the potential misuse and its consequence of LIM in the appendices.","sentences":["We propose a novel data-driven linear inverse model, called Colored-LIM, to extract the linear dynamics and diffusion matrix that define a linear stochastic process driven by an Ornstein-Uhlenbeck colored-noise.","The Colored-LIM is a new variant of the classical linear inverse model (LIM) which relies on the white noise assumption.","Similar to LIM, the Colored-LIM approximates the linear dynamics from a finite realization of a stochastic process and then solves the diffusion matrix based on, for instance, a generalized fluctuation-dissipation relation, which can be done by solving a system of linear equations.","The main difficulty is that in practice, the colored-noise process can be hardly observed while it is correlated to the stochastic process of interest.","Nevertheless, we show that the local behavior of the correlation function of the observable encodes the dynamics of the stochastic process and the diffusive behavior of the colored-noise.   ","In this article, we review the classical LIM and develop Colored-LIM with a mathematical background and rigorous derivations.","In the numerical experiments, we examine the performance of both LIM and Colored-LIM.","Finally, we discuss some false attempts to build a linear inverse model for colored-noise driven processes, and investigate the potential misuse and its consequence of LIM in the appendices."],"url":"http://arxiv.org/abs/2402.15184v1","category":"math.NA"}
{"created":"2024-02-23 08:29:42","title":"GraphEdit: Large Language Models for Graph Structure Learning","abstract":"Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: https://github.com/HKUDS/GraphEdit.","sentences":["Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures.","Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies.","However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity.","In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data.","By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning.","Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure.","We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings.","We have made our model implementation available at: https://github.com/HKUDS/GraphEdit."],"url":"http://arxiv.org/abs/2402.15183v1","category":"cs.LG"}
{"created":"2024-02-23 08:20:53","title":"Traversable wormholes satisfying energy conditions in $f(Q)$ gravity","abstract":"In this article, a new family of asymptotically flat wormhole solutions in the context of symmetric teleparallel gravity, i.e., $f(Q)$ theory of gravity, are presented. Considering a power-law shape function and some different forms for $f(Q)$ function, we show that a wide variety of wormhole solutions for which the matter fields satisfy some energy conditions, are accessible. We explore that the presence of $f(Q)$ gravity will be enough to sustain a traversable wormhole without exotic matter. The influence of free parameters in shape function and $f(Q)$ models on the energy conditions is investigated. The equation of state and boundary conditions are analyzed.","sentences":["In this article, a new family of asymptotically flat wormhole solutions in the context of symmetric teleparallel gravity, i.e., $f(Q)$ theory of gravity, are presented.","Considering a power-law shape function and some different forms for $f(Q)$ function, we show that a wide variety of wormhole solutions for which the matter fields satisfy some energy conditions, are accessible.","We explore that the presence of $f(Q)$ gravity will be enough to sustain a traversable wormhole without exotic matter.","The influence of free parameters in shape function and $f(Q)$ models on the energy conditions is investigated.","The equation of state and boundary conditions are analyzed."],"url":"http://arxiv.org/abs/2402.15178v1","category":"gr-qc"}
{"created":"2024-02-23 18:32:39","title":"On the Complexity of Community-aware Network Sparsification","abstract":"Network sparsification is the task of reducing the number of edges of a given graph while preserving some crucial graph property. In community-aware network sparsification, the preserved property concerns the subgraphs that are induced by the communities of the graph which are given as vertex subsets. This is formalized in the $\\Pi$-Network Sparsification problem: given an edge-weighted graph $G$, a collection $Z$ of $c$ subsets of $V(G)$ (communities), and two numbers $\\ell, b$, the question is whether there exists a spanning subgraph $G'$ of $G$ with at most $\\ell$ edges of total weight at most $b$ such that $G'[C]$ fulfills $\\Pi$ for each community $C$. Here, we consider two graph properties $\\Pi$: the connectivity property (Connectivity NWS) and the property of having a spanning star (Stars NWS). Since both problems are NP-hard, we study their parameterized and fine-grained complexity.   We provide a tight $2^{\\Omega(n^2+c)} poly(n+|Z|)$-time running time lower bound based on the ETH for both problems, where $n$ is the number of vertices in $G$. The lower bound holds even in the restricted case when all communities have size at most 4, $G$ is a clique, and every edge has unit weight. For the connectivity property, the unit weight case with $G$ being a clique is the well-studied problem of computing a hypergraph support with a minimum number of edges. We then study the complexity of both problems parameterized by the feedback edge number $t$ of the solution graph $G'$. For Stars NWS, we present an XP-algorithm for $t$. This answers an open question by Korach and Stern [Disc. Appl. Math. '08] who asked for the existence of polynomial-time algorithms for $t=0$. In contrast, we show for Connectivity NWS that known polynomial-time algorithms for $t=0$ [Korach and Stern, Math. Program. '03; Klemz et al., SWAT '14] cannot be extended by showing that Connectivity NWS is NP-hard for $t=1$.","sentences":["Network sparsification is the task of reducing the number of edges of a given graph while preserving some crucial graph property.","In community-aware network sparsification, the preserved property concerns the subgraphs that are induced by the communities of the graph which are given as vertex subsets.","This is formalized in the $\\Pi$-Network Sparsification problem: given an edge-weighted graph $G$, a collection $Z$ of $c$ subsets of $V(G)$ (communities), and two numbers $\\ell, b$, the question is whether there exists a spanning subgraph $G'$ of $G$ with at most $\\ell$ edges of total weight at most $b$ such that $G'[C]$ fulfills $\\Pi$ for each community $C$.","Here, we consider two graph properties $\\Pi$: the connectivity property (Connectivity NWS) and the property of having a spanning star (Stars NWS).","Since both problems are NP-hard, we study their parameterized and fine-grained complexity.   ","We provide a tight $2^{\\Omega(n^2+c)}","poly(n+|Z|)$-time running time lower bound based on the ETH for both problems, where $n$ is the number of vertices in $G$. The lower bound holds even in the restricted case when all communities have size at most 4, $G$ is a clique, and every edge has unit weight.","For the connectivity property, the unit weight case with $G$ being a clique is the well-studied problem of computing a hypergraph support with a minimum number of edges.","We then study the complexity of both problems parameterized by the feedback edge number $t$ of the solution graph $G'$. For Stars NWS, we present an XP-algorithm for $t$. This answers an open question by Korach and Stern [Disc.","Appl.","Math.","'08] who asked for the existence of polynomial-time algorithms for $t=0$. In contrast, we show for Connectivity NWS that known polynomial-time algorithms for $t=0$","[Korach and Stern, Math. Program.","'03; Klemz et al., SWAT '14] cannot be extended by showing that Connectivity NWS is NP-hard for $t=1$."],"url":"http://arxiv.org/abs/2402.15494v1","category":"cs.DS"}
{"created":"2024-02-23 18:28:04","title":"On inference for modularity statistics in structured networks","abstract":"This paper revisits the classical concept of network modularity and its spectral relaxations used throughout graph data analysis. We formulate and study several modularity statistic variants for which we establish asymptotic distributional results in the large-network limit for networks exhibiting nodal community structure. Our work facilitates testing for network differences and can be used in conjunction with existing theoretical guarantees for stochastic blockmodel random graphs. Our results are enabled by recent advances in the study of low-rank truncations of large network adjacency matrices. We provide confirmatory simulation studies and real data analysis pertaining to the network neuroscience study of psychosis, specifically schizophrenia. Collectively, this paper contributes to the limited existing literature to date on statistical inference for modularity-based network analysis. Supplemental materials for this article are available online.","sentences":["This paper revisits the classical concept of network modularity and its spectral relaxations used throughout graph data analysis.","We formulate and study several modularity statistic variants for which we establish asymptotic distributional results in the large-network limit for networks exhibiting nodal community structure.","Our work facilitates testing for network differences and can be used in conjunction with existing theoretical guarantees for stochastic blockmodel random graphs.","Our results are enabled by recent advances in the study of low-rank truncations of large network adjacency matrices.","We provide confirmatory simulation studies and real data analysis pertaining to the network neuroscience study of psychosis, specifically schizophrenia.","Collectively, this paper contributes to the limited existing literature to date on statistical inference for modularity-based network analysis.","Supplemental materials for this article are available online."],"url":"http://arxiv.org/abs/2402.15489v1","category":"stat.ME"}
{"created":"2024-02-23 18:05:06","title":"Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant reduction in dataset size (just $940$ samples) while advancing the state-of-the-art. Our contributions include a novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion Summarization, and a human preference dataset (OpinPref). The proposed methodology opens avenues for efficient RLHF, making it more adaptable to diverse applications with varying human values. We release the artifacts for usage under MIT License.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals.","The key to the strategy is employing a reward model ({$\\varphi$}) which can reflect a latent reward model with humans.","While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\\varphi$}.","Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used.","However, human values/goals are subjective and depend on the nature of the task.","This poses a challenge in collecting diverse preferences for downstream applications.","To address this, we propose a novel methodology to infuse domain knowledge into {$\\varphi$}, which reduces the size of preference annotation required.","We validate our approach in E-Commerce Opinion Summarization, with a significant reduction in dataset size (just $940$ samples) while advancing the state-of-the-art.","Our contributions include a novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion Summarization, and a human preference dataset (OpinPref).","The proposed methodology opens avenues for efficient RLHF, making it more adaptable to diverse applications with varying human values.","We release the artifacts for usage under MIT License."],"url":"http://arxiv.org/abs/2402.15473v1","category":"cs.CL"}
{"created":"2024-02-23 18:04:54","title":"FAIR: Filtering of Automatically Induced Rules","abstract":"The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set. We experiment with three ARI approaches and five text classification datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches. Further, we show that achieves statistically significant results in comparison to existing rule-filtering approaches.","sentences":["The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains.","Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules.","However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data.","Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them.","In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules.","In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set.","We experiment with three ARI approaches and five text classification datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches.","Further, we show that achieves statistically significant results in comparison to existing rule-filtering approaches."],"url":"http://arxiv.org/abs/2402.15472v1","category":"cs.LG"}
{"created":"2024-02-23 16:30:05","title":"PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning","abstract":"Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilect","sentences":["Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs.","However, formulating realistic policies for robots demands responses from humans to an extensive array of queries.","In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting.","To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans.","To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM.","In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications.","Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape.","We provide video examples of the trained policies at https://sites.google.com/view/rl-predilect"],"url":"http://arxiv.org/abs/2402.15420v1","category":"cs.RO"}
{"created":"2024-02-23 14:55:58","title":"Efficient semi-supervised inference for logistic regression under case-control studies","abstract":"Semi-supervised learning has received increasingly attention in statistics and machine learning. In semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. We consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. Under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. However, the intercept parameter is not identifiable. Consequently, the marginal case proportion cannot be estimated from case-control data. We find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting. We construct the likelihood function of the observed labeled and unlabeled data and obtain the maximum likelihood estimator via an iterative algorithm. The proposed estimator is shown to be consistent, asymptotically normal, and semiparametrically efficient. Extensive simulation studies are conducted to show the finite sample performance of the proposed method. The results imply that the unlabeled data not only helps to identify the intercept but also improves the estimation efficiency of the slope parameter. Meanwhile, the marginal case proportion can be estimated accurately by the proposed method.","sentences":["Semi-supervised learning has received increasingly attention in statistics and machine learning.","In semi-supervised learning settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected.","We consider an inference problem in semi-supervised settings where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling.","Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data.","Under the logistic model assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model.","However, the intercept parameter is not identifiable.","Consequently, the marginal case proportion cannot be estimated from case-control data.","We find out that with the availability of the unlabeled data, the intercept parameter can be identified in semi-supervised learning setting.","We construct the likelihood function of the observed labeled and unlabeled data and obtain the maximum likelihood estimator via an iterative algorithm.","The proposed estimator is shown to be consistent, asymptotically normal, and semiparametrically efficient.","Extensive simulation studies are conducted to show the finite sample performance of the proposed method.","The results imply that the unlabeled data not only helps to identify the intercept but also improves the estimation efficiency of the slope parameter.","Meanwhile, the marginal case proportion can be estimated accurately by the proposed method."],"url":"http://arxiv.org/abs/2402.15365v1","category":"stat.ML"}
{"created":"2024-02-23 10:34:07","title":"Magnons and fundamental magnetic interactions in a ferromagnetic monolayer: The case of Ni monolayer","abstract":"The experimental investigations of the magnetic interactions in an atomically thin magnetic layer are essential to understand the physics of low-dimensional magnets. The full spectrum of collective magnetic excitations (magnons) would provide an access to these fundamental interactions on the atomic scale. Here in order to be able to excite the magnons by means of spin-polarized electrons we couple a Ni monolayer to one and two atomic layers of Co and probe the full experimental magnon dispersion relation up to the Brillouin zone boundary. Comparing to the results of ab initio calculations we quantify the complex pattern of the magnetic exchange interaction in the Ni monolayer. We show that although the magnons in this system are rather stiff, the Heisenberg exchange coupling between the Ni spins is weak. We unravel the origin of the observed large magnon stiffness constant being a consequence of the small spin density of the Ni atoms.","sentences":["The experimental investigations of the magnetic interactions in an atomically thin magnetic layer are essential to understand the physics of low-dimensional magnets.","The full spectrum of collective magnetic excitations (magnons) would provide an access to these fundamental interactions on the atomic scale.","Here in order to be able to excite the magnons by means of spin-polarized electrons we couple a Ni monolayer to one and two atomic layers of Co and probe the full experimental magnon dispersion relation up to the Brillouin zone boundary.","Comparing to the results of ab initio calculations we quantify the complex pattern of the magnetic exchange interaction in the Ni monolayer.","We show that although the magnons in this system are rather stiff, the Heisenberg exchange coupling between the Ni spins is weak.","We unravel the origin of the observed large magnon stiffness constant being a consequence of the small spin density of the Ni atoms."],"url":"http://arxiv.org/abs/2402.15251v1","category":"cond-mat.str-el"}
{"created":"2024-02-23 09:45:49","title":"Systematic effects on a Compton polarimeter at the focus of an X-ray mirror","abstract":"XL-Calibur is a balloon-borne Compton polarimeter for X-rays in the $\\sim$15-80 keV range. Using an X-ray mirror with a 12 m focal length for collecting photons onto a beryllium scattering rod surrounded by CZT detectors, a minimum-detectable polarization as low as $\\sim$3% is expected during a 24-hour on-target observation of a 1 Crab source at 45$^{\\circ}$ elevation. Systematic effects alter the reconstructed polarization as the mirror focal spot moves across the beryllium scatterer, due to pointing offsets, mechanical misalignment or deformation of the carbon-fiber truss supporting the mirror and the polarimeter. Unaddressed, this can give rise to a spurious polarization signal for an unpolarized flux, or a change in reconstructed polarization fraction and angle for a polarized flux. Using bench-marked Monte-Carlo simulations and an accurate mirror point-spread function characterized at synchrotron beam-lines, systematic effects are quantified, and mitigation strategies discussed. By recalculating the scattering site for a shifted beam, systematic errors can be reduced from several tens of percent to the few-percent level for any shift within the scattering element. The treatment of these systematic effects will be important for any polarimetric instrument where a focused X-ray beam is impinging on a scattering element surrounded by counting detectors.","sentences":["XL-Calibur is a balloon-borne Compton polarimeter for X-rays in the $\\sim$15-80 keV range.","Using an X-ray mirror with a 12 m focal length for collecting photons onto a beryllium scattering rod surrounded by CZT detectors, a minimum-detectable polarization as low as $\\sim$3% is expected during a 24-hour on-target observation of a 1 Crab source at 45$^{\\circ}$ elevation.","Systematic effects alter the reconstructed polarization as the mirror focal spot moves across the beryllium scatterer, due to pointing offsets, mechanical misalignment or deformation of the carbon-fiber truss supporting the mirror and the polarimeter.","Unaddressed, this can give rise to a spurious polarization signal for an unpolarized flux, or a change in reconstructed polarization fraction and angle for a polarized flux.","Using bench-marked Monte-Carlo simulations and an accurate mirror point-spread function characterized at synchrotron beam-lines, systematic effects are quantified, and mitigation strategies discussed.","By recalculating the scattering site for a shifted beam, systematic errors can be reduced from several tens of percent to the few-percent level for any shift within the scattering element.","The treatment of these systematic effects will be important for any polarimetric instrument where a focused X-ray beam is impinging on a scattering element surrounded by counting detectors."],"url":"http://arxiv.org/abs/2402.15229v1","category":"astro-ph.IM"}
{"created":"2024-02-23 18:56:31","title":"Tensor network simulations for non-orientable surfaces","abstract":"In this study, we explore the geometric construction of the Klein bottle and the real projective plane ($\\mathrm{RP}^2$) within the framework of tensor networks, focusing on the implementation of crosscap and rainbow boundaries. Previous investigations have applied boundary matrix product state (MPS) techniques to study these boundaries. We introduce a novel approach that incorporates such boundaries into the tensor renormalization group (TRG) methodology, facilitated by an efficient representation of the spatial reflection operator. This advancement enables us to compute the crosscap and rainbow free energy terms and the one-point function on $\\mathrm{RP}^2$ with enhanced efficiency and for larger system sizes. Additionally, our method is capable of calculating the partition function under isotropic conditions of space and imaginary time. The versatility of this approach is further underscored by its applicability to constructing other (un)orientable surfaces of higher genus.","sentences":["In this study, we explore the geometric construction of the Klein bottle and the real projective plane ($\\mathrm{RP}^2$) within the framework of tensor networks, focusing on the implementation of crosscap and rainbow boundaries.","Previous investigations have applied boundary matrix product state (MPS) techniques to study these boundaries.","We introduce a novel approach that incorporates such boundaries into the tensor renormalization group (TRG) methodology, facilitated by an efficient representation of the spatial reflection operator.","This advancement enables us to compute the crosscap and rainbow free energy terms and the one-point function on $\\mathrm{RP}^2$ with enhanced efficiency and for larger system sizes.","Additionally, our method is capable of calculating the partition function under isotropic conditions of space and imaginary time.","The versatility of this approach is further underscored by its applicability to constructing other (un)orientable surfaces of higher genus."],"url":"http://arxiv.org/abs/2402.15507v1","category":"cond-mat.str-el"}
{"created":"2024-02-23 18:54:32","title":"Unique continuation for a gradient inequality with $L^n$ potential","abstract":"We establish a unique continuation property for solutions of the differential inequality $|\\nabla u|\\leq V|u|$, where $V$ is locally $L^n$ integrable on a domain in $\\mathbb R^n$. A stronger uniqueness result is obtained if in addition the solutions are locally Lipschitz. One application is a finite order vanishing property in the $L^2$ sense for the exponential of $W^{1,n}$ functions. We further discuss related results for the Cauchy-Riemann operator $\\bar\\partial$ and characterize the vanishing order for smooth extension of holomorphic functions across the boundary.","sentences":["We establish a unique continuation property for solutions of the differential inequality $|\\nabla u|\\leq V|u|$, where $V$ is locally $L^n$ integrable on a domain in $\\mathbb R^n$. A stronger uniqueness result is obtained if in addition the solutions are locally Lipschitz.","One application is a finite order vanishing property in the $L^2$ sense for the exponential of $W^{1,n}$ functions.","We further discuss related results for the Cauchy-Riemann operator $\\bar\\partial$ and characterize the vanishing order for smooth extension of holomorphic functions across the boundary."],"url":"http://arxiv.org/abs/2402.15503v1","category":"math.AP"}
{"created":"2024-02-23 18:31:02","title":"Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage","abstract":"Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges. As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without human intervention, and customized for each mechanical structure without the need for complex baseline models. We present a novel \"deploy-and-forget\" approach for automated detection and localization of damages in structures. It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed autoencoder. Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state's response characteristics. After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen damage. Results from numerical simulations and experiments indicate that incorporating the mechanical characteristics into the variational autoencoder allows for up to 35\\% earlier detection and localization of damage over a standard autoencoder. Our approach holds substantial promise for a significant reduction in human intervention and inspection costs and enables proactive and preventive maintenance strategies, thus extending the lifespan, reliability, and sustainability of civil infrastructures.","sentences":["Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges.","As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without human intervention, and customized for each mechanical structure without the need for complex baseline models.","We present a novel \"deploy-and-forget\" approach for automated detection and localization of damages in structures.","It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed autoencoder.","Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state's response characteristics.","After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen damage.","Results from numerical simulations and experiments indicate that incorporating the mechanical characteristics into the variational autoencoder allows for up to 35\\% earlier detection and localization of damage over a standard autoencoder.","Our approach holds substantial promise for a significant reduction in human intervention and inspection costs and enables proactive and preventive maintenance strategies, thus extending the lifespan, reliability, and sustainability of civil infrastructures."],"url":"http://arxiv.org/abs/2402.15492v1","category":"cs.LG"}
{"created":"2024-02-23 18:24:52","title":"System-environment quantum information flow","abstract":"The characterization of the dynamics of a quantum system that interacts with an environment reveals how quantum resources are transformed or consumed during the action of a physical process. In some scenarios, such consumption is reversed due to an environment-induced back-action that causes the return of the information to the system. This phenomenon can be related to existence of non-Markovian mechanisms in the environment and such transformation of resources can be useful for quantum information applications. Thus, understanding the details of the system-environment information dynamics, i.e., the transference of quantum resources is of key importance to design noise-resilient quantum technologies. In this work, we show how a quantum resource, named quantum coherence, propagates from the main system to an environment, using as model a single qubit coupled to two linear chains of qubits, and also the information dynamics among the environment qubits. In this way, we characterize the propagation of information leaving the main qubit and going through the environment, as well as its return from the environment to the main system. Finally, we connect the conditions for the emergence of this dynamics to the existence of quantum Darwinism.","sentences":["The characterization of the dynamics of a quantum system that interacts with an environment reveals how quantum resources are transformed or consumed during the action of a physical process.","In some scenarios, such consumption is reversed due to an environment-induced back-action that causes the return of the information to the system.","This phenomenon can be related to existence of non-Markovian mechanisms in the environment and such transformation of resources can be useful for quantum information applications.","Thus, understanding the details of the system-environment information dynamics, i.e., the transference of quantum resources is of key importance to design noise-resilient quantum technologies.","In this work, we show how a quantum resource, named quantum coherence, propagates from the main system to an environment, using as model a single qubit coupled to two linear chains of qubits, and also the information dynamics among the environment qubits.","In this way, we characterize the propagation of information leaving the main qubit and going through the environment, as well as its return from the environment to the main system.","Finally, we connect the conditions for the emergence of this dynamics to the existence of quantum Darwinism."],"url":"http://arxiv.org/abs/2402.15483v1","category":"quant-ph"}
{"created":"2024-02-23 18:15:37","title":"Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks","abstract":"Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function. This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep convolutional neural networks (CNNs). Retinotopic mapping was integrated into the inputs of standard off-the-shelf convolutional neural networks (CNNs), which were then retrained on the ImageNet task. As expected, the logarithmic-polar mapping improved the network's ability to handle arbitrary image zooms and rotations, particularly for isolated objects. Surprisingly, the retinotopically mapped network achieved comparable performance in classification. Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted. This replicates a crucial ability of the human visual system that is absent in typical convolutional neural networks (CNNs). These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes.","sentences":["Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function.","This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep convolutional neural networks (CNNs).","Retinotopic mapping was integrated into the inputs of standard off-the-shelf convolutional neural networks (CNNs), which were then retrained on the ImageNet task.","As expected, the logarithmic-polar mapping improved the network's ability to handle arbitrary image zooms and rotations, particularly for isolated objects.","Surprisingly, the retinotopically mapped network achieved comparable performance in classification.","Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted.","This replicates a crucial ability of the human visual system that is absent in typical convolutional neural networks (CNNs).","These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes."],"url":"http://arxiv.org/abs/2402.15480v1","category":"cs.CV"}
{"created":"2024-02-23 17:50:22","title":"CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration","abstract":"We present CLIPPER+, an algorithm for finding maximal cliques in unweighted graphs for outlier-robust global registration. The registration problem can be formulated as a graph and solved by finding its maximum clique. This formulation leads to extreme robustness to outliers; however, finding the maximum clique is an NP-hard problem, and therefore approximation is required in practice for large-size problems. The performance of an approximation algorithm is evaluated by its computational complexity (the lower the runtime, the better) and solution accuracy (how close the solution is to the maximum clique). Accordingly, the main contribution of CLIPPER+ is outperforming the state-of-the-art in accuracy while maintaining a relatively low runtime. CLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the graph by removing vertices that have a small core number and cannot be a part of the maximum clique. This will result in a smaller graph, on which the maximum clique can be estimated considerably faster. We evaluate the performance of CLIPPER+ on standard graph benchmarks, as well as synthetic and real-world point cloud registration problems. These evaluations demonstrate that CLIPPER+ has the highest accuracy and can register point clouds in scenarios where over $99\\%$ of associations are outliers. Our code and evaluation benchmarks are released at https://github.com/ariarobotics/clipperp.","sentences":["We present CLIPPER+, an algorithm for finding maximal cliques in unweighted graphs for outlier-robust global registration.","The registration problem can be formulated as a graph and solved by finding its maximum clique.","This formulation leads to extreme robustness to outliers; however, finding the maximum clique is an NP-hard problem, and therefore approximation is required in practice for large-size problems.","The performance of an approximation algorithm is evaluated by its computational complexity (the lower the runtime, the better) and solution accuracy (how close the solution is to the maximum clique).","Accordingly, the main contribution of CLIPPER+ is outperforming the state-of-the-art in accuracy while maintaining a relatively low runtime.","CLIPPER+ builds on prior work (CLIPPER [1] and PMC","[2]) and prunes the graph by removing vertices that have a small core number and cannot be a part of the maximum clique.","This will result in a smaller graph, on which the maximum clique can be estimated considerably faster.","We evaluate the performance of CLIPPER+ on standard graph benchmarks, as well as synthetic and real-world point cloud registration problems.","These evaluations demonstrate that CLIPPER+ has the highest accuracy and can register point clouds in scenarios where over $99\\%$ of associations are outliers.","Our code and evaluation benchmarks are released at https://github.com/ariarobotics/clipperp."],"url":"http://arxiv.org/abs/2402.15464v1","category":"cs.RO"}
{"created":"2024-02-23 17:35:20","title":"Magnetoelectric effect in superconductors with d-wave magnetization","abstract":"We report on a study of the interplay between the supercurrent and spin-polarization in a two-dimensional superconducting system in the presence of a d-wave symmetric antiferromagnetic exchange interaction (altermagnetism). It is demonstrated that the supercurrent exhibits a transverse contribution in the presence of both constant and momentum-dependent exchange interactions. We also discuss the analog of the Edelstein effect for such magnetic material, showing that the induced spin polarization is quadratic in the supercurrent and d-wave symmetric.","sentences":["We report on a study of the interplay between the supercurrent and spin-polarization in a two-dimensional superconducting system in the presence of a d-wave symmetric antiferromagnetic exchange interaction (altermagnetism).","It is demonstrated that the supercurrent exhibits a transverse contribution in the presence of both constant and momentum-dependent exchange interactions.","We also discuss the analog of the Edelstein effect for such magnetic material, showing that the induced spin polarization is quadratic in the supercurrent and d-wave symmetric."],"url":"http://arxiv.org/abs/2402.15459v1","category":"cond-mat.supr-con"}
{"created":"2024-02-23 17:32:42","title":"Profile cut-off phenomenon for the ergodic Feller root process","abstract":"The present manuscript is devoted to the study of the convergence to equilibrium as the noise intensity $\\varepsilon>0$ tends to zero for ergodic random systems out of equilibrium of the type \\begin{align*} \\mathrm{d} X^{\\varepsilon}_t(x) = (\\mathfrak{b}-\\mathfrak{a} X^{\\varepsilon}_t(x))\\mathrm{d} t+\\varepsilon \\sqrt{X^{\\varepsilon}_t(x)}\\mathrm{d} B_t, \\quad X^{\\varepsilon}_0(x) = x, \\quad t\\geqslant 0, \\end{align*} where $x\\geqslant 0$, $\\mathfrak{a}>0$ and $\\mathfrak{b}>0$ are constants, and $(B_t)_{t \\geqslant 0}$ is a one dimensional standard Brownian motion. More precisely, we show the strongest notion of asymptotic profile cut-off phenomenon in the total variation distance and in the renormalized Wasserstein distance when $\\varepsilon$ tends to zero with explicit cut-off time, explicit time window, and explicit profile function. In addition, asymptotics of the so-called mixing times are given explicitly.","sentences":["The present manuscript is devoted to the study of the convergence to equilibrium as the noise intensity $\\varepsilon>0$ tends to zero for ergodic random systems out of equilibrium of the type \\begin{align*} \\mathrm{d} X^{\\varepsilon}_t(x) = (\\mathfrak{b}-\\mathfrak{a} X^{\\varepsilon}_t(x))\\mathrm{d} t+\\varepsilon","\\sqrt{X^{\\varepsilon}_t(x)}\\mathrm{d} B_t, \\quad X^{\\varepsilon}_0(x) = x, \\quad t\\geqslant 0, \\end{align*} where $x\\geqslant 0$, $\\mathfrak{a}>0$ and $\\mathfrak{b}>0$ are constants, and $(B_t)_{t \\geqslant 0}$ is a one dimensional standard Brownian motion.","More precisely, we show the strongest notion of asymptotic profile cut-off phenomenon in the total variation distance and in the renormalized Wasserstein distance when $\\varepsilon$ tends to zero with explicit cut-off time, explicit time window, and explicit profile function.","In addition, asymptotics of the so-called mixing times are given explicitly."],"url":"http://arxiv.org/abs/2402.15457v1","category":"math.PR"}
{"created":"2024-02-23 17:29:57","title":"Non-Markovian bath-induced coupling revealed by two-dimensional spectroscopy","abstract":"Problems in the field of open quantum systems often involve an environment that greatly impacts excitation dynamics. Here we show that there can be coherent coupling between different system states of a form that only occurs in a non-Markovian treatment of the bath. Because this involves entangled system-bath states, we demonstrate that there are distinct signatures of this physics in simple absorption spectra and two-dimensional electronic spectroscopy. To do this we introduce a numerical method to simulate optical spectra of non-Markovian open quantum systems. The method employs a process tensor framework to efficiently compute multi-time correlation in a numerically exact way.","sentences":["Problems in the field of open quantum systems often involve an environment that greatly impacts excitation dynamics.","Here we show that there can be coherent coupling between different system states of a form that only occurs in a non-Markovian treatment of the bath.","Because this involves entangled system-bath states, we demonstrate that there are distinct signatures of this physics in simple absorption spectra and two-dimensional electronic spectroscopy.","To do this we introduce a numerical method to simulate optical spectra of non-Markovian open quantum systems.","The method employs a process tensor framework to efficiently compute multi-time correlation in a numerically exact way."],"url":"http://arxiv.org/abs/2402.15454v1","category":"quant-ph"}
{"created":"2024-02-23 17:28:37","title":"I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering","abstract":"Trust in digital systems depends on secure hardware, often assured through Hardware Reverse Engineering (HRE). This work develops methods for investigating human problem-solving processes in HRE, an underexplored yet critical aspect. Since reverse engineers rely heavily on visual information, eye tracking holds promise for studying their cognitive processes. To gain further insights, we additionally employ verbal thought protocols during and immediately after HRE tasks: Concurrent and Retrospective Think Aloud. We evaluate the combination of eye tracking and Think Aloud with 41 participants in an HRE simulation. Eye tracking accurately identifies fixations on individual circuit elements and highlights critical components. Based on two use cases, we demonstrate that eye tracking and Think Aloud can complement each other to improve data quality. Our methodological insights can inform future studies in HRE, a specific setting of human-computer interaction, and in other problem-solving settings involving misleading or missing information.","sentences":["Trust in digital systems depends on secure hardware, often assured through Hardware Reverse Engineering (HRE).","This work develops methods for investigating human problem-solving processes in HRE, an underexplored yet critical aspect.","Since reverse engineers rely heavily on visual information, eye tracking holds promise for studying their cognitive processes.","To gain further insights, we additionally employ verbal thought protocols during and immediately after HRE tasks: Concurrent and Retrospective Think Aloud.","We evaluate the combination of eye tracking and Think Aloud with 41 participants in an HRE simulation.","Eye tracking accurately identifies fixations on individual circuit elements and highlights critical components.","Based on two use cases, we demonstrate that eye tracking and Think Aloud can complement each other to improve data quality.","Our methodological insights can inform future studies in HRE, a specific setting of human-computer interaction, and in other problem-solving settings involving misleading or missing information."],"url":"http://arxiv.org/abs/2402.15452v1","category":"cs.HC"}
{"created":"2024-02-23 16:57:39","title":"Charting Ethical Tensions in Multispecies Technology Research through Beneficiary-Epistemology Space","abstract":"While ethical challenges are widely discussed in HCI, far less is reported about the ethical processes that researchers routinely navigate. We reflect on a multispecies project that negotiated an especially complex ethical approval process. Cat Royale was an artist-led exploration of creating an artwork to engage audiences in exploring trust in autonomous systems. The artwork took the form of a robot that played with three cats. Gaining ethical approval required an extensive dialogue with three Institutional Review Boards (IRBs) covering computer science, veterinary science and animal welfare, raising tensions around the welfare of the cats, perceived benefits and appropriate methods, and reputational risk to the University. To reveal these tensions we introduce beneficiary-epistemology space, that makes explicit who benefits from research (humans or animals) and underlying epistemologies. Positioning projects and IRBs in this space can help clarify tensions and highlight opportunities to recruit additional expertise.","sentences":["While ethical challenges are widely discussed in HCI, far less is reported about the ethical processes that researchers routinely navigate.","We reflect on a multispecies project that negotiated an especially complex ethical approval process.","Cat Royale was an artist-led exploration of creating an artwork to engage audiences in exploring trust in autonomous systems.","The artwork took the form of a robot that played with three cats.","Gaining ethical approval required an extensive dialogue with three Institutional Review Boards (IRBs) covering computer science, veterinary science and animal welfare, raising tensions around the welfare of the cats, perceived benefits and appropriate methods, and reputational risk to the University.","To reveal these tensions we introduce beneficiary-epistemology space, that makes explicit who benefits from research (humans or animals) and underlying epistemologies.","Positioning projects and IRBs in this space can help clarify tensions and highlight opportunities to recruit additional expertise."],"url":"http://arxiv.org/abs/2402.15439v1","category":"cs.HC"}
{"created":"2024-02-23 16:57:22","title":"Abundant sub-micron grains revealed in newly discovered extreme debris discs","abstract":"Extreme debris discs (EDDs) are bright and warm circumstellar dusty structures around main sequence stars. They may represent the outcome of giant collisions occuring in the terrestrial region between large planetesimals or planetary bodies, and thus provide a rare opportunity to peer into the aftermaths of these events. Here, we report on results of a mini-survey we conducted with the aim to increase the number of known EDDs, investigate the presence of solid-state features around 10{\\mu}m in eight EDDs, and classify them into the silica or silicate dominated groups. We identify four new EDDs and derive their fundamental properties. For these, and for four other previously known discs, we study the spectral energy distribution around 10{\\mu}m by means of VLT/VISIR photometry in three narrow-band filters and conclude that all eight objects likely exhibit solid-state emission features from sub-micron grains. We find that four discs probably belong to the silicate dominated subgroup. Considering the age distribution of the entire EDD sample, we find that their incidence begins to decrease only after 300 Myr, suggesting that the earlier common picture that these objects are related to the formation of rocky planets may not be exclusive, and that other processes may be involved for older objects (>100 Myr). Because most of the older EDD systems have wide, eccentric companions, we suggest that binarity may play a role in triggering late giant collisions.","sentences":["Extreme debris discs (EDDs) are bright and warm circumstellar dusty structures around main sequence stars.","They may represent the outcome of giant collisions occuring in the terrestrial region between large planetesimals or planetary bodies, and thus provide a rare opportunity to peer into the aftermaths of these events.","Here, we report on results of a mini-survey we conducted with the aim to increase the number of known EDDs, investigate the presence of solid-state features around 10{\\mu}m in eight EDDs, and classify them into the silica or silicate dominated groups.","We identify four new EDDs and derive their fundamental properties.","For these, and for four other previously known discs, we study the spectral energy distribution around 10{\\mu}m by means of VLT/VISIR photometry in three narrow-band filters and conclude that all eight objects likely exhibit solid-state emission features from sub-micron grains.","We find that four discs probably belong to the silicate dominated subgroup.","Considering the age distribution of the entire EDD sample, we find that their incidence begins to decrease only after 300 Myr, suggesting that the earlier common picture that these objects are related to the formation of rocky planets may not be exclusive, and that other processes may be involved for older objects (>100 Myr).","Because most of the older EDD systems have wide, eccentric companions, we suggest that binarity may play a role in triggering late giant collisions."],"url":"http://arxiv.org/abs/2402.15438v1","category":"astro-ph.EP"}
{"created":"2024-02-23 16:52:04","title":"Decoding the Pulse of Community during Disasters: Resilience Analysis Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation Networks","abstract":"Examining the impact of disasters on life activities of populations is critical for understanding community resilience dynamics, yet it remains insufficiently studied in the existing literature. In this study, we leveraged data from more than 1.2 million anonymized human mobility communications across 30 parishes in Louisiana to construct a temporal network that tracks visitation to places from which we characterized human lifestyle signatures before, during, and after Hurricane Ida in 2021. Utilizing the motif model, we distilled complex human lifestyles into identifiable patterns and clustered them into classes: commute, healthcare, dining out, and youth-oriented lifestyle. We defined two metrics to evaluate disruption and recovery fluctuations in lifestyle patterns during the perturbation period compared to the steady period: 1) frequency (daily number of motifs), and 2) proximity (daily average distance of motifs). The results indicate significant dynamics in lifestyle patterns due to the hurricane, with essential facilities (e.g., healthcare) demonstrating a swift recovery. The study underscores the heterogeneity of locations visited and the necessity of integrating both essential and non-essential facilities into disaster response initiatives. Furthermore, our study reveals sustained changes in lifestyle patterns, highlighting the long-term impact of the hurricane on daily life. These insights demonstrate the significance of examining lifestyle signatures and their fluctuations in evaluating disaster resilience patterns for affected communities. The outcomes of this study are poised to aid emergency managers and public officials to more effectively evaluate and monitor disaster impacts and recovery based on changes in lifestyle patterns in the community.","sentences":["Examining the impact of disasters on life activities of populations is critical for understanding community resilience dynamics, yet it remains insufficiently studied in the existing literature.","In this study, we leveraged data from more than 1.2 million anonymized human mobility communications across 30 parishes in Louisiana to construct a temporal network that tracks visitation to places from which we characterized human lifestyle signatures before, during, and after Hurricane Ida in 2021.","Utilizing the motif model, we distilled complex human lifestyles into identifiable patterns and clustered them into classes: commute, healthcare, dining out, and youth-oriented lifestyle.","We defined two metrics to evaluate disruption and recovery fluctuations in lifestyle patterns during the perturbation period compared to the steady period: 1) frequency (daily number of motifs), and 2) proximity (daily average distance of motifs).","The results indicate significant dynamics in lifestyle patterns due to the hurricane, with essential facilities (e.g., healthcare) demonstrating a swift recovery.","The study underscores the heterogeneity of locations visited and the necessity of integrating both essential and non-essential facilities into disaster response initiatives.","Furthermore, our study reveals sustained changes in lifestyle patterns, highlighting the long-term impact of the hurricane on daily life.","These insights demonstrate the significance of examining lifestyle signatures and their fluctuations in evaluating disaster resilience patterns for affected communities.","The outcomes of this study are poised to aid emergency managers and public officials to more effectively evaluate and monitor disaster impacts and recovery based on changes in lifestyle patterns in the community."],"url":"http://arxiv.org/abs/2402.15434v1","category":"cs.SI"}
{"created":"2024-02-23 16:51:09","title":"Designing Multispecies Worlds for Robots, Cats, and Humans","abstract":"We reflect on the design of a multispecies world centred around a bespoke enclosure in which three cats and a robot arm coexist for six hours a day during a twelve-day installation as part of an artist-led project. In this paper, we present the project's design process, encompassing various interconnected components, including the cats, the robot and its autonomous systems, the custom end-effectors and robot attachments, the diverse roles of the humans-in-the-loop, and the custom-designed enclosure. Subsequently, we provide a detailed account of key moments during the deployment and discuss the design implications for future multispecies systems. Specifically, we argue that designing the technology and its interactions is not sufficient, but that it is equally important to consider the design of the `world' in which the technology operates. Finally, we highlight the necessity of human involvement in areas such as breakdown recovery, animal welfare, and their role as audience.","sentences":["We reflect on the design of a multispecies world centred around a bespoke enclosure in which three cats and a robot arm coexist for six hours a day during a twelve-day installation as part of an artist-led project.","In this paper, we present the project's design process, encompassing various interconnected components, including the cats, the robot and its autonomous systems, the custom end-effectors and robot attachments, the diverse roles of the humans-in-the-loop, and the custom-designed enclosure.","Subsequently, we provide a detailed account of key moments during the deployment and discuss the design implications for future multispecies systems.","Specifically, we argue that designing the technology and its interactions is not sufficient, but that it is equally important to consider the design of the `world' in which the technology operates.","Finally, we highlight the necessity of human involvement in areas such as breakdown recovery, animal welfare, and their role as audience."],"url":"http://arxiv.org/abs/2402.15431v1","category":"cs.HC"}
{"created":"2024-02-23 16:34:00","title":"Specialising Neural-network Quantum States for the Bose Hubbard Model","abstract":"Projected variational wavefunctions such as the Gutzwiller, many-body correlator and Jastrow ansatzes have provided crucial insight into the nature of superfluid-Mott insulator transition in the Bose Hubbard model (BHM) in two or more spatial dimensions. However, these ansatzes have no obvious tractable and systematic way of being improved. A promising alternative is to use Neural-network quantum states (NQS) based on Restricted Boltzmann Machines (RBMs). With binary visible and hidden units NQS have proven to be a highly effective at describing quantum states of interacting spin-1/2 lattice systems. The application of NQS to bosonic systems has so far been based on one-hot encoding from machine learning where the multi-valued site occupation is distributed across several binary-valued visible units of an RBM. Compared to spin-1/2 systems one-hot encoding greatly increases the number of variational parameters whilst also making their physical interpretation opaque. Here we revisit the construction of NQS for bosonic systems by reformulating a one-hot encoded RBM into a correlation operator applied to a reference state, analogous to the structure of the projected variational ansatzes. In this form we then propose a number of specialisations of the RBM motivated by the physics of the BHM and the ability to capture exactly the projected variational ansatzes. We analyse in detail the variational performance of these new RBM variants for a 10 x 10 BHM, using both a standard Bose condensate state and a pre-optimised Jastrow + many-body correlator state as the reference state of the calculation. Several of our new ansatzes give robust results as nearly good as one-hot encoding across the regimes of the BHM, but at a substantially reduced cost. Such specialised NQS are thus primed tackle bosonic lattice problems beyond the accuracy of classic variational wavefunctions.","sentences":["Projected variational wavefunctions such as the Gutzwiller, many-body correlator and Jastrow ansatzes have provided crucial insight into the nature of superfluid-Mott insulator transition in the Bose Hubbard model (BHM) in two or more spatial dimensions.","However, these ansatzes have no obvious tractable and systematic way of being improved.","A promising alternative is to use Neural-network quantum states (NQS) based on Restricted Boltzmann Machines (RBMs).","With binary visible and hidden units NQS have proven to be a highly effective at describing quantum states of interacting spin-1/2 lattice systems.","The application of NQS to bosonic systems has so far been based on one-hot encoding from machine learning where the multi-valued site occupation is distributed across several binary-valued visible units of an RBM.","Compared to spin-1/2 systems one-hot encoding greatly increases the number of variational parameters whilst also making their physical interpretation opaque.","Here we revisit the construction of NQS for bosonic systems by reformulating a one-hot encoded RBM into a correlation operator applied to a reference state, analogous to the structure of the projected variational ansatzes.","In this form we then propose a number of specialisations of the RBM motivated by the physics of the BHM and the ability to capture exactly the projected variational ansatzes.","We analyse in detail the variational performance of these new RBM variants for a 10 x 10 BHM, using both a standard Bose condensate state and a pre-optimised Jastrow + many-body correlator state as the reference state of the calculation.","Several of our new ansatzes give robust results as nearly good as one-hot encoding across the regimes of the BHM, but at a substantially reduced cost.","Such specialised NQS are thus primed tackle bosonic lattice problems beyond the accuracy of classic variational wavefunctions."],"url":"http://arxiv.org/abs/2402.15424v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-23 16:33:49","title":"Performance Analysis of Systems with Coupled and Decoupled RISs","abstract":"We analyze and compare different methods for handling the mutual coupling in RIS-aided communication systems. A new mutual coupling aware algorithm is derived where the reactance of each element is updated successively with a closed-form solution. In comparison to existing element-wise methods, this approach leads to a considerably reduced computational complexity. Furthermore, we introduce decoupling networks for the RIS array as a potential solution for handling mutual coupling. With these networks, the system model reduces to the same structure as when no mutual coupling were present. Including decoupling networks, we can optimize the channel gain of a RIS-aided SISO system in closed-form which allows to analyze the scenario under mutual coupling analytically and to draw connections to the conventional transmit array gain. In particular, a super-quadratic channel gain can be achieved which scales as N^4 where N is the number of RIS elements.","sentences":["We analyze and compare different methods for handling the mutual coupling in RIS-aided communication systems.","A new mutual coupling aware algorithm is derived where the reactance of each element is updated successively with a closed-form solution.","In comparison to existing element-wise methods, this approach leads to a considerably reduced computational complexity.","Furthermore, we introduce decoupling networks for the RIS array as a potential solution for handling mutual coupling.","With these networks, the system model reduces to the same structure as when no mutual coupling were present.","Including decoupling networks, we can optimize the channel gain of a RIS-aided SISO system in closed-form which allows to analyze the scenario under mutual coupling analytically and to draw connections to the conventional transmit array gain.","In particular, a super-quadratic channel gain can be achieved which scales as N^4 where N is the number of RIS elements."],"url":"http://arxiv.org/abs/2402.15423v1","category":"cs.IT"}
{"created":"2024-02-23 16:29:03","title":"Uncertainty Quantification in Atomistic Simulations of Silicon using Interatomic Potentials","abstract":"Atomistic simulations often rely on interatomic potentials to access greater time- and length- scales than those accessible to first principles methods such as density functional theory (DFT). However, since a parameterised potential typically cannot reproduce the true potential energy surface of a given system, we should expect a decrease in accuracy and increase in error in quantities of interest calculated from simulations. Quantifying the uncertainty on the outputs of atomistic simulations is thus an important, necessary step so that there is confidence in results and available metrics to explore improvements in said simulations. Here, we address this research question by forming ensembles of Atomic Cluster Expansion (ACE) potentials, and using Conformal Prediction with DFT training data to provide meaningful, calibrated error bars on several quantities of interest for silicon: the bulk modulus, elastic constants, relaxed vacancy formation energy, and the vacancy migration barrier. We evaluate the effects on uncertainty bounds using a range of different potentials and training sets.","sentences":["Atomistic simulations often rely on interatomic potentials to access greater time- and length- scales than those accessible to first principles methods such as density functional theory (DFT).","However, since a parameterised potential typically cannot reproduce the true potential energy surface of a given system, we should expect a decrease in accuracy and increase in error in quantities of interest calculated from simulations.","Quantifying the uncertainty on the outputs of atomistic simulations is thus an important, necessary step so that there is confidence in results and available metrics to explore improvements in said simulations.","Here, we address this research question by forming ensembles of Atomic Cluster Expansion (ACE) potentials, and using Conformal Prediction with DFT training data to provide meaningful, calibrated error bars on several quantities of interest for silicon: the bulk modulus, elastic constants, relaxed vacancy formation energy, and the vacancy migration barrier.","We evaluate the effects on uncertainty bounds using a range of different potentials and training sets."],"url":"http://arxiv.org/abs/2402.15419v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 16:26:01","title":"The Impact of LoRA on the Emergence of Clusters in Transformers","abstract":"In this paper, we employ the mathematical framework on Transformers developed by \\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \\cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models.","sentences":["In this paper, we employ the mathematical framework on Transformers developed by \\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters.","Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences.","This work contributes to the fine-tuning field through practical applications to the LoRA algorithm \\cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced Transformer models."],"url":"http://arxiv.org/abs/2402.15415v1","category":"cs.LG"}
{"created":"2024-02-23 16:05:51","title":"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior","abstract":"We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability guided by task-level rewards. We leverage the foundation model CLIP for object matching, policy learning and self-termination. A series of experiments indicate that GSP can conduct unknown object rearrangement with higher completion rate and less steps.","sentences":["We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image.","Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules.","However, they are sensitive to perception error, and pay less attention to task-level performance.","In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise.","We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality.","We propose GSP, a dual-loop system with the decoupled structure as prior.","For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place.","For the outer loop, we learn a grasp policy aware of object matching and grasp capability guided by task-level rewards.","We leverage the foundation model CLIP for object matching, policy learning and self-termination.","A series of experiments indicate that GSP can conduct unknown object rearrangement with higher completion rate and less steps."],"url":"http://arxiv.org/abs/2402.15402v1","category":"cs.RO"}
{"created":"2024-02-23 15:59:38","title":"Phonon softening and atomic modulations in EuAl$_4$","abstract":"EuAl$_4$ is a rare earth intermetallic in which competing itinerant and/or indirect exchange mechanisms give rise to a complex magnetic phase diagram, including a centrosymmetric skyrmion lattice. These phenomena arise not in the tetragonal parent structure but in the presence of a charge density wave (CDW), which lowers the crystal symmetry and renormalizes the electronic structure. Microscopic knowledge of the corresponding atomic modulations and their driving mechanism is a prerequisite for a deeper understanding of the resulting equilibrium of electronic correlations and how it might be manipulated. Here, we use synchrotron single-crystal X-ray diffraction, inelastic X-ray scattering, and lattice dynamics calculations to clarify the origin of the CDW in EuAl$_4$. We observe a broad softening of a transverse acoustic phonon mode that sets in well above room temperature and, at $T_\\mathrm{CDW}=142$ K, freezes out in an atomic displacement mode described by the superspace group $Immm(00\\gamma)s00$. In the context of previous work, our observation is a clear confirmation that the CDW in EuAl$_4$ is driven by electron-phonon coupling. This result is relevant for a wider family of BaAl$_4$ and ThCr$_2$Si$_2$-type rare-earth intermetallics known to combine CDW instabilities and complex magnetism.","sentences":["EuAl$_4$ is a rare earth intermetallic in which competing itinerant and/or indirect exchange mechanisms give rise to a complex magnetic phase diagram, including a centrosymmetric skyrmion lattice.","These phenomena arise not in the tetragonal parent structure but in the presence of a charge density wave (CDW), which lowers the crystal symmetry and renormalizes the electronic structure.","Microscopic knowledge of the corresponding atomic modulations and their driving mechanism is a prerequisite for a deeper understanding of the resulting equilibrium of electronic correlations and how it might be manipulated.","Here, we use synchrotron single-crystal X-ray diffraction, inelastic X-ray scattering, and lattice dynamics calculations to clarify the origin of the CDW in EuAl$_4$. We observe a broad softening of a transverse acoustic phonon mode that sets in well above room temperature and, at $T_\\mathrm{CDW}=142$ K, freezes out in an atomic displacement mode described by the superspace group $Immm(00\\gamma)s00$. In the context of previous work, our observation is a clear confirmation that the CDW in EuAl$_4$ is driven by electron-phonon coupling.","This result is relevant for a wider family of BaAl$_4$ and ThCr$_2$Si$_2$-type rare-earth intermetallics known to combine CDW instabilities and complex magnetism."],"url":"http://arxiv.org/abs/2402.15397v1","category":"cond-mat.str-el"}
{"created":"2024-02-23 15:49:46","title":"Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms","abstract":"Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the introduction an original learning framework that copes with the intrinsic difficulty of the setting, for which the data coverage is not under control. Then, we propose two computationally and statistically efficient algorithms, IRLO and PIRLO, for addressing the problem. In particular, the latter adopts a specific form of pessimism to enforce the novel desirable property of inclusion monotonicity of the delivered feasible set. With this work, we aim to provide a panorama of the challenges of the offline IRL problem and how they can be fruitfully addressed.","sentences":["Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior.","It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations.","For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward.","However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will.","This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario.","In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation.","This requires the introduction an original learning framework that copes with the intrinsic difficulty of the setting, for which the data coverage is not under control.","Then, we propose two computationally and statistically efficient algorithms, IRLO and PIRLO, for addressing the problem.","In particular, the latter adopts a specific form of pessimism to enforce the novel desirable property of inclusion monotonicity of the delivered feasible set.","With this work, we aim to provide a panorama of the challenges of the offline IRL problem and how they can be fruitfully addressed."],"url":"http://arxiv.org/abs/2402.15392v1","category":"cs.LG"}
{"created":"2024-02-23 15:30:13","title":"A simple model for predicting tropical cyclone minimum central pressure from intensity and size","abstract":"Minimum central pressure ($P_{min}$) is an integrated measure of the tropical cyclone wind field and is known to be a useful indicator of storm damage potential. A simple model that predicts $P_{min}$ from routinely-estimated quantities, including storm size, would be of great value. Here we present a simple linear empirical model for predicting $P_{min}$ from maximum wind speed, the radius of 34-knot winds ($R_{34kt}$), storm-center latitude, and the environmental pressure. An empirical model for the pressure deficit is first developed that takes as predictors specific combinations of these quantities that are derived directly from theory, based on gradient wind balance and a modified-Rankine-type wind profile known to capture storm structure inside of $R_{34kt}$. Model coefficients are estimated using data from the southwestern North Atlantic and eastern North Pacific from 2004--2022 using aircraft-based estimates of $P_{min}$, Extended Best Track data, and estimates of environmental pressure from Global Forecast System (GFS) analyses. The model has near-zero conditional bias even for low $P_{min}$, explaining 94.4\\% of the variance. Performance is superior to a variety of other model formulations, including a standard wind-pressure model that does not account for storm size or latitude (89.4\\% variance explained). Model performance is also strong when applied to high-latitude data and data near coastlines. Finally, the model is shown to perform comparably well in an operations-like setting based solely on routinely-estimated variables, including the pressure of the outermost closed isobar. Case study applications to five impactful historical storms are discussed. Overall, the model offers a simple and fast prediction for $P_{min}$ for practical use in operations and research.","sentences":["Minimum central pressure ($P_{min}$) is an integrated measure of the tropical cyclone wind field and is known to be a useful indicator of storm damage potential.","A simple model that predicts $P_{min}$ from routinely-estimated quantities, including storm size, would be of great value.","Here we present a simple linear empirical model for predicting $P_{min}$ from maximum wind speed, the radius of 34-knot winds ($R_{34kt}$), storm-center latitude, and the environmental pressure.","An empirical model for the pressure deficit is first developed that takes as predictors specific combinations of these quantities that are derived directly from theory, based on gradient wind balance and a modified-Rankine-type wind profile known to capture storm structure inside of $R_{34kt}$. Model coefficients are estimated using data from the southwestern North Atlantic and eastern North Pacific from 2004--2022 using aircraft-based estimates of $P_{min}$, Extended Best Track data, and estimates of environmental pressure from Global Forecast System (GFS) analyses.","The model has near-zero conditional bias even for low $P_{min}$, explaining 94.4\\% of the variance.","Performance is superior to a variety of other model formulations, including a standard wind-pressure model that does not account for storm size or latitude (89.4\\% variance explained).","Model performance is also strong when applied to high-latitude data and data near coastlines.","Finally, the model is shown to perform comparably well in an operations-like setting based solely on routinely-estimated variables, including the pressure of the outermost closed isobar.","Case study applications to five impactful historical storms are discussed.","Overall, the model offers a simple and fast prediction for $P_{min}$ for practical use in operations and research."],"url":"http://arxiv.org/abs/2402.15383v1","category":"physics.geo-ph"}
{"created":"2024-02-23 15:28:10","title":"Optimisation-based alignment of wideband integrated superconducting spectrometers for sub-mm astronomy","abstract":"Integrated superconducting spectrometers (ISSs) for wideband sub-mm astronomy utilise quasi-optical systems for coupling radiation from the telescope to the instrument. Misalignment in these systems is detrimental to the system performance. The common method of using an optical laser to align the quasi-optical components requires accurate alignment of the laser to the sub-mm beam coming from the instrument, which is not always guaranteed to a sufficient accuracy. We develop an alignment strategy for wideband ISSs directly utilising the sub-mm beam of the wideband ISS. The strategy should be applicable in both telescope and laboratory environments. Moreover, the strategy should deliver similar quality of the alignment across the spectral range of the wideband ISS. We measure misalignment in a quasi-optical system operating at sub-mm wavelengths using a novel phase and amplitude measurement scheme, capable of simultaneously measuring the complex beam patterns of a direct-detecting ISS across a harmonic range of frequencies. The direct detection nature of the MKID detectors in our device-under-test, DESHIMA 2.0, necessitates the use of this measurement scheme. Using geometrical optics, the measured misalignment, a mechanical hexapod, and an optimisation algorithm, we follow a numerical approach to optimise the positioning of corrective optics with respect to a given cost function. Laboratory measurements of the complex beam patterns are taken across a harmonic range between 205 and 391 GHz and simulated through a model of the ASTE telescope in order to assess the performance of the optimisation at the ASTE telescope. Laboratory measurements show that the optimised optical setup corrects for tilts and offsets of the sub-mm beam. Moreover, we find that the simulated telescope aperture efficiency is increased across the frequency range of the ISS after the optimisation.","sentences":["Integrated superconducting spectrometers (ISSs) for wideband sub-mm astronomy utilise quasi-optical systems for coupling radiation from the telescope to the instrument.","Misalignment in these systems is detrimental to the system performance.","The common method of using an optical laser to align the quasi-optical components requires accurate alignment of the laser to the sub-mm beam coming from the instrument, which is not always guaranteed to a sufficient accuracy.","We develop an alignment strategy for wideband ISSs directly utilising the sub-mm beam of the wideband ISS.","The strategy should be applicable in both telescope and laboratory environments.","Moreover, the strategy should deliver similar quality of the alignment across the spectral range of the wideband ISS.","We measure misalignment in a quasi-optical system operating at sub-mm wavelengths using a novel phase and amplitude measurement scheme, capable of simultaneously measuring the complex beam patterns of a direct-detecting ISS across a harmonic range of frequencies.","The direct detection nature of the MKID detectors in our device-under-test, DESHIMA 2.0, necessitates the use of this measurement scheme.","Using geometrical optics, the measured misalignment, a mechanical hexapod, and an optimisation algorithm, we follow a numerical approach to optimise the positioning of corrective optics with respect to a given cost function.","Laboratory measurements of the complex beam patterns are taken across a harmonic range between 205 and 391 GHz and simulated through a model of the ASTE telescope in order to assess the performance of the optimisation at the ASTE telescope.","Laboratory measurements show that the optimised optical setup corrects for tilts and offsets of the sub-mm beam.","Moreover, we find that the simulated telescope aperture efficiency is increased across the frequency range of the ISS after the optimisation."],"url":"http://arxiv.org/abs/2402.15381v1","category":"astro-ph.IM"}
{"created":"2024-02-23 15:21:38","title":"Probing critical phenomena in open quantum systems using atom arrays","abstract":"At continuous phase transitions, quantum many-body systems exhibit scale-invariance and complex, emergent universal behavior. Most strikingly, at a quantum critical point, correlations decay as a power law, with exponents determined by a set of universal scaling dimensions. Experimentally probing such power-law correlations is extremely challenging, owing to the complex interplay between decoherence, the vanishing energy gap, and boundary effects. Here, we employ a Rydberg quantum simulator to adiabatically prepare critical ground states of both a one-dimensional ring and a two-dimensional square lattice. By accounting for and tuning the openness of our quantum system, which is well-captured by the introduction of a single phenomenological length scale, we are able to directly observe power-law correlations and extract the corresponding scaling dimensions. Moreover, in two dimensions, we observe a decoupling between phase transitions in the bulk and on the boundary, allowing us to identify two distinct boundary universality classes. Our work demonstrates that direct adiabatic preparation of critical states in quantum simulators can complement recent approaches to studying quantum criticality using the Kibble-Zurek mechanism or digital quantum circuits.","sentences":["At continuous phase transitions, quantum many-body systems exhibit scale-invariance and complex, emergent universal behavior.","Most strikingly, at a quantum critical point, correlations decay as a power law, with exponents determined by a set of universal scaling dimensions.","Experimentally probing such power-law correlations is extremely challenging, owing to the complex interplay between decoherence, the vanishing energy gap, and boundary effects.","Here, we employ a Rydberg quantum simulator to adiabatically prepare critical ground states of both a one-dimensional ring and a two-dimensional square lattice.","By accounting for and tuning the openness of our quantum system, which is well-captured by the introduction of a single phenomenological length scale, we are able to directly observe power-law correlations and extract the corresponding scaling dimensions.","Moreover, in two dimensions, we observe a decoupling between phase transitions in the bulk and on the boundary, allowing us to identify two distinct boundary universality classes.","Our work demonstrates that direct adiabatic preparation of critical states in quantum simulators can complement recent approaches to studying quantum criticality using the Kibble-Zurek mechanism or digital quantum circuits."],"url":"http://arxiv.org/abs/2402.15376v1","category":"quant-ph"}
{"created":"2024-02-23 15:09:19","title":"Non-adiabatic quantum dynamics with fermionic subspace-expansion algorithms on quantum computers","abstract":"We introduce a novel computational framework for excited-states molecular quantum dynamics simulations driven by quantum computing-based electronic-structure calculations. This framework leverages the fewest-switches surface-hopping method for simulating the nuclear dynamics, and calculates the required excited-state transition properties with different flavors of the quantum subspace expansion and quantum equation-of-motion algorithms. We apply our method to simulate the collision reaction between a hydrogen atom and a hydrogen molecule. For this system, we critically compare the accuracy and efficiency of different quantum subspace expansion and equation-of-motion algorithms and show that only methods that can capture both weak and strong electron correlation effects can properly describe the non-adiabatic effects that tune the reactive event.","sentences":["We introduce a novel computational framework for excited-states molecular quantum dynamics simulations driven by quantum computing-based electronic-structure calculations.","This framework leverages the fewest-switches surface-hopping method for simulating the nuclear dynamics, and calculates the required excited-state transition properties with different flavors of the quantum subspace expansion and quantum equation-of-motion algorithms.","We apply our method to simulate the collision reaction between a hydrogen atom and a hydrogen molecule.","For this system, we critically compare the accuracy and efficiency of different quantum subspace expansion and equation-of-motion algorithms and show that only methods that can capture both weak and strong electron correlation effects can properly describe the non-adiabatic effects that tune the reactive event."],"url":"http://arxiv.org/abs/2402.15371v1","category":"quant-ph"}
{"created":"2024-02-23 15:04:13","title":"Minimal stretch factors of orientation-reversing fully-punctured pseudo-Anosov maps","abstract":"We show that the stretch factor $\\lambda(f)$ of an orientation-reversing fully-punctured pseudo-Anosov map $f$ on a finite-type orientable surface $S$, with $-\\chi(S) \\geq 4$ and having at least two puncture orbits, satisfies the inequality $\\lambda(f)^{-\\chi(S)} \\geq \\sigma^2$, where $\\sigma=1+\\sqrt{2}$ is the silver ratio. We provide examples showing that this bound is asymptotically sharp. This extends previous results of Hironaka and the third author to orientation-reversing maps.","sentences":["We show that the stretch factor $\\lambda(f)$ of an orientation-reversing fully-punctured pseudo-Anosov map $f$ on a finite-type orientable surface $S$, with $-\\chi(S) \\geq 4$ and having at least two puncture orbits, satisfies the inequality $\\lambda(f)^{-\\chi(S)} \\geq \\sigma^2$, where $\\sigma=1+\\sqrt{2}$ is the silver ratio.","We provide examples showing that this bound is asymptotically sharp.","This extends previous results of Hironaka and the third author to orientation-reversing maps."],"url":"http://arxiv.org/abs/2402.15369v1","category":"math.GT"}
{"created":"2024-02-23 14:53:39","title":"Essential dimensions of isogenies","abstract":"We give a formula for the essential dimensions of isogenies $A\\to A'$ of complex abelian varieties when the degree of the isogeny is coprime to $(\\dim A)!$. As a corollary, we show that the $p$-multiplication map (where $p$ is a prime) on an abelian variety is incompressible when $p>\\dim A$ or when $\\dim A\\le 4$. This confirms a conjecture of Brosnan for most primes.","sentences":["We give a formula for the essential dimensions of isogenies $A\\to A'$ of complex abelian varieties when the degree of the isogeny is coprime to $(\\dim A)!$. As a corollary, we show that the $p$-multiplication map (where $p$ is a prime) on an abelian variety is incompressible when $p>\\dim A$ or when $\\dim A\\le 4$.","This confirms a conjecture of Brosnan for most primes."],"url":"http://arxiv.org/abs/2402.15362v1","category":"math.AG"}
{"created":"2024-02-23 14:52:05","title":"Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations","abstract":"We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a novel approach for modeling a stream of spatiotemporally distributed, sparse, high-dimensional categorical observations. The proposed approach efficiently learns global and local patterns in spatiotemporal data, allowing for fast inference and querying with a bounded time complexity. Using a high-resolution data series of plankton images classified with a neural network, we demonstrate the ability of the approach to make more accurate predictions compared to a Variational Gaussian Process (VGP), and to learn a predictive distribution of observations from streaming categorical data. S-GDRFs open the door to enabling efficient informative path planning over high-dimensional categorical observations, which until now has not been feasible.","sentences":["We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a novel approach for modeling a stream of spatiotemporally distributed, sparse, high-dimensional categorical observations.","The proposed approach efficiently learns global and local patterns in spatiotemporal data, allowing for fast inference and querying with a bounded time complexity.","Using a high-resolution data series of plankton images classified with a neural network, we demonstrate the ability of the approach to make more accurate predictions compared to a Variational Gaussian Process (VGP), and to learn a predictive distribution of observations from streaming categorical data.","S-GDRFs open the door to enabling efficient informative path planning over high-dimensional categorical observations, which until now has not been feasible."],"url":"http://arxiv.org/abs/2402.15359v1","category":"cs.RO"}
{"created":"2024-02-23 14:41:35","title":"Rapid Bayesian identification of sparse nonlinear dynamics from scarce and noisy data","abstract":"We propose a fast probabilistic framework for identifying differential equations governing the dynamics of observed data. We recast the SINDy method within a Bayesian framework and use Gaussian approximations for the prior and likelihood to speed up computation. The resulting method, Bayesian-SINDy, not only quantifies uncertainty in the parameters estimated but also is more robust when learning the correct model from limited and noisy data. Using both synthetic and real-life examples such as Lynx-Hare population dynamics, we demonstrate the effectiveness of the new framework in learning correct model equations and compare its computational and data efficiency with existing methods. Because Bayesian-SINDy can quickly assimilate data and is robust against noise, it is particularly suitable for biological data and real-time system identification in control. Its probabilistic framework also enables the calculation of information entropy, laying the foundation for an active learning strategy.","sentences":["We propose a fast probabilistic framework for identifying differential equations governing the dynamics of observed data.","We recast the SINDy method within a Bayesian framework and use Gaussian approximations for the prior and likelihood to speed up computation.","The resulting method, Bayesian-SINDy, not only quantifies uncertainty in the parameters estimated but also is more robust when learning the correct model from limited and noisy data.","Using both synthetic and real-life examples such as Lynx-Hare population dynamics, we demonstrate the effectiveness of the new framework in learning correct model equations and compare its computational and data efficiency with existing methods.","Because Bayesian-SINDy can quickly assimilate data and is robust against noise, it is particularly suitable for biological data and real-time system identification in control.","Its probabilistic framework also enables the calculation of information entropy, laying the foundation for an active learning strategy."],"url":"http://arxiv.org/abs/2402.15357v1","category":"stat.ME"}
{"created":"2024-02-23 14:39:28","title":"Background Denoising for Ptychography via Wigner Distribution Deconvolution","abstract":"Ptychography is a computational imaging technique that aims to reconstruct the object of interest from a set of diffraction patterns. Each of these is obtained by a localized illumination of the object, which is shifted after each illumination to cover its whole domain. As in the resulting measurements the phase information is lost, ptychography gives rise to solving a phase retrieval problem. In this work, we consider ptychographic measurements corrupted with background noise, a type of additive noise that is independent of the shift, i.e., it is the same for all diffraction patterns. Two algorithms are provided, for arbitrary objects and for so-called phase objects that do not absorb the light but only scatter it. For the second type, a uniqueness of reconstruction is established for almost every object. Our approach is based on the Wigner Distribution Deconvolution, which lifts the object to a higher-dimensional matrix space where the recovery can be reformulated as a linear problem. Background noise only affects a few equations of the linear system that are therefore discarded. The lost information is then restored using redundancy in the higher-dimensional space.   Keywords: phase retrieval, ptychography, background noise, Wigner Distribution Deconvolution, uniqueness of reconstruction.","sentences":["Ptychography is a computational imaging technique that aims to reconstruct the object of interest from a set of diffraction patterns.","Each of these is obtained by a localized illumination of the object, which is shifted after each illumination to cover its whole domain.","As in the resulting measurements the phase information is lost, ptychography gives rise to solving a phase retrieval problem.","In this work, we consider ptychographic measurements corrupted with background noise, a type of additive noise that is independent of the shift, i.e., it is the same for all diffraction patterns.","Two algorithms are provided, for arbitrary objects and for so-called phase objects that do not absorb the light but only scatter it.","For the second type, a uniqueness of reconstruction is established for almost every object.","Our approach is based on the Wigner Distribution Deconvolution, which lifts the object to a higher-dimensional matrix space where the recovery can be reformulated as a linear problem.","Background noise only affects a few equations of the linear system that are therefore discarded.","The lost information is then restored using redundancy in the higher-dimensional space.   ","Keywords: phase retrieval, ptychography, background noise, Wigner Distribution Deconvolution, uniqueness of reconstruction."],"url":"http://arxiv.org/abs/2402.15353v1","category":"eess.IV"}
{"created":"2024-02-23 14:33:56","title":"Two-dimensional photonic crystal cavities in ZnSe quantum well structures","abstract":"ZnSe and related materials like ZnMgSe and ZnCdSe are promising II-VI host materials for optically mediated quantum information technology such as single photon sources or spin qubits. Integrating these heterostructures into photonic crystal (PC) cavities enables further improvements, for example realizing Purcell-enhanced single photon sources with increased quantum efficiency. Here we report on the successful implementation of two-dimensional (2D) PC cavities in strained ZnSe quantum wells (QW) on top of a novel AlAs supporting layer. This approach overcomes typical obstacles associated with PC membrane fabrication in strained materials, such as cracks and strain relaxation in the corresponding devices. We demonstrate the attainment of the required mechanical stability in our PC devices, complete strain retainment and effective vertical optical confinement. Structural analysis of our PC cavities reveals excellent etching anisotropy. Additionally, elemental mapping in a scanning transmission electron microscope confirms the transformation of AlAs into AlOx by post-growth wet oxidation and reveals partial oxidation of ZnMgSe at the etched sidewalls in the PC. This knowledge is utilized to tailor FDTD simulations and to extract the ZnMgSe dispersion relation with small oxygen content. Optical characterization of the PC cavities with cross-polarized resonance scattering spectroscopy verifies the presence of cavity modes. The excellent agreement between simulation and measured cavity mode energies demonstrates wide tunability of the PC cavity and proves the pertinence of our model. This implementation of 2D PC cavities in the ZnSe material system establishes a solid foundation for future developments of ZnSe quantum devices.","sentences":["ZnSe and related materials like ZnMgSe and ZnCdSe are promising II-VI host materials for optically mediated quantum information technology such as single photon sources or spin qubits.","Integrating these heterostructures into photonic crystal (PC) cavities enables further improvements, for example realizing Purcell-enhanced single photon sources with increased quantum efficiency.","Here we report on the successful implementation of two-dimensional (2D) PC cavities in strained ZnSe quantum wells (QW) on top of a novel AlAs supporting layer.","This approach overcomes typical obstacles associated with PC membrane fabrication in strained materials, such as cracks and strain relaxation in the corresponding devices.","We demonstrate the attainment of the required mechanical stability in our PC devices, complete strain retainment and effective vertical optical confinement.","Structural analysis of our PC cavities reveals excellent etching anisotropy.","Additionally, elemental mapping in a scanning transmission electron microscope confirms the transformation of AlAs into AlOx by post-growth wet oxidation and reveals partial oxidation of ZnMgSe at the etched sidewalls in the PC.","This knowledge is utilized to tailor FDTD simulations and to extract the ZnMgSe dispersion relation with small oxygen content.","Optical characterization of the PC cavities with cross-polarized resonance scattering spectroscopy verifies the presence of cavity modes.","The excellent agreement between simulation and measured cavity mode energies demonstrates wide tunability of the PC cavity and proves the pertinence of our model.","This implementation of 2D PC cavities in the ZnSe material system establishes a solid foundation for future developments of ZnSe quantum devices."],"url":"http://arxiv.org/abs/2402.15349v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 14:24:45","title":"Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates","abstract":"The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical comparisons of SGD with the existing first-order optimizers and show the usefulness of SGD using a critical batch size. Moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results.","sentences":["The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size.","They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training.","In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size.","Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity.","We also provide numerical comparisons of SGD with the existing first-order optimizers and show the usefulness of SGD using a critical batch size.","Moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results."],"url":"http://arxiv.org/abs/2402.15344v1","category":"stat.ML"}
{"created":"2024-02-23 14:22:46","title":"Ab-initio insights into the mechanical, phonon, bonding, electronic, optical and thermal properties of hexagonal W2N3 for potential applications","abstract":"We investigated the structural, elastic, electronic, vibrational, optical, thermodynamic and a number of thermophysical properties of W2N3 in this study using DFT based formalisms. The mechanical and dynamical stabilities have been confirmed. The Pugh and Poisson ratios are located quite close to the brittle to ductile borderline. The electronic band structure and energy density of states show metallic behavior. The Fermi surface features are investigated. The analysis of charge density distribution map clearly shows that W atoms have comparatively high electron density around than the N atoms. Presence of covalent bondings are anticipated. High melting temperature and high phonon thermal conductivity at room temperature of W2N3 imply that the compound has potential to be used as a heat sink system. The optical characteristics demonstrate anisotropy for W2N3. The compound can be used in optoelectronic device applications due to its high absorption coefficient and low reflectivity in the visible to ultraviolet spectrum. Furthermore, the quasiharmonic Debye model is used to examine temperature and pressure dependent thermal characteristics for the first time.","sentences":["We investigated the structural, elastic, electronic, vibrational, optical, thermodynamic and a number of thermophysical properties of W2N3 in this study using DFT based formalisms.","The mechanical and dynamical stabilities have been confirmed.","The Pugh and Poisson ratios are located quite close to the brittle to ductile borderline.","The electronic band structure and energy density of states show metallic behavior.","The Fermi surface features are investigated.","The analysis of charge density distribution map clearly shows that W atoms have comparatively high electron density around than the N atoms.","Presence of covalent bondings are anticipated.","High melting temperature and high phonon thermal conductivity at room temperature of W2N3 imply that the compound has potential to be used as a heat sink system.","The optical characteristics demonstrate anisotropy for W2N3.","The compound can be used in optoelectronic device applications due to its high absorption coefficient and low reflectivity in the visible to ultraviolet spectrum.","Furthermore, the quasiharmonic Debye model is used to examine temperature and pressure dependent thermal characteristics for the first time."],"url":"http://arxiv.org/abs/2402.15342v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 14:21:11","title":"MetaStates: An Approach for Representing Human Workers Psychophysiological States in the Industrial Metaverse","abstract":"Photo-realistic avatar is a modern term referring to the digital asset that represents a human in computer graphic advance systems such as video games and simulation tools. These avatars utilize the advances in graphic technologies on both software and hardware aspects. While photorealistic avatars are increasingly used in industrial simulations, representing human factors such as human workers internal states, remains a challenge. This article addresses this issue by introducing the concept of MetaStates which are the digitization and representation of the psychophysiological states of a human worker in the digital world. The MetaStates influence the physical representation and performance of a digital human worker while performing a task. To demonstrate this concept the study presents a development of a photorealistic avatar which is integrated into a simulated environment and enhanced with a multi-level graphical representation of different psychophysiological states. This approach represents a major step forward in the use of digital humans for industrial simulations, allowing companies to better leverage the benefits of the Industrial Metaverse in their daily operations and simulations while keeping human workers at the center of the system.","sentences":["Photo-realistic avatar is a modern term referring to the digital asset that represents a human in computer graphic advance systems such as video games and simulation tools.","These avatars utilize the advances in graphic technologies on both software and hardware aspects.","While photorealistic avatars are increasingly used in industrial simulations, representing human factors such as human workers internal states, remains a challenge.","This article addresses this issue by introducing the concept of MetaStates which are the digitization and representation of the psychophysiological states of a human worker in the digital world.","The MetaStates influence the physical representation and performance of a digital human worker while performing a task.","To demonstrate this concept the study presents a development of a photorealistic avatar which is integrated into a simulated environment and enhanced with a multi-level graphical representation of different psychophysiological states.","This approach represents a major step forward in the use of digital humans for industrial simulations, allowing companies to better leverage the benefits of the Industrial Metaverse in their daily operations and simulations while keeping human workers at the center of the system."],"url":"http://arxiv.org/abs/2402.15340v1","category":"cs.HC"}
{"created":"2024-02-23 14:01:27","title":"A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster Networks","abstract":"Emergency communication is critical but challenging after natural disasters when ground infrastructure is devastated. Unmanned aerial vehicles (UAVs) offer enormous potential for agile relief coordination in these scenarios. However, effectively leveraging UAV fleets poses additional challenges around security, privacy, and efficient collaboration across response agencies. This paper presents a robust blockchain-enabled framework to address these challenges by integrating a consortium blockchain model, smart contracts, and cryptographic techniques to securely coordinate UAV fleets for disaster response. Specifically, we make two key contributions: a consortium blockchain architecture for secure and private multi-agency coordination; and an optimized consensus protocol balancing efficiency and fault tolerance using a delegated proof of stake practical byzantine fault tolerance (DPoS-PBFT). Comprehensive simulations showcase the framework's ability to enhance transparency, automation, scalability, and cyber-attack resilience for UAV coordination in post-disaster networks.","sentences":["Emergency communication is critical but challenging after natural disasters when ground infrastructure is devastated.","Unmanned aerial vehicles (UAVs) offer enormous potential for agile relief coordination in these scenarios.","However, effectively leveraging UAV fleets poses additional challenges around security, privacy, and efficient collaboration across response agencies.","This paper presents a robust blockchain-enabled framework to address these challenges by integrating a consortium blockchain model, smart contracts, and cryptographic techniques to securely coordinate UAV fleets for disaster response.","Specifically, we make two key contributions: a consortium blockchain architecture for secure and private multi-agency coordination; and an optimized consensus protocol balancing efficiency and fault tolerance using a delegated proof of stake practical byzantine fault tolerance (DPoS-PBFT).","Comprehensive simulations showcase the framework's ability to enhance transparency, automation, scalability, and cyber-attack resilience for UAV coordination in post-disaster networks."],"url":"http://arxiv.org/abs/2402.15331v1","category":"cs.CR"}
{"created":"2024-02-23 13:57:35","title":"AI-powered simulation-based inference of a genuinely spatial-stochastic model of early mouse embryogenesis","abstract":"Understanding how multicellular organisms reliably orchestrate cell-fate decisions is a central challenge in developmental biology. This is particularly intriguing in early mammalian development, where early cell-lineage differentiation arises from processes that initially appear cell-autonomous but later materialize reliably at the tissue level. In this study, we develop a multi-scale, spatial-stochastic simulator of mouse embryogenesis, focusing on inner-cell mass (ICM) differentiation in the blastocyst stage. Our model features biophysically realistic regulatory interactions and accounts for the innate stochasticity of the biological processes driving cell-fate decisions at the cellular scale. We advance event-driven simulation techniques to incorporate relevant tissue-scale phenomena and integrate them with Simulation-Based Inference (SBI), building on a recent AI-based parameter learning method: the Sequential Neural Posterior Estimation (SNPE) algorithm. Using this framework, we carry out a large-scale Bayesian inferential analysis and determine parameter sets that reproduce the experimentally observed system behavior. We elucidate how autocrine and paracrine feedbacks via the signaling protein FGF4 orchestrate the inherently stochastic expression of fate-specifying genes at the cellular level into reproducible ICM patterning at the tissue scale. This mechanism is remarkably independent of the system size. FGF4 not only ensures correct cell lineage ratios in the ICM, but also enhances its resilience to perturbations. Intriguingly, we find that high variability in intracellular initial conditions does not compromise, but rather enhance the accuracy and precision of tissue-level dynamics. Our work provides a genuinely spatial-stochastic description of the biochemical processes driving ICM differentiation and the necessary conditions under which it can proceed robustly.","sentences":["Understanding how multicellular organisms reliably orchestrate cell-fate decisions is a central challenge in developmental biology.","This is particularly intriguing in early mammalian development, where early cell-lineage differentiation arises from processes that initially appear cell-autonomous but later materialize reliably at the tissue level.","In this study, we develop a multi-scale, spatial-stochastic simulator of mouse embryogenesis, focusing on inner-cell mass (ICM) differentiation in the blastocyst stage.","Our model features biophysically realistic regulatory interactions and accounts for the innate stochasticity of the biological processes driving cell-fate decisions at the cellular scale.","We advance event-driven simulation techniques to incorporate relevant tissue-scale phenomena and integrate them with Simulation-Based Inference (SBI), building on a recent AI-based parameter learning method: the Sequential Neural Posterior Estimation (SNPE) algorithm.","Using this framework, we carry out a large-scale Bayesian inferential analysis and determine parameter sets that reproduce the experimentally observed system behavior.","We elucidate how autocrine and paracrine feedbacks via the signaling protein FGF4 orchestrate the inherently stochastic expression of fate-specifying genes at the cellular level into reproducible ICM patterning at the tissue scale.","This mechanism is remarkably independent of the system size.","FGF4 not only ensures correct cell lineage ratios in the ICM, but also enhances its resilience to perturbations.","Intriguingly, we find that high variability in intracellular initial conditions does not compromise, but rather enhance the accuracy and precision of tissue-level dynamics.","Our work provides a genuinely spatial-stochastic description of the biochemical processes driving ICM differentiation and the necessary conditions under which it can proceed robustly."],"url":"http://arxiv.org/abs/2402.15330v1","category":"physics.bio-ph"}
{"created":"2024-02-23 13:38:00","title":"Mott transition for a Lieb-Liniger gas in a shallow quasiperiodic potential: Delocalization induced by disorder","abstract":"Disorder or quasi-disorder is known to favor the localization in many-body Bose systems. Here in contrast, we demonstrate an anomalous delocalization effect induced by incommensurability in quasiperiodic lattices. Loading ultracold atoms in two shallow periodic lattices with equal amplitude and either equal or incommensurate spatial periods, we show the onset of a Mott transition not only in the periodic case but also in the quasiperiodic case. Upon increase of the incommensurate component of the potential we find that the Mott insulator turns into a delocalized superfluid. Our experimental results agree with quantum Monte Carlo calculations, showing anomalous delocalization induced by the interplay between the commensuration and interaction.","sentences":["Disorder or quasi-disorder is known to favor the localization in many-body Bose systems.","Here in contrast, we demonstrate an anomalous delocalization effect induced by incommensurability in quasiperiodic lattices.","Loading ultracold atoms in two shallow periodic lattices with equal amplitude and either equal or incommensurate spatial periods, we show the onset of a Mott transition not only in the periodic case but also in the quasiperiodic case.","Upon increase of the incommensurate component of the potential we find that the Mott insulator turns into a delocalized superfluid.","Our experimental results agree with quantum Monte Carlo calculations, showing anomalous delocalization induced by the interplay between the commensuration and interaction."],"url":"http://arxiv.org/abs/2402.15318v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-23 13:30:52","title":"Stability of viscous three-dimensional stratified Couette flow via dispersion and mixing","abstract":"This article explores the stability of stratified Couette flow in the viscous $3d$ Boussinesq equations. In this system, mixing effects arise from the shearing background, and gravity acts as a restoring force leading to dispersive internal gravity waves. These mechanisms are of fundamentally different nature and relevant in complementary dynamical regimes. Our study combines them to establish a bound for the nonlinear transition threshold, which is quantitatively larger than the inverse Reynolds number $\\nu$, and increases with stronger stratification resp. gravity.","sentences":["This article explores the stability of stratified Couette flow in the viscous $3d$ Boussinesq equations.","In this system, mixing effects arise from the shearing background, and gravity acts as a restoring force leading to dispersive internal gravity waves.","These mechanisms are of fundamentally different nature and relevant in complementary dynamical regimes.","Our study combines them to establish a bound for the nonlinear transition threshold, which is quantitatively larger than the inverse Reynolds number $\\nu$, and increases with stronger stratification resp.","gravity."],"url":"http://arxiv.org/abs/2402.15312v1","category":"math.AP"}
{"created":"2024-02-23 13:07:55","title":"Assessing the Impact of Nuclear Mass Models on the Prediction of Synthesis Cross Sections for Superheavy Elements","abstract":"Within the framework of the dinuclear system model, this study delves into the impact of various nuclear mass models on evaluating the fusion probability of superheavy nuclei. Nuclear mass models, as crucial inputs to the DNS model, exhibit slight variations in binding energy, quadrupole deformation, and extrapolation ability; these subtle differences can significantly influence the model's outcomes. Specifically, the study finds that nuclear mass plays a pivotal role in determining fusion probability, and Q-value. By numerically solving a set of master equations, the study examines how binding energies from different mass models affect the fusion probability of colliding nuclei, taking the example of $^{48}$Ca + $^{243}$Am $\\rightarrow$ $^{291}$Mc. A careful analysis of the potential energy surface (PES) reveals that the inner fusion barriers lead to variations in fusion probabilities. Importantly, the study demonstrates that the synthesis cross sections of superheavy nuclei calculated using different nuclear mass models align well with experimental data, falling within an error range of one order of magnitude. This finding underscores the reliability of our model predictions. Looking ahead, the study utilizes five distinct nuclear mass models to predict the synthesis cross sections of superheavy elements 119 and 120, along with their associated uncertainties. These predictions offer valuable insights into the feasibility of synthesizing these elusive elements and pave the way for future experimental explorations.","sentences":["Within the framework of the dinuclear system model, this study delves into the impact of various nuclear mass models on evaluating the fusion probability of superheavy nuclei.","Nuclear mass models, as crucial inputs to the DNS model, exhibit slight variations in binding energy, quadrupole deformation, and extrapolation ability; these subtle differences can significantly influence the model's outcomes.","Specifically, the study finds that nuclear mass plays a pivotal role in determining fusion probability, and Q-value.","By numerically solving a set of master equations, the study examines how binding energies from different mass models affect the fusion probability of colliding nuclei, taking the example of $^{48}$Ca + $^{243}$Am $\\rightarrow$ $^{291}$Mc.","A careful analysis of the potential energy surface (PES) reveals that the inner fusion barriers lead to variations in fusion probabilities.","Importantly, the study demonstrates that the synthesis cross sections of superheavy nuclei calculated using different nuclear mass models align well with experimental data, falling within an error range of one order of magnitude.","This finding underscores the reliability of our model predictions.","Looking ahead, the study utilizes five distinct nuclear mass models to predict the synthesis cross sections of superheavy elements 119 and 120, along with their associated uncertainties.","These predictions offer valuable insights into the feasibility of synthesizing these elusive elements and pave the way for future experimental explorations."],"url":"http://arxiv.org/abs/2402.15304v1","category":"nucl-th"}
{"created":"2024-02-23 13:04:13","title":"Analytic pseudo-rotations II","abstract":"We construct analytic surface symplectomorphisms with unstable elliptic fixed points; this solves a problem of Birkhoff (1927). More precisely, we construct analytic symplectomorphisms of the sphere and of the disk which are transitive, with respectively only 2 and 1 periodic points. This also solves problems of proposed by Herman (1998), Fayad-Katok (2004) and Fayad-Krikorian (2018). To establish these results, we introduce a principle that enables to realize, by an analytic symplectomorphism, properties which are $C^0$-realizable by the approximation by conjugacy method of Anosov-Katok.","sentences":["We construct analytic surface symplectomorphisms with unstable elliptic fixed points; this solves a problem of Birkhoff (1927).","More precisely, we construct analytic symplectomorphisms of the sphere and of the disk which are transitive, with respectively only 2 and 1 periodic points.","This also solves problems of proposed by Herman (1998), Fayad-Katok (2004) and Fayad-Krikorian (2018).","To establish these results, we introduce a principle that enables to realize, by an analytic symplectomorphism, properties which are $C^0$-realizable by the approximation by conjugacy method of Anosov-Katok."],"url":"http://arxiv.org/abs/2402.15303v1","category":"math.DS"}
{"created":"2024-02-23 12:53:16","title":"Restoring Adiabatic State Transfer in Time-Modulated Non-Hermitian Systems","abstract":"Non-Hermitian systems have attracted much interest in recent decades, driven partly by the existence of exotic spectral singularities, known as exceptional points (EPs), where the dimensionality of the system evolution operator is reduced. Among various intriguing applications, the discovery of EPs has suggested the potential for implementing a symmetric mode switch, when encircling them in a system parameter space. However, subsequent theoretical and experimental works have revealed that {\\it dynamical} encirclement of EPs invariably results in asymmetric mode conversion; namely, the mode switching depends only on the winding direction but not on the initial state. This chirality arises from the failure of adiabaticity due to the complex spectrum of non-Hermitian systems. Although the chirality revealed has undoubtedly made a significant impact in the field, a realization of the originally sought symmetric adiabatic passage in non-Hermitian systems with EPs has since been elusive. In this work, we bridge this gap and theoretically demonstrate that adiabaticity, and therefore a symmetric state transfer, is achievable when dynamically winding around an EP. This becomes feasible by specifically choosing a trajectory in the system parameter space along which the corresponding evolution operator attains a real spectrum. Our findings, thus, offer a promise for advancing various wave manipulation protocols in both quantum and classical domains.","sentences":["Non-Hermitian systems have attracted much interest in recent decades, driven partly by the existence of exotic spectral singularities, known as exceptional points (EPs), where the dimensionality of the system evolution operator is reduced.","Among various intriguing applications, the discovery of EPs has suggested the potential for implementing a symmetric mode switch, when encircling them in a system parameter space.","However, subsequent theoretical and experimental works have revealed that {\\it dynamical} encirclement of EPs invariably results in asymmetric mode conversion; namely, the mode switching depends only on the winding direction but not on the initial state.","This chirality arises from the failure of adiabaticity due to the complex spectrum of non-Hermitian systems.","Although the chirality revealed has undoubtedly made a significant impact in the field, a realization of the originally sought symmetric adiabatic passage in non-Hermitian systems with EPs has since been elusive.","In this work, we bridge this gap and theoretically demonstrate that adiabaticity, and therefore a symmetric state transfer, is achievable when dynamically winding around an EP.","This becomes feasible by specifically choosing a trajectory in the system parameter space along which the corresponding evolution operator attains a real spectrum.","Our findings, thus, offer a promise for advancing various wave manipulation protocols in both quantum and classical domains."],"url":"http://arxiv.org/abs/2402.15298v1","category":"quant-ph"}
{"created":"2024-02-23 12:41:28","title":"SoK: What don't we know? Understanding Security Vulnerabilities in SNARKs","abstract":"Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept providing privacy and verifiability to having practical, real-world implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge) emerging as one of the most significant innovations. Prior work has mainly focused on designing more efficient SNARK systems and providing security proofs for them. Many think of SNARKs as \"just math,\" implying that what is proven to be correct and secure is correct in practice. In contrast, this paper focuses on assessing end-to-end security properties of real-life SNARK implementations. We start by building foundations with a system model and by establishing threat models and defining adversarial roles for systems that use SNARKs. Our study encompasses an extensive analysis of 141 actual vulnerabilities in SNARK implementations, providing a detailed taxonomy to aid developers and security researchers in understanding the security threats in systems employing SNARKs. Finally, we evaluate existing defense mechanisms and offer recommendations for enhancing the security of SNARK-based systems, paving the way for more robust and reliable implementations in the future.","sentences":["Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept providing privacy and verifiability to having practical, real-world implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge) emerging as one of the most significant innovations.","Prior work has mainly focused on designing more efficient SNARK systems and providing security proofs for them.","Many think of SNARKs as \"just math,\" implying that what is proven to be correct and secure is correct in practice.","In contrast, this paper focuses on assessing end-to-end security properties of real-life SNARK implementations.","We start by building foundations with a system model and by establishing threat models and defining adversarial roles for systems that use SNARKs.","Our study encompasses an extensive analysis of 141 actual vulnerabilities in SNARK implementations, providing a detailed taxonomy to aid developers and security researchers in understanding the security threats in systems employing SNARKs.","Finally, we evaluate existing defense mechanisms and offer recommendations for enhancing the security of SNARK-based systems, paving the way for more robust and reliable implementations in the future."],"url":"http://arxiv.org/abs/2402.15293v1","category":"cs.CR"}
{"created":"2024-02-23 12:31:56","title":"Fractional phase jumps in stochastic systems with tilted periodic double-well potentials","abstract":"We present a theoretical investigation of the stochastic dynamics of a damped particle in a tilted periodic potential with a double well per period. By applying the matrix continued fraction technique to the Fokker-Planck equation in conjunction with the full counting statistics and master equation approaches, we determine the rates of specific processes contributing to the system's overall dynamics. At low temperatures, the system can exhibit one running state and two distinct locked metastable states. We focus primarily on two aspects: the dynamics of phase jumps, which are rare thermally induced particle jumps over potential maxima, and their impact on the overall velocity noise; and the retrapping process, involving the transition from the running to the locked metastable states. We demonstrate the existence of fractional (in units of $2\\pi$) phase slips that differ qualitatively from conventional $2\\pi$ jumps observed in single-well systems. Fractional phase slips significantly influence the system dynamics even in regimes dominated by dichotomous-like switching between running and locked states. Furthermore, we introduce a simple master equation approach that proves effective in analyzing various stages of the retrapping process. Interestingly, our analysis shows that even for a system featuring a well-developed double-well periodic potential, there exists a broad parameter range where the stochastic dynamics can be accurately described by an effective single-well periodic model. The techniques introduced here allow for valuable insights into the complex behavior of the system, offering avenues for understanding and controlling its steady-state and transient dynamics, which go beyond or can be complementary to direct stochastic simulations.","sentences":["We present a theoretical investigation of the stochastic dynamics of a damped particle in a tilted periodic potential with a double well per period.","By applying the matrix continued fraction technique to the Fokker-Planck equation in conjunction with the full counting statistics and master equation approaches, we determine the rates of specific processes contributing to the system's overall dynamics.","At low temperatures, the system can exhibit one running state and two distinct locked metastable states.","We focus primarily on two aspects: the dynamics of phase jumps, which are rare thermally induced particle jumps over potential maxima, and their impact on the overall velocity noise; and the retrapping process, involving the transition from the running to the locked metastable states.","We demonstrate the existence of fractional (in units of $2\\pi$) phase slips that differ qualitatively from conventional $2\\pi$ jumps observed in single-well systems.","Fractional phase slips significantly influence the system dynamics even in regimes dominated by dichotomous-like switching between running and locked states.","Furthermore, we introduce a simple master equation approach that proves effective in analyzing various stages of the retrapping process.","Interestingly, our analysis shows that even for a system featuring a well-developed double-well periodic potential, there exists a broad parameter range where the stochastic dynamics can be accurately described by an effective single-well periodic model.","The techniques introduced here allow for valuable insights into the complex behavior of the system, offering avenues for understanding and controlling its steady-state and transient dynamics, which go beyond or can be complementary to direct stochastic simulations."],"url":"http://arxiv.org/abs/2402.15287v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-23 12:31:15","title":"E(n)-Equivariant Cartesian Tensor Passing Potential","abstract":"Machine learning potential (MLP) has been a popular topic in recent years for its potential to replace expensive first-principles calculations in some large systems. Meanwhile, message passing networks have gained significant attention due to their remarkable accuracy, and a wave of message passing networks based on Cartesian coordinates has emerged. However, the information of the node in these models is limited to scalars, vectors, and tensors. In this work, we proposed High-order Tensor Passing Potential (HotPP), an E(n) equivariant message passing neural network that extends the node embedding and message to an arbitrary order tensor. By performing some basic equivariant operations, high order tensors can be coupled very simply and thus the model can make direct predictions of high-order tensors such as dipole moments and polarizabilities without any modifications. Compared to high order tensor models based on spherical vectors, this network is simpler and can achieve comparable accuracy with much fewer parameters. The tests in several datasets demonstrate HotPP is a promising new approach that warrants further investigation.","sentences":["Machine learning potential (MLP) has been a popular topic in recent years for its potential to replace expensive first-principles calculations in some large systems.","Meanwhile, message passing networks have gained significant attention due to their remarkable accuracy, and a wave of message passing networks based on Cartesian coordinates has emerged.","However, the information of the node in these models is limited to scalars, vectors, and tensors.","In this work, we proposed High-order Tensor Passing Potential (HotPP), an E(n) equivariant message passing neural network that extends the node embedding and message to an arbitrary order tensor.","By performing some basic equivariant operations, high order tensors can be coupled very simply and thus the model can make direct predictions of high-order tensors such as dipole moments and polarizabilities without any modifications.","Compared to high order tensor models based on spherical vectors, this network is simpler and can achieve comparable accuracy with much fewer parameters.","The tests in several datasets demonstrate HotPP is a promising new approach that warrants further investigation."],"url":"http://arxiv.org/abs/2402.15286v1","category":"physics.comp-ph"}
{"created":"2024-02-23 11:33:22","title":"Combined thermal and particle shape effects on powder spreading in additive manufacturing via discrete element simulations","abstract":"The thermal and mechanical behaviors of powders are important for various additive manufacturing technologies. For powder bed fusion, capturing the temperature profile and the packing structure of the powders prior to melting is challenging due to both the various pathways of heat transfer and the complicated properties of powder system. Furthermore, these two effects can be coupled due to the temperature dependence of particle properties. This study addresses this challenge using a discrete element model that simulates non-spherical particles with thermal properties in powder spreading. Thermal conduction and radiation are introduced to a multi-sphere particle formulation for capturing the heat transfer among irregular-shaped powders, which have temperature-dependent elastic properties. The model is utilized to simulate the spreading of pre-heated PA12 powder through a hot substrate representing the part under manufacturing. Differences in the temperature profiles were found in the spreading cases with different particle shapes, spreading speed, and temperature dependence of the elastic moduli. The temperature of particles below the spreading blade is found to be dependent on the kinematics of the heap of particles in front, which eventually is influenced by the temperature-dependent properties of the particles.","sentences":["The thermal and mechanical behaviors of powders are important for various additive manufacturing technologies.","For powder bed fusion, capturing the temperature profile and the packing structure of the powders prior to melting is challenging due to both the various pathways of heat transfer and the complicated properties of powder system.","Furthermore, these two effects can be coupled due to the temperature dependence of particle properties.","This study addresses this challenge using a discrete element model that simulates non-spherical particles with thermal properties in powder spreading.","Thermal conduction and radiation are introduced to a multi-sphere particle formulation for capturing the heat transfer among irregular-shaped powders, which have temperature-dependent elastic properties.","The model is utilized to simulate the spreading of pre-heated PA12 powder through a hot substrate representing the part under manufacturing.","Differences in the temperature profiles were found in the spreading cases with different particle shapes, spreading speed, and temperature dependence of the elastic moduli.","The temperature of particles below the spreading blade is found to be dependent on the kinematics of the heap of particles in front, which eventually is influenced by the temperature-dependent properties of the particles."],"url":"http://arxiv.org/abs/2402.15271v1","category":"cond-mat.soft"}
{"created":"2024-02-23 11:18:35","title":"Hamiltonian regularisation of the unidimensional barotropic Euler equations","abstract":"Recently, a Hamiltonian regularised shallow water (Saint-Venant) system has been introduced by Clamond and Dutykh. This system is Galilean invariant, linearly non-dispersive and conserves formally an $H^1$-like energy. In this paper, we generalise this regularisation for the barotropic Euler system preserving the same properties. We prove the local (in time) well-posedness of the regularised barotropic Euler system and a periodic generalised two-component Hunterr-Saxton system. We also show for both systems that if singularities appear in finite time, they are necessary in the first derivatives.","sentences":["Recently, a Hamiltonian regularised shallow water (Saint-Venant) system has been introduced by Clamond and Dutykh.","This system is Galilean invariant, linearly non-dispersive and conserves formally an $H^1$-like energy.","In this paper, we generalise this regularisation for the barotropic Euler system preserving the same properties.","We prove the local (in time) well-posedness of the regularised barotropic Euler system and a periodic generalised two-component Hunterr-Saxton system.","We also show for both systems that if singularities appear in finite time, they are necessary in the first derivatives."],"url":"http://arxiv.org/abs/2402.15261v1","category":"math.AP"}
{"created":"2024-02-23 11:04:33","title":"Open Ad Hoc Teamwork with Cooperative Game Theory","abstract":"Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on GPL framework, to complement the critical features that facilitate learning, but overlooked in GPL. Through experiments, we demonstrate the correctness of our theory by comparing the performance of the resulting algorithm with GPL in dynamic team compositions.","sentences":["Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training.","Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams.","The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams.","GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability.","In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory.","Building on our theory, we propose a novel algorithm based on GPL framework, to complement the critical features that facilitate learning, but overlooked in GPL.","Through experiments, we demonstrate the correctness of our theory by comparing the performance of the resulting algorithm with GPL in dynamic team compositions."],"url":"http://arxiv.org/abs/2402.15259v1","category":"cs.MA"}
{"created":"2024-02-23 10:32:59","title":"Optimal regularity for all time for entropy solutions of conservation laws in $BV^s$","abstract":"This paper deals with the optimal regularity for entropy solutions of conservation laws. For this purpose, we use two key ingredients: (a) fine structure of entropy solutions and (b) fractional $BV$ spaces. We show that optimality of the regularizing effect for the initial value problem from $L^\\infty$ to fractional Sobolev space and fractional $BV$ spaces is valid for all time. Previously, such optimality was proven only for a finite time, before the nonlinear interaction of waves. Here for some well-chosen examples, the sharp regularity is obtained after the interaction of waves. Moreover , we prove sharp smoothing in $BV^s$ for a convex scalar conservation law with a linear source term. Next, we provide an upper bound of the maximal smoothing effect for nonlinear scalar multi-dimensional conservation laws and some hyperbolic systems in one or multi-dimension.","sentences":["This paper deals with the optimal regularity for entropy solutions of conservation laws.","For this purpose, we use two key ingredients: (a) fine structure of entropy solutions and (b) fractional $BV$ spaces.","We show that optimality of the regularizing effect for the initial value problem from $L^\\infty$ to fractional Sobolev space and fractional $BV$ spaces is valid for all time.","Previously, such optimality was proven only for a finite time, before the nonlinear interaction of waves.","Here for some well-chosen examples, the sharp regularity is obtained after the interaction of waves.","Moreover , we prove sharp smoothing in $BV^s$ for a convex scalar conservation law with a linear source term.","Next, we provide an upper bound of the maximal smoothing effect for nonlinear scalar multi-dimensional conservation laws and some hyperbolic systems in one or multi-dimension."],"url":"http://arxiv.org/abs/2402.15250v1","category":"math.AP"}
{"created":"2024-02-23 10:12:35","title":"On the existence of unbiased resilient estimators in discrete quantum systems","abstract":"Cram\\'er-Rao constitutes a crucial lower bound for the mean squared error of an estimator in frequentist parameter estimation, albeit paradoxically demanding highly accurate prior knowledge of the parameter to be estimated. Indeed, this information is needed to construct the optimal unbiased estimator, which is highly dependent on the parameter. Conversely, Bhattacharyya bounds result in a more resilient estimation about prior accuracy by imposing additional constraints on the estimator. Initially, we conduct a quantitative comparison of the performance between Cram\\'er-Rao and Bhattacharyya bounds when faced with less-than-ideal prior knowledge of the parameter. Furthermore, we demonstrate that the $n^{th}$order classical and quantum Bhattacharyya bounds cannot be computed -- given the absence of estimators satisfying the constraints -- under specific conditions tied to the dimension $m$ of the discrete system. Intriguingly, for a system with the same dimension $m$, the maximum non-trivial order $n$ is $m-1$ in the classical case, while in the quantum realm, it extends to $m(m+1)/2-1$. Consequently, for a given system dimension, one can construct estimators in quantum systems that exhibit increased robustness to prior ignorance.","sentences":["Cram\\'er-Rao constitutes a crucial lower bound for the mean squared error of an estimator in frequentist parameter estimation, albeit paradoxically demanding highly accurate prior knowledge of the parameter to be estimated.","Indeed, this information is needed to construct the optimal unbiased estimator, which is highly dependent on the parameter.","Conversely, Bhattacharyya bounds result in a more resilient estimation about prior accuracy by imposing additional constraints on the estimator.","Initially, we conduct a quantitative comparison of the performance between Cram\\'er-Rao and Bhattacharyya bounds when faced with less-than-ideal prior knowledge of the parameter.","Furthermore, we demonstrate that the $n^{th}$order classical and quantum Bhattacharyya bounds cannot be computed -- given the absence of estimators satisfying the constraints -- under specific conditions tied to the dimension $m$ of the discrete system.","Intriguingly, for a system with the same dimension $m$, the maximum non-trivial order $n$ is $m-1$ in the classical case, while in the quantum realm, it extends to $m(m+1)/2-1$.","Consequently, for a given system dimension, one can construct estimators in quantum systems that exhibit increased robustness to prior ignorance."],"url":"http://arxiv.org/abs/2402.15242v1","category":"quant-ph"}
{"created":"2024-02-23 09:46:49","title":"Open Energy Services -- Forecasting and Optimization as a Service for Energy Management Applications at Scale","abstract":"Energy management, in sense of computing optimized operation schedules for devices, will likely play a vital role in future carbon neutral energy systems, as it allows unlocking energy efficiency and flexibility potentials. However, energy management systems need to be applied at large scales to realize the desired effect, which clearly requires minimization of costs for setup and operation of the individual applications. In order to push the latter forward, we promote an approach to split the complex optimization algorithms employed by energy management systems into standardized components, which can be provided as a service with marginal costs at scale. This work is centered around the systematic design of a framework supporting the efficient implementation and operation of such forecasting and optimization services. Furthermore, it describes the implementation of the design concept which we release under the name \\emph{Energy Service Generics} as a free and open source repository. Finally, this paper marks the starting point of the \\emph{Open Energy Services} community, our effort to continuously push the development and operation of services for energy management applications at scale, for which we invite researchers and practitioners to participate.","sentences":["Energy management, in sense of computing optimized operation schedules for devices, will likely play a vital role in future carbon neutral energy systems, as it allows unlocking energy efficiency and flexibility potentials.","However, energy management systems need to be applied at large scales to realize the desired effect, which clearly requires minimization of costs for setup and operation of the individual applications.","In order to push the latter forward, we promote an approach to split the complex optimization algorithms employed by energy management systems into standardized components, which can be provided as a service with marginal costs at scale.","This work is centered around the systematic design of a framework supporting the efficient implementation and operation of such forecasting and optimization services.","Furthermore, it describes the implementation of the design concept which we release under the name \\emph{Energy Service Generics} as a free and open source repository.","Finally, this paper marks the starting point of the \\emph{Open Energy Services} community, our effort to continuously push the development and operation of services for energy management applications at scale, for which we invite researchers and practitioners to participate."],"url":"http://arxiv.org/abs/2402.15230v1","category":"cs.SE"}
{"created":"2024-02-23 09:37:05","title":"Ballistic vs. diffusive transport in metals","abstract":"Using the Boltzmann transport model, we show that, somewhat unintuitively, ballistic transport of electrons in metals is weaker than diffusive transport. This happens because the femtosecond-scale collision rates of the non-thermal electrons makes their mean-free path negligible. Our predictions are correlated with various photoluminescence and nonlinear optics experimental examples both for Continuous Wave (CW) and pulsed illumination, and open the way to easy modelling of the non-thermal electron distributions in metal nanostructures of arbitrary complexity.","sentences":["Using the Boltzmann transport model, we show that, somewhat unintuitively, ballistic transport of electrons in metals is weaker than diffusive transport.","This happens because the femtosecond-scale collision rates of the non-thermal electrons makes their mean-free path negligible.","Our predictions are correlated with various photoluminescence and nonlinear optics experimental examples both for Continuous Wave (CW) and pulsed illumination, and open the way to easy modelling of the non-thermal electron distributions in metal nanostructures of arbitrary complexity."],"url":"http://arxiv.org/abs/2402.15226v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-23 09:30:25","title":"Weak Reproductive Solutions for a Convection-Diffusion Model Describing a Binary Alloy Solidification Processes","abstract":"We study the existence of reproductive weak solutions for a system of equations describing a solidification process of a binary alloy confined into a bounded and regular domain in $\\mathbb{R}^3$, having mixed boundary conditions.","sentences":["We study the existence of reproductive weak solutions for a system of equations describing a solidification process of a binary alloy confined into a bounded and regular domain in $\\mathbb{R}^3$, having mixed boundary conditions."],"url":"http://arxiv.org/abs/2402.15221v1","category":"math.AP"}
{"created":"2024-02-23 09:29:19","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition","abstract":"Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.","sentences":["Self-attention is an essential component of large language models(LLMs)","but a significant source of inference latency for long sequences.","In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes.","In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache.","This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree.","Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts.","Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096."],"url":"http://arxiv.org/abs/2402.15220v1","category":"cs.LG"}
{"created":"2024-02-23 09:29:16","title":"Global Rotation of Skyrmion Bags under Vertical Microwave Fields","abstract":"Magnetic skyrmion bags are composite topological spin textures with arbitrary topological charges. Here, we computationally study the transient rotational motion of skyrmion bags, which is characterized by a global rotation of the inner skyrmions around the central point. Distinct from conventional rotational modes found in skyrmions, the observed rotation is a forced motion associated with the breathing mode induced solely by vertical microwave fields. The driving force behind this rotation originates from the interactions between outer and inner skyrmions, with the angular velocity determined by the phase difference resulting from their asynchronous breathing behaviors. It is also found that skyrmion bags with larger skyrmion numbers are more conducive to the occurrence of the rotation. Our results are useful for understanding the cluster dynamics of complex topological spin textures driven by dynamic fields.","sentences":["Magnetic skyrmion bags are composite topological spin textures with arbitrary topological charges.","Here, we computationally study the transient rotational motion of skyrmion bags, which is characterized by a global rotation of the inner skyrmions around the central point.","Distinct from conventional rotational modes found in skyrmions, the observed rotation is a forced motion associated with the breathing mode induced solely by vertical microwave fields.","The driving force behind this rotation originates from the interactions between outer and inner skyrmions, with the angular velocity determined by the phase difference resulting from their asynchronous breathing behaviors.","It is also found that skyrmion bags with larger skyrmion numbers are more conducive to the occurrence of the rotation.","Our results are useful for understanding the cluster dynamics of complex topological spin textures driven by dynamic fields."],"url":"http://arxiv.org/abs/2402.15219v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-23 09:24:04","title":"Item-side Fairness of Large Language Model-based Recommendation System","abstract":"Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on \\textit{MovieLens} and \\textit{Steam} datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.","sentences":["Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations.","The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs).","From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems.","To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS.","Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS.","IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS.","We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on \\textit{MovieLens} and \\textit{Steam} datasets, and observe significant item-side fairness improvements.","The code can be found in https://github.com/JiangM-C/IFairLRS.git."],"url":"http://arxiv.org/abs/2402.15215v1","category":"cs.IR"}
{"created":"2024-02-23 09:19:33","title":"ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification","abstract":"The accuracy of modern automatic speaker verification (ASV) systems, when trained exclusively on adult data, drops substantially when applied to children's speech. The scarcity of children's speech corpora hinders fine-tuning ASV systems for children's speech. Hence, there is a timely need to explore more effective ways of reusing adults' speech data. One promising approach is to align vocal-tract parameters between adults and children through children-specific data augmentation, referred here to as ChildAugment. Specifically, we modify the formant frequencies and formant bandwidths of adult speech to emulate children's speech. The modified spectra are used to train ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in time-delay neural network) recognizer for children. We compare ChildAugment against various state-of-the-art data augmentation techniques for children's ASV. We also extensively compare different scoring methods, including cosine scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural PLDA). We also propose a low-complexity weighted cosine score for extremely low-resource children ASV. Our findings on the CSLU kids corpus indicate that ChildAugment holds promise as a simple, acoustics-motivated approach, for improving state-of-the-art deep learning based ASV for children. We achieve up to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.","sentences":["The accuracy of modern automatic speaker verification (ASV) systems, when trained exclusively on adult data, drops substantially when applied to children's speech.","The scarcity of children's speech corpora hinders fine-tuning ASV systems for children's speech.","Hence, there is a timely need to explore more effective ways of reusing adults' speech data.","One promising approach is to align vocal-tract parameters between adults and children through children-specific data augmentation, referred here to as ChildAugment.","Specifically, we modify the formant frequencies and formant bandwidths of adult speech to emulate children's speech.","The modified spectra are used to train ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in time-delay neural network) recognizer for children.","We compare ChildAugment against various state-of-the-art data augmentation techniques for children's ASV.","We also extensively compare different scoring methods, including cosine scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural PLDA).","We also propose a low-complexity weighted cosine score for extremely low-resource children ASV.","Our findings on the CSLU kids corpus indicate that ChildAugment holds promise as a simple, acoustics-motivated approach, for improving state-of-the-art deep learning based ASV for children.","We achieve up to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline."],"url":"http://arxiv.org/abs/2402.15214v1","category":"eess.AS"}
{"created":"2024-02-23 09:13:56","title":"(Almost) Everything is a Dicke model -- Mapping correlated light-matter systems to the exactly solvable Dicke model","abstract":"We investigate classes of interacting quantum spin systems in a single-mode cavity with a Dicke coupling, as a paradigmatic example of strongly correlated light-matter systems. Coming from the limit of weak light-matter couplings and large number of matter entities, we map the relevant low-energy sector of a broad class of models onto the exactly solvable Dicke model. We apply the outcomes to the Dicke-Ising model as a paradigmatic example, in agreement with results obtained by mean-field theory. We further accompany and verify our findings with finite-size calculations, using exact diagonalization and the series expansion method pcst++.","sentences":["We investigate classes of interacting quantum spin systems in a single-mode cavity with a Dicke coupling, as a paradigmatic example of strongly correlated light-matter systems.","Coming from the limit of weak light-matter couplings and large number of matter entities, we map the relevant low-energy sector of a broad class of models onto the exactly solvable Dicke model.","We apply the outcomes to the Dicke-Ising model as a paradigmatic example, in agreement with results obtained by mean-field theory.","We further accompany and verify our findings with finite-size calculations, using exact diagonalization and the series expansion method pcst++."],"url":"http://arxiv.org/abs/2402.15209v1","category":"cond-mat.str-el"}
{"created":"2024-02-23 09:00:54","title":"Dynamics and energetics of ion adsorption at the interface between a pure ionic liquid and carbon electrodes","abstract":"Molecular dynamics simulations have been used extensively to determine equilibrium properties of the electrode-electrolyte interface in supercapacitors held at various potentials. While such studies are essential to understand and optimize the performance of such energy storage systems, investigations of the dynamics of adsorption during the charge of the supercapacitors is also necessary. Dynamical properties are especially important to get an insight into the power density of supercapacitors, one of their main assets. In this work, we propose a new method to analyze the trajectories of adsorbing ions. We focus on pure 1-ethyl-3-methylimidazolium bis(trifluoromethylsulfonyl)imide in contact with planar carbon electrodes. We characterize the evolution of the ion orientation and ion-electrode distance during adsorption and show that ions reorientate as they adsorb. We then determine the forces experienced by the adsorbing ions and demonstrate that Coulomb forces dominate at long range while van der Waals forces dominate at short range. We also show that there is an almost equal contribution from the two forces at an intermediate distance, explaining the peak of ion density close to the electrode surface.","sentences":["Molecular dynamics simulations have been used extensively to determine equilibrium properties of the electrode-electrolyte interface in supercapacitors held at various potentials.","While such studies are essential to understand and optimize the performance of such energy storage systems, investigations of the dynamics of adsorption during the charge of the supercapacitors is also necessary.","Dynamical properties are especially important to get an insight into the power density of supercapacitors, one of their main assets.","In this work, we propose a new method to analyze the trajectories of adsorbing ions.","We focus on pure 1-ethyl-3-methylimidazolium bis(trifluoromethylsulfonyl)imide in contact with planar carbon electrodes.","We characterize the evolution of the ion orientation and ion-electrode distance during adsorption and show that ions reorientate as they adsorb.","We then determine the forces experienced by the adsorbing ions and demonstrate that Coulomb forces dominate at long range while van der Waals forces dominate at short range.","We also show that there is an almost equal contribution from the two forces at an intermediate distance, explaining the peak of ion density close to the electrode surface."],"url":"http://arxiv.org/abs/2402.15199v1","category":"cond-mat.soft"}
{"created":"2024-02-23 08:59:04","title":"Bidirectional Uncertainty-Based Active Learning for Open Set Annotation","abstract":"Active learning (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes. Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence. Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples. In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \\textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework. Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \\textit{Random Label Negative Learning} method. Then, we propose a \\textit{Bidirectional Uncertainty sampling} strategy by jointly estimating uncertainty posed by both positive and negative learning to perform consistent and stable sampling. BUAL successfully extends existing uncertainty-based AL methods to complex open-set scenarios. Extensive experiments on multiple datasets with varying openness demonstrate that BUAL achieves state-of-the-art performance.","sentences":["Active learning (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes.","Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence.","Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples.","In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \\textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework.","Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \\textit{Random Label Negative Learning} method.","Then, we propose a \\textit{Bidirectional Uncertainty sampling} strategy by jointly estimating uncertainty posed by both positive and negative learning to perform consistent and stable sampling.","BUAL successfully extends existing uncertainty-based AL methods to complex open-set scenarios.","Extensive experiments on multiple datasets with varying openness demonstrate that BUAL achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.15198v1","category":"cs.LG"}
{"created":"2024-02-23 08:52:26","title":"Time-Varying Wireless Power Transfer Systems for Improving Efficiency","abstract":"Conventional wireless power transfer systems are linear and time-invariant, which sets fundamental limitations on their performance, including a tradeoff between transfer efficiency and the level of transferred power. In this paper, we introduce and study a possibility of temporal modulation for inductive wireless power transfer systems and uncover that this tradeoff is avoided as a consequence of varying the inductive coupling strength in time. Our theoretical analysis reveals that under the optimal modulation depth and phase, the time modulation can yield a substantial improvement in the WPT efficiency, while the received power at the load is also improved compared to the static WPT reference system. We experimentally demonstrate the concept with a low-frequency system and observe a threefold improvement in efficiency over the reference static counterpart. This technical capability reconciles the inherent tradeoff between the WPT efficiency and transferred power, paving the way for simultaneous advancements in both efficiency and delivered power.","sentences":["Conventional wireless power transfer systems are linear and time-invariant, which sets fundamental limitations on their performance, including a tradeoff between transfer efficiency and the level of transferred power.","In this paper, we introduce and study a possibility of temporal modulation for inductive wireless power transfer systems and uncover that this tradeoff is avoided as a consequence of varying the inductive coupling strength in time.","Our theoretical analysis reveals that under the optimal modulation depth and phase, the time modulation can yield a substantial improvement in the WPT efficiency, while the received power at the load is also improved compared to the static WPT reference system.","We experimentally demonstrate the concept with a low-frequency system and observe a threefold improvement in efficiency over the reference static counterpart.","This technical capability reconciles the inherent tradeoff between the WPT efficiency and transferred power, paving the way for simultaneous advancements in both efficiency and delivered power."],"url":"http://arxiv.org/abs/2402.15193v1","category":"physics.app-ph"}
{"created":"2024-02-23 08:47:34","title":"Exact equilibrium properties of square-well and square-shoulder disks in single-file confinement","abstract":"This study investigates the (longitudinal) thermodynamic and structural characteristics of single-file confined square-well and square-shoulder disks by employing a mapping technique that transforms the original system into a one-dimensional polydisperse mixture of nonadditive rods. Leveraging standard statistical-mechanical techniques, exact results are derived for key properties, including the equation of state, internal energy, radial distribution function, and structure factor. The asymptotic behavior of the radial distribution function is explored, revealing structural changes in the spatial correlations. Additionally, exact analytical expressions for the second virial coefficient are presented. Comparisons with Monte Carlo simulations demonstrate an excellent agreement with theory.","sentences":["This study investigates the (longitudinal) thermodynamic and structural characteristics of single-file confined square-well and square-shoulder disks by employing a mapping technique that transforms the original system into a one-dimensional polydisperse mixture of nonadditive rods.","Leveraging standard statistical-mechanical techniques, exact results are derived for key properties, including the equation of state, internal energy, radial distribution function, and structure factor.","The asymptotic behavior of the radial distribution function is explored, revealing structural changes in the spatial correlations.","Additionally, exact analytical expressions for the second virial coefficient are presented.","Comparisons with Monte Carlo simulations demonstrate an excellent agreement with theory."],"url":"http://arxiv.org/abs/2402.15192v1","category":"cond-mat.soft"}
{"created":"2024-02-23 08:45:35","title":"A Digital Twinning Platform for Integrated Sensing, Communications and Robotics","abstract":"In this paper, a digital twinning framework for indoor integrated sensing, communications, and robotics is proposed, designed, and implemented. Besides leveraging powerful robotics and ray-tracing technologies, the framework also enables integration with real-world sensors and reactive updates triggered by changes in the environment. The framework is designed with commercial, off-the-shelf components in mind, thus facilitating experimentation in the different areas of communication, sensing, and robotics. Experimental results showcase the feasibility and accuracy of indoor localization using digital twins and validate our implementation both qualitatively and quantitatively.","sentences":["In this paper, a digital twinning framework for indoor integrated sensing, communications, and robotics is proposed, designed, and implemented.","Besides leveraging powerful robotics and ray-tracing technologies, the framework also enables integration with real-world sensors and reactive updates triggered by changes in the environment.","The framework is designed with commercial, off-the-shelf components in mind, thus facilitating experimentation in the different areas of communication, sensing, and robotics.","Experimental results showcase the feasibility and accuracy of indoor localization using digital twins and validate our implementation both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2402.15191v1","category":"cs.RO"}
{"created":"2024-02-23 08:32:50","title":"Pre-Chirp-Domain Index Modulation for Affine Frequency Division Multiplexing","abstract":"Affine frequency division multiplexing (AFDM), tailored as a novel multicarrier technique utilizing chirp signals for high-mobility communications, exhibits marked advantages compared to traditional orthogonal frequency division multiplexing (OFDM). AFDM is based on the discrete affine Fourier transform (DAFT) with two modifiable parameters of the chirp signals, termed as the pre-chirp parameter and post-chirp parameter, respectively. These parameters can be fine-tuned to avoid overlapping channel paths with different delays or Doppler shifts, leading to performance enhancement especially for doubly dispersive channel. In this paper, we propose a novel AFDM structure with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM), which can embed additional information bits into the pre-chirp parameter design for both spectral and energy efficiency enhancement. Specifically, we first demonstrate that the application of distinct pre-chirp parameters to various subcarriers in the AFDM modulation process maintains the orthogonality among these subcarriers. Then, different pre-chirp parameters are flexibly assigned to each AFDM subcarrier according to the incoming bits. By such arrangement, aside from classical phase/amplitude modulation, extra binary bits can be implicitly conveyed by the indices of selected pre-chirping parameters realizations without additional energy consumption. At the receiver, both a maximum likelihood (ML) detector and a reduced-complexity ML-minimum mean square error (ML-MMSE) detector are employed to recover the information bits. It has been shown via simulations that the proposed AFDM-PIM exhibits superior bit error rate (BER) performance compared to classical AFDM, OFDM and IM-aided OFDM algorithms.","sentences":["Affine frequency division multiplexing (AFDM), tailored as a novel multicarrier technique utilizing chirp signals for high-mobility communications, exhibits marked advantages compared to traditional orthogonal frequency division multiplexing (OFDM).","AFDM is based on the discrete affine Fourier transform (DAFT) with two modifiable parameters of the chirp signals, termed as the pre-chirp parameter and post-chirp parameter, respectively.","These parameters can be fine-tuned to avoid overlapping channel paths with different delays or Doppler shifts, leading to performance enhancement especially for doubly dispersive channel.","In this paper, we propose a novel AFDM structure with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM), which can embed additional information bits into the pre-chirp parameter design for both spectral and energy efficiency enhancement.","Specifically, we first demonstrate that the application of distinct pre-chirp parameters to various subcarriers in the AFDM modulation process maintains the orthogonality among these subcarriers.","Then, different pre-chirp parameters are flexibly assigned to each AFDM subcarrier according to the incoming bits.","By such arrangement, aside from classical phase/amplitude modulation, extra binary bits can be implicitly conveyed by the indices of selected pre-chirping parameters realizations without additional energy consumption.","At the receiver, both a maximum likelihood (ML) detector and a reduced-complexity ML-minimum mean square error (ML-MMSE) detector are employed to recover the information bits.","It has been shown via simulations that the proposed AFDM-PIM exhibits superior bit error rate (BER) performance compared to classical AFDM, OFDM and IM-aided OFDM algorithms."],"url":"http://arxiv.org/abs/2402.15185v1","category":"cs.IT"}
{"created":"2024-02-23 18:54:03","title":"Electrical Scanning Probe Microscope Measurements Reveal Surprisingly High Dark Conductivity in Y6 and PM6:Y6 and Non-Langevin Recombination in PM6:Y6","abstract":"We used broadband local dielectric spectroscopy (BLDS), an electric force microscopy technique, to make non-contact measurements of conductivity in the dark and under illumination of PM6:Y6 and Y6 prepared on ITO and PEDOT:PSS/ITO. Over a range of illumination intensities, BLDS spectra were acquired and fit to an impedance model of the tip-sample interaction to obtain a sample resistance and capacitance. By comparing two descriptions of cantilever friction, an impedance model and a microscopic model, we connected the sample resistance inferred from impedance modeling to a microscopic sample conductivity. A charge recombination rate was estimated from plots of the conductivity versus light intensity and found to be sub-Langevin. The dark conductivity was orders of magnitude higher than expected from Fermi-level equilibration of the PM6:Y6 with the substrate, suggesting that dark carriers may be a source of open-circuit voltage loss in PM6:Y6.","sentences":["We used broadband local dielectric spectroscopy (BLDS), an electric force microscopy technique, to make non-contact measurements of conductivity in the dark and under illumination of PM6:Y6 and Y6 prepared on ITO and PEDOT:PSS/ITO.","Over a range of illumination intensities, BLDS spectra were acquired and fit to an impedance model of the tip-sample interaction to obtain a sample resistance and capacitance.","By comparing two descriptions of cantilever friction, an impedance model and a microscopic model, we connected the sample resistance inferred from impedance modeling to a microscopic sample conductivity.","A charge recombination rate was estimated from plots of the conductivity versus light intensity and found to be sub-Langevin.","The dark conductivity was orders of magnitude higher than expected from Fermi-level equilibration of the PM6:Y6 with the substrate, suggesting that dark carriers may be a source of open-circuit voltage loss in PM6:Y6."],"url":"http://arxiv.org/abs/2402.15501v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 18:48:42","title":"Using CPI in Loss Given Default Forecasting Models for Commercial Real Estate Portfolio","abstract":"Forecasting the loss given default (LGD) for defaulted Commercial Real Estate (CRE) loans poses a significant challenge due to the extended resolution and workout time associated with such defaults, particularly in CCAR and CECL framework where the utilization of post-default information, including macroeconomic variables (MEVs) such as unemployment (UER) and various rates, is restricted. The current environment of persistent inflation and resultant elevated rates further compounds the uncertainty surrounding predictive LGD models. In this paper, we leverage both internal and public data sources, including observations from the COVID-19 period, to present a list of evidence indicating that the growth rates of the Consumer Price, such as Year-over-Year (YoY) growth and logarithmic growth, are good leading indicators for various CRE related rates and indices. These include the Federal Funds Effective Rate and CRE market sales price indices in key locations such as Los Angeles, New York, and nationwide, encompassing both apartment and office segments. Furthermore, with CRE LGD data we demonstrate how incorporating CPI at the time of default can improve the accuracy of predicting CRE workout LGD. This is particularly helpful in addressing the common issue of early downturn underestimation encountered in CRE LGD models.","sentences":["Forecasting the loss given default (LGD) for defaulted Commercial Real Estate (CRE) loans poses a significant challenge due to the extended resolution and workout time associated with such defaults, particularly in CCAR and CECL framework where the utilization of post-default information, including macroeconomic variables (MEVs) such as unemployment (UER) and various rates, is restricted.","The current environment of persistent inflation and resultant elevated rates further compounds the uncertainty surrounding predictive LGD models.","In this paper, we leverage both internal and public data sources, including observations from the COVID-19 period, to present a list of evidence indicating that the growth rates of the Consumer Price, such as Year-over-Year (YoY) growth and logarithmic growth, are good leading indicators for various CRE related rates and indices.","These include the Federal Funds Effective Rate and CRE market sales price indices in key locations such as Los Angeles, New York, and nationwide, encompassing both apartment and office segments.","Furthermore, with CRE LGD data we demonstrate how incorporating CPI at the time of default can improve the accuracy of predicting CRE workout LGD.","This is particularly helpful in addressing the common issue of early downturn underestimation encountered in CRE LGD models."],"url":"http://arxiv.org/abs/2402.15498v1","category":"q-fin.RM"}
{"created":"2024-02-23 18:11:32","title":"Debiasing Machine Learning Models by Using Weakly Supervised Learning","abstract":"We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also makes use of a reasonably large amount of input observations and their corresponding predictions. Only a small fraction of the true output predictions should be known. This therefore limits the need for expert interventions. Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics.","sentences":["We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous.","Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous.","Typical examples are unfair decisions made with respect to the age or the financial status.","In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics.","In addition to solve this new problem, our bias mitigation strategy is a weakly supervised learning method which requires that a small portion of the data can be measured in a fair manner.","It is model agnostic, in the sense that it does not make any hypothesis on the prediction model.","It also makes use of a reasonably large amount of input observations and their corresponding predictions.","Only a small fraction of the true output predictions should be known.","This therefore limits the need for expert interventions.","Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics."],"url":"http://arxiv.org/abs/2402.15477v1","category":"cs.LG"}
{"created":"2024-02-23 17:37:28","title":"Potential outcome simulation for efficient head-to-head comparison of adaptive dose-finding designs","abstract":"Dose-finding trials are a key component of the drug development process and rely on a statistical design to help inform dosing decisions. Triallists wishing to choose a design require knowledge of operating characteristics of competing methods. This is often assessed using a large-scale simulation study with multiple designs and configurations investigated, which can be time-consuming and therefore limits the scope of the simulation.   We introduce a new approach to the design of simulation studies of dose-finding trials. The approach simulates all potential outcomes that individuals could experience at each dose level in the trial. Datasets are simulated in advance and then the same datasets are applied to each of the competing methods to enable a more efficient head-to-head comparison.   In two case-studies we show sizeable reductions in Monte Carlo error for comparing a performance metric between two competing designs. Efficiency gains depend on the similarity of the designs. Comparing two Phase I/II design variants, with high correlation of recommending the same optimal biologic dose, we show that the new approach requires a simulation study that is approximately 30 times smaller than the conventional approach. Furthermore, advance-simulated trial datasets can be reused to assess the performance of designs across multiple configurations.   We recommend researchers consider this more efficient simulation approach in their dose-finding studies and we have updated the R package escalation to help facilitate implementation.","sentences":["Dose-finding trials are a key component of the drug development process and rely on a statistical design to help inform dosing decisions.","Triallists wishing to choose a design require knowledge of operating characteristics of competing methods.","This is often assessed using a large-scale simulation study with multiple designs and configurations investigated, which can be time-consuming and therefore limits the scope of the simulation.   ","We introduce a new approach to the design of simulation studies of dose-finding trials.","The approach simulates all potential outcomes that individuals could experience at each dose level in the trial.","Datasets are simulated in advance and then the same datasets are applied to each of the competing methods to enable a more efficient head-to-head comparison.   ","In two case-studies we show sizeable reductions in Monte Carlo error for comparing a performance metric between two competing designs.","Efficiency gains depend on the similarity of the designs.","Comparing two Phase I/II design variants, with high correlation of recommending the same optimal biologic dose, we show that the new approach requires a simulation study that is approximately 30 times smaller than the conventional approach.","Furthermore, advance-simulated trial datasets can be reused to assess the performance of designs across multiple configurations.   ","We recommend researchers consider this more efficient simulation approach in their dose-finding studies and we have updated the R package escalation to help facilitate implementation."],"url":"http://arxiv.org/abs/2402.15460v1","category":"stat.CO"}
{"created":"2024-02-23 16:34:49","title":"Prime+Retouch: When Cache is Locked and Leaked","abstract":"Caches on the modern commodity CPUs have become one of the major sources of side-channel leakages and been abused as a new attack vector. To thwart the cache-based side-channel attacks, two types of countermeasures have been proposed: detection-based ones that limit the amount of microarchitectural traces an attacker can leave, and cache prefetching-and-locking techniques that claim to prevent such leakage by disallowing evictions on sensitive data. In this paper, we present the Prime+Retouch attack that completely bypasses these defense schemes by accurately inferring the cache activities with the metadata of the cache replacement policy. Prime+Retouch has three noticeable properties: 1) it incurs no eviction on the victim's data, allowing us to bypass the two known mitigation schemes, 2) it requires minimal synchronization of only one memory access to the attacker's pre-primed cache lines, and 3) it leaks data via non-shared memory, yet because underlying eviction metadata is shared.   We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and emerging Apple M1. We elucidate how Prime+Retouch can break the T-table implementation of AES with robust cache side-channel mitigations such as Cloak, under both normal and SGX-protected environments. We also manifest feasibility of the Prime+Retouch attack on the M1 platform imposing more restrictions where the precise measurement tools such as core clock cycle timer and performance counters are inaccessible to the attacker. Furthermore, we first demystify undisclosed cache architecture and its eviction policy of L1 data cache on Apple M1 architecture. We also devise a user-space noise-free cache monitoring tool by repurposing Intel TSX.","sentences":["Caches on the modern commodity CPUs have become one of the major sources of side-channel leakages and been abused as a new attack vector.","To thwart the cache-based side-channel attacks, two types of countermeasures have been proposed: detection-based ones that limit the amount of microarchitectural traces an attacker can leave, and cache prefetching-and-locking techniques that claim to prevent such leakage by disallowing evictions on sensitive data.","In this paper, we present the Prime+Retouch attack that completely bypasses these defense schemes by accurately inferring the cache activities with the metadata of the cache replacement policy.","Prime+Retouch has three noticeable properties: 1) it incurs no eviction on the victim's data, allowing us to bypass the two known mitigation schemes, 2) it requires minimal synchronization of only one memory access to the attacker's pre-primed cache lines, and 3) it leaks data via non-shared memory, yet because underlying eviction metadata is shared.   ","We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and emerging Apple M1.","We elucidate how Prime+Retouch can break the T-table implementation of AES with robust cache side-channel mitigations such as Cloak, under both normal and SGX-protected environments.","We also manifest feasibility of the Prime+Retouch attack on the M1 platform imposing more restrictions where the precise measurement tools such as core clock cycle timer and performance counters are inaccessible to the attacker.","Furthermore, we first demystify undisclosed cache architecture and its eviction policy of L1 data cache on Apple M1 architecture.","We also devise a user-space noise-free cache monitoring tool by repurposing Intel TSX."],"url":"http://arxiv.org/abs/2402.15425v1","category":"cs.CR"}
{"created":"2024-02-23 16:30:09","title":"Photo-nuclear cross sections on $^{197}$Au","abstract":"A method was developed for measuring photonuclear reactions concurrently at several discrete photon beam energies on a stack of different target materials via a single irradiation. Concentric ring targets of the materials (in order from front to back targets: Au, TiO$_2$, Zn, Os, and Au) were irradiated at the High Intensity Gamma-ray Source (HI$\\gamma$S). As a proof of principle, we report the result of the cross section measurements from the front Au target. The excitation functions of the $^{197}$Au($\\gamma$,n)$^{196}$Au and $^{197}$Au($\\gamma$,3n)$^{194}$Au reactions were determined in the incident photon energy range of 13-31 MeV using quasi-monoenergetic photon beams provided at HI$\\gamma$S. The cross sections of the combined ground state (2$^{-}$) and short-lived first isomeric state (m1, 5$^{+}$), and of the second isomeric state (m2, 12$^{-}$) in the $^{196}$Au production are obtained separately by subtracting the $\\gamma$ rays from the internal conversion of the second isomeric state. The excitation function of the second isomeric state via the photon-induced reaction $^{197}$Au($\\gamma$,n)$^{196m2}$Au was measured for the first time. By using the activation method rather than direct neutron counting, the exclusive cross sections for the ($\\gamma$,n) and ($\\gamma$,3n) reactions were determined. Comparing the yields from the front and back gold targets validates our ability to simulate the effect of photon scattering in the target stack and provides a method for assessing the systematic uncertainty of our technique.","sentences":["A method was developed for measuring photonuclear reactions concurrently at several discrete photon beam energies on a stack of different target materials via a single irradiation.","Concentric ring targets of the materials (in order from front to back targets: Au, TiO$_2$, Zn, Os, and Au) were irradiated at the High Intensity Gamma-ray Source (HI$\\gamma$S).","As a proof of principle, we report the result of the cross section measurements from the front Au target.","The excitation functions of the $^{197}$Au($\\gamma$,n)$^{196}$Au and $^{197}$Au($\\gamma$,3n)$^{194}$Au reactions were determined in the incident photon energy range of 13-31 MeV using quasi-monoenergetic photon beams provided at HI$\\gamma$S. The cross sections of the combined ground state (2$^{-}$) and short-lived first isomeric state (m1, 5$^{+}$), and of the second isomeric state (m2, 12$^{-}$) in the $^{196}$Au production are obtained separately by subtracting the $\\gamma$ rays from the internal conversion of the second isomeric state.","The excitation function of the second isomeric state via the photon-induced reaction $^{197}$Au($\\gamma$,n)$^{196m2}$Au was measured for the first time.","By using the activation method rather than direct neutron counting, the exclusive cross sections for the ($\\gamma$,n) and ($\\gamma$,3n) reactions were determined.","Comparing the yields from the front and back gold targets validates our ability to simulate the effect of photon scattering in the target stack and provides a method for assessing the systematic uncertainty of our technique."],"url":"http://arxiv.org/abs/2402.15421v1","category":"nucl-ex"}
{"created":"2024-02-23 16:19:32","title":"Optimisic Information Directed Sampling","abstract":"We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a less conservative version of the Decision Estimation Coefficient of Foster et al. (2021). Keywords: Contextual bandits, information-directed sampling, decision estimation coefficient, first-order regret bounds.","sentences":["We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class.","We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient.","Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions.","The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a less conservative version of the Decision Estimation Coefficient of Foster et al. (2021).","Keywords: Contextual bandits, information-directed sampling, decision estimation coefficient, first-order regret bounds."],"url":"http://arxiv.org/abs/2402.15411v1","category":"cs.LG"}
{"created":"2024-02-23 15:52:04","title":"Dendrites with corners","abstract":"A phase-field model for diffusion-limited crystal growth is formulated that is capable of handling highly anisotropic interfaces. It uses a Willmore regularization that yields corners of finite size. An asymptotic analysis reveals that Herring's law is recovered for the advancing surfaces. The model is validated by conducting simulations of dendritic growth for low anistorpies and comparing the results to the data from the literature. The model makes it possible to simulate high anisotropy dendrites for which the standard phase-field models are ill-posed. In this regime, the interplay between a Herring instability on the dendrite flanks and the corner regularization creates zig-zag shaped corrugations and leads to a non-monotonic trend of tip velocity as a function of anisotropy strength.","sentences":["A phase-field model for diffusion-limited crystal growth is formulated that is capable of handling highly anisotropic interfaces.","It uses a Willmore regularization that yields corners of finite size.","An asymptotic analysis reveals that Herring's law is recovered for the advancing surfaces.","The model is validated by conducting simulations of dendritic growth for low anistorpies and comparing the results to the data from the literature.","The model makes it possible to simulate high anisotropy dendrites for which the standard phase-field models are ill-posed.","In this regime, the interplay between a Herring instability on the dendrite flanks and the corner regularization creates zig-zag shaped corrugations and leads to a non-monotonic trend of tip velocity as a function of anisotropy strength."],"url":"http://arxiv.org/abs/2402.15394v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 15:34:15","title":"Higher order measures of risk and stochastic dominance","abstract":"Higher order risk measures are stochastic optimization problems by design, and for this reason they enjoy valuable properties in optimization under uncertainties. They nicely integrate with stochastic optimization problems, as has been observed by the intriguing concept of the risk quadrangles, for example. Stochastic dominance is a binary relation for random variables to compare random outcomes. It is demonstrated that the concepts of higher order risk measures and stochastic dominance are equivalent, they can be employed to characterize the other. The paper explores these relations and connects stochastic orders, higher order risk measures and the risk quadrangle. Expectiles are employed to exemplify the relations obtained.","sentences":["Higher order risk measures are stochastic optimization problems by design, and for this reason they enjoy valuable properties in optimization under uncertainties.","They nicely integrate with stochastic optimization problems, as has been observed by the intriguing concept of the risk quadrangles, for example.","Stochastic dominance is a binary relation for random variables to compare random outcomes.","It is demonstrated that the concepts of higher order risk measures and stochastic dominance are equivalent, they can be employed to characterize the other.","The paper explores these relations and connects stochastic orders, higher order risk measures and the risk quadrangle.","Expectiles are employed to exemplify the relations obtained."],"url":"http://arxiv.org/abs/2402.15387v1","category":"q-fin.RM"}
{"created":"2024-02-23 15:29:18","title":"The Logic of Correct Models","abstract":"For each $n\\in\\mathbb{N}$, let $[n]\\phi$ mean \"the sentence $\\phi$ is true in all $\\Sigma_{n+1}$-correct transitive sets.\" Assuming G\\\"odel's axiom $V = L$, we prove the following graded variant of Solovay's completeness theorem: the set of formulas valid under this interpretation is precisely the set of theorems of the linear provability logic GLP.3. We also show that this result is not provable in ZFC, so the hypothesis V = L cannot be removed. As part of the proof, we derive (in ZFC) the following purely modal-logical results which are of independent interest: the logic GLP.3 coincides with the logic of closed substitutions of GLP, and is the maximal non-degenerate, normal extension of GLP.","sentences":["For each $n\\in\\mathbb{N}$, let $[n]\\phi$ mean \"the sentence $\\phi$ is true in all $\\Sigma_{n+1}$-correct transitive sets.\"","Assuming G\\\"odel's axiom $V = L$, we prove the following graded variant of Solovay's completeness theorem: the set of formulas valid under this interpretation is precisely the set of theorems of the linear provability logic GLP.3.","We also show that this result is not provable in ZFC, so the hypothesis V = L cannot be removed.","As part of the proof, we derive (in ZFC) the following purely modal-logical results which are of independent interest: the logic GLP.3 coincides with the logic of closed substitutions of GLP, and is the maximal non-degenerate, normal extension of GLP."],"url":"http://arxiv.org/abs/2402.15382v1","category":"math.LO"}
{"created":"2024-02-23 15:19:37","title":"Outlier detection by ensembling uncertainty with negative objectness","abstract":"Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.","sentences":["Outlier detection is an essential capability in safety-critical applications of supervised visual recognition.","Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data.","However, that approach conflates prediction uncertainty with recognition of the negative class.","We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class.","This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness.","Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data.","We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes.","The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances.","Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data."],"url":"http://arxiv.org/abs/2402.15374v1","category":"cs.CV"}
{"created":"2024-02-23 14:53:39","title":"A priori error estimates of Runge-Kutta discontinuous Galerkin schemes to smooth solutions of fractional conservation laws","abstract":"We give a priori error estimates of second order in time fully explicit Runge-Kutta discontinuous Galerkin schemes using upwind fluxes to smooth solutions of scalar fractional conservation laws in one space dimension. Under the time step restrictions $\\tau\\leq c h$ for piecewise linear and $\\tau\\lesssim h^{4/3}$ for higher order finite elements, we prove a convergence rate for the energy norm $\\|\\cdot\\|_{L^\\infty_tL^2_x}+|\\cdot|_{L^2_tH^{\\lambda/2}_x}$ that is optimal for solutions and flux functions that are smooth enough. Our proof relies on a novel upwind projection of the exact solution.","sentences":["We give a priori error estimates of second order in time fully explicit Runge-Kutta discontinuous Galerkin schemes using upwind fluxes to smooth solutions of scalar fractional conservation laws in one space dimension.","Under the time step restrictions $\\tau\\leq c h$ for piecewise linear and $\\tau\\lesssim h^{4/3}$ for higher order finite elements, we prove a convergence rate for the energy norm $\\|\\cdot\\|_{L^\\infty_tL^2_x}+|\\cdot|_{L^2_tH^{\\lambda/2}_x}$ that is optimal for solutions and flux functions that are smooth enough.","Our proof relies on a novel upwind projection of the exact solution."],"url":"http://arxiv.org/abs/2402.15361v1","category":"math.NA"}
{"created":"2024-02-23 13:40:34","title":"Optimal Transport on the Lie Group of Roto-translations","abstract":"The roto-translation group SE2 has been of active interest in image analysis due to methods that lift the image data to multi-orientation representations defined on this Lie group. This has led to impactful applications of crossing-preserving flows for image de-noising, geodesic tracking, and roto-translation equivariant deep learning. In this paper, we develop a computational framework for optimal transportation over Lie groups, with a special focus on SE2. We make several theoretical contributions (generalizable to matrix Lie groups) such as the non-optimality of group actions as transport maps, invariance and equivariance of optimal transport, and the quality of the entropic-regularized optimal transport plan using geodesic distance approximations. We develop a Sinkhorn like algorithm that can be efficiently implemented using fast and accurate distance approximations of the Lie group and GPU-friendly group convolutions. We report valuable advancements in the experiments on 1) image barycenters, 2) interpolation of planar orientation fields, and 3) Wasserstein gradient flows on SE2. We observe that our framework of lifting images to SE2 and optimal transport with left-invariant anisotropic metrics leads to equivariant transport along dominant contours and salient line structures in the image. This yields sharper and more meaningful interpolations compared to their counterparts on $\\mathbb{R}^2$","sentences":["The roto-translation group SE2 has been of active interest in image analysis due to methods that lift the image data to multi-orientation representations defined on this Lie group.","This has led to impactful applications of crossing-preserving flows for image de-noising, geodesic tracking, and roto-translation equivariant deep learning.","In this paper, we develop a computational framework for optimal transportation over Lie groups, with a special focus on SE2.","We make several theoretical contributions (generalizable to matrix Lie groups) such as the non-optimality of group actions as transport maps, invariance and equivariance of optimal transport, and the quality of the entropic-regularized optimal transport plan using geodesic distance approximations.","We develop a Sinkhorn like algorithm that can be efficiently implemented using fast and accurate distance approximations of the Lie group and GPU-friendly group convolutions.","We report valuable advancements in the experiments on 1) image barycenters, 2) interpolation of planar orientation fields, and 3) Wasserstein gradient flows on SE2.","We observe that our framework of lifting images to SE2 and optimal transport with left-invariant anisotropic metrics leads to equivariant transport along dominant contours and salient line structures in the image.","This yields sharper and more meaningful interpolations compared to their counterparts on $\\mathbb{R}^2$"],"url":"http://arxiv.org/abs/2402.15322v1","category":"cs.CV"}
{"created":"2024-02-23 13:33:43","title":"The origin of phase separation in binary aluminosilicate glasses","abstract":"The quest for hard and tough transparent oxide glasses is at the core of glass science and technology. Aluminosilicate glasses exhibiting nanoscale phase separation emerge as promising candidates for such materials. Nevertheless, proper control of the phase separation represents a daunting challenge due to its elusive origins. Here we employ large-scale molecular dynamics simulations and structural analysis to unravel the underlying mechanisms of the phase separation in aluminosilicate. The observed phase separation originates from an arrangement of SiO$_4$ and AlO$_n$ polyhedra, which manifests from the second coordination shell and extends to higher shells. This specific arrangement is driven by repulsion between the polyhedra, reaching its maximum at around 50 mol% of Al$_2$O$_3$. This behavior becomes pronounced around and below the glass transition temperature. This work sheds light on the origin of phase separation and provides a route for further exploration across other compositions to develop glasses with adapted mechanical performance.","sentences":["The quest for hard and tough transparent oxide glasses is at the core of glass science and technology.","Aluminosilicate glasses exhibiting nanoscale phase separation emerge as promising candidates for such materials.","Nevertheless, proper control of the phase separation represents a daunting challenge due to its elusive origins.","Here we employ large-scale molecular dynamics simulations and structural analysis to unravel the underlying mechanisms of the phase separation in aluminosilicate.","The observed phase separation originates from an arrangement of SiO$_4$ and AlO$_n$ polyhedra, which manifests from the second coordination shell and extends to higher shells.","This specific arrangement is driven by repulsion between the polyhedra, reaching its maximum at around 50 mol% of Al$_2$O$_3$. This behavior becomes pronounced around and below the glass transition temperature.","This work sheds light on the origin of phase separation and provides a route for further exploration across other compositions to develop glasses with adapted mechanical performance."],"url":"http://arxiv.org/abs/2402.15314v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 13:28:31","title":"Scaling Limit of Kuramoto Model on Random Geometric Graphs","abstract":"We consider the Kuramoto model on a graph with nodes given by $n$ i.i.d. points uniformly distributed on the $d$ dimensional torus. Two nodes are declared neighbors if they are at distance less than $\\epsilon$. We prove a scaling limit for this model in compact time intervals as $n\\to\\infty$ and $\\epsilon \\to 0$ such that $\\epsilon^{d+2}n/\\log n \\to \\infty$. The limiting object is given by the heat equation. On the one hand this shows that the nonlinearity given by the sine function disappears under this scaling and on the other hand, provides evidence that stable equilibria of the Kuramoto model on these graphs are, as $n\\to\\infty$, in correspondence with those of the heat equation, which are explicit and given by twisted states. In view of this, we conjecture the existence of twisted stable equilibria with high probability as $n\\to \\infty$.","sentences":["We consider the Kuramoto model on a graph with nodes given by $n$ i.i.d. points uniformly distributed on the $d$ dimensional torus.","Two nodes are declared neighbors if they are at distance less than $\\epsilon$. We prove a scaling limit for this model in compact time intervals as $n\\to\\infty$ and $\\epsilon \\to 0$ such that $\\epsilon^{d+2}n/\\log n \\to \\infty$. The limiting object is given by the heat equation.","On the one hand this shows that the nonlinearity given by the sine function disappears under this scaling and on the other hand, provides evidence that stable equilibria of the Kuramoto model on these graphs are, as $n\\to\\infty$, in correspondence with those of the heat equation, which are explicit and given by twisted states.","In view of this, we conjecture the existence of twisted stable equilibria with high probability as $n\\to \\infty$."],"url":"http://arxiv.org/abs/2402.15311v1","category":"math.PR"}
{"created":"2024-02-23 13:13:28","title":"Curve fitting on a quantum annealer for an advanced navigation method","abstract":"We explore the applicability of quantum annealing to the approximation task of curve fitting. To this end, we consider a function that shall approximate a given set of data points and is written as a finite linear combination of standardized functions, e.g., orthogonal polynomials. Consequently, the decision variables subject to optimization are the coefficients of that expansion. Although this task can be accomplished classically, it can also be formulated as a quadratic unconstrained binary optimization problem, which is suited to be solved with quantum annealing. Given the size of the problem stays below a certain threshold, we find that quantum annealing yields comparable results to the classical solution. Regarding a real-word use case, we discuss the problem to find an optimized speed profile for a vessel using the framework of dynamic programming and outline how the aforementioned approximation task can be put into play.","sentences":["We explore the applicability of quantum annealing to the approximation task of curve fitting.","To this end, we consider a function that shall approximate a given set of data points and is written as a finite linear combination of standardized functions, e.g., orthogonal polynomials.","Consequently, the decision variables subject to optimization are the coefficients of that expansion.","Although this task can be accomplished classically, it can also be formulated as a quadratic unconstrained binary optimization problem, which is suited to be solved with quantum annealing.","Given the size of the problem stays below a certain threshold, we find that quantum annealing yields comparable results to the classical solution.","Regarding a real-word use case, we discuss the problem to find an optimized speed profile for a vessel using the framework of dynamic programming and outline how the aforementioned approximation task can be put into play."],"url":"http://arxiv.org/abs/2402.15308v1","category":"math.OC"}
{"created":"2024-02-23 13:09:17","title":"Ubiquitous short-range order in multi-principal element alloys","abstract":"Recent research in multi-principal element alloys (MPEAs) has increasingly focused on the exploration and exploitation of short-range order (SRO) to enhance material performance. However, the understanding of SRO formation and the precise tuning of it within MPEAs remains poorly understood, limiting the comprehension of its impact on material properties and impeding the advancement of SRO engineering. Here, leveraging advanced additive manufacturing techniques that produce samples with a wide range of cooling rates (up to 10^7 K/s) and an improved quantitative electron microscopy method, we characterize SRO in three CoCrNi-based MPEAs to unravel the role of processing route and thermal history on SRO. Surprisingly, irrespective of the processing and thermal treatment applied, all samples exhibit similar levels of SRO, suggesting that prevalent SRO may form during the solidification process. Atomistic simulations of solidification verify that local chemical ordering arises in the liquid-solid interface (solidification front) even under the extreme cooling rate of 10^11 K/s. This phenomenon stems from the swift atomic diffusion in the supercooled liquid, which matches or even surpasses the rate of solidification. Therefore, SRO is an inherent characteristic of most MPEAs, insensitive to variations in cooling rates and annealing treatments typically available in experiments. Integrating thermal treatment with other strategies, such as mechanical deformation and irradiation, might be more effective approaches for harnessing SRO to achieve controlled material properties.","sentences":["Recent research in multi-principal element alloys (MPEAs) has increasingly focused on the exploration and exploitation of short-range order (SRO) to enhance material performance.","However, the understanding of SRO formation and the precise tuning of it within MPEAs remains poorly understood, limiting the comprehension of its impact on material properties and impeding the advancement of SRO engineering.","Here, leveraging advanced additive manufacturing techniques that produce samples with a wide range of cooling rates (up to 10^7 K/s) and an improved quantitative electron microscopy method, we characterize SRO in three CoCrNi-based MPEAs to unravel the role of processing route and thermal history on SRO.","Surprisingly, irrespective of the processing and thermal treatment applied, all samples exhibit similar levels of SRO, suggesting that prevalent SRO may form during the solidification process.","Atomistic simulations of solidification verify that local chemical ordering arises in the liquid-solid interface (solidification front) even under the extreme cooling rate of 10^11 K/s. This phenomenon stems from the swift atomic diffusion in the supercooled liquid, which matches or even surpasses the rate of solidification.","Therefore, SRO is an inherent characteristic of most MPEAs, insensitive to variations in cooling rates and annealing treatments typically available in experiments.","Integrating thermal treatment with other strategies, such as mechanical deformation and irradiation, might be more effective approaches for harnessing SRO to achieve controlled material properties."],"url":"http://arxiv.org/abs/2402.15305v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 12:48:02","title":"Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling","abstract":"This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.","sentences":["This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled.","We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value.","On this basis, we propose a semi-supervised crowd-counting model.","Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t.","different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently.","Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings.","Code will be released at https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling."],"url":"http://arxiv.org/abs/2402.15297v1","category":"cs.CV"}
{"created":"2024-02-23 12:46:59","title":"Polaritonic Chemistry Enabled by Non-Local Metasurfaces","abstract":"Vibrational strong coupling can modify chemical reaction pathways in unconventional ways. Thus far, Fabry-Perot cavities formed by pairs of facing mirrors have been mostly utilized to achieve vibrational strong coupling. In this study, we demonstrate the application of plasmonic microparticle arrays defining non-local metasurfaces that can sustain surface lattice resonances as a novel tool to enable chemical reactions under vibrational strong coupling. We show that the solvolysis kinetics of \\textit{para}-nitrophenyl acetate can be accelerated by a factor of 2.7 by strong coupling to the carbonyl bond of the solvent and the solute with a surface lattice resonance. Our work introduces a new platform to investigate and control polaritonic chemical reactions. In contrast to Fabry-Perot cavities, metasurfaces define open optical cavities with single surfaces, which removes alignment hurdles, facilitating polaritonic chemistry across large areas.","sentences":["Vibrational strong coupling can modify chemical reaction pathways in unconventional ways.","Thus far, Fabry-Perot cavities formed by pairs of facing mirrors have been mostly utilized to achieve vibrational strong coupling.","In this study, we demonstrate the application of plasmonic microparticle arrays defining non-local metasurfaces that can sustain surface lattice resonances as a novel tool to enable chemical reactions under vibrational strong coupling.","We show that the solvolysis kinetics of \\textit{para}-nitrophenyl acetate can be accelerated by a factor of 2.7 by strong coupling to the carbonyl bond of the solvent and the solute with a surface lattice resonance.","Our work introduces a new platform to investigate and control polaritonic chemical reactions.","In contrast to Fabry-Perot cavities, metasurfaces define open optical cavities with single surfaces, which removes alignment hurdles, facilitating polaritonic chemistry across large areas."],"url":"http://arxiv.org/abs/2402.15296v1","category":"physics.chem-ph"}
{"created":"2024-02-23 12:38:27","title":"adjustedCurves: Estimating Confounder-Adjusted Survival Curves in R","abstract":"Kaplan-Meier curves stratified by treatment allocation are the most popular way to depict causal effects in studies with right-censored time-to-event endpoints. If the treatment is randomly assigned and the sample size of the study is adequate, this method produces unbiased estimates of the population-averaged counterfactual survival curves. However, in the presence of confounding, this is no longer the case. Instead, specific methods that allow adjustment for confounding must be used. We present the adjustedCurves R package, which can be used to estimate and plot these confounder-adjusted survival curves using a variety of methods from the literature. It provides a convenient wrapper around existing R packages on the topic and adds additional methods and functionality on top of it, uniting the sometimes vastly different methods under one consistent framework. Among the additional features are the estimation of confidence intervals, confounder-adjusted restricted mean survival times and confounder-adjusted survival time quantiles. After giving a brief overview of the implemented methods, we illustrate the package using publicly available data from an observational study including 2982 breast cancer.","sentences":["Kaplan-Meier curves stratified by treatment allocation are the most popular way to depict causal effects in studies with right-censored time-to-event endpoints.","If the treatment is randomly assigned and the sample size of the study is adequate, this method produces unbiased estimates of the population-averaged counterfactual survival curves.","However, in the presence of confounding, this is no longer the case.","Instead, specific methods that allow adjustment for confounding must be used.","We present the adjustedCurves R package, which can be used to estimate and plot these confounder-adjusted survival curves using a variety of methods from the literature.","It provides a convenient wrapper around existing R packages on the topic and adds additional methods and functionality on top of it, uniting the sometimes vastly different methods under one consistent framework.","Among the additional features are the estimation of confidence intervals, confounder-adjusted restricted mean survival times and confounder-adjusted survival time quantiles.","After giving a brief overview of the implemented methods, we illustrate the package using publicly available data from an observational study including 2982 breast cancer."],"url":"http://arxiv.org/abs/2402.15292v1","category":"stat.ME"}
{"created":"2024-02-23 12:01:16","title":"Small positive values and limit theorems for supercritical branching processes with immigration in random environment","abstract":"Let $(Z_n)$ be a supercritical branching process with immigration in a random environment. The small positive values and some lower deviation inequalities for $Z$ are investigated. Based on these results, the central limit theorem of $\\log Z_n$ and the Edgeworth expansion are obtained. The study is taken under the assumption that each individual produces $0$ offspring with a positive probability.","sentences":["Let $(Z_n)$ be a supercritical branching process with immigration in a random environment.","The small positive values and some lower deviation inequalities for $Z$ are investigated.","Based on these results, the central limit theorem of $\\log Z_n$ and the Edgeworth expansion are obtained.","The study is taken under the assumption that each individual produces $0$ offspring with a positive probability."],"url":"http://arxiv.org/abs/2402.15279v1","category":"math.PR"}
{"created":"2024-02-23 11:37:56","title":"Classification Under Strategic Self-Selection","abstract":"When users stand to gain from certain predictions, they are prone to act strategically to obtain favorable predictive outcomes. Whereas most works on strategic classification consider user actions that manifest as feature modifications, we study a novel setting in which users decide -- in response to the learned classifier -- whether to at all participate (or not). For learning approaches of increasing strategic awareness, we study the effects of self-selection on learning, and the implications of learning on the composition of the self-selected population. We then propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively. We conclude with experiments on real data and simulated behavior that both complement our analysis and demonstrate the utility of our approach.","sentences":["When users stand to gain from certain predictions, they are prone to act strategically to obtain favorable predictive outcomes.","Whereas most works on strategic classification consider user actions that manifest as feature modifications, we study a novel setting in which users decide -- in response to the learned classifier -- whether to at all participate (or not).","For learning approaches of increasing strategic awareness, we study the effects of self-selection on learning, and the implications of learning on the composition of the self-selected population.","We then propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively.","We conclude with experiments on real data and simulated behavior that both complement our analysis and demonstrate the utility of our approach."],"url":"http://arxiv.org/abs/2402.15274v1","category":"cs.LG"}
{"created":"2024-02-23 10:37:25","title":"PICO: Accelerating All k-Core Paradigms on GPU","abstract":"Core decomposition is a well-established graph mining problem with various applications that involves partitioning the graph into hierarchical subgraphs. Solutions to this problem have been developed using both bottom-up and top-down approaches from the perspective of vertex convergence dependency. However, existing algorithms have not effectively harnessed GPU performance to expedite core decomposition, despite the growing need for enhanced performance. Moreover, approaching performance limitations of core decomposition from two different directions within a parallel synchronization structure has not been thoroughly explored. This paper introduces an efficient GPU acceleration framework, PICO, for the Peel and Index2core paradigms of k-core decomposition. We propose PeelOne, a Peel-based algorithm designed to simplify the parallel logic and minimize atomic operations by eliminating vertices that are 'under-core'. We also propose an Index2core-based algorithm, named HistoCore, which addresses the issue of extensive redundant computations across both vertices and edges. Extensive experiments on NVIDIA RTX 3090 GPU show that PeelOne outperforms all other Peel-based algorithms, and HistoCore outperforms all other Index2core-based algorithms. Furthermore, HistoCore even outperforms PeelOne by 1.1x - 3.2x speedup on six datasets, which breaks the stereotype that the Index2core paradigm performs much worse than the Peel in a shared memory parallel setting.","sentences":["Core decomposition is a well-established graph mining problem with various applications that involves partitioning the graph into hierarchical subgraphs.","Solutions to this problem have been developed using both bottom-up and top-down approaches from the perspective of vertex convergence dependency.","However, existing algorithms have not effectively harnessed GPU performance to expedite core decomposition, despite the growing need for enhanced performance.","Moreover, approaching performance limitations of core decomposition from two different directions within a parallel synchronization structure has not been thoroughly explored.","This paper introduces an efficient GPU acceleration framework, PICO, for the Peel and Index2core paradigms of k-core decomposition.","We propose PeelOne, a Peel-based algorithm designed to simplify the parallel logic and minimize atomic operations by eliminating vertices that are 'under-core'.","We also propose an Index2core-based algorithm, named HistoCore, which addresses the issue of extensive redundant computations across both vertices and edges.","Extensive experiments on NVIDIA RTX 3090 GPU show that PeelOne outperforms all other Peel-based algorithms, and HistoCore outperforms all other Index2core-based algorithms.","Furthermore, HistoCore even outperforms PeelOne by 1.1x - 3.2x speedup on six datasets, which breaks the stereotype that the Index2core paradigm performs much worse than the Peel in a shared memory parallel setting."],"url":"http://arxiv.org/abs/2402.15253v1","category":"cs.DC"}
{"created":"2024-02-23 10:27:54","title":"Interferometric microscale measurement of refractive index at VIS and IR wavelengths","abstract":"Determination of refractive index of micro-disks of a calcinated ($1100^\\circ$C in air) photo-resist SZ2080$^\\mathrm{TM}$ was carried out using transmission and reflection spectroscopy. Interference fringes at specific wavenumbers/wavelengths were selected for determination of the optical thickness, hence, the refractive index when the thickness of micro-disks was measured by scanning electron microscopy (SEM). Refractive index of disks of $\\sim 6\\pm 1~\\mu$m thickness were determined at visible and IR (2.5-13~$\\mu$m) spectral ranges and where $2.2\\pm 0.2$ at visible and IR wavelengths. Peculiarities of optical characterisation of micro-optical structures are discussed in view of possible uncertainties in the definition of geometric parameters, shape and mass density redistribution.","sentences":["Determination of refractive index of micro-disks of a calcinated ($1100^\\circ$C in air) photo-resist SZ2080$^\\mathrm{TM}$ was carried out using transmission and reflection spectroscopy.","Interference fringes at specific wavenumbers/wavelengths were selected for determination of the optical thickness, hence, the refractive index when the thickness of micro-disks was measured by scanning electron microscopy (SEM).","Refractive index of disks of $\\sim 6\\pm 1~\\mu$m thickness were determined at visible and IR (2.5-13~$\\mu$m) spectral ranges and where $2.2\\pm 0.2$ at visible and IR wavelengths.","Peculiarities of optical characterisation of micro-optical structures are discussed in view of possible uncertainties in the definition of geometric parameters, shape and mass density redistribution."],"url":"http://arxiv.org/abs/2402.15249v1","category":"physics.optics"}
{"created":"2024-02-23 09:26:35","title":"How to identify earth pressures on in-service tunnel linings: A Bayesian learning perspective","abstract":"The identification of earth pressures acting on in-service transportation tunnel linings is essential for their health monitoring and performance prediction, especially for those exhibiting poor structural performance. Since pressure gauges incur substantial costs, the inversion of pressures based on easily observed structural responses, such as deformations, is desirable. The inherent challenge in this inverse problem lies in the non-uniqueness of solutions, which arises from the fact that various pressures can yield structural responses fitting equally well with the observed data. However, existing approaches for pressure inversion predominantly rely on a deterministic framework, often neglecting a detailed discussion on this non-uniqueness. In addressing this gap, this study introduces a Bayesian approach. The proposed statistical framework enables the quantification of uncertainty induced by non-uniqueness in inversion results. The analysis identifies the uniform component in distributed pressures as the primary source of non-uniqueness. The mitigation of solution non-uniqueness can be achieved by increasing the quantity of deformation data or incorporating an observation of internal normal force in a tunnel lining -- the latter proving to be notably more effective. The practical application in a numerical case demonstrates the effectiveness of this approach and the associated findings. In addition, our investigation recommends maintaining deformation measurement accuracy within the range of [-1, 1] mm to ensure satisfactory outcomes. Finally, deficiencies and potential future extensions of this approach are discussed.","sentences":["The identification of earth pressures acting on in-service transportation tunnel linings is essential for their health monitoring and performance prediction, especially for those exhibiting poor structural performance.","Since pressure gauges incur substantial costs, the inversion of pressures based on easily observed structural responses, such as deformations, is desirable.","The inherent challenge in this inverse problem lies in the non-uniqueness of solutions, which arises from the fact that various pressures can yield structural responses fitting equally well with the observed data.","However, existing approaches for pressure inversion predominantly rely on a deterministic framework, often neglecting a detailed discussion on this non-uniqueness.","In addressing this gap, this study introduces a Bayesian approach.","The proposed statistical framework enables the quantification of uncertainty induced by non-uniqueness in inversion results.","The analysis identifies the uniform component in distributed pressures as the primary source of non-uniqueness.","The mitigation of solution non-uniqueness can be achieved by increasing the quantity of deformation data or incorporating an observation of internal normal force in a tunnel lining -- the latter proving to be notably more effective.","The practical application in a numerical case demonstrates the effectiveness of this approach and the associated findings.","In addition, our investigation recommends maintaining deformation measurement accuracy within the range of [-1, 1] mm to ensure satisfactory outcomes.","Finally, deficiencies and potential future extensions of this approach are discussed."],"url":"http://arxiv.org/abs/2402.15217v1","category":"stat.AP"}
{"created":"2024-02-23 09:06:12","title":"How do wavelength correlations affect your transmission spectrum? Application of a new fast and flexible 2D Gaussian process framework to transiting exoplanet spectroscopy","abstract":"The use of Gaussian processes (GPs) is a common approach to account for correlated noise in exoplanet time-series, particularly for transmission and emission spectroscopy. This analysis has typically been performed for each wavelength channel separately, with the retrieved uncertainties in the transmission spectrum assumed to be independent. However, the presence of noise correlated in wavelength could cause these uncertainties to be correlated, which could significantly affect the results of atmospheric retrievals. We present a method which uses a GP to model noise correlated in both wavelength and time simultaneously for the full spectroscopic data set while avoiding the use of a 'common-mode' correction. To make this analysis computationally tractable, we introduce a fast and flexible GP method which can analyse 2D data sets when the input points lie on a (potentially non-uniform) 2D grid - in our case a time by wavelength grid - and the kernel function has a Kronecker product structure. This simultaneously fits all light curves and enables the retrieval of the covariance matrix of the transmission spectrum. By testing on synthetic data sets, we demonstrate that our new approach can reliably recover atmospheric features contaminated by noise correlated in time and wavelength. In contrast, fitting each spectroscopic light curve separately performed poorly when wavelength-correlated noise was present. It frequently underestimated the uncertainty of the scattering slope and overestimated the uncertainty in the strength of sharp absorption peaks in transmission spectra. Two archival VLT/FORS2 transit observations of WASP-31b were re-analysed, with our method strongly constraining the presence of wavelength-correlated noise in both data sets and recovering significantly different constraints on atmospheric features such as the scattering slope and strength of sodium and potassium features.","sentences":["The use of Gaussian processes (GPs) is a common approach to account for correlated noise in exoplanet time-series, particularly for transmission and emission spectroscopy.","This analysis has typically been performed for each wavelength channel separately, with the retrieved uncertainties in the transmission spectrum assumed to be independent.","However, the presence of noise correlated in wavelength could cause these uncertainties to be correlated, which could significantly affect the results of atmospheric retrievals.","We present a method which uses a GP to model noise correlated in both wavelength and time simultaneously for the full spectroscopic data set while avoiding the use of a 'common-mode' correction.","To make this analysis computationally tractable, we introduce a fast and flexible GP method which can analyse 2D data sets when the input points lie on a (potentially non-uniform) 2D grid - in our case a time by wavelength grid - and the kernel function has a Kronecker product structure.","This simultaneously fits all light curves and enables the retrieval of the covariance matrix of the transmission spectrum.","By testing on synthetic data sets, we demonstrate that our new approach can reliably recover atmospheric features contaminated by noise correlated in time and wavelength.","In contrast, fitting each spectroscopic light curve separately performed poorly when wavelength-correlated noise was present.","It frequently underestimated the uncertainty of the scattering slope and overestimated the uncertainty in the strength of sharp absorption peaks in transmission spectra.","Two archival VLT/FORS2 transit observations of WASP-31b were re-analysed, with our method strongly constraining the presence of wavelength-correlated noise in both data sets and recovering significantly different constraints on atmospheric features such as the scattering slope and strength of sodium and potassium features."],"url":"http://arxiv.org/abs/2402.15204v1","category":"astro-ph.EP"}
{"created":"2024-02-23 08:22:24","title":"Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement","abstract":"Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be easily utilized in real-world service.","sentences":["Caution:","This paper includes offensive words that could potentially cause unpleasantness.","Language models (LMs) are vulnerable to exploitation for adversarial misuse.","Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks.","We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks.","Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations.","We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses.","In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be easily utilized in real-world service."],"url":"http://arxiv.org/abs/2402.15180v1","category":"cs.LG"}
{"created":"2024-02-23 14:54:29","title":"Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues","abstract":"In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot's traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains. Code is available at https://github.com/yurimjeon1892/FtFoot.","sentences":["In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments.","An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability.","This study highlights three primary factors that affect a robot's traversability in an off-road environment: surface slope, semantic information, and robot platform.","We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM).","The first strategy involves building a novel GFN using a newly designed guide filter layer.","The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation.","The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint.","This enables the prediction of traversability that reflects the characteristics of the robot platform.","Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability.","Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains.","Code is available at https://github.com/yurimjeon1892/FtFoot."],"url":"http://arxiv.org/abs/2402.15363v1","category":"cs.RO"}
{"created":"2024-02-23 13:34:08","title":"Beyond-mean-field stochastic corrections to the blueshift of a driven-dissipative exciton-polariton condensate","abstract":"In the absence of vortices or phase slips, the phase dynamics of exciton-polariton condensates was shown to map onto the Kardar-Parisi-Zhang (KPZ) equation, which describes the stochastic growth of a classical interface. This implies that the coherence of such non-equilibrium quasi-condensates decays in space and time following stretched exponentials, characterized by KPZ universal critical exponents. In this work, we focus on the time evolution of the average phase of a one-dimensional exciton-polariton condensate in the KPZ regime and determine the frequency of its evolution, which is given by the blueshift, i.e. the non-equilibrium analog of the chemical potential. We determine the stochastic corrections to the blueshift within Bogoliubov linearized theory and find that while this correction physically originates from short scale effects, and depends both on density and phase fluctuations, it can still be related to the effective large-scale KPZ parameters. Using numerical simulations of the full dynamics, we investigate the dependence of these blueshift corrections on both noise and interaction strength, and compare the results to the Bogoliubov prediction. Our finding contributes both to the close comparison between equilibrium and non-equilibrium condensates, and to the theoretical understanding of the KPZ mapping.","sentences":["In the absence of vortices or phase slips, the phase dynamics of exciton-polariton condensates was shown to map onto the Kardar-Parisi-Zhang (KPZ) equation, which describes the stochastic growth of a classical interface.","This implies that the coherence of such non-equilibrium quasi-condensates decays in space and time following stretched exponentials, characterized by KPZ universal critical exponents.","In this work, we focus on the time evolution of the average phase of a one-dimensional exciton-polariton condensate in the KPZ regime and determine the frequency of its evolution, which is given by the blueshift, i.e. the non-equilibrium analog of the chemical potential.","We determine the stochastic corrections to the blueshift within Bogoliubov linearized theory and find that while this correction physically originates from short scale effects, and depends both on density and phase fluctuations, it can still be related to the effective large-scale KPZ parameters.","Using numerical simulations of the full dynamics, we investigate the dependence of these blueshift corrections on both noise and interaction strength, and compare the results to the Bogoliubov prediction.","Our finding contributes both to the close comparison between equilibrium and non-equilibrium condensates, and to the theoretical understanding of the KPZ mapping."],"url":"http://arxiv.org/abs/2402.15316v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-23 10:56:47","title":"High Resolution Guitar Transcription via Domain Adaptation","abstract":"Automatic music transcription (AMT) has achieved high accuracy for piano due to the availability of large, high-quality datasets such as MAESTRO and MAPS, but comparable datasets are not yet available for other instruments. In recent work, however, it has been demonstrated that aligning scores to transcription model activations can produce high quality AMT training data for instruments other than piano. Focusing on the guitar, we refine this approach to training on score data using a dataset of commercially available score-audio pairs. We propose the use of a high-resolution piano transcription model to train a new guitar transcription model. The resulting model obtains state-of-the-art transcription results on GuitarSet in a zero-shot context, improving on previously published methods.","sentences":["Automatic music transcription (AMT) has achieved high accuracy for piano due to the availability of large, high-quality datasets such as MAESTRO and MAPS, but comparable datasets are not yet available for other instruments.","In recent work, however, it has been demonstrated that aligning scores to transcription model activations can produce high quality AMT training data for instruments other than piano.","Focusing on the guitar, we refine this approach to training on score data using a dataset of commercially available score-audio pairs.","We propose the use of a high-resolution piano transcription model to train a new guitar transcription model.","The resulting model obtains state-of-the-art transcription results on GuitarSet in a zero-shot context, improving on previously published methods."],"url":"http://arxiv.org/abs/2402.15258v1","category":"eess.AS"}
{"created":"2024-02-23 10:50:04","title":"Quasi-likelihood analysis for adaptive estimation of a degenerate diffusion process","abstract":"The adaptive quasi-likelihood analysis is developed for a degenerate diffusion process. Asymptotic normality and moment convergence are proved for the quasi-maximum likelihood estimators and quasi-Bayesian estimators, in the adaptive scheme.","sentences":["The adaptive quasi-likelihood analysis is developed for a degenerate diffusion process.","Asymptotic normality and moment convergence are proved for the quasi-maximum likelihood estimators and quasi-Bayesian estimators, in the adaptive scheme."],"url":"http://arxiv.org/abs/2402.15256v1","category":"math.ST"}
{"created":"2024-02-23 10:01:22","title":"Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning","abstract":"Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.","sentences":["Unsupervised domain adaptation (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain domain-invariant predictive models.","Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis.","This paper proposes a simple yet potent contrastive learning framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution.","Our method is validated on cerebral vessel datasets.","Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data."],"url":"http://arxiv.org/abs/2402.15237v1","category":"cs.CV"}
{"created":"2024-02-23 09:07:20","title":"Source-Guided Similarity Preservation for Online Person Re-Identification","abstract":"Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks.","sentences":["Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source domain dataset to a target domain observed as a data stream.","In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift.","In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems.","Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data.","This support set is used to identify feature similarities that must be preserved during the learning process.","S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting.","Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks."],"url":"http://arxiv.org/abs/2402.15206v1","category":"cs.CV"}
{"created":"2024-02-23 08:21:02","title":"Advancing Parameter Efficiency in Fine-tuning via Representation Editing","abstract":"Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2, and the results demonstrate the efficiency and efficacy of RED, positioning it as a promising PEFT approach for large neural models.","sentences":["Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters.","Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts.","In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer.","RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA.","Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods.","Extensive experiments were conducted across models of varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2, and the results demonstrate the efficiency and efficacy of RED, positioning it as a promising PEFT approach for large neural models."],"url":"http://arxiv.org/abs/2402.15179v1","category":"cs.LG"}
{"created":"2024-02-23 18:12:53","title":"Transformers are Expressive, But Are They Expressive Enough for Regression?","abstract":"Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: \"\\textit{Are Transformers truly Universal Function Approximators}?\" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysis pinpointing the root of Transformers' limitation in function approximation and extensive experiments to verify the limitation. By shedding light on these challenges, we advocate a refined understanding of Transformers' capabilities.","sentences":["Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization.","Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers.","Expressivity of a neural network is the class of functions it can approximate.","A neural network is fully expressive if it can act as a universal function approximator.","We attempt to analyze the same for Transformers.","Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals.","The central question emerges as: \"\\textit{Are Transformers truly Universal Function Approximators}?\"","To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments.","Our contributions include a theoretical analysis pinpointing the root of Transformers' limitation in function approximation and extensive experiments to verify the limitation.","By shedding light on these challenges, we advocate a refined understanding of Transformers' capabilities."],"url":"http://arxiv.org/abs/2402.15478v1","category":"cs.LG"}
{"created":"2024-02-23 18:10:18","title":"The dichotomy of Nikodym sets and local smoothing estimates for wave equations","abstract":"We show that Nikodym sets and local smoothing estimates for linear wave equations form a dichotomy: If Nikodym sets for a family of curves exist, then the related maximal operator is not bounded on $L^p(\\mathbb{R}^2)$ for any $p<\\infty$; if Nikodym sets do not exist, then local smoothing estimates hold, and the related maximal operator is bounded on $L^p(\\mathbb{R}^2)$ for some $p<\\infty$. Whenever the maximal operator is bounded on $L^p(\\mathbb{R}^2)$ for some $p<\\infty$, we also determine the sharp exponent for $L^p(\\mathbb{R}^2)$ bounds.","sentences":["We show that Nikodym sets and local smoothing estimates for linear wave equations form a dichotomy: If Nikodym sets for a family of curves exist, then the related maximal operator is not bounded on $L^p(\\mathbb{R}^2)$ for any $p<\\infty$; if Nikodym sets do not exist, then local smoothing estimates hold, and the related maximal operator is bounded on $L^p(\\mathbb{R}^2)$ for some $p<\\infty$. Whenever the maximal operator is bounded on $L^p(\\mathbb{R}^2)$ for some $p<\\infty$, we also determine the sharp exponent for $L^p(\\mathbb{R}^2)$ bounds."],"url":"http://arxiv.org/abs/2402.15476v1","category":"math.CA"}
{"created":"2024-02-23 18:06:26","title":"Functional renormalization group study of the quark-meson model with omega and rho vector mesons","abstract":"The functional renormalization group (FRG) is a non-perturbative method that considers quantum and thermal fluctuations. Using the FRG flow equations, the critical region of the two-flavor quark-meson model in a finite isospin chemical potential with omega and rho vector mesons interactions is investigated in this work. We also use the traditional mean-field method to calculate the phase diagram in the chiral limit for comparison. The results show that the influences of the omega meson and rho meson on the phase structure are quite different. The existence of the isospin chemical potential also causes significant changes in the phase structure.","sentences":["The functional renormalization group (FRG) is a non-perturbative method that considers quantum and thermal fluctuations.","Using the FRG flow equations, the critical region of the two-flavor quark-meson model in a finite isospin chemical potential with omega and rho vector mesons interactions is investigated in this work.","We also use the traditional mean-field method to calculate the phase diagram in the chiral limit for comparison.","The results show that the influences of the omega meson and rho meson on the phase structure are quite different.","The existence of the isospin chemical potential also causes significant changes in the phase structure."],"url":"http://arxiv.org/abs/2402.15474v1","category":"hep-ph"}
{"created":"2024-02-23 17:25:10","title":"Repetition Improves Language Model Embeddings","abstract":"Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, \"echo embeddings,\" in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared to prior open source models that do not leverage synthetic fine-tuning data.","sentences":["Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions.","In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input.","To address this limitation, we propose a simple approach, \"echo embeddings,\" in which we repeat the input twice in context and extract embeddings from the second occurrence.","We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings.","On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned.","Echo embeddings with a Mistral-7B model achieve state-of-the-art compared to prior open source models that do not leverage synthetic fine-tuning data."],"url":"http://arxiv.org/abs/2402.15449v1","category":"cs.CL"}
{"created":"2024-02-23 17:16:02","title":"High-order accurate positivity-preserving and well-balanced discontinuous Galerkin schemes for ten-moment Gaussian closure equations with source terms","abstract":"This paper proposes novel high-order accurate discontinuous Galerkin (DG) schemes for the one- and two-dimensional ten-moment Gaussian closure equations with source terms defined by a known potential function. Our DG schemes exhibit the desirable capability of being well-balanced (WB) for a known hydrostatic equilibrium state while simultaneously preserving positive density and positive-definite anisotropic pressure tensor. The well-balancedness is built on carefully modifying the solution states in the Harten-Lax-van Leer-contact (HLLC) flux, and appropriate reformulation and discretization of the source terms. Our novel modification technique overcomes the difficulties posed by the anisotropic effects, maintains the high-order accuracy, and ensures that the modified solution state remains within the physically admissible state set. Positivity-preserving analyses of our WB DG schemes are conducted by using several key properties of the admissible state set, the HLLC flux and the HLLC solver, as well as the geometric quasilinearization (GQL) approach in [Wu & Shu, SIAM Review, 65: 1031-1073, 2023], which was originally applied to analyze the admissible state set and physical-constraints-preserving schemes for the relativistic magnetohydrodynamics in [Wu & Tang, M3AS, 27: 1871-1928, 2017], to address the difficulties arising from the nonlinear constraints on pressure tensor. Moreover, the proposed WB DG schemes satisfy the weak positivity for the cell averages, implying the use of a scaling limiter to enforce the physical admissibility of the DG solution polynomials at certain points of interest. Extensive numerical experiments are conducted to validate the preservation of equilibrium states, accuracy in capturing small perturbations to such states, robustness in solving problems involving low density or low pressure, and high resolution for both smooth and discontinuous solutions.","sentences":["This paper proposes novel high-order accurate discontinuous Galerkin (DG) schemes for the one- and two-dimensional ten-moment Gaussian closure equations with source terms defined by a known potential function.","Our DG schemes exhibit the desirable capability of being well-balanced (WB) for a known hydrostatic equilibrium state while simultaneously preserving positive density and positive-definite anisotropic pressure tensor.","The well-balancedness is built on carefully modifying the solution states in the Harten-Lax-van Leer-contact (HLLC) flux, and appropriate reformulation and discretization of the source terms.","Our novel modification technique overcomes the difficulties posed by the anisotropic effects, maintains the high-order accuracy, and ensures that the modified solution state remains within the physically admissible state set.","Positivity-preserving analyses of our WB DG schemes are conducted by using several key properties of the admissible state set, the HLLC flux and the HLLC solver, as well as the geometric quasilinearization (GQL) approach in [Wu & Shu, SIAM Review, 65: 1031-1073, 2023], which was originally applied to analyze the admissible state set and physical-constraints-preserving schemes for the relativistic magnetohydrodynamics in [Wu & Tang, M3AS, 27: 1871-1928, 2017], to address the difficulties arising from the nonlinear constraints on pressure tensor.","Moreover, the proposed WB DG schemes satisfy the weak positivity for the cell averages, implying the use of a scaling limiter to enforce the physical admissibility of the DG solution polynomials at certain points of interest.","Extensive numerical experiments are conducted to validate the preservation of equilibrium states, accuracy in capturing small perturbations to such states, robustness in solving problems involving low density or low pressure, and high resolution for both smooth and discontinuous solutions."],"url":"http://arxiv.org/abs/2402.15446v1","category":"math.NA"}
{"created":"2024-02-23 16:45:23","title":"Rota--Baxter operators and skew left brace structures over Heisenberg Group","abstract":"Rota--Baxter operators over groups have been recently defined in \\cite{LHY2021}, and they share a close connection with skew braces, as demonstrated in \\cite{VV2022}. In this paper, we classify all Rota--Baxter operators of weight 1 over the Heisenberg Lie algebra of dimension 3 by directly solving the operators defining equations. Using the fact that the exponential map from the Heisenberg Lie algebra to the Heisenberg Group is bijective, we induces these operators to the Heisenberg Group. Finally, we enumerate all skew left brace structures over the Heisenberg Group induced by these Rota--Baxter operators.","sentences":["Rota--Baxter operators over groups have been recently defined in \\cite{LHY2021}, and they share a close connection with skew braces, as demonstrated in \\cite{VV2022}.","In this paper, we classify all Rota--Baxter operators of weight 1 over the Heisenberg Lie algebra of dimension 3 by directly solving the operators defining equations.","Using the fact that the exponential map from the Heisenberg Lie algebra to the Heisenberg Group is bijective, we induces these operators to the Heisenberg Group.","Finally, we enumerate all skew left brace structures over the Heisenberg Group induced by these Rota--Baxter operators."],"url":"http://arxiv.org/abs/2402.15428v1","category":"math.RA"}
{"created":"2024-02-23 14:39:12","title":"On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey","abstract":"Image denoising is probably the oldest and still one of the most active research topic in image processing. Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of convolutional neural networks and supervised deep learning. In this paper, we propose a survey of guided tour of supervised and unsupervised learning methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in supervised learning. It is conceived as a tutorial organizing in a comprehensive framework current approaches. We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them. Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed with most of supervised methods. It is of paramount importance that intensity shifting or scaling applied to the input image results in a corresponding change in the denoiser output.","sentences":["Image denoising is probably the oldest and still one of the most active research topic in image processing.","Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of convolutional neural networks and supervised deep learning.","In this paper, we propose a survey of guided tour of supervised and unsupervised learning methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in supervised learning.","It is conceived as a tutorial organizing in a comprehensive framework current approaches.","We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them.","Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed with most of supervised methods.","It is of paramount importance that intensity shifting or scaling applied to the input image results in a corresponding change in the denoiser output."],"url":"http://arxiv.org/abs/2402.15352v1","category":"cs.CV"}
{"created":"2024-02-23 13:39:16","title":"GPTVQ: The Blessing of Dimensionality for LLM Quantization","abstract":"In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format.","sentences":["In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality.","We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs).","Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE.","Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm.","The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression.","GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral.","Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization setting.","Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format."],"url":"http://arxiv.org/abs/2402.15319v1","category":"cs.LG"}
{"created":"2024-02-23 12:35:43","title":"Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models","abstract":"Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models. Our code is publicly available at https://github.com/Qlb6x/DiffusionABSA.","sentences":["Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text.","However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions.","We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step.","Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner.","To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text.","Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models.","Our code is publicly available at https://github.com/Qlb6x/DiffusionABSA."],"url":"http://arxiv.org/abs/2402.15289v1","category":"cs.CL"}
{"created":"2024-02-23 12:06:48","title":"Neural Implicit Swept Volume Models for Fast Collision Detection","abstract":"Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application.","sentences":["Collision detection is one of the most time-consuming operations during motion planning.","Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning.","A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion.","Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations.","This allows to quickly compute signed distances for any point in the task space to the robot motion.","Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers.","We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application."],"url":"http://arxiv.org/abs/2402.15281v1","category":"cs.RO"}
{"created":"2024-02-23 11:14:00","title":"Exploring Gamma-Ray Burst Diversity: Clustering analysis of emission characteristics of Fermi and BATSE detected GRBs","abstract":"Gamma-ray bursts (GRBs) are commonly attributed to the demise of massive stars or the merger of binary compact objects. However, their varied emission characteristics strongly imply the existence of multiple GRB classes based on progenitor types, radiation mechanisms, central engines etc. This study utilizes unsupervised clustering with the Nested Gaussian Mixture Model algorithm to analyze {\\it Fermi} and BATSE GRB data, identifying four classes (A, B, C, and D) based on duration, spectral peak, and spectral index, comprising approximately 70\\%, 10\\%, 3\\%, and 17\\% of the dataset, respectively. Classes A and B consist of long GRBs, C mainly short GRBs, and class D encompasses both short and long GRBs. Using the spectral index, $\\alpha$, for the differentiation of radiation models, it is found that classes B and C align with photospheric emission models, while A and D predominantly show synchrotron radiation characteristics. Short GRBs predominantly exhibit photospheric emission, whereas long GRBs show consistency with synchrotron emission. Overall, 63\\% of the total bursts exhibit $\\alpha$ profiles indicative of synchrotron emission, with the remaining 37\\% associated with photospheric emission. The classes were further examined for their progenitor origins, revealing that classes A and D demonstrate a hybrid nature, while classes B and C are predominantly associated with collapsar and merger origins, respectively. This clustering analysis reveals distinct GRB classes, shedding light on their diversity in radiation, duration and progenitor.","sentences":["Gamma-ray bursts (GRBs) are commonly attributed to the demise of massive stars or the merger of binary compact objects.","However, their varied emission characteristics strongly imply the existence of multiple GRB classes based on progenitor types, radiation mechanisms, central engines etc.","This study utilizes unsupervised clustering with the Nested Gaussian Mixture Model algorithm to analyze {\\it Fermi} and BATSE GRB data, identifying four classes (A, B, C, and D) based on duration, spectral peak, and spectral index, comprising approximately 70\\%, 10\\%, 3\\%, and 17\\% of the dataset, respectively.","Classes A and B consist of long GRBs, C mainly short GRBs, and class D encompasses both short and long GRBs.","Using the spectral index, $\\alpha$, for the differentiation of radiation models, it is found that classes B and C align with photospheric emission models, while A and D predominantly show synchrotron radiation characteristics.","Short GRBs predominantly exhibit photospheric emission, whereas long GRBs show consistency with synchrotron emission.","Overall, 63\\% of the total bursts exhibit $\\alpha$ profiles indicative of synchrotron emission, with the remaining 37\\% associated with photospheric emission.","The classes were further examined for their progenitor origins, revealing that classes","A and D demonstrate a hybrid nature, while classes B and C are predominantly associated with collapsar and merger origins, respectively.","This clustering analysis reveals distinct GRB classes, shedding light on their diversity in radiation, duration and progenitor."],"url":"http://arxiv.org/abs/2402.15260v1","category":"astro-ph.HE"}
{"created":"2024-02-23 10:00:25","title":"Font Impression Estimation in the Wild","abstract":"This paper addresses the challenging task of estimating font impressions from real font images. We use a font dataset with annotation about font impressions and a convolutional neural network (CNN) framework for this task. However, impressions attached to individual fonts are often missing and noisy because of the subjective characteristic of font impression annotation. To realize stable impression estimation even with such a dataset, we propose an exemplar-based impression estimation approach, which relies on a strategy of ensembling impressions of exemplar fonts that are similar to the input image. In addition, we train CNN with synthetic font images that mimic scanned word images so that CNN estimates impressions of font images in the wild. We evaluate the basic performance of the proposed estimation method quantitatively and qualitatively. Then, we conduct a correlation analysis between book genres and font impressions on real book cover images; it is important to note that this analysis is only possible with our impression estimation method. The analysis reveals various trends in the correlation between them - this fact supports a hypothesis that book cover designers carefully choose a font for a book cover considering the impression given by the font.","sentences":["This paper addresses the challenging task of estimating font impressions from real font images.","We use a font dataset with annotation about font impressions and a convolutional neural network (CNN) framework for this task.","However, impressions attached to individual fonts are often missing and noisy because of the subjective characteristic of font impression annotation.","To realize stable impression estimation even with such a dataset, we propose an exemplar-based impression estimation approach, which relies on a strategy of ensembling impressions of exemplar fonts that are similar to the input image.","In addition, we train CNN with synthetic font images that mimic scanned word images so that CNN estimates impressions of font images in the wild.","We evaluate the basic performance of the proposed estimation method quantitatively and qualitatively.","Then, we conduct a correlation analysis between book genres and font impressions on real book cover images; it is important to note that this analysis is only possible with our impression estimation method.","The analysis reveals various trends in the correlation between them - this fact supports a hypothesis that book cover designers carefully choose a font for a book cover considering the impression given by the font."],"url":"http://arxiv.org/abs/2402.15236v1","category":"cs.CV"}
{"created":"2024-02-23 09:19:26","title":"Statistical Agnostic Regression: a machine learning method to validate regression models","abstract":"Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introduce a method, named Statistical Agnostic Regression (SAR), for evaluating the statistical significance of an ML-based linear regression based on concentration inequalities of the actual risk using the analysis of the worst case. To achieve this goal, similar to the classification problem, we define a threshold to establish that there is sufficient evidence with a probability of at least 1-eta to conclude that there is a linear relationship in the population between the explanatory (feature) and the response (label) variables. Simulations in only two dimensions demonstrate the ability of the proposed agnostic test to provide a similar analysis of variance given by the classical $F$ test for the slope parameter.","sentences":["Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables.","Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference.","Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance.","At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection.","In this paper, we introduce a method, named Statistical Agnostic Regression (SAR), for evaluating the statistical significance of an ML-based linear regression based on concentration inequalities of the actual risk using the analysis of the worst case.","To achieve this goal, similar to the classification problem, we define a threshold to establish that there is sufficient evidence with a probability of at least 1-eta to conclude that there is a linear relationship in the population between the explanatory (feature) and the response (label) variables.","Simulations in only two dimensions demonstrate the ability of the proposed agnostic test to provide a similar analysis of variance given by the classical $F$ test for the slope parameter."],"url":"http://arxiv.org/abs/2402.15213v1","category":"stat.ML"}
{"created":"2024-02-23 09:09:53","title":"On the regularity of weak solution for the Oberbeck-Boussinesq equations","abstract":"We prove new regularity criteria of the Prodi-Serrin type with weak Lebesgue integrability in both space and time for a viscous active chemical fluid in a bounded domain.","sentences":["We prove new regularity criteria of the Prodi-Serrin type with weak Lebesgue integrability in both space and time for a viscous active chemical fluid in a bounded domain."],"url":"http://arxiv.org/abs/2402.15207v1","category":"math.AP"}
{"created":"2024-02-23 16:51:17","title":"Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models","abstract":"Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such mixtures, we establish that Bregman hard clustering, a variant of Lloyd's algorithm employing a Bregman divergence, is rate optimal.","sentences":["Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models.","The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios.","Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate.","In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios.","We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors.","Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family.","In such mixtures, we establish that Bregman hard clustering, a variant of Lloyd's algorithm employing a Bregman divergence, is rate optimal."],"url":"http://arxiv.org/abs/2402.15432v1","category":"math.ST"}
{"created":"2024-02-23 16:06:38","title":"United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once","abstract":"In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets, even from 75 at once.","sentences":["In natural language processing and vision, pretraining is utilized to learn effective representations.","Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target.","Actually, common belief is that multi-dataset pretraining does not work for time series!","Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification.","Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss.","Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes.","This disproves the common belief: We can actually learn from multiple time series datasets, even from 75 at once."],"url":"http://arxiv.org/abs/2402.15404v1","category":"cs.LG"}
{"created":"2024-02-23 15:27:29","title":"Extended Fayans energy density functional: optimization and analysis","abstract":"The Fayans energy density functional (EDF) has been very successful in describing global nuclear properties (binding energies, charge radii, and especially differences of radii) within nuclear density functional theory. In a recent study, supervised machine learning methods were used to calibrate the Fayans EDF. Building on this experience, in this work we explore the effect of adding isovector pairing terms, which are responsible for different proton and neutron pairing fields, by comparing a 13D model without the isovector pairing term against the extended 14D model. At the heart of the calibration is a carefully selected heterogeneous dataset of experimental observables representing ground-state properties of spherical even-even nuclei. To quantify the impact of the calibration dataset on model parameters and the importance of the new terms, we carry out advanced sensitivity and correlation analysis on both models. The extension to 14D improves the overall quality of the model by about 30%. The enhanced degrees of freedom of the 14D model reduce correlations between model parameters and enhance sensitivity.","sentences":["The Fayans energy density functional (EDF) has been very successful in describing global nuclear properties (binding energies, charge radii, and especially differences of radii) within nuclear density functional theory.","In a recent study, supervised machine learning methods were used to calibrate the Fayans EDF.","Building on this experience, in this work we explore the effect of adding isovector pairing terms, which are responsible for different proton and neutron pairing fields, by comparing a 13D model without the isovector pairing term against the extended 14D model.","At the heart of the calibration is a carefully selected heterogeneous dataset of experimental observables representing ground-state properties of spherical even-even nuclei.","To quantify the impact of the calibration dataset on model parameters and the importance of the new terms, we carry out advanced sensitivity and correlation analysis on both models.","The extension to 14D improves the overall quality of the model by about 30%.","The enhanced degrees of freedom of the 14D model reduce correlations between model parameters and enhance sensitivity."],"url":"http://arxiv.org/abs/2402.15380v1","category":"nucl-th"}
{"created":"2024-02-23 15:27:23","title":"Identifying probabilistic weather regimes targeted to a local-scale impact variable","abstract":"Weather regimes are recurrent and persistent large-scale atmospheric circulation patterns that modulate the occurrence of local impact variables such as extreme precipitation. In their capacity as mediators between long-range teleconnections and these local extremes, they have shown potential for improving sub-seasonal forecasting as well as long-term climate projections. However, existing methods for identifying weather regimes are not designed to capture the physical processes relevant to the impact variable in question while still representing the full atmospheric phase space. This paper introduces a novel probabilistic machine learning method, RMM-VAE, for identifying weather regimes targeted to a local-scale impact variable. Based on a variational autoencoder architecture, the method combines non-linear dimensionality reduction with a prediction task and probabilistic clustering in a coherent architecture. The new method is applied to identify circulation patterns over the Mediterranean region targeted to precipitation over Morocco and compared to three existing approaches, two established linear methods and another machine learning approach. The RMM-VAE method identifies regimes that are more predictive of the target variable compared to the two linear methods, and more robust and persistent compared to the alternative machine learning method, while also improving the reconstruction of the input space. The results demonstrate the potential benefit of the new method for use in various climate applications such as sub-seasonal forecasting, while also highlighting the trade-offs involved in targeted clustering.","sentences":["Weather regimes are recurrent and persistent large-scale atmospheric circulation patterns that modulate the occurrence of local impact variables such as extreme precipitation.","In their capacity as mediators between long-range teleconnections and these local extremes, they have shown potential for improving sub-seasonal forecasting as well as long-term climate projections.","However, existing methods for identifying weather regimes are not designed to capture the physical processes relevant to the impact variable in question while still representing the full atmospheric phase space.","This paper introduces a novel probabilistic machine learning method, RMM-VAE, for identifying weather regimes targeted to a local-scale impact variable.","Based on a variational autoencoder architecture, the method combines non-linear dimensionality reduction with a prediction task and probabilistic clustering in a coherent architecture.","The new method is applied to identify circulation patterns over the Mediterranean region targeted to precipitation over Morocco and compared to three existing approaches, two established linear methods and another machine learning approach.","The RMM-VAE method identifies regimes that are more predictive of the target variable compared to the two linear methods, and more robust and persistent compared to the alternative machine learning method, while also improving the reconstruction of the input space.","The results demonstrate the potential benefit of the new method for use in various climate applications such as sub-seasonal forecasting, while also highlighting the trade-offs involved in targeted clustering."],"url":"http://arxiv.org/abs/2402.15379v1","category":"physics.geo-ph"}
{"created":"2024-02-23 14:17:01","title":"Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies","abstract":"Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results. We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom.","sentences":["Conceptual spaces represent entities in terms of their primitive semantic features.","Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features.","Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy.","However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies.","We focus in particular on the task of ranking entities according to a given conceptual space dimension.","Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare.","We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features.","We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results.","We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom."],"url":"http://arxiv.org/abs/2402.15337v1","category":"cs.CL"}
{"created":"2024-02-23 12:56:01","title":"Structure prediction of stable sodium germanides at 0 and 10 GPa","abstract":"In this work we used $\\textit{ab-initio}$ random structure searching (AIRSS) to carry out a systematic search for crystalline Na-Ge materials at both 0 and 10 GPa. The high-throughput structural relaxations were accelerated using a machine-learned interatomic potential (MLIP) fit to density-functional theory (DFT) reference data, allowing $\\sim$1.5 million structures to be relaxed. At ambient conditions we predict three new Zintl phases, Na$_3$Ge$_2$, Na$_2$Ge and Na$_9$Ge$_4$, to be stable and a number of Ge-rich layered structures to lie in close proximity to the convex hull. The known Na$_\\delta$Ge$_{34}$ clathrate and Na$_4$Ge$_{13}$ host-guest structures are found to be relatively stabilized at higher temperature by vibrational contributions to the free energy. Overall, the low energy phases exhibit exceptional structural diversity, with the expected mixture of covalent and ionic bonding confirmed using the electron-localisation function (ELF). The local Ge structural motifs present at each composition were determined using Smooth Overlap of Atomic Positions (SOAP) descriptors and the Ge-K edge was simulated for representatives of each motif, providing a direct link to experimental x-ray absorption spectroscopy (XAS). Two Ge-rich phases are predicted to be stable at 10 GPa; NaGe$_3$ and NaGe$_2$ have simple kagome and simple hexagonal Ge lattices respectively with Na contained in the pores. NaGe$_3$ is isostructural with the MgB$_3$ and MgSi$_3$ family of kagome superconductors and remains dynamically stable at 0 GPa. Removing the Na from NaGe$_2$ results in the hexagonal lonsdalite Ge allotrope, which has a direct band gap.","sentences":["In this work we used $\\textit{ab-initio}$ random structure searching (AIRSS) to carry out a systematic search for crystalline Na-Ge materials at both 0 and 10 GPa.","The high-throughput structural relaxations were accelerated using a machine-learned interatomic potential (MLIP) fit to density-functional theory (DFT) reference data, allowing $\\sim$1.5 million structures to be relaxed.","At ambient conditions we predict three new Zintl phases, Na$_3$Ge$_2$, Na$_2$Ge and Na$_9$Ge$_4$, to be stable and a number of Ge-rich layered structures to lie in close proximity to the convex hull.","The known Na$_\\delta$Ge$_{34}$ clathrate and Na$_4$Ge$_{13}$ host-guest structures are found to be relatively stabilized at higher temperature by vibrational contributions to the free energy.","Overall, the low energy phases exhibit exceptional structural diversity, with the expected mixture of covalent and ionic bonding confirmed using the electron-localisation function (ELF).","The local Ge structural motifs present at each composition were determined using Smooth Overlap of Atomic Positions (SOAP) descriptors and the Ge-K edge was simulated for representatives of each motif, providing a direct link to experimental x-ray absorption spectroscopy (XAS).","Two Ge-rich phases are predicted to be stable at 10 GPa; NaGe$_3$ and NaGe$_2$ have simple kagome and simple hexagonal Ge lattices respectively with Na contained in the pores.","NaGe$_3$ is isostructural with the MgB$_3$ and MgSi$_3$ family of kagome superconductors and remains dynamically stable at 0 GPa.","Removing the Na from NaGe$_2$ results in the hexagonal lonsdalite Ge allotrope, which has a direct band gap."],"url":"http://arxiv.org/abs/2402.15299v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-23 09:47:27","title":"Which Model to Transfer? A Survey on Transferability Estimation","abstract":"Transfer learning methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks. With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task. Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive. Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings. In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation. Each category is systematically defined, accompanied by a comprehensive taxonomy. Besides, we address challenges and outline future research directions, intending to provide a comprehensive guide to aid researchers and practitioners.","sentences":["Transfer learning methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks.","With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task.","Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive.","Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings.","In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation.","Each category is systematically defined, accompanied by a comprehensive taxonomy.","Besides, we address challenges and outline future research directions, intending to provide a comprehensive guide to aid researchers and practitioners."],"url":"http://arxiv.org/abs/2402.15231v1","category":"cs.LG"}
{"created":"2024-02-23 08:23:26","title":"Substrate Prediction for RiPP Biosynthetic Enzymes via Masked Language Modeling and Transfer Learning","abstract":"Ribosomally synthesized and post-translationally modified peptide (RiPP) biosynthetic enzymes often exhibit promiscuous substrate preferences that cannot be reduced to simple rules. Large language models are promising tools for predicting such peptide fitness landscapes. However, state-of-the-art protein language models are trained on relatively few peptide sequences. A previous study comprehensively profiled the peptide substrate preferences of LazBF (a two-component serine dehydratase) and LazDEF (a three-component azole synthetase) from the lactazole biosynthetic pathway. We demonstrated that masked language modeling of LazBF substrate preferences produced language model embeddings that improved downstream classification models of both LazBF and LazDEF substrates. Similarly, masked language modeling of LazDEF substrate preferences produced embeddings that improved the performance of classification models of both LazBF and LazDEF substrates. Our results suggest that the models learned functional forms that are transferable between distinct enzymatic transformations that act within the same biosynthetic pathway. Our transfer learning method improved performance and data efficiency in data-scarce scenarios. We then fine-tuned models on each data set and showed that the fine-tuned models provided interpretable insight that we anticipate will facilitate the design of substrate libraries that are compatible with desired RiPP biosynthetic pathways.","sentences":["Ribosomally synthesized and post-translationally modified peptide (RiPP) biosynthetic enzymes often exhibit promiscuous substrate preferences that cannot be reduced to simple rules.","Large language models are promising tools for predicting such peptide fitness landscapes.","However, state-of-the-art protein language models are trained on relatively few peptide sequences.","A previous study comprehensively profiled the peptide substrate preferences of LazBF (a two-component serine dehydratase) and LazDEF (a three-component azole synthetase) from the lactazole biosynthetic pathway.","We demonstrated that masked language modeling of LazBF substrate preferences produced language model embeddings that improved downstream classification models of both LazBF and LazDEF substrates.","Similarly, masked language modeling of LazDEF substrate preferences produced embeddings that improved the performance of classification models of both LazBF and LazDEF substrates.","Our results suggest that the models learned functional forms that are transferable between distinct enzymatic transformations that act within the same biosynthetic pathway.","Our transfer learning method improved performance and data efficiency in data-scarce scenarios.","We then fine-tuned models on each data set and showed that the fine-tuned models provided interpretable insight that we anticipate will facilitate the design of substrate libraries that are compatible with desired RiPP biosynthetic pathways."],"url":"http://arxiv.org/abs/2402.15181v1","category":"q-bio.QM"}
{"created":"2024-02-23 18:25:54","title":"Graph Partitioning With Limited Moves","abstract":"In many real world networks, there already exists a (not necessarily optimal) $k$-partitioning of the network. Oftentimes, one aims to find a $k$-partitioning with a smaller cut value for such networks by moving only a few nodes across partitions. The number of nodes that can be moved across partitions is often a constraint forced by budgetary limitations. Motivated by such real-world applications, we introduce and study the $r$-move $k$-partitioning~problem, a natural variant of the Multiway cut problem. Given a graph, a set of $k$ terminals and an initial partitioning of the graph, the $r$-move $k$-partitioning~problem aims to find a $k$-partitioning with the minimum-weighted cut among all the $k$-partitionings that can be obtained by moving at most $r$ non-terminal nodes to partitions different from their initial ones. Our main result is a polynomial time $3(r+1)$ approximation algorithm for this problem. We further show that this problem is $W[1]$-hard, and give an FPTAS for when $r$ is a small constant.","sentences":["In many real world networks, there already exists a (not necessarily optimal) $k$-partitioning of the network.","Oftentimes, one aims to find a $k$-partitioning with a smaller cut value for such networks by moving only a few nodes across partitions.","The number of nodes that can be moved across partitions is often a constraint forced by budgetary limitations.","Motivated by such real-world applications, we introduce and study the $r$-move $k$-partitioning~problem, a natural variant of the Multiway cut problem.","Given a graph, a set of $k$ terminals and an initial partitioning of the graph, the $r$-move $k$-partitioning~problem aims to find a $k$-partitioning with the minimum-weighted cut among all the $k$-partitionings that can be obtained by moving at most $r$ non-terminal nodes to partitions different from their initial ones.","Our main result is a polynomial time $3(r+1)$ approximation algorithm for this problem.","We further show that this problem is $W[1]$-hard, and give an FPTAS for when $r$ is a small constant."],"url":"http://arxiv.org/abs/2402.15485v1","category":"cs.DS"}
{"created":"2024-02-23 16:06:16","title":"Stimulated Forward Brillouin Scattering in Subwavelength Silicon Membranes","abstract":"On-chip Brillouin scattering plays a key role in numerous applications in the domain of signal processing and microwave photonics due to the coherent bidirectional coupling between near-infrared optical signals and GHz mechanical modes, which enables selective amplification and attenuation with remarkably narrow linewidths, in the kHz to MHz range. Subwavelength periodic nanostructures provide precise control of the propagation of light and sound in silicon photonic circuits, key to maximize the efficiency of Brillouin interactions. Here, we propose and demonstrate a new subwavelength waveguide geometry allowing independent control of optical and mechanical modes. Two silicon lattices are combined, one with a subwavelength period for the light and one with a total bandgap for the sound, to confine optical and mechanical modes, respectively. Based on this approach, we experimentally demonstrate optomechanical coupling between near-infrared optical modes and GHz mechanical modes with with 5-8 MHz linewidth and a coupling strength of GB = 1360 1/(W m). A Stokes gain of 1.5 dB, and anti-Stoke loss of -2 dB are observed for a 6 mm-long waveguide with 35.5 mW of input power. We show tuning of the mechanical frequency between 5 and 8 GHz by geometrical optimization, without loss of the optomechanical coupling strength.","sentences":["On-chip Brillouin scattering plays a key role in numerous applications in the domain of signal processing and microwave photonics due to the coherent bidirectional coupling between near-infrared optical signals and GHz mechanical modes, which enables selective amplification and attenuation with remarkably narrow linewidths, in the kHz to MHz range.","Subwavelength periodic nanostructures provide precise control of the propagation of light and sound in silicon photonic circuits, key to maximize the efficiency of Brillouin interactions.","Here, we propose and demonstrate a new subwavelength waveguide geometry allowing independent control of optical and mechanical modes.","Two silicon lattices are combined, one with a subwavelength period for the light and one with a total bandgap for the sound, to confine optical and mechanical modes, respectively.","Based on this approach, we experimentally demonstrate optomechanical coupling between near-infrared optical modes and GHz mechanical modes with with 5-8 MHz linewidth and a coupling strength of GB = 1360 1/(W m).","A Stokes gain of 1.5 dB, and anti-Stoke loss of -2 dB are observed for a 6 mm-long waveguide with 35.5 mW of input power.","We show tuning of the mechanical frequency between 5 and 8 GHz by geometrical optimization, without loss of the optomechanical coupling strength."],"url":"http://arxiv.org/abs/2402.15403v1","category":"physics.optics"}
{"created":"2024-02-23 14:31:52","title":"Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths","abstract":"We examine the possibility of approximating Maximum Vertex-Disjoint Shortest Paths. In this problem, the input is an edge-weighted (directed or undirected) $n$-vertex graph $G$ along with $k$ terminal pairs $(s_1,t_1),(s_2,t_2),\\ldots,(s_k,t_k)$. The task is to connect as many terminal pairs as possible by pairwise vertex-disjoint paths such that each path is a shortest path between the respective terminals. Our work is anchored in the recent breakthrough by Lochet [SODA '21], which demonstrates the polynomial-time solvability of the problem for a fixed value of $k$.   Lochet's result implies the existence of a polynomial-time $ck$-approximation for Maximum Vertex-Disjoint Shortest Paths, where $c \\leq 1$ is a constant. Our first result suggests that this approximation algorithm is, in a sense, the best we can hope for. More precisely, assuming the gap-ETH, we exclude the existence of an $o(k)$-approximations within $f(k) \\cdot $poly($n$) time for any function $f$ that only depends on $k$.   Our second result demonstrates the infeasibility of achieving an approximation ratio of $n^{\\frac{1}{2}-\\varepsilon}$ in polynomial time, unless P = NP. It is not difficult to show that a greedy algorithm selecting a path with the minimum number of arcs results in a $\\lceil\\sqrt{\\ell}\\rceil$-approximation, where $\\ell$ is the number of edges in all the paths of an optimal solution. Since $\\ell \\leq n$, this underscores the tightness of the $n^{\\frac{1}{2}-\\varepsilon}$-inapproximability bound.   Additionally, we establish that Maximum Vertex-Disjoint Shortest Paths is fixed-parameter tractable when parameterized by $\\ell$ but does not admit a polynomial kernel. Our hardness results hold for undirected graphs with unit weights, while our positive results extend to scenarios where the input graph is directed and features arbitrary (non-negative) edge weights.","sentences":["We examine the possibility of approximating Maximum Vertex-Disjoint Shortest Paths.","In this problem, the input is an edge-weighted (directed or undirected) $n$-vertex graph $G$ along with $k$ terminal pairs $(s_1,t_1),(s_2,t_2),\\ldots,(s_k,t_k)$. The task is to connect as many terminal pairs as possible by pairwise vertex-disjoint paths such that each path is a shortest path between the respective terminals.","Our work is anchored in the recent breakthrough by Lochet [SODA '21], which demonstrates the polynomial-time solvability of the problem for a fixed value of $k$.   Lochet's result implies the existence of a polynomial-time $ck$-approximation for Maximum Vertex-Disjoint Shortest Paths, where $c \\leq 1$ is a constant.","Our first result suggests that this approximation algorithm is, in a sense, the best we can hope for.","More precisely, assuming the gap-ETH, we exclude the existence of an $o(k)$-approximations within $f(k)","\\cdot $poly($n$) time for any function $f$ that only depends on $k$.   Our second result demonstrates the infeasibility of achieving an approximation ratio of $n^{\\frac{1}{2}-\\varepsilon}$ in polynomial time, unless P = NP.","It is not difficult to show that a greedy algorithm selecting a path with the minimum number of arcs results in a $\\lceil\\sqrt{\\ell}\\rceil$-approximation, where $\\ell$ is the number of edges in all the paths of an optimal solution.","Since $\\ell \\leq n$, this underscores the tightness of the $n^{\\frac{1}{2}-\\varepsilon}$-inapproximability bound.   ","Additionally, we establish that Maximum Vertex-Disjoint Shortest Paths is fixed-parameter tractable when parameterized by $\\ell$ but does not admit a polynomial kernel.","Our hardness results hold for undirected graphs with unit weights, while our positive results extend to scenarios where the input graph is directed and features arbitrary (non-negative) edge weights."],"url":"http://arxiv.org/abs/2402.15348v1","category":"cs.DS"}
{"created":"2024-02-23 14:10:41","title":"Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$ Regularization","abstract":"While iterative matrix inversion methods excel in computational efficiency, memory optimization, and support for parallel and distributed computing when managing large matrices, their limitations are also evident in multiple-input multiple-output (MIMO) fading channels. These methods encounter challenges related to slow convergence and diminished accuracy, especially in ill-conditioned scenarios, hindering their application in future MIMO networks such as extra-large aperture array (ELAA). To address these challenges, this paper proposes a novel matrix regularization method termed symmetric rank-$1$ regularization (SR-$1$R). The proposed method functions by augmenting the channel matrix with a symmetric rank-$1$ matrix, with the primary goal of minimizing the condition number of the resultant regularized matrix. This significantly improves the matrix condition, enabling fast and accurate iterative inversion of the regularized matrix. Then, the inverse of the original channel matrix is obtained by applying the Sherman-Morrison transform on the outcome of iterative inversions. Our eigenvalue analysis unveils the best channel condition that can be achieved by an optimized SR-$1$R matrix. Moreover, a power iteration-assisted (PIA) approach is proposed to find the optimum SR-$1$R matrix without need of eigenvalue decomposition. The proposed approach exhibits logarithmic algorithm-depth in parallel computing for MIMO precoding. Finally, computer simulations demonstrate that SR-$1$R has the potential to reduce iterative iterations by up to $33\\%$, while also significantly improve symbol error probability by approximately an order of magnitude.","sentences":["While iterative matrix inversion methods excel in computational efficiency, memory optimization, and support for parallel and distributed computing when managing large matrices, their limitations are also evident in multiple-input multiple-output (MIMO) fading channels.","These methods encounter challenges related to slow convergence and diminished accuracy, especially in ill-conditioned scenarios, hindering their application in future MIMO networks such as extra-large aperture array (ELAA).","To address these challenges, this paper proposes a novel matrix regularization method termed symmetric rank-$1$ regularization (SR-$1$R).","The proposed method functions by augmenting the channel matrix with a symmetric rank-$1$ matrix, with the primary goal of minimizing the condition number of the resultant regularized matrix.","This significantly improves the matrix condition, enabling fast and accurate iterative inversion of the regularized matrix.","Then, the inverse of the original channel matrix is obtained by applying the Sherman-Morrison transform on the outcome of iterative inversions.","Our eigenvalue analysis unveils the best channel condition that can be achieved by an optimized SR-$1$R matrix.","Moreover, a power iteration-assisted (PIA) approach is proposed to find the optimum SR-$1$R matrix without need of eigenvalue decomposition.","The proposed approach exhibits logarithmic algorithm-depth in parallel computing for MIMO precoding.","Finally, computer simulations demonstrate that SR-$1$R has the potential to reduce iterative iterations by up to $33\\%$, while also significantly improve symbol error probability by approximately an order of magnitude."],"url":"http://arxiv.org/abs/2402.15334v1","category":"cs.IT"}
{"created":"2024-02-23 10:43:17","title":"Improving the low-energy muon beam quality of the LEM beamline at PSI: Characterisation of ultra-thin carbon foils","abstract":"The Low-Energy Muon beamline (LEM) at the Paul Scherrer Institute currently stands as the world's only facility providing a continuous beam of low-energy muons with keV energies for conducting muon spin rotation experiments on a nanometer depth scale in heterostructures and near a sample's surface. As such, optimizing the beam quality to reach its full potential is of paramount importance. One of the ongoing efforts is dedicated to improving the already applied technique of single muon tagging through the detection of secondary electrons emerging from an ultra-thin carbon foil. In this work, we present the results from installing a thinner foil with a nominal thickness of 0.5 $\\mu g~cm^{-2}$ and compare its performance to that of the previously installed foil with a nominal thickness of 2.0 $\\mu g~cm^{-2}$. Our findings indicate improved beam quality, characterized by smaller beam spots, reduced energy loss and straggling of the muons, and enhanced tagging efficiency. Additionally, we introduce a method utilizing blue laser irradiation for cleaning the carbon foil, further improving and maintaining its characteristics","sentences":["The Low-Energy Muon beamline (LEM) at the Paul Scherrer Institute currently stands as the world's only facility providing a continuous beam of low-energy muons with keV energies for conducting muon spin rotation experiments on a nanometer depth scale in heterostructures and near a sample's surface.","As such, optimizing the beam quality to reach its full potential is of paramount importance.","One of the ongoing efforts is dedicated to improving the already applied technique of single muon tagging through the detection of secondary electrons emerging from an ultra-thin carbon foil.","In this work, we present the results from installing a thinner foil with a nominal thickness of 0.5 $\\mu g~cm^{-2}$ and compare its performance to that of the previously installed foil with a nominal thickness of 2.0 $\\mu g~cm^{-2}$. Our findings indicate improved beam quality, characterized by smaller beam spots, reduced energy loss and straggling of the muons, and enhanced tagging efficiency.","Additionally, we introduce a method utilizing blue laser irradiation for cleaning the carbon foil, further improving and maintaining its characteristics"],"url":"http://arxiv.org/abs/2402.15254v1","category":"physics.ins-det"}
