{"created":"2024-04-29 17:59:16","title":"Stylus: Automatic Adapter Selection for Diffusion Models","abstract":"Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions. This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.","sentences":["Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs.","As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions.","This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters.","We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords.","Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt.","To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings.","In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model.","See stylus-diffusion.github.io for more."],"url":"http://arxiv.org/abs/2404.18928v1","category":"cs.CV"}
{"created":"2024-04-29 17:58:30","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","abstract":"In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive real-world alignment experiments verify the effectiveness of the proposed approach.","sentences":["In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning.","Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies.","To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information.","Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation.","Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal.","Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently.","For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO.","DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage.","Extensive real-world alignment experiments verify the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2404.18922v1","category":"cs.LG"}
{"created":"2024-04-29 17:44:28","title":"Detecting critical treatment effect bias in small subgroups","abstract":"Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.","sentences":["Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice.","Observational studies, on the other hand, cover a broader patient population but are prone to various biases.","Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial.","We propose a novel strategy to benchmark observational studies beyond the average treatment effect.","First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance.","We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study.","Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge."],"url":"http://arxiv.org/abs/2404.18905v1","category":"stat.ME"}
{"created":"2024-04-29 17:27:37","title":"IPixMatch: Boost Semi-supervised Semantic Segmentation with Inter-Pixel Relation","abstract":"The scarcity of labeled data in real-world scenarios is a critical bottleneck of deep learning's effectiveness. Semi-supervised semantic segmentation has been a typical solution to achieve a desirable tradeoff between annotation cost and segmentation performance. However, previous approaches, whether based on consistency regularization or self-training, tend to neglect the contextual knowledge embedded within inter-pixel relations. This negligence leads to suboptimal performance and limited generalization. In this paper, we propose a novel approach IPixMatch designed to mine the neglected but valuable Inter-Pixel information for semi-supervised learning. Specifically, IPixMatch is constructed as an extension of the standard teacher-student network, incorporating additional loss terms to capture inter-pixel relations. It shines in low-data regimes by efficiently leveraging the limited labeled data and extracting maximum utility from the available unlabeled data. Furthermore, IPixMatch can be integrated seamlessly into most teacher-student frameworks without the need of model modification or adding additional components. Our straightforward IPixMatch method demonstrates consistent performance improvements across various benchmark datasets under different partitioning protocols.","sentences":["The scarcity of labeled data in real-world scenarios is a critical bottleneck of deep learning's effectiveness.","Semi-supervised semantic segmentation has been a typical solution to achieve a desirable tradeoff between annotation cost and segmentation performance.","However, previous approaches, whether based on consistency regularization or self-training, tend to neglect the contextual knowledge embedded within inter-pixel relations.","This negligence leads to suboptimal performance and limited generalization.","In this paper, we propose a novel approach IPixMatch designed to mine the neglected but valuable Inter-Pixel information for semi-supervised learning.","Specifically, IPixMatch is constructed as an extension of the standard teacher-student network, incorporating additional loss terms to capture inter-pixel relations.","It shines in low-data regimes by efficiently leveraging the limited labeled data and extracting maximum utility from the available unlabeled data.","Furthermore, IPixMatch can be integrated seamlessly into most teacher-student frameworks without the need of model modification or adding additional components.","Our straightforward IPixMatch method demonstrates consistent performance improvements across various benchmark datasets under different partitioning protocols."],"url":"http://arxiv.org/abs/2404.18891v1","category":"cs.CV"}
{"created":"2024-04-29 17:26:44","title":"An optimal lower bound for smooth convex functions","abstract":"First order methods endowed with global convergence guarantees operate using global lower bounds on the objective. The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance. In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information. The bound can be readily employed by the Gradient Method with Memory to improve its performance. Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant. We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search. Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods.","sentences":["First order methods endowed with global convergence guarantees operate using global lower bounds on the objective.","The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance.","In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information.","The bound can be readily employed by the Gradient Method with Memory to improve its performance.","Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant.","We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search.","Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods."],"url":"http://arxiv.org/abs/2404.18889v1","category":"math.OC"}
{"created":"2024-04-29 17:19:40","title":"A Survey on Diffusion Models for Time Series and Spatio-Temporal Data","abstract":"The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.","sentences":["The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors.","Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions.","Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining.","Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks.","In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain.","In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately.","Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation.","Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks.","Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data.","Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework."],"url":"http://arxiv.org/abs/2404.18886v1","category":"cs.LG"}
{"created":"2024-04-29 17:06:44","title":"OpenStreetView-5M: The Many Roads to Global Visual Geolocation","abstract":"Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies. All associated codes and models can be found at https://github.com/gastruc/osv5m.","sentences":["Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms.","Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential.","To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories.","In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization.","To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies.","All associated codes and models can be found at https://github.com/gastruc/osv5m."],"url":"http://arxiv.org/abs/2404.18873v1","category":"cs.CV"}
{"created":"2024-04-29 17:00:53","title":"More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness","abstract":"The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified. Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects. Together, our results underscore the need for more nuanced approaches for model alignment. By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy.","sentences":["The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power.","Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified.","Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy.","For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO).","Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects.","Together, our results underscore the need for more nuanced approaches for model alignment.","By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy."],"url":"http://arxiv.org/abs/2404.18870v1","category":"cs.CL"}
{"created":"2024-04-29 16:52:38","title":"Performance-Aligned LLMs for Generating Fast Code","abstract":"Optimizing scientific software is a difficult task because codebases are often large and complex, and performance can depend upon several factors including the algorithm, its implementation, and hardware among others. Causes of poor performance can originate from disparate sources and be difficult to diagnose. Recent years have seen a multitude of work that use large language models (LLMs) to assist in software development tasks. However, these tools are trained to model the distribution of code as text, and are not specifically designed to understand performance aspects of code. In this work, we introduce a reinforcement learning based methodology to align the outputs of code LLMs with performance. This allows us to build upon the current code modeling capabilities of LLMs and extend them to generate better performing code. We demonstrate that our fine-tuned model improves the expected speedup of generated code over base models for a set of benchmark tasks from 0.9 to 1.6 for serial code and 1.9 to 4.5 for OpenMP code.","sentences":["Optimizing scientific software is a difficult task because codebases are often large and complex, and performance can depend upon several factors including the algorithm, its implementation, and hardware among others.","Causes of poor performance can originate from disparate sources and be difficult to diagnose.","Recent years have seen a multitude of work that use large language models (LLMs) to assist in software development tasks.","However, these tools are trained to model the distribution of code as text, and are not specifically designed to understand performance aspects of code.","In this work, we introduce a reinforcement learning based methodology to align the outputs of code LLMs with performance.","This allows us to build upon the current code modeling capabilities of LLMs and extend them to generate better performing code.","We demonstrate that our fine-tuned model improves the expected speedup of generated code over base models for a set of benchmark tasks from 0.9 to 1.6 for serial code and 1.9 to 4.5 for OpenMP code."],"url":"http://arxiv.org/abs/2404.18864v1","category":"cs.DC"}
{"created":"2024-04-29 16:42:26","title":"FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition","abstract":"Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning. Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection. Federated learning, which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution. However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) into federated learning can effectively address this problem. However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FFT). To overcome this, we propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method in federated learning. FeDeRA uses the same adapter module as LoRA. However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components. We conducted extensive experiments, using RoBERTa and DeBERTaV3, on six datasets, comparing the methods including FFT and the other three different PEFT methods. FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FFT method. We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks. Compared to FFT, FeDeRA reduces the training time by 95.9\\%, 97.9\\%, 96.9\\% and 97.3\\%, 96.5\\%, 96.5\\% respectively on three tasks using RoBERTa and DeBERTaV3. The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency.","sentences":["Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning.","Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection.","Federated learning, which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution.","However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses.","Introducing Parameter-Efficient Fine-Tuning(PEFT) into federated learning can effectively address this problem.","However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FFT).","To overcome this, we propose FeDeRA, an improvement over the Low-Rank Adaption(LoRA) method in federated learning.","FeDeRA uses the same adapter module as LoRA.","However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components.","We conducted extensive experiments, using RoBERTa and DeBERTaV3, on six datasets, comparing the methods including FFT and the other three different PEFT methods.","FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FFT method.","We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks.","Compared to FFT, FeDeRA reduces the training time by 95.9\\%, 97.9\\%, 96.9\\% and 97.3\\%, 96.5\\%, 96.5\\% respectively on three tasks using RoBERTa and DeBERTaV3.","The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency."],"url":"http://arxiv.org/abs/2404.18848v2","category":"cs.LG"}
{"created":"2024-04-29 16:35:42","title":"Phantom matter: a challenging solution to the cosmological tensions","abstract":"The idea of composite dark energy (DE) is quite natural since on general grounds we expect that the vacuum energy (associated to the cosmological term $\\Lambda$) may appear in combination with other effective forms of DE, collectively denoted $X$. This was indeed the old proposal from 2006, (cf. Ref.[41]) called the `$\\Lambda$XCDM model', which was primordially designed to explain the cosmic coincidence problem. We now find that it can also have far reaching consequences on the current cosmological tensions on $H_0$ and the growth of large scale structure (LSS). The $\\Lambda$XCDM may involve both a phantom-like component $X$ and a constant cosmological term $\\Lambda$ (positive or negative) or even a running one, $\\Lambda=\\Lambda(H)$. In the current work, we deal with a simplified version of the model and exploit the possibility that $X$ behaves as `phantom matter' (PM). The latter appears in stringy versions of the running vacuum model (RVM). Unlike phantom DE, it satisfies the strong energy condition like usual matter, hence bringing to bear positive pressure at the expense of negative energy. Bubbles of PM may appear in the manner of a transitory `phantom vacuum' tunneled into the late universe before it heads towards a new de Sitter era, thereby offering a crop field for the growing of structures earlier than expected. Using SNIa, cosmic chronometers, transversal BAO, LSS data and the full CMB likelihood from Planck 2018, we find that the tensions virtually disappear in this stringy RVM scenario characterized by axionic dark matter. The value of $H_0$ emerging from our analysis proves compatible with SH0ES to within less than $0.25\\sigma$ and the LSS growth tension is nonexistent. The statistical information criteria point to very strong evidence in favor of the PM solution.","sentences":["The idea of composite dark energy (DE) is quite natural since on general grounds we expect that the vacuum energy (associated to the cosmological term $\\Lambda$) may appear in combination with other effective forms of DE, collectively denoted $X$. This was indeed the old proposal from 2006, (cf.","Ref.[41]) called the `$\\Lambda$XCDM model', which was primordially designed to explain the cosmic coincidence problem.","We now find that it can also have far reaching consequences on the current cosmological tensions on $H_0$ and the growth of large scale structure (LSS).","The $\\Lambda$XCDM may involve both a phantom-like component $X$ and a constant cosmological term $\\Lambda$ (positive or negative) or even a running one, $\\Lambda=\\Lambda(H)$. In the current work, we deal with a simplified version of the model and exploit the possibility that $X$ behaves as `phantom matter' (PM).","The latter appears in stringy versions of the running vacuum model (RVM).","Unlike phantom DE, it satisfies the strong energy condition like usual matter, hence bringing to bear positive pressure at the expense of negative energy.","Bubbles of PM may appear in the manner of a transitory `phantom vacuum' tunneled into the late universe before it heads towards a new de Sitter era, thereby offering a crop field for the growing of structures earlier than expected.","Using SNIa, cosmic chronometers, transversal BAO, LSS data and the full CMB likelihood from Planck 2018, we find that the tensions virtually disappear in this stringy RVM scenario characterized by axionic dark matter.","The value of $H_0$ emerging from our analysis proves compatible with SH0ES to within less than $0.25\\sigma$ and the LSS growth tension is nonexistent.","The statistical information criteria point to very strong evidence in favor of the PM solution."],"url":"http://arxiv.org/abs/2404.18845v1","category":"astro-ph.CO"}
{"created":"2024-04-29 16:30:24","title":"VISION: Toward a Standardized Process for Radiology Image Management at the National Level","abstract":"The compilation and analysis of radiological images poses numerous challenges for researchers. The sheer volume of data as well as the computational needs of algorithms capable of operating on images are extensive. Additionally, the assembly of these images alone is difficult, as these exams may differ widely in terms of clinical context, structured annotation available for model training, modality, and patient identifiers. In this paper, we describe our experiences and challenges in establishing a trusted collection of radiology images linked to the United States Department of Veterans Affairs (VA) electronic health record database. We also discuss implications in making this repository research-ready for medical investigators. Key insights include uncovering the specific procedures required for transferring images from a clinical to a research-ready environment, as well as roadblocks and bottlenecks in this process that may hinder future efforts at automation.","sentences":["The compilation and analysis of radiological images poses numerous challenges for researchers.","The sheer volume of data as well as the computational needs of algorithms capable of operating on images are extensive.","Additionally, the assembly of these images alone is difficult, as these exams may differ widely in terms of clinical context, structured annotation available for model training, modality, and patient identifiers.","In this paper, we describe our experiences and challenges in establishing a trusted collection of radiology images linked to the United States Department of Veterans Affairs (VA) electronic health record database.","We also discuss implications in making this repository research-ready for medical investigators.","Key insights include uncovering the specific procedures required for transferring images from a clinical to a research-ready environment, as well as roadblocks and bottlenecks in this process that may hinder future efforts at automation."],"url":"http://arxiv.org/abs/2404.18842v1","category":"cs.CV"}
{"created":"2024-04-29 16:19:47","title":"It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation of Patient Comments","abstract":"Sentiment analysis is an important tool for aggregating patient voices, in order to provide targeted improvements in healthcare services. A prerequisite for this is the availability of in-domain data annotated for sentiment. This article documents an effort to add sentiment annotations to free-text comments in patient surveys collected by the Norwegian Institute of Public Health (NIPH). However, annotation can be a time-consuming and resource-intensive process, particularly when it requires domain expertise. We therefore also evaluate a possible alternative to human annotation, using large language models (LLMs) as annotators. We perform an extensive evaluation of the approach for two openly available pretrained LLMs for Norwegian, experimenting with different configurations of prompts and in-context learning, comparing their performance to human annotators. We find that even for zero-shot runs, models perform well above the baseline for binary sentiment, but still cannot compete with human annotators on the full dataset.","sentences":["Sentiment analysis is an important tool for aggregating patient voices, in order to provide targeted improvements in healthcare services.","A prerequisite for this is the availability of in-domain data annotated for sentiment.","This article documents an effort to add sentiment annotations to free-text comments in patient surveys collected by the Norwegian Institute of Public Health (NIPH).","However, annotation can be a time-consuming and resource-intensive process, particularly when it requires domain expertise.","We therefore also evaluate a possible alternative to human annotation, using large language models (LLMs) as annotators.","We perform an extensive evaluation of the approach for two openly available pretrained LLMs for Norwegian, experimenting with different configurations of prompts and in-context learning, comparing their performance to human annotators.","We find that even for zero-shot runs, models perform well above the baseline for binary sentiment, but still cannot compete with human annotators on the full dataset."],"url":"http://arxiv.org/abs/2404.18832v1","category":"cs.CL"}
{"created":"2024-04-29 16:16:42","title":"ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization","abstract":"Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression. This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization. Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class. We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations. We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks. We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively. In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain.","sentences":["Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression.","This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization.","Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class.","We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations.","We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks.","We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively.","In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain."],"url":"http://arxiv.org/abs/2404.18831v1","category":"cs.CV"}
{"created":"2024-04-29 16:15:32","title":"Disentangling the development of collective flow in high energy proton proton collisions with a multiphase transport model","abstract":"In this work, we investigate the collective flow development in high energy proton proton (pp) collisions with a multiphase transport model (AMPT) based on PYTHIA8 initial conditions with a sub-nucleon structure. It is found that the PYTHIA8 based AMPT model can reasonably describe both the charged hadron productions and elliptic flow experimental data measured in pp collisions at $\\sqrt{s}=13$ TeV. By turning on the parton and hadron rescatterings in AMPT separately, we find that the observed collective flow in pp collisions is largely developed during the parton evolutions, while no significant flow effect can be generated with the pure hadronic rescatterings. It is also shown that the parton escape mechanism is important for describing both the magnitude of the two-particle cumulant and the sign of the four-particle cumulants. We emphasize that the strong mass ordering of the elliptic flow results from the coalescence process in the transport model can thus be regarded as unique evidence related to the creation of deconfined parton matter in high energy pp collisions.","sentences":["In this work, we investigate the collective flow development in high energy proton proton (pp) collisions with a multiphase transport model (AMPT) based on PYTHIA8 initial conditions with a sub-nucleon structure.","It is found that the PYTHIA8 based AMPT model can reasonably describe both the charged hadron productions and elliptic flow experimental data measured in pp collisions at $\\sqrt{s}=13$ TeV. By turning on the parton and hadron rescatterings in AMPT separately, we find that the observed collective flow in pp collisions is largely developed during the parton evolutions, while no significant flow effect can be generated with the pure hadronic rescatterings.","It is also shown that the parton escape mechanism is important for describing both the magnitude of the two-particle cumulant and the sign of the four-particle cumulants.","We emphasize that the strong mass ordering of the elliptic flow results from the coalescence process in the transport model can thus be regarded as unique evidence related to the creation of deconfined parton matter in high energy pp collisions."],"url":"http://arxiv.org/abs/2404.18829v1","category":"nucl-th"}
{"created":"2024-04-29 16:07:36","title":"Harmonic Machine Learning Models are Robust","abstract":"We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels. It is based on functional deviation from the harmonic mean value property, indicating instability and lack of explainability. We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes.","sentences":["We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels.","It is based on functional deviation from the harmonic mean value property, indicating instability and lack of explainability.","We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes."],"url":"http://arxiv.org/abs/2404.18825v1","category":"cs.LG"}
{"created":"2024-04-29 16:05:36","title":"Benchmarking Benchmark Leakage in Large Language Models","abstract":"Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the \"Benchmark Transparency Card\" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.","sentences":["Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs).","This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development.","To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages.","By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons.","These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations.","Notably, we propose the \"Benchmark Transparency Card\" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs.","we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research."],"url":"http://arxiv.org/abs/2404.18824v1","category":"cs.CL"}
{"created":"2024-04-29 16:03:21","title":"Control Policy Correction Framework for Reinforcement Learning-based Energy Arbitrage Strategies","abstract":"A continuous rise in the penetration of renewable energy sources, along with the use of the single imbalance pricing, provides a new opportunity for balance responsible parties to reduce their cost through energy arbitrage in the imbalance settlement mechanism. Model-free reinforcement learning (RL) methods are an appropriate choice for solving the energy arbitrage problem due to their outstanding performance in solving complex stochastic sequential problems. However, RL is rarely deployed in real-world applications since its learned policy does not necessarily guarantee safety during the execution phase. In this paper, we propose a new RL-based control framework for batteries to obtain a safe energy arbitrage strategy in the imbalance settlement mechanism. In our proposed control framework, the agent initially aims to optimize the arbitrage revenue. Subsequently, in the post-processing step, we correct (constrain) the learned policy following a knowledge distillation process based on properties that follow human intuition. Our post-processing step is a generic method and is not restricted to the energy arbitrage domain. We use the Belgian imbalance price of 2023 to evaluate the performance of our proposed framework. Furthermore, we deploy our proposed control framework on a real battery to show its capability in the real world.","sentences":["A continuous rise in the penetration of renewable energy sources, along with the use of the single imbalance pricing, provides a new opportunity for balance responsible parties to reduce their cost through energy arbitrage in the imbalance settlement mechanism.","Model-free reinforcement learning (RL) methods are an appropriate choice for solving the energy arbitrage problem due to their outstanding performance in solving complex stochastic sequential problems.","However, RL is rarely deployed in real-world applications since its learned policy does not necessarily guarantee safety during the execution phase.","In this paper, we propose a new RL-based control framework for batteries to obtain a safe energy arbitrage strategy in the imbalance settlement mechanism.","In our proposed control framework, the agent initially aims to optimize the arbitrage revenue.","Subsequently, in the post-processing step, we correct (constrain) the learned policy following a knowledge distillation process based on properties that follow human intuition.","Our post-processing step is a generic method and is not restricted to the energy arbitrage domain.","We use the Belgian imbalance price of 2023 to evaluate the performance of our proposed framework.","Furthermore, we deploy our proposed control framework on a real battery to show its capability in the real world."],"url":"http://arxiv.org/abs/2404.18821v2","category":"eess.SY"}
{"created":"2024-04-29 15:52:45","title":"AppPoet: Large Language Model based Android malware detection via multi-view prompt engineering","abstract":"Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions. However, numerous feature engineering based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection. Besides, the majority of existing feature engineering based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports. Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection. Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views. Subsequently, it steers the LLM to produce function descriptions and behavioral summaries for views via our meticulously devised multi-view prompt engineering technique to realize the deep mining of view semantics. Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the heuristic diagnostic reports. Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline method Drebin and its variant. Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports.","sentences":["Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions.","However, numerous feature engineering based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection.","Besides, the majority of existing feature engineering based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports.","Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection.","Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views.","Subsequently, it steers the LLM to produce function descriptions and behavioral summaries for views via our meticulously devised multi-view prompt engineering technique to realize the deep mining of view semantics.","Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the heuristic diagnostic reports.","Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline method Drebin and its variant.","Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports."],"url":"http://arxiv.org/abs/2404.18816v1","category":"cs.CR"}
{"created":"2024-04-29 15:33:56","title":"Efficiency-Effectiveness Tradeoff of Probabilistic Structured Queries for Cross-Language Information Retrieval","abstract":"Probabilistic Structured Queries (PSQ) is a cross-language information retrieval (CLIR) method that uses translation probabilities statistically derived from aligned corpora. PSQ is a strong baseline for efficient CLIR using sparse indexing. It is, therefore, useful as the first stage in a cascaded neural CLIR system whose second stage is more effective but too inefficient to be used on its own to search a large text collection. In this reproducibility study, we revisit PSQ by introducing an efficient Python implementation. Unconstrained use of all translation probabilities that can be estimated from aligned parallel text would in the limit assign a weight to every vocabulary term, precluding use of an inverted index to serve queries efficiently. Thus, PSQ's effectiveness and efficiency both depend on how translation probabilities are pruned. This paper presents experiments over a range of modern CLIR test collections to demonstrate that achieving Pareto optimal PSQ effectiveness-efficiency tradeoffs benefits from multi-criteria pruning, which has not been fully explored in prior work. Our Python PSQ implementation is available on GitHub(https://github.com/hltcoe/PSQ) and unpruned translation tables are available on Huggingface Models(https://huggingface.co/hltcoe/psq_translation_tables).","sentences":["Probabilistic Structured Queries (PSQ) is a cross-language information retrieval (CLIR) method that uses translation probabilities statistically derived from aligned corpora.","PSQ is a strong baseline for efficient CLIR using sparse indexing.","It is, therefore, useful as the first stage in a cascaded neural CLIR system whose second stage is more effective but too inefficient to be used on its own to search a large text collection.","In this reproducibility study, we revisit PSQ by introducing an efficient Python implementation.","Unconstrained use of all translation probabilities that can be estimated from aligned parallel text would in the limit assign a weight to every vocabulary term, precluding use of an inverted index to serve queries efficiently.","Thus, PSQ's effectiveness and efficiency both depend on how translation probabilities are pruned.","This paper presents experiments over a range of modern CLIR test collections to demonstrate that achieving Pareto optimal PSQ effectiveness-efficiency tradeoffs benefits from multi-criteria pruning, which has not been fully explored in prior work.","Our Python PSQ implementation is available on GitHub(https://github.com/hltcoe/PSQ) and unpruned translation tables are available on Huggingface Models(https://huggingface.co/hltcoe/psq_translation_tables)."],"url":"http://arxiv.org/abs/2404.18797v1","category":"cs.IR"}
{"created":"2024-04-29 15:33:23","title":"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models","abstract":"As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.","sentences":["As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality.","Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge.","To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs.","Evaluations most commonly use a single large model like GPT4.","While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary.","We propose instead to evaluate models using a Panel of LLm evaluators (PoLL).","Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive."],"url":"http://arxiv.org/abs/2404.18796v1","category":"cs.CL"}
{"created":"2024-04-29 15:23:26","title":"Certification of Speaker Recognition Models to Additive Perturbations","abstract":"Speaker recognition technology is applied in various tasks ranging from personal virtual assistants to secure access systems. However, the robustness of these systems against adversarial attacks, particularly to additive perturbations, remains a significant challenge. In this paper, we pioneer applying robustness certification techniques to speaker recognition, originally developed for the image domain. In our work, we cover this gap by transferring and improving randomized smoothing certification techniques against norm-bounded additive perturbations for classification and few-shot learning tasks to speaker recognition. We demonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for several models. We expect this work to improve voice-biometry robustness, establish a new certification benchmark, and accelerate research of certification methods in the audio domain.","sentences":["Speaker recognition technology is applied in various tasks ranging from personal virtual assistants to secure access systems.","However, the robustness of these systems against adversarial attacks, particularly to additive perturbations, remains a significant challenge.","In this paper, we pioneer applying robustness certification techniques to speaker recognition, originally developed for the image domain.","In our work, we cover this gap by transferring and improving randomized smoothing certification techniques against norm-bounded additive perturbations for classification and few-shot learning tasks to speaker recognition.","We demonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for several models.","We expect this work to improve voice-biometry robustness, establish a new certification benchmark, and accelerate research of certification methods in the audio domain."],"url":"http://arxiv.org/abs/2404.18791v1","category":"cs.SD"}
{"created":"2024-04-29 15:23:16","title":"3D Mapping of Glacier Moulins: Challenges and lessons learned","abstract":"In this paper, we present a field report of the mapping of the Athabasca Glacier, using a custom-made lidar-inertial mapping platform. With the increasing autonomy of robotics, a wider spectrum of applications emerges. Among these, the surveying of environmental areas presents arduous and hazardous challenges for human operators. Leveraging automated platforms for data collection holds the promise of unlocking new applications and a deeper comprehension of the environment. Over the course of a week-long deployment, we collected glacier data using a tailor-made measurement platform and reflected on the inherent challenges associated with such experiments. We focus on the insights gained and the forthcoming challenges that robotics must surmount to effectively map these terrains.","sentences":["In this paper, we present a field report of the mapping of the Athabasca Glacier, using a custom-made lidar-inertial mapping platform.","With the increasing autonomy of robotics, a wider spectrum of applications emerges.","Among these, the surveying of environmental areas presents arduous and hazardous challenges for human operators.","Leveraging automated platforms for data collection holds the promise of unlocking new applications and a deeper comprehension of the environment.","Over the course of a week-long deployment, we collected glacier data using a tailor-made measurement platform and reflected on the inherent challenges associated with such experiments.","We focus on the insights gained and the forthcoming challenges that robotics must surmount to effectively map these terrains."],"url":"http://arxiv.org/abs/2404.18790v1","category":"cs.RO"}
{"created":"2024-04-29 15:18:33","title":"Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy Multilingual User Input","abstract":"Geo-entity linking is the task of linking a location mention to the real-world geographic location. In this paper we explore the challenging task of geo-entity linking for noisy, multilingual social media data. There are few open-source multilingual geo-entity linking tools available and existing ones are often rule-based, which break easily in social media settings, or LLM-based, which are too expensive for large-scale datasets. We present a method which represents real-world locations as averaged embeddings from labeled user-input location names and allows for selective prediction via an interpretable confidence score. We show that our approach improves geo-entity linking on a global and multilingual social media dataset, and discuss progress and problems with evaluating at different geographic granularities.","sentences":["Geo-entity linking is the task of linking a location mention to the real-world geographic location.","In this paper we explore the challenging task of geo-entity linking for noisy, multilingual social media data.","There are few open-source multilingual geo-entity linking tools available and existing ones are often rule-based, which break easily in social media settings, or LLM-based, which are too expensive for large-scale datasets.","We present a method which represents real-world locations as averaged embeddings from labeled user-input location names and allows for selective prediction via an interpretable confidence score.","We show that our approach improves geo-entity linking on a global and multilingual social media dataset, and discuss progress and problems with evaluating at different geographic granularities."],"url":"http://arxiv.org/abs/2404.18784v1","category":"cs.CL"}
{"created":"2024-04-29 15:09:00","title":"Self-training superconducting neuromorphic circuits using reinforcement learning rules","abstract":"Reinforcement learning algorithms are used in a wide range of applications, from gaming and robotics to autonomous vehicles. In this paper we describe a set of reinforcement learning-based local weight update rules and their implementation in superconducting hardware. Using SPICE circuit simulations, we implement a small-scale neural network with a learning time of order one nanosecond. This network can be trained to learn new functions simply by changing the target output for a given set of inputs, without the need for any external adjustments to the network. In this implementation the weights are adjusted based on the current state of the overall network response and locally stored information about the previous action. This removes the need to program explicit weight values in these networks, which is one of the primary challenges that analog hardware implementations of neural networks face. The adjustment of weights is based on a global reinforcement signal that obviates the need for circuitry to back-propagate errors.","sentences":["Reinforcement learning algorithms are used in a wide range of applications, from gaming and robotics to autonomous vehicles.","In this paper we describe a set of reinforcement learning-based local weight update rules and their implementation in superconducting hardware.","Using SPICE circuit simulations, we implement a small-scale neural network with a learning time of order one nanosecond.","This network can be trained to learn new functions simply by changing the target output for a given set of inputs, without the need for any external adjustments to the network.","In this implementation the weights are adjusted based on the current state of the overall network response and locally stored information about the previous action.","This removes the need to program explicit weight values in these networks, which is one of the primary challenges that analog hardware implementations of neural networks face.","The adjustment of weights is based on a global reinforcement signal that obviates the need for circuitry to back-propagate errors."],"url":"http://arxiv.org/abs/2404.18774v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 15:05:42","title":"Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural Networks and the Brain","abstract":"Deep learning algorithms lack human-interpretable accounts of how they transform raw visual input into a robust semantic understanding, which impedes comparisons between different architectures, training objectives, and the human brain. In this work, we take inspiration from neuroscience and employ representational approaches to shed light on how neural networks encode information at low (visual saliency) and high (semantic similarity) levels of abstraction. Moreover, we introduce a custom image dataset where we systematically manipulate salient and semantic information. We find that ResNets are more sensitive to saliency information than ViTs, when trained with object classification objectives. We uncover that networks suppress saliency in early layers, a process enhanced by natural language supervision (CLIP) in ResNets. CLIP also enhances semantic encoding in both architectures. Finally, we show that semantic encoding is a key factor in aligning AI with human visual perception, while saliency suppression is a non-brain-like strategy.","sentences":["Deep learning algorithms lack human-interpretable accounts of how they transform raw visual input into a robust semantic understanding, which impedes comparisons between different architectures, training objectives, and the human brain.","In this work, we take inspiration from neuroscience and employ representational approaches to shed light on how neural networks encode information at low (visual saliency) and high (semantic similarity) levels of abstraction.","Moreover, we introduce a custom image dataset where we systematically manipulate salient and semantic information.","We find that ResNets are more sensitive to saliency information than ViTs, when trained with object classification objectives.","We uncover that networks suppress saliency in early layers, a process enhanced by natural language supervision (CLIP) in ResNets.","CLIP also enhances semantic encoding in both architectures.","Finally, we show that semantic encoding is a key factor in aligning AI with human visual perception, while saliency suppression is a non-brain-like strategy."],"url":"http://arxiv.org/abs/2404.18772v1","category":"cs.CV"}
{"created":"2024-04-29 15:02:14","title":"PECC: Problem Extraction and Coding Challenges","abstract":"Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMs' capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver.","sentences":["Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning.","Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored.","Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems.","Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code.","A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities.","Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems.","By probing the limits of LLMs' capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver."],"url":"http://arxiv.org/abs/2404.18766v1","category":"cs.AI"}
{"created":"2024-04-29 14:47:32","title":"Evaluating the Effectiveness of Video Anomaly Detection in the Wild: Online Learning and Inference for Real-world Deployment","abstract":"Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare. Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts. Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility. Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously. This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages. Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy. We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains. Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain.","sentences":["Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare.","Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts.","Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility.","Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously.","This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages.","Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy.","We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains.","Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain."],"url":"http://arxiv.org/abs/2404.18747v1","category":"cs.CV"}
{"created":"2024-04-29 14:34:43","title":"Mapping the Potential of Explainable Artificial Intelligence (XAI) for Fairness Along the AI Lifecycle","abstract":"The widespread use of artificial intelligence (AI) systems across various domains is increasingly highlighting issues related to algorithmic fairness, especially in high-stakes scenarios. Thus, critical considerations of how fairness in AI systems might be improved, and what measures are available to aid this process, are overdue. Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems. However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous. Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle. Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle. In this paper, we set out to bridge both these gaps: We distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them. We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata.","sentences":["The widespread use of artificial intelligence (AI) systems across various domains is increasingly highlighting issues related to algorithmic fairness, especially in high-stakes scenarios.","Thus, critical considerations of how fairness in AI systems might be improved, and what measures are available to aid this process, are overdue.","Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems.","However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous.","Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle.","Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle.","In this paper, we set out to bridge both these gaps: We distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them.","We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata."],"url":"http://arxiv.org/abs/2404.18736v2","category":"cs.LG"}
{"created":"2024-04-29 14:19:22","title":"Two-way Homogeneity Pursuit for Quantile Network Vector Autoregression","abstract":"While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data. To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes. Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability. To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure. Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups. With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences. Moreover, we propose a quantile information criterion for consistently selecting the number of groups. Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks.","sentences":["While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data.","To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes.","Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability.","To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure.","Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups.","With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences.","Moreover, we propose a quantile information criterion for consistently selecting the number of groups.","Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks."],"url":"http://arxiv.org/abs/2404.18732v1","category":"stat.ME"}
{"created":"2024-04-29 14:17:52","title":"Real Time Multi Organ Classification on Computed Tomography Images","abstract":"Organ segmentation is a fundamental task in medical imaging, and it is useful for many clinical automation pipelines. Typically, the process involves segmenting the entire volume, which can be unnecessary when the points of interest are limited. In those cases, a classifier could be used instead of segmentation. However, there is an inherent trade-off between the context size and the speed of classifiers. To address this issue, we propose a new method that employs a data selection strategy with sparse sampling across a wide field of view without image resampling. This sparse sampling strategy makes it possible to classify voxels into multiple organs in real time without using accelerators. Although our method is an independent classifier, it can generate full segmentation by querying grid locations at any resolution. We have compared our method with existing segmentation techniques, demonstrating its potential for superior runtime in practical applications in medical imaging.","sentences":["Organ segmentation is a fundamental task in medical imaging, and it is useful for many clinical automation pipelines.","Typically, the process involves segmenting the entire volume, which can be unnecessary when the points of interest are limited.","In those cases, a classifier could be used instead of segmentation.","However, there is an inherent trade-off between the context size and the speed of classifiers.","To address this issue, we propose a new method that employs a data selection strategy with sparse sampling across a wide field of view without image resampling.","This sparse sampling strategy makes it possible to classify voxels into multiple organs in real time without using accelerators.","Although our method is an independent classifier, it can generate full segmentation by querying grid locations at any resolution.","We have compared our method with existing segmentation techniques, demonstrating its potential for superior runtime in practical applications in medical imaging."],"url":"http://arxiv.org/abs/2404.18731v1","category":"cs.CV"}
{"created":"2024-04-29 14:16:16","title":"CVTN: Cross Variable and Temporal Integration for Time Series Forecasting","abstract":"In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.","sentences":["In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies.","To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN).","This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences.","Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning.","Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance.","CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning.","This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field."],"url":"http://arxiv.org/abs/2404.18730v1","category":"cs.LG"}
{"created":"2024-04-29 14:16:08","title":"Fast Swarming of UAVs in GNSS-denied Feature-poor Environments without Explicit Communication","abstract":"A decentralized swarm approach for the fast cooperative flight of Unmanned Aerial Vehicles (UAVs) in feature-poor environments without any external localization and communication is introduced in this paper.   A novel model of a UAV neighborhood is proposed to achieve robust onboard mutual perception and flocking state feedback control, which is designed to decrease the inter-agent oscillations common in standard reactive swarm models employed in fast collective motion.   The novel swarming methodology is supplemented with an enhanced Multi-Robot State Estimation (MRSE) strategy to increase the reliability of the purely onboard localization, which may be unreliable in real environments.   Although MRSE and the neighborhood model may rely on information exchange between agents, we introduce a communication-less version of the swarming framework based on estimating communicated states to decrease dependence on the often unreliable communication networks of large swarms.   The proposed solution has been verified by a set of complex real-world experiments to demonstrate its overall capability in different conditions, including a UAV interception-motivated task with a group velocity reaching the physical limits of the individual hardware platforms.","sentences":["A decentralized swarm approach for the fast cooperative flight of Unmanned Aerial Vehicles (UAVs) in feature-poor environments without any external localization and communication is introduced in this paper.   ","A novel model of a UAV neighborhood is proposed to achieve robust onboard mutual perception and flocking state feedback control, which is designed to decrease the inter-agent oscillations common in standard reactive swarm models employed in fast collective motion.   ","The novel swarming methodology is supplemented with an enhanced Multi-Robot State Estimation (MRSE) strategy to increase the reliability of the purely onboard localization, which may be unreliable in real environments.   ","Although MRSE and the neighborhood model may rely on information exchange between agents, we introduce a communication-less version of the swarming framework based on estimating communicated states to decrease dependence on the often unreliable communication networks of large swarms.   ","The proposed solution has been verified by a set of complex real-world experiments to demonstrate its overall capability in different conditions, including a UAV interception-motivated task with a group velocity reaching the physical limits of the individual hardware platforms."],"url":"http://arxiv.org/abs/2404.18729v1","category":"cs.RO"}
{"created":"2024-04-29 14:14:33","title":"The Constant in HATE: Analyzing Toxicity in Reddit across Topics and Languages","abstract":"Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities. This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations. We collect 1.5 million comment threads from 481 communities in six languages: English, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such as Culture, Politics, and News. We thoroughly analyze how toxicity spikes within different communities in relation to specific topics. We observe consistent patterns of increased toxicity across languages for certain topics, while also noting significant variations within specific language communities.","sentences":["Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities.","This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations.","We collect 1.5 million comment threads from 481 communities in six languages: English, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such as Culture, Politics, and News.","We thoroughly analyze how toxicity spikes within different communities in relation to specific topics.","We observe consistent patterns of increased toxicity across languages for certain topics, while also noting significant variations within specific language communities."],"url":"http://arxiv.org/abs/2404.18726v1","category":"cs.CL"}
{"created":"2024-04-29 14:02:02","title":"Adaptive Reinforcement Learning for Robot Control","abstract":"Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}.","sentences":["Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes.","To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions.","The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential.","The agent is trained using a custom, highly parallelized simulator built on IsaacGym.","We perform zero-shot transfer to fly the blimp in the real world to solve various tasks.","We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}."],"url":"http://arxiv.org/abs/2404.18713v1","category":"cs.RO"}
{"created":"2024-04-29 13:59:10","title":"Three-state Opinion Dynamics for Financial Markets on Complex Networks","abstract":"This work investigates the effects of complex networks on the collective behavior of a three-state opinion formation model in economic systems. Our model considers two distinct types of investors in financial markets: noise traders and fundamentalists. Financial states evolve via probabilistic dynamics that include economic strategies with local and global influences. The local majoritarian opinion drives noise traders' market behavior, while the market index influences the financial decisions of fundamentalist agents. We introduce a level of market anxiety $q$ present in the decision-making process that influences financial action. In our investigation, nodes of a complex network represent market agents, whereas the links represent their financial interactions. We investigate the stochastic dynamics of the model on three distinct network topologies, including scale-free networks, small-world networks and Erd{\\\"o}s-R\\'enyi random graphs. Our model mirrors various traits observed in real-world financial return series, such as heavy-tailed return distributions, volatility clustering, and short-term memory correlation of returns. The histograms of returns are fitted by coupled Gaussian distributions, quantitatively revealing transitions from a leptokurtic to a mesokurtic regime under specific economic heterogeneity. We show that the market dynamics depend mainly on the average agent connectivity, anxiety level, and market composition rather than on specific features of network topology.","sentences":["This work investigates the effects of complex networks on the collective behavior of a three-state opinion formation model in economic systems.","Our model considers two distinct types of investors in financial markets: noise traders and fundamentalists.","Financial states evolve via probabilistic dynamics that include economic strategies with local and global influences.","The local majoritarian opinion drives noise traders' market behavior, while the market index influences the financial decisions of fundamentalist agents.","We introduce a level of market anxiety $q$ present in the decision-making process that influences financial action.","In our investigation, nodes of a complex network represent market agents, whereas the links represent their financial interactions.","We investigate the stochastic dynamics of the model on three distinct network topologies, including scale-free networks, small-world networks and Erd{\\\"o}s-R\\'enyi random graphs.","Our model mirrors various traits observed in real-world financial return series, such as heavy-tailed return distributions, volatility clustering, and short-term memory correlation of returns.","The histograms of returns are fitted by coupled Gaussian distributions, quantitatively revealing transitions from a leptokurtic to a mesokurtic regime under specific economic heterogeneity.","We show that the market dynamics depend mainly on the average agent connectivity, anxiety level, and market composition rather than on specific features of network topology."],"url":"http://arxiv.org/abs/2404.18709v1","category":"physics.soc-ph"}
{"created":"2024-04-29 13:57:02","title":"The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses","abstract":"This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936. These lists contain information about individuals living in France and their households. We aim at extracting all the information contained in these tables using automatic handwritten table recognition. At the end of the Socface project, in which our work is taking place, the extracted information will be redistributed to the departmental archives, and the nominative lists will be freely available to the public, allowing anyone to browse hundreds of millions of records. The extracted data will be used by demographers to analyze social change over time, significantly improving our understanding of French economic and social structures. For this project, we developed a complete processing workflow: large-scale data collection from French departmental archives, collaborative annotation of documents, training of handwritten table text and structure recognition models, and mass processing of millions of images. We present the tools we have developed to easily collect and process millions of pages. We also show that it is possible to process such a wide variety of tables with a single table recognition model that uses the image of the entire page to recognize information about individuals, categorize them and automatically group them into households. The entire process has been successfully used to process the documents of a departmental archive, representing more than 450,000 images.","sentences":["This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936.","These lists contain information about individuals living in France and their households.","We aim at extracting all the information contained in these tables using automatic handwritten table recognition.","At the end of the Socface project, in which our work is taking place, the extracted information will be redistributed to the departmental archives, and the nominative lists will be freely available to the public, allowing anyone to browse hundreds of millions of records.","The extracted data will be used by demographers to analyze social change over time, significantly improving our understanding of French economic and social structures.","For this project, we developed a complete processing workflow: large-scale data collection from French departmental archives, collaborative annotation of documents, training of handwritten table text and structure recognition models, and mass processing of millions of images.","We present the tools we have developed to easily collect and process millions of pages.","We also show that it is possible to process such a wide variety of tables with a single table recognition model that uses the image of the entire page to recognize information about individuals, categorize them and automatically group them into households.","The entire process has been successfully used to process the documents of a departmental archive, representing more than 450,000 images."],"url":"http://arxiv.org/abs/2404.18706v1","category":"cs.CV"}
{"created":"2024-04-29 13:56:32","title":"Wireless Information and Energy Transfer in the Era of 6G Communications","abstract":"Wireless information and energy transfer (WIET) represents an emerging paradigm which employs controllable transmission of radio-frequency signals for the dual purpose of data communication and wireless charging. As such, WIET is widely regarded as an enabler of envisioned 6G use cases that rely on energy-sustainable Internet-of-Things (IoT) networks, such as smart cities and smart grids. Meeting the quality-of-service demands of WIET, in terms of both data transfer and power delivery, requires effective co-design of the information and energy signals. In this article, we present the main principles and design aspects of WIET, focusing on its integration in 6G networks. First, we discuss how conventional communication notions such as resource allocation and waveform design need to be revisited in the context of WIET. Next, we consider various candidate 6G technologies that can boost WIET efficiency, namely, holographic multiple-input multiple-output, near-field beamforming, terahertz communication, intelligent reflecting surfaces (IRSs), and reconfigurable (fluid) antenna arrays. We introduce respective WIET design methods, analyze the promising performance gains of these WIET systems, and discuss challenges, open issues, and future research directions. Finally, a near-field energy beamforming scheme and a power-based IRS beamforming algorithm are experimentally validated using a wireless energy transfer testbed. The vision of WIET in communication systems has been gaining momentum in recent years, with constant progress with respect to theoretical but also practical aspects. The comprehensive overview of the state of the art of WIET presented in this paper highlights the potentials of WIET systems as well as their overall benefits in 6G networks.","sentences":["Wireless information and energy transfer (WIET) represents an emerging paradigm which employs controllable transmission of radio-frequency signals for the dual purpose of data communication and wireless charging.","As such, WIET is widely regarded as an enabler of envisioned 6G use cases that rely on energy-sustainable Internet-of-Things (IoT) networks, such as smart cities and smart grids.","Meeting the quality-of-service demands of WIET, in terms of both data transfer and power delivery, requires effective co-design of the information and energy signals.","In this article, we present the main principles and design aspects of WIET, focusing on its integration in 6G networks.","First, we discuss how conventional communication notions such as resource allocation and waveform design need to be revisited in the context of WIET.","Next, we consider various candidate 6G technologies that can boost WIET efficiency, namely, holographic multiple-input multiple-output, near-field beamforming, terahertz communication, intelligent reflecting surfaces (IRSs), and reconfigurable (fluid) antenna arrays.","We introduce respective WIET design methods, analyze the promising performance gains of these WIET systems, and discuss challenges, open issues, and future research directions.","Finally, a near-field energy beamforming scheme and a power-based IRS beamforming algorithm are experimentally validated using a wireless energy transfer testbed.","The vision of WIET in communication systems has been gaining momentum in recent years, with constant progress with respect to theoretical but also practical aspects.","The comprehensive overview of the state of the art of WIET presented in this paper highlights the potentials of WIET systems as well as their overall benefits in 6G networks."],"url":"http://arxiv.org/abs/2404.18705v1","category":"cs.IT"}
{"created":"2024-04-29 13:51:41","title":"Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots","abstract":"The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.","sentences":["The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making.","This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots.","This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain.","As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions.","This framework can produce multiple fooled PD plots via a single model.","By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model.","Managerial insights for regulators and practitioners are provided based on the findings."],"url":"http://arxiv.org/abs/2404.18702v1","category":"cs.LG"}
{"created":"2024-04-29 13:42:55","title":"Beyond Gaze Points: Augmenting Eye Movement with Brainwave Data for Multimodal User Authentication in Extended Reality","abstract":"The increasing adoption of Extended Reality (XR) in various applications underscores the need for secure and user-friendly authentication methods. However, existing methods can disrupt the immersive experience in XR settings, or suffer from higher false acceptance rates. In this paper, we introduce a multimodal biometric authentication system that combines eye movement and brainwave patterns, as captured by consumer-grade low-fidelity sensors. Our multimodal authentication exploits the non-invasive and hands-free properties of eye movement and brainwaves to provide a seamless XR user experience and enhanced security as well. Using synchronized eye and brainwave data collected from 30 participants through consumer-grade devices, we investigated whether twin neural networks can utilize these biometrics for identity verification. Our multimodal authentication system yields an excellent Equal Error Rate (EER) of 0.298\\%, which means an 83.6\\% reduction in EER compared to the single eye movement modality or a 93.9\\% reduction in EER compared to the single brainwave modality.","sentences":["The increasing adoption of Extended Reality (XR) in various applications underscores the need for secure and user-friendly authentication methods.","However, existing methods can disrupt the immersive experience in XR settings, or suffer from higher false acceptance rates.","In this paper, we introduce a multimodal biometric authentication system that combines eye movement and brainwave patterns, as captured by consumer-grade low-fidelity sensors.","Our multimodal authentication exploits the non-invasive and hands-free properties of eye movement and brainwaves to provide a seamless XR user experience and enhanced security as well.","Using synchronized eye and brainwave data collected from 30 participants through consumer-grade devices, we investigated whether twin neural networks can utilize these biometrics for identity verification.","Our multimodal authentication system yields an excellent Equal Error Rate (EER) of 0.298\\%, which means an 83.6\\% reduction in EER compared to the single eye movement modality or a 93.9\\% reduction in EER compared to the single brainwave modality."],"url":"http://arxiv.org/abs/2404.18694v1","category":"cs.CR"}
{"created":"2024-04-29 13:34:19","title":"Socially Adaptive Path Planning Based on Generative Adversarial Network","abstract":"The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians. Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments. However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated. In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm. Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios. Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments. Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths. In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path. In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments. Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths.","sentences":["The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians.","Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments.","However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated.","In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm.","Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios.","Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments.","Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths.","In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path.","In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments.","Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths."],"url":"http://arxiv.org/abs/2404.18687v1","category":"cs.RO"}
{"created":"2024-04-29 13:24:23","title":"LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs","abstract":"Machine learning's influence is expanding rapidly, now integral to decision-making processes from corporate strategy to the advancements in Industry 4.0. The efficacy of Artificial Intelligence broadly hinges on the caliber of data used during its training phase; optimal performance is tied to exceptional data quality. Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality. Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts.   In light of these challenges, this paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets. LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model. To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors. The results of our evaluation indicate that our automated approach can achieve data cleaning efficacy comparable with that of context models crafted by human experts.","sentences":["Machine learning's influence is expanding rapidly, now integral to decision-making processes from corporate strategy to the advancements in Industry 4.0.","The efficacy of Artificial Intelligence broadly hinges on the caliber of data used during its training phase; optimal performance is tied to exceptional data quality.","Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality.","Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts.   ","In light of these challenges, this paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets.","LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model.","To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors.","The results of our evaluation indicate that our automated approach can achieve data cleaning efficacy comparable with that of context models crafted by human experts."],"url":"http://arxiv.org/abs/2404.18681v1","category":"cs.DB"}
{"created":"2024-04-29 13:15:20","title":"Self-Propelled Collective Motion with Multiplicative Scalar Noise","abstract":"The emergence of order from initial disordered movement in self-propelled collective motion is an instance of nonequilibrium phase transition, which is known to be first order in the thermodynamic limit. Here, we introduce a multiplicative scalar noise model of collective motion as a modification of the original Vicsek model, which more closely mimics the particles' behavior. We allow for more individual movement in sparsely populated neighborhoods, the mechanism of which is not incorporated in the original Vicsek model. This is especially important in the low velocity and density regime where the probability of a clear neighborhood is relatively high. The modification, thus, removes the shortcoming of the Vicsek model in predicting continuous phase transition in this regime. The onset of collective motion in the proposed model is numerically studied in detail, indicating a first order phase transition in both high and low velocity/density regimes for systems with comparatively smaller size which is computationally desirable.","sentences":["The emergence of order from initial disordered movement in self-propelled collective motion is an instance of nonequilibrium phase transition, which is known to be first order in the thermodynamic limit.","Here, we introduce a multiplicative scalar noise model of collective motion as a modification of the original Vicsek model, which more closely mimics the particles' behavior.","We allow for more individual movement in sparsely populated neighborhoods, the mechanism of which is not incorporated in the original Vicsek model.","This is especially important in the low velocity and density regime where the probability of a clear neighborhood is relatively high.","The modification, thus, removes the shortcoming of the Vicsek model in predicting continuous phase transition in this regime.","The onset of collective motion in the proposed model is numerically studied in detail, indicating a first order phase transition in both high and low velocity/density regimes for systems with comparatively smaller size which is computationally desirable."],"url":"http://arxiv.org/abs/2404.18675v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 13:12:08","title":"Graph Convolutional Networks and Graph Attention Networks for Approximating Arguments Acceptability -- Technical Report","abstract":"Various approaches have been proposed for providing efficient computational approaches for abstract argumentation. Among them, neural networks have permitted to solve various decision problems, notably related to arguments (credulous or skeptical) acceptability. In this work, we push further this study in various ways. First, relying on the state-of-the-art approach AFGCN, we show how we can improve the performances of the Graph Convolutional Networks (GCNs) regarding both runtime and accuracy. Then, we show that it is possible to improve even more the efficiency of the approach by modifying the architecture of the network, using Graph Attention Networks (GATs) instead.","sentences":["Various approaches have been proposed for providing efficient computational approaches for abstract argumentation.","Among them, neural networks have permitted to solve various decision problems, notably related to arguments (credulous or skeptical) acceptability.","In this work, we push further this study in various ways.","First, relying on the state-of-the-art approach AFGCN, we show how we can improve the performances of the Graph Convolutional Networks (GCNs) regarding both runtime and accuracy.","Then, we show that it is possible to improve even more the efficiency of the approach by modifying the architecture of the network, using Graph Attention Networks (GATs) instead."],"url":"http://arxiv.org/abs/2404.18672v1","category":"cs.AI"}
{"created":"2024-04-29 12:57:05","title":"Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting","abstract":"Recent developments in neural rendering techniques have greatly enhanced the rendering of photo-realistic 3D scenes across both academic and commercial fields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new benchmarks for rendering quality and speed. Nevertheless, the limitations of 3D-GS become pronounced in synthesizing new viewpoints, especially for views that greatly deviate from those seen during training. Additionally, issues such as dilation and aliasing arise when zooming in or out. These challenges can all be traced back to a single underlying issue: insufficient sampling. In our paper, we present a bootstrapping method that significantly addresses this problem. This approach employs a diffusion model to enhance the rendering of novel views using trained 3D-GS, thereby streamlining the training process. Our results indicate that bootstrapping effectively reduces artifacts, as well as clear enhancements on the evaluation metrics. Furthermore, we show that our method is versatile and can be easily integrated, allowing various 3D reconstruction projects to benefit from our approach.","sentences":["Recent developments in neural rendering techniques have greatly enhanced the rendering of photo-realistic 3D scenes across both academic and commercial fields.","The latest method, known as 3D Gaussian Splatting (3D-GS), has set new benchmarks for rendering quality and speed.","Nevertheless, the limitations of 3D-GS become pronounced in synthesizing new viewpoints, especially for views that greatly deviate from those seen during training.","Additionally, issues such as dilation and aliasing arise when zooming in or out.","These challenges can all be traced back to a single underlying issue: insufficient sampling.","In our paper, we present a bootstrapping method that significantly addresses this problem.","This approach employs a diffusion model to enhance the rendering of novel views using trained 3D-GS, thereby streamlining the training process.","Our results indicate that bootstrapping effectively reduces artifacts, as well as clear enhancements on the evaluation metrics.","Furthermore, we show that our method is versatile and can be easily integrated, allowing various 3D reconstruction projects to benefit from our approach."],"url":"http://arxiv.org/abs/2404.18669v1","category":"cs.GR"}
{"created":"2024-04-29 12:46:33","title":"Diversity in the radiation-induced transcriptomic temporal response of mouse brain tissue regions","abstract":"A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities. This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation. This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation. Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy). This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months). The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA). It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees. The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals. Furthermore, in addition to radiation-responsive pathways, Cytoscape's functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2] [3], indicating late behavioural changes in animals prenatally exposed to radiation.","sentences":["A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities.","This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation.","This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation.","Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy).","This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months).","The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA).","It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees.","The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals.","Furthermore, in addition to radiation-responsive pathways, Cytoscape's functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2]","[3], indicating late behavioural changes in animals prenatally exposed to radiation."],"url":"http://arxiv.org/abs/2404.18660v1","category":"q-bio.NC"}
{"created":"2024-04-29 12:38:26","title":"Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods","abstract":"Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM's parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge.","sentences":["Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights.","The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining.","This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components.","Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically.","Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA.","To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA.","We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods.","Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA.","Nevertheless, IA provides unique and valuable insights into the LM's parametric knowledge, which are not revealed by NA.","Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge."],"url":"http://arxiv.org/abs/2404.18655v1","category":"cs.CL"}
{"created":"2024-04-29 12:32:14","title":"Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection","abstract":"In this paper we propose a new framework for evaluating the performance of explanation methods on the decisions of a deepfake detector. This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector's prediction or reduce its initial prediction; we anticipate a larger drop in deepfake detection accuracy and prediction, for methods that spot these regions more accurately. Based on this framework, we conduct a comparative study using a state-of-the-art model for deepfake detection that has been trained on the FaceForensics++ dataset, and five explanation methods from the literature. The findings of our quantitative and qualitative evaluations document the advanced performance of the LIME explanation method against the other compared ones, and indicate this method as the most appropriate for explaining the decisions of the utilized deepfake detector.","sentences":["In this paper we propose a new framework for evaluating the performance of explanation methods on the decisions of a deepfake detector.","This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector's prediction or reduce its initial prediction; we anticipate a larger drop in deepfake detection accuracy and prediction, for methods that spot these regions more accurately.","Based on this framework, we conduct a comparative study using a state-of-the-art model for deepfake detection that has been trained on the FaceForensics++ dataset, and five explanation methods from the literature.","The findings of our quantitative and qualitative evaluations document the advanced performance of the LIME explanation method against the other compared ones, and indicate this method as the most appropriate for explaining the decisions of the utilized deepfake detector."],"url":"http://arxiv.org/abs/2404.18649v1","category":"cs.CV"}
{"created":"2024-04-29 12:28:52","title":"Dynamical Photon Condensation into Wannier-Stark States","abstract":"Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand. Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice. The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint. We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes. Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations. As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations. These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system.","sentences":["Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand.","Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice.","The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint.","We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes.","Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations.","As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations.","These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system."],"url":"http://arxiv.org/abs/2404.18647v1","category":"quant-ph"}
{"created":"2024-04-29 12:16:08","title":"Reinforcement Learning Problem Solving with Large Language Models","abstract":"Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks. This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems. However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving. Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks. In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs. We then show the practicality of our approach through two detailed case studies for \"Research Scientist\" and \"Legal Matter Intake\" workflows.","sentences":["Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks.","This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems.","However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving.","Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks.","We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks.","In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs.","We then show the practicality of our approach through two detailed case studies for \"Research Scientist\" and \"Legal Matter Intake\" workflows."],"url":"http://arxiv.org/abs/2404.18638v1","category":"cs.AI"}
{"created":"2024-04-29 12:11:26","title":"Feature importance to explain multimodal prediction models. A clinical use case","abstract":"Surgery to treat elderly hip fracture patients may cause complications that can lead to early mortality. An early warning system for complications could provoke clinicians to monitor high-risk patients more carefully and address potential complications early, or inform the patient. In this work, we develop a multimodal deep-learning model for post-operative mortality prediction using pre-operative and per-operative data from elderly hip fracture patients. Specifically, we include static patient data, hip and chest images before surgery in pre-operative data, vital signals, and medications administered during surgery in per-operative data. We extract features from image modalities using ResNet and from vital signals using LSTM. Explainable model outcomes are essential for clinical applicability, therefore we compute Shapley values to explain the predictions of our multimodal black box model. We find that i) Shapley values can be used to estimate the relative contribution of each modality both locally and globally, and ii) a modified version of the chain rule can be used to propagate Shapley values through a sequence of models supporting interpretable local explanations. Our findings imply that a multimodal combination of black box models can be explained by propagating Shapley values through the model sequence.","sentences":["Surgery to treat elderly hip fracture patients may cause complications that can lead to early mortality.","An early warning system for complications could provoke clinicians to monitor high-risk patients more carefully and address potential complications early, or inform the patient.","In this work, we develop a multimodal deep-learning model for post-operative mortality prediction using pre-operative and per-operative data from elderly hip fracture patients.","Specifically, we include static patient data, hip and chest images before surgery in pre-operative data, vital signals, and medications administered during surgery in per-operative data.","We extract features from image modalities using ResNet and from vital signals using LSTM.","Explainable model outcomes are essential for clinical applicability, therefore we compute Shapley values to explain the predictions of our multimodal black box model.","We find that i)","Shapley values can be used to estimate the relative contribution of each modality both locally and globally, and ii) a modified version of the chain rule can be used to propagate Shapley values through a sequence of models supporting interpretable local explanations.","Our findings imply that a multimodal combination of black box models can be explained by propagating Shapley values through the model sequence."],"url":"http://arxiv.org/abs/2404.18631v1","category":"cs.LG"}
{"created":"2024-04-29 12:06:06","title":"4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations","abstract":"The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.","sentences":["The studies of human clothing for digital avatars have predominantly relied on synthetic datasets.","While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics.","Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes.","4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans.","Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans.","To address this, we develop a semi-automatic 4D human parsing pipeline.","We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements.","Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction.","4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing.","Website: https://ait.ethz.ch/4d-dress."],"url":"http://arxiv.org/abs/2404.18630v1","category":"cs.CV"}
{"created":"2024-04-29 11:52:20","title":"Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?","abstract":"Vision and language models (VLMs) are currently the most generally performant architectures on multimodal tasks. Next to their predictions, they can also produce explanations, either in post-hoc or CoT settings. However, it is not clear how much they use the vision and text modalities when generating predictions or explanations. In this work, we investigate if VLMs rely on modalities differently when generating explanations as opposed to when they provide answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing tests and measures to VLM decoders. We find that VLMs are less self-consistent than LLMs. The text contributions in VL decoders are much larger than the image contributions across all measured tasks. And the contributions of the image are significantly larger for explanation generations than for answer generation. This difference is even larger in CoT compared to the post-hoc explanation setting. We also provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which to date focused only on VL encoders. We find that VL decoders are still struggling with most phenomena tested by VALSE.","sentences":["Vision and language models (VLMs) are currently the most generally performant architectures on multimodal tasks.","Next to their predictions, they can also produce explanations, either in post-hoc or CoT settings.","However, it is not clear how much they use the vision and text modalities when generating predictions or explanations.","In this work, we investigate if VLMs rely on modalities differently when generating explanations as opposed to when they provide answers.","We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing tests and measures to VLM decoders.","We find that VLMs are less self-consistent than LLMs.","The text contributions in VL decoders are much larger than the image contributions across all measured tasks.","And the contributions of the image are significantly larger for explanation generations than for answer generation.","This difference is even larger in CoT compared to the post-hoc explanation setting.","We also provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which to date focused only on VL encoders.","We find that VL decoders are still struggling with most phenomena tested by VALSE."],"url":"http://arxiv.org/abs/2404.18624v1","category":"cs.CL"}
{"created":"2024-04-29 11:40:27","title":"CoSense3D: an Agent-based Efficient Learning Framework for Collective Perception","abstract":"Collective Perception has attracted significant attention in recent years due to its advantage for mitigating occlusion and expanding the field-of-view, thereby enhancing reliability, efficiency, and, most crucially, decision-making safety. However, developing collective perception models is highly resource demanding due to extensive requirements of processing input data for many agents, usually dozens of images and point clouds for a single frame. This not only slows down the model development process for collective perception but also impedes the utilization of larger models. In this paper, we propose an agent-based training framework that handles the deep learning modules and agent data separately to have a cleaner data flow structure. This framework not only provides an API for flexibly prototyping the data processing pipeline and defining the gradient calculation for each agent, but also provides the user interface for interactive training, testing and data visualization. Training experiment results of four collective object detection models on the prominent collective perception benchmark OPV2V show that the agent-based training can significantly reduce the GPU memory consumption and training time while retaining inference performance. The framework and model implementations are available at \\url{https://github.com/YuanYunshuang/CoSense3D}","sentences":["Collective Perception has attracted significant attention in recent years due to its advantage for mitigating occlusion and expanding the field-of-view, thereby enhancing reliability, efficiency, and, most crucially, decision-making safety.","However, developing collective perception models is highly resource demanding due to extensive requirements of processing input data for many agents, usually dozens of images and point clouds for a single frame.","This not only slows down the model development process for collective perception but also impedes the utilization of larger models.","In this paper, we propose an agent-based training framework that handles the deep learning modules and agent data separately to have a cleaner data flow structure.","This framework not only provides an API for flexibly prototyping the data processing pipeline and defining the gradient calculation for each agent, but also provides the user interface for interactive training, testing and data visualization.","Training experiment results of four collective object detection models on the prominent collective perception benchmark OPV2V show that the agent-based training can significantly reduce the GPU memory consumption and training time while retaining inference performance.","The framework and model implementations are available at \\url{https://github.com/YuanYunshuang/CoSense3D}"],"url":"http://arxiv.org/abs/2404.18617v1","category":"cs.CV"}
{"created":"2024-04-29 11:30:50","title":"Enhancing Prosthetic Safety and Environmental Adaptability: A Visual-Inertial Prosthesis Motion Estimation Approach on Uneven Terrains","abstract":"Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles. However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences. In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them. To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles. Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation. The motion of prosthetic joint and toe are derived using the prosthesis model parameters. Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm. The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains.","sentences":["Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles.","However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences.","In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them.","To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles.","Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation.","The motion of prosthetic joint and toe are derived using the prosthesis model parameters.","Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm.","The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains."],"url":"http://arxiv.org/abs/2404.18612v1","category":"cs.RO"}
{"created":"2024-04-29 11:19:15","title":"CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial Animation Generation","abstract":"Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods.","sentences":["Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations.","The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions.","Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations.","Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions.","To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns.","To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions.","We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions.","Experimental results demonstrate that our method outperforms existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.18604v1","category":"cs.CV"}
{"created":"2024-04-29 11:17:42","title":"Unraveling the Italian and English Telegram Conspiracy Spheres through Message Forwarding","abstract":"Telegram has grown into a significant platform for news and information sharing, favored for its anonymity and minimal moderation. This openness, however, makes it vulnerable to misinformation and conspiracy theories. In this study, we explore the dynamics of conspiratorial narrative dissemination within Telegram, focusing on Italian and English landscapes. In particular, we leverage the mechanism of message forwarding within Telegram and collect two extensive datasets through snowball strategy. We adopt a network-based approach and build the Italian and English Telegram networks to reveal their respective communities. By employing topic modeling, we uncover distinct narratives and dynamics of misinformation spread. Results highlight differences between Italian and English conspiracy landscapes, with Italian discourse involving assorted conspiracy theories and alternative news sources intertwined with legitimate news sources, whereas English discourse is characterized by a more focused approach on specific narratives such as QAnon and political conspiracies. Finally, we show that our methodology exhibits robustness across initial seed selections, suggesting broader applicability. This study contributes to understanding information and misinformation spread on Italian and English Telegram ecosystems through the mechanism of message forwarding","sentences":["Telegram has grown into a significant platform for news and information sharing, favored for its anonymity and minimal moderation.","This openness, however, makes it vulnerable to misinformation and conspiracy theories.","In this study, we explore the dynamics of conspiratorial narrative dissemination within Telegram, focusing on Italian and English landscapes.","In particular, we leverage the mechanism of message forwarding within Telegram and collect two extensive datasets through snowball strategy.","We adopt a network-based approach and build the Italian and English Telegram networks to reveal their respective communities.","By employing topic modeling, we uncover distinct narratives and dynamics of misinformation spread.","Results highlight differences between Italian and English conspiracy landscapes, with Italian discourse involving assorted conspiracy theories and alternative news sources intertwined with legitimate news sources, whereas English discourse is characterized by a more focused approach on specific narratives such as QAnon and political conspiracies.","Finally, we show that our methodology exhibits robustness across initial seed selections, suggesting broader applicability.","This study contributes to understanding information and misinformation spread on Italian and English Telegram ecosystems through the mechanism of message forwarding"],"url":"http://arxiv.org/abs/2404.18602v1","category":"cs.SI"}
{"created":"2024-04-29 11:11:26","title":"FauxPy: A Fault Localization Tool for Python","abstract":"This paper presents FauxPy, a fault localization tool for Python programs. FauxPy supports seven well-known fault localization techniques in four families: spectrum-based, mutation-based, predicate switching, and stack trace fault localization. It is implemented as plugin of the popular Pytest testing framework, but also works with tests written for Unittest and Hypothesis (two other popular testing frameworks). The paper showcases how to use FauxPy on two illustrative examples, and then discusses its main features and capabilities from a user's perspective. To demonstrate that FauxPy is applicable to analyze Python projects of realistic size, the paper also summarizes the results of an extensive experimental evaluation that applied FauxPy to 135 real-world bugs from the BugsInPy curated collection. To our knowledge, FauxPy is the first open-source fault localization tool for Python that supports multiple fault localization families.","sentences":["This paper presents FauxPy, a fault localization tool for Python programs.","FauxPy supports seven well-known fault localization techniques in four families: spectrum-based, mutation-based, predicate switching, and stack trace fault localization.","It is implemented as plugin of the popular Pytest testing framework, but also works with tests written for Unittest and Hypothesis (two other popular testing frameworks).","The paper showcases how to use FauxPy on two illustrative examples, and then discusses its main features and capabilities from a user's perspective.","To demonstrate that FauxPy is applicable to analyze Python projects of realistic size, the paper also summarizes the results of an extensive experimental evaluation that applied FauxPy to 135 real-world bugs from the BugsInPy curated collection.","To our knowledge, FauxPy is the first open-source fault localization tool for Python that supports multiple fault localization families."],"url":"http://arxiv.org/abs/2404.18596v1","category":"cs.SE"}
{"created":"2024-04-29 10:12:04","title":"Injecting Salesperson's Dialogue Strategies in Large Language Models with Chain-of-Thought Reasoning","abstract":"Recent research in dialogue systems and corpora has focused on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD systems help users accomplish specific tasks, while open-domain systems aim to create engaging conversations. However, in real-world scenarios, user intents are often revealed during interactions. A recent study introduced SalesBot, which simulates dialogues transitioning from chit-chat to task-oriented scenarios to train sales agents. Unfortunately, the initial data lacked smooth transitions and coherent long-turn dialogues, resulting in poor naturalness in sales-customer interactions. To address these issues, this paper presents SalesBot 2.0, an improved dataset. It leverages commonsense knowledge from large language models (LLMs) through strategic prompting. Additionally, we introduce a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning. This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies. Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs. Furthermore, SalesBot 2.0 enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions.","sentences":["Recent research in dialogue systems and corpora has focused on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues.","TOD systems help users accomplish specific tasks, while open-domain systems aim to create engaging conversations.","However, in real-world scenarios, user intents are often revealed during interactions.","A recent study introduced SalesBot, which simulates dialogues transitioning from chit-chat to task-oriented scenarios to train sales agents.","Unfortunately, the initial data lacked smooth transitions and coherent long-turn dialogues, resulting in poor naturalness in sales-customer interactions.","To address these issues, this paper presents SalesBot 2.0, an improved dataset.","It leverages commonsense knowledge from large language models (LLMs) through strategic prompting.","Additionally, we introduce a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning.","This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies.","Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs.","Furthermore, SalesBot 2.0 enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions."],"url":"http://arxiv.org/abs/2404.18564v1","category":"cs.CL"}
{"created":"2024-04-29 10:02:45","title":"LangBiTe: A Platform for Testing Bias in Large Language Models","abstract":"The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases. Typically, those models are trained on a vast amount of data scrapped from forums, websites, social media and other internet sources, which may instill harmful and discriminating behavior into the model. To address this issue, we present LangBiTe, a testing platform to systematically assess the presence of biases within an LLM. LangBiTe enables development teams to tailor their test scenarios, and automatically generate and execute the test cases according to a set of user-defined ethical requirements. Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLM's response for the identification of biases. LangBite provides users with the bias evaluation of LLMs, and end-to-end traceability between the initial ethical requirements and the insights obtained.","sentences":["The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases.","Typically, those models are trained on a vast amount of data scrapped from forums, websites, social media and other internet sources, which may instill harmful and discriminating behavior into the model.","To address this issue, we present LangBiTe, a testing platform to systematically assess the presence of biases within an LLM.","LangBiTe enables development teams to tailor their test scenarios, and automatically generate and execute the test cases according to a set of user-defined ethical requirements.","Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLM's response for the identification of biases.","LangBite provides users with the bias evaluation of LLMs, and end-to-end traceability between the initial ethical requirements and the insights obtained."],"url":"http://arxiv.org/abs/2404.18558v1","category":"cs.SE"}
{"created":"2024-04-29 09:54:06","title":"Machine Learning for Quantum Computing Specialists","abstract":"Quantum machine learning (QML) is a promising early use case for quantum computing. There has been progress in the last five years from theoretical studies and numerical simulations to proof of concepts. Use cases demonstrated on contemporary quantum devices include classifying medical images and items from the Iris dataset, classifying and generating handwritten images, toxicity screening, and learning a probability distribution. Potential benefits of QML include faster training and identification of feature maps not found classically. Although, these examples lack the scale for commercial exploitation, and it may be several years before QML algorithms replace the classical solutions, QML is an exciting area.   This article is written for those who already have a sound knowledge of quantum computing and now wish to gain a basic overview of the terminology and some applications of classical machine learning ready to study quantum machine learning. The reader will already understand the relevant relevant linear algebra, including Hilbert spaces, a vector space with an inner product.","sentences":["Quantum machine learning (QML) is a promising early use case for quantum computing.","There has been progress in the last five years from theoretical studies and numerical simulations to proof of concepts.","Use cases demonstrated on contemporary quantum devices include classifying medical images and items from the Iris dataset, classifying and generating handwritten images, toxicity screening, and learning a probability distribution.","Potential benefits of QML include faster training and identification of feature maps not found classically.","Although, these examples lack the scale for commercial exploitation, and it may be several years before QML algorithms replace the classical solutions, QML is an exciting area.   ","This article is written for those who already have a sound knowledge of quantum computing and now wish to gain a basic overview of the terminology and some applications of classical machine learning ready to study quantum machine learning.","The reader will already understand the relevant relevant linear algebra, including Hilbert spaces, a vector space with an inner product."],"url":"http://arxiv.org/abs/2404.18555v1","category":"quant-ph"}
{"created":"2024-04-29 09:51:25","title":"Evaluating the effectiveness of predicting covariates in LSTM Networks for Time Series Forecasting","abstract":"Autoregressive Recurrent Neural Networks are widely employed in time-series forecasting tasks, demonstrating effectiveness in univariate and certain multivariate scenarios. However, their inherent structure does not readily accommodate the integration of future, time-dependent covariates. A proposed solution, outlined by Salinas et al 2019, suggests forecasting both covariates and the target variable in a multivariate framework. In this study, we conducted comprehensive tests on publicly available time-series datasets, artificially introducing highly correlated covariates to future time-step values. Our evaluation aimed to assess the performance of an LSTM network when considering these covariates and compare it against a univariate baseline. As part of this study we introduce a novel approach using seasonal time segments in combination with an RNN architecture, which is both simple and extremely effective over long forecast horizons with comparable performance to many state of the art architectures. Our findings from the results of more than 120 models reveal that under certain conditions jointly training covariates with target variables can improve overall performance of the model, but often there exists a significant performance disparity between multivariate and univariate predictions. Surprisingly, even when provided with covariates informing the network about future target values, multivariate predictions exhibited inferior performance. In essence, compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates. These results suggest that LSTM architectures may not be suitable for forecasting tasks where predicting covariates would typically be expected to enhance model accuracy.","sentences":["Autoregressive Recurrent Neural Networks are widely employed in time-series forecasting tasks, demonstrating effectiveness in univariate and certain multivariate scenarios.","However, their inherent structure does not readily accommodate the integration of future, time-dependent covariates.","A proposed solution, outlined by Salinas et al 2019, suggests forecasting both covariates and the target variable in a multivariate framework.","In this study, we conducted comprehensive tests on publicly available time-series datasets, artificially introducing highly correlated covariates to future time-step values.","Our evaluation aimed to assess the performance of an LSTM network when considering these covariates and compare it against a univariate baseline.","As part of this study we introduce a novel approach using seasonal time segments in combination with an RNN architecture, which is both simple and extremely effective over long forecast horizons with comparable performance to many state of the art architectures.","Our findings from the results of more than 120 models reveal that under certain conditions jointly training covariates with target variables can improve overall performance of the model, but often there exists a significant performance disparity between multivariate and univariate predictions.","Surprisingly, even when provided with covariates informing the network about future target values, multivariate predictions exhibited inferior performance.","In essence, compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates.","These results suggest that LSTM architectures may not be suitable for forecasting tasks where predicting covariates would typically be expected to enhance model accuracy."],"url":"http://arxiv.org/abs/2404.18553v1","category":"cs.LG"}
{"created":"2024-04-29 09:50:16","title":"SIDBench: A Python Framework for Reliably Assessing Synthetic Image Detection Methods","abstract":"The generative AI technology offers an increasing variety of tools for generating entirely synthetic images that are increasingly indistinguishable from real ones. Unlike methods that alter portions of an image, the creation of completely synthetic images presents a unique challenge and several Synthetic Image Detection (SID) methods have recently appeared to tackle it. Yet, there is often a large gap between experimental results on benchmark datasets and the performance of methods in the wild. To better address the evaluation needs of SID and help close this gap, this paper introduces a benchmarking framework that integrates several state-of-the-art SID models. Our selection of integrated models was based on the utilization of varied input features, and different network architectures, aiming to encompass a broad spectrum of techniques. The framework leverages recent datasets with a diverse set of generative models, high level of photo-realism and resolution, reflecting the rapid improvements in image synthesis technology. Additionally, the framework enables the study of how image transformations, common in assets shared online, such as JPEG compression, affect detection performance. SIDBench is available on https://github.com/mever-team/sidbench and is designed in a modular manner to enable easy inclusion of new datasets and SID models.","sentences":["The generative AI technology offers an increasing variety of tools for generating entirely synthetic images that are increasingly indistinguishable from real ones.","Unlike methods that alter portions of an image, the creation of completely synthetic images presents a unique challenge and several Synthetic Image Detection (SID) methods have recently appeared to tackle it.","Yet, there is often a large gap between experimental results on benchmark datasets and the performance of methods in the wild.","To better address the evaluation needs of SID and help close this gap, this paper introduces a benchmarking framework that integrates several state-of-the-art SID models.","Our selection of integrated models was based on the utilization of varied input features, and different network architectures, aiming to encompass a broad spectrum of techniques.","The framework leverages recent datasets with a diverse set of generative models, high level of photo-realism and resolution, reflecting the rapid improvements in image synthesis technology.","Additionally, the framework enables the study of how image transformations, common in assets shared online, such as JPEG compression, affect detection performance.","SIDBench is available on https://github.com/mever-team/sidbench and is designed in a modular manner to enable easy inclusion of new datasets and SID models."],"url":"http://arxiv.org/abs/2404.18552v1","category":"cs.CV"}
{"created":"2024-04-29 09:45:46","title":"IncidentResponseGPT: Generating Traffic Incident Response Plans with Generative Artificial Intelligence","abstract":"Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion. Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle. This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans. By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents. The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems. Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents. The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management. By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems.","sentences":["Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion.","Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle.","This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans.","By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents.","The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems.","Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents.","The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management.","By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems."],"url":"http://arxiv.org/abs/2404.18550v1","category":"cs.LG"}
{"created":"2024-04-29 09:37:24","title":"ir_explain: a Python Library of Explainable IR Methods","abstract":"While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods. Consequently, Explainability and Interpretability have emerged as important research topics in IR. Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed. This article presents \\irexplain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework. \\irexplain supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations. The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods. To facilitate adoption, \\irexplain is well-integrated with widely-used toolkits such as Pyserini and \\irdatasets.","sentences":["While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods.","Consequently, Explainability and Interpretability have emerged as important research topics in IR.","Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed.","This article presents \\irexplain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework.","\\irexplain supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations.","The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods.","To facilitate adoption, \\irexplain is well-integrated with widely-used toolkits such as Pyserini and \\irdatasets."],"url":"http://arxiv.org/abs/2404.18546v1","category":"cs.IR"}
{"created":"2024-04-29 09:28:57","title":"Machine Learning for Windows Malware Detection and Classification: Methods, Challenges and Ongoing Research","abstract":"In this chapter, readers will explore how machine learning has been applied to build malware detection systems designed for the Windows operating system. This chapter starts by introducing the main components of a Machine Learning pipeline, highlighting the challenges of collecting and maintaining up-to-date datasets. Following this introduction, various state-of-the-art malware detectors are presented, encompassing both feature-based and deep learning-based detectors. Subsequent sections introduce the primary challenges encountered by machine learning-based malware detectors, including concept drift and adversarial attacks. Lastly, this chapter concludes by providing a brief overview of the ongoing research on adversarial defenses.","sentences":["In this chapter, readers will explore how machine learning has been applied to build malware detection systems designed for the Windows operating system.","This chapter starts by introducing the main components of a Machine Learning pipeline, highlighting the challenges of collecting and maintaining up-to-date datasets.","Following this introduction, various state-of-the-art malware detectors are presented, encompassing both feature-based and deep learning-based detectors.","Subsequent sections introduce the primary challenges encountered by machine learning-based malware detectors, including concept drift and adversarial attacks.","Lastly, this chapter concludes by providing a brief overview of the ongoing research on adversarial defenses."],"url":"http://arxiv.org/abs/2404.18541v1","category":"cs.CR"}
{"created":"2024-04-29 09:27:31","title":"Enhancing Boundary Segmentation for Topological Accuracy with Skeleton-based Methods","abstract":"Topological consistency plays a crucial role in the task of boundary segmentation for reticular images, such as cell membrane segmentation in neuron electron microscopic images, grain boundary segmentation in material microscopic images and road segmentation in aerial images. In these fields, topological changes in segmentation results have a serious impact on the downstream tasks, which can even exceed the misalignment of the boundary itself. To enhance the topology accuracy in segmentation results, we propose the Skea-Topo Aware loss, which is a novel loss function that takes into account the shape of each object and topological significance of the pixels. It consists of two components. First, the skeleton-aware weighted loss improves the segmentation accuracy by better modeling the object geometry with skeletons. Second, a boundary rectified term effectively identifies and emphasizes topological critical pixels in the prediction errors using both foreground and background skeletons in the ground truth and predictions. Experiments prove that our method improves topological consistency by up to 7 points in VI compared to 13 state-of-art methods, based on objective and subjective assessments across three different boundary segmentation datasets. The code is available at https://github.com/clovermini/Skea_topo.","sentences":["Topological consistency plays a crucial role in the task of boundary segmentation for reticular images, such as cell membrane segmentation in neuron electron microscopic images, grain boundary segmentation in material microscopic images and road segmentation in aerial images.","In these fields, topological changes in segmentation results have a serious impact on the downstream tasks, which can even exceed the misalignment of the boundary itself.","To enhance the topology accuracy in segmentation results, we propose the Skea-Topo Aware loss, which is a novel loss function that takes into account the shape of each object and topological significance of the pixels.","It consists of two components.","First, the skeleton-aware weighted loss improves the segmentation accuracy by better modeling the object geometry with skeletons.","Second, a boundary rectified term effectively identifies and emphasizes topological critical pixels in the prediction errors using both foreground and background skeletons in the ground truth and predictions.","Experiments prove that our method improves topological consistency by up to 7 points in VI compared to 13 state-of-art methods, based on objective and subjective assessments across three different boundary segmentation datasets.","The code is available at https://github.com/clovermini/Skea_topo."],"url":"http://arxiv.org/abs/2404.18539v1","category":"cs.CV"}
{"created":"2024-04-29 09:27:15","title":"Time Series Data Augmentation as an Imbalanced Learning Problem","abstract":"Recent state-of-the-art forecasting methods are trained on collections of time series. These methods, often referred to as global models, can capture common patterns in different time series to improve their generalization performance. However, they require large amounts of data that might not be readily available. Besides this, global models sometimes fail to capture relevant patterns unique to a particular time series. In these cases, data augmentation can be useful to increase the sample size of time series datasets. The main contribution of this work is a novel method for generating univariate time series synthetic samples. Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations. In this context, we frame the problem of training a forecasting model as an imbalanced learning task. Oversampling strategies are popular approaches used to deal with the imbalance problem in machine learning. We use these techniques to create synthetic time series observations and improve the accuracy of forecasting models. We carried out experiments using 7 different databases that contain a total of 5502 univariate time series. We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches.","sentences":["Recent state-of-the-art forecasting methods are trained on collections of time series.","These methods, often referred to as global models, can capture common patterns in different time series to improve their generalization performance.","However, they require large amounts of data that might not be readily available.","Besides this, global models sometimes fail to capture relevant patterns unique to a particular time series.","In these cases, data augmentation can be useful to increase the sample size of time series datasets.","The main contribution of this work is a novel method for generating univariate time series synthetic samples.","Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations.","In this context, we frame the problem of training a forecasting model as an imbalanced learning task.","Oversampling strategies are popular approaches used to deal with the imbalance problem in machine learning.","We use these techniques to create synthetic time series observations and improve the accuracy of forecasting models.","We carried out experiments using 7 different databases that contain a total of 5502 univariate time series.","We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches."],"url":"http://arxiv.org/abs/2404.18537v1","category":"cs.LG"}
{"created":"2024-04-29 09:22:54","title":"Evaluating and Mitigating Linguistic Discrimination in Large Language Models","abstract":"By training on text in various languages, large language models (LLMs) typically possess multilingual support and demonstrate remarkable capabilities in solving tasks described in different languages. However, LLMs can exhibit linguistic discrimination due to the uneven distribution of training data across languages. That is, LLMs are hard to keep the consistency of responses when faced with the same task but depicted in different languages.   In this study, we first explore the consistency in the LLMs' outputs responding to queries in various languages from two aspects: safety and quality. We conduct this analysis with two datasets (AdvBench and NQ) based on four LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro). The results show that LLMs exhibit stronger human alignment capabilities with queries in English, French, Russian, and Spanish (only 1.04\\% of harmful queries successfully jailbreak on average) compared to queries in Bengali, Georgian, Nepali and Maithili (27.7\\% of harmful queries jailbreak successfully on average). Moreover, for queries in English, Danish, Czech and Slovenian, LLMs tend to produce responses with a higher quality (with 0.1494 $F_1$ score on average) compared to the other languages. Upon these findings, we propose LDFighter, a similarity-based voting, to mitigate the linguistic discrimination in LLMs. LDFighter ensures consistent service for different language speakers. We evaluate LDFighter with both benign queries and harmful queries. The results show that LDFighter not only significantly reduces the jailbreak success rate but also improve the response quality on average, demonstrating its effectiveness.","sentences":["By training on text in various languages, large language models (LLMs) typically possess multilingual support and demonstrate remarkable capabilities in solving tasks described in different languages.","However, LLMs can exhibit linguistic discrimination due to the uneven distribution of training data across languages.","That is, LLMs are hard to keep the consistency of responses when faced with the same task but depicted in different languages.   ","In this study, we first explore the consistency in the LLMs' outputs responding to queries in various languages from two aspects: safety and quality.","We conduct this analysis with two datasets (AdvBench and NQ) based on four LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro).","The results show that LLMs exhibit stronger human alignment capabilities with queries in English, French, Russian, and Spanish (only 1.04\\% of harmful queries successfully jailbreak on average) compared to queries in Bengali, Georgian, Nepali and Maithili (27.7\\% of harmful queries jailbreak successfully on average).","Moreover, for queries in English, Danish, Czech and Slovenian, LLMs tend to produce responses with a higher quality (with 0.1494 $F_1$ score on average) compared to the other languages.","Upon these findings, we propose LDFighter, a similarity-based voting, to mitigate the linguistic discrimination in LLMs.","LDFighter ensures consistent service for different language speakers.","We evaluate LDFighter with both benign queries and harmful queries.","The results show that LDFighter not only significantly reduces the jailbreak success rate but also improve the response quality on average, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2404.18534v1","category":"cs.CL"}
{"created":"2024-04-29 09:20:25","title":"Evaluating Concept-based Explanations of Language Models: A Study on Faithfulness and Readability","abstract":"Despite the surprisingly high intelligence exhibited by Large Language Models (LLMs), we are somehow intimidated to fully deploy them into real-life applications considering their black-box nature. Concept-based explanations arise as a promising avenue for explaining what the LLMs have learned, making them more transparent to humans. However, current evaluations for concepts tend to be heuristic and non-deterministic, e.g. case study or human evaluation, hindering the development of the field. To bridge the gap, we approach concept-based explanation evaluation via faithfulness and readability. We first introduce a formal definition of concept generalizable to diverse concept-based explanations. Based on this, we quantify faithfulness via the difference in the output upon perturbation. We then provide an automatic measure for readability, by measuring the coherence of patterns that maximally activate a concept. This measure serves as a cost-effective and reliable substitute for human evaluation. Finally, based on measurement theory, we describe a meta-evaluation method for evaluating the above measures via reliability and validity, which can be generalized to other tasks as well. Extensive experimental analysis has been conducted to validate and inform the selection of concept evaluation measures.","sentences":["Despite the surprisingly high intelligence exhibited by Large Language Models (LLMs), we are somehow intimidated to fully deploy them into real-life applications considering their black-box nature.","Concept-based explanations arise as a promising avenue for explaining what the LLMs have learned, making them more transparent to humans.","However, current evaluations for concepts tend to be heuristic and non-deterministic, e.g. case study or human evaluation, hindering the development of the field.","To bridge the gap, we approach concept-based explanation evaluation via faithfulness and readability.","We first introduce a formal definition of concept generalizable to diverse concept-based explanations.","Based on this, we quantify faithfulness via the difference in the output upon perturbation.","We then provide an automatic measure for readability, by measuring the coherence of patterns that maximally activate a concept.","This measure serves as a cost-effective and reliable substitute for human evaluation.","Finally, based on measurement theory, we describe a meta-evaluation method for evaluating the above measures via reliability and validity, which can be generalized to other tasks as well.","Extensive experimental analysis has been conducted to validate and inform the selection of concept evaluation measures."],"url":"http://arxiv.org/abs/2404.18533v2","category":"cs.AI"}
{"created":"2024-04-29 09:19:05","title":"MileBench: Benchmarking MLLMs in Long Context","abstract":"Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.","sentences":["Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope.","Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs.","To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.","This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation.","We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios.","Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations.","Interestingly, the performance gap tends to widen with an increase in the number of images.","We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images."],"url":"http://arxiv.org/abs/2404.18532v1","category":"cs.CL"}
{"created":"2024-04-29 09:17:36","title":"A Framework to Model ML Engineering Processes","abstract":"The development of Machine Learning (ML) based systems is complex and requires multidisciplinary teams with diverse skill sets. This may lead to communication issues or misapplication of best practices. Process models can alleviate these challenges by standardizing task orchestration, providing a common language to facilitate communication, and nurturing a collaborative environment. Unfortunately, current process modeling languages are not suitable for describing the development of such systems. In this paper, we introduce a framework for modeling ML-based software development processes, built around a domain-specific language and derived from an analysis of scientific and gray literature. A supporting toolkit is also available.","sentences":["The development of Machine Learning (ML) based systems is complex and requires multidisciplinary teams with diverse skill sets.","This may lead to communication issues or misapplication of best practices.","Process models can alleviate these challenges by standardizing task orchestration, providing a common language to facilitate communication, and nurturing a collaborative environment.","Unfortunately, current process modeling languages are not suitable for describing the development of such systems.","In this paper, we introduce a framework for modeling ML-based software development processes, built around a domain-specific language and derived from an analysis of scientific and gray literature.","A supporting toolkit is also available."],"url":"http://arxiv.org/abs/2404.18531v1","category":"cs.SE"}
{"created":"2024-04-29 09:12:31","title":"Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning","abstract":"Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns. This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties. Hyperparameter tuning of the models is achieved through Bayesian Optimization. To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector. The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy. FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model. The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters. This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques.","sentences":["Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns.","This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties.","Hyperparameter tuning of the models is achieved through Bayesian Optimization.","To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector.","The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy.","FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model.","The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters.","This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques."],"url":"http://arxiv.org/abs/2404.18527v1","category":"cs.LG"}
{"created":"2024-04-29 09:05:01","title":"On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks","abstract":"Federated Learning (FL) allows multiple privacy-sensitive applications to leverage their dataset for a global model construction without any disclosure of the information. One of those domains is healthcare, where groups of silos collaborate in order to generate a global predictor with improved accuracy and generalization. However, the inherent challenge lies in the high heterogeneity of medical data, necessitating sophisticated techniques for assessment and compensation. This paper presents a comprehensive exploration of the mathematical formalization and taxonomy of heterogeneity within FL environments, focusing on the intricacies of medical data. In particular, we address the evaluation and comparison of the most popular FL algorithms with respect to their ability to cope with quantity-based, feature and label distribution-based heterogeneity. The goal is to provide a quantitative evaluation of the impact of data heterogeneity in FL systems for healthcare networks as well as a guideline on FL algorithm selection. Our research extends beyond existing studies by benchmarking seven of the most common FL algorithms against the unique challenges posed by medical data use cases. The paper targets the prediction of the risk of stroke recurrence through a set of tabular clinical reports collected by different federated hospital silos: data heterogeneity frequently encountered in this scenario and its impact on FL performance are discussed.","sentences":["Federated Learning (FL) allows multiple privacy-sensitive applications to leverage their dataset for a global model construction without any disclosure of the information.","One of those domains is healthcare, where groups of silos collaborate in order to generate a global predictor with improved accuracy and generalization.","However, the inherent challenge lies in the high heterogeneity of medical data, necessitating sophisticated techniques for assessment and compensation.","This paper presents a comprehensive exploration of the mathematical formalization and taxonomy of heterogeneity within FL environments, focusing on the intricacies of medical data.","In particular, we address the evaluation and comparison of the most popular FL algorithms with respect to their ability to cope with quantity-based, feature and label distribution-based heterogeneity.","The goal is to provide a quantitative evaluation of the impact of data heterogeneity in FL systems for healthcare networks as well as a guideline on FL algorithm selection.","Our research extends beyond existing studies by benchmarking seven of the most common FL algorithms against the unique challenges posed by medical data use cases.","The paper targets the prediction of the risk of stroke recurrence through a set of tabular clinical reports collected by different federated hospital silos: data heterogeneity frequently encountered in this scenario and its impact on FL performance are discussed."],"url":"http://arxiv.org/abs/2404.18519v1","category":"cs.LG"}
{"created":"2024-04-29 09:03:19","title":"From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital Humanities Research and Services?","abstract":"Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application. This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation. The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research. Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books. Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation. Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies.","sentences":["Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application.","This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation.","The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research.","Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books.","Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation.","Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies."],"url":"http://arxiv.org/abs/2404.18518v1","category":"cs.DL"}
{"created":"2024-04-29 09:01:39","title":"Downlink Pilots are Essential for Cell-Free Massive MIMO with Multi-Antenna Users","abstract":"We consider a cell-free massive MIMO system with multiple antennas on the users and access points. In previous works, the downlink spectral efficiency (SE) has been evaluated using the hardening bound that requires no downlink pilots. This approach works well when having single-antenna users. In this paper, we show that much higher SEs can be achieved if downlink pilots are sent since the effective channel matrix does not harden when having multi-antenna users. We propose a pilot-based downlink estimation scheme and derive a new SE expression that utilizes zero-forcing combining. We show numerically how the number of users and user antennas affects the SE.","sentences":["We consider a cell-free massive MIMO system with multiple antennas on the users and access points.","In previous works, the downlink spectral efficiency (SE) has been evaluated using the hardening bound that requires no downlink pilots.","This approach works well when having single-antenna users.","In this paper, we show that much higher SEs can be achieved if downlink pilots are sent since the effective channel matrix does not harden when having multi-antenna users.","We propose a pilot-based downlink estimation scheme and derive a new SE expression that utilizes zero-forcing combining.","We show numerically how the number of users and user antennas affects the SE."],"url":"http://arxiv.org/abs/2404.18516v1","category":"eess.SP"}
{"created":"2024-04-29 17:59:16","title":"Symmetry defect of $n$-dimensional complete intersections in $\\mathbb{C}^{2n-1}$","abstract":"Let $X, Y \\subset \\mathbb{C}^{2n-1}$ be $n$-dimensional strong complete intersections in a general position. In this note, we consider the set of midpoints of chords connecting a point $x \\in X$ to a point $y \\in Y$. This set is defined as the image of the map $\\Phi(x,y)=\\frac{x+y}{2}.$ Under geometric conditions on $X$ and $Y$, we prove that the symmetry defect of $X$ and $Y$, which is the bifurcation set $B(X,Y)$ of the mapping $\\Phi$, is an algebraic variety, characterized by a topological invariant. We introduce a hypersurface that approximates the set $B(X,Y)$ and we present an estimate for its degree. Moreover, for any two $n$-dimensional strong complete intersections $X,Y\\subset \\mathbb{C}^{2n-1}$ (including the case $X=Y$) we introduce a generic symmetry defect set $\\tilde{B}(X,Y)$ of $X$ and $Y$, which is defined up to homeomorphism.","sentences":["Let $X, Y \\subset \\mathbb{C}^{2n-1}$ be $n$-dimensional strong complete intersections in a general position.","In this note, we consider the set of midpoints of chords connecting a point $x \\in X$ to a point $y \\in Y$.","This set is defined as the image of the map $\\Phi(x,y)=\\frac{x+y}{2}.$ Under geometric conditions on $X$ and $Y$, we prove that the symmetry defect of $X$ and $Y$, which is the bifurcation set $B(X,Y)$ of the mapping $\\Phi$, is an algebraic variety, characterized by a topological invariant.","We introduce a hypersurface that approximates the set $B(X,Y)$ and we present an estimate for its degree.","Moreover, for any two $n$-dimensional strong complete intersections $X,Y\\subset \\mathbb{C}^{2n-1}$ (including the case $X=Y$) we introduce a generic symmetry defect set $\\tilde{B}(X,Y)$ of $X$ and $Y$, which is defined up to homeomorphism."],"url":"http://arxiv.org/abs/2404.18927v1","category":"math.AG"}
{"created":"2024-04-29 17:54:06","title":"Two-axis twisting using Floquet-engineered XYZ spin models with polar molecules","abstract":"Polar molecules confined in an optical lattice are a versatile platform to explore spin-motion dynamics based on strong, long-range dipolar interactions. The precise tunability of Ising and spin-exchange interactions with both microwave and dc electric fields makes the molecular system particularly suitable for engineering complex many-body dynamics. Here, we used Floquet engineering to realize interesting quantum many-body systems of polar molecules. Using a spin encoded in the two lowest rotational states of ultracold KRb molecules, we mutually validated XXZ spin models tuned by a Floquet microwave pulse sequence against those tuned by a dc electric field through observations of Ramsey contrast dynamics, setting the stage for the realization of Hamiltonians inaccessible with static fields. In particular, we observed two-axis twisting mean-field dynamics, generated by a Floquet-engineered XYZ model using itinerant molecules in 2D layers. In the future, Floquet-engineered Hamiltonians could generate entangled states for molecule-based precision measurement or could take advantage of the rich molecular structure for quantum simulation of multi-level systems.","sentences":["Polar molecules confined in an optical lattice are a versatile platform to explore spin-motion dynamics based on strong, long-range dipolar interactions.","The precise tunability of Ising and spin-exchange interactions with both microwave and dc electric fields makes the molecular system particularly suitable for engineering complex many-body dynamics.","Here, we used Floquet engineering to realize interesting quantum many-body systems of polar molecules.","Using a spin encoded in the two lowest rotational states of ultracold KRb molecules, we mutually validated XXZ spin models tuned by a Floquet microwave pulse sequence against those tuned by a dc electric field through observations of Ramsey contrast dynamics, setting the stage for the realization of Hamiltonians inaccessible with static fields.","In particular, we observed two-axis twisting mean-field dynamics, generated by a Floquet-engineered XYZ model using itinerant molecules in 2D layers.","In the future, Floquet-engineered Hamiltonians could generate entangled states for molecule-based precision measurement or could take advantage of the rich molecular structure for quantum simulation of multi-level systems."],"url":"http://arxiv.org/abs/2404.18913v2","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 17:53:55","title":"Interaction driven topological phase transitions of hardcore bosons on a two-leg ladder","abstract":"We investigate the topological properties of hardcore bosons possessing nearest-neighbor repulsive interactions on a two-leg ladder. We show that by allowing dimerized interactions instead of hopping dimerization, the system exhibits topological phases and phase transitions under proper conditions. First, by assuming uniform hopping throughout the ladder, we show that when interaction along the legs are dimerized and the dimerization pattern is different in the legs, a trivial rung-Mott insulator to a topological bond order phase transition occurs as a function of the dimerization strength. However, for a fixed dimerization strength, the system exhibits a topological to trivial phase transition with increase in the rung hopping. A completely different scenario appears when the rung interaction is turned on. We obtain that for a ladder with uniform hopping, the repulsive interaction either turns the topological phase into a trivial rung-Mott insulator or a charge density wave phase. Such feature is absent when the dimerization pattern in the nearest neighbour interaction is considered to be identical in both the legs of the ladder. We numerically obtain the ground state properties and show the signatures of topological phase transition through Thouless charge pumping.","sentences":["We investigate the topological properties of hardcore bosons possessing nearest-neighbor repulsive interactions on a two-leg ladder.","We show that by allowing dimerized interactions instead of hopping dimerization, the system exhibits topological phases and phase transitions under proper conditions.","First, by assuming uniform hopping throughout the ladder, we show that when interaction along the legs are dimerized and the dimerization pattern is different in the legs, a trivial rung-Mott insulator to a topological bond order phase transition occurs as a function of the dimerization strength.","However, for a fixed dimerization strength, the system exhibits a topological to trivial phase transition with increase in the rung hopping.","A completely different scenario appears when the rung interaction is turned on.","We obtain that for a ladder with uniform hopping, the repulsive interaction either turns the topological phase into a trivial rung-Mott insulator or a charge density wave phase.","Such feature is absent when the dimerization pattern in the nearest neighbour interaction is considered to be identical in both the legs of the ladder.","We numerically obtain the ground state properties and show the signatures of topological phase transition through Thouless charge pumping."],"url":"http://arxiv.org/abs/2404.18912v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 17:53:54","title":"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting","abstract":"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.","sentences":["Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution.","However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly.","Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model.","We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability.","It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model.","To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens.","Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold.","Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo.","Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M).","The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo."],"url":"http://arxiv.org/abs/2404.18911v1","category":"cs.CL"}
{"created":"2024-04-29 17:51:47","title":"Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty","abstract":"To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.","sentences":["To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties.","While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions.","This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set.","This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria.","Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria.","We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length."],"url":"http://arxiv.org/abs/2404.18909v1","category":"cs.LG"}
{"created":"2024-04-29 17:36:19","title":"Finite Element Approximation of the Fractional Porous Medium Equation","abstract":"We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition. We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration. This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian. The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method. We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time.","sentences":["We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition.","We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration.","This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian.","The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method.","We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time."],"url":"http://arxiv.org/abs/2404.18901v1","category":"math.NA"}
{"created":"2024-04-29 17:33:52","title":"Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models","abstract":"Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years. In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance. The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference. The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios. We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution. AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training. Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance. The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches. Code will be available at https://github.com/argmax-ai/aime-v2.","sentences":["Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years.","In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance.","The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference.","The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios.","We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution.","AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training.","Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance.","The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches.","Code will be available at https://github.com/argmax-ai/aime-v2."],"url":"http://arxiv.org/abs/2404.18896v1","category":"cs.LG"}
{"created":"2024-04-29 17:30:41","title":"Odd viscosity suppresses intermittency in direct turbulent cascades","abstract":"Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations. In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains. Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales. This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations. Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling. Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency.","sentences":["Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations.","In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains.","Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales.","This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations.","Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling.","Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency."],"url":"http://arxiv.org/abs/2404.18894v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:16:22","title":"Spivavtor: An Instruction Tuned Ukrainian Text Editing Model","abstract":"We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language. Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model. Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian. This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models. We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them. We publicly release our best-performing models and data as resources to the community to advance further research in this space.","sentences":["We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language.","Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model.","Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian.","This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models.","We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them.","We publicly release our best-performing models and data as resources to the community to advance further research in this space."],"url":"http://arxiv.org/abs/2404.18880v1","category":"cs.CL"}
{"created":"2024-04-29 17:14:21","title":"Spin coupling is all you need: Encoding strong electron correlation on quantum computers","abstract":"The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector. In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small. Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states. This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction. We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates. Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude. Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states. We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers. For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states. Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems.","sentences":["The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector.","In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small.","Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states.","This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction.","We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates.","Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude.","Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states.","We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers.","For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states.","Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems."],"url":"http://arxiv.org/abs/2404.18878v1","category":"quant-ph"}
{"created":"2024-04-29 16:42:58","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","abstract":"In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model. This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget. This would mean having a single model that is able to deal with any modalities. To describe this, we coined the term anymodal learning. An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on. This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture. Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training. To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities. This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets. Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference. Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark. Code: https://github.com/heitorrapela/MiPa","sentences":["In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model.","This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget.","This would mean having a single model that is able to deal with any modalities.","To describe this, we coined the term anymodal learning.","An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on.","This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture.","Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training.","To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities.","This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets.","Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference.","Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark.","Code: https://github.com/heitorrapela/MiPa"],"url":"http://arxiv.org/abs/2404.18849v1","category":"cs.CV"}
{"created":"2024-04-29 16:35:06","title":"The Role of Normal and Non-Normal Contributions to Enstrophy Production in the Near-Wall Region of a Turbulent Channel Flow","abstract":"The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients. Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height. In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities. What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction. Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean. Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component.","sentences":["The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients.","Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height.","In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities.","What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction.","Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean.","Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component."],"url":"http://arxiv.org/abs/2404.18844v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 16:28:38","title":"Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction","abstract":"We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations. The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis. In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems. Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation. For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders. The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries.","sentences":["We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations.","The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis.","In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems.","Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation.","For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders.","The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries."],"url":"http://arxiv.org/abs/2404.18841v1","category":"math.NA"}
{"created":"2024-04-29 16:26:27","title":"Accurate adaptive deep learning method for solving elliptic problems","abstract":"Deep learning method is of great importance in solving partial differential equations. In this paper, inspired by the failure-informed idea proposed by Gao et.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems. Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling. Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy. Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases.","sentences":["Deep learning method is of great importance in solving partial differential equations.","In this paper, inspired by the failure-informed idea proposed by Gao et.al.","(SIAM Journal on Scientific Computing 45(4)(2023))","and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems.","Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling.","Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy.","Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases."],"url":"http://arxiv.org/abs/2404.18838v1","category":"math.NA"}
{"created":"2024-04-29 16:15:01","title":"Demonstration of system-bath physics on gate-based quantum computer","abstract":"We demonstrate algorithmic cooling on IBM-Q devices. We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath. The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed. In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian. We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins.","sentences":["We demonstrate algorithmic cooling on IBM-Q devices.","We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath.","The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed.","In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian.","We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins."],"url":"http://arxiv.org/abs/2404.18828v2","category":"quant-ph"}
{"created":"2024-04-29 15:40:40","title":"A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden","abstract":"This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs). Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model. We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities. We verify our reproduced implementation and present qualitative results on the COCO dataset. Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow. This replication process is not straightforward and requires substantial engineering efforts. Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning. The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer","sentences":["This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs).","Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model.","We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities.","We verify our reproduced implementation and present qualitative results on the COCO dataset.","Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow.","This replication process is not straightforward and requires substantial engineering efforts.","Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning.","The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer"],"url":"http://arxiv.org/abs/2404.18801v1","category":"cs.CV"}
{"created":"2024-04-29 15:38:14","title":"Extending h adaptivity with refinement patterns","abstract":"This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method. The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest. There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type. The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements. The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries.","sentences":["This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method.","The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest.","There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type.","The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements.","The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries."],"url":"http://arxiv.org/abs/2404.18800v2","category":"math.NA"}
{"created":"2024-04-29 15:36:13","title":"Location-Based Load Balancing for Energy-Efficient Cell-Free Networks","abstract":"Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks. It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity. However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient. To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep. Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads. Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs. In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations. We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled. We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs.","sentences":["Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks.","It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity.","However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient.","To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep.","Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads.","Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs.","In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations.","We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled.","We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs."],"url":"http://arxiv.org/abs/2404.18799v1","category":"eess.SP"}
{"created":"2024-04-29 15:34:32","title":"Multi-Agent Synchronization Tasks","abstract":"In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone. The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial. In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks. We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies. Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component. Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context.","sentences":["In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone.","The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial.","In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks.","We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies.","Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component.","Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context."],"url":"http://arxiv.org/abs/2404.18798v1","category":"cs.MA"}
{"created":"2024-04-29 15:18:07","title":"Improved bounds for group testing in arbitrary hypergraphs","abstract":"Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E). This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings. The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage. In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm. These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d. The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand. Further, we derive a lower bound for non-adaptive group testing. For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|).","sentences":["Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E).","This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings.","The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage.","In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm.","These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d.","The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand.","Further, we derive a lower bound for non-adaptive group testing.","For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|)."],"url":"http://arxiv.org/abs/2404.18783v2","category":"cs.DS"}
{"created":"2024-04-29 15:17:59","title":"Whale Optimization Algorithm-based Fractional Order Fuzzy Type-II PI Control for Modular Multilevel Converters","abstract":"Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios. The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response. This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller. The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions. The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system. Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers. To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed. The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC.","sentences":["Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios.","The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response.","This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller.","The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions.","The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system.","Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers.","To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed.","The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC."],"url":"http://arxiv.org/abs/2404.18782v1","category":"eess.SY"}
{"created":"2024-04-29 15:17:30","title":"Wavelet-based tools to analyze, filter, and reconstruct transient gravitational-wave signals","abstract":"The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing. Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete. While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps. In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition. In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124]. We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform. The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals. Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals.","sentences":["The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing.","Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete.","While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps.","In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition.","In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124].","We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform.","The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals.","Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals."],"url":"http://arxiv.org/abs/2404.18781v1","category":"gr-qc"}
{"created":"2024-04-29 14:56:11","title":"Transitive Vision-Language Prompt Learning for Domain Generalization","abstract":"The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains. The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability. Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets.","sentences":["The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains.","The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability.","Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets."],"url":"http://arxiv.org/abs/2404.18758v1","category":"cs.CV"}
{"created":"2024-04-29 14:46:35","title":"Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models","abstract":"Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.","sentences":["Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics.","Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database.","However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall.","These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness.","To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting.","This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration.","Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models.","To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query.","Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall.","Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation."],"url":"http://arxiv.org/abs/2404.18746v1","category":"cs.MM"}
{"created":"2024-04-29 14:42:32","title":"Torsion-induced axions in string theory, quantum gravity and the cosmological tensions","abstract":"We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms. Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting. The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms. The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems. Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today. In the talk we discuss the conditions under which such a result may be achieved. We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components. Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe. With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra.","sentences":["We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms.","Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting.","The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms.","The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems.","Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today.","In the talk we discuss the conditions under which such a result may be achieved.","We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components.","Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe.","With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra."],"url":"http://arxiv.org/abs/2404.18741v1","category":"gr-qc"}
{"created":"2024-04-29 14:10:08","title":"Innovative Integration of Visual Foundation Model with a Robotic Arm on a Mobile Platform","abstract":"In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration. This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform. The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties. By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical. This synthesis enables dynamic object segmentation, tracking, and grasping. It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems. Empirical assessments in both simulated and real-world demonstrate the system's capabilities. This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond.","sentences":["In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration.","This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform.","The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties.","By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical.","This synthesis enables dynamic object segmentation, tracking, and grasping.","It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems.","Empirical assessments in both simulated and real-world demonstrate the system's capabilities.","This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond."],"url":"http://arxiv.org/abs/2404.18720v1","category":"cs.RO"}
{"created":"2024-04-29 13:49:12","title":"Real-fluid Transport Property Computations Based on the Boltzmann-weighted Full-dimensional Potential Model","abstract":"The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc. We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations. It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters. The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations. Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species. Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions. These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients. It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively. Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy. It indicates significant prediction improvements of complex interactions between various particles. The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness.","sentences":["The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc.","We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations.","It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters.","The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations.","Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species.","Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions.","These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients.","It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively.","Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy.","It indicates significant prediction improvements of complex interactions between various particles.","The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness."],"url":"http://arxiv.org/abs/2404.18700v1","category":"physics.app-ph"}
{"created":"2024-04-29 13:43:49","title":"Dual-Modal Prompting for Sketch-Based Image Retrieval","abstract":"Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images. In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval. Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories. Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed. Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales. By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category. With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset. Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance.","sentences":["Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images.","In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval.","Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories.","Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed.","Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales.","By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category.","With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset.","Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance."],"url":"http://arxiv.org/abs/2404.18695v1","category":"cs.CV"}
{"created":"2024-04-29 13:23:38","title":"How Deep Is Your Gaze? Leveraging Distance in Image-Based Gaze Analysis","abstract":"Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data. They are typically extracted with a constant size, approximating the foveated area. As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques. In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance. Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area. We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths. Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization. We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization.","sentences":["Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data.","They are typically extracted with a constant size, approximating the foveated area.","As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques.","In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance.","Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area.","We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths.","Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization.","We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization."],"url":"http://arxiv.org/abs/2404.18680v1","category":"cs.HC"}
{"created":"2024-04-29 13:15:22","title":"Exact symmetry conservation and automatic mesh refinement in discrete initial boundary value problems","abstract":"We present a novel solution procedure for initial boundary value problems. The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom. This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields. Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR). The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved. The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions. An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators.","sentences":["We present a novel solution procedure for initial boundary value problems.","The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom.","This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields.","Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR).","The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved.","The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions.","An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators."],"url":"http://arxiv.org/abs/2404.18676v1","category":"math.NA"}
{"created":"2024-04-29 13:13:10","title":"Open-Source Drift Detection Tools in Action: Insights from Two Use Cases","abstract":"Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability. In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools. D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.We prioritize assessing the functional suitability of these tools to identify and analyze data drifts. Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands. Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy.","sentences":["Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability.","In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools.","D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.","We prioritize assessing the functional suitability of these tools to identify and analyze data drifts.","Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands.","Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy."],"url":"http://arxiv.org/abs/2404.18673v1","category":"cs.DB"}
{"created":"2024-04-29 12:48:42","title":"Terrain characterisation for online adaptability of automated sonar processing: Lessons learnt from operationally applying ATR to sidescan sonar in MCM applications","abstract":"The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments. Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects. This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions. Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing. Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity. The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm. The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation. The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches. We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune.","sentences":["The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments.","Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects.","This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions.","Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing.","Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity.","The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm.","The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation.","The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches.","We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune."],"url":"http://arxiv.org/abs/2404.18663v1","category":"cs.CV"}
{"created":"2024-04-29 12:24:43","title":"Low-Overhead Defect-Adaptive Surface Code with Bandage-Like Super-Stabilizers","abstract":"To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes. However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations. In this context, we introduce an automatic adapter to implement the surface code on defective lattices. Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight. For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%. This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates. Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications.","sentences":["To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes.","However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations.","In this context, we introduce an automatic adapter to implement the surface code on defective lattices.","Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight.","For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%.","This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates.","Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications."],"url":"http://arxiv.org/abs/2404.18644v1","category":"quant-ph"}
{"created":"2024-04-29 11:41:34","title":"FlexiFilm: Long Video Generation with Flexible Conditions","abstract":"Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/","sentences":["Generating long and consistent videos has emerged as a significant yet challenging problem.","While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation.","This results in prominent temporal inconsistency and overexposure.","Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation.","Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure.","Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses.","Project page: https://y-ichen.github.io/FlexiFilm-Page/"],"url":"http://arxiv.org/abs/2404.18620v1","category":"cs.CV"}
{"created":"2024-04-29 11:39:59","title":"Nonlinear Superconducting Magnetoelectric Effect","abstract":"A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction. Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow. Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent. Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response. This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics.","sentences":["A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction.","Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow.","Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent.","Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response.","This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics."],"url":"http://arxiv.org/abs/2404.18616v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 11:04:43","title":"A hybrid prognosis approach for robust lifetime control of commercial wind turbines","abstract":"Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components. To reduce structural loads in WTs, advanced control schemes have been proposed. In recent years, prognosis-based lifetime control of WTs has become increasingly important. In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime. However, stochastic wind dynamics complicates estimation of the SOH of a WT. More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives. Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate. However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality. In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed. A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC). The regression model uses available WT measurements to predict tower load. Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower. The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime. The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement. Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components.","sentences":["Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components.","To reduce structural loads in WTs, advanced control schemes have been proposed.","In recent years, prognosis-based lifetime control of WTs has become increasingly important.","In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime.","However, stochastic wind dynamics complicates estimation of the SOH of a WT.","More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives.","Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate.","However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality.","In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed.","A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC).","The regression model uses available WT measurements to predict tower load.","Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower.","The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime.","The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement.","Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components."],"url":"http://arxiv.org/abs/2404.18593v1","category":"eess.SY"}
{"created":"2024-04-29 10:59:07","title":"The link between hyperuniformity, Coulomb energy, and Wasserstein distance to Lebesgue for two-dimensional point processes","abstract":"We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity. In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications. However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process.","sentences":["We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity.","In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications.","However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   ","Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process."],"url":"http://arxiv.org/abs/2404.18588v1","category":"math.PR"}
{"created":"2024-04-29 10:13:49","title":"Pre-relaxation in quantum, classical, and quantum-classical two-impurity models","abstract":"We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities. In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time. However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time. We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins. Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities. The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects.","sentences":["We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities.","In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time.","However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time.","We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins.","Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities.","The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects."],"url":"http://arxiv.org/abs/2404.18566v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 09:56:32","title":"Doubly Adaptive Importance Sampling","abstract":"We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.","sentences":["We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors.","Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow.","Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling.","The amount of damping in the updates adapts to the posterior and guarantees the effective sample size.","Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling.","The result is a generic, embarrassingly parallel and adaptive posterior approximation method.","Numerical studies on simulated and real data show its competitiveness with other, less general methods."],"url":"http://arxiv.org/abs/2404.18556v1","category":"stat.CO"}
{"created":"2024-04-29 09:34:25","title":"Time Machine GPT","abstract":"Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.","sentences":["Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata.","This approach is not aligned with the evolving nature of language.","Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data.","This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative.","This ensures they remain uninformed about future factual information and linguistic changes.","This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic.","We provide access to both the models and training datasets."],"url":"http://arxiv.org/abs/2404.18543v1","category":"cs.CL"}
{"created":"2024-04-29 09:33:53","title":"OAEI Machine Learning Dataset for Online Model Generation","abstract":"Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI). More and more systems use machine learning-based approaches, including large language models. The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used. This sampling is against the OAEI rules and makes a fair comparison impossible. Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks. In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks. Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems. We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems.","sentences":["Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI).","More and more systems use machine learning-based approaches, including large language models.","The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used.","This sampling is against the OAEI rules and makes a fair comparison impossible.","Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks.","In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks.","Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems.","We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems."],"url":"http://arxiv.org/abs/2404.18542v1","category":"cs.IR"}
{"created":"2024-04-29 09:25:18","title":"Adaptive (re)operations facilitate environmental flow maintenance downstream of multi-purpose reservoirs","abstract":"Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services. However, impoundment of water impacts instream aquatic ecosystems. Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases. To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry. A related issue is - how does the order in which releases are prioritized influences sectoral performances? We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India. We formulate two versions of a multi-objective decision problem. PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases. PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases. Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives. We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals.","sentences":["Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services.","However, impoundment of water impacts instream aquatic ecosystems.","Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.","But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases.","To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry.","A related issue is - how does the order in which releases are prioritized influences sectoral performances?","We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India.","We formulate two versions of a multi-objective decision problem.","PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases.","PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases.","Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives.","We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals."],"url":"http://arxiv.org/abs/2404.18535v1","category":"math.OC"}
{"created":"2024-04-29 08:50:17","title":"Towards Image Synthesis with Photon Counting Stellar Intensity Interferometry","abstract":"Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly. A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array. Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data. Here we present the project and the present status of the activities.","sentences":["Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly.","A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array.","Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data.","Here we present the project and the present status of the activities."],"url":"http://arxiv.org/abs/2404.18507v1","category":"astro-ph.IM"}
{"created":"2024-04-29 08:01:40","title":"Emergent dynamics of the inertial Kuramoto model with frustration on a locally coupled graph","abstract":"We study the synchronized behavior of the inertial Kuramoto oscillators with frustration effect under a symmetric and connected network. Due to the lack of second-order gradient flow structure and singularity of second-order derivative of diameter, we shift to construct convex combinations of oscillators and related new energy functions that can control the phase and frequency diameters. Under sufficient frameworks on initial data and system parameters, we derive first-order dissipative differential inequalities of constructed energy functions. This eventually gives rise to the emergence of frequency synchronization exponentially fast.","sentences":["We study the synchronized behavior of the inertial Kuramoto oscillators with frustration effect under a symmetric and connected network.","Due to the lack of second-order gradient flow structure and singularity of second-order derivative of diameter, we shift to construct convex combinations of oscillators and related new energy functions that can control the phase and frequency diameters.","Under sufficient frameworks on initial data and system parameters, we derive first-order dissipative differential inequalities of constructed energy functions.","This eventually gives rise to the emergence of frequency synchronization exponentially fast."],"url":"http://arxiv.org/abs/2404.18488v1","category":"math.DS"}
{"created":"2024-04-29 07:56:36","title":"Exponential synchronization of the Kuramoto model with inertia and frustration under locally coupled network","abstract":"We study the collective synchronized behavior of the Kuramoto model with inertia and frustration effects on a connected and symmetric network. We aim to establish sufficient frameworks for achieving complete frequency synchronization, taking into account initial configuration, small inertia and frustration, and large coupling strength. More precisely, we first demonstrate that the phase diameter will be uniformly bounded by a small value after a finite time. Then we prove that the frequency diameter exhibits exponential decay to zero. Our approach relies on a careful construction of energy functionals, which effectively control the dissipation of phase and frequency diameters.","sentences":["We study the collective synchronized behavior of the Kuramoto model with inertia and frustration effects on a connected and symmetric network.","We aim to establish sufficient frameworks for achieving complete frequency synchronization, taking into account initial configuration, small inertia and frustration, and large coupling strength.","More precisely, we first demonstrate that the phase diameter will be uniformly bounded by a small value after a finite time.","Then we prove that the frequency diameter exhibits exponential decay to zero.","Our approach relies on a careful construction of energy functionals, which effectively control the dissipation of phase and frequency diameters."],"url":"http://arxiv.org/abs/2404.18487v1","category":"math.DS"}
{"created":"2024-04-29 07:25:52","title":"Spectral distortions from acoustic dissipation with non-Gaussian (or not) perturbations","abstract":"A well-known route to form primordial black holes in the early universe relies on the existence of unusually large primordial curvature fluctuations, confined to a narrow range of wavelengths that would be too small to be constrained by Cosmic Microwave Background (CMB) anisotropies. This scenario would however boost the generation of $\\mu$-type spectral distortions in the CMB due to an enhanced dissipation of acoustic waves. Previous studies of $\\mu$-distortion bounds on the primordial spectrum were based on the assumptions of Gaussian primordial fluctuations. In this work, we push the calculation of $\\mu$-distortions to one higher order in photon anisotropies. We discuss how to derive bounds on primordial spectrum peaks obeying non-Gaussian statistics under the assumption of local (perturbative or not) non-Gaussianity. We find that, depending on the value of the peak scale, the bounds may either remain stable or get tighter by several orders of magnitude, but only when the departure from Gaussian statistics is very strong. Our results are translated in terms of bounds on primordial supermassive black hole mass in a companion paper.","sentences":["A well-known route to form primordial black holes in the early universe relies on the existence of unusually large primordial curvature fluctuations, confined to a narrow range of wavelengths that would be too small to be constrained by Cosmic Microwave Background (CMB) anisotropies.","This scenario would however boost the generation of $\\mu$-type spectral distortions in the CMB due to an enhanced dissipation of acoustic waves.","Previous studies of $\\mu$-distortion bounds on the primordial spectrum were based on the assumptions of Gaussian primordial fluctuations.","In this work, we push the calculation of $\\mu$-distortions to one higher order in photon anisotropies.","We discuss how to derive bounds on primordial spectrum peaks obeying non-Gaussian statistics under the assumption of local (perturbative or not) non-Gaussianity.","We find that, depending on the value of the peak scale, the bounds may either remain stable or get tighter by several orders of magnitude, but only when the departure from Gaussian statistics is very strong.","Our results are translated in terms of bounds on primordial supermassive black hole mass in a companion paper."],"url":"http://arxiv.org/abs/2404.18474v1","category":"astro-ph.CO"}
{"created":"2024-04-29 06:59:30","title":"M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework","abstract":"Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.","sentences":["Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling.","Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods.","To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework.","M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives.","We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner.","Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks.","The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization.","To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively.","Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance.","The implementation code is available to ensure reproducibility."],"url":"http://arxiv.org/abs/2404.18465v1","category":"cs.IR"}
{"created":"2024-04-29 06:44:33","title":"Clicks2Line: Using Lines for Interactive Image Segmentation","abstract":"For click-based interactive segmentation methods, reducing the number of clicks required to obtain a desired segmentation result is essential. Although recent click-based methods yield decent segmentation results, we observe that substantial amount of clicks are required to segment elongated regions. To reduce the amount of user-effort required, we propose using lines instead of clicks for such cases. In this paper, an interactive segmentation algorithm which adaptively adopts either clicks or lines as input is proposed. Experimental results demonstrate that using lines can generate better segmentation results than clicks for several cases.","sentences":["For click-based interactive segmentation methods, reducing the number of clicks required to obtain a desired segmentation result is essential.","Although recent click-based methods yield decent segmentation results, we observe that substantial amount of clicks are required to segment elongated regions.","To reduce the amount of user-effort required, we propose using lines instead of clicks for such cases.","In this paper, an interactive segmentation algorithm which adaptively adopts either clicks or lines as input is proposed.","Experimental results demonstrate that using lines can generate better segmentation results than clicks for several cases."],"url":"http://arxiv.org/abs/2404.18461v1","category":"cs.CV"}
{"created":"2024-04-29 06:35:34","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild","abstract":"Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at https://github.com/GitGyun/chameleon.","sentences":["Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training.","However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks.","Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists.","In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios.","To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability.","We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks.","Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches.","Codes are available at https://github.com/GitGyun/chameleon."],"url":"http://arxiv.org/abs/2404.18459v1","category":"cs.CV"}
{"created":"2024-04-29 06:32:28","title":"Autonomous Quality and Hallucination Assessment for Virtual Tissue Staining and Digital Pathology","abstract":"Histopathological staining of human tissue is essential in the diagnosis of various diseases. The recent advances in virtual tissue staining technologies using AI alleviate some of the costly and tedious steps involved in the traditional histochemical staining process, permitting multiplexed rapid staining of label-free tissue without using staining reagents, while also preserving tissue. However, potential hallucinations and artifacts in these virtually stained tissue images pose concerns, especially for the clinical utility of these approaches. Quality assessment of histology images is generally performed by human experts, which can be subjective and depends on the training level of the expert. Here, we present an autonomous quality and hallucination assessment method (termed AQuA), mainly designed for virtual tissue staining, while also being applicable to histochemical staining. AQuA achieves 99.8% accuracy when detecting acceptable and unacceptable virtually stained tissue images without access to ground truth, also presenting an agreement of 98.5% with the manual assessments made by board-certified pathologists. Besides, AQuA achieves super-human performance in identifying realistic-looking, virtually stained hallucinatory images that would normally mislead human diagnosticians by deceiving them into diagnosing patients that never existed. We further demonstrate the wide adaptability of AQuA across various virtually and histochemically stained tissue images and showcase its strong external generalization to detect unseen hallucination patterns of virtual staining network models as well as artifacts observed in the traditional histochemical staining workflow. This framework creates new opportunities to enhance the reliability of virtual staining and will provide quality assurance for various image generation and transformation tasks in digital pathology and computational imaging.","sentences":["Histopathological staining of human tissue is essential in the diagnosis of various diseases.","The recent advances in virtual tissue staining technologies using AI alleviate some of the costly and tedious steps involved in the traditional histochemical staining process, permitting multiplexed rapid staining of label-free tissue without using staining reagents, while also preserving tissue.","However, potential hallucinations and artifacts in these virtually stained tissue images pose concerns, especially for the clinical utility of these approaches.","Quality assessment of histology images is generally performed by human experts, which can be subjective and depends on the training level of the expert.","Here, we present an autonomous quality and hallucination assessment method (termed AQuA), mainly designed for virtual tissue staining, while also being applicable to histochemical staining.","AQuA achieves 99.8% accuracy when detecting acceptable and unacceptable virtually stained tissue images without access to ground truth, also presenting an agreement of 98.5% with the manual assessments made by board-certified pathologists.","Besides, AQuA achieves super-human performance in identifying realistic-looking, virtually stained hallucinatory images that would normally mislead human diagnosticians by deceiving them into diagnosing patients that never existed.","We further demonstrate the wide adaptability of AQuA across various virtually and histochemically stained tissue images and showcase its strong external generalization to detect unseen hallucination patterns of virtual staining network models as well as artifacts observed in the traditional histochemical staining workflow.","This framework creates new opportunities to enhance the reliability of virtual staining and will provide quality assurance for various image generation and transformation tasks in digital pathology and computational imaging."],"url":"http://arxiv.org/abs/2404.18458v1","category":"eess.IV"}
{"created":"2024-04-29 05:22:23","title":"Metasurface-based Toroidal Lenslet Array Design for Addressing Laser Guide Star Elongation","abstract":"The Giant Magellan Telescope will use laser tomography adaptive optics to correct for atmospheric turbulence using artificial guide stars created in the sodium layer of the atmosphere (altitude ~95km). The sodium layer has appreciable thickness (~11km) and this results in the laser guide star being an elongated cylinder shape. Wavefront sensing with a Shack-Hartmann is challenging, as subapertures located further away from the laser launch position image an increasingly elongated perspective of the laser guide star. Large detectors can be used to adequately pack and sample the images on the detector, however, this increases readout noise and limits the design space available for the wavefront sensor. To tackle this challenge, we propose an original solution based on nano-engineered meta-optics tailored to produce a spatially varying anamorphic image scale compression. We present meta-lenslet array designs that can deliver ~100% of the full anamorphic image size reduction required for focal lengths down to 8mm, and greater than 50% image size reduction for focal lengths down to 2mm. This will allow greatly improved sampling of the available information across the whole wavefront sensor, while still being a viable design within the limits of current-generation fabrication facilities.","sentences":["The Giant Magellan Telescope will use laser tomography adaptive optics to correct for atmospheric turbulence using artificial guide stars created in the sodium layer of the atmosphere (altitude ~95km).","The sodium layer has appreciable thickness (~11km) and this results in the laser guide star being an elongated cylinder shape.","Wavefront sensing with a Shack-Hartmann is challenging, as subapertures located further away from the laser launch position image an increasingly elongated perspective of the laser guide star.","Large detectors can be used to adequately pack and sample the images on the detector, however, this increases readout noise and limits the design space available for the wavefront sensor.","To tackle this challenge, we propose an original solution based on nano-engineered meta-optics tailored to produce a spatially varying anamorphic image scale compression.","We present meta-lenslet array designs that can deliver ~100% of the full anamorphic image size reduction required for focal lengths down to 8mm, and greater than 50% image size reduction for focal lengths down to 2mm.","This will allow greatly improved sampling of the available information across the whole wavefront sensor, while still being a viable design within the limits of current-generation fabrication facilities."],"url":"http://arxiv.org/abs/2404.18435v1","category":"astro-ph.IM"}
{"created":"2024-04-29 05:15:45","title":"Structure-preserving particle methods for the Landau collision operator using the metriplectic framework","abstract":"We present a novel family of particle discretisation methods for the nonlinear Landau collision operator. We exploit the metriplectic structure underlying the Vlasov-Maxwell-Landau system in order to obtain disretisation schemes that automatically preserve mass, momentum, and energy, warrant monotonic dissipation of entropy, and are thus guaranteed to respect the laws of thermodynamics. In contrast to recent works that used radial basis functions and similar methods for regularisation, here we use an auxiliary spline or finite element representation of the distribution function to this end. Discrete gradient methods are employed to guarantee the aforementioned properties in the time discrete domain as well.","sentences":["We present a novel family of particle discretisation methods for the nonlinear Landau collision operator.","We exploit the metriplectic structure underlying the Vlasov-Maxwell-Landau system in order to obtain disretisation schemes that automatically preserve mass, momentum, and energy, warrant monotonic dissipation of entropy, and are thus guaranteed to respect the laws of thermodynamics.","In contrast to recent works that used radial basis functions and similar methods for regularisation, here we use an auxiliary spline or finite element representation of the distribution function to this end.","Discrete gradient methods are employed to guarantee the aforementioned properties in the time discrete domain as well."],"url":"http://arxiv.org/abs/2404.18432v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 04:53:09","title":"Design of Tunable Perfect Absorbers in the Mid-IR Spectrum Using Graphene-Based Multilayer Structures: Emerging Applications in Atmospheric Window Matching","abstract":"This paper introduces tunable and switchable Perfect Absorbers (PAs) operating within the mid-infrared spectrum, specifically targeting the 3 to 5 um range at 0.25 um intervals. This spectrum is engineered for minimal atmospheric absorption and unique transmission characteristics. Our approach uses graphene-based nanophotonic aperiodic multilayer structures, optimized through micro-genetic algorithms within an inverse design framework. This combination broadens the design space, enabling highly accurate absorption control. Using the Transfer-Matrix-Method for simulations, we tailor absorption characteristics while keeping the structures under 2 um thick. Our results demonstrate PA tunability and switchability by adjusting graphene layers' chemical potentials. For instance, a 4 um peak can shift to 4.22 um by changing the graphene potential from 0 eV to 1 eV, without reducing efficiency. Our research also reveals PA adaptability to incident angles, maintaining 90% absorption up to 52 degrees, demonstrating versatility for applications like thermal photovoltaics, sensors, and stealth technology. This research deepens our understanding of nanophotonic materials and advances optical devices for the mid-IR range.","sentences":["This paper introduces tunable and switchable Perfect Absorbers (PAs) operating within the mid-infrared spectrum, specifically targeting the 3 to 5 um range at 0.25 um intervals.","This spectrum is engineered for minimal atmospheric absorption and unique transmission characteristics.","Our approach uses graphene-based nanophotonic aperiodic multilayer structures, optimized through micro-genetic algorithms within an inverse design framework.","This combination broadens the design space, enabling highly accurate absorption control.","Using the Transfer-Matrix-Method for simulations, we tailor absorption characteristics while keeping the structures under 2 um thick.","Our results demonstrate PA tunability and switchability by adjusting graphene layers' chemical potentials.","For instance, a 4 um peak can shift to 4.22 um by changing the graphene potential from 0 eV to 1 eV, without reducing efficiency.","Our research also reveals PA adaptability to incident angles, maintaining 90% absorption up to 52 degrees, demonstrating versatility for applications like thermal photovoltaics, sensors, and stealth technology.","This research deepens our understanding of nanophotonic materials and advances optical devices for the mid-IR range."],"url":"http://arxiv.org/abs/2404.18425v1","category":"physics.app-ph"}
{"created":"2024-04-29 03:11:13","title":"Dflow, a Python framework for constructing cloud-native AI-for-Science workflows","abstract":"In the AI-for-science era, scientific computing scenarios such as concurrent learning and high-throughput computing demand a new generation of infrastructure that supports scalable computing resources and automated workflow management on both cloud and high-performance supercomputers. Here we introduce Dflow, an open-source Python toolkit designed for scientists to construct workflows with simple programming interfaces. It enables complex process control and task scheduling across a distributed, heterogeneous infrastructure, leveraging containers and Kubernetes for flexibility. Dflow is highly observable and can scale to thousands of concurrent nodes per workflow, enhancing the efficiency of complex scientific computing tasks. The basic unit in Dflow, known as an Operation (OP), is reusable and independent of the underlying infrastructure or context. Dozens of workflow projects have been developed based on Dflow, spanning a wide range of projects. We anticipate that the reusability of Dflow and its components will encourage more scientists to publish their workflows and OP components. These components, in turn, can be adapted and reused in various contexts, fostering greater collaboration and innovation in the scientific community.","sentences":["In the AI-for-science era, scientific computing scenarios such as concurrent learning and high-throughput computing demand a new generation of infrastructure that supports scalable computing resources and automated workflow management on both cloud and high-performance supercomputers.","Here we introduce Dflow, an open-source Python toolkit designed for scientists to construct workflows with simple programming interfaces.","It enables complex process control and task scheduling across a distributed, heterogeneous infrastructure, leveraging containers and Kubernetes for flexibility.","Dflow is highly observable and can scale to thousands of concurrent nodes per workflow, enhancing the efficiency of complex scientific computing tasks.","The basic unit in Dflow, known as an Operation (OP), is reusable and independent of the underlying infrastructure or context.","Dozens of workflow projects have been developed based on Dflow, spanning a wide range of projects.","We anticipate that the reusability of Dflow and its components will encourage more scientists to publish their workflows and OP components.","These components, in turn, can be adapted and reused in various contexts, fostering greater collaboration and innovation in the scientific community."],"url":"http://arxiv.org/abs/2404.18392v1","category":"cs.DC"}
{"created":"2024-04-29 03:04:37","title":"Critical grid method: An extensible Smoothed Particle Hydrodynamics fluid general interpolation method for Fluid-Structure Interaction surface coupling based on preCICE","abstract":"Solving Fluid-Structure Interaction (FSI) problems using traditional methods is a big challenge in the field of numerical simulation. As a powerful multi-physical field coupled library, preCICE has a bright application prospect for solving FSI, which supports many open/closed source software and commercial CFD solvers to solve FSI problems in the form of a black box. However, this library currently only supports mesh-based coupling schemes. This paper proposes a critical grid (mesh) as an intermediate medium for the particle method to connect a bidirectional coupling tool named preCICE. The particle and critical mesh are used to interpolate the displacement and force so that the pure Lagrangian Smoothed Particle Hydrodynamic (SPH) method can also solve the FSI problem. This method is called the particle mesh coupling (PMC) method, which theoretically solves the mesh mismatch problem based on the particle method to connect preCICE. In addition, we conduct experiments to verify the performance of the PMC method, in which the fluid and the structure is discretized by SPH and the Finite Element Method (FEM), respectively. The results show that the PMC method given in this paper is effective for solving FSI problems. Finally, our source code for the SPH fluid adapter is open-source and available on GitHub for further developing preCICE compatibility with more meshless methods.","sentences":["Solving Fluid-Structure Interaction (FSI) problems using traditional methods is a big challenge in the field of numerical simulation.","As a powerful multi-physical field coupled library, preCICE has a bright application prospect for solving FSI, which supports many open/closed source software and commercial CFD solvers to solve FSI problems in the form of a black box.","However, this library currently only supports mesh-based coupling schemes.","This paper proposes a critical grid (mesh) as an intermediate medium for the particle method to connect a bidirectional coupling tool named preCICE.","The particle and critical mesh are used to interpolate the displacement and force so that the pure Lagrangian Smoothed Particle Hydrodynamic (SPH) method can also solve the FSI problem.","This method is called the particle mesh coupling (PMC) method, which theoretically solves the mesh mismatch problem based on the particle method to connect preCICE.","In addition, we conduct experiments to verify the performance of the PMC method, in which the fluid and the structure is discretized by SPH and the Finite Element Method (FEM), respectively.","The results show that the PMC method given in this paper is effective for solving FSI problems.","Finally, our source code for the SPH fluid adapter is open-source and available on GitHub for further developing preCICE compatibility with more meshless methods."],"url":"http://arxiv.org/abs/2404.18390v1","category":"math.NA"}
{"created":"2024-04-29 02:24:25","title":"Trajectory Optimization for Adaptive Informative Path Planning with Multimodal Sensing","abstract":"We consider the problem of an autonomous agent equipped with multiple sensors, each with different sensing precision and energy costs. The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments. The challenge lies in reasoning about the effects of sensing and movement while respecting the agent's resource and dynamic constraints. We formulate the problem as a trajectory optimization problem and solve it using a projection-based trajectory optimization approach where the objective is to reduce the variance of the Gaussian process world belief. Our approach outperforms previous approaches in long horizon trajectories by achieving an overall variance reduction of up to 85% and reducing the root-mean square error in the environment belief by 50%. This approach was developed in support of rover path planning for the NASA VIPER Mission.","sentences":["We consider the problem of an autonomous agent equipped with multiple sensors, each with different sensing precision and energy costs.","The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments.","The challenge lies in reasoning about the effects of sensing and movement while respecting the agent's resource and dynamic constraints.","We formulate the problem as a trajectory optimization problem and solve it using a projection-based trajectory optimization approach where the objective is to reduce the variance of the Gaussian process world belief.","Our approach outperforms previous approaches in long horizon trajectories by achieving an overall variance reduction of up to 85% and reducing the root-mean square error in the environment belief by 50%.","This approach was developed in support of rover path planning for the NASA VIPER Mission."],"url":"http://arxiv.org/abs/2404.18374v1","category":"cs.RO"}
{"created":"2024-04-28 21:23:40","title":"BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models","abstract":"The growing demand for Large Language Models (LLMs) across diverse applications has prompted a paradigm shift in the design of deep learning serving systems. Deploying LLMs, especially in multi-tenant environments, presents considerable challenges due to their high computational and memory demands. We present BlockLLM, a serving system that exploits the potential of sharing components among fine-tuned LLM models to offer an efficient and flexible solution for LLM workloads. BlockLLM partitions the models into finer-grained blocks to enable the reuse of model components and independent provisioning to improve the computation efficiency. BlockLLM consists of an offline block zoo, for storing the blocks, and an online system to serve the requests through chains of blocks. It offers multi-fold flexibility: (1) Adaptive assembly of block chains on-the-fly is achieved with the help of equivalence evaluation among blocks in the zoo. (2) We enable per-block batch size and configure best-effort KV cache coordination at individual block level. (3) We adopt speculative execution and locality-aware block placement to mitigate the communication costs from dynamic block resource allocation. Our evaluation demonstrates that BlockLLM reduces memory and storage footprints and improves computation efficiency, outperforming existing serving approach in 95\\%ile latency and GPU utilization by 33.5\\% and 20.1\\%, respectively.","sentences":["The growing demand for Large Language Models (LLMs) across diverse applications has prompted a paradigm shift in the design of deep learning serving systems.","Deploying LLMs, especially in multi-tenant environments, presents considerable challenges due to their high computational and memory demands.","We present BlockLLM, a serving system that exploits the potential of sharing components among fine-tuned LLM models to offer an efficient and flexible solution for LLM workloads.","BlockLLM partitions the models into finer-grained blocks to enable the reuse of model components and independent provisioning to improve the computation efficiency.","BlockLLM consists of an offline block zoo, for storing the blocks, and an online system to serve the requests through chains of blocks.","It offers multi-fold flexibility: (1) Adaptive assembly of block chains on-the-fly is achieved with the help of equivalence evaluation among blocks in the zoo.","(2) We enable per-block batch size and configure best-effort KV cache coordination at individual block level.","(3) We adopt speculative execution and locality-aware block placement to mitigate the communication costs from dynamic block resource allocation.","Our evaluation demonstrates that BlockLLM reduces memory and storage footprints and improves computation efficiency, outperforming existing serving approach in 95\\%ile latency and GPU utilization by 33.5\\% and 20.1\\%, respectively."],"url":"http://arxiv.org/abs/2404.18322v1","category":"cs.DC"}
{"created":"2024-04-28 21:04:46","title":"Reconstructing random graphs from distance queries","abstract":"We estimate the minimum number of distance queries that is sufficient to reconstruct the binomial random graph $G(n,p)$ with constant diameter with high probability. We get a tight (up to a constant factor) answer for all $p>n^{-1+o(1)}$ outside \"threshold windows\" around $n^{-k/(k+1)+o(1)}$, $k\\in\\mathbb{Z}_{>0}$: with high probability the query complexity equals $\\Theta(n^{4-d}p^{2-d})$, where $d$ is the diameter of the random graph. This demonstrates the following non-monotone behaviour: the query complexity jumps down at moments when the diameter gets larger; yet, between these moments the query complexity grows. We also show that there exists a non-adaptive algorithm that reconstructs the random graph with $O(n^{4-d}p^{2-d}\\ln n)$ distance queries with high probability, and this is best possible.","sentences":["We estimate the minimum number of distance queries that is sufficient to reconstruct the binomial random graph $G(n,p)$ with constant diameter with high probability.","We get a tight (up to a constant factor) answer for all $p>n^{-1+o(1)}$ outside \"threshold windows\" around $n^{-k/(k+1)+o(1)}$, $k\\in\\mathbb{Z}_{>0}$: with high probability the query complexity equals $\\Theta(n^{4-d}p^{2-d})$, where $d$ is the diameter of the random graph.","This demonstrates the following non-monotone behaviour: the query complexity jumps down at moments when the diameter gets larger; yet, between these moments the query complexity grows.","We also show that there exists a non-adaptive algorithm that reconstructs the random graph with $O(n^{4-d}p^{2-d}\\ln n)$ distance queries with high probability, and this is best possible."],"url":"http://arxiv.org/abs/2404.18318v1","category":"math.CO"}
{"created":"2024-04-28 20:44:53","title":"Towards Real-time Learning in Large Language Models: A Critical Review","abstract":"Real-time learning concerns the ability of learning systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data may be insufficient or difficult to obtain. This review provides a comprehensive analysis of real-time learning in Large Language Models. It synthesizes the state-of-the-art real-time learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for real-time learning by describing specific achievements from these related topics and their critical factors. Finally, the paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of real-time learning and its implications for designing and developing LLM-based learning systems addressing real-world problems.","sentences":["Real-time learning concerns the ability of learning systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks.","It is a critical ability for intelligent, real-world systems, especially when data may be insufficient or difficult to obtain.","This review provides a comprehensive analysis of real-time learning in Large Language Models.","It synthesizes the state-of-the-art real-time learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning.","We demonstrate their utility for real-time learning by describing specific achievements from these related topics and their critical factors.","Finally, the paper highlights current problems and challenges for future research in the field.","By consolidating the latest relevant research developments, this review offers a comprehensive understanding of real-time learning and its implications for designing and developing LLM-based learning systems addressing real-world problems."],"url":"http://arxiv.org/abs/2404.18311v2","category":"cs.LG"}
{"created":"2024-04-28 19:44:56","title":"Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms","abstract":"Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee's perspective. This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents' continuous entry and exit from the system. In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes. Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility. We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment. We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments.","sentences":["Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee's perspective.","This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents' continuous entry and exit from the system.","In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes.","Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility.","We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment.","We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments."],"url":"http://arxiv.org/abs/2404.18296v1","category":"cs.AI"}
{"created":"2024-04-28 18:16:58","title":"LINOCS: Lookahead Inference of Networked Operators for Continuous Stability","abstract":"Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems. Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging. Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process. Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators. Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions. We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples.","sentences":["Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior.","For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems.","Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging.","Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process.","Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators.","Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data.","LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions.","We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples."],"url":"http://arxiv.org/abs/2404.18267v1","category":"eess.SY"}
{"created":"2024-04-28 17:50:58","title":"Align, Minimize and Diversify: A Source-Free Unsupervised Domain Adaptation Method for Handwritten Text Recognition","abstract":"This paper serves to introduce the Align, Minimize and Diversify (AMD) method, a Source-Free Unsupervised Domain Adaptation approach for Handwritten Text Recognition (HTR). This framework decouples the adaptation process from the source data, thus not only sidestepping the resource-intensive retraining process but also making it possible to leverage the wealth of pre-trained knowledge encoded in modern Deep Learning architectures. Our method explicitly eliminates the need to revisit the source data during adaptation by incorporating three distinct regularization terms: the Align term, which reduces the feature distribution discrepancy between source and target data, ensuring the transferability of the pre-trained representation; the Minimize term, which encourages the model to make assertive predictions, pushing the outputs towards one-hot-like distributions in order to minimize prediction uncertainty, and finally, the Diversify term, which safeguards against the degeneracy in predictions by promoting varied and distinctive sequences throughout the target data, preventing informational collapse. Experimental results from several benchmarks demonstrated the effectiveness and robustness of AMD, showing it to be competitive and often outperforming DA methods in HTR.","sentences":["This paper serves to introduce the Align, Minimize and Diversify (AMD) method, a Source-Free Unsupervised Domain Adaptation approach for Handwritten Text Recognition (HTR).","This framework decouples the adaptation process from the source data, thus not only sidestepping the resource-intensive retraining process but also making it possible to leverage the wealth of pre-trained knowledge encoded in modern Deep Learning architectures.","Our method explicitly eliminates the need to revisit the source data during adaptation by incorporating three distinct regularization terms: the Align term, which reduces the feature distribution discrepancy between source and target data, ensuring the transferability of the pre-trained representation; the Minimize term, which encourages the model to make assertive predictions, pushing the outputs towards one-hot-like distributions in order to minimize prediction uncertainty, and finally, the Diversify term, which safeguards against the degeneracy in predictions by promoting varied and distinctive sequences throughout the target data, preventing informational collapse.","Experimental results from several benchmarks demonstrated the effectiveness and robustness of AMD, showing it to be competitive and often outperforming DA methods in HTR."],"url":"http://arxiv.org/abs/2404.18260v1","category":"cs.CV"}
{"created":"2024-04-28 17:41:07","title":"Semiparametric causal mediation analysis in cluster-randomized experiments","abstract":"In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments--the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.","sentences":["In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome.","Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators.","In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments--the natural indirect effect, natural direct effect, and spillover mediation effect.","We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator.","We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case.","Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments."],"url":"http://arxiv.org/abs/2404.18256v1","category":"stat.ME"}
{"created":"2024-04-28 16:58:53","title":"AdaFSNet: Time Series Classification Based on Convolutional Network with a Adaptive and Effective Kernel Size Configuration","abstract":"Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance. Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets. In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification. This network includes two Dense Blocks. Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length. We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF. To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively. Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks.","sentences":["Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance.","Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets.","In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification.","This network includes two Dense Blocks.","Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length.","We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF.","To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively.","Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks."],"url":"http://arxiv.org/abs/2404.18246v1","category":"cs.LG"}
{"created":"2024-04-28 16:29:22","title":"Flood Data Analysis on SpaceNet 8 Using Apache Sedona","abstract":"With the escalating frequency of floods posing persistent threats to human life and property, satellite remote sensing has emerged as an indispensable tool for monitoring flood hazards. SpaceNet8 offers a unique opportunity to leverage cutting-edge artificial intelligence technologies to assess these hazards. A significant contribution of this research is its application of Apache Sedona, an advanced platform specifically designed for the efficient and distributed processing of large-scale geospatial data. This platform aims to enhance the efficiency of error analysis, a critical aspect of improving flood damage detection accuracy. Based on Apache Sedona, we introduce a novel approach that addresses the challenges associated with inaccuracies in flood damage detection. This approach involves the retrieval of cases from historical flood events, the adaptation of these cases to current scenarios, and the revision of the model based on clustering algorithms to refine its performance. Through the replication of both the SpaceNet8 baseline and its top-performing models, we embark on a comprehensive error analysis. This analysis reveals several main sources of inaccuracies. To address these issues, we employ data visual interpretation and histogram equalization techniques, resulting in significant improvements in model metrics. After these enhancements, our indicators show a notable improvement, with precision up by 5%, F1 score by 2.6%, and IoU by 4.5%. This work highlights the importance of advanced geospatial data processing tools, such as Apache Sedona. By improving the accuracy and efficiency of flood detection, this research contributes to safeguarding public safety and strengthening infrastructure resilience in flood-prone areas, making it a valuable addition to the field of remote sensing and disaster management.","sentences":["With the escalating frequency of floods posing persistent threats to human life and property, satellite remote sensing has emerged as an indispensable tool for monitoring flood hazards.","SpaceNet8 offers a unique opportunity to leverage cutting-edge artificial intelligence technologies to assess these hazards.","A significant contribution of this research is its application of Apache Sedona, an advanced platform specifically designed for the efficient and distributed processing of large-scale geospatial data.","This platform aims to enhance the efficiency of error analysis, a critical aspect of improving flood damage detection accuracy.","Based on Apache Sedona, we introduce a novel approach that addresses the challenges associated with inaccuracies in flood damage detection.","This approach involves the retrieval of cases from historical flood events, the adaptation of these cases to current scenarios, and the revision of the model based on clustering algorithms to refine its performance.","Through the replication of both the SpaceNet8 baseline and its top-performing models, we embark on a comprehensive error analysis.","This analysis reveals several main sources of inaccuracies.","To address these issues, we employ data visual interpretation and histogram equalization techniques, resulting in significant improvements in model metrics.","After these enhancements, our indicators show a notable improvement, with precision up by 5%, F1 score by 2.6%, and IoU by 4.5%.","This work highlights the importance of advanced geospatial data processing tools, such as Apache Sedona.","By improving the accuracy and efficiency of flood detection, this research contributes to safeguarding public safety and strengthening infrastructure resilience in flood-prone areas, making it a valuable addition to the field of remote sensing and disaster management."],"url":"http://arxiv.org/abs/2404.18235v1","category":"cs.CV"}
{"created":"2024-04-29 17:59:30","title":"DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing","abstract":"We consider the problem of editing 3D objects and scenes based on open-ended language instructions. The established paradigm to solve this problem is to use a 2D image generator or editor to guide the 3D editing process. However, this is often slow as it requires do update a computationally expensive 3D representations such as a neural radiance field, and to do so by using contradictory guidance from a 2D model which is inherently not multi-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two ways. First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent. We do so by utilizing a training-free approach which integrates cues from the underlying 3D geometry of the scene. Second, given a multi-view consistent edited sequence of images of the object, we directly and efficiently optimize the 3D object representation, which is based on 3D Gaussian Splatting. Because it does not require to apply edits incrementally and iteratively, DGE is significantly more efficient than existing approaches, and comes with other perks such as allowing selective editing of parts of the scene.","sentences":["We consider the problem of editing 3D objects and scenes based on open-ended language instructions.","The established paradigm to solve this problem is to use a 2D image generator or editor to guide the 3D editing process.","However, this is often slow as it requires do update a computationally expensive 3D representations such as a neural radiance field, and to do so by using contradictory guidance from a 2D model which is inherently not multi-view consistent.","We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two ways.","First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent.","We do so by utilizing a training-free approach which integrates cues from the underlying 3D geometry of the scene.","Second, given a multi-view consistent edited sequence of images of the object, we directly and efficiently optimize the 3D object representation, which is based on 3D Gaussian Splatting.","Because it does not require to apply edits incrementally and iteratively, DGE is significantly more efficient than existing approaches, and comes with other perks such as allowing selective editing of parts of the scene."],"url":"http://arxiv.org/abs/2404.18929v1","category":"cs.CV"}
{"created":"2024-04-29 17:58:19","title":"Analytically weak solutions to stochastic heat equations with spatially rough noise","abstract":"In [HHL+17] the authors showed existence and uniqueness of solutions to the nonlinear one-dimensional stochastic heat equation driven by a Gaussian noise that is white in time and rougher than white in space (in particular, its covariance is not a measure). Here we present a simple alternative to derive such results by considering the equations in the analytically weak sense, using either the variational approach or Krylov's $L^p$-theory. Various improvements are obtained as corollaries.","sentences":["In [HHL+17] the authors showed existence and uniqueness of solutions to the nonlinear one-dimensional stochastic heat equation driven by a Gaussian noise that is white in time and rougher than white in space (in particular, its covariance is not a measure).","Here we present a simple alternative to derive such results by considering the equations in the analytically weak sense, using either the variational approach or Krylov's $L^p$-theory.","Various improvements are obtained as corollaries."],"url":"http://arxiv.org/abs/2404.18920v1","category":"math.PR"}
{"created":"2024-04-29 17:57:11","title":"Strong solutions to McKean-Vlasov SDEs associated to a class of degenerate Fokker-Planck equations with coefficients of Nemytskii-type","abstract":"While the nondegenerate case is well-known, there are only few results on the existence of strong solutions to McKean-Vlasov SDEs with coefficients of Nemytskii-type in the degenerate case. We consider a broad class of degenerate nonlinear Fokker-Planck(-Kolmogorov) equations with coefficients of Nemytskii-type. This includes, in particular, the classical porous medium equation perturbed by a first-order term with initial datum in a subset of probability densities, which is dense with respect to the topology inherited from $L^1$, and, in the one-dimensional setting, the classical porous medium equation with initial datum in an arbitrary point $x\\in\\mathbb{R}$. For these kind of equations the existence of a Schwartz-distributional solution $u$ is well-known. We show that there exists a unique strong solution to the associated degenerate McKean-Vlasov SDE with time marginal law densities $u$. In particular, every weak solution to this equation with time marginal law densities $u$ can be written as a functional of the driving Brownian motion.   Moreover, plugging any Brownian motion into this very functional yields a weak solution with time marginal law densities $u$.","sentences":["While the nondegenerate case is well-known, there are only few results on the existence of strong solutions to McKean-Vlasov SDEs with coefficients of Nemytskii-type in the degenerate case.","We consider a broad class of degenerate nonlinear Fokker-Planck(-Kolmogorov) equations with coefficients of Nemytskii-type.","This includes, in particular, the classical porous medium equation perturbed by a first-order term with initial datum in a subset of probability densities, which is dense with respect to the topology inherited from $L^1$, and, in the one-dimensional setting, the classical porous medium equation with initial datum in an arbitrary point $x\\in\\mathbb{R}$.","For these kind of equations the existence of a Schwartz-distributional solution $u$ is well-known.","We show that there exists a unique strong solution to the associated degenerate McKean-Vlasov SDE with time marginal law densities $u$. In particular, every weak solution to this equation with time marginal law densities $u$ can be written as a functional of the driving Brownian motion.   ","Moreover, plugging any Brownian motion into this very functional yields a weak solution with time marginal law densities $u$."],"url":"http://arxiv.org/abs/2404.18918v1","category":"math.PR"}
{"created":"2024-04-29 17:55:27","title":"On the behavior of pressure in a low Mach number flow","abstract":"In our recent works, we proposed a theory of turbulence via the mean field effect of an intermolecular potential, which in part relies on an empirically observed \"equilibrated\" behavior of the pressure variable in a low Mach number flow -- that is, while other variables may exhibit considerable variations at low Mach numbers, the pressure is (nearly) constant. At the same time, conventional kinetic theory does not offer a satisfactory explanation for such a behavior of the pressure variable, instead leading to an isentropic flow in the form of the usual compressible Euler equations.   In the current work, we introduce a novel correction term into the pair correlation function of the BBGKY closure for the collision integral, which adjusts the density of incident or recedent pairs of particles depending on the macroscopic compression or expansion rate of the gas. Remarkably, this term does not affect the density and momentum transport equations, and manifests solely in the pressure transport equation. We also find that the effect of the novel term on the pressure dynamics matches the observed behavior -- that is, it tends to attenuate the acoustic waves and smooth out the pressure solution. It appears that the missing piece of the low Mach number puzzle has finally been identified.","sentences":["In our recent works, we proposed a theory of turbulence via the mean field effect of an intermolecular potential, which in part relies on an empirically observed \"equilibrated\" behavior of the pressure variable in a low Mach number flow -- that is, while other variables may exhibit considerable variations at low Mach numbers, the pressure is (nearly) constant.","At the same time, conventional kinetic theory does not offer a satisfactory explanation for such a behavior of the pressure variable, instead leading to an isentropic flow in the form of the usual compressible Euler equations.   ","In the current work, we introduce a novel correction term into the pair correlation function of the BBGKY closure for the collision integral, which adjusts the density of incident or recedent pairs of particles depending on the macroscopic compression or expansion rate of the gas.","Remarkably, this term does not affect the density and momentum transport equations, and manifests solely in the pressure transport equation.","We also find that the effect of the novel term on the pressure dynamics matches the observed behavior -- that is, it tends to attenuate the acoustic waves and smooth out the pressure solution.","It appears that the missing piece of the low Mach number puzzle has finally been identified."],"url":"http://arxiv.org/abs/2404.18914v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:50:32","title":"On the uncommonness of minimal rank-2 systems of linear equations","abstract":"We prove that suitably generic pairs of linear equations on an even number of variables are uncommon. This verifies a conjecture of Kam\\v{c}ev, Morrison and the second author. Moreover, we prove that any large system containing such a $(2\\times k)$-system as a minimal subsystem is uncommon.","sentences":["We prove that suitably generic pairs of linear equations on an even number of variables are uncommon.","This verifies a conjecture of Kam\\v{c}ev, Morrison and the second author.","Moreover, we prove that any large system containing such a $(2\\times k)$-system as a minimal subsystem is uncommon."],"url":"http://arxiv.org/abs/2404.18908v1","category":"math.CO"}
{"created":"2024-04-29 17:39:59","title":"Capacity threshold for the Ising perceptron","abstract":"We show that the capacity of the Ising perceptron is with high probability upper bounded by the constant $\\alpha_\\star \\approx 0.833$ conjectured by Krauth and M\\'ezard, under the condition that an explicit two-variable function $\\mathscr{S}_\\star(\\lambda_1,\\lambda_2)$ is maximized at $(1,0)$. The earlier work of Ding and Sun proves the matching lower bound subject to a similar numerical condition, and together these results give a conditional proof of the conjecture of Krauth and M\\'ezard.","sentences":["We show that the capacity of the Ising perceptron is with high probability upper bounded by the constant $\\alpha_\\star \\approx 0.833$ conjectured by Krauth and M\\'ezard, under the condition that an explicit two-variable function $\\mathscr{S}_\\star(\\lambda_1,\\lambda_2)$ is maximized at $(1,0)$. The earlier work of Ding and Sun proves the matching lower bound subject to a similar numerical condition, and together these results give a conditional proof of the conjecture of Krauth and M\\'ezard."],"url":"http://arxiv.org/abs/2404.18902v1","category":"math.PR"}
{"created":"2024-04-29 17:34:42","title":"Celestial Optical Theorem","abstract":"We establish the nonperturbative celestial optical theorem from the unitarity of $S$-matrix. This theorem provides a set of nonperturbative bootstrap equations of the conformal partial wave (CPW) coefficients. The celestial optical theorem implies that the imaginary part of CPW coefficient with appropriate conformal dimensions is non-negative. By making certain assumptions and using the celestial optical theorem, we derive nonperturbative results concerning the analytic structure of CPW coefficients. We discover that the CPW coefficients of four massless particles must and only have simple poles located at specific positions. The CPW coefficients involving massive particles exhibit double-trace poles, indicating the existence of double-trace operators in nonperturbative CCFT. It is worth noting that, in contrast to AdS/CFT, the conformal dimensions of double-trace operators do not receive anomalous dimensions.","sentences":["We establish the nonperturbative celestial optical theorem from the unitarity of $S$-matrix.","This theorem provides a set of nonperturbative bootstrap equations of the conformal partial wave (CPW) coefficients.","The celestial optical theorem implies that the imaginary part of CPW coefficient with appropriate conformal dimensions is non-negative.","By making certain assumptions and using the celestial optical theorem, we derive nonperturbative results concerning the analytic structure of CPW coefficients.","We discover that the CPW coefficients of four massless particles must and only have simple poles located at specific positions.","The CPW coefficients involving massive particles exhibit double-trace poles, indicating the existence of double-trace operators in nonperturbative CCFT.","It is worth noting that, in contrast to AdS/CFT, the conformal dimensions of double-trace operators do not receive anomalous dimensions."],"url":"http://arxiv.org/abs/2404.18898v1","category":"hep-th"}
{"created":"2024-04-29 17:34:24","title":"Neural network prediction of model parameters for strong lensing samples from Hyper Suprime-Cam Survey","abstract":"Galaxies that cause the strong gravitational lensing of background galaxies provide us crucial information about the distribution of matter around them. Traditional modelling methods that analyse such strong lenses are both time and resource consuming, require sophisticated lensing codes and modelling expertise. To study the large lens population expected from imaging surveys such as LSST, we need fast and automated analysis methods. In this work, we build and train a simple convolutional neural network with an aim to rapidly predict model parameters of gravitational lenses. We focus on the most important lens mass model parameters, namely, the Einstein radius, the axis ratio and the position angle of the major axis of the mass distribution. The network is trained on a variety of simulated data with an increasing degree of realism and shows satisfactory performance on simulated test data. The trained network is then applied to the real sample of galaxy-scale candidate lenses from the Subaru HSC, a precursor survey to LSST. Unlike the simulated lenses, we do not have the ground truth for the real lenses. Therefore, we have compared our predictions with those from YattaLens, a lens modelling pipeline. Additionally, we also compare the parameter predictions for 10 HSC lenses that were also studied by other conventional modelling methods. These comparisons show a fair quantitative agreement on the Einstein radius, although the axis ratio and the position angle from the network as well as the individual modelling methods, seem to have systematic uncertainties beyond the quoted errors.","sentences":["Galaxies that cause the strong gravitational lensing of background galaxies provide us crucial information about the distribution of matter around them.","Traditional modelling methods that analyse such strong lenses are both time and resource consuming, require sophisticated lensing codes and modelling expertise.","To study the large lens population expected from imaging surveys such as LSST, we need fast and automated analysis methods.","In this work, we build and train a simple convolutional neural network with an aim to rapidly predict model parameters of gravitational lenses.","We focus on the most important lens mass model parameters, namely, the Einstein radius, the axis ratio and the position angle of the major axis of the mass distribution.","The network is trained on a variety of simulated data with an increasing degree of realism and shows satisfactory performance on simulated test data.","The trained network is then applied to the real sample of galaxy-scale candidate lenses from the Subaru HSC, a precursor survey to LSST.","Unlike the simulated lenses, we do not have the ground truth for the real lenses.","Therefore, we have compared our predictions with those from YattaLens, a lens modelling pipeline.","Additionally, we also compare the parameter predictions for 10 HSC lenses that were also studied by other conventional modelling methods.","These comparisons show a fair quantitative agreement on the Einstein radius, although the axis ratio and the position angle from the network as well as the individual modelling methods, seem to have systematic uncertainties beyond the quoted errors."],"url":"http://arxiv.org/abs/2404.18897v1","category":"astro-ph.CO"}
{"created":"2024-04-29 17:31:00","title":"RSCaMa: Remote Sensing Image Change Captioning with State Space Model","abstract":"Remote Sensing Image Change Captioning (RSICC) aims to identify surface changes in multi-temporal remote sensing images and describe them in natural language. Current methods typically rely on an encoder-decoder architecture and focus on designing a sophisticated neck to process bi-temporal features extracted by the backbone. Recently, State Space Models (SSMs), especially Mamba, have demonstrated outstanding performance in many fields, owing to their efficient feature-selective modelling capability. However, their potential in the RSICC task remains unexplored. In this paper, we introduce Mamba into RSICC and propose a novel approach called RSCaMa (Remote Sensing Change Captioning Mamba). Specifically, we utilize Siamese backbones to extract bi-temporal features, which are then processed through multiple CaMa layers consisting of Spatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM). SD-SSM uses differential features to enhance change perception, while TT-SSM promotes bitemporal interactions in a token-wise cross-scanning manner. Experimental results validate the effectiveness of CaMa layers and demonstrate the superior performance of RSCaMa, as well as the potential of Mamba in the RSICC task. Additionally, we systematically compare the effects of three language decoders, including Mamba, GPT-style decoder with causal attention mechanism, and Transformer decoder with cross-attention mechanism. This provides valuable insights for future RSICC research. The code will be available at https://github.com/Chen-Yang-Liu/RSCaMa","sentences":["Remote Sensing Image Change Captioning (RSICC) aims to identify surface changes in multi-temporal remote sensing images and describe them in natural language.","Current methods typically rely on an encoder-decoder architecture and focus on designing a sophisticated neck to process bi-temporal features extracted by the backbone.","Recently, State Space Models (SSMs), especially Mamba, have demonstrated outstanding performance in many fields, owing to their efficient feature-selective modelling capability.","However, their potential in the RSICC task remains unexplored.","In this paper, we introduce Mamba into RSICC and propose a novel approach called RSCaMa","(Remote Sensing Change Captioning Mamba).","Specifically, we utilize Siamese backbones to extract bi-temporal features, which are then processed through multiple CaMa layers consisting of Spatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM).","SD-SSM uses differential features to enhance change perception, while TT-SSM promotes bitemporal interactions in a token-wise cross-scanning manner.","Experimental results validate the effectiveness of CaMa layers and demonstrate the superior performance of RSCaMa, as well as the potential of Mamba in the RSICC task.","Additionally, we systematically compare the effects of three language decoders, including Mamba, GPT-style decoder with causal attention mechanism, and Transformer decoder with cross-attention mechanism.","This provides valuable insights for future RSICC research.","The code will be available at https://github.com/Chen-Yang-Liu/RSCaMa"],"url":"http://arxiv.org/abs/2404.18895v1","category":"cs.CV"}
{"created":"2024-04-29 17:19:07","title":"A Thom Isotopy Theorem for nonproper semialgebraic maps","abstract":"We prove a version of the Thom Isotopy Theorem for nonproper semialgebraic maps $f\\colon X\\rightarrow \\mathbb{R}^m$, where $X \\subset\\mathbb{R}^n$ is a semialgebraic set and $f$ is the restriction to $X$ of a smooth semialgebraic map $F:\\mathbb{R}^n\\to \\mathbb{R}^m$.","sentences":["We prove a version of the Thom Isotopy Theorem for nonproper semialgebraic maps $f\\colon X\\rightarrow \\mathbb{R}^m$, where $X \\subset\\mathbb{R}^n$ is a semialgebraic set and $f$ is the restriction to $X$ of a smooth semialgebraic map $F:\\mathbb{R}^n\\to \\mathbb{R}^m$."],"url":"http://arxiv.org/abs/2404.18883v1","category":"math.DG"}
{"created":"2024-04-29 17:14:08","title":"High-Energy Reaction Dynamics of N$_{3}$","abstract":"The atom-exchange and atomization dissociation dynamics for the N($^4$S) + N$_2(^1 \\Sigma_{\\rm g}^+)$ reaction is studied using a reproducing kernel Hilbert space (RKHS)-based, global potential energy surface (PES) at the MRCI-F12/aug-cc-pVTZ-F12 level of theory. For the atom exchange reaction $({\\rm N_A N_B} + {\\rm N_C} \\rightarrow {\\rm   N_A N_C} + {\\rm N_B}$), computed thermal rates and their temperature dependence from quasi-classical trajectory (QCT) simulations agree to within error bars with the available experiments. Companion QCT simulations using a recently published CASPT2-based PES confirm these findings. For the atomization reaction, leading to three N$(^4{\\rm   S})$ atoms, the computed rates from the RKHS-PES overestimate the experimentally reported rates by one order of magnitude whereas those from the PIP-PES agree favourably, and the $T$-dependence of both computations is consistent with experiment. These differences can be traced back to the different methods and basis sets used. The lifetime of the metastable N$_3$ molecule is estimated to be $\\sim 200$ fs depending on the initial state of the reactants. Finally, neural network-based exhaustive state-to-distribution models are presented using both PESs for the atom exchange reaction. These models will be instrumental for a broader exploration of the reaction dynamics of air.","sentences":["The atom-exchange and atomization dissociation dynamics for the N($^4$S)","+","N$_2(^1 \\Sigma_{\\rm g}^+)$ reaction is studied using a reproducing kernel Hilbert space (RKHS)-based, global potential energy surface (PES) at the MRCI-F12/aug-cc-pVTZ-F12 level of theory.","For the atom exchange reaction $({\\rm N_A N_B} + {\\rm N_C} \\rightarrow {\\rm   N_A N_C} + {\\rm N_B}$), computed thermal rates and their temperature dependence from quasi-classical trajectory (QCT) simulations agree to within error bars with the available experiments.","Companion QCT simulations using a recently published CASPT2-based PES confirm these findings.","For the atomization reaction, leading to three N$(^4{\\rm   S})$ atoms, the computed rates from the RKHS-PES overestimate the experimentally reported rates by one order of magnitude whereas those from the PIP-PES agree favourably, and the $T$-dependence of both computations is consistent with experiment.","These differences can be traced back to the different methods and basis sets used.","The lifetime of the metastable N$_3$ molecule is estimated to be $\\sim 200$ fs depending on the initial state of the reactants.","Finally, neural network-based exhaustive state-to-distribution models are presented using both PESs for the atom exchange reaction.","These models will be instrumental for a broader exploration of the reaction dynamics of air."],"url":"http://arxiv.org/abs/2404.18877v1","category":"physics.chem-ph"}
{"created":"2024-04-29 17:02:45","title":"Avalanche Dynamics and the Effect of Straining in Dislocation Systems with Quenched Disorder","abstract":"The plastic deformation of crystalline and other heterogeneous materials often manifests in stochastic intermittent events indicating the criticality of plastic behavior. Previous studies demonstrated that the presence of short-ranged quenched disorder modifies this behavior disrupting long-range static and dynamic correlations consequently localizing dislocation avalanches. However, these observations were mostly confined to relaxed materials devoid of deformation history. In this work our focus is on how straining affects static and dynamic correlations, avalanche dynamics and local yield stresses. We demonstrate that the interplay between severe straining and confining quenched disorder induces critical behavior characterized by dislocation avalanches distinct from those at lower stresses. Namely, near the flow stress many avalanches, even if triggered locally, evolve into events affecting a larger region by exciting small clusters of dislocations all around the sample. This type of avalanches differ from the ones at low strains where plastic events typically consist of one compact cluster of dislocations which is either local or it is already quite extended at the onset of the avalanche. Furthermore, we examine the impact of avalanches on local yield stresses. It is shown in detail in this work that while some statistical features of the local yield thresholds are robust to straining, others are significantly affected by the deformation history.","sentences":["The plastic deformation of crystalline and other heterogeneous materials often manifests in stochastic intermittent events indicating the criticality of plastic behavior.","Previous studies demonstrated that the presence of short-ranged quenched disorder modifies this behavior disrupting long-range static and dynamic correlations consequently localizing dislocation avalanches.","However, these observations were mostly confined to relaxed materials devoid of deformation history.","In this work our focus is on how straining affects static and dynamic correlations, avalanche dynamics and local yield stresses.","We demonstrate that the interplay between severe straining and confining quenched disorder induces critical behavior characterized by dislocation avalanches distinct from those at lower stresses.","Namely, near the flow stress many avalanches, even if triggered locally, evolve into events affecting a larger region by exciting small clusters of dislocations all around the sample.","This type of avalanches differ from the ones at low strains where plastic events typically consist of one compact cluster of dislocations which is either local or it is already quite extended at the onset of the avalanche.","Furthermore, we examine the impact of avalanches on local yield stresses.","It is shown in detail in this work that while some statistical features of the local yield thresholds are robust to straining, others are significantly affected by the deformation history."],"url":"http://arxiv.org/abs/2404.18871v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 16:54:21","title":"K\u00e4hler Soliton Surfaces Are Generically Toric","abstract":"Let $(M, g, \\omega, f, \\lambda)$ be a K\\\"{a}hler gradient Ricci soliton in real dimension four. The first theorem states that it is an integrable Hamiltonian system in a classical sense. Furthermore, it is either of cohomogeneity one or the integrals of motion are given by the potential function $f$ and the scalar curvature $\\text{S}$. The second theorem states that it must be toric under a generic assumption. That is, one assumes that the system is non-degenerate and the potential function $f$ is proper. In case $\\lambda=0$, one further assumes that $f$ is bounded below or above. Then there is an effective, completely integrable Hamiltonian toric $\\mathbb{T}^2$- action on $(M, \\omega)$.","sentences":["Let $(M, g, \\omega, f, \\lambda)$ be a K\\\"{a}hler gradient Ricci soliton in real dimension four.","The first theorem states that it is an integrable Hamiltonian system in a classical sense.","Furthermore, it is either of cohomogeneity one or the integrals of motion are given by the potential function $f$ and the scalar curvature $\\text{S}$. The second theorem states that it must be toric under a generic assumption.","That is, one assumes that the system is non-degenerate and the potential function $f$ is proper.","In case $\\lambda=0$, one further assumes that $f$ is bounded below or above.","Then there is an effective, completely integrable Hamiltonian toric $\\mathbb{T}^2$- action on $(M, \\omega)$."],"url":"http://arxiv.org/abs/2404.18866v1","category":"math.DG"}
{"created":"2024-04-29 16:52:07","title":"PlanNetX: Learning an Efficient Neural Network Planner from MPC for Longitudinal Control","abstract":"Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.","sentences":["Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems.","However, the computational complexity of online optimization can be problematic on embedded devices.","Especially, when we need to guarantee fixed control frequencies.","Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network.","In this work, we instead learn the whole planned trajectory of the MPC.","We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC.","We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data.","Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning."],"url":"http://arxiv.org/abs/2404.18863v1","category":"cs.RO"}
{"created":"2024-04-29 16:51:30","title":"A Survey on Vision Mamba: Models, Applications and Challenges","abstract":"Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks. Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting. Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers. Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model. Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works. To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches. This paper begins by delineating the formulation of the original Mamba model. Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba. We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others. Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion. Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.","sentences":["Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks.","Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting.","Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers.","Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model.","Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works.","To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches.","This paper begins by delineating the formulation of the original Mamba model.","Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba.","We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others.","Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion.","Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area.","A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models."],"url":"http://arxiv.org/abs/2404.18861v1","category":"cs.CV"}
{"created":"2024-04-29 16:27:29","title":"Construction of local reduced spaces for Friedrichs' systems via randomized training","abstract":"This contribution extends the localized training approach, traditionally employed for multiscale problems and parameterized partial differential equations (PDEs) featuring locally heterogeneous coefficients, to the class of linear, positive symmetric operators, known as Friedrichs' operators. Considering a local subdomain with corresponding oversampling domain we prove the compactness of the transfer operator which maps boundary data to solutions on the interior domain. While a Caccioppoli-inequality quantifying the energy decay to the interior holds true for all Friedrichs' systems, showing a compactness result for the graph-spaces hosting the solution is additionally necessary. We discuss the mixed formulation of a convection-diffusion-reaction problem where the necessary compactness result is obtained by the Picard-Weck-Weber theorem. Our numerical results, focusing on a scenario involving heterogeneous diffusion fields with multiple high-conductivity channels, demonstrate the effectiveness of the proposed method.","sentences":["This contribution extends the localized training approach, traditionally employed for multiscale problems and parameterized partial differential equations (PDEs) featuring locally heterogeneous coefficients, to the class of linear, positive symmetric operators, known as Friedrichs' operators.","Considering a local subdomain with corresponding oversampling domain we prove the compactness of the transfer operator which maps boundary data to solutions on the interior domain.","While a Caccioppoli-inequality quantifying the energy decay to the interior holds true for all Friedrichs' systems, showing a compactness result for the graph-spaces hosting the solution is additionally necessary.","We discuss the mixed formulation of a convection-diffusion-reaction problem where the necessary compactness result is obtained by the Picard-Weck-Weber theorem.","Our numerical results, focusing on a scenario involving heterogeneous diffusion fields with multiple high-conductivity channels, demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.18839v1","category":"math.NA"}
{"created":"2024-04-29 16:25:07","title":"Continuity of attractors of parabolic equations with nonlinear boundary conditions and rapidly varying boundaries. The case of a Lipschitz deformation","abstract":"In this paper we obtain the continuity of attractors for nonlinear parabolic equations with nonlinear boundary conditions when the boundary of the domain varies very rapidly as a parameter $\\epsilon$ goes to zero. We want to consider the case which the boundary of the domain presents a highly oscillatory behavior as $\\epsilon$ goes to zero. For the case where we have a Lipschitz deformation of the boundary with the Lipschitz constant uniformly bounded in $\\epsilon$ but the boundaries do not approach in a Lipschitz sense, the solutions of these equations $E$-converge to the solution of a limit parabolic equation of the same type, where the boundary condition has a factor that captures the oscillations of the boundary. To address this problem, it is necessary to consider the notion of convergence of functions defined in varying domains and the convergence of a family of operators defined in different Banach spaces. Since the problems have nonlinear terms at the boundary, then it is necessary to extend these concepts to the case of spaces with negative exponents and to operators defined between these spaces.","sentences":["In this paper we obtain the continuity of attractors for nonlinear parabolic equations with nonlinear boundary conditions when the boundary of the domain varies very rapidly as a parameter $\\epsilon$ goes to zero.","We want to consider the case which the boundary of the domain presents a highly oscillatory behavior as $\\epsilon$ goes to zero.","For the case where we have a Lipschitz deformation of the boundary with the Lipschitz constant uniformly bounded in $\\epsilon$ but the boundaries do not approach in a Lipschitz sense, the solutions of these equations $E$-converge to the solution of a limit parabolic equation of the same type, where the boundary condition has a factor that captures the oscillations of the boundary.","To address this problem, it is necessary to consider the notion of convergence of functions defined in varying domains and the convergence of a family of operators defined in different Banach spaces.","Since the problems have nonlinear terms at the boundary, then it is necessary to extend these concepts to the case of spaces with negative exponents and to operators defined between these spaces."],"url":"http://arxiv.org/abs/2404.18836v1","category":"math.AP"}
{"created":"2024-04-29 16:04:49","title":"Non-parametric estimation for the stochastic wave equation","abstract":"The spatially dependent wave speed of a stochastic wave equation driven by space-time white noise is estimated using the local observation scheme. Given a fixed time horizon, we prove asymptotic normality for an augmented maximum likelihood estimator as the resolution level of the observations tends to zero. We show that the expectation and variance of the observed Fisher information are intrinsically related to the kinetic energy within an associated deterministic wave equation and prove an asymptotic equipartition of energy principle using the notion of asymptotic Riemann-Lebesgue operators.","sentences":["The spatially dependent wave speed of a stochastic wave equation driven by space-time white noise is estimated using the local observation scheme.","Given a fixed time horizon, we prove asymptotic normality for an augmented maximum likelihood estimator as the resolution level of the observations tends to zero.","We show that the expectation and variance of the observed Fisher information are intrinsically related to the kinetic energy within an associated deterministic wave equation and prove an asymptotic equipartition of energy principle using the notion of asymptotic Riemann-Lebesgue operators."],"url":"http://arxiv.org/abs/2404.18823v1","category":"math.ST"}
{"created":"2024-04-29 15:52:05","title":"Bifurcations for Lagrangian systems and geodesics","abstract":"In this paper we shall use the abstract bifurcation theorems developed by the author in previous papers to study bifurcations of solutions for Lagrangian systems on manifolds linearly or nonlinearly dependent on parameters under various boundary value conditions. As applications, many bifurcation results for geodesics on Finsler and Riemannian manifolds are derived.","sentences":["In this paper we shall use the abstract bifurcation theorems developed by the author in previous papers to study bifurcations of solutions for Lagrangian systems on manifolds linearly or nonlinearly dependent on parameters under various boundary value conditions.","As applications, many bifurcation results for geodesics on Finsler and Riemannian manifolds are derived."],"url":"http://arxiv.org/abs/2404.18815v1","category":"math.DS"}
{"created":"2024-04-29 15:51:49","title":"Belt and Brace: When Federated Learning Meets Differential Privacy","abstract":"Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.Differential privacy (DP) is the de facto standard of privacy protection with provable guarantees.Advances in ML suggest that DP would be a perfect fit for FL with comprehensive privacy preservation. Hence, extensive efforts have been devoted to achieving practically usable FL with DP, which however is still challenging.Practitioners often not only are not fully aware of its development and categorization, but also face a hard choice between privacy and utility. Therefore, it calls for a holistic review of current advances and an investigation on the challenges and opportunities for highly usable FL systems with a DP guarantee. In this article, we first introduce the primary concepts of FL and DP, and highlight the benefits of integration. We then review the current developments by categorizing different paradigms and notions. Aiming at usable FL with DP, we present the optimization principles to seek a better tradeoff between model utility and privacy loss. Finally, we discuss future challenges in the emergent areas and relevant research topics.","sentences":["Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.","Differential privacy (DP) is the de facto standard of privacy protection with provable guarantees.","Advances in ML suggest that DP would be a perfect fit for FL with comprehensive privacy preservation.","Hence, extensive efforts have been devoted to achieving practically usable FL with DP, which however is still challenging.","Practitioners often not only are not fully aware of its development and categorization, but also face a hard choice between privacy and utility.","Therefore, it calls for a holistic review of current advances and an investigation on the challenges and opportunities for highly usable FL systems with a DP guarantee.","In this article, we first introduce the primary concepts of FL and DP, and highlight the benefits of integration.","We then review the current developments by categorizing different paradigms and notions.","Aiming at usable FL with DP, we present the optimization principles to seek a better tradeoff between model utility and privacy loss.","Finally, we discuss future challenges in the emergent areas and relevant research topics."],"url":"http://arxiv.org/abs/2404.18814v1","category":"cs.CR"}
{"created":"2024-04-29 15:49:37","title":"Safe Reach Set Computation via Neural Barrier Certificates","abstract":"We present a novel technique for online safety verification of autonomous systems, which performs reachability analysis efficiently for both bounded and unbounded horizons by employing neural barrier certificates. Our approach uses barrier certificates given by parameterized neural networks that depend on a given initial set, unsafe sets, and time horizon. Such networks are trained efficiently offline using system simulations sampled from regions of the state space. We then employ a meta-neural network to generalize the barrier certificates to state space regions that are outside the training set. These certificates are generated and validated online as sound over-approximations of the reachable states, thus either ensuring system safety or activating appropriate alternative actions in unsafe scenarios. We demonstrate our technique on case studies from linear models to nonlinear control-dependent models for online autonomous driving scenarios.","sentences":["We present a novel technique for online safety verification of autonomous systems, which performs reachability analysis efficiently for both bounded and unbounded horizons by employing neural barrier certificates.","Our approach uses barrier certificates given by parameterized neural networks that depend on a given initial set, unsafe sets, and time horizon.","Such networks are trained efficiently offline using system simulations sampled from regions of the state space.","We then employ a meta-neural network to generalize the barrier certificates to state space regions that are outside the training set.","These certificates are generated and validated online as sound over-approximations of the reachable states, thus either ensuring system safety or activating appropriate alternative actions in unsafe scenarios.","We demonstrate our technique on case studies from linear models to nonlinear control-dependent models for online autonomous driving scenarios."],"url":"http://arxiv.org/abs/2404.18813v1","category":"eess.SY"}
{"created":"2024-04-29 15:42:36","title":"Convergence of dynamical stationary fluctuations","abstract":"We present a general black box theorem that ensures convergence of a sequence of stationary Markov processes, provided a few assumptions are satisfied. This theorem relies on a control of the resolvents of the sequence of Markov processes, and on a suitable characterization of the resolvents of the limit. One major advantage of this approach is that it circumvents the use of the Boltzmann-Gibbs principle: in particular, we deduce in a rather simple way that the stationary fluctuations of the one-dimensional zero-range process converge to the stochastic heat equation. It also allows to establish results that were probably out of reach of existing methods: using the black box result, we are able to prove that the stationary fluctuations of a discrete model of ordered interfaces, that was considered previously in the statistical physics literature, converge to a system of reflected stochastic PDEs.","sentences":["We present a general black box theorem that ensures convergence of a sequence of stationary Markov processes, provided a few assumptions are satisfied.","This theorem relies on a control of the resolvents of the sequence of Markov processes, and on a suitable characterization of the resolvents of the limit.","One major advantage of this approach is that it circumvents the use of the Boltzmann-Gibbs principle: in particular, we deduce in a rather simple way that the stationary fluctuations of the one-dimensional zero-range process converge to the stochastic heat equation.","It also allows to establish results that were probably out of reach of existing methods: using the black box result, we are able to prove that the stationary fluctuations of a discrete model of ordered interfaces, that was considered previously in the statistical physics literature, converge to a system of reflected stochastic PDEs."],"url":"http://arxiv.org/abs/2404.18803v1","category":"math.PR"}
{"created":"2024-04-29 15:29:24","title":"When Lawvere meets Peirce: an equational presentation of boolean hyperdoctrines","abstract":"Fo-bicategories are a categorification of Peirce's calculus of relations. Notably, their laws provide a proof system for first-order logic that is both purely equational and complete. This paper illustrates a correspondence between fo-bicategories and Lawvere's hyperdoctrines. To streamline our proof, we introduce peircean bicategories, which offer a more succinct characterization of fo-bicategories.","sentences":["Fo-bicategories are a categorification of Peirce's calculus of relations.","Notably, their laws provide a proof system for first-order logic that is both purely equational and complete.","This paper illustrates a correspondence between fo-bicategories and Lawvere's hyperdoctrines.","To streamline our proof, we introduce peircean bicategories, which offer a more succinct characterization of fo-bicategories."],"url":"http://arxiv.org/abs/2404.18795v1","category":"math.CT"}
{"created":"2024-04-29 15:27:09","title":"A real-time digital twin of azimuthal thermoacoustic instabilities","abstract":"When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.","sentences":["When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines.","We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor.","The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations.","First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic.","Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously.","This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem.","Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations.","Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios.","We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables.","The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models.","This work opens new opportunities for real-time digital twinning of multi-physics problems."],"url":"http://arxiv.org/abs/2404.18793v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 15:26:58","title":"Bergman local isometries are biholomorphisms","abstract":"We prove that a proper holomorphic local isometry between bounded domains with respect to the Bergman metrics is necessarily a biholomorphism. The proof relies on a new method grounded in Information Geometry theories.","sentences":["We prove that a proper holomorphic local isometry between bounded domains with respect to the Bergman metrics is necessarily a biholomorphism.","The proof relies on a new method grounded in Information Geometry theories."],"url":"http://arxiv.org/abs/2404.18792v1","category":"math.CV"}
{"created":"2024-04-29 15:16:33","title":"Optimal time sampling in physics-informed neural networks","abstract":"Physics-informed neural networks (PINN) is a extremely powerful paradigm used to solve equations encountered in scientific computing applications. An important part of the procedure is the minimization of the equation residual which includes, when the equation is time-dependent, a time sampling. It was argued in the literature that the sampling need not be uniform but should overweight initial time instants, but no rigorous explanation was provided for these choice. In this paper we take some prototypical examples and, under standard hypothesis concerning the neural network convergence, we show that the optimal time sampling follows a truncated exponential distribution. In particular we explain when the time sampling is best to be uniform and when it should not be. The findings are illustrated with numerical examples on linear equation, Burgers' equation and the Lorenz system.","sentences":["Physics-informed neural networks (PINN) is a extremely powerful paradigm used to solve equations encountered in scientific computing applications.","An important part of the procedure is the minimization of the equation residual which includes, when the equation is time-dependent, a time sampling.","It was argued in the literature that the sampling need not be uniform but should overweight initial time instants, but no rigorous explanation was provided for these choice.","In this paper we take some prototypical examples and, under standard hypothesis concerning the neural network convergence, we show that the optimal time sampling follows a truncated exponential distribution.","In particular we explain when the time sampling is best to be uniform and when it should not be.","The findings are illustrated with numerical examples on linear equation, Burgers' equation and the Lorenz system."],"url":"http://arxiv.org/abs/2404.18780v1","category":"cs.LG"}
{"created":"2024-04-29 15:04:25","title":"Generalizing Space Logistics Network Optimization with Integrated Machine Learning and Mathematical Programming","abstract":"Recent growing complexity in space missions has led to an active research field of space logistics and mission design. This research field leverages the key ideas and methods used to handle complex terrestrial logistics to tackle space logistics design problems. A typical goal in space logistics is to optimize the commodity flow to satisfy some mission objectives with the lowest cost. One of the successful space logistics approaches is network flow modeling and optimization using mixed-integer linear programming (MILP). A caveat of the conventional MILP-based network approach for space logistics is its incapability of handling nonlinearity. For example, in the MILP formulation, the spacecraft structure mass and fuel/payload capacity are approximated by a linear relationship. However, this oversimplified relationship cannot characterize a realistic spacecraft design. Other types of nonlinearity can appear when a nonlinear time-dependent trajectory model is considered in an event-driven network, where the time step of each event itself is a variable. In response to this challenge, this Note develops a new systematic general framework to handle nonlinearity in the MILP-based space logistics formulation using machine learning (ML). Specifically, we replace the nonlinear constraints in the space logistics formulation with trained ML models that are compatible with MILP. The MILP-compatible ML model includes linear regression, PWL approximations, neural networks (NN) with Rectified Linear Unit (ReLU) activations, decision tree regression, and random forest regression, among others; these models can be translated into MILP formulations with a definition of additional variables and constraints while maintaining the linearity. This Note provides the first demonstration of using such trained ML models directly in a MILP-based space logistics optimization formulation.","sentences":["Recent growing complexity in space missions has led to an active research field of space logistics and mission design.","This research field leverages the key ideas and methods used to handle complex terrestrial logistics to tackle space logistics design problems.","A typical goal in space logistics is to optimize the commodity flow to satisfy some mission objectives with the lowest cost.","One of the successful space logistics approaches is network flow modeling and optimization using mixed-integer linear programming (MILP).","A caveat of the conventional MILP-based network approach for space logistics is its incapability of handling nonlinearity.","For example, in the MILP formulation, the spacecraft structure mass and fuel/payload capacity are approximated by a linear relationship.","However, this oversimplified relationship cannot characterize a realistic spacecraft design.","Other types of nonlinearity can appear when a nonlinear time-dependent trajectory model is considered in an event-driven network, where the time step of each event itself is a variable.","In response to this challenge, this Note develops a new systematic general framework to handle nonlinearity in the MILP-based space logistics formulation using machine learning (ML).","Specifically, we replace the nonlinear constraints in the space logistics formulation with trained ML models that are compatible with MILP.","The MILP-compatible ML model includes linear regression, PWL approximations, neural networks (NN) with Rectified Linear Unit (ReLU) activations, decision tree regression, and random forest regression, among others; these models can be translated into MILP formulations with a definition of additional variables and constraints while maintaining the linearity.","This Note provides the first demonstration of using such trained ML models directly in a MILP-based space logistics optimization formulation."],"url":"http://arxiv.org/abs/2404.18770v1","category":"math.OC"}
{"created":"2024-04-29 15:04:07","title":"Learning with Norm Constrained, Over-parameterized, Two-layer Neural Networks","abstract":"Recent studies show that a reproducing kernel Hilbert space (RKHS) is not a suitable space to model functions by neural networks as the curse of dimensionality (CoD) cannot be evaded when trying to approximate even a single ReLU neuron (Bach, 2017). In this paper, we study a suitable function space for over-parameterized two-layer neural networks with bounded norms (e.g., the path norm, the Barron norm) in the perspective of sample complexity and generalization properties. First, we show that the path norm (as well as the Barron norm) is able to obtain width-independence sample complexity bounds, which allows for uniform convergence guarantees. Based on this result, we derive the improved result of metric entropy for $\\epsilon$-covering up to $\\mathcal{O}(\\epsilon^{-\\frac{2d}{d+2}})$ ($d$ is the input dimension and the depending constant is at most polynomial order of $d$) via the convex hull technique, which demonstrates the separation with kernel methods with $\\Omega(\\epsilon^{-d})$ to learn the target function in a Barron space. Second, this metric entropy result allows for building a sharper generalization bound under a general moment hypothesis setting, achieving the rate at $\\mathcal{O}(n^{-\\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers a sharper and refined estimation for metric entropy (with a clear dependence relationship on the dimension $d$) and unbounded sampling in the estimation of the sample error and the output error.","sentences":["Recent studies show that a reproducing kernel Hilbert space (RKHS) is not a suitable space to model functions by neural networks as the curse of dimensionality (CoD) cannot be evaded when trying to approximate even a single ReLU neuron (Bach, 2017).","In this paper, we study a suitable function space for over-parameterized two-layer neural networks with bounded norms (e.g., the path norm, the Barron norm) in the perspective of sample complexity and generalization properties.","First, we show that the path norm (as well as the Barron norm) is able to obtain width-independence sample complexity bounds, which allows for uniform convergence guarantees.","Based on this result, we derive the improved result of metric entropy for $\\epsilon$-covering up to $\\mathcal{O}(\\epsilon^{-\\frac{2d}{d+2}})$ ($d$ is the input dimension and the depending constant is at most polynomial order of $d$) via the convex hull technique, which demonstrates the separation with kernel methods with $\\Omega(\\epsilon^{-d})$ to learn the target function in a Barron space.","Second, this metric entropy result allows for building a sharper generalization bound under a general moment hypothesis setting, achieving the rate at $\\mathcal{O}(n^{-\\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers a sharper and refined estimation for metric entropy (with a clear dependence relationship on the dimension $d$) and unbounded sampling in the estimation of the sample error and the output error."],"url":"http://arxiv.org/abs/2404.18769v1","category":"stat.ML"}
{"created":"2024-04-29 15:02:57","title":"A Port-Hamiltonian System Perspective on Electromagneto-Quasistatic Field Formulations of Darwin-Type","abstract":"Electromagneto-quasistatic (EMQS) field formulations are often dubbed as Darwin-type field formulations which approximate the Maxwell equations by neglecting radiation effects while modelling resistive, capacitive, and inductive effects. A common feature of EMQS field models is the Darwin-Amp\\'ere equation formulated with the magnetic vector potential and the electric scalar potential. EMQS field formulations yield different approximations to the Maxwell equations by choice of additional gauge equations. These EMQS formulations are analyzed within the port-Hamiltonian system (PHS) framework. It is shown via the PHS compatibility equation that formulations based on the combination of the Darwin-Amp\\'ere equation and the full Maxwell continuity equation yield port-Hamiltonian systems implying numerical stability and specific EMQS energy conservation.","sentences":["Electromagneto-quasistatic (EMQS) field formulations are often dubbed as Darwin-type field formulations which approximate the Maxwell equations by neglecting radiation effects while modelling resistive, capacitive, and inductive effects.","A common feature of EMQS field models is the Darwin-Amp\\'ere equation formulated with the magnetic vector potential and the electric scalar potential.","EMQS field formulations yield different approximations to the Maxwell equations by choice of additional gauge equations.","These EMQS formulations are analyzed within the port-Hamiltonian system (PHS) framework.","It is shown via the PHS compatibility equation that formulations based on the combination of the Darwin-Amp\\'ere equation and the full Maxwell continuity equation yield port-Hamiltonian systems implying numerical stability and specific EMQS energy conservation."],"url":"http://arxiv.org/abs/2404.18767v1","category":"cs.CE"}
{"created":"2024-04-29 15:01:46","title":"Mathematical modelling of heat transfer in closed electrical contacts and electrical potential field dynamics with Thomson effect","abstract":"In this study we develop a mathematical model that describe the behavior of electromagnetic fields and heat transfer in closed electrical contacts that arises when instantaneous explosion of the micro-asperity which involves vaporization zone and liquid, solid zones where temperature is defined by a generalized heat equation with Thomson effect. This model account for the nonlinear nature of the thermal coefficients and electrical conductivity depended on temperature. Our proposed solutions are based on similarity transformation which allows us to reduce a Stefan-type problem to a system of nonlinear integral equations whose existence of solution is proved by the fixed point theory in Banach spaces.","sentences":["In this study we develop a mathematical model that describe the behavior of electromagnetic fields and heat transfer in closed electrical contacts that arises when instantaneous explosion of the micro-asperity which involves vaporization zone and liquid, solid zones where temperature is defined by a generalized heat equation with Thomson effect.","This model account for the nonlinear nature of the thermal coefficients and electrical conductivity depended on temperature.","Our proposed solutions are based on similarity transformation which allows us to reduce a Stefan-type problem to a system of nonlinear integral equations whose existence of solution is proved by the fixed point theory in Banach spaces."],"url":"http://arxiv.org/abs/2404.18765v1","category":"math.AP"}
{"created":"2024-04-29 14:56:04","title":"A Gauss curvature flow approach to the p-harmonic measure of Minkowski problem","abstract":"The Minkowski problem of harmonic measures was first studied by Jerison \\cite{JER1991}. Recently, Akman and Mukherjee \\cite{AKM2023} studied the Minkowski problem corresponding to $p$-harmonic measures on convex domains and generalized Jerison's results. In this paper, we obtain the existence of the smooth solution of the $p$-harmonic measure Minkowski problem by method of the Gauss curvature flow.","sentences":["The Minkowski problem of harmonic measures was first studied by Jerison \\cite{JER1991}.","Recently, Akman and Mukherjee \\cite{AKM2023} studied the Minkowski problem corresponding to $p$-harmonic measures on convex domains and generalized Jerison's results.","In this paper, we obtain the existence of the smooth solution of the $p$-harmonic measure Minkowski problem by method of the Gauss curvature flow."],"url":"http://arxiv.org/abs/2404.18757v1","category":"math.AP"}
{"created":"2024-04-29 14:52:11","title":"Spectral measures and iterative bounds for effective diffusivity of steady and space-time periodic flows","abstract":"Over three decades ago the advection-diffusion equation for a steady fluid velocity field was homogenized, leading to a Stieltjes integral representation for the effective diffusivity, which is given in terms of a spectral measure of a compact self-adjoint operator and the P\\'eclet number of the fluid flow. This result was recently extended to space-time periodic flows, instead involving an unbounded self-adjoint operator. Pad\\'e approximants provide rigorous upper and lower bounds for Stieltjes functions in terms of the moments of the spectral measure. However, with the lack of a method for calculating the moments of the spectral measure for general fluid velocity fields, the utility of this powerful mathematical framework for calculating bounds for the effective diffusivity has not been fully realized. Here we significantly expand the applicability of this framework by providing an iterative method that enables an arbitrary number of moments, hence bounds, to be calculated analytically in closed form for both spatially and space-time periodic flows. The method is demonstrated for periodic flows in two spatial dimensions. The known asymptotic behavior of the effective diffusivity for a steady flow is accurately captured by high order upper and lower bounds, demonstrating the ability of the method to provide accurate estimates for the effective diffusivity for a broad range of parameter values.","sentences":["Over three decades ago the advection-diffusion equation for a steady fluid velocity field was homogenized, leading to a Stieltjes integral representation for the effective diffusivity, which is given in terms of a spectral measure of a compact self-adjoint operator and the P\\'eclet number of the fluid flow.","This result was recently extended to space-time periodic flows, instead involving an unbounded self-adjoint operator.","Pad\\'e approximants provide rigorous upper and lower bounds for Stieltjes functions in terms of the moments of the spectral measure.","However, with the lack of a method for calculating the moments of the spectral measure for general fluid velocity fields, the utility of this powerful mathematical framework for calculating bounds for the effective diffusivity has not been fully realized.","Here we significantly expand the applicability of this framework by providing an iterative method that enables an arbitrary number of moments, hence bounds, to be calculated analytically in closed form for both spatially and space-time periodic flows.","The method is demonstrated for periodic flows in two spatial dimensions.","The known asymptotic behavior of the effective diffusivity for a steady flow is accurately captured by high order upper and lower bounds, demonstrating the ability of the method to provide accurate estimates for the effective diffusivity for a broad range of parameter values."],"url":"http://arxiv.org/abs/2404.18754v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 14:44:24","title":"Differential Inclusions Involving the Curl Operator","abstract":"In this article, we study the existence of $\\eta\\in W_0^{1,\\infty}(\\Omega;\\mathbb R^n)$ satisfying $$\\textrm{curl} \\ \\eta\\in E \\textrm{ a.e. in }\\Omega,$$ where $n\\in \\mathbb N, \\Omega\\subseteq \\mathbb R^n$ is open, bounded and $E\\subseteq \\Lambda^2.$","sentences":["In this article, we study the existence of $\\eta\\in W_0^{1,\\infty}(\\Omega;\\mathbb R^n)$ satisfying $$\\textrm{curl} \\ \\eta\\in E \\textrm{ a.e. in }\\Omega,$$ where $n\\in \\mathbb N, \\Omega\\subseteq \\mathbb R^n$ is open, bounded and $E\\subseteq \\Lambda^2.$"],"url":"http://arxiv.org/abs/2404.18744v1","category":"math.AP"}
{"created":"2024-04-29 14:43:15","title":"An unconventional deformation of the nonrelativistic spin-1/2 Fermi gas","abstract":"We explore a generalization of nonrelativistic fermionic statistics that interpolates between bosons and fermions, in which up to $K$ particles may occupy a single-particle state. We show that it can be mapped exactly to $K$ flavors of fermions with imaginary polarization. In particular, for $K\\!=\\!2$, we use such a mapping to derive the virial coefficients and relate them to those of conventional spin-1/2 fermions in an exact fashion. We also use the mapping to derive next-to-leading-order perturbative results for the pressure equation of state. Our results indicate that the $K\\!=\\!2$ particles are more strongly coupled than conventional spin-$1/2$ fermions, as measured by the interaction effects on the virial expansion and on the pressure equation of state. In the regime set by the unitary limit, the proposed $K\\!=\\!2$ deformation represents a universal many-body system whose properties remain largely unknown. In particular, the system can be expected to become superfluid at a critical temperature $T_c$ higher than that of the unitary limit. We suggest it may be possible to realize this system experimentally by engineering a polarized coupling to an electrostatic potential. Finally, we show that the $K\\!=\\!2$ system does not display a sign problem for determinantal Monte Carlo calculations, which indicates that $T_c$ can at least in principle be calculated with conventional methods.","sentences":["We explore a generalization of nonrelativistic fermionic statistics that interpolates between bosons and fermions, in which up to $K$ particles may occupy a single-particle state.","We show that it can be mapped exactly to $K$ flavors of fermions with imaginary polarization.","In particular, for $K\\!=\\!2$, we use such a mapping to derive the virial coefficients and relate them to those of conventional spin-1/2 fermions in an exact fashion.","We also use the mapping to derive next-to-leading-order perturbative results for the pressure equation of state.","Our results indicate that the $K\\!=\\!2$ particles are more strongly coupled than conventional spin-$1/2$ fermions, as measured by the interaction effects on the virial expansion and on the pressure equation of state.","In the regime set by the unitary limit, the proposed $K\\!=\\!2$ deformation represents a universal many-body system whose properties remain largely unknown.","In particular, the system can be expected to become superfluid at a critical temperature $T_c$ higher than that of the unitary limit.","We suggest it may be possible to realize this system experimentally by engineering a polarized coupling to an electrostatic potential.","Finally, we show that the $K\\!=\\!2$ system does not display a sign problem for determinantal Monte Carlo calculations, which indicates that $T_c$ can at least in principle be calculated with conventional methods."],"url":"http://arxiv.org/abs/2404.18743v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 14:36:53","title":"A general framework for active space embedding methods: applications in quantum computing","abstract":"We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding calculations based on an orbital space separation of the fragment and environment degrees of freedom. We show its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion approach are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian. Application of this scheme to study strongly correlated molecular systems and localized electronic states in materials is showcased through the accurate prediction of the optical properties for the neutral oxygen vacancy in magnesium oxide (MgO). Despite some discrepancies in absorption predictions, the method demonstrates competitive performance with state-of-the-art ab initio approaches, particularly evidenced by the accurate prediction of the photoluminescence emission peak.","sentences":["We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding calculations based on an orbital space separation of the fragment and environment degrees of freedom.","We show its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion approach are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian.","Application of this scheme to study strongly correlated molecular systems and localized electronic states in materials is showcased through the accurate prediction of the optical properties for the neutral oxygen vacancy in magnesium oxide (MgO).","Despite some discrepancies in absorption predictions, the method demonstrates competitive performance with state-of-the-art ab initio approaches, particularly evidenced by the accurate prediction of the photoluminescence emission peak."],"url":"http://arxiv.org/abs/2404.18737v1","category":"physics.chem-ph"}
{"created":"2024-04-29 14:25:12","title":"Dynamical friction in the quasi-linear formulation of MOND","abstract":"Aims. We explore the dynamical friction on a test mass in gravitational systems in the Quasi linear formulation of Modified Newtonian Dynamics (QuMOND). Methods. Exploiting the quasi linearity of QuMOND we derive a simple expression for the dynamical friction in akin to its Newtonian counterpart in the standard Chandrasekhar derivation. Moreover, adopting a mean field approach based on the Liouville equation we obtain a more rigorous (though in integral form) dynamical friction formula that can be evaluated numerically for a given choice of the QuMOND interpolation function. Results. Consistently with previous work, we observe that dynamical friction is stronger in MOND with respect to a baryon only Newtonian system with the same mass distribution. This amounts to a correction of the Coulomb logarithmic factor via extra terms proportional to the MOND radius of the system. Moreover, with the aid of simple numerical experiments we confirm our theoretical predictions and those of previous work on MOND.","sentences":["Aims.","We explore the dynamical friction on a test mass in gravitational systems in the Quasi linear formulation of Modified Newtonian Dynamics (QuMOND).","Methods.","Exploiting the quasi linearity of QuMOND we derive a simple expression for the dynamical friction in akin to its Newtonian counterpart in the standard Chandrasekhar derivation.","Moreover, adopting a mean field approach based on the Liouville equation we obtain a more rigorous (though in integral form) dynamical friction formula that can be evaluated numerically for a given choice of the QuMOND interpolation function.","Results.","Consistently with previous work, we observe that dynamical friction is stronger in MOND with respect to a baryon only Newtonian system with the same mass distribution.","This amounts to a correction of the Coulomb logarithmic factor via extra terms proportional to the MOND radius of the system.","Moreover, with the aid of simple numerical experiments we confirm our theoretical predictions and those of previous work on MOND."],"url":"http://arxiv.org/abs/2404.18733v1","category":"astro-ph.GA"}
{"created":"2024-04-29 13:55:42","title":"A geometric approach for stability analysis of delay systems: Applications to network dynamics","abstract":"Investigating the network stability or synchronization dynamics of multi-agent systems with time delays is of significant importance in numerous real-world applications. Such investigations often rely on solving the transcendental characteristic equations (TCEs) obtained from linearization of the considered systems around specific solutions. While stability results based on the TCEs with real-valued coefficients induced by symmetric networks in time-delayed models have been extensively explored in the literature, there remains a notable gap in stability analysis for the TCEs with complexvalued coefficients arising from asymmetric networked dynamics with time delays. To address this challenge comprehensively, we propose a rigorously geometric approach. By identifying and studying the stability crossing curves in the complex plane, we are able to determine the stability region of these systems. This approach is not only suitable for analyzing the stability of models with discrete time delays but also for models with various types of delays, including distributed time delays. Additionally, it can also handle random networks. We demonstrate the efficacy of this approach in designing delayed control strategies for car-following systems, mechanical systems, and deep brain stimulation modeling, where involved are complex-valued TCEs or/and different types of delays. All these therefore highlight the broad applicability of our approach across diverse domains.","sentences":["Investigating the network stability or synchronization dynamics of multi-agent systems with time delays is of significant importance in numerous real-world applications.","Such investigations often rely on solving the transcendental characteristic equations (TCEs) obtained from linearization of the considered systems around specific solutions.","While stability results based on the TCEs with real-valued coefficients induced by symmetric networks in time-delayed models have been extensively explored in the literature, there remains a notable gap in stability analysis for the TCEs with complexvalued coefficients arising from asymmetric networked dynamics with time delays.","To address this challenge comprehensively, we propose a rigorously geometric approach.","By identifying and studying the stability crossing curves in the complex plane, we are able to determine the stability region of these systems.","This approach is not only suitable for analyzing the stability of models with discrete time delays but also for models with various types of delays, including distributed time delays.","Additionally, it can also handle random networks.","We demonstrate the efficacy of this approach in designing delayed control strategies for car-following systems, mechanical systems, and deep brain stimulation modeling, where involved are complex-valued TCEs or/and different types of delays.","All these therefore highlight the broad applicability of our approach across diverse domains."],"url":"http://arxiv.org/abs/2404.18704v1","category":"math.DS"}
{"created":"2024-04-29 13:46:12","title":"Investigation of shallow water waves near the coast or in lake environments via the KdV-Calogero-Bogoyavlenskii-Schiff equation","abstract":"Shallow water waves phenomena in nature attract the attention of scholars and play an important role in fields such as tsunamis, tidal waves, solitary waves, and hydraulic engineering. Hereby, for the shallow water waves phenomena in various natural environments, we study the KdV-Calogero-Bogoyavlenskii-Schiff (KdV-CBS) equation. Based on the Bell polynomial theory, the B{\\\"a}cklund transformation, Lax pair and infinite conservation laws of the KdV-CBS equation are derived, and it is proved that it is completely integrable in Lax pair sense. Various types of mixed solutions are constructed by using a combination of Homoclinic test method and Mathematica symbolic computations. These findings have important significance for the discipline, offering vital insights into the intricate dynamics of the KdV-CBS equation. We hope that our research results could help the researchers understand the nonlinear complex phenomena of the shallow water waves in oceans, rivers and coastal areas.","sentences":["Shallow water waves phenomena in nature attract the attention of scholars and play an important role in fields such as tsunamis, tidal waves, solitary waves, and hydraulic engineering.","Hereby, for the shallow water waves phenomena in various natural environments, we study the KdV-Calogero-Bogoyavlenskii-Schiff (KdV-CBS) equation.","Based on the Bell polynomial theory, the B{\\\"a}cklund transformation, Lax pair and infinite conservation laws of the KdV-CBS equation are derived, and it is proved that it is completely integrable in Lax pair sense.","Various types of mixed solutions are constructed by using a combination of Homoclinic test method and Mathematica symbolic computations.","These findings have important significance for the discipline, offering vital insights into the intricate dynamics of the KdV-CBS equation.","We hope that our research results could help the researchers understand the nonlinear complex phenomena of the shallow water waves in oceans, rivers and coastal areas."],"url":"http://arxiv.org/abs/2404.18697v1","category":"nlin.SI"}
{"created":"2024-04-29 13:41:35","title":"Private graph colouring with limited defectiveness","abstract":"Differential privacy is the gold standard in the problem of privacy preserving data analysis, which is crucial in a wide range of disciplines. Vertex colouring is one of the most fundamental questions about a graph. In this paper, we study the vertex colouring problem in the differentially private setting.   To be edge-differentially private, a colouring algorithm needs to be defective: a colouring is d-defective if a vertex can share a colour with at most d of its neighbours. Without defectiveness, the only differentially private colouring algorithm needs to assign n different colours to the n different vertices. We show the following lower bound for the defectiveness: a differentially private c-edge colouring algorithm of a graph of maximum degree {\\Delta} > 0 has defectiveness at least d = {\\Omega} (log n / (log c+log {\\Delta})).   We also present an {\\epsilon}-differentially private algorithm to {\\Theta} ( {\\Delta} / log n + 1 / {\\epsilon})-colour a graph with defectiveness at most {\\Theta}(log n).","sentences":["Differential privacy is the gold standard in the problem of privacy preserving data analysis, which is crucial in a wide range of disciplines.","Vertex colouring is one of the most fundamental questions about a graph.","In this paper, we study the vertex colouring problem in the differentially private setting.   ","To be edge-differentially private, a colouring algorithm needs to be defective: a colouring is d-defective if a vertex can share a colour with at most d of its neighbours.","Without defectiveness, the only differentially private colouring algorithm needs to assign n different colours to the n different vertices.","We show the following lower bound for the defectiveness: a differentially private c-edge colouring algorithm of a graph of maximum degree {\\Delta} > 0 has defectiveness at least d = {\\Omega} (log n / (log c+log {\\Delta})).   ","We also present an {\\epsilon}-differentially private algorithm to {\\Theta} ( {\\Delta} / log n + 1 / {\\epsilon})-colour a graph with defectiveness at most {\\Theta}(log n)."],"url":"http://arxiv.org/abs/2404.18692v1","category":"cs.DS"}
{"created":"2024-04-29 17:59:11","title":"Point Cloud Models Improve Visual Robustness in Robotic Learners","abstract":"Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Project Webpage: https://pvskand.github.io/projects/PCWM","sentences":["Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences.","In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies.","To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies.","Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts.","Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training.","Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners.","Project Webpage: https://pvskand.github.io/projects/PCWM"],"url":"http://arxiv.org/abs/2404.18926v1","category":"cs.RO"}
{"created":"2024-04-29 17:30:36","title":"Learning general Gaussian mixtures with efficient score matching","abstract":"We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions. We make no separation assumptions on the underlying mixture components: we only require that the covariance matrices have bounded condition number and that the means and covariances lie in a ball of bounded radius. We give an algorithm that draws $d^{\\mathrm{poly}(k/\\varepsilon)}$ samples from the target mixture, runs in sample-polynomial time, and constructs a sampler whose output distribution is $\\varepsilon$-far from the unknown mixture in total variation. Prior works for this problem either (i) required exponential runtime in the dimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical covariances or clusterability), or (iii) had doubly exponential dependence on the number of components $k$.   Our approach departs from commonly used techniques for this problem like the method of moments. Instead, we leverage a recently developed reduction, based on diffusion models, from distribution learning to a supervised learning task called score matching. We give an algorithm for the latter by proving a structural result showing that the score function of a Gaussian mixture can be approximated by a piecewise-polynomial function, and there is an efficient algorithm for finding it. To our knowledge, this is the first example of diffusion models achieving a state-of-the-art theoretical guarantee for an unsupervised learning task.","sentences":["We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions.","We make no separation assumptions on the underlying mixture components: we only require that the covariance matrices have bounded condition number and that the means and covariances lie in a ball of bounded radius.","We give an algorithm that draws $d^{\\mathrm{poly}(k/\\varepsilon)}$ samples from the target mixture, runs in sample-polynomial time, and constructs a sampler whose output distribution is $\\varepsilon$-far from the unknown mixture in total variation.","Prior works for this problem either (i) required exponential runtime in the dimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical covariances or clusterability), or (iii) had doubly exponential dependence on the number of components $k$.   Our approach departs from commonly used techniques for this problem like the method of moments.","Instead, we leverage a recently developed reduction, based on diffusion models, from distribution learning to a supervised learning task called score matching.","We give an algorithm for the latter by proving a structural result showing that the score function of a Gaussian mixture can be approximated by a piecewise-polynomial function, and there is an efficient algorithm for finding it.","To our knowledge, this is the first example of diffusion models achieving a state-of-the-art theoretical guarantee for an unsupervised learning task."],"url":"http://arxiv.org/abs/2404.18893v1","category":"cs.DS"}
{"created":"2024-04-29 17:16:27","title":"Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking","abstract":"Data augmentation techniques apply transformations to existing texts to generate additional data. The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension. Analyzing the synthetically generated texts and their corresponding labels is slow and demanding. To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling. INSPECTOR allows users to group related texts by their transformation provenance, i.e., the transformations applied to the original text, or feature provenance, the linguistic features of the original text. For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model. In a user study, INSPECTOR increases the number of texts with correct labels identified by 3X on a sentiment analysis task and by 4X on a hate speech detection task. The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique. Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful. Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort. This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort.","sentences":["Data augmentation techniques apply transformations to existing texts to generate additional data.","The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension.","Analyzing the synthetically generated texts and their corresponding labels is slow and demanding.","To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique.","INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling.","INSPECTOR allows users to group related texts by their transformation provenance, i.e., the transformations applied to the original text, or feature provenance, the linguistic features of the original text.","For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model.","In a user study, INSPECTOR increases the number of texts with correct labels identified by 3X on a sentiment analysis task and by 4X on a hate speech detection task.","The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique.","Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful.","Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort.","This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort."],"url":"http://arxiv.org/abs/2404.18881v1","category":"cs.HC"}
{"created":"2024-04-29 17:00:20","title":"Learning Mixtures of Gaussians Using Diffusion Models","abstract":"We give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial ($O(n^{\\text{poly log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample complexity, under a minimum weight assumption. Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models. Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution. Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees. We proceed by deriving higher-order Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to poly-logarithmic degree), and combine this with known convergence results for diffusion models. Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius. In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number.","sentences":["We give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial ($O(n^{\\text{poly log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample complexity, under a minimum weight assumption.","Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models.","Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution.","Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees.","We proceed by deriving higher-order Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to poly-logarithmic degree), and combine this with known convergence results for diffusion models.","Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius.","In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number."],"url":"http://arxiv.org/abs/2404.18869v1","category":"cs.LG"}
{"created":"2024-04-29 16:49:44","title":"VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter","abstract":"Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.","sentences":["Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions.","However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring.","Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility.","To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation.","We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality.","Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations."],"url":"http://arxiv.org/abs/2404.18857v1","category":"stat.ME"}
{"created":"2024-04-29 16:45:03","title":"VERT: Verified Equivalent Rust Transpilation with Few-Shot Learning","abstract":"Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.","sentences":["Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default.","Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust.","Existing work falls into two categories: rule-based and large language model (LLM)-based.","While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language.","On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness.","In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness.","VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages.","VERT first uses the Web Assembly compiler to obtain an oracle Rust program.","In parallel, VERT uses an LLM to generate a readable candidate Rust program.","This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds.","We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks.","Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone.","In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers.","Our results provide insights into the limitations of LLMs to write safe Rust."],"url":"http://arxiv.org/abs/2404.18852v1","category":"cs.PL"}
{"created":"2024-04-29 16:44:27","title":"A Comprehensive Rubric for Annotating Pathological Speech","abstract":"Rubrics are a commonly used tool for labeling voice corpora in speech quality assessment, although their application in the context of pathological speech remains relatively limited. In this study, we introduce a comprehensive rubric based on various dimensions of speech quality, including phonetics, fluency, and prosody. The objective is to establish standardized criteria for identifying errors within the speech of individuals with Down syndrome, thereby enabling the development of automated assessment systems. To achieve this objective, we utilized the Prautocal corpus. To assess the quality of annotations using our rubric, two experiments were conducted, focusing on phonetics and fluency. For phonetic evaluation, we employed the Goodness of Pronunciation (GoP) metric, utilizing automatic segmentation systems and correlating the results with evaluations conducted by a specialized speech therapist. While the obtained correlation values were not notably high, a positive trend was observed. In terms of fluency assessment, deep learning models like wav2vec were used to extract audio features, and we employed an SVM classifier trained on a corpus focused on identifying fluency issues to categorize Prautocal corpus samples. The outcomes highlight the complexities of evaluating such phenomena, with variability depending on the specific type of disfluency detected.","sentences":["Rubrics are a commonly used tool for labeling voice corpora in speech quality assessment, although their application in the context of pathological speech remains relatively limited.","In this study, we introduce a comprehensive rubric based on various dimensions of speech quality, including phonetics, fluency, and prosody.","The objective is to establish standardized criteria for identifying errors within the speech of individuals with Down syndrome, thereby enabling the development of automated assessment systems.","To achieve this objective, we utilized the Prautocal corpus.","To assess the quality of annotations using our rubric, two experiments were conducted, focusing on phonetics and fluency.","For phonetic evaluation, we employed the Goodness of Pronunciation (GoP) metric, utilizing automatic segmentation systems and correlating the results with evaluations conducted by a specialized speech therapist.","While the obtained correlation values were not notably high, a positive trend was observed.","In terms of fluency assessment, deep learning models like wav2vec were used to extract audio features, and we employed an SVM classifier trained on a corpus focused on identifying fluency issues to categorize Prautocal corpus samples.","The outcomes highlight the complexities of evaluating such phenomena, with variability depending on the specific type of disfluency detected."],"url":"http://arxiv.org/abs/2404.18851v1","category":"cs.CL"}
{"created":"2024-04-29 17:55:40","title":"Colloidal dispersions of sterically and electrostatically stabilized PbS quantum dots: the effect of stabilization mechanism on structure factors, second virial coefficients, and film-forming properties","abstract":"Electrostatically stabilized nanocrystals (NCs) and, in particular, quantum dots (QDs) hold promise for forming strongly coupled superlattices due to their compact and electronically conductive surface ligands. However, studies on the colloidal dispersion and interparticle interactions of electrostatically stabilized sub-10 nm NCs have been limited, hindering the optimization of colloidal stability and self-assembly. In this study, we employed small-angle X ray scattering (SAXS) experiments to investigate the interparticle interactions and arrangement of PbS QDs with thiostannate ligands (PbS-Sn2S64-) in polar solvents. The study reveals significant deviations from ideal solution behavior in electrostatically stabilized QD dispersions. Our results demonstrate that PbS-Sn2S64- QDs exhibit long-range interactions within the solvent, in contrast to the short-range steric repulsion characteristic of PbS QDs with oleate ligands (PbS-OA). Introducing highly charged multivalent electrolytes screens electrostatic interactions between charged QDs, reducing the length scale of the repulsive interactions. Furthermore, we calculate the second virial (B2) coefficients from SAXS data, providing insights into how surface chemistry, solvent, and size influence pair potentials. Finally, we explore the influence of long-range interparticle interactions of PbS-Sn2S64- QDs on the morphology of films produced by drying or spin-coating colloidal solutions. The long-range repulsive term of PbS-Sn2S64- QDs promotes the formation of amorphous films, and screening the electrostatic repulsion by addition of an electrolyte enables the formation of a crystalline film. These findings highlight the critical role of NC-NC interactions in tailoring the properties of functional nanomaterials.","sentences":["Electrostatically stabilized nanocrystals (NCs) and, in particular, quantum dots (QDs) hold promise for forming strongly coupled superlattices due to their compact and electronically conductive surface ligands.","However, studies on the colloidal dispersion and interparticle interactions of electrostatically stabilized sub-10 nm NCs have been limited, hindering the optimization of colloidal stability and self-assembly.","In this study, we employed small-angle X ray scattering (SAXS) experiments to investigate the interparticle interactions and arrangement of PbS QDs with thiostannate ligands (PbS-Sn2S64-) in polar solvents.","The study reveals significant deviations from ideal solution behavior in electrostatically stabilized QD dispersions.","Our results demonstrate that PbS-Sn2S64- QDs exhibit long-range interactions within the solvent, in contrast to the short-range steric repulsion characteristic of PbS QDs with oleate ligands (PbS-OA).","Introducing highly charged multivalent electrolytes screens electrostatic interactions between charged QDs, reducing the length scale of the repulsive interactions.","Furthermore, we calculate the second virial (B2) coefficients from SAXS data, providing insights into how surface chemistry, solvent, and size influence pair potentials.","Finally, we explore the influence of long-range interparticle interactions of PbS-Sn2S64- QDs on the morphology of films produced by drying or spin-coating colloidal solutions.","The long-range repulsive term of PbS-Sn2S64- QDs promotes the formation of amorphous films, and screening the electrostatic repulsion by addition of an electrolyte enables the formation of a crystalline film.","These findings highlight the critical role of NC-NC interactions in tailoring the properties of functional nanomaterials."],"url":"http://arxiv.org/abs/2404.18915v1","category":"cond-mat.soft"}
{"created":"2024-04-29 17:35:05","title":"High-performance gate-controlled superconducting switches: large output voltage and reproducibility","abstract":"Logic circuits consist of devices that can be controlled between two distinct states. The recent demonstration that a superconducting current flowing in a constriction can be controlled via a gate voltage ($V_G$) - can lead to superconducting logic with better performance than existing logics. However, before such logic is developed, high reproducibility in the functioning of GCS devices and optimization of their performance must be achieved. Here, we report an investigation of gated Nb devices showing GCS with unprecedently-high reproducibility. Based on the investigation of a statistically-significant number of devices, we demonstrate that the GCS is independent of the constriction width, in contrast with previous reports, and confirm a strong correlation between the GCS and the leakage current ($I_{leak}$) induced by $V_G$. We also achieve a voltage output in our devices larger than the typical values reported to date by at least one order of magnitude, which is relevant for the future interconnection of devices, and show that $I_{leak}$ can be used as a tool to modulate the operational $V_G$ of devices on a $SiO_2$ substrates. These results altogether represent an important step forward towards the optimization of reproducibility and performance of GCS devices, and the future development of a GCS-based logic.","sentences":["Logic circuits consist of devices that can be controlled between two distinct states.","The recent demonstration that a superconducting current flowing in a constriction can be controlled via a gate voltage ($V_G$) - can lead to superconducting logic with better performance than existing logics.","However, before such logic is developed, high reproducibility in the functioning of GCS devices and optimization of their performance must be achieved.","Here, we report an investigation of gated Nb devices showing GCS with unprecedently-high reproducibility.","Based on the investigation of a statistically-significant number of devices, we demonstrate that the GCS is independent of the constriction width, in contrast with previous reports, and confirm a strong correlation between the GCS and the leakage current ($I_{leak}$) induced by $V_G$. We also achieve a voltage output in our devices larger than the typical values reported to date by at least one order of magnitude, which is relevant for the future interconnection of devices, and show that $I_{leak}$ can be used as a tool to modulate the operational $V_G$ of devices on a $SiO_2$ substrates.","These results altogether represent an important step forward towards the optimization of reproducibility and performance of GCS devices, and the future development of a GCS-based logic."],"url":"http://arxiv.org/abs/2404.18899v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 17:19:24","title":"Reputation in Repeated Global Games of Regime Change with Exit","abstract":"I study a repeated binary-action supermodular game with endogenous exit where many short-lived agents attempt to coordinate a revolt against a regime. The regime undertakes costly actions to increase the short-run players' coordination frictions, though acts only after if the revolt is unsuccessful, inducing a lack-of-commitment problem. In the complete-information repeated game, a folk theorem holds, with payoff multiplicity arising due to both the regime's dynamic incentives and agents' stage-game strategic complementarities. Neither the regime's reputational incentives nor belief dispersion among agents (via global-games type uncertainty) alone meaningfully refine the equilibrium payoff set. Together, though, the interaction between these two forces uniquely select the regime's highest payoff in equilibrium. Furthermore, under a Markov refinement, they select a unique equilibrium where the regime plays their optimal commitment action. Methodologically, I develop tools to analyze repeated games with endogenous exit where the regime's commitment action flexibly varies with their discount rate.","sentences":["I study a repeated binary-action supermodular game with endogenous exit where many short-lived agents attempt to coordinate a revolt against a regime.","The regime undertakes costly actions to increase the short-run players' coordination frictions, though acts only after if the revolt is unsuccessful, inducing a lack-of-commitment problem.","In the complete-information repeated game, a folk theorem holds, with payoff multiplicity arising due to both the regime's dynamic incentives and agents' stage-game strategic complementarities.","Neither the regime's reputational incentives nor belief dispersion among agents (via global-games type uncertainty) alone meaningfully refine the equilibrium payoff set.","Together, though, the interaction between these two forces uniquely select the regime's highest payoff in equilibrium.","Furthermore, under a Markov refinement, they select a unique equilibrium where the regime plays their optimal commitment action.","Methodologically, I develop tools to analyze repeated games with endogenous exit where the regime's commitment action flexibly varies with their discount rate."],"url":"http://arxiv.org/abs/2404.18884v1","category":"econ.TH"}
{"created":"2024-04-29 17:10:41","title":"A Multilevel Strategy to Improve People Tracking in a Real-World Scenario","abstract":"The Pal\\'acio do Planalto, office of the President of Brazil, was invaded by protesters on January 8, 2023. Surveillance videos taken from inside the building were subsequently released by the Brazilian Supreme Court for public scrutiny. We used segments of such footage to create the UFPR-Planalto801 dataset for people tracking and re-identification in a real-world scenario. This dataset consists of more than 500,000 images. This paper presents a tracking approach targeting this dataset. The method proposed in this paper relies on the use of known state-of-the-art trackers combined in a multilevel hierarchy to correct the ID association over the trajectories. We evaluated our method using IDF1, MOTA, MOTP and HOTA metrics. The results show improvements for every tracker used in the experiments, with IDF1 score increasing by a margin up to 9.5%.","sentences":["The Pal\\'acio do Planalto, office of the President of Brazil, was invaded by protesters on January 8, 2023.","Surveillance videos taken from inside the building were subsequently released by the Brazilian Supreme Court for public scrutiny.","We used segments of such footage to create the UFPR-Planalto801 dataset for people tracking and re-identification in a real-world scenario.","This dataset consists of more than 500,000 images.","This paper presents a tracking approach targeting this dataset.","The method proposed in this paper relies on the use of known state-of-the-art trackers combined in a multilevel hierarchy to correct the ID association over the trajectories.","We evaluated our method using IDF1, MOTA, MOTP and HOTA metrics.","The results show improvements for every tracker used in the experiments, with IDF1 score increasing by a margin up to 9.5%."],"url":"http://arxiv.org/abs/2404.18876v1","category":"cs.CV"}
{"created":"2024-04-29 16:57:42","title":"Optimization of District Heating Network Parameters in Steady-State Operation","abstract":"We examine the modeling, simulation, and optimization of district heating systems, which are widely used for thermal transport using steam or hot water as a carrier. We propose a generalizable framework to specify network models and scenario parameters, and develop an optimization method for evaluating system states including pressures, fluid flow rates, and temperatures throughout the network. The network modeling includes pipes, thermal plants, pumps, and passive or controllable loads as system components. We propose basic models for thermodynamic fluid transport and enforce the balance of physical quantities in steady-state flow over co-located outgoing and return networks. We formulate an optimization problem with steam and hot water as the outgoing and return carriers, as in legacy 20th century systems. The physical laws and engineering limitations are specified for each component type, and the thermal network flow optimization (TNFO) problem is formulated and solved for a realistic test network under several scenarios.","sentences":["We examine the modeling, simulation, and optimization of district heating systems, which are widely used for thermal transport using steam or hot water as a carrier.","We propose a generalizable framework to specify network models and scenario parameters, and develop an optimization method for evaluating system states including pressures, fluid flow rates, and temperatures throughout the network.","The network modeling includes pipes, thermal plants, pumps, and passive or controllable loads as system components.","We propose basic models for thermodynamic fluid transport and enforce the balance of physical quantities in steady-state flow over co-located outgoing and return networks.","We formulate an optimization problem with steam and hot water as the outgoing and return carriers, as in legacy 20th century systems.","The physical laws and engineering limitations are specified for each component type, and the thermal network flow optimization (TNFO) problem is formulated and solved for a realistic test network under several scenarios."],"url":"http://arxiv.org/abs/2404.18868v1","category":"math.OC"}
{"created":"2024-04-29 16:28:14","title":"Fast Quantum Process Tomography via Riemannian Gradient Descent","abstract":"Constrained optimization plays a crucial role in the fields of quantum physics and quantum information science and becomes especially challenging for high-dimensional complex structure problems. One specific issue is that of quantum process tomography, in which the goal is to retrieve the underlying quantum process based on a given set of measurement data. In this paper, we introduce a modified version of stochastic gradient descent on a Riemannian manifold that integrates recent advancements in numerical methods for Riemannian optimization. This approach inherently supports the physically driven constraints of a quantum process, takes advantage of state-of-the-art large-scale stochastic objective optimization, and has superior performance to traditional approaches such as maximum likelihood estimation and projected least squares. The data-driven approach enables accurate, order-of-magnitude faster results, and works with incomplete data. We demonstrate our approach on simulations of quantum processes and in hardware by characterizing an engineered process on quantum computers.","sentences":["Constrained optimization plays a crucial role in the fields of quantum physics and quantum information science and becomes especially challenging for high-dimensional complex structure problems.","One specific issue is that of quantum process tomography, in which the goal is to retrieve the underlying quantum process based on a given set of measurement data.","In this paper, we introduce a modified version of stochastic gradient descent on a Riemannian manifold that integrates recent advancements in numerical methods for Riemannian optimization.","This approach inherently supports the physically driven constraints of a quantum process, takes advantage of state-of-the-art large-scale stochastic objective optimization, and has superior performance to traditional approaches such as maximum likelihood estimation and projected least squares.","The data-driven approach enables accurate, order-of-magnitude faster results, and works with incomplete data.","We demonstrate our approach on simulations of quantum processes and in hardware by characterizing an engineered process on quantum computers."],"url":"http://arxiv.org/abs/2404.18840v1","category":"quant-ph"}
{"created":"2024-04-29 16:21:59","title":"Interpolating between Optimal Transport and KL regularized Optimal Transport using R\u00e9nyi Divergences","abstract":"Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's paper with Kullback-Leibler (KL) divergence regularized OT. In this paper, we propose to regularize the OT problem using the family of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\\alpha \\nearrow 1$. The advantage of introducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$ we obtain convergence to the unregularized OT problem. For the KL regularized OT problem, this was achieved by letting the regularization parameter tend to zero, which causes numerical instabilities. We present two different ways to obtain premetrics on probability measures, namely by R\\'enyi divergence constraints and penalization. The latter premetric interpolates between the unregularized and KL regularized OT problem with weak convergence of the minimizer, generalizing the interpolating property of KL regularized OT. We use a nested mirror descent algorithm for solving the primal formulation. Both on real and synthetic data sets R\\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better.","sentences":["Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's paper with Kullback-Leibler (KL) divergence regularized OT.","In this paper, we propose to regularize the OT problem using the family of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\\alpha \\nearrow 1$.","The advantage of introducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$ we obtain convergence to the unregularized OT problem.","For the KL regularized OT problem, this was achieved by letting the regularization parameter tend to zero, which causes numerical instabilities.","We present two different ways to obtain premetrics on probability measures, namely by R\\'enyi divergence constraints and penalization.","The latter premetric interpolates between the unregularized and KL regularized OT problem with weak convergence of the minimizer, generalizing the interpolating property of KL regularized OT.","We use a nested mirror descent algorithm for solving the primal formulation.","Both on real and synthetic data sets R\\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better."],"url":"http://arxiv.org/abs/2404.18834v1","category":"math.OC"}
{"created":"2024-04-29 16:11:49","title":"Extreme fragmentation of a Bose gas","abstract":"Fragmentation of an interacting Bose gas refers to the macroscopic occupation of a finite set of single-particle eigenstates. This phenomenon is related to the notion of particle-number squeezing in quantum optics, an exquisite property of quantum states that can offer metrological gain. So far, fragmentation has only been partially achieved in experiments involving a large number $N$ of bosons in few modes. Here, we introduce a practical and efficient scheme to prepare fragmented states in systems realizing the $L$-mode Bose-Hubbard model. We demonstrate how a large energy detuning between the modes can be used as a practical control parameter to successfully fragment a Bose gas over an extremely short preparation time. Applying an optimal-control approach within realistic experimental constraints, we obtain total fragmentation at a high filling factor, realizing $\\ket{N/L,...,N/L}$ Fock states with hundreds of bosons in very few modes over a few tunneling times.","sentences":["Fragmentation of an interacting Bose gas refers to the macroscopic occupation of a finite set of single-particle eigenstates.","This phenomenon is related to the notion of particle-number squeezing in quantum optics, an exquisite property of quantum states that can offer metrological gain.","So far, fragmentation has only been partially achieved in experiments involving a large number $N$ of bosons in few modes.","Here, we introduce a practical and efficient scheme to prepare fragmented states in systems realizing the $L$-mode Bose-Hubbard model.","We demonstrate how a large energy detuning between the modes can be used as a practical control parameter to successfully fragment a Bose gas over an extremely short preparation time.","Applying an optimal-control approach within realistic experimental constraints, we obtain total fragmentation at a high filling factor, realizing $\\ket{N/L,...,N/L}$ Fock states with hundreds of bosons in very few modes over a few tunneling times."],"url":"http://arxiv.org/abs/2404.18827v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 16:04:25","title":"A Multi-Period Black-Litterman Model","abstract":"The Black-Litterman model is a framework for incorporating forward-looking expert views in a portfolio optimization problem. Existing work focuses almost exclusively on single-period problems and assumes that the horizon of expert forecasts matches that of the investor. We consider a multi-period generalization where the horizon of expert views may differ from that of a dynamically-trading investor. By exploiting an underlying graphical structure relating the asset prices and views, we derive the conditional distribution of asset returns when the price process is geometric Brownian motion. We also show that it can be written in terms of a multi-dimensional Brownian bridge. The new price process is an affine factor model with the conditional log-price process playing the role of a vector of factors. We derive an explicit expression for the optimal dynamic investment policy and analyze the hedging demand associated with the new covariate. More generally, the paper shows that Bayesian graphical models are a natural framework for incorporating complex information structures in the Black-Litterman model.","sentences":["The Black-Litterman model is a framework for incorporating forward-looking expert views in a portfolio optimization problem.","Existing work focuses almost exclusively on single-period problems and assumes that the horizon of expert forecasts matches that of the investor.","We consider a multi-period generalization where the horizon of expert views may differ from that of a dynamically-trading investor.","By exploiting an underlying graphical structure relating the asset prices and views, we derive the conditional distribution of asset returns when the price process is geometric Brownian motion.","We also show that it can be written in terms of a multi-dimensional Brownian bridge.","The new price process is an affine factor model with the conditional log-price process playing the role of a vector of factors.","We derive an explicit expression for the optimal dynamic investment policy and analyze the hedging demand associated with the new covariate.","More generally, the paper shows that Bayesian graphical models are a natural framework for incorporating complex information structures in the Black-Litterman model."],"url":"http://arxiv.org/abs/2404.18822v1","category":"q-fin.PM"}
{"created":"2024-04-29 15:45:50","title":"Architecture for fast implementation of qLDPC codes with optimized Rydberg gates","abstract":"We propose an implementation of bivariate bicycle codes (Nature {\\bf 627}, 778 (2024)) based on long-range Rydberg gates between stationary neutral atom qubits. An optimized layout of data and ancilla qubits reduces the maximum Euclidean communication distance needed for non-local parity check operators. An optimized Rydberg gate pulse design enables $\\sf CZ$ entangling operations with fidelity ${\\mathcal F}>0.999$ at a distance greater than $12~\\mu\\rm m$. The combination of optimized layout and gate design leads to a quantum error correction cycle time of $\\sim 1.2~\\rm ms$ for a $[[144,12,12]]$ code, an order of magnitude improvement over previous designs.","sentences":["We propose an implementation of bivariate bicycle codes (Nature {\\bf 627}, 778 (2024)) based on long-range Rydberg gates between stationary neutral atom qubits.","An optimized layout of data and ancilla qubits reduces the maximum Euclidean communication distance needed for non-local parity check operators.","An optimized Rydberg gate pulse design enables $\\sf CZ$ entangling operations with fidelity ${\\mathcal F}>0.999$ at a distance greater than $12~\\mu\\rm m$.","The combination of optimized layout and gate design leads to a quantum error correction cycle time of $\\sim 1.2~\\rm ms$ for a $[[144,12,12]]$ code, an order of magnitude improvement over previous designs."],"url":"http://arxiv.org/abs/2404.18809v1","category":"quant-ph"}
{"created":"2024-04-29 15:27:50","title":"Optimality and uniqueness of the D_4 root system","abstract":"We prove that the $D_4$ root system (the set of vertices of the regular $24$-cell) is the unique optimal kissing configuration in $\\mathbb R^4$, and is an optimal spherical code. For this, we use semidefinite programming to compute an exact optimal solution to the second level of the Lasserre hierarchy. We also improve the upper bound for the kissing number problem in $\\mathbb R^6$ to $77$.","sentences":["We prove that the $D_4$ root system (the set of vertices of the regular $24$-cell) is the unique optimal kissing configuration in $\\mathbb R^4$, and is an optimal spherical code.","For this, we use semidefinite programming to compute an exact optimal solution to the second level of the Lasserre hierarchy.","We also improve the upper bound for the kissing number problem in $\\mathbb R^6$ to $77$."],"url":"http://arxiv.org/abs/2404.18794v1","category":"math.MG"}
{"created":"2024-04-29 15:01:30","title":"Enhanced and Robust Contrast in CEST MRI: Saturation Pulse Shape Design via Optimal Control","abstract":"Purpose: To employ optimal control for the numerical design of CEST saturation pulses to maximize contrast and stability against $B_0$ inhomogeneities.   Theory and Methods: We applied an optimal control framework for the design pulse shapes for CEST saturation pulse trains. The cost functional minimized both the pulse energy and the discrepancy between the corresponding CEST spectrum and the target spectrum based on a continuous RF pulse. The optimization is subject to hardware limitations. In measurements on a 7 T preclinical scanner, the optimal control pulses were compared to continuous-wave and Gaussian saturation methods. We conducted a comparison of the optimal control pulses were compared to with Gaussian, block pulse trains, and adiabatic spin-lock pulses.   Results: The optimal control pulse train demonstrated saturation levels comparable to continuous-wave saturation and surpassed Gaussian saturation by up to 50 \\% in phantom measurements. In phantom measurements at 3 T the optimized pulses not only showcased the highest CEST contrast, but also the highest stability against field inhomogeneities. In contrast, block pulse saturation resulted in severe artifacts. Dynamic Bloch-McConnell simulations were employed to identify the source of these artifacts, and underscore the $B_0$ robustness of the optimized pulses.   Conclusion: In this work, it was shown that a substantial improvement in pulsed saturation CEST imaging can be achieved by using Optimal Control design principles. It is possible to overcome the sensitivity of saturation to B0 inhomogeneities while achieving CEST contrast close to continuous wave saturation.","sentences":["Purpose: To employ optimal control for the numerical design of CEST saturation pulses to maximize contrast and stability against $B_0$ inhomogeneities.   ","Theory and Methods: We applied an optimal control framework for the design pulse shapes for CEST saturation pulse trains.","The cost functional minimized both the pulse energy and the discrepancy between the corresponding CEST spectrum and the target spectrum based on a continuous RF pulse.","The optimization is subject to hardware limitations.","In measurements on a 7 T preclinical scanner, the optimal control pulses were compared to continuous-wave and Gaussian saturation methods.","We conducted a comparison of the optimal control pulses were compared to with Gaussian, block pulse trains, and adiabatic spin-lock pulses.   ","Results:","The optimal control pulse train demonstrated saturation levels comparable to continuous-wave saturation and surpassed Gaussian saturation by up to 50 \\% in phantom measurements.","In phantom measurements at 3 T the optimized pulses not only showcased the highest CEST contrast, but also the highest stability against field inhomogeneities.","In contrast, block pulse saturation resulted in severe artifacts.","Dynamic Bloch-McConnell simulations were employed to identify the source of these artifacts, and underscore the $B_0$ robustness of the optimized pulses.   ","Conclusion: In this work, it was shown that a substantial improvement in pulsed saturation CEST imaging can be achieved by using Optimal Control design principles.","It is possible to overcome the sensitivity of saturation to B0 inhomogeneities while achieving CEST contrast close to continuous wave saturation."],"url":"http://arxiv.org/abs/2404.18764v1","category":"physics.med-ph"}
{"created":"2024-04-29 15:01:09","title":"From Density to Geometry: YOLOv8 Instance Segmentation for Reverse Engineering of Optimized Structures","abstract":"This paper introduces YOLOv8-TO, a novel approach for reverse engineering of topology-optimized structures into interpretable geometric parameters using the YOLOv8 instance segmentation model. Density-based topology optimization methods require post-processing to convert the optimal density distribution into a parametric representation for design exploration and integration with CAD tools. Traditional methods such as skeletonization struggle with complex geometries and require manual intervention. YOLOv8-TO addresses these challenges by training a custom YOLOv8 model to automatically detect and reconstruct structural components from binary density distributions. The model is trained on a diverse dataset of both optimized and random structures generated using the Moving Morphable Components method. A custom reconstruction loss function based on the dice coefficient of the predicted geometry is used to train the new regression head of the model via self-supervised learning. The method is evaluated on test sets generated from different topology optimization methods, including out-of-distribution samples, and compared against a skeletonization approach. Results show that YOLOv8-TO significantly outperforms skeletonization in reconstructing visually and structurally similar designs. The method showcases an average improvement of 13.84% in the Dice coefficient, with peak enhancements reaching 20.78%. The method demonstrates good generalization to complex geometries and fast inference times, making it suitable for integration into design workflows using regular workstations. Limitations include the sensitivity to non-max suppression thresholds. YOLOv8-TO represents a significant advancement in topology optimization post-processing, enabling efficient and accurate reverse engineering of optimized structures for design exploration and manufacturing.","sentences":["This paper introduces YOLOv8-TO, a novel approach for reverse engineering of topology-optimized structures into interpretable geometric parameters using the YOLOv8 instance segmentation model.","Density-based topology optimization methods require post-processing to convert the optimal density distribution into a parametric representation for design exploration and integration with CAD tools.","Traditional methods such as skeletonization struggle with complex geometries and require manual intervention.","YOLOv8-TO addresses these challenges by training a custom YOLOv8 model to automatically detect and reconstruct structural components from binary density distributions.","The model is trained on a diverse dataset of both optimized and random structures generated using the Moving Morphable Components method.","A custom reconstruction loss function based on the dice coefficient of the predicted geometry is used to train the new regression head of the model via self-supervised learning.","The method is evaluated on test sets generated from different topology optimization methods, including out-of-distribution samples, and compared against a skeletonization approach.","Results show that YOLOv8-TO significantly outperforms skeletonization in reconstructing visually and structurally similar designs.","The method showcases an average improvement of 13.84% in the Dice coefficient, with peak enhancements reaching 20.78%.","The method demonstrates good generalization to complex geometries and fast inference times, making it suitable for integration into design workflows using regular workstations.","Limitations include the sensitivity to non-max suppression thresholds.","YOLOv8-TO represents a significant advancement in topology optimization post-processing, enabling efficient and accurate reverse engineering of optimized structures for design exploration and manufacturing."],"url":"http://arxiv.org/abs/2404.18763v1","category":"cs.CV"}
{"created":"2024-04-29 14:12:33","title":"Exploring Chebyshev Polynomial Approximations: Error Estimates for Functions of Bounded Variation","abstract":"Approximation theory plays a central role in numerical analysis, undergoing continuous evolution through a spectrum of methodologies. Notably, Lebesgue, Weierstrass, Fourier, and Chebyshev approximations stand out among these methods. However, each technique possesses inherent limitations, underscoring the critical importance of selecting an appropriate approximation method tailored to specific problem domains. This article delves into the utilization of Chebyshev polynomials at Chebyshev nodes for approximation. For sufficiently smooth functions, the partial sum of Chebyshev series expansion offers optimal polynomial approximation, rendering it a preferred choice in various applications such as digital signal processing and graph filters due to its computational efficiency. In this article, we focus on functions of bounded variation, which find numerous applications across mathematical physics, hyperbolic conservations, and optimization. We present two optimal error estimations associated with Chebyshev polynomial approximations tailored for such functions. To validate our theoretical assertions, we conduct numerical experiments. Additionally, we delineate promising future avenues aligned with this research, particularly within the realms of machine learning and related fields.","sentences":["Approximation theory plays a central role in numerical analysis, undergoing continuous evolution through a spectrum of methodologies.","Notably, Lebesgue, Weierstrass, Fourier, and Chebyshev approximations stand out among these methods.","However, each technique possesses inherent limitations, underscoring the critical importance of selecting an appropriate approximation method tailored to specific problem domains.","This article delves into the utilization of Chebyshev polynomials at Chebyshev nodes for approximation.","For sufficiently smooth functions, the partial sum of Chebyshev series expansion offers optimal polynomial approximation, rendering it a preferred choice in various applications such as digital signal processing and graph filters due to its computational efficiency.","In this article, we focus on functions of bounded variation, which find numerous applications across mathematical physics, hyperbolic conservations, and optimization.","We present two optimal error estimations associated with Chebyshev polynomial approximations tailored for such functions.","To validate our theoretical assertions, we conduct numerical experiments.","Additionally, we delineate promising future avenues aligned with this research, particularly within the realms of machine learning and related fields."],"url":"http://arxiv.org/abs/2404.18723v1","category":"math.NA"}
{"created":"2024-04-29 14:12:33","title":"Barrier Algorithms for Constrained Non-Convex Optimization","abstract":"In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization. To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints. Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively.","sentences":["In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization.","To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints.","Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively."],"url":"http://arxiv.org/abs/2404.18724v1","category":"math.OC"}
{"created":"2024-04-29 14:08:11","title":"Rapid Computation of the Plasma Dispersion Function: Rational and Multi-pole Approximation, and Improved Accuracy","abstract":"The plasma dispersion function $Z(s)$ is a fundamental complex special integral function widely used in the field of plasma physics. The simplest and most rapid, yet accurate, approach to calculating it is through rational or equivalent multi-pole expansions. In this work, we summarize the numerical coefficients that are practically useful to the community. Besides the Pade approximation to obtain coefficients, which are accurate for both small and large arguments, we also employ optimization methods to enhance the accuracy of the approximation for the intermediate range. The best coefficients provided here for calculating $Z(s)$ can deliver twelve significant decimal digits. This work serves as a foundational database for the community for further applications.","sentences":["The plasma dispersion function $Z(s)$ is a fundamental complex special integral function widely used in the field of plasma physics.","The simplest and most rapid, yet accurate, approach to calculating it is through rational or equivalent multi-pole expansions.","In this work, we summarize the numerical coefficients that are practically useful to the community.","Besides the Pade approximation to obtain coefficients, which are accurate for both small and large arguments, we also employ optimization methods to enhance the accuracy of the approximation for the intermediate range.","The best coefficients provided here for calculating $Z(s)$ can deliver twelve significant decimal digits.","This work serves as a foundational database for the community for further applications."],"url":"http://arxiv.org/abs/2404.18719v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 13:30:27","title":"Work Smarter...Not Harder: Efficient Minimization of Dependency Length in SOV Languages","abstract":"Dependency length minimization is a universally observed quantitative property of natural languages. However, the extent of dependency length minimization, and the cognitive mechanisms through which the language processor achieves this minimization remain unclear. This research offers mechanistic insights by postulating that moving a short preverbal constituent next to the main verb explains preverbal constituent ordering decisions better than global minimization of dependency length in SOV languages. This approach constitutes a least-effort strategy because it's just one operation but simultaneously reduces the length of all preverbal dependencies linked to the main verb. We corroborate this strategy using large-scale corpus evidence across all seven SOV languages that are prominently represented in the Universal Dependency Treebank. These findings align with the concept of bounded rationality, where decision-making is influenced by 'quick-yet-economical' heuristics rather than exhaustive searches for optimal solutions. Overall, this work sheds light on the role of bounded rationality in linguistic decision-making and language evolution.","sentences":["Dependency length minimization is a universally observed quantitative property of natural languages.","However, the extent of dependency length minimization, and the cognitive mechanisms through which the language processor achieves this minimization remain unclear.","This research offers mechanistic insights by postulating that moving a short preverbal constituent next to the main verb explains preverbal constituent ordering decisions better than global minimization of dependency length in SOV languages.","This approach constitutes a least-effort strategy because it's just one operation but simultaneously reduces the length of all preverbal dependencies linked to the main verb.","We corroborate this strategy using large-scale corpus evidence across all seven SOV languages that are prominently represented in the Universal Dependency Treebank.","These findings align with the concept of bounded rationality, where decision-making is influenced by 'quick-yet-economical' heuristics rather than exhaustive searches for optimal solutions.","Overall, this work sheds light on the role of bounded rationality in linguistic decision-making and language evolution."],"url":"http://arxiv.org/abs/2404.18684v1","category":"cs.CL"}
{"created":"2024-04-29 13:11:20","title":"Uncertainty relation and the constrained quadratic programming","abstract":"The uncertainty relation is a fundamental concept in quantum theory, plays a pivotal role in various quantum information processing tasks. In this study, we explore the additive uncertainty relation pertaining to two or more observables, in terms of their variance,by utilizing the generalized Gell-Mann representation in qudit systems. We find that the tight state-independent lower bound of the variance sum can be characterized as a quadratic programming problem with nonlinear constraints in optimization theory. As illustrative examples, we derive analytical solutions for these quadratic programming problems in lower-dimensional systems, which align with the state-independent lower bounds. Additionally, we introduce a numerical algorithm tailored for solving these quadratic programming instances, highlighting its efficiency and accuracy. The advantage of our approach lies in its potential ability to simultaneously achieve the optimal value of the quadratic programming problem with nonlinear constraints but also precisely identify the extremal state where this optimal value is attained. This enables us to establish a tight state-independent lower bound for the sum of variances, and further identify the extremal state at which this lower bound is realized.","sentences":["The uncertainty relation is a fundamental concept in quantum theory, plays a pivotal role in various quantum information processing tasks.","In this study, we explore the additive uncertainty relation pertaining to two or more observables, in terms of their variance,by utilizing the generalized Gell-Mann representation in qudit systems.","We find that the tight state-independent lower bound of the variance sum can be characterized as a quadratic programming problem with nonlinear constraints in optimization theory.","As illustrative examples, we derive analytical solutions for these quadratic programming problems in lower-dimensional systems, which align with the state-independent lower bounds.","Additionally, we introduce a numerical algorithm tailored for solving these quadratic programming instances, highlighting its efficiency and accuracy.","The advantage of our approach lies in its potential ability to simultaneously achieve the optimal value of the quadratic programming problem with nonlinear constraints but also precisely identify the extremal state where this optimal value is attained.","This enables us to establish a tight state-independent lower bound for the sum of variances, and further identify the extremal state at which this lower bound is realized."],"url":"http://arxiv.org/abs/2404.18671v1","category":"quant-ph"}
{"created":"2024-04-29 12:35:03","title":"A Scoping Review on Simulation-based Design Optimization in Marine Engineering: Trends, Best Practices, and Gaps","abstract":"This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas. Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins. It also covers marine structures and renewable energy systems. A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches. The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments. Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness. This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field.","sentences":["This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas.","Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins.","It also covers marine structures and renewable energy systems.","A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches.","The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments.","Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness.","This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field."],"url":"http://arxiv.org/abs/2404.18654v1","category":"math.OC"}
{"created":"2024-04-29 12:32:35","title":"Energy Efficiency Optimization of Multi-unit System with Different Devices","abstract":"The energy efficiency optimization of the power generation system and the energy efficiency optimization of the energy consumption system are unified into the same optimization problem, and a simple method to achieve energy efficiency optimization without establishing an accurate mathematical model of the system is proposed. For systems with similar energy efficiency, it is proved that the best load distribution method between equipment is to keep the operating energy efficiency of each operating device equal, Yao's theorem 1. It is proved that the optimal switching method for the number of operating units between equipment with different energy efficiency is to keep the energy efficiency of the switching point equal, or at the maximum load point of the equipment, Yao's Theorem 2. This article gives two cases, a system composed of equipment with similar efficiency and a system composed of equipment with different efficiency.","sentences":["The energy efficiency optimization of the power generation system and the energy efficiency optimization of the energy consumption system are unified into the same optimization problem, and a simple method to achieve energy efficiency optimization without establishing an accurate mathematical model of the system is proposed.","For systems with similar energy efficiency, it is proved that the best load distribution method between equipment is to keep the operating energy efficiency of each operating device equal, Yao's theorem 1.","It is proved that the optimal switching method for the number of operating units between equipment with different energy efficiency is to keep the energy efficiency of the switching point equal, or at the maximum load point of the equipment, Yao's Theorem 2.","This article gives two cases, a system composed of equipment with similar efficiency and a system composed of equipment with different efficiency."],"url":"http://arxiv.org/abs/2404.18652v1","category":"math.OC"}
{"created":"2024-04-29 12:32:28","title":"Enhancing RSS-Based Visible Light Positioning by Optimal Calibrating the LED Tilt and Gain","abstract":"This paper presents an optimal calibration scheme and a weighted least squares (LS) localization algorithm for received signal strength (RSS) based visible light positioning (VLP) systems, focusing on the often overlooked impact of light emitting diode (LED) tilt. By optimally calibrating LED tilt and gain, we significantly enhance VLP localization accuracy. Our algorithm outperforms both machine learning Gaussian processes (GPs) and traditional multilateration techniques. Against GPs, it achieves improvements of 58% and 74% in the 50th and 99th percentiles, respectively. When compared to multilateration, it reduces the 50th percentile error from 7.4 cm to 3.2 cm and the 99th percentile error from 25.7 cm to 11 cm. We introduce a low-complexity estimator for tilt and gain that meets the Cramer-Rao lower bound (CRLB) for the mean squared error (MSE), emphasizing its precision and efficiency. Further, we elaborate on optimal calibration measurement placement and refine the observation model to include residual calibration errors, thereby improving localization performance. The weighted LS algorithm's effectiveness is validated through simulations and real-world data, consistently outperforming GPs and multilateration, across various training set sizes and reducing outlier errors. Our findings underscore the critical role of LED tilt calibration in advancing VLP system accuracy and contribute to a more precise model for indoor positioning technologies.","sentences":["This paper presents an optimal calibration scheme and a weighted least squares (LS) localization algorithm for received signal strength (RSS) based visible light positioning (VLP) systems, focusing on the often overlooked impact of light emitting diode (LED) tilt.","By optimally calibrating LED tilt and gain, we significantly enhance VLP localization accuracy.","Our algorithm outperforms both machine learning Gaussian processes (GPs) and traditional multilateration techniques.","Against GPs, it achieves improvements of 58% and 74% in the 50th and 99th percentiles, respectively.","When compared to multilateration, it reduces the 50th percentile error from 7.4 cm to 3.2 cm and the 99th percentile error from 25.7 cm to 11 cm.","We introduce a low-complexity estimator for tilt and gain that meets the Cramer-Rao lower bound (CRLB) for the mean squared error (MSE), emphasizing its precision and efficiency.","Further, we elaborate on optimal calibration measurement placement and refine the observation model to include residual calibration errors, thereby improving localization performance.","The weighted LS algorithm's effectiveness is validated through simulations and real-world data, consistently outperforming GPs and multilateration, across various training set sizes and reducing outlier errors.","Our findings underscore the critical role of LED tilt calibration in advancing VLP system accuracy and contribute to a more precise model for indoor positioning technologies."],"url":"http://arxiv.org/abs/2404.18650v1","category":"eess.SP"}
{"created":"2024-04-29 12:27:41","title":"Graph Search Trees and the Intermezzo Problem","abstract":"The last in-tree recognition problem asks whether a given spanning tree can be derived by connecting each vertex with its rightmost left neighbor of some search ordering. In this study, we demonstrate that the last-in-tree recognition problem for Generic Search is $\\mathsf{NP}$-complete. We utilize this finding to strengthen a complexity result from order theory. Given partial order $\\pi$ and a set of triples, the $\\mathsf{NP}$-complete intermezzo problem asks for a linear extension of $\\pi$ where each first element of a triple is not between the other two. We show that this problem remains $\\mathsf{NP}$-complete even when the Hasse diagram of the partial order forms a tree of bounded height. In contrast, we give an $\\mathsf{XP}$ algorithm for the problem when parameterized by the width of the partial order. Furthermore, we show that $\\unicode{x2013}$ under the assumption of the Exponential Time Hypothesis $\\unicode{x2013}$ the running time of this algorithm is asymptotically optimal.","sentences":["The last in-tree recognition problem asks whether a given spanning tree can be derived by connecting each vertex with its rightmost left neighbor of some search ordering.","In this study, we demonstrate that the last-in-tree recognition problem for Generic Search is $\\mathsf{NP}$-complete.","We utilize this finding to strengthen a complexity result from order theory.","Given partial order $\\pi$ and a set of triples, the $\\mathsf{NP}$-complete intermezzo problem asks for a linear extension of $\\pi$ where each first element of a triple is not between the other two.","We show that this problem remains $\\mathsf{NP}$-complete even when the Hasse diagram of the partial order forms a tree of bounded height.","In contrast, we give an $\\mathsf{XP}$ algorithm for the problem when parameterized by the width of the partial order.","Furthermore, we show that $\\unicode{x2013}$ under the assumption of the Exponential Time Hypothesis $\\unicode{x2013}$ the running time of this algorithm is asymptotically optimal."],"url":"http://arxiv.org/abs/2404.18645v1","category":"cs.DM"}
{"created":"2024-04-29 12:18:21","title":"Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems","abstract":"Two typical forms of bias in user interaction data with recommender systems (RSs) are popularity bias and positivity bias, which manifest themselves as the over-representation of interactions with popular items or items that users prefer, respectively. Debiasing methods aim to mitigate the effect of selection bias on the evaluation and optimization of RSs. However, existing debiasing methods only consider single-factor forms of bias, e.g., only the item (popularity) or only the rating value (positivity). This is in stark contrast with the real world where user selections are generally affected by multiple factors at once. In this work, we consider multifactorial selection bias in RSs. Our focus is on selection bias affected by both item and rating value factors, which is a generalization and combination of popularity and positivity bias. While the concept of multifactorial bias is intuitive, it brings a severe practical challenge as it requires substantially more data for accurate bias estimation. As a solution, we propose smoothing and alternating gradient descent techniques to reduce variance and improve the robustness of its optimization. Our experimental results reveal that, with our proposed techniques, multifactorial bias corrections are more effective and robust than single-factor counterparts on real-world and synthetic datasets.","sentences":["Two typical forms of bias in user interaction data with recommender systems (RSs) are popularity bias and positivity bias, which manifest themselves as the over-representation of interactions with popular items or items that users prefer, respectively.","Debiasing methods aim to mitigate the effect of selection bias on the evaluation and optimization of RSs.","However, existing debiasing methods only consider single-factor forms of bias, e.g., only the item (popularity) or only the rating value (positivity).","This is in stark contrast with the real world where user selections are generally affected by multiple factors at once.","In this work, we consider multifactorial selection bias in RSs.","Our focus is on selection bias affected by both item and rating value factors, which is a generalization and combination of popularity and positivity bias.","While the concept of multifactorial bias is intuitive, it brings a severe practical challenge as it requires substantially more data for accurate bias estimation.","As a solution, we propose smoothing and alternating gradient descent techniques to reduce variance and improve the robustness of its optimization.","Our experimental results reveal that, with our proposed techniques, multifactorial bias corrections are more effective and robust than single-factor counterparts on real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2404.18640v1","category":"cs.IR"}
{"created":"2024-04-29 11:55:19","title":"Towards topology optimization of a hybrid-excited machine using recursive material interpolation","abstract":"Hybrid-excited electrical machines aim to combine the advantages of permanent magnet machines (high efficiency and torque density) with those of separately excited machines (ease of flux-weakening at high speed). These machines are of interest to electric vehicles, and only parametric approaches are available in the literature for their optimization. This work proposes a more general topology optimization methodology by extending the formalism of density methods. The difficulty lies in integrating the numerous natures of materials (conductors, permanent magnets, ferromagnetic material, air...) without strongly deconvexifying the optimization problem, which leads to non-physical results with unsatisfactory performance. To address this issue, a recursive material interpolation is introduced. The hybrid-excited rotors optimized by this approach are compared with those of existing techniques, demonstrating a clear superiority of the recursive interpolation.","sentences":["Hybrid-excited electrical machines aim to combine the advantages of permanent magnet machines (high efficiency and torque density) with those of separately excited machines (ease of flux-weakening at high speed).","These machines are of interest to electric vehicles, and only parametric approaches are available in the literature for their optimization.","This work proposes a more general topology optimization methodology by extending the formalism of density methods.","The difficulty lies in integrating the numerous natures of materials (conductors, permanent magnets, ferromagnetic material, air...) without strongly deconvexifying the optimization problem, which leads to non-physical results with unsatisfactory performance.","To address this issue, a recursive material interpolation is introduced.","The hybrid-excited rotors optimized by this approach are compared with those of existing techniques, demonstrating a clear superiority of the recursive interpolation."],"url":"http://arxiv.org/abs/2404.18625v1","category":"math.OC"}
{"created":"2024-04-29 11:13:37","title":"Anywhere: A Multi-Agent Framework for Reliable and Diverse Foreground-Conditioned Image Inpainting","abstract":"Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as \"over-imagination\", inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating \"over-imagination\", resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results.","sentences":["Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes.","However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as \"over-imagination\", inconsistency between foreground and background, and limited diversity.","In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues.","Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models.","This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer.","The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts.","In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image.","The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed.","Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating \"over-imagination\", resolving foreground-background discrepancies, and enhancing diversity.","It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results."],"url":"http://arxiv.org/abs/2404.18598v1","category":"cs.CV"}
{"created":"2024-04-29 10:19:41","title":"Multigrid method for nonlinear eigenvalue problems based on Newton iteration","abstract":"In this paper, a novel multigrid method based on Newton iteration is proposed to solve nonlinear eigenvalue problems. Instead of handling the eigenvalue $\\lambda$ and eigenfunction $u$ separately, we treat the eigenpair $(\\lambda, u)$ as one element in a product space $\\mathbb R \\times H_0^1(\\Omega)$. Then in the presented multigrid method, only one discrete linear boundary value problem needs to be solved for each level of the multigrid sequence. Because we avoid solving large-scale nonlinear eigenvalue problems directly, the overall efficiency is significantly improved. The optimal error estimate and linear computational complexity can be derived simultaneously. In addition, we also provide an improved multigrid method coupled with a mixing scheme to further guarantee the convergence and stability of the iteration scheme. More importantly, we prove convergence for the residuals after each iteration step. For nonlinear eigenvalue problems, such theoretical analysis is missing from the existing literatures on the mixing iteration scheme.","sentences":["In this paper, a novel multigrid method based on Newton iteration is proposed to solve nonlinear eigenvalue problems.","Instead of handling the eigenvalue $\\lambda$ and eigenfunction $u$ separately, we treat the eigenpair $(\\lambda, u)$ as one element in a product space $\\mathbb R \\times H_0^1(\\Omega)$.","Then in the presented multigrid method, only one discrete linear boundary value problem needs to be solved for each level of the multigrid sequence.","Because we avoid solving large-scale nonlinear eigenvalue problems directly, the overall efficiency is significantly improved.","The optimal error estimate and linear computational complexity can be derived simultaneously.","In addition, we also provide an improved multigrid method coupled with a mixing scheme to further guarantee the convergence and stability of the iteration scheme.","More importantly, we prove convergence for the residuals after each iteration step.","For nonlinear eigenvalue problems, such theoretical analysis is missing from the existing literatures on the mixing iteration scheme."],"url":"http://arxiv.org/abs/2404.18568v1","category":"math.NA"}
{"created":"2024-04-29 10:07:05","title":"Social Optima of Linear Forward-Backward Stochastic System","abstract":"A linear quadratic (LQ) stochastic optimization system involving large population, which is driven by forward-backward stochastic differential equation (FBSDE), is investigated in this paper. Agents cooperate with each other to minimize the so-called social objective, which is rather different from mean field (MF) game. Employing forward-backward person-by-person optimality principle, we derive an auxiliary LQ control problem by decentralized information. A decentralized strategy is obtained by virtue of an MF-type forward-backward stochastic differential equation consistency condition. Applying Riccati equation decoupling method, we solve the consistency condition system. We also verify the asymptotic social optimality in this framework.","sentences":["A linear quadratic (LQ) stochastic optimization system involving large population, which is driven by forward-backward stochastic differential equation (FBSDE), is investigated in this paper.","Agents cooperate with each other to minimize the so-called social objective, which is rather different from mean field (MF) game.","Employing forward-backward person-by-person optimality principle, we derive an auxiliary LQ control problem by decentralized information.","A decentralized strategy is obtained by virtue of an MF-type forward-backward stochastic differential equation consistency condition.","Applying Riccati equation decoupling method, we solve the consistency condition system.","We also verify the asymptotic social optimality in this framework."],"url":"http://arxiv.org/abs/2404.18561v1","category":"math.OC"}
{"created":"2024-04-29 10:06:55","title":"Non-convex Pose Graph Optimization in SLAM via Proximal Linearized Riemannian ADMM","abstract":"Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem. In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution. The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization. Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel. Furthermore, we establish the iteration complexity of $O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of our model. The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets.","sentences":["Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem.","In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution.","The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization.","Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel.","Furthermore, we establish the iteration complexity of $O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of our model.","The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets."],"url":"http://arxiv.org/abs/2404.18560v1","category":"math.OC"}
{"created":"2024-04-29 09:11:15","title":"Dynamical Blockade Optimizing via Particle Swarm Optimization Algorithm","abstract":"Photon blockade in weak nonlinear regime is an exciting and promising subject that has been extensively studied in the steady state. However, how to achieve dynamic blockade in a single bosonic mode with weak nonlinearity using only pulsed driving field remains unexplored. Here, we propose to optimize the parameters of the pulsed driving field to achieve dynamic blockade in a single bosonic mode with weak nonlinearity via the particle swarm optimization (PSO) algorithm. We demonstrate that both Gaussian and rectangular pulses can be used to generate dynamic photon blockade in a single bosonic mode with weak nonlinearity. Based on the Fourier series expansions of the pulsed driving field, we identify that there are many paths for two-photon excitation in the bosonic mode, even only driven by pulsed field, and the dynamic blockade in weak nonlinear regime is induced by the destructive interference between them. Our work not only highlights the effectiveness of PSO algorithm in optimizing dynamical blockade, but also opens a way to optimize the parameters for other quantum effects, such as quantum entanglement and quantum squeezing.","sentences":["Photon blockade in weak nonlinear regime is an exciting and promising subject that has been extensively studied in the steady state.","However, how to achieve dynamic blockade in a single bosonic mode with weak nonlinearity using only pulsed driving field remains unexplored.","Here, we propose to optimize the parameters of the pulsed driving field to achieve dynamic blockade in a single bosonic mode with weak nonlinearity via the particle swarm optimization (PSO) algorithm.","We demonstrate that both Gaussian and rectangular pulses can be used to generate dynamic photon blockade in a single bosonic mode with weak nonlinearity.","Based on the Fourier series expansions of the pulsed driving field, we identify that there are many paths for two-photon excitation in the bosonic mode, even only driven by pulsed field, and the dynamic blockade in weak nonlinear regime is induced by the destructive interference between them.","Our work not only highlights the effectiveness of PSO algorithm in optimizing dynamical blockade, but also opens a way to optimize the parameters for other quantum effects, such as quantum entanglement and quantum squeezing."],"url":"http://arxiv.org/abs/2404.18523v1","category":"quant-ph"}
{"created":"2024-04-29 08:43:58","title":"Towards Classical Software Verification using Quantum Computers","abstract":"We explore the possibility of accelerating the formal verification of classical programs with a quantum computer.   A common source of security flaws stems from the existence of common programming errors like use after free, null-pointer dereference, or division by zero. To aid in the discovery of such errors, we try to verify that no such flaws exist.   In our approach, for some code snippet and undesired behaviour, a SAT instance is generated, which is satisfiable precisely if the behavior is present in the code. It is in turn converted to an optimization problem, that is solved on a quantum computer. This approach holds the potential of an asymptotically polynomial speedup.   Minimal examples of common errors, like out-of-bounds and overflows, but also synthetic instances with special properties, specific number of solutions, or structure, are tested with different solvers and tried on a quantum device.   We use the near-standard Quantum Approximation Optimisation Algorithm, an application of the Grover algorithm, and the Quantum Singular Value Transformation to find the optimal solution, and with it a satisfying assignment.","sentences":["We explore the possibility of accelerating the formal verification of classical programs with a quantum computer.   ","A common source of security flaws stems from the existence of common programming errors like use after free, null-pointer dereference, or division by zero.","To aid in the discovery of such errors, we try to verify that no such flaws exist.   ","In our approach, for some code snippet and undesired behaviour, a SAT instance is generated, which is satisfiable precisely if the behavior is present in the code.","It is in turn converted to an optimization problem, that is solved on a quantum computer.","This approach holds the potential of an asymptotically polynomial speedup.   ","Minimal examples of common errors, like out-of-bounds and overflows, but also synthetic instances with special properties, specific number of solutions, or structure, are tested with different solvers and tried on a quantum device.   ","We use the near-standard Quantum Approximation Optimisation Algorithm, an application of the Grover algorithm, and the Quantum Singular Value Transformation to find the optimal solution, and with it a satisfying assignment."],"url":"http://arxiv.org/abs/2404.18502v1","category":"quant-ph"}
{"created":"2024-04-29 08:41:32","title":"Hyperplane Representations of Interventional Characteristic Imset Polytopes","abstract":"Characteristic imsets are 0/1-vectors representing directed acyclic graphs whose edges represent direct cause-effect relations between jointly distributed random variables. A characteristic imset (CIM) polytope is the convex hull of a collection of characteristic imsets. CIM polytopes arise as feasible regions of a linear programming approach to the problem of causal disovery, which aims to infer a cause-effect structure from data. Linear optimization methods typically require a hyperplane representation of the feasible region, which has proven difficult to compute for CIM polytopes despite continued efforts. We solve this problem for CIM polytopes that are the convex hull of imsets associated to DAGs whose underlying graph of adjacencies is a tree. Our methods use the theory of toric fiber products as well as the novel notion of interventional CIM polytopes. Our solution is obtained as a corollary of a more general result for interventional CIM polytopes. The identified hyperplanes are applied to yield a linear optimization-based causal discovery algorithm for learning polytree causal networks from a combination of observational and interventional data.","sentences":["Characteristic imsets are 0/1-vectors representing directed acyclic graphs whose edges represent direct cause-effect relations between jointly distributed random variables.","A characteristic imset (CIM) polytope is the convex hull of a collection of characteristic imsets.","CIM polytopes arise as feasible regions of a linear programming approach to the problem of causal disovery, which aims to infer a cause-effect structure from data.","Linear optimization methods typically require a hyperplane representation of the feasible region, which has proven difficult to compute for CIM polytopes despite continued efforts.","We solve this problem for CIM polytopes that are the convex hull of imsets associated to DAGs whose underlying graph of adjacencies is a tree.","Our methods use the theory of toric fiber products as well as the novel notion of interventional CIM polytopes.","Our solution is obtained as a corollary of a more general result for interventional CIM polytopes.","The identified hyperplanes are applied to yield a linear optimization-based causal discovery algorithm for learning polytree causal networks from a combination of observational and interventional data."],"url":"http://arxiv.org/abs/2404.18500v1","category":"math.CO"}
{"created":"2024-04-29 08:34:35","title":"PHOBIC: Perfect Hashing with Optimized Bucket Sizes and Interleaved Coding","abstract":"A minimal perfect hash function (MPHF) maps a set of n keys to {1, ..., n} without collisions. Such functions find widespread application e.g. in bioinformatics and databases. In this paper we revisit PTHash - a construction technique particularly designed for fast queries. PTHash distributes the input keys into small buckets and, for each bucket, it searches for a hash function seed that places its keys in the output domain without collisions. The collection of all seeds is then stored in a compressed way. Since the first buckets are easier to place, buckets are considered in non-increasing order of size. Additionally, PTHash heuristically produces an imbalanced distribution of bucket sizes by distributing 60% of the keys into 30% of the buckets. Our main contribution is to characterize, up to lower order terms, an optimal distribution of expected bucket sizes. We arrive at a simple, closed form solution which improves construction throughput for space efficient configurations in practice. Our second contribution is a novel encoding scheme for the seeds. We split the keys into partitions. Within each partition, we run the bucket distribution and search step. We then store the seeds in an interleaved way by consecutively placing the seeds for the i-th buckets from all partitions. The seeds for the i-th bucket of each partition follow the same statistical distribution. This allows us to tune a compressor for each bucket. Hence, we call our technique PHOBIC - Perfect Hashing with Optimized Bucket sizes and Interleaved Coding. Compared to PTHash, PHOBIC is 0.17 bits/key more space efficient for same query time and construction throughput. We also contribute a GPU implementation to further accelerate MPHF construction. For a configuration with fast queries, PHOBIC-GPU can construct a perfect hash function at 2.17 bits/key in 28 ns per key, which can be queried in 37 ns on the CPU.","sentences":["A minimal perfect hash function (MPHF) maps a set of n keys to {1, ..., n} without collisions.","Such functions find widespread application e.g. in bioinformatics and databases.","In this paper we revisit PTHash - a construction technique particularly designed for fast queries.","PTHash distributes the input keys into small buckets and, for each bucket, it searches for a hash function seed that places its keys in the output domain without collisions.","The collection of all seeds is then stored in a compressed way.","Since the first buckets are easier to place, buckets are considered in non-increasing order of size.","Additionally, PTHash heuristically produces an imbalanced distribution of bucket sizes by distributing 60% of the keys into 30% of the buckets.","Our main contribution is to characterize, up to lower order terms, an optimal distribution of expected bucket sizes.","We arrive at a simple, closed form solution which improves construction throughput for space efficient configurations in practice.","Our second contribution is a novel encoding scheme for the seeds.","We split the keys into partitions.","Within each partition, we run the bucket distribution and search step.","We then store the seeds in an interleaved way by consecutively placing the seeds for the i-th buckets from all partitions.","The seeds for the i-th bucket of each partition follow the same statistical distribution.","This allows us to tune a compressor for each bucket.","Hence, we call our technique PHOBIC - Perfect Hashing with Optimized Bucket sizes and Interleaved Coding.","Compared to PTHash, PHOBIC is 0.17 bits/key more space efficient for same query time and construction throughput.","We also contribute a GPU implementation to further accelerate MPHF construction.","For a configuration with fast queries, PHOBIC-GPU can construct a perfect hash function at 2.17 bits/key in 28 ns per key, which can be queried in 37 ns on the CPU."],"url":"http://arxiv.org/abs/2404.18497v1","category":"cs.DS"}
{"created":"2024-04-29 08:27:50","title":"AI-powered Code Review with LLMs: Early Results","abstract":"In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues. Our proposed LLM-based AI agent model is trained on large code repositories. This training includes code reviews, bug reports, and documentation of best practices. It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code. Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code. This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques. Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback. For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods. This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews. Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education.","sentences":["In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues.","Our proposed LLM-based AI agent model is trained on large code repositories.","This training includes code reviews, bug reports, and documentation of best practices.","It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code.","Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code.","This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques.","Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback.","For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods.","This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews.","Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education."],"url":"http://arxiv.org/abs/2404.18496v1","category":"cs.SE"}
{"created":"2024-04-29 08:16:30","title":"Reduced-Rank Multi-objective Policy Learning and Optimization","abstract":"Evaluating the causal impacts of possible interventions is crucial for informing decision-making, especially towards improving access to opportunity. However, if causal effects are heterogeneous and predictable from covariates, personalized treatment decisions can improve individual outcomes and contribute to both efficiency and equity. In practice, however, causal researchers do not have a single outcome in mind a priori and often collect multiple outcomes of interest that are noisy estimates of the true target of interest. For example, in government-assisted social benefit programs, policymakers collect many outcomes to understand the multidimensional nature of poverty. The ultimate goal is to learn an optimal treatment policy that in some sense maximizes multiple outcomes simultaneously. To address such issues, we present a data-driven dimensionality-reduction methodology for multiple outcomes in the context of optimal policy learning with multiple objectives. We learn a low-dimensional representation of the true outcome from the observed outcomes using reduced rank regression. We develop a suite of estimates that use the model to denoise observed outcomes, including commonly-used index weightings. These methods improve estimation error in policy evaluation and optimization, including on a case study of real-world cash transfer and social intervention data. Reducing the variance of noisy social outcomes can improve the performance of algorithmic allocations.","sentences":["Evaluating the causal impacts of possible interventions is crucial for informing decision-making, especially towards improving access to opportunity.","However, if causal effects are heterogeneous and predictable from covariates, personalized treatment decisions can improve individual outcomes and contribute to both efficiency and equity.","In practice, however, causal researchers do not have a single outcome in mind a priori and often collect multiple outcomes of interest that are noisy estimates of the true target of interest.","For example, in government-assisted social benefit programs, policymakers collect many outcomes to understand the multidimensional nature of poverty.","The ultimate goal is to learn an optimal treatment policy that in some sense maximizes multiple outcomes simultaneously.","To address such issues, we present a data-driven dimensionality-reduction methodology for multiple outcomes in the context of optimal policy learning with multiple objectives.","We learn a low-dimensional representation of the true outcome from the observed outcomes using reduced rank regression.","We develop a suite of estimates that use the model to denoise observed outcomes, including commonly-used index weightings.","These methods improve estimation error in policy evaluation and optimization, including on a case study of real-world cash transfer and social intervention data.","Reducing the variance of noisy social outcomes can improve the performance of algorithmic allocations."],"url":"http://arxiv.org/abs/2404.18490v1","category":"cs.LG"}
{"created":"2024-04-29 07:28:51","title":"Mobile Networks on the Move: Optimizing Moving Base Stations Dynamics in Urban Scenarios","abstract":"Base station densification is one of the key approaches for delivering high capacity in radio access networks. However, current static deployments are often impractical and financially unsustainable, as they increase both capital and operational expenditures of the network. An alternative paradigm is the moving base stations (MBSs) approach, by which part of base stations are installed on vehicles. However, to the best of our knowledge, it is still unclear if and up to which point MBSs allow decreasing the number of static base stations (BSs) deployed in urban settings. In this work, we start tackling this issue by proposing a modeling approach for a first-order evaluation of potential infrastructure savings enabled by the MBSs paradigm. Starting from a set of stochastic geometry results, and a traffic demand profile over time, we formulate an optimization problem for the derivation of the optimal combination of moving and static BSs which minimizes the overall amount of BSs deployed, while guaranteeing a target mean QoS for users. Initial results on a two-district scenario with measurement-based network traffic profiles suggest that substantial infrastructure savings are achievable. We show that these results are robust against different values of user density.","sentences":["Base station densification is one of the key approaches for delivering high capacity in radio access networks.","However, current static deployments are often impractical and financially unsustainable, as they increase both capital and operational expenditures of the network.","An alternative paradigm is the moving base stations (MBSs) approach, by which part of base stations are installed on vehicles.","However, to the best of our knowledge, it is still unclear if and up to which point MBSs allow decreasing the number of static base stations (BSs) deployed in urban settings.","In this work, we start tackling this issue by proposing a modeling approach for a first-order evaluation of potential infrastructure savings enabled by the MBSs paradigm.","Starting from a set of stochastic geometry results, and a traffic demand profile over time, we formulate an optimization problem for the derivation of the optimal combination of moving and static BSs which minimizes the overall amount of BSs deployed, while guaranteeing a target mean QoS for users.","Initial results on a two-district scenario with measurement-based network traffic profiles suggest that substantial infrastructure savings are achievable.","We show that these results are robust against different values of user density."],"url":"http://arxiv.org/abs/2404.18476v1","category":"cs.NI"}
{"created":"2024-04-29 07:07:58","title":"HFT: Half Fine-Tuning for Large Language Models","abstract":"Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge. Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge. We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term. Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time.","sentences":["Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences.","However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data.","In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge.","Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge.","We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term.","Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks.","Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT.","Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time."],"url":"http://arxiv.org/abs/2404.18466v1","category":"cs.CL"}
