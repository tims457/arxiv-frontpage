{"created":"2024-01-31 18:59:59","title":"Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators","abstract":"Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.","sentences":["Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale.","However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult.","To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move.","Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network.","Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image.","By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image.","We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images."],"url":"http://arxiv.org/abs/2401.18085v1","category":"cs.CV"}
{"created":"2024-01-31 18:59:57","title":"Binding Touch to Everything: Learning Unified Multimodal Tactile Representations","abstract":"The ability to associate touch with other modalities has huge implications for humans and computational systems. However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs. We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound. We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities. We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time. UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering. To the best of our knowledge, UniTouch is the first to demonstrate such capabilities. Project page: https://cfeng16.github.io/UniTouch/","sentences":["The ability to associate touch with other modalities has huge implications for humans and computational systems.","However, multimodal learning with touch remains challenging due to the expensive data collection process and non-standardized sensor outputs.","We introduce UniTouch, a unified tactile model for vision-based touch sensors connected to multiple modalities, including vision, language, and sound.","We achieve this by aligning our UniTouch embeddings to pretrained image embeddings already associated with a variety of other modalities.","We further propose learnable sensor-specific tokens, allowing the model to learn from a set of heterogeneous tactile sensors, all at the same time.","UniTouch is capable of conducting various touch sensing tasks in the zero-shot setting, from robot grasping prediction to touch image question answering.","To the best of our knowledge, UniTouch is the first to demonstrate such capabilities.","Project page: https://cfeng16.github.io/UniTouch/"],"url":"http://arxiv.org/abs/2401.18084v1","category":"cs.CV"}
{"created":"2024-01-31 18:59:12","title":"Improved Scene Landmark Detection for Camera Localization","abstract":"Camera localization methods based on retrieval, local feature matching, and 3D structure-based pose estimation are accurate but require high storage, are slow, and are not privacy-preserving. A method based on scene landmark detection (SLD) was recently proposed to address these limitations. It involves training a convolutional neural network (CNN) to detect a few predetermined, salient, scene-specific 3D points or landmarks and computing camera pose from the associated 2D-3D correspondences. Although SLD outperformed existing learning-based approaches, it was notably less accurate than 3D structure-based methods. In this paper, we show that the accuracy gap was due to insufficient model capacity and noisy labels during training. To mitigate the capacity issue, we propose to split the landmarks into subgroups and train a separate network for each subgroup. To generate better training labels, we propose using dense reconstructions to estimate visibility of scene landmarks. Finally, we present a compact architecture to improve memory efficiency. Accuracy wise, our approach is on par with state of the art structure based methods on the INDOOR-6 dataset but runs significantly faster and uses less storage. Code and models can be found at https://github.com/microsoft/SceneLandmarkLocalization.","sentences":["Camera localization methods based on retrieval, local feature matching, and 3D structure-based pose estimation are accurate but require high storage, are slow, and are not privacy-preserving.","A method based on scene landmark detection (SLD) was recently proposed to address these limitations.","It involves training a convolutional neural network (CNN) to detect a few predetermined, salient, scene-specific 3D points or landmarks and computing camera pose from the associated 2D-3D correspondences.","Although SLD outperformed existing learning-based approaches, it was notably less accurate than 3D structure-based methods.","In this paper, we show that the accuracy gap was due to insufficient model capacity and noisy labels during training.","To mitigate the capacity issue, we propose to split the landmarks into subgroups and train a separate network for each subgroup.","To generate better training labels, we propose using dense reconstructions to estimate visibility of scene landmarks.","Finally, we present a compact architecture to improve memory efficiency.","Accuracy wise, our approach is on par with state of the art structure based methods on the INDOOR-6 dataset but runs significantly faster and uses less storage.","Code and models can be found at https://github.com/microsoft/SceneLandmarkLocalization."],"url":"http://arxiv.org/abs/2401.18083v1","category":"cs.CV"}
{"created":"2024-01-31 18:58:35","title":"Storage of telecom wavelength heralded single photons in a fiber cavity quantum memory","abstract":"We demonstrate the storage and retrieval of heralded single photons in a fiber-based cavity quantum memory. The photons are stored, and retrieved, from the memory using quantum frequency conversion which switches the photon into, and out of, resonance with the cavity. The photons, generated in the telecom O-band with a bandwidth of 81\\,GHz, are retrieved from the memory with a $1/e$ lifetime of 1.64$\\mu$s, or 32.8 cavity round trips. We show that non-classical photon statistics remain for 70 round trips. The internal memory efficiency after 0.5$\\mu$s of storage is $10.9 \\pm 0.5$%; a coupling efficiency of 60% into the memory cavity yields a total efficiency of $6.0\\pm0.3$%. These results mark a crucial step forward in the development of fiber-based quantum memories, and high-bandwidth memories operating at telecom wavelengths, with applications to photon source multiplexing and fiber-based quantum networking.","sentences":["We demonstrate the storage and retrieval of heralded single photons in a fiber-based cavity quantum memory.","The photons are stored, and retrieved, from the memory using quantum frequency conversion which switches the photon into, and out of, resonance with the cavity.","The photons, generated in the telecom O-band with a bandwidth of 81\\,GHz, are retrieved from the memory with a $1/e$ lifetime of 1.64$\\mu$s, or 32.8 cavity round trips.","We show that non-classical photon statistics remain for 70 round trips.","The internal memory efficiency after 0.5$\\mu$s of storage is $10.9 \\pm 0.5$%; a coupling efficiency of 60% into the memory cavity yields a total efficiency of $6.0\\pm0.3$%.","These results mark a crucial step forward in the development of fiber-based quantum memories, and high-bandwidth memories operating at telecom wavelengths, with applications to photon source multiplexing and fiber-based quantum networking."],"url":"http://arxiv.org/abs/2401.18081v1","category":"quant-ph"}
{"created":"2024-01-31 18:58:14","title":"KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization","abstract":"LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.","sentences":["LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference.","Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit.","In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges; and (v) Q-Norm, where we normalize quantization centroids in order to mitigate distribution shift, providing additional benefits for 2-bit quantization.","By applying our method to the LLaMA, LLaMA-2, and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches.","Our method enables serving the LLaMA-7B model with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system."],"url":"http://arxiv.org/abs/2401.18079v1","category":"cs.LG"}
{"created":"2024-01-31 18:56:44","title":"Toward deterministic sources: Photon generation in a fiber-cavity quantum memory","abstract":"We demonstrate the generation of photons within a fiber-cavity quantum memory, followed by later on-demand readout. Signal photons are generated by spontaneous four-wave mixing in a fiber cavity comprising a birefringent fiber with dichroic reflective end facets. The detection of the partner herald photon indicates the creation of the stored signal photon. After a delay, the signal photon is switched out of resonance with the fiber cavity by intracavity frequency translation using Bragg scattering four-wave mixing, driven by ancillary control pulses. We measure sub-Poissonian statistics in the output signal mode, with $g^{(2)}_{AC}=0.54(1)$ in the first readout bin and a readout frequency translation efficiency of $\\approx$80%. The 1/e memory lifetime is $\\approx$67 cavity cycles, or 1.68$\\mu$s. In an alternate fiber cavity, we show a strategy for noise reduction and measure $g^{(2)}_{AC}=0.068(10)$ after one cavity cycle.","sentences":["We demonstrate the generation of photons within a fiber-cavity quantum memory, followed by later on-demand readout.","Signal photons are generated by spontaneous four-wave mixing in a fiber cavity comprising a birefringent fiber with dichroic reflective end facets.","The detection of the partner herald photon indicates the creation of the stored signal photon.","After a delay, the signal photon is switched out of resonance with the fiber cavity by intracavity frequency translation using Bragg scattering four-wave mixing, driven by ancillary control pulses.","We measure sub-Poissonian statistics in the output signal mode, with $g^{(2)}_{AC}=0.54(1)$ in the first readout bin and a readout frequency translation efficiency of $\\approx$80%.","The 1/e memory lifetime is $\\approx$67 cavity cycles, or 1.68$\\mu$s.","In an alternate fiber cavity, we show a strategy for noise reduction and measure $g^{(2)}_{AC}=0.068(10)$ after one cavity cycle."],"url":"http://arxiv.org/abs/2401.18077v1","category":"quant-ph"}
{"created":"2024-01-31 18:56:21","title":"Searching for scalar field dark matter with LIGO","abstract":"We report on a direct search for scalar field dark matter using data from LIGO's third observing run. We analyse the coupling of size oscillations of the interferometer's beamsplitter and arm test masses that may be caused by scalar field dark matter. Using new efficient search methods to maximise sensitivity for signatures of such oscillations, we set new upper limits for the coupling constants of scalar field dark matter as a function of its mass, which improve upon bounds from previous direct searches by several orders of magnitude in a frequency band from 10 Hz to 180 Hz.","sentences":["We report on a direct search for scalar field dark matter using data from LIGO's third observing run.","We analyse the coupling of size oscillations of the interferometer's beamsplitter and arm test masses that may be caused by scalar field dark matter.","Using new efficient search methods to maximise sensitivity for signatures of such oscillations, we set new upper limits for the coupling constants of scalar field dark matter as a function of its mass, which improve upon bounds from previous direct searches by several orders of magnitude in a frequency band from 10 Hz to 180 Hz."],"url":"http://arxiv.org/abs/2401.18076v1","category":"astro-ph.CO"}
{"created":"2024-01-31 18:56:09","title":"CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting","abstract":"We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting, a method for predicting future 3D scenes given past observations, such as 2D ego-centric images. Our method maps an image to a distribution over plausible 3D latent scene configurations using a probabilistic encoder, and predicts the evolution of the hypothesized scenes through time. Our latent scene representation conditions a global Neural Radiance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward downstream applications. This approach extends beyond previous neural rendering work by considering complex scenarios of uncertainty in environmental states and dynamics. We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network. We demonstrate the utility of our method in realistic scenarios using the CARLA driving simulator, where CARFF can be used to enable efficient trajectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions.","sentences":["We propose CARFF:","Conditional Auto-encoded Radiance Field for 3D Scene Forecasting, a method for predicting future 3D scenes given past observations, such as 2D ego-centric images.","Our method maps an image to a distribution over plausible 3D latent scene configurations using a probabilistic encoder, and predicts the evolution of the hypothesized scenes through time.","Our latent scene representation conditions a global Neural Radiance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward downstream applications.","This approach extends beyond previous neural rendering work by considering complex scenarios of uncertainty in environmental states and dynamics.","We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations.","Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network.","We demonstrate the utility of our method in realistic scenarios using the CARLA driving simulator, where CARFF can be used to enable efficient trajectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions."],"url":"http://arxiv.org/abs/2401.18075v1","category":"cs.CV"}
{"created":"2024-01-31 18:49:03","title":"Dual frame optimization for informationally complete quantum measurements","abstract":"Randomized measurement protocols such as classical shadows represent powerful resources for quantum technologies, with applications ranging from quantum state characterization and process tomography to machine learning and error mitigation. Recently, the notion of measurement dual frames, in which classical shadows are generalized to dual operators of POVM effects, resurfaced in the literature. This brought attention to additional degrees of freedom in the post-processing stage of randomized measurements that are often neglected by established techniques. In this work, we leverage dual frames to construct improved observable estimators from informationally complete measurement samples. We introduce novel classes of parametrized frame superoperators and optimization-free dual frames based on empirical frequencies, which offer advantages over their canonical counterparts while retaining computational efficiency. Remarkably, this comes at almost no quantum or classical cost, thus rendering dual frame optimization a valuable addition to the randomized measurement toolbox.","sentences":["Randomized measurement protocols such as classical shadows represent powerful resources for quantum technologies, with applications ranging from quantum state characterization and process tomography to machine learning and error mitigation.","Recently, the notion of measurement dual frames, in which classical shadows are generalized to dual operators of POVM effects, resurfaced in the literature.","This brought attention to additional degrees of freedom in the post-processing stage of randomized measurements that are often neglected by established techniques.","In this work, we leverage dual frames to construct improved observable estimators from informationally complete measurement samples.","We introduce novel classes of parametrized frame superoperators and optimization-free dual frames based on empirical frequencies, which offer advantages over their canonical counterparts while retaining computational efficiency.","Remarkably, this comes at almost no quantum or classical cost, thus rendering dual frame optimization a valuable addition to the randomized measurement toolbox."],"url":"http://arxiv.org/abs/2401.18071v1","category":"quant-ph"}
{"created":"2024-01-31 18:48:20","title":"Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?","abstract":"There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not during the final step which relies on the problem's arithmetic expressions (solution execution).","sentences":["There is increasing interest in employing large language models (LLMs) as cognitive models.","For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not.","In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems.","Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution.","We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs.","We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features.","We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not during the final step which relies on the problem's arithmetic expressions (solution execution)."],"url":"http://arxiv.org/abs/2401.18070v1","category":"cs.CL"}
{"created":"2024-01-31 18:47:44","title":"Classification-Oriented Semantic Wireless Communications","abstract":"We propose semantic communication over wireless channels for various modalities, e.g., text and images, in a task-oriented communications setup where the task is classification. We present two approaches based on memory and learning. Both approaches rely on a pre-trained neural network to extract semantic information but differ in codebook construction. In the memory-based approach, we use semantic quantization and compression models, leveraging past source realizations as a codebook to eliminate the need for further training. In the learning-based approach, we use a semantic vector quantized autoencoder model that learns a codebook from scratch. Both are followed by a channel coder in order to reliably convey semantic information to the receiver (classifier) through the wireless medium. In addition to classification accuracy, we define system time efficiency as a new performance metric. Our results demonstrate that the proposed memory-based approach outperforms its learning-based counterpart with respect to system time efficiency while offering comparable accuracy to semantic agnostic conventional baselines.","sentences":["We propose semantic communication over wireless channels for various modalities, e.g., text and images, in a task-oriented communications setup where the task is classification.","We present two approaches based on memory and learning.","Both approaches rely on a pre-trained neural network to extract semantic information but differ in codebook construction.","In the memory-based approach, we use semantic quantization and compression models, leveraging past source realizations as a codebook to eliminate the need for further training.","In the learning-based approach, we use a semantic vector quantized autoencoder model that learns a codebook from scratch.","Both are followed by a channel coder in order to reliably convey semantic information to the receiver (classifier) through the wireless medium.","In addition to classification accuracy, we define system time efficiency as a new performance metric.","Our results demonstrate that the proposed memory-based approach outperforms its learning-based counterpart with respect to system time efficiency while offering comparable accuracy to semantic agnostic conventional baselines."],"url":"http://arxiv.org/abs/2401.18069v1","category":"cs.IT"}
{"created":"2024-01-31 18:46:52","title":"Geodesic structure, shadow and optical appearance of black hole immersed in Chaplygin-like dark fluid","abstract":"In this study, we focus on a black hole immersed in a cosmological Chaplygin-like dark fluid (CDF), characterized by the equation of state $p=-B/\\rho$ and an additional parameter $q$ influencing the energy density of the fluid. We investigate the geodesic structure, shadow, and optical appearance of such a black hole. Through analysis on the effective potential and the epicyclic frequencies, it is found that the existence of innermost/outermost stable circular orbits for a timelike particle is governed by the CDF parameters. The behaviors of the orbital conserved quantities and Keplerian frequency are also examined. Due to the existence of pseudo-cosmological horizon, the determination of the shadow radius depends significantly on the position of the observer. Viewing the CDF as driving cosmic expansion and placing the static observer far from both event and cosmological horizons, the CDF parameters are constrained by EHT observations. We investigate the effect of CDF on the shadows and optical images of the black hole, surrounded by various profiles of accretions. For the thin disk accretion, the light trajectories are categorized into direct emission, lensing ring, and photon ring based on impact parameters. Due to the existence of outermost stable circular orbits, outer edges could exist in the direct and lensing ring images. The observed brightness is mainly due to direct emission, with a minor contribution from the lensing ring, while the contribution from the photon ring is negligible due to extreme demagnetization. In the case of spherical accretion, we consider both static and infalling accretion models. The images obtained under infalling accretion are slightly darker than those under static accretion, attributed to the Doppler effect. Throughout the study, we analyze the influence of the parameters $B$ and $q$ on the results.","sentences":["In this study, we focus on a black hole immersed in a cosmological Chaplygin-like dark fluid (CDF), characterized by the equation of state $p=-B/\\rho$ and an additional parameter $q$ influencing the energy density of the fluid.","We investigate the geodesic structure, shadow, and optical appearance of such a black hole.","Through analysis on the effective potential and the epicyclic frequencies, it is found that the existence of innermost/outermost stable circular orbits for a timelike particle is governed by the CDF parameters.","The behaviors of the orbital conserved quantities and Keplerian frequency are also examined.","Due to the existence of pseudo-cosmological horizon, the determination of the shadow radius depends significantly on the position of the observer.","Viewing the CDF as driving cosmic expansion and placing the static observer far from both event and cosmological horizons, the CDF parameters are constrained by EHT observations.","We investigate the effect of CDF on the shadows and optical images of the black hole, surrounded by various profiles of accretions.","For the thin disk accretion, the light trajectories are categorized into direct emission, lensing ring, and photon ring based on impact parameters.","Due to the existence of outermost stable circular orbits, outer edges could exist in the direct and lensing ring images.","The observed brightness is mainly due to direct emission, with a minor contribution from the lensing ring, while the contribution from the photon ring is negligible due to extreme demagnetization.","In the case of spherical accretion, we consider both static and infalling accretion models.","The images obtained under infalling accretion are slightly darker than those under static accretion, attributed to the Doppler effect.","Throughout the study, we analyze the influence of the parameters $B$ and $q$ on the results."],"url":"http://arxiv.org/abs/2401.18066v1","category":"gr-qc"}
{"created":"2024-01-31 18:41:22","title":"Game susceptibility, Correlation and Payoff capacity as a measure of Cooperative behavior in the thermodynamic limit of some Social dilemmas","abstract":"Analytically, finding the origins of cooperative behavior in infinite-player games is an exciting topic of current interest. Previously, cooperative behavior has been studied by considering game magnetization and individual player's average payoff as indicators. This paper shows that game susceptibility, correlation, and payoff capacity can aid in understanding cooperative behavior in social dilemmas in the thermodynamic limit. In this paper, we compare three analytical methods, i.e., Nash equilibrium mapping (NEM), Darwinian selection (DS), and Aggregate selection (AS), with a numerical-based method (ABM) via the game susceptibility, correlation, and payoff capacity as indicators of cooperative behavior. AS and DS fail compared to NEM and ABM by giving incorrect results for the indicators in question. The results obtained via NEM and ABM are in good agreement for all three indicators in question, for both Hawk-Dove and the Public goods games. After comparing the results obtained for all five indicators, we see that individual players' average payoff and payoff capacity are the best indicators to study cooperative behavior among players in the thermodynamic limit. This paper finds that NEM and ABM, along with the selected indicators, offer valuable insights into cooperative behavior in infinite-player games, contributing to understanding social dilemmas in the thermodynamic limit.","sentences":["Analytically, finding the origins of cooperative behavior in infinite-player games is an exciting topic of current interest.","Previously, cooperative behavior has been studied by considering game magnetization and individual player's average payoff as indicators.","This paper shows that game susceptibility, correlation, and payoff capacity can aid in understanding cooperative behavior in social dilemmas in the thermodynamic limit.","In this paper, we compare three analytical methods, i.e., Nash equilibrium mapping (NEM), Darwinian selection (DS), and Aggregate selection (AS), with a numerical-based method (ABM) via the game susceptibility, correlation, and payoff capacity as indicators of cooperative behavior.","AS and DS fail compared to NEM and ABM by giving incorrect results for the indicators in question.","The results obtained via NEM and ABM are in good agreement for all three indicators in question, for both Hawk-Dove and the Public goods games.","After comparing the results obtained for all five indicators, we see that individual players' average payoff and payoff capacity are the best indicators to study cooperative behavior among players in the thermodynamic limit.","This paper finds that NEM and ABM, along with the selected indicators, offer valuable insights into cooperative behavior in infinite-player games, contributing to understanding social dilemmas in the thermodynamic limit."],"url":"http://arxiv.org/abs/2401.18065v1","category":"cond-mat.stat-mech"}
{"created":"2024-01-31 18:41:08","title":"Neural Locality Sensitive Hashing for Entity Blocking","abstract":"Locality-sensitive hashing (LSH) is a fundamental algorithmic technique widely employed in large-scale data processing applications, such as nearest-neighbor search, entity resolution, and clustering. However, its applicability in some real-world scenarios is limited due to the need for careful design of hashing functions that align with specific metrics. Existing LSH-based Entity Blocking solutions primarily rely on generic similarity metrics such as Jaccard similarity, whereas practical use cases often demand complex and customized similarity rules surpassing the capabilities of generic similarity metrics. Consequently, designing LSH functions for these customized similarity rules presents considerable challenges. In this research, we propose a neuralization approach to enhance locality-sensitive hashing by training deep neural networks to serve as hashing functions for complex metrics. We assess the effectiveness of this approach within the context of the entity resolution problem, which frequently involves the use of task-specific metrics in real-world applications. Specifically, we introduce NLSHBlock (Neural-LSH Block), a novel blocking methodology that leverages pre-trained language models, fine-tuned with a novel LSH-based loss function. Through extensive evaluations conducted on a diverse range of real-world datasets, we demonstrate the superiority of NLSHBlock over existing methods, exhibiting significant performance improvements. Furthermore, we showcase the efficacy of NLSHBlock in enhancing the performance of the entity matching phase, particularly within the semi-supervised setting.","sentences":["Locality-sensitive hashing (LSH) is a fundamental algorithmic technique widely employed in large-scale data processing applications, such as nearest-neighbor search, entity resolution, and clustering.","However, its applicability in some real-world scenarios is limited due to the need for careful design of hashing functions that align with specific metrics.","Existing LSH-based Entity Blocking solutions primarily rely on generic similarity metrics such as Jaccard similarity, whereas practical use cases often demand complex and customized similarity rules surpassing the capabilities of generic similarity metrics.","Consequently, designing LSH functions for these customized similarity rules presents considerable challenges.","In this research, we propose a neuralization approach to enhance locality-sensitive hashing by training deep neural networks to serve as hashing functions for complex metrics.","We assess the effectiveness of this approach within the context of the entity resolution problem, which frequently involves the use of task-specific metrics in real-world applications.","Specifically, we introduce NLSHBlock (Neural-LSH Block), a novel blocking methodology that leverages pre-trained language models, fine-tuned with a novel LSH-based loss function.","Through extensive evaluations conducted on a diverse range of real-world datasets, we demonstrate the superiority of NLSHBlock over existing methods, exhibiting significant performance improvements.","Furthermore, we showcase the efficacy of NLSHBlock in enhancing the performance of the entity matching phase, particularly within the semi-supervised setting."],"url":"http://arxiv.org/abs/2401.18064v1","category":"cs.IR"}
{"created":"2024-01-31 18:37:39","title":"AoII-Optimum Sampling of CTMC Information Sources Under Sampling Rate Constraints","abstract":"We consider a sensor that samples an $N$-state continuous-time Markov chain (CTMC)-based information source process, and transmits the observed state of the source, to a remote monitor tasked with timely tracking of the source process. The mismatch between the source and monitor processes is quantified by age of incorrect information (AoII), which penalizes the mismatch as it stays longer, and our objective is to minimize the average AoII under an average sampling rate constraint. We assume a perfect reverse channel and hence the sensor has information of the estimate while initiating a transmission or preempting an ongoing transmission. First, by modeling the problem as an average cost constrained semi-Markov decision process (CSMDP), we show that the structure of the problem gives rise to an optimum threshold policy for which the sensor initiates a transmission once the AoII exceeds a threshold depending on the instantaneous values of both the source and monitor processes. However, due to the high complexity of obtaining the optimum policy in this general setting, we consider a relaxed problem where the thresholds are allowed to be dependent only on the estimate. We show that this relaxed problem can be solved with a novel CSMDP formulation based on the theory of absorbing MCs, with a computational complexity of $\\mathcal{O}(N^4)$, allowing one to obtain optimum policies for general CTMCs with over a hundred states.","sentences":["We consider a sensor that samples an $N$-state continuous-time Markov chain (CTMC)-based information source process, and transmits the observed state of the source, to a remote monitor tasked with timely tracking of the source process.","The mismatch between the source and monitor processes is quantified by age of incorrect information (AoII), which penalizes the mismatch as it stays longer, and our objective is to minimize the average AoII under an average sampling rate constraint.","We assume a perfect reverse channel and hence the sensor has information of the estimate while initiating a transmission or preempting an ongoing transmission.","First, by modeling the problem as an average cost constrained semi-Markov decision process (CSMDP), we show that the structure of the problem gives rise to an optimum threshold policy for which the sensor initiates a transmission once the AoII exceeds a threshold depending on the instantaneous values of both the source and monitor processes.","However, due to the high complexity of obtaining the optimum policy in this general setting, we consider a relaxed problem where the thresholds are allowed to be dependent only on the estimate.","We show that this relaxed problem can be solved with a novel CSMDP formulation based on the theory of absorbing MCs, with a computational complexity of $\\mathcal{O}(N^4)$, allowing one to obtain optimum policies for general CTMCs with over a hundred states."],"url":"http://arxiv.org/abs/2401.18063v1","category":"cs.IT"}
{"created":"2024-01-31 18:30:21","title":"RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval","abstract":"Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.","sentences":["Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge.","However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context.","We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up.","At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction.","Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks.","On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy."],"url":"http://arxiv.org/abs/2401.18059v1","category":"cs.CL"}
{"created":"2024-01-31 18:29:39","title":"LongAlign: A Recipe for Long Context Alignment of Large Language Models","abstract":"Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.","sentences":["Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.","To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment.","First, we construct a long instruction-following dataset using Self-Instruct.","To ensure the data diversity, it covers a broad range of tasks from various long context sources.","Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions.","Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training.","Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length.","Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks.","The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign."],"url":"http://arxiv.org/abs/2401.18058v1","category":"cs.CL"}
{"created":"2024-01-31 18:29:10","title":"Rank Supervised Contrastive Learning for Time Series Classification","abstract":"Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different weights for different levels of positive samples, enable the encoder to extract the fine-grained information of the same class, and produce a clear boundary among different classes. Thoroughly empirical studies on 128 UCR datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods.","sentences":["Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance.","A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart.","Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited.","To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification.","Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples.","Moreover, a novel rank loss is developed to assign different weights for different levels of positive samples, enable the encoder to extract the fine-grained information of the same class, and produce a clear boundary among different classes.","Thoroughly empirical studies on 128 UCR datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods."],"url":"http://arxiv.org/abs/2401.18057v1","category":"cs.LG"}
{"created":"2024-01-31 18:28:16","title":"Multi-zone trapped-ion qubit control in an integrated photonics QCCD device","abstract":"Multiplexed operations and extended coherent control over multiple trapping sites are fundamental requirements for a trapped-ion processor in a large scale architecture. Here we demonstrate these building blocks using a surface electrode trap with integrated photonic components which are scalable to larger numbers of zones. We implement a Ramsey sequence using the integrated light in two zones, separated by 375 $\\mu$m, performing transport of the ion from one zone to the other in 200 $\\mu$s between pulses. In order to achieve low motional excitation during transport we developed techniques to measure and mitigate the effect of the exposed dielectric surfaces used to deliver the integrated light to the ion. We also demonstrate simultaneous control of two ions in separate zones with low optical crosstalk, and use this to perform simultaneous spectroscopy to correlate field noise between the two sites. Our work demonstrates the first transport and coherent multi-zone operations in integrated photonic ion trap systems, forming the basis for further scaling in the trapped-ion QCCD architecture.","sentences":["Multiplexed operations and extended coherent control over multiple trapping sites are fundamental requirements for a trapped-ion processor in a large scale architecture.","Here we demonstrate these building blocks using a surface electrode trap with integrated photonic components which are scalable to larger numbers of zones.","We implement a Ramsey sequence using the integrated light in two zones, separated by 375 $\\mu$m, performing transport of the ion from one zone to the other in 200 $\\mu$s between pulses.","In order to achieve low motional excitation during transport we developed techniques to measure and mitigate the effect of the exposed dielectric surfaces used to deliver the integrated light to the ion.","We also demonstrate simultaneous control of two ions in separate zones with low optical crosstalk, and use this to perform simultaneous spectroscopy to correlate field noise between the two sites.","Our work demonstrates the first transport and coherent multi-zone operations in integrated photonic ion trap systems, forming the basis for further scaling in the trapped-ion QCCD architecture."],"url":"http://arxiv.org/abs/2401.18056v1","category":"quant-ph"}
{"created":"2024-01-31 18:20:42","title":"Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition","abstract":"Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task-order robust methods can still be class-order sensitive and observe results that contradict previous empirical observations on architectural sensitivity in CL.","sentences":["Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch.","Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL.","Thus, we focus on studying GNN in the continual graph learning (CGL) setting.","We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting.","The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition.","Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths.","We reveal that task-order robust methods can still be class-order sensitive and observe results that contradict previous empirical observations on architectural sensitivity in CL."],"url":"http://arxiv.org/abs/2401.18054v1","category":"cs.CV"}
{"created":"2024-01-31 18:20:36","title":"How to Measure TLS, X.509 Certificates, and Web PKI: A Tutorial and Brief Survey","abstract":"Transport Layer Security (TLS) is the base for many Internet applications and services to achieve end-to-end security. In this paper, we provide guidance on how to measure TLS deployments, including X.509 certificates and Web PKI. We introduce common data sources and tools, and systematically describe necessary steps to conduct sound measurements and data analysis. By surveying prior TLS measurement studies we find that diverging results are rather rooted in different setups instead of different deployments. To improve the situation, we identify common pitfalls and introduce a framework to describe TLS and Web PKI measurements. Where necessary, our insights are bolstered by a data-driven approach, in which we complement arguments by additional measurements.","sentences":["Transport Layer Security (TLS) is the base for many Internet applications and services to achieve end-to-end security.","In this paper, we provide guidance on how to measure TLS deployments, including X.509 certificates and Web PKI.","We introduce common data sources and tools, and systematically describe necessary steps to conduct sound measurements and data analysis.","By surveying prior TLS measurement studies we find that diverging results are rather rooted in different setups instead of different deployments.","To improve the situation, we identify common pitfalls and introduce a framework to describe TLS and Web PKI measurements.","Where necessary, our insights are bolstered by a data-driven approach, in which we complement arguments by additional measurements."],"url":"http://arxiv.org/abs/2401.18053v1","category":"cs.NI"}
{"created":"2024-01-31 18:14:36","title":"An electrodynamic wave model for the action potential","abstract":"An alternative to the standard Hodgkin-Huxley model for the axon action potential is presented. It is based upon our recently developed theory of electric field wave propagation in anisotropic and inhomogeneous brain tissues that has been shown to explain a broad range of observed coherent synchronous brain electrical processes. Here we demonstrate that this theory also explains the spiking behavior of single neurons, thus bridging the gap between the fundamental element of brain electrical activity (the neuron) and large scale coherent synchronous electrical activity. This new theory overcomes the limitations of the Hodgkin-Huxley model such as the inability to explain extracellular spiking, efficient brain synchronization, saltatory conduction along myelinated axons, and a variety of other observed coherent macroscopic brain electrical phenomena. We also show that the standard cable axon theory can be recovered by our approach using the very crude assumptions of the piece-wise homogeneity and isotropy, but the diffusion process that the cable equation describes is not capable of supporting the action potential propagation in a wide range of experimentally reported axon parameters.","sentences":["An alternative to the standard Hodgkin-Huxley model for the axon action potential is presented.","It is based upon our recently developed theory of electric field wave propagation in anisotropic and inhomogeneous brain tissues that has been shown to explain a broad range of observed coherent synchronous brain electrical processes.","Here we demonstrate that this theory also explains the spiking behavior of single neurons, thus bridging the gap between the fundamental element of brain electrical activity (the neuron) and large scale coherent synchronous electrical activity.","This new theory overcomes the limitations of the Hodgkin-Huxley model such as the inability to explain extracellular spiking, efficient brain synchronization, saltatory conduction along myelinated axons, and a variety of other observed coherent macroscopic brain electrical phenomena.","We also show that the standard cable axon theory can be recovered by our approach using the very crude assumptions of the piece-wise homogeneity and isotropy, but the diffusion process that the cable equation describes is not capable of supporting the action potential propagation in a wide range of experimentally reported axon parameters."],"url":"http://arxiv.org/abs/2401.18051v1","category":"physics.bio-ph"}
{"created":"2024-01-31 18:14:13","title":"Hypermultiplexed Integrated Tensor Optical Processor","abstract":"Optical processors hold great potential to accelerate deep learning tasks with their high clock-rates and low-loss data transmission. However, existing integrated systems are hindered by low scalability due to the quadratic scaling of device counts, energy costs with high-speed analog-to-digital converters, and lack of inline nonlinearity. Here, we overcome these challenges with a wavelength-space-time multiplexed optical tensor processor. Hyperdimensional parallelism allows matrix-matrix multiplications ($N^{3}$ operations) using $O(N)$ devices. We incorporated wavelength-multiplexed III/V-based micron-scale lasers (spanning ~1 THz) for input activation with inline rectifier (ReLU) nonlinearities and thin-film Lithium-Niobate electro-optic modulators ($V_{\\pi}\\approx1.3 V$) for dynamic weighting. With each device encoding 10-billion activations per second, we demonstrated a machine-learning model with 405,000 parameters. High-clock-rate (10 GS/s), low-energy (500 fJ/OP) parallel computing with real-time programmability unlocks the full potential of light for next-generation scalable AI accelerators.","sentences":["Optical processors hold great potential to accelerate deep learning tasks with their high clock-rates and low-loss data transmission.","However, existing integrated systems are hindered by low scalability due to the quadratic scaling of device counts, energy costs with high-speed analog-to-digital converters, and lack of inline nonlinearity.","Here, we overcome these challenges with a wavelength-space-time multiplexed optical tensor processor.","Hyperdimensional parallelism allows matrix-matrix multiplications ($N^{3}$ operations) using $O(N)$ devices.","We incorporated wavelength-multiplexed III/V-based micron-scale lasers (spanning ~1 THz) for input activation with inline rectifier (ReLU) nonlinearities and thin-film Lithium-Niobate electro-optic modulators ($V_{\\pi}\\approx1.3 V$) for dynamic weighting.","With each device encoding 10-billion activations per second, we demonstrated a machine-learning model with 405,000 parameters.","High-clock-rate (10 GS/s), low-energy (500 fJ/OP) parallel computing with real-time programmability unlocks the full potential of light for next-generation scalable AI accelerators."],"url":"http://arxiv.org/abs/2401.18050v1","category":"cs.ET"}
{"created":"2024-01-31 18:13:42","title":"Enhanced observable estimation through classical optimization of informationally over-complete measurement data -- beyond classical shadows","abstract":"In recent years, informationally complete measurements have attracted considerable attention, especially in the context of classical shadows. In the particular case of informationally over-complete measurements, for which the number of possible outcomes exceeds the dimension of the space of linear operators in Hilbert space, the dual POVM operators used to interpret the measurement outcomes are not uniquely defined. In this work, we propose a method to optimize the dual operators after the measurements have been carried out in order to produce sharper, unbiased estimations of observables of interest. We discuss how this procedure can produce zero-variance estimations in cases where the classical shadows formalism, which relies on so-called canonical duals, incurs exponentially large measurement overheads. We also analyze the algorithm in the context of quantum simulation with randomized Pauli measurements, and show that it can significantly reduce statistical errors with respect to canonical duals on multiple observable estimations.","sentences":["In recent years, informationally complete measurements have attracted considerable attention, especially in the context of classical shadows.","In the particular case of informationally over-complete measurements, for which the number of possible outcomes exceeds the dimension of the space of linear operators in Hilbert space, the dual POVM operators used to interpret the measurement outcomes are not uniquely defined.","In this work, we propose a method to optimize the dual operators after the measurements have been carried out in order to produce sharper, unbiased estimations of observables of interest.","We discuss how this procedure can produce zero-variance estimations in cases where the classical shadows formalism, which relies on so-called canonical duals, incurs exponentially large measurement overheads.","We also analyze the algorithm in the context of quantum simulation with randomized Pauli measurements, and show that it can significantly reduce statistical errors with respect to canonical duals on multiple observable estimations."],"url":"http://arxiv.org/abs/2401.18049v1","category":"quant-ph"}
{"created":"2024-01-31 18:08:06","title":"Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning","abstract":"Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary. To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning. The model mainly caters to three objectives for better prediction: 1. Periodic estimation of the model parameters. 2. Incorporating impact of all the aspects using data fitting and parameter optimization 3. Deep learning based prediction of the model parameters. In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters. Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic. So, we estimate the model parameters periodically (weekly). We use PSO to identify the optimum values of the model parameters. We next train the stacked-LSTM on the optimized parameters, and perform forecasting of the model parameters for upcoming four weeks. Further, we fed the LSTM forecasted parameters into the SIRD model to forecast the number of COVID-19 cases. We evaluate the model for highly affected three countries namely; the USA, India, and the UK. The proposed hybrid model is able to deal with multiple waves, and has outperformed existing methods on all the three datasets.","sentences":["Epidemiological models are best suitable to model an epidemic if the spread pattern is stationary.","To deal with non-stationary patterns and multiple waves of an epidemic, we develop a hybrid model encompassing epidemic modeling, particle swarm optimization, and deep learning.","The model mainly caters to three objectives for better prediction:","1.","Periodic estimation of the model parameters.","2. Incorporating impact of all the aspects using data fitting and parameter optimization 3.","Deep learning based prediction of the model parameters.","In our model, we use a system of ordinary differential equations (ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling, Particle Swarm Optimization (PSO) for model parameter optimization, and stacked-LSTM for forecasting the model parameters.","Initial or one time estimation of model parameters is not able to model multiple waves of an epidemic.","So, we estimate the model parameters periodically (weekly).","We use PSO to identify the optimum values of the model parameters.","We next train the stacked-LSTM on the optimized parameters, and perform forecasting of the model parameters for upcoming four weeks.","Further, we fed the LSTM forecasted parameters into the SIRD model to forecast the number of COVID-19 cases.","We evaluate the model for highly affected three countries namely; the USA, India, and the UK.","The proposed hybrid model is able to deal with multiple waves, and has outperformed existing methods on all the three datasets."],"url":"http://arxiv.org/abs/2401.18047v1","category":"cs.LG"}
{"created":"2024-01-31 18:07:12","title":"Multipath parsing in the brain","abstract":"Humans understand sentences word-by-word, in the order that they hear them. This incrementality entails resolving temporary ambiguities about syntactic relationships. We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook. In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one. This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset. In both English and Chinese data, we find evidence for multipath parsing. Brain regions associated with this multipath effect include bilateral superior temporal gyrus.","sentences":["Humans understand sentences word-by-word, in the order that they hear them.","This incrementality entails resolving temporary ambiguities about syntactic relationships.","We investigate how humans process these syntactic ambiguities by correlating predictions from incremental generative dependency parsers with timecourse data from people undergoing functional neuroimaging while listening to an audiobook.","In particular, we compare competing hypotheses regarding the number of developing syntactic analyses in play during word-by-word comprehension: one vs more than one.","This comparison involves evaluating syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted encodings against an existing fMRI dataset.","In both English and Chinese data, we find evidence for multipath parsing.","Brain regions associated with this multipath effect include bilateral superior temporal gyrus."],"url":"http://arxiv.org/abs/2401.18046v1","category":"cs.CL"}
{"created":"2024-01-31 18:06:29","title":"SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition","abstract":"Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our proposed SpeechComposer can improve the performance of both primary tasks and composite tasks, showing the effectiveness of the shared prompt tokens. Remarkably, the unified decoder-only model achieves a comparable and even better performance than the baselines which are expert models designed for single tasks.","sentences":["Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks.","Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model.","However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task.","In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens.","Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement.","The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner.","Experimental results demonstrate that our proposed SpeechComposer can improve the performance of both primary tasks and composite tasks, showing the effectiveness of the shared prompt tokens.","Remarkably, the unified decoder-only model achieves a comparable and even better performance than the baselines which are expert models designed for single tasks."],"url":"http://arxiv.org/abs/2401.18045v1","category":"cs.CL"}
{"created":"2024-01-31 18:06:28","title":"Observation of the scaling dimension of fractional quantum Hall anyons","abstract":"Unconventional quasiparticles emerging in the fractional quantum Hall regime present the challenge of observing their exotic properties unambiguously. Although the fractional charge of quasiparticles has been demonstrated since nearly three decades, the first convincing evidence of their anyonic quantum statistics has only recently been obtained and, so far, the so-called scaling dimension that determines the quasiparticles propagation dynamics remains elusive. In particular, while the non-linearity of the tunneling quasiparticle current should reveal their scaling dimension, the measurements fail to match theory, arguably because this observable is not robust to non-universal complications. Here we achieve an unambiguous measurement of the scaling dimension from the thermal to shot noise cross-over, and observe a long-awaited agreement with expectations. Measurements are fitted to the predicted finite temperature expression involving both the quasiparticles scaling dimension and their charge, in contrast to previous charge investigations focusing on the high bias shot noise regime. A systematic analysis, repeated on multiple constrictions and experimental conditions, consistently matches the theoretical scaling dimensions for the fractional quasiparticles emerging at filling factors 1/3, 2/5 and 2/3. This establishes a central property of fractional quantum Hall anyons, and demonstrates a powerful and complementary window into exotic quasiparticles.","sentences":["Unconventional quasiparticles emerging in the fractional quantum Hall regime present the challenge of observing their exotic properties unambiguously.","Although the fractional charge of quasiparticles has been demonstrated since nearly three decades, the first convincing evidence of their anyonic quantum statistics has only recently been obtained and, so far, the so-called scaling dimension that determines the quasiparticles propagation dynamics remains elusive.","In particular, while the non-linearity of the tunneling quasiparticle current should reveal their scaling dimension, the measurements fail to match theory, arguably because this observable is not robust to non-universal complications.","Here we achieve an unambiguous measurement of the scaling dimension from the thermal to shot noise cross-over, and observe a long-awaited agreement with expectations.","Measurements are fitted to the predicted finite temperature expression involving both the quasiparticles scaling dimension and their charge, in contrast to previous charge investigations focusing on the high bias shot noise regime.","A systematic analysis, repeated on multiple constrictions and experimental conditions, consistently matches the theoretical scaling dimensions for the fractional quasiparticles emerging at filling factors 1/3, 2/5 and 2/3.","This establishes a central property of fractional quantum Hall anyons, and demonstrates a powerful and complementary window into exotic quasiparticles."],"url":"http://arxiv.org/abs/2401.18044v1","category":"cond-mat.mes-hall"}
{"created":"2024-01-31 18:03:39","title":"Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability","abstract":"End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, show that intrinsic motivation-based debate systems outperform policies that depend on extrinsic incentives. By adopting random network distillation, for example, which is trained using semantic similarity between user-system dialogues, an astounding average success rate of 73% is achieved. This is a significant improvement over the baseline Proximal Policy Optimization (PPO), which has an average success rate of 60%. In addition, performance indicators such as booking rates and completion rates show a 10% rise over the baseline. Furthermore, these intrinsic incentive models help improve the system's policy's resilience in an increasing amount of domains. This implies that they could be useful in scaling up to settings that cover a wider range of domains.","sentences":["End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline.","Among these, the policy module is essential for deciding what to do in response to user input.","This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal.","The current dialogue systems, however, only provide meagre and simplistic rewards.","Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study.","Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system.","In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances.","Experimental results on MultiWOZ, a heterogeneous dataset, show that intrinsic motivation-based debate systems outperform policies that depend on extrinsic incentives.","By adopting random network distillation, for example, which is trained using semantic similarity between user-system dialogues, an astounding average success rate of 73% is achieved.","This is a significant improvement over the baseline Proximal Policy Optimization (PPO), which has an average success rate of 60%.","In addition, performance indicators such as booking rates and completion rates show a 10% rise over the baseline.","Furthermore, these intrinsic incentive models help improve the system's policy's resilience in an increasing amount of domains.","This implies that they could be useful in scaling up to settings that cover a wider range of domains."],"url":"http://arxiv.org/abs/2401.18040v1","category":"cs.CL"}
{"created":"2024-01-31 18:01:36","title":"Variable selection for Na\u00efve Bayes classification","abstract":"The Na\\\"ive Bayes has proven to be a tractable and efficient method for classification in multivariate analysis. However, features are usually correlated, a fact that violates the Na\\\"ive Bayes' assumption of conditional independence, and may deteriorate the method's performance. Moreover, datasets are often characterized by a large number of features, which may complicate the interpretation of the results as well as slow down the method's execution.   In this paper we propose a sparse version of the Na\\\"ive Bayes classifier that is characterized by three properties. First, the sparsity is achieved taking into account the correlation structure of the covariates. Second, different performance measures can be used to guide the selection of features. Third, performance constraints on groups of higher interest can be included. Our proposal leads to a smart search, which yields competitive running times, whereas the flexibility in terms of performance measure for classification is integrated. Our findings show that, when compared against well-referenced feature selection approaches, the proposed sparse Na\\\"ive Bayes obtains competitive results regarding accuracy, sparsity and running times for balanced datasets. In the case of datasets with unbalanced (or with different importance) classes, a better compromise between classification rates for the different classes is achieved.","sentences":["The Na\\\"ive Bayes has proven to be a tractable and efficient method for classification in multivariate analysis.","However, features are usually correlated, a fact that violates the Na\\\"ive Bayes' assumption of conditional independence, and may deteriorate the method's performance.","Moreover, datasets are often characterized by a large number of features, which may complicate the interpretation of the results as well as slow down the method's execution.   ","In this paper we propose a sparse version of the Na\\\"ive Bayes classifier that is characterized by three properties.","First, the sparsity is achieved taking into account the correlation structure of the covariates.","Second, different performance measures can be used to guide the selection of features.","Third, performance constraints on groups of higher interest can be included.","Our proposal leads to a smart search, which yields competitive running times, whereas the flexibility in terms of performance measure for classification is integrated.","Our findings show that, when compared against well-referenced feature selection approaches, the proposed sparse Na\\\"ive Bayes obtains competitive results regarding accuracy, sparsity and running times for balanced datasets.","In the case of datasets with unbalanced (or with different importance) classes, a better compromise between classification rates for the different classes is achieved."],"url":"http://arxiv.org/abs/2401.18039v1","category":"stat.ML"}
{"created":"2024-01-31 17:59:57","title":"Optimizing contrastive learning for cortical folding pattern detection","abstract":"The human cerebral cortex has many bumps and grooves called gyri and sulci. Even though there is a high inter-individual consistency for the main cortical folds, this is not the case when we examine the exact shapes and details of the folding patterns. Because of this complexity, characterizing the cortical folding variability and relating them to subjects' behavioral characteristics or pathologies is still an open scientific problem. Classical approaches include labeling a few specific patterns, either manually or semi-automatically, based on geometric distances, but the recent availability of MRI image datasets of tens of thousands of subjects makes modern deep-learning techniques particularly attractive. Here, we build a self-supervised deep-learning model to detect folding patterns in the cingulate region. We train a contrastive self-supervised model (SimCLR) on both Human Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with topological-based augmentations on the cortical skeletons, which are topological objects that capture the shape of the folds. We explore several backbone architectures (convolutional network, DenseNet, and PointNet) for the SimCLR. For evaluation and testing, we perform a linear classification task on a database manually labeled for the presence of the \"double-parallel\" folding pattern in the cingulate region, which is related to schizophrenia characteristics. The best model, giving a test AUC of 0.76, is a convolutional network with 6 layers, a 10-dimensional latent space, a linear projection head, and using the branch-clipping augmentation. This is the first time that a self-supervised deep learning model has been applied to cortical skeletons on such a large dataset and quantitatively evaluated. We can now envisage the next step: applying it to other brain regions to detect other biomarkers.","sentences":["The human cerebral cortex has many bumps and grooves called gyri and sulci.","Even though there is a high inter-individual consistency for the main cortical folds, this is not the case when we examine the exact shapes and details of the folding patterns.","Because of this complexity, characterizing the cortical folding variability and relating them to subjects' behavioral characteristics or pathologies is still an open scientific problem.","Classical approaches include labeling a few specific patterns, either manually or semi-automatically, based on geometric distances, but the recent availability of MRI image datasets of tens of thousands of subjects makes modern deep-learning techniques particularly attractive.","Here, we build a self-supervised deep-learning model to detect folding patterns in the cingulate region.","We train a contrastive self-supervised model (SimCLR) on both Human Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with topological-based augmentations on the cortical skeletons, which are topological objects that capture the shape of the folds.","We explore several backbone architectures (convolutional network, DenseNet, and PointNet) for the SimCLR.","For evaluation and testing, we perform a linear classification task on a database manually labeled for the presence of the \"double-parallel\" folding pattern in the cingulate region, which is related to schizophrenia characteristics.","The best model, giving a test AUC of 0.76, is a convolutional network with 6 layers, a 10-dimensional latent space, a linear projection head, and using the branch-clipping augmentation.","This is the first time that a self-supervised deep learning model has been applied to cortical skeletons on such a large dataset and quantitatively evaluated.","We can now envisage the next step: applying it to other brain regions to detect other biomarkers."],"url":"http://arxiv.org/abs/2401.18035v1","category":"cs.LG"}
{"created":"2024-01-31 17:58:10","title":"Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models","abstract":"We present Gyan AI Paramanu (\"atom\"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics for Bangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models outperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B, GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite being smaller in size by 66 to 20 times compared to standard 7B LLMs. To run inference on our pretrained models, CPU is enough, and GPU is not needed. We also instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu models on 23k instructions in respective languages. Our pretrained and instruction-tuned models which are first of its kind, most powerful efficient small generative language models ever developed for Indic languages, and the various results lead to the conclusion that high quality generative language models are possible without high amount of compute power and humongous number of parameters. We plan to release our models at https://www.bharatgpts.com.","sentences":["We present Gyan AI Paramanu (\"atom\"), a family of novel language models for Indian languages.","It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU.","The models are very efficient, small, fast, and powerful.","We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages.","In order to avoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script.","We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics for Bangla, Hindi, and Sanskrit.","Our Bangla, Hindi, and Sanskrit models outperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B, GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite being smaller in size by 66 to 20 times compared to standard 7B LLMs.","To run inference on our pretrained models, CPU is enough, and GPU is not needed.","We also instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu models on 23k instructions in respective languages.","Our pretrained and instruction-tuned models which are first of its kind, most powerful efficient small generative language models ever developed for Indic languages, and the various results lead to the conclusion that high quality generative language models are possible without high amount of compute power and humongous number of parameters.","We plan to release our models at https://www.bharatgpts.com."],"url":"http://arxiv.org/abs/2401.18034v1","category":"cs.CL"}
{"created":"2024-01-31 17:54:43","title":"DROP: Decouple Re-Identification and Human Parsing with Task-specific Features for Occluded Person Re-identification","abstract":"The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP) method for occluded person re-identification (ReID). Unlike mainstream approaches using global features for simultaneous multi-task learning of ReID and human parsing, or relying on semantic information for attention guidance, DROP argues that the inferior performance of the former is due to distinct granularity requirements for ReID and human parsing features. ReID focuses on instance part-level differences between pedestrian parts, while human parsing centers on semantic spatial context, reflecting the internal structure of the human body. To address this, DROP decouples features for ReID and human parsing, proposing detail-preserving upsampling to combine varying resolution feature maps. Parsing-specific features for human parsing are decoupled, and human position information is exclusively added to the human parsing branch. In the ReID branch, a part-aware compactness loss is introduced to enhance instance-level part differences. Experimental results highlight the efficacy of DROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke, surpassing two mainstream methods. The codebase is accessible at https://github.com/shuguang-52/DROP.","sentences":["The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP) method for occluded person re-identification (ReID).","Unlike mainstream approaches using global features for simultaneous multi-task learning of ReID and human parsing, or relying on semantic information for attention guidance, DROP argues that the inferior performance of the former is due to distinct granularity requirements for ReID and human parsing features.","ReID focuses on instance part-level differences between pedestrian parts, while human parsing centers on semantic spatial context, reflecting the internal structure of the human body.","To address this, DROP decouples features for ReID and human parsing, proposing detail-preserving upsampling to combine varying resolution feature maps.","Parsing-specific features for human parsing are decoupled, and human position information is exclusively added to the human parsing branch.","In the ReID branch, a part-aware compactness loss is introduced to enhance instance-level part differences.","Experimental results highlight the efficacy of DROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke, surpassing two mainstream methods.","The codebase is accessible at https://github.com/shuguang-52/DROP."],"url":"http://arxiv.org/abs/2401.18032v1","category":"cs.CV"}
{"created":"2024-01-31 17:49:09","title":"Distributed fixed-point algorithms for dynamic convex optimization over decentralized and unbalanced wireless networks","abstract":"We consider problems where agents in a network seek a common quantity, measured independently and periodically by each agent through a local time-varying process. Numerous solvers addressing such problems have been developed in the past, featuring various adaptations of the local processing and the consensus step. However, existing solvers still lack support for advanced techniques, such as superiorization and over-the-air function computation (OTA-C). To address this limitation, we introduce a comprehensive framework for the analysis of distributed algorithms by characterizing them using the quasi-Fej\\'er type algorithms and an extensive communication model. Under weak assumptions, we prove almost sure convergence of the algorithm to a common estimate for all agents. Moreover, we develop a specific class of algorithms within this framework to tackle distributed optimization problems with time-varying objectives, and, assuming that a time-invariant solution exists, prove its convergence to a solution. We also present a novel OTA-C protocol for consensus step in large decentralized networks, reducing communication overhead and enhancing network autonomy as compared to the existing protocols. The effectiveness of the algorithm, featuring superiorization and OTA-C, is demonstrated in a real-world application of distributed supervised learning over time-varying wireless networks, highlighting its low-latency and energy-efficiency compared to standard approaches.","sentences":["We consider problems where agents in a network seek a common quantity, measured independently and periodically by each agent through a local time-varying process.","Numerous solvers addressing such problems have been developed in the past, featuring various adaptations of the local processing and the consensus step.","However, existing solvers still lack support for advanced techniques, such as superiorization and over-the-air function computation (OTA-C).","To address this limitation, we introduce a comprehensive framework for the analysis of distributed algorithms by characterizing them using the quasi-Fej\\'er type algorithms and an extensive communication model.","Under weak assumptions, we prove almost sure convergence of the algorithm to a common estimate for all agents.","Moreover, we develop a specific class of algorithms within this framework to tackle distributed optimization problems with time-varying objectives, and, assuming that a time-invariant solution exists, prove its convergence to a solution.","We also present a novel OTA-C protocol for consensus step in large decentralized networks, reducing communication overhead and enhancing network autonomy as compared to the existing protocols.","The effectiveness of the algorithm, featuring superiorization and OTA-C, is demonstrated in a real-world application of distributed supervised learning over time-varying wireless networks, highlighting its low-latency and energy-efficiency compared to standard approaches."],"url":"http://arxiv.org/abs/2401.18030v1","category":"math.OC"}
{"created":"2024-01-31 17:45:05","title":"Context-Sensitive Abstract Interpretation of Dynamic Languages","abstract":"There is a vast gap in the quality of IDE tooling between static languages like Java and dynamic languages like Python or JavaScript. Modern frameworks and libraries in these languages heavily use their dynamic capabilities to achieve the best ergonomics and readability. This has a side effect of making the current generation of IDEs blind to control flow and data flow, which often breaks navigation, autocompletion and refactoring. In this thesis we propose an algorithm that can bridge this gap between tooling for dynamic and static languages by statically analyzing dynamic metaprogramming and runtime reflection in programs. We use a technique called abstract interpretation to partially execute programs and extract information that is usually only available at runtime. Our algorithm has been implemented in a prototype analyzer that can analyze programs written in a subset of JavaScript.","sentences":["There is a vast gap in the quality of IDE tooling between static languages like Java and dynamic languages like Python or JavaScript.","Modern frameworks and libraries in these languages heavily use their dynamic capabilities to achieve the best ergonomics and readability.","This has a side effect of making the current generation of IDEs blind to control flow and data flow, which often breaks navigation, autocompletion and refactoring.","In this thesis we propose an algorithm that can bridge this gap between tooling for dynamic and static languages by statically analyzing dynamic metaprogramming and runtime reflection in programs.","We use a technique called abstract interpretation to partially execute programs and extract information that is usually only available at runtime.","Our algorithm has been implemented in a prototype analyzer that can analyze programs written in a subset of JavaScript."],"url":"http://arxiv.org/abs/2401.18029v1","category":"cs.PL"}
{"created":"2024-01-31 17:43:04","title":"Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI","abstract":"Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3) using a sample from this baseline. We find that the generated impacts using Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a larger scale model such as GPT-4. Moreover, we find that these LLMs generate impacts that largely reflect the taxonomy of negative impacts identified in the news media, however the impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models. This research highlights a potential bias in state-of-the-art LLMs when used for anticipating impacts and demonstrates the advantages of aligning smaller LLMs with a diverse range of impacts, such as those reflected in the news media, to better reflect such impacts during anticipatory exercises.","sentences":["Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development.","An understudied approach to such anticipation is the use of LLMs to enhance and guide this process.","Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks.","Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating.","In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against.","By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts.","We then evaluate both instruction-based (GPT-4 and Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3) using a sample from this baseline.","We find that the generated impacts using Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively on par with impacts generated using a larger scale model such as GPT-4.","Moreover, we find that these LLMs generate impacts that largely reflect the taxonomy of negative impacts identified in the news media, however the impacts produced by instruction-based models had gaps in the production of certain categories of impacts in comparison to fine-tuned models.","This research highlights a potential bias in state-of-the-art LLMs when used for anticipating impacts and demonstrates the advantages of aligning smaller LLMs with a diverse range of impacts, such as those reflected in the news media, to better reflect such impacts during anticipatory exercises."],"url":"http://arxiv.org/abs/2401.18028v1","category":"cs.CL"}
{"created":"2024-01-31 17:41:39","title":"On affine Toda field theories related to ${\\bf D}_r$ algebras and their real Hamiltonian forms","abstract":"The paper deals with affine 2-dimensional Toda field theories related to simple Lie algebras of the classical series ${\\bf D}_r$. We demonstrate that the complexification procedure followed by a restriction to a specified real Hamiltonian form commutes with the external automorphisms of $\\mathfrak{g}$. This is illustrated on the examples ${\\bf D}_{r+1}^{(1)} \\to {\\bf B}_r^{(1)}$ and ${\\bf D}_4^{(1)} \\to {\\bf G}_2^{(1)}$ using external automorphisms of the corresponding extended Dynkin diagrams.","sentences":["The paper deals with affine 2-dimensional Toda field theories related to simple Lie algebras of the classical series ${\\bf D}_r$. We demonstrate that the complexification procedure followed by a restriction to a specified real Hamiltonian form commutes with the external automorphisms of $\\mathfrak{g}$. This is illustrated on the examples ${\\bf D}_{r+1}^{(1)} \\to {\\bf B}_r^{(1)}$ and ${\\bf D}_4^{(1)} \\to {\\bf G}_2^{(1)}$ using external automorphisms of the corresponding extended Dynkin diagrams."],"url":"http://arxiv.org/abs/2401.18027v1","category":"nlin.SI"}
{"created":"2024-01-31 17:38:34","title":"Benchmarking Private Population Data Release Mechanisms: Synthetic Data vs. TopDown","abstract":"Differential privacy (DP) is increasingly used to protect the release of hierarchical, tabular population data, such as census data. A common approach for implementing DP in this setting is to release noisy responses to a predefined set of queries. For example, this is the approach of the TopDown algorithm used by the US Census Bureau. Such methods have an important shortcoming: they cannot answer queries for which they were not optimized. An appealing alternative is to generate DP synthetic data, which is drawn from some generating distribution. Like the TopDown method, synthetic data can also be optimized to answer specific queries, while also allowing the data user to later submit arbitrary queries over the synthetic population data. To our knowledge, there has not been a head-to-head empirical comparison of these approaches. This study conducts such a comparison between the TopDown algorithm and private synthetic data generation to determine how accuracy is affected by query complexity, in-distribution vs. out-of-distribution queries, and privacy guarantees. Our results show that for in-distribution queries, the TopDown algorithm achieves significantly better privacy-fidelity tradeoffs than any of the synthetic data methods we evaluated; for instance, in our experiments, TopDown achieved at least $20\\times$ lower error on counting queries than the leading synthetic data method at the same privacy budget. Our findings suggest guidelines for practitioners and the synthetic data research community.","sentences":["Differential privacy (DP) is increasingly used to protect the release of hierarchical, tabular population data, such as census data.","A common approach for implementing DP in this setting is to release noisy responses to a predefined set of queries.","For example, this is the approach of the TopDown algorithm used by the US Census Bureau.","Such methods have an important shortcoming: they cannot answer queries for which they were not optimized.","An appealing alternative is to generate DP synthetic data, which is drawn from some generating distribution.","Like the TopDown method, synthetic data can also be optimized to answer specific queries, while also allowing the data user to later submit arbitrary queries over the synthetic population data.","To our knowledge, there has not been a head-to-head empirical comparison of these approaches.","This study conducts such a comparison between the TopDown algorithm and private synthetic data generation to determine how accuracy is affected by query complexity, in-distribution vs. out-of-distribution queries, and privacy guarantees.","Our results show that for in-distribution queries, the TopDown algorithm achieves significantly better privacy-fidelity tradeoffs than any of the synthetic data methods we evaluated; for instance, in our experiments, TopDown achieved at least $20\\times$ lower error on counting queries than the leading synthetic data method at the same privacy budget.","Our findings suggest guidelines for practitioners and the synthetic data research community."],"url":"http://arxiv.org/abs/2401.18024v1","category":"cs.CR"}
{"created":"2024-01-31 17:29:16","title":"Joining Entities Across Relation and Graph with a Unified Model","abstract":"This paper introduces RG (Relational Genetic) model, a revised relational model to represent graph-structured data in RDBMS while preserving its topology, for efficiently and effectively extracting data in different formats from disparate sources. Along with: (a) SQL$_\\delta$, an SQL dialect augmented with graph pattern queries and tuple-vertex joins, such that one can extract graph properties via graph pattern matching, and \"semantically\" match entities across relations and graphs; (b) a logical representation of graphs in RDBMS, which introduces an exploration operator for efficient pattern querying, supports also browsing and updating graph-structured data; and (c) a strategy to uniformly evaluate SQL, pattern and hybrid queries that join tuples and vertices, all inside an RDBMS by leveraging its optimizer without performance degradation on switching different execution engines. A lightweight system, WhiteDB, is developed as an implementation to evaluate the benefits it can actually bring on real-life data. We empirically verified that the RG model enables the graph pattern queries to be answered as efficiently as in native graph engines; can consider the access on graph and relation in any order for optimal plan; and supports effective data enrichment.","sentences":["This paper introduces RG (Relational Genetic) model, a revised relational model to represent graph-structured data in RDBMS while preserving its topology, for efficiently and effectively extracting data in different formats from disparate sources.","Along with: (a) SQL$_\\delta$, an SQL dialect augmented with graph pattern queries and tuple-vertex joins, such that one can extract graph properties via graph pattern matching, and \"semantically\" match entities across relations and graphs; (b) a logical representation of graphs in RDBMS, which introduces an exploration operator for efficient pattern querying, supports also browsing and updating graph-structured data; and (c) a strategy to uniformly evaluate SQL, pattern and hybrid queries that join tuples and vertices, all inside an RDBMS by leveraging its optimizer without performance degradation on switching different execution engines.","A lightweight system, WhiteDB, is developed as an implementation to evaluate the benefits it can actually bring on real-life data.","We empirically verified that the RG model enables the graph pattern queries to be answered as efficiently as in native graph engines; can consider the access on graph and relation in any order for optimal plan; and supports effective data enrichment."],"url":"http://arxiv.org/abs/2401.18019v1","category":"cs.DB"}
{"created":"2024-01-31 17:28:24","title":"Prompt-Driven LLM Safeguarding via Directed Representation Optimization","abstract":"Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts as continuous, trainable embeddings and learns to move the representations of harmful/harmless queries along/opposite the direction in which the model's refusal probability increases. We demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts and outperforms strong baselines, as evaluated on out-of-domain benchmarks, without compromising the general model capability.","sentences":["Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents.","However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety.","Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations.","We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts.","Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless.","Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization.","DRO treats safety prompts as continuous, trainable embeddings and learns to move the representations of harmful/harmless queries along/opposite the direction in which the model's refusal probability increases.","We demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts and outperforms strong baselines, as evaluated on out-of-domain benchmarks, without compromising the general model capability."],"url":"http://arxiv.org/abs/2401.18018v1","category":"cs.LG"}
{"created":"2024-01-31 17:28:05","title":"Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms","abstract":"The discovery of causal relationships in a set of random variables is a fundamental objective of science and has also recently been argued as being an essential component towards real machine intelligence. One class of causal discovery techniques are founded based on the argument that there are inherent structural asymmetries between the causal and anti-causal direction which could be leveraged in determining the direction of causation. To go about capturing these discrepancies between cause and effect remains to be a challenge and many current state-of-the-art algorithms propose to compare the norms of the kernel mean embeddings of the conditional distributions. In this work, we argue that such approaches based on RKHS embeddings are insufficient in capturing principal markers of cause-effect asymmetry involving higher-order structural variabilities of the conditional distributions. We propose Kernel Intrinsic Invariance Measure with Heterogeneous Transform (KIIM-HT) which introduces a novel score measure based on heterogeneous transformation of RKHS embeddings to extract relevant higher-order moments of the conditional densities for causal discovery. Inference is made via comparing the score of each hypothetical cause-effect direction. Tests and comparisons on a synthetic dataset, a two-dimensional synthetic dataset and the real-world benchmark dataset T\\\"ubingen Cause-Effect Pairs verify our approach. In addition, we conduct a sensitivity analysis to the regularization parameter to faithfully compare previous work to our method and an experiment with trials on varied hyperparameter values to showcase the robustness of our algorithm.","sentences":["The discovery of causal relationships in a set of random variables is a fundamental objective of science and has also recently been argued as being an essential component towards real machine intelligence.","One class of causal discovery techniques are founded based on the argument that there are inherent structural asymmetries between the causal and anti-causal direction which could be leveraged in determining the direction of causation.","To go about capturing these discrepancies between cause and effect remains to be a challenge and many current state-of-the-art algorithms propose to compare the norms of the kernel mean embeddings of the conditional distributions.","In this work, we argue that such approaches based on RKHS embeddings are insufficient in capturing principal markers of cause-effect asymmetry involving higher-order structural variabilities of the conditional distributions.","We propose Kernel Intrinsic Invariance Measure with Heterogeneous Transform (KIIM-HT) which introduces a novel score measure based on heterogeneous transformation of RKHS embeddings to extract relevant higher-order moments of the conditional densities for causal discovery.","Inference is made via comparing the score of each hypothetical cause-effect direction.","Tests and comparisons on a synthetic dataset, a two-dimensional synthetic dataset and the real-world benchmark dataset T\\\"ubingen Cause-Effect Pairs verify our approach.","In addition, we conduct a sensitivity analysis to the regularization parameter to faithfully compare previous work to our method and an experiment with trials on varied hyperparameter values to showcase the robustness of our algorithm."],"url":"http://arxiv.org/abs/2401.18017v1","category":"stat.ML"}
{"created":"2024-01-31 17:27:07","title":"Nanomechanically-induced nonequilibrium quantum phase transition to a self-organized density wave of a Bose-Einstein condensate","abstract":"We report on a nonequilibrium quantum phase transition (NQPT) in a hybrid quantum many-body system consisting of a vibrational mode of a damped nanomembrane interacting optomechanically with a cavity, whose output light couples to two internal states of an ultracold Bose gas held in an external quasi-one-dimensional box potential. For small effective membrane-atom couplings, the system is in a homogeneous Bose-Einstein condensate (BEC) steady state, with no membrane displacement. Depending on the transition frequency between the two internal atomic states, either one or both internal states are occupied. By increasing the atom-membrane couplings, the system transitions to a symmetry-broken self-organized BEC phase, which is characterized by a considerably displaced membrane steady-state and density wave-like BEC profiles. This NQPT can be both discontinuous and continuous for a certain interval of transition frequencies, and is purely discontinuous outside of it.","sentences":["We report on a nonequilibrium quantum phase transition (NQPT) in a hybrid quantum many-body system consisting of a vibrational mode of a damped nanomembrane interacting optomechanically with a cavity, whose output light couples to two internal states of an ultracold Bose gas held in an external quasi-one-dimensional box potential.","For small effective membrane-atom couplings, the system is in a homogeneous Bose-Einstein condensate (BEC) steady state, with no membrane displacement.","Depending on the transition frequency between the two internal atomic states, either one or both internal states are occupied.","By increasing the atom-membrane couplings, the system transitions to a symmetry-broken self-organized BEC phase, which is characterized by a considerably displaced membrane steady-state and density wave-like BEC profiles.","This NQPT can be both discontinuous and continuous for a certain interval of transition frequencies, and is purely discontinuous outside of it."],"url":"http://arxiv.org/abs/2401.18015v1","category":"cond-mat.quant-gas"}
{"created":"2024-01-31 17:20:36","title":"On The Power of Subtle Expressive Cues in the Perception of Human Affects","abstract":"In this study, we introduce a sketch-based method for testing how subtle expressive cues influence the perception of affect in illustrations of human figures. We specifically study the impact of human posture and gaze direction, implicitly specified through nose orientation, on perceived emotions and mood. Through a series of user studies using sketchy illustrations of a running figure, where a professional illustrator manipulated gaze direction through adjustments on the nose orientation, we found that this simple change resulted in a diverse range of perceived affects, spanning from fear to concern and wonder. These findings shed light on the importance of fine details in defining context for context-aware system designs and underscore the importance of recognizing and expressing affect. Understanding minor expressive cues is crucial to developing emotionally intelligent systems capable of expressing affect.","sentences":["In this study, we introduce a sketch-based method for testing how subtle expressive cues influence the perception of affect in illustrations of human figures.","We specifically study the impact of human posture and gaze direction, implicitly specified through nose orientation, on perceived emotions and mood.","Through a series of user studies using sketchy illustrations of a running figure, where a professional illustrator manipulated gaze direction through adjustments on the nose orientation, we found that this simple change resulted in a diverse range of perceived affects, spanning from fear to concern and wonder.","These findings shed light on the importance of fine details in defining context for context-aware system designs and underscore the importance of recognizing and expressing affect.","Understanding minor expressive cues is crucial to developing emotionally intelligent systems capable of expressing affect."],"url":"http://arxiv.org/abs/2401.18013v1","category":"cs.HC"}
{"created":"2024-01-31 17:20:28","title":"Causal Coordinated Concurrent Reinforcement Learning","abstract":"In this work, we propose a novel algorithmic framework for data sharing and coordinated exploration for the purpose of learning more data-efficient and better performing policies under a concurrent reinforcement learning (CRL) setting. In contrast to other work which make the assumption that all agents act under identical environments, we relax this restriction and instead consider the formulation where each agent acts within an environment which shares a global structure but also exhibits individual variations. Our algorithm leverages a causal inference algorithm in the form of Additive Noise Model - Mixture Model (ANM-MM) in extracting model parameters governing individual differentials via independence enforcement. We propose a new data sharing scheme based on a similarity measure of the extracted model parameters and demonstrate superior learning speeds on a set of autoregressive, pendulum and cart-pole swing-up tasks and finally, we show the effectiveness of diverse action selection between common agents under a sparse reward setting. To the best of our knowledge, this is the first work in considering non-identical environments in CRL and one of the few works which seek to integrate causal inference with reinforcement learning (RL).","sentences":["In this work, we propose a novel algorithmic framework for data sharing and coordinated exploration for the purpose of learning more data-efficient and better performing policies under a concurrent reinforcement learning (CRL) setting.","In contrast to other work which make the assumption that all agents act under identical environments, we relax this restriction and instead consider the formulation where each agent acts within an environment which shares a global structure but also exhibits individual variations.","Our algorithm leverages a causal inference algorithm in the form of Additive Noise Model - Mixture Model (ANM-MM) in extracting model parameters governing individual differentials via independence enforcement.","We propose a new data sharing scheme based on a similarity measure of the extracted model parameters and demonstrate superior learning speeds on a set of autoregressive, pendulum and cart-pole swing-up tasks and finally, we show the effectiveness of diverse action selection between common agents under a sparse reward setting.","To the best of our knowledge, this is the first work in considering non-identical environments in CRL and one of the few works which seek to integrate causal inference with reinforcement learning (RL)."],"url":"http://arxiv.org/abs/2401.18012v1","category":"stat.ML"}
{"created":"2024-01-31 17:08:34","title":"EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation","abstract":"In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity). This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities. Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts. Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM). EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data. Furthermore, it offers the distinct advantages of providing intermediate reasoning steps and coordinating specialist EEG tools across multiple scales in its operation, offering transparent and interpretable step-by-step verification, thereby promoting trustworthiness in clinical contexts.","sentences":["In conventional machine learning (ML) approaches applied to electroencephalography (EEG), this is often a limited focus, isolating specific brain activities occurring across disparate temporal scales (from transient spikes in milliseconds to seizures lasting minutes) and spatial scales (from localized high-frequency oscillations to global sleep activity).","This siloed approach limits the development EEG ML models that exhibit multi-scale electrophysiological understanding and classification capabilities.","Moreover, typical ML EEG approaches utilize black-box approaches, limiting their interpretability and trustworthiness in clinical contexts.","Thus, we propose EEG-GPT, a unifying approach to EEG classification that leverages advances in large language models (LLM).","EEG-GPT achieves excellent performance comparable to current state-of-the-art deep learning methods in classifying normal from abnormal EEG in a few-shot learning paradigm utilizing only 2% of training data.","Furthermore, it offers the distinct advantages of providing intermediate reasoning steps and coordinating specialist EEG tools across multiple scales in its operation, offering transparent and interpretable step-by-step verification, thereby promoting trustworthiness in clinical contexts."],"url":"http://arxiv.org/abs/2401.18006v1","category":"eess.SP"}
{"created":"2024-01-31 17:08:22","title":"Quantum influences and event relativity","abstract":"We develop a new interpretation of quantum theory by combining insights from extended Wigner's friend scenarios and quantum causal modelling. In this interpretation, which synthesizes ideas from relational quantum mechanics and consistent histories, events obtain relative to a set of systems, and correspond to projectors that are picked out by causal structure. We articulate these ideas using a precise mathematical formalism. Using this formalism, we show through specific examples and general constructions how quantum phenomena can be modelled and paradoxes avoided; how different scenarios may be classified and the framework of quantum causal models extended; and how one can approach decoherence and emergent classicality without relying on quantum states.","sentences":["We develop a new interpretation of quantum theory by combining insights from extended Wigner's friend scenarios and quantum causal modelling.","In this interpretation, which synthesizes ideas from relational quantum mechanics and consistent histories, events obtain relative to a set of systems, and correspond to projectors that are picked out by causal structure.","We articulate these ideas using a precise mathematical formalism.","Using this formalism, we show through specific examples and general constructions how quantum phenomena can be modelled and paradoxes avoided; how different scenarios may be classified and the framework of quantum causal models extended; and how one can approach decoherence and emergent classicality without relying on quantum states."],"url":"http://arxiv.org/abs/2401.18005v1","category":"quant-ph"}
{"created":"2024-01-31 17:06:52","title":"Gauge theory on $\u03c1$-Minkowski space-time","abstract":"We construct a gauge theory model on the 4-dimensional $\\rho$-Minkowski space-time, a particular deformation of the Minkowski space-time recently considered. The corresponding star product results from a combination of Weyl quantization map and properties of the convolution algebra of the special Euclidean group. We use noncommutative differential calculi based on twisted derivations together with a twisted notion of noncommutative connection. The twisted derivations pertain to the Hopf algebra of $\\rho$-deformed translations, a Hopf subalgebra of the $\\rho$-deformed Poincar\\'e algebra which can be viewed as defining the quantum symmetries of the $\\rho$-Minkowski space-time. The gauge theory model is left invariant under the action of the $\\rho$-deformed Poincar\\'e algebra. At the classical level, a property of the gauge model is that the in-vacuo dispersion relation for the gauge potential is not deformed, implying a standard propagation for photons in the $\\rho$-Minkowski space-time.","sentences":["We construct a gauge theory model on the 4-dimensional $\\rho$-Minkowski space-time, a particular deformation of the Minkowski space-time recently considered.","The corresponding star product results from a combination of Weyl quantization map and properties of the convolution algebra of the special Euclidean group.","We use noncommutative differential calculi based on twisted derivations together with a twisted notion of noncommutative connection.","The twisted derivations pertain to the Hopf algebra of $\\rho$-deformed translations, a Hopf subalgebra of the $\\rho$-deformed Poincar\\'e algebra which can be viewed as defining the quantum symmetries of the $\\rho$-Minkowski space-time.","The gauge theory model is left invariant under the action of the $\\rho$-deformed Poincar\\'e algebra.","At the classical level, a property of the gauge model is that the in-vacuo dispersion relation for the gauge potential is not deformed, implying a standard propagation for photons in the $\\rho$-Minkowski space-time."],"url":"http://arxiv.org/abs/2401.18004v1","category":"hep-th"}
{"created":"2024-01-31 17:02:31","title":"Desiderata for the Context Use of Question Answering Systems","abstract":"Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model's parametric knowledge, little robustness to noise, and a lack of consistency with their answers. However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them. We aim to close this gap, by first outlining a set of -- previously discussed as well as novel -- desiderata for QA models. We then survey relevant analysis and methods papers to provide an overview of the state of the field. The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once. We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise are more likely to correctly answer according to a context that conflicts with their parametric knowledge; and (3) the combination of conflicting knowledge and noise can reduce system performance by up to 96%. As such, our desiderata help increase our understanding of how these models work and reveal potential avenues for improvements.","sentences":["Prior work has uncovered a set of common problems in state-of-the-art context-based question answering (QA) systems: a lack of attention to the context when the latter conflicts with a model's parametric knowledge, little robustness to noise, and a lack of consistency with their answers.","However, most prior work focus on one or two of those problems in isolation, which makes it difficult to see trends across them.","We aim to close this gap, by first outlining a set of -- previously discussed as well as novel -- desiderata for QA models.","We then survey relevant analysis and methods papers to provide an overview of the state of the field.","The second part of our work presents experiments where we evaluate 15 QA systems on 5 datasets according to all desiderata at once.","We find many novel trends, including (1) systems that are less susceptible to noise are not necessarily more consistent with their answers when given irrelevant context; (2) most systems that are more susceptible to noise are more likely to correctly answer according to a context that conflicts with their parametric knowledge; and (3) the combination of conflicting knowledge and noise can reduce system performance by up to 96%.","As such, our desiderata help increase our understanding of how these models work and reveal potential avenues for improvements."],"url":"http://arxiv.org/abs/2401.18001v1","category":"cs.CL"}
{"created":"2024-01-31 16:55:44","title":"Development and Adaptation of Robotic Vision in the Real-World: the Challenge of Door Detection","abstract":"Mobile service robots are increasingly prevalent in human-centric, real-world domains, operating autonomously in unconstrained indoor environments. In such a context, robotic vision plays a central role in enabling service robots to perceive high-level environmental features from visual observations. Despite the data-driven approaches based on deep learning push the boundaries of vision systems, applying these techniques to real-world robotic scenarios presents unique methodological challenges. Traditional models fail to represent the challenging perception constraints typical of service robots and must be adapted for the specific environment where robots finally operate. We propose a method leveraging photorealistic simulations that balances data quality and acquisition costs for synthesizing visual datasets from the robot perspective used to train deep architectures. Then, we show the benefits in qualifying a general detector for the target domain in which the robot is deployed, showing also the trade-off between the effort for obtaining new examples from such a setting and the performance gain. In our extensive experimental campaign, we focus on the door detection task (namely recognizing the presence and the traversability of doorways) that, in dynamic settings, is useful to infer the topology of the map. Our findings are validated in a real-world robot deployment, comparing prominent deep-learning models and demonstrating the effectiveness of our approach in practical settings.","sentences":["Mobile service robots are increasingly prevalent in human-centric, real-world domains, operating autonomously in unconstrained indoor environments.","In such a context, robotic vision plays a central role in enabling service robots to perceive high-level environmental features from visual observations.","Despite the data-driven approaches based on deep learning push the boundaries of vision systems, applying these techniques to real-world robotic scenarios presents unique methodological challenges.","Traditional models fail to represent the challenging perception constraints typical of service robots and must be adapted for the specific environment where robots finally operate.","We propose a method leveraging photorealistic simulations that balances data quality and acquisition costs for synthesizing visual datasets from the robot perspective used to train deep architectures.","Then, we show the benefits in qualifying a general detector for the target domain in which the robot is deployed, showing also the trade-off between the effort for obtaining new examples from such a setting and the performance gain.","In our extensive experimental campaign, we focus on the door detection task (namely recognizing the presence and the traversability of doorways) that, in dynamic settings, is useful to infer the topology of the map.","Our findings are validated in a real-world robot deployment, comparing prominent deep-learning models and demonstrating the effectiveness of our approach in practical settings."],"url":"http://arxiv.org/abs/2401.17996v1","category":"cs.RO"}
{"created":"2024-01-31 16:52:19","title":"Multilinear Operator Networks","abstract":"Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.","sentences":["Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated.","On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures.","In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators.","The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token.","MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks.","The proposed model outperforms prior polynomial networks and performs on par with modern architectures.","We believe that MONet can inspire further research on models that use entirely multilinear operations."],"url":"http://arxiv.org/abs/2401.17992v1","category":"cs.CV"}
{"created":"2024-01-31 16:51:23","title":"Evaluating the Effectiveness of GPT-4 Turbo in Creating Defeaters for Assurance Cases","abstract":"Assurance cases (ACs) are structured arguments that support the verification of the correct implementation of systems' non-functional requirements, such as safety and security, thereby preventing system failures which could lead to catastrophic outcomes, including loss of lives. ACs facilitate the certification of systems in accordance with industrial standards, for example, DO-178C and ISO 26262. Identifying defeaters arguments that refute these ACs is essential for improving the robustness and confidence in ACs. To automate this task, we introduce a novel method that leverages the capabilities of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, to identify defeaters within ACs formalized using the Eliminative Argumentation (EA) notation. Our initial evaluation gauges the model's proficiency in understanding and generating arguments within this framework. The findings indicate that GPT-4 Turbo excels in EA notation and is capable of generating various types of defeaters.","sentences":["Assurance cases (ACs) are structured arguments that support the verification of the correct implementation of systems' non-functional requirements, such as safety and security, thereby preventing system failures which could lead to catastrophic outcomes, including loss of lives.","ACs facilitate the certification of systems in accordance with industrial standards, for example, DO-178C and ISO 26262.","Identifying defeaters arguments that refute these ACs is essential for improving the robustness and confidence in ACs.","To automate this task, we introduce a novel method that leverages the capabilities of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, to identify defeaters within ACs formalized using the Eliminative Argumentation (EA) notation.","Our initial evaluation gauges the model's proficiency in understanding and generating arguments within this framework.","The findings indicate that GPT-4 Turbo excels in EA notation and is capable of generating various types of defeaters."],"url":"http://arxiv.org/abs/2401.17991v1","category":"cs.SE"}
{"created":"2024-01-31 16:51:18","title":"Dark Matter Searches with Levitated Sensors","abstract":"Motivated by the current interest in employing quantum sensors on Earth and in space to conduct searches for new physics, we provide a perspective on the suitability of large-mass levitated optomechanical systems for observing dark matter signatures. We discuss conservative approaches of recoil detection through spectral analysis of coherently scattered light, enhancements of directional effects due to cross-correlation spectral densities, and the possibility of using quantum superpositions of mesoscopic test particles to measure rare events.","sentences":["Motivated by the current interest in employing quantum sensors on Earth and in space to conduct searches for new physics, we provide a perspective on the suitability of large-mass levitated optomechanical systems for observing dark matter signatures.","We discuss conservative approaches of recoil detection through spectral analysis of coherently scattered light, enhancements of directional effects due to cross-correlation spectral densities, and the possibility of using quantum superpositions of mesoscopic test particles to measure rare events."],"url":"http://arxiv.org/abs/2401.17990v1","category":"quant-ph"}
{"created":"2024-01-31 16:49:00","title":"Interplay of synchronization and cortical input in models of brain networks","abstract":"It is well known that synchronization patterns and coherence have a major role in the functioning of brain networks, both in pathological and in healthy states. In particular, in the perception of sound, one can observe an increase in coherence between the global dynamics in the network and the auditory input. In this perspective article, we show that synchronization scenarios are determined by a fine interplay between network topology, the location of the input, and frequencies of these cortical input signals. To this end, we analyze the influence of an external stimulation in a network of FitzHugh-Nagumo oscillators with empirically measured structural connectivity, and discuss different areas of cortical stimulation, including the auditory cortex.","sentences":["It is well known that synchronization patterns and coherence have a major role in the functioning of brain networks, both in pathological and in healthy states.","In particular, in the perception of sound, one can observe an increase in coherence between the global dynamics in the network and the auditory input.","In this perspective article, we show that synchronization scenarios are determined by a fine interplay between network topology, the location of the input, and frequencies of these cortical input signals.","To this end, we analyze the influence of an external stimulation in a network of FitzHugh-Nagumo oscillators with empirically measured structural connectivity, and discuss different areas of cortical stimulation, including the auditory cortex."],"url":"http://arxiv.org/abs/2401.17988v1","category":"nlin.AO"}
{"created":"2024-01-31 16:44:20","title":"Shrub of a thousand faces: an individual segmentation from satellite images using deep learning","abstract":"Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model. We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance. Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods. They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty. Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively.","sentences":["Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems.","Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision.","Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes.","However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   ","This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain).","In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectively develop and externally validate the model.","We also propose a new shrub-tailored evaluation algorithm based on a new metric called Multiple Intersections over Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation performance.","Finally, we deploy the developed model for the first time to generate a wall-to-wall map of Juniperus individuals.   ","The experimental results demonstrate the efficiency of our dual data construction approach in overcoming the limitations associated with traditional field survey methods.","They also highlight the robustness of MIoGTA metric in evaluating instance segmentation models on species with complex growth patterns showing more resilience against data annotation uncertainty.","Furthermore, they show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%, respectively."],"url":"http://arxiv.org/abs/2401.17985v1","category":"cs.CV"}
{"created":"2024-01-31 16:44:11","title":"Makinote: An FPGA-Based HW/SW Platform for Pre-Silicon Emulation of RISC-V Designs","abstract":"Emulating chip functionality before silicon production is crucial, especially with the increasing prevalence of RISC-V-based designs. FPGAs are promising candidates for such purposes due to their high-speed and reconfigurable architecture. In this paper, we introduce our Makinote, an FPGA-based Cluster platform, hosted at Barcelona Supercomputing Center (BSC-CNS), which is composed of a large number of FPGAs (in total 96 AMD/Xilinx Alveo U55c) to emulate massive size RTL designs (up to 750M ASIC cells). In addition, we introduce our FPGA shell as a powerful tool to facilitate the utilization of such a large FPGA cluster with minimal effort needed by the designers. The proposed FPGA shell provides an easy-to-use interface for the RTL developers to rapidly port such design into several FPGAs by automatically connecting to the necessary ports, e.g., PCIe Gen4, DRAM (DDR4 and HBM), ETH10g/100g. Moreover, specific drivers for exploiting RISC-V based architectures are provided within the set of tools associated with the FPGA shell. We release the tool online for further extensions.   We validate the efficiency of our hardware platform (i.e., FPGA cluster) and the software tool (i.e., FPGA Shell) by emulating a RISC-V processor and experimenting HPC Challenge application running on 32 FPGAs. Our results demonstrate that the performance improves by 8 times over the single-FPGA case.","sentences":["Emulating chip functionality before silicon production is crucial, especially with the increasing prevalence of RISC-V-based designs.","FPGAs are promising candidates for such purposes due to their high-speed and reconfigurable architecture.","In this paper, we introduce our Makinote, an FPGA-based Cluster platform, hosted at Barcelona Supercomputing Center (BSC-CNS), which is composed of a large number of FPGAs (in total 96 AMD/Xilinx Alveo U55c) to emulate massive size RTL designs (up to 750M ASIC cells).","In addition, we introduce our FPGA shell as a powerful tool to facilitate the utilization of such a large FPGA cluster with minimal effort needed by the designers.","The proposed FPGA shell provides an easy-to-use interface for the RTL developers to rapidly port such design into several FPGAs by automatically connecting to the necessary ports, e.g., PCIe Gen4, DRAM (DDR4 and HBM), ETH10g/100g.","Moreover, specific drivers for exploiting RISC-V based architectures are provided within the set of tools associated with the FPGA shell.","We release the tool online for further extensions.   ","We validate the efficiency of our hardware platform (i.e., FPGA cluster) and the software tool (i.e., FPGA Shell) by emulating a RISC-V processor and experimenting HPC Challenge application running on 32 FPGAs.","Our results demonstrate that the performance improves by 8 times over the single-FPGA case."],"url":"http://arxiv.org/abs/2401.17984v1","category":"cs.AR"}
{"created":"2024-01-31 16:42:05","title":"Intra-community link formation and modularity in ultracold growing hyperbolic networks","abstract":"Hyperbolic network models, centered around the idea of placing nodes at random in a hyperbolic space and drawing links according to a probability that decreases as a function of the distance, provide a simple, yet also very capable framework for grasping the small-world, scale-free, highly clustered and modular nature of complex systems that are often referred to as real-world networks. In the present work we study the community structure of networks generated by the Popularity Similarity Optimization model (corresponding to one of the fundamental, widely known hyperbolic models) when the temperature parameter (responsible for tuning the clustering coefficient) is set to the limiting value of zero. By focusing on the intra-community link formation we derive analytical expressions for the expected modularity of a partitioning consisting of equally sized angular sectors in the native disk representation of the 2d hyperbolic space. Our formulas improve earlier results to a great extent, being able to estimate the average modularity (measured by numerical simulations) with high precision in a considerably larger range both in terms of the model parameters and also the relative size of the communities with respect to the entire network. These findings enhance our comprehension of how modules form in hyperbolic networks. The existence of these modules is somewhat unexpected, given the absence of explicit community formation steps in the model definition.","sentences":["Hyperbolic network models, centered around the idea of placing nodes at random in a hyperbolic space and drawing links according to a probability that decreases as a function of the distance, provide a simple, yet also very capable framework for grasping the small-world, scale-free, highly clustered and modular nature of complex systems that are often referred to as real-world networks.","In the present work we study the community structure of networks generated by the Popularity Similarity Optimization model (corresponding to one of the fundamental, widely known hyperbolic models) when the temperature parameter (responsible for tuning the clustering coefficient) is set to the limiting value of zero.","By focusing on the intra-community link formation we derive analytical expressions for the expected modularity of a partitioning consisting of equally sized angular sectors in the native disk representation of the 2d hyperbolic space.","Our formulas improve earlier results to a great extent, being able to estimate the average modularity (measured by numerical simulations) with high precision in a considerably larger range both in terms of the model parameters and also the relative size of the communities with respect to the entire network.","These findings enhance our comprehension of how modules form in hyperbolic networks.","The existence of these modules is somewhat unexpected, given the absence of explicit community formation steps in the model definition."],"url":"http://arxiv.org/abs/2401.17983v1","category":"physics.soc-ph"}
{"created":"2024-01-31 16:38:32","title":"Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study","abstract":"Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.","sentences":["Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements.","This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses.","Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models.","We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths.","The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding.","We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs."],"url":"http://arxiv.org/abs/2401.17981v1","category":"cs.CV"}
{"created":"2024-01-31 16:35:27","title":"No epistemic model can explain anti-distinguishability of quantum mixed preparations","abstract":"We address the fundamental question of whether epistemic models can reproduce the empirical predictions of general quantum preparations. This involves comparing the quantum overlap determined by the anti-distinguishability of a set of mixed preparations with the epistemic overlap of the probability distribution over the ontic states describing these preparations. A set of quantum mixed states is deemed to be 'non-epistemic' when the epistemic overlap must be zero while the corresponding quantum overlap remains non-zero. In its strongest manifestation, a set of mixed quantum states is 'fully non-epistemic' if the epistemic overlap vanishes while the quantum overlap reaches its maximum value of one. Remarkably, we show that there exist sets of non-epistemic mixed states even in dimension 2, when the overlap between three mixed preparations is concerned. Moreover, we present quantum mixed states in dimensions 3 and 4 that are fully non-epistemic concerning the overlap between four and three preparations, respectively. We also establish a generic upper bound on the average ratio between the epistemic and quantum overlap for two mixed states. The ratio is shown to be arbitrarily small for certain pairs of quantum mixed states, signifying they are non-epistemic. All our findings are robustly testable in the prepare-and-measure experiments. In addition, we identify the instances where the existence of non-epistemic mixed states leads to the refutation of $\\psi-$epistemic models and further note that some of the examples obtained here indeed fall into this category. Interestingly, any proof of preparation contextuality implies that the respective mixed states are non-maximally epistemic, a weaker version of non-epistemic where the epistemic overlap is required to be less than the quantum overlap.","sentences":["We address the fundamental question of whether epistemic models can reproduce the empirical predictions of general quantum preparations.","This involves comparing the quantum overlap determined by the anti-distinguishability of a set of mixed preparations with the epistemic overlap of the probability distribution over the ontic states describing these preparations.","A set of quantum mixed states is deemed to be 'non-epistemic' when the epistemic overlap must be zero while the corresponding quantum overlap remains non-zero.","In its strongest manifestation, a set of mixed quantum states is 'fully non-epistemic' if the epistemic overlap vanishes while the quantum overlap reaches its maximum value of one.","Remarkably, we show that there exist sets of non-epistemic mixed states even in dimension 2, when the overlap between three mixed preparations is concerned.","Moreover, we present quantum mixed states in dimensions 3 and 4 that are fully non-epistemic concerning the overlap between four and three preparations, respectively.","We also establish a generic upper bound on the average ratio between the epistemic and quantum overlap for two mixed states.","The ratio is shown to be arbitrarily small for certain pairs of quantum mixed states, signifying they are non-epistemic.","All our findings are robustly testable in the prepare-and-measure experiments.","In addition, we identify the instances where the existence of non-epistemic mixed states leads to the refutation of $\\psi-$epistemic models and further note that some of the examples obtained here indeed fall into this category.","Interestingly, any proof of preparation contextuality implies that the respective mixed states are non-maximally epistemic, a weaker version of non-epistemic where the epistemic overlap is required to be less than the quantum overlap."],"url":"http://arxiv.org/abs/2401.17980v1","category":"quant-ph"}
{"created":"2024-01-31 16:34:10","title":"Entity Linking in the Job Market Domain","abstract":"In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@$k$).","sentences":["In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain.","Disambiguating skill mentions can help us get insight into the current labor market demands.","In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014).","Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill.","In this work, we link more fine-grained span-level mentions of skills.","We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention--skill pair dataset and evaluate them on a human-annotated skill-linking benchmark.","Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts.","Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@$k$)."],"url":"http://arxiv.org/abs/2401.17979v1","category":"cs.CL"}
{"created":"2024-01-31 16:33:12","title":"Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning","abstract":"Quantum computing holds immense potential for solving classically intractable problems by leveraging the unique properties of quantum mechanics. The scalability of quantum architectures remains a significant challenge. Multi-core quantum architectures are proposed to solve the scalability problem, arising a new set of challenges in hardware, communications and compilation, among others. One of these challenges is to adapt a quantum algorithm to fit within the different cores of the quantum computer. This paper presents a novel approach for circuit partitioning using Deep Reinforcement Learning, contributing to the advancement of both quantum computing and graph partitioning. This work is the first step in integrating Deep Reinforcement Learning techniques into Quantum Circuit Mapping, opening the door to a new paradigm of solutions to such problems.","sentences":["Quantum computing holds immense potential for solving classically intractable problems by leveraging the unique properties of quantum mechanics.","The scalability of quantum architectures remains a significant challenge.","Multi-core quantum architectures are proposed to solve the scalability problem, arising a new set of challenges in hardware, communications and compilation, among others.","One of these challenges is to adapt a quantum algorithm to fit within the different cores of the quantum computer.","This paper presents a novel approach for circuit partitioning using Deep Reinforcement Learning, contributing to the advancement of both quantum computing and graph partitioning.","This work is the first step in integrating Deep Reinforcement Learning techniques into Quantum Circuit Mapping, opening the door to a new paradigm of solutions to such problems."],"url":"http://arxiv.org/abs/2401.17976v1","category":"quant-ph"}
{"created":"2024-01-31 16:31:54","title":"Understanding polysemanticity in neural networks through coding theory","abstract":"Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.~(2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.","sentences":["Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output.","This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron.","In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes.","We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix.","Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is.","This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.~(2022).","Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability."],"url":"http://arxiv.org/abs/2401.17975v1","category":"cs.LG"}
{"created":"2024-01-31 16:30:50","title":"GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres","abstract":"As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to several model architectures enhances performance and helps derive higher-quality summaries by alleviating the entity hallucination problem in existing abstractive summarization.","sentences":["As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization.","In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations.","We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary.","Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries.","We also show that predicting or providing salient entities to several model architectures enhances performance and helps derive higher-quality summaries by alleviating the entity hallucination problem in existing abstractive summarization."],"url":"http://arxiv.org/abs/2401.17974v1","category":"cs.CL"}
{"created":"2024-01-31 16:27:47","title":"MelNet: A Real-Time Deep Learning Algorithm for Object Detection","abstract":"In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that of other pre-trained models.","sentences":["In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced.","MelNet underwent training utilizing the KITTI dataset for object detection.","Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732.","Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   ","The outcomes underscore the efficacy of employing transfer learning in certain instances.","Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results.","Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset.","This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs.","Consequently, post-training, MelNet's performance closely aligns with that of other pre-trained models."],"url":"http://arxiv.org/abs/2401.17972v1","category":"cs.CV"}
{"created":"2024-01-31 16:24:21","title":"Exploring the Enigmatic Chiral Phase Transition of QCD at FiniteTemperature","abstract":"The study of the critical point of chiral phase transition in Quantum Chromodynamics (QCD)has always been a topic of significant scientific importance. This paper provides an overview of the latest technological advancements in understanding the critical point of chiral phase transition, particularly within the framework of Effective Field Theories in QCD. The emphasis is on elucidating the intricacies of these key points and employing theoretical tools to explore associated phenomena.","sentences":["The study of the critical point of chiral phase transition in Quantum Chromodynamics (QCD)has always been a topic of significant scientific importance.","This paper provides an overview of the latest technological advancements in understanding the critical point of chiral phase transition, particularly within the framework of Effective Field Theories in QCD.","The emphasis is on elucidating the intricacies of these key points and employing theoretical tools to explore associated phenomena."],"url":"http://arxiv.org/abs/2401.17970v1","category":"hep-th"}
{"created":"2024-01-31 16:16:48","title":"CONCORD: Towards a DSL for Configurable Graph Code Representation","abstract":"Deep learning is widely used to uncover hidden patterns in large code corpora. To achieve this, constructing a format that captures the relevant characteristics and features of source code is essential. Graph-based representations have gained attention for their ability to model structural and semantic information. However, existing tools lack flexibility in constructing graphs across different programming languages, limiting their use. Additionally, the output of these tools often lacks interoperability and results in excessively large graphs, making graph-based neural networks training slower and less scalable.   We introduce CONCORD, a domain-specific language to build customizable graph representations. It implements reduction heuristics to reduce graphs' size complexity. We demonstrate its effectiveness in code smell detection as an illustrative use case and show that: first, CONCORD can produce code representations automatically per the specified configuration, and second, our heuristics can achieve comparable performance with significantly reduced size. CONCORD will help researchers a) create and experiment with customizable graph-based code representations for different software engineering tasks involving DL, b) reduce the engineering work to generate graph representations, c) address the issue of scalability in GNN models, and d) enhance the reproducibility of experiments in research through a standardized approach to code representation and analysis.","sentences":["Deep learning is widely used to uncover hidden patterns in large code corpora.","To achieve this, constructing a format that captures the relevant characteristics and features of source code is essential.","Graph-based representations have gained attention for their ability to model structural and semantic information.","However, existing tools lack flexibility in constructing graphs across different programming languages, limiting their use.","Additionally, the output of these tools often lacks interoperability and results in excessively large graphs, making graph-based neural networks training slower and less scalable.   ","We introduce CONCORD, a domain-specific language to build customizable graph representations.","It implements reduction heuristics to reduce graphs' size complexity.","We demonstrate its effectiveness in code smell detection as an illustrative use case and show that: first, CONCORD can produce code representations automatically per the specified configuration, and second, our heuristics can achieve comparable performance with significantly reduced size.","CONCORD will help researchers a) create and experiment with customizable graph-based code representations for different software engineering tasks involving DL, b) reduce the engineering work to generate graph representations, c) address the issue of scalability in GNN models, and d) enhance the reproducibility of experiments in research through a standardized approach to code representation and analysis."],"url":"http://arxiv.org/abs/2401.17967v1","category":"cs.SE"}
{"created":"2024-01-31 16:08:01","title":"University Students Motives and Challenges in Utilising Institutional Repository Resources","abstract":"One of the core functions of an academic institution is to generate knowledge, disseminate it to the intended audiences, and preserve it for future use. Academic institutions are now establishing Institutional Repositories (IRs) to collect produced resources to facilitate accessibility, dissemination, utilization, and management of intellectual materials produced within an institution. This study aimed to assess postgraduate students motives for utilizing IR resources and the challenges they encounter when utilizing IR resources at the University of Dar es Salaam. This study was conducted using a descriptive study design whereby it used both qualitative and quantitative research approaches. The population of this study comprised postgraduate students, librarians, and ICT personnel from the University of Dar es Salaam. A sample of 102 respondents was drawn conveniently and purposively for this study. Data were collected through questionnaires, interviews, as well as a review of documentary sources. Quantitative data were analyzed through a Version 16 Statistics Package for Social Science and qualitative data were analyzed using content analysis. The findings indicate that access to fulltext documents, the relevance of IR resources, and easy searching of the materials in the repository system motivate the utilization of IR resources. However, several challenges impede the utilization of these resources including unreliable internet access, inaccessibility of full-text and lack of guiding policy have been revealed as the major challenges toward utilization of IR resources. The study recommends training postgraduate students on the general use of IRs. Also, the University management should develop an IR policy that will guide the utilization of IR resources","sentences":["One of the core functions of an academic institution is to generate knowledge, disseminate it to the intended audiences, and preserve it for future use.","Academic institutions are now establishing Institutional Repositories (IRs) to collect produced resources to facilitate accessibility, dissemination, utilization, and management of intellectual materials produced within an institution.","This study aimed to assess postgraduate students motives for utilizing IR resources and the challenges they encounter when utilizing IR resources at the University of Dar es Salaam.","This study was conducted using a descriptive study design whereby it used both qualitative and quantitative research approaches.","The population of this study comprised postgraduate students, librarians, and ICT personnel from the University of Dar es Salaam.","A sample of 102 respondents was drawn conveniently and purposively for this study.","Data were collected through questionnaires, interviews, as well as a review of documentary sources.","Quantitative data were analyzed through a Version 16 Statistics Package for Social Science and qualitative data were analyzed using content analysis.","The findings indicate that access to fulltext documents, the relevance of IR resources, and easy searching of the materials in the repository system motivate the utilization of IR resources.","However, several challenges impede the utilization of these resources including unreliable internet access, inaccessibility of full-text and lack of guiding policy have been revealed as the major challenges toward utilization of IR resources.","The study recommends training postgraduate students on the general use of IRs.","Also, the University management should develop an IR policy that will guide the utilization of IR resources"],"url":"http://arxiv.org/abs/2401.17959v1","category":"cs.DL"}
{"created":"2024-01-31 16:07:44","title":"Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances","abstract":"Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications. While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited. In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates. We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers.","sentences":["Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications.","While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited.","In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates.","We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers."],"url":"http://arxiv.org/abs/2401.17958v1","category":"stat.ML"}
{"created":"2024-01-31 15:59:16","title":"Error-Tolerant E-Discovery Protocols","abstract":"We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) in the context of electronic discovery (e-discovery). Based on a request for production from the requesting party, the responding party is required to provide documents that are responsive to the request except for those that are legally privileged. Our goal is to find a protocol that verifies that the responding party sends almost all responsive documents while minimizing the disclosure of non-responsive documents. We provide protocols in the challenging non-realizable setting, where the instance may not be perfectly separated by a linear classifier. We demonstrate empirically that our protocol successfully manages to find almost all relevant documents, while incurring only a small disclosure of non-responsive documents. We complement this with a theoretical analysis of our protocol in the single-dimensional setting, and other experiments on simulated data which suggest that the non-responsive disclosure incurred by our protocol may be unavoidable.","sentences":["We consider the multi-party classification problem introduced by Dong, Hartline, and Vijayaraghavan (2022) in the context of electronic discovery (e-discovery).","Based on a request for production from the requesting party, the responding party is required to provide documents that are responsive to the request except for those that are legally privileged.","Our goal is to find a protocol that verifies that the responding party sends almost all responsive documents while minimizing the disclosure of non-responsive documents.","We provide protocols in the challenging non-realizable setting, where the instance may not be perfectly separated by a linear classifier.","We demonstrate empirically that our protocol successfully manages to find almost all relevant documents, while incurring only a small disclosure of non-responsive documents.","We complement this with a theoretical analysis of our protocol in the single-dimensional setting, and other experiments on simulated data which suggest that the non-responsive disclosure incurred by our protocol may be unavoidable."],"url":"http://arxiv.org/abs/2401.17952v1","category":"cs.CY"}
{"created":"2024-01-31 15:57:21","title":"HyperZ$\\cdot$Z$\\cdot$W Operator Connects Slow-Fast Networks for Full Context Interaction","abstract":"The self-attention mechanism utilizes large implicit weight matrices, programmed through dot product-based activations with very few trainable parameters, to enable long sequence modeling. In this paper, we investigate the possibility of discarding residual learning by employing large implicit kernels to achieve full context interaction at each layer of the network. To accomplish it, we introduce coordinate-based implicit MLPs as a slow network to generate hyper-kernels for another fast convolutional network. To get context-varying weights for fast dynamic encoding, we propose a $\\mathrm{Hyper}\\mathcal{Z{\\cdot}Z{\\cdot}W}$ operator that connects hyper-kernels ($\\mathcal{W}$) and hidden activations ($\\mathcal{Z}$) through simple elementwise multiplication, followed by convolution of $\\mathcal{Z}$ using the context-dependent $\\mathcal{W}$. Based on this design, we present a novel Terminator architecture that integrates hyper-kernels of different sizes to produce multi-branch hidden representations for enhancing the feature extraction capability of each layer. Additionally, a bottleneck layer is employed to compress the concatenated channels, allowing only valuable information to propagate to the subsequent layers. Notably, our model incorporates several innovative components and exhibits excellent properties, such as introducing local feedback error for updating the slow network, stable zero-mean features, faster training convergence, and fewer model parameters. Extensive experimental results on pixel-level 1D and 2D image classification benchmarks demonstrate the superior performance of our architecture.","sentences":["The self-attention mechanism utilizes large implicit weight matrices, programmed through dot product-based activations with very few trainable parameters, to enable long sequence modeling.","In this paper, we investigate the possibility of discarding residual learning by employing large implicit kernels to achieve full context interaction at each layer of the network.","To accomplish it, we introduce coordinate-based implicit MLPs as a slow network to generate hyper-kernels for another fast convolutional network.","To get context-varying weights for fast dynamic encoding, we propose a $\\mathrm{Hyper}\\mathcal{Z{\\cdot}Z{\\cdot}W}$ operator that connects hyper-kernels ($\\mathcal{W}$) and hidden activations ($\\mathcal{Z}$) through simple elementwise multiplication, followed by convolution of $\\mathcal{Z}$ using the context-dependent $\\mathcal{W}$. Based on this design, we present a novel Terminator architecture that integrates hyper-kernels of different sizes to produce multi-branch hidden representations for enhancing the feature extraction capability of each layer.","Additionally, a bottleneck layer is employed to compress the concatenated channels, allowing only valuable information to propagate to the subsequent layers.","Notably, our model incorporates several innovative components and exhibits excellent properties, such as introducing local feedback error for updating the slow network, stable zero-mean features, faster training convergence, and fewer model parameters.","Extensive experimental results on pixel-level 1D and 2D image classification benchmarks demonstrate the superior performance of our architecture."],"url":"http://arxiv.org/abs/2401.17948v1","category":"cs.CV"}
{"created":"2024-01-31 15:55:16","title":"Euclid preparation. The Near-IR Background Dipole Experiment with Euclid","abstract":"Verifying the fully kinematic nature of the cosmic microwave background (CMB) dipole is of fundamental importance in cosmology. In the standard cosmological model with the Friedman-Lemaitre-Robertson-Walker (FLRW) metric from the inflationary expansion the CMB dipole should be entirely kinematic. Any non-kinematic CMB dipole component would thus reflect the preinflationary structure of spacetime probing the extent of the FLRW applicability. Cosmic backgrounds from galaxies after the matter-radiation decoupling, should have kinematic dipole component identical in velocity with the CMB kinematic dipole. Comparing the two can lead to isolating the CMB non-kinematic dipole. It was recently proposed that such measurement can be done using the near-IR cosmic infrared background (CIB) measured with the currently operating Euclid telescope, and later with Roman. The proposed method reconstructs the resolved CIB, the Integrated Galaxy Light (IGL), from Euclid's Wide Survey and probes its dipole, with a kinematic component amplified over that of the CMB by the Compton-Getting effect. The amplification coupled with the extensive galaxy samples forming the IGL would determine the CIB dipole with an overwhelming signal/noise, isolating its direction to sub-degree accuracy. We develop details of the method for Euclid's Wide Survey in 4 bands spanning 0.6 to 2 mic. We isolate the systematic and other uncertainties and present methodologies to minimize them, after confining the sample to the magnitude range with negligible IGL/CIB dipole from galaxy clustering. These include the required star-galaxy separation, accounting for the extinction correction dipole using the method newly developed here achieving total separation, accounting for the Earth's orbital motion and other systematic effects. (Abridged)","sentences":["Verifying the fully kinematic nature of the cosmic microwave background (CMB) dipole is of fundamental importance in cosmology.","In the standard cosmological model with the Friedman-Lemaitre-Robertson-Walker (FLRW) metric from the inflationary expansion the CMB dipole should be entirely kinematic.","Any non-kinematic CMB dipole component would thus reflect the preinflationary structure of spacetime probing the extent of the FLRW applicability.","Cosmic backgrounds from galaxies after the matter-radiation decoupling, should have kinematic dipole component identical in velocity with the CMB kinematic dipole.","Comparing the two can lead to isolating the CMB non-kinematic dipole.","It was recently proposed that such measurement can be done using the near-IR cosmic infrared background (CIB) measured with the currently operating Euclid telescope, and later with Roman.","The proposed method reconstructs the resolved CIB, the Integrated Galaxy Light (IGL), from Euclid's Wide Survey and probes its dipole, with a kinematic component amplified over that of the CMB by the Compton-Getting effect.","The amplification coupled with the extensive galaxy samples forming the IGL would determine the CIB dipole with an overwhelming signal/noise, isolating its direction to sub-degree accuracy.","We develop details of the method for Euclid's Wide Survey in 4 bands spanning 0.6 to 2 mic.","We isolate the systematic and other uncertainties and present methodologies to minimize them, after confining the sample to the magnitude range with negligible IGL/CIB dipole from galaxy clustering.","These include the required star-galaxy separation, accounting for the extinction correction dipole using the method newly developed here achieving total separation, accounting for the Earth's orbital motion and other systematic effects.","(Abridged)"],"url":"http://arxiv.org/abs/2401.17945v1","category":"astro-ph.CO"}
{"created":"2024-01-31 15:50:50","title":"Gaussian Entanglement Measure: Applications to Multipartite Entanglement of Graph States and Bosonic Field Theory","abstract":"Computationally feasible multipartite entanglement measures are needed to advance our understanding of complex quantum systems. An entanglement measure based on the Fubini-Study metric has been recently introduced by Cocchiarella and co-workers, showing several advantages over existing methods, including ease of computation, a deep geometrical interpretation, and applicability to multipartite entanglement. Here, we present the Gaussian Entanglement Measure (GEM), a generalization of geometric entanglement measure for multimode Gaussian states, based on the purity of fragments of the whole systems. Our analysis includes the application of GEM to a two-mode Gaussian state coupled through a combined beamsplitter and a squeezing transformation. Additionally, we explore 3-mode and 4-mode graph states, where each vertex represents a bosonic mode, and each edge represents a quadratic transformation for various graph topologies. Interestingly, the ratio of the geometric entanglement measures for graph states with different topologies naturally captures properties related to the connectivity of the underlying graphs. Finally, by providing a computable multipartite entanglement measure for systems with a large number of degrees of freedom, we show that our definition can be used to obtain insights into a free bosonic field theory on $\\mathbb R_t\\times S^1$, going beyond the standard bipartite entanglement entropy approach between different regions of spacetime. The results presented herein suggest how the GEM paves the way for using quantum information-theoretical tools to study the topological properties of the space on which a quantum field theory is defined.","sentences":["Computationally feasible multipartite entanglement measures are needed to advance our understanding of complex quantum systems.","An entanglement measure based on the Fubini-Study metric has been recently introduced by Cocchiarella and co-workers, showing several advantages over existing methods, including ease of computation, a deep geometrical interpretation, and applicability to multipartite entanglement.","Here, we present the Gaussian Entanglement Measure (GEM), a generalization of geometric entanglement measure for multimode Gaussian states, based on the purity of fragments of the whole systems.","Our analysis includes the application of GEM to a two-mode Gaussian state coupled through a combined beamsplitter and a squeezing transformation.","Additionally, we explore 3-mode and 4-mode graph states, where each vertex represents a bosonic mode, and each edge represents a quadratic transformation for various graph topologies.","Interestingly, the ratio of the geometric entanglement measures for graph states with different topologies naturally captures properties related to the connectivity of the underlying graphs.","Finally, by providing a computable multipartite entanglement measure for systems with a large number of degrees of freedom, we show that our definition can be used to obtain insights into a free bosonic field theory on $\\mathbb R_t\\times S^1$, going beyond the standard bipartite entanglement entropy approach between different regions of spacetime.","The results presented herein suggest how the GEM paves the way for using quantum information-theoretical tools to study the topological properties of the space on which a quantum field theory is defined."],"url":"http://arxiv.org/abs/2401.17938v1","category":"quant-ph"}
{"created":"2024-01-31 15:38:26","title":"Stability of vortices in exciton-polariton condensates with spin-orbital-angular-momentum coupling","abstract":"The existence and dynamics of stable quantized vortices is an important subject of quantum many-body physics. Spin-orbital-angular-momentum coupling (SOAMC), a special type of spin-orbit coupling, has been experimentally achieved to create vortices in atomic Bose-Einstein condensate (BEC). Here, we generalize the concept of SOAMC to a two-component polariton BEC and analyze the emergence and configuration of vortices under a finite-size circular pumping. We discover that the regular configuration of vortex lattices induced by a finite-size circular pump is significantly distorted by the spatially dependent Raman coupling of SOAMC. Meanwhile, a vortex induced by SOAMC located at the center of the polariton cloud remains stable and pinned in place. When the Raman coupling is sufficiently strong, the polariton BEC with SOAMC is torn apart to become fragmented.","sentences":["The existence and dynamics of stable quantized vortices is an important subject of quantum many-body physics.","Spin-orbital-angular-momentum coupling (SOAMC), a special type of spin-orbit coupling, has been experimentally achieved to create vortices in atomic Bose-Einstein condensate (BEC).","Here, we generalize the concept of SOAMC to a two-component polariton BEC and analyze the emergence and configuration of vortices under a finite-size circular pumping.","We discover that the regular configuration of vortex lattices induced by a finite-size circular pump is significantly distorted by the spatially dependent Raman coupling of SOAMC.","Meanwhile, a vortex induced by SOAMC located at the center of the polariton cloud remains stable and pinned in place.","When the Raman coupling is sufficiently strong, the polariton BEC with SOAMC is torn apart to become fragmented."],"url":"http://arxiv.org/abs/2401.17927v1","category":"cond-mat.quant-gas"}
{"created":"2024-01-31 15:38:13","title":"Intrinsic correlations for statistical ensembles of Dirac-like structures","abstract":"The Weyl-Wigner formalism for evaluating the intrinsic information of Dirac bispinors as correlated qubits (localized) in a magnetic field is investigated in the extension to statistical ensembles. The confining external field quantizes the quantum correlation measures implied by the spin-parity qubit structure of the Dirac equation in 3+1 dimensions, which simplifies the computation of the entanglement quantifier for mixed states in relativistic Landau levels. This allows for the evaluation of quantum and classical correlations in terms of entropy measures for Dirac structures that are eventually mixed. Our results are twofold. First, a family of mixed Gaussian states is obtained in phase space, and its intrinsic correlation structure is computed in closed form. Second, the partition function for the low-dimensional Dirac equation in a magnetic field is derived through complex integration techniques. It describes the low-temperature regime in terms of analytically continued Zeta functions and the high temperature limit as a polynomial on the temperature variable. The connection with lower dimensional systems is further elicited by mapping the spin-parity qubits to valley-sublattice bispinors of the low-energy effective Hamiltonian of graphene.","sentences":["The Weyl-Wigner formalism for evaluating the intrinsic information of Dirac bispinors as correlated qubits (localized) in a magnetic field is investigated in the extension to statistical ensembles.","The confining external field quantizes the quantum correlation measures implied by the spin-parity qubit structure of the Dirac equation in 3+1 dimensions, which simplifies the computation of the entanglement quantifier for mixed states in relativistic Landau levels.","This allows for the evaluation of quantum and classical correlations in terms of entropy measures for Dirac structures that are eventually mixed.","Our results are twofold.","First, a family of mixed Gaussian states is obtained in phase space, and its intrinsic correlation structure is computed in closed form.","Second, the partition function for the low-dimensional Dirac equation in a magnetic field is derived through complex integration techniques.","It describes the low-temperature regime in terms of analytically continued Zeta functions and the high temperature limit as a polynomial on the temperature variable.","The connection with lower dimensional systems is further elicited by mapping the spin-parity qubits to valley-sublattice bispinors of the low-energy effective Hamiltonian of graphene."],"url":"http://arxiv.org/abs/2401.17926v1","category":"quant-ph"}
{"created":"2024-01-31 15:35:21","title":"[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs","abstract":"Coreference annotation and resolution is a vital component of computational literary studies. However, it has previously been difficult to build high quality systems for fiction. Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language. New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations. We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models.","sentences":["Coreference annotation and resolution is a vital component of computational literary studies.","However, it has previously been difficult to build high quality systems for fiction.","Coreference requires complicated structured outputs, and literary text involves subtle inferences and highly varied language.","New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations.","We create, evaluate, and release several trained models for coreference, as well as a workflow for training new models."],"url":"http://arxiv.org/abs/2401.17922v1","category":"cs.CL"}
{"created":"2024-01-31 15:34:56","title":"Quantum Ripple-Carry Adders and Comparator","abstract":"Addition is the most elementary arithmetic operation, and the basic building block of many algorithms. Having an efficient adder in terms of both physical resources and time is naturally essential. In this paper, we propose new quantum adders using the ripple-carry strategy as well as a new comparator. In particular, we show that a delay of 8n+O(1) is enough for adding or comparing two n-bit numbers and that there exists a circuit with a quantum cost of 12n+O(1) and a delay of 10n+O(1) for the addition. Even when focusing on the Clifford+T gate set, we obtain circuits using less gates than what was previously known. All our circuits use at most a single ancillary qubit and do not produce any garbage output.","sentences":["Addition is the most elementary arithmetic operation, and the basic building block of many algorithms.","Having an efficient adder in terms of both physical resources and time is naturally essential.","In this paper, we propose new quantum adders using the ripple-carry strategy as well as a new comparator.","In particular, we show that a delay of 8n+O(1) is enough for adding or comparing two n-bit numbers and that there exists a circuit with a quantum cost of 12n+O(1) and a delay of 10n+O(1) for the addition.","Even when focusing on the Clifford+T gate set, we obtain circuits using less gates than what was previously known.","All our circuits use at most a single ancillary qubit and do not produce any garbage output."],"url":"http://arxiv.org/abs/2401.17921v1","category":"quant-ph"}
{"created":"2024-01-31 15:33:37","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","abstract":"State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of $O(L \\log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.","sentences":["State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies.","We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs.","With a computational complexity of $O(L \\log L)$, this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns.","We evaluate our model on a series of long document abstractive summarization tasks.","The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference.","Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing."],"url":"http://arxiv.org/abs/2401.17919v1","category":"cs.CL"}
{"created":"2024-01-31 15:33:29","title":"GuardFS: a File System for Integrated Detection and Mitigation of Linux-based Ransomware","abstract":"Although ransomware has received broad attention in media and research, this evolving threat vector still poses a systematic threat. Related literature has explored their detection using various approaches leveraging Machine and Deep Learning. While these approaches are effective in detecting malware, they do not answer how to use this intelligence to protect against threats, raising concerns about their applicability in a hostile environment. Solutions that focus on mitigation rarely explore how to prevent and not just alert or halt its execution, especially when considering Linux-based samples. This paper presents GuardFS, a file system-based approach to investigate the integration of detection and mitigation of ransomware. Using a bespoke overlay file system, data is extracted before files are accessed. Models trained on this data are used by three novel defense configurations that obfuscate, delay, or track access to the file system. The experiments on GuardFS test the configurations in a reactive setting. The results demonstrate that although data loss cannot be completely prevented, it can be significantly reduced. Usability and performance analysis demonstrate that the defense effectiveness of the configurations relates to their impact on resource consumption and usability.","sentences":["Although ransomware has received broad attention in media and research, this evolving threat vector still poses a systematic threat.","Related literature has explored their detection using various approaches leveraging Machine and Deep Learning.","While these approaches are effective in detecting malware, they do not answer how to use this intelligence to protect against threats, raising concerns about their applicability in a hostile environment.","Solutions that focus on mitigation rarely explore how to prevent and not just alert or halt its execution, especially when considering Linux-based samples.","This paper presents GuardFS, a file system-based approach to investigate the integration of detection and mitigation of ransomware.","Using a bespoke overlay file system, data is extracted before files are accessed.","Models trained on this data are used by three novel defense configurations that obfuscate, delay, or track access to the file system.","The experiments on GuardFS test the configurations in a reactive setting.","The results demonstrate that although data loss cannot be completely prevented, it can be significantly reduced.","Usability and performance analysis demonstrate that the defense effectiveness of the configurations relates to their impact on resource consumption and usability."],"url":"http://arxiv.org/abs/2401.17917v1","category":"cs.CR"}
{"created":"2024-01-31 15:32:44","title":"Source-free Domain Adaptive Object Detection in Remote Sensing Images","abstract":"Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images. However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process. This setting is often impractical in the real world due to RS data privacy and transmission difficulty. To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model. We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment. The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias. The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels. By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features. Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images. Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well. Our code will be available at: https://weixliu.github.io/ .","sentences":["Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images.","However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process.","This setting is often impractical in the real world due to RS data privacy and transmission difficulty.","To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model.","We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment.","The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias.","The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels.","By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features.","Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images.","Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well.","Our code will be available at: https://weixliu.github.io/ ."],"url":"http://arxiv.org/abs/2401.17916v1","category":"cs.CV"}
{"created":"2024-01-31 15:24:13","title":"Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning","abstract":"Learning robot navigation strategies among pedestrian is crucial for domain based applications. Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL). However, these works do not consider multi-robot scenarios. In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Network combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a model-free RL framework to learn multi-agent policies. We evaluate our approach on simulation and provide a series of experiments in a set of various conditions (number of agents / pedestrians). Empirical results show that our method learns faster than social navigation deep RL mono-agent techniques, and enables efficient multi-agent implicit coordination in challenging crowd navigation with multiple heterogeneous humans. Furthermore, by incorporating customizable meta-parameters, we can adjust the neighborhood density to take into account in our navigation strategy.","sentences":["Learning robot navigation strategies among pedestrian is crucial for domain based applications.","Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL).","However, these works do not consider multi-robot scenarios.","In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL.","Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents).","Each agent uses a model based on two Graph Neural Network combined with attention mechanisms.","First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others.","This is incorporated into a model-free RL framework to learn multi-agent policies.","We evaluate our approach on simulation and provide a series of experiments in a set of various conditions (number of agents / pedestrians).","Empirical results show that our method learns faster than social navigation deep RL mono-agent techniques, and enables efficient multi-agent implicit coordination in challenging crowd navigation with multiple heterogeneous humans.","Furthermore, by incorporating customizable meta-parameters, we can adjust the neighborhood density to take into account in our navigation strategy."],"url":"http://arxiv.org/abs/2401.17914v1","category":"cs.RO"}
{"created":"2024-01-31 15:16:25","title":"SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks","abstract":"As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing. One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures. Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information. However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures. In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment analysis. Furthermore, we go on to propose a new method of encoding text as spikes that outperforms a widely-used rate-coding technique, Poisson rate-coding, by around 13\\% on our benchmark NLP tasks. Subsequently, we demonstrate the energy efficiency of SNNs implemented in hardware for the sentiment analysis task compared to traditional deep neural networks, observing an energy efficiency increase of more than 32x during inference and 60x during training while incurring the expected energy-performance tradeoff.","sentences":["As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing.","One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures.","Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information.","However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures.","In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment analysis.","Furthermore, we go on to propose a new method of encoding text as spikes that outperforms a widely-used rate-coding technique, Poisson rate-coding, by around 13\\% on our benchmark NLP tasks.","Subsequently, we demonstrate the energy efficiency of SNNs implemented in hardware for the sentiment analysis task compared to traditional deep neural networks, observing an energy efficiency increase of more than 32x during inference and 60x during training while incurring the expected energy-performance tradeoff."],"url":"http://arxiv.org/abs/2401.17911v1","category":"cs.CL"}
{"created":"2024-01-31 15:15:41","title":"Controllable Dense Captioner with Multimodal Embedding Bridging","abstract":"In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance. ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module. While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance. BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions. Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP). Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning. Code is available at https://github.com/callsys/ControlCap.","sentences":["In this paper, we propose a controllable dense captioner (ControlCap), which accommodates user's intention to dense captioning by introducing linguistic guidance.","ControlCap is defined as a multimodal embedding bridging architecture, which comprises multimodal embedding generation (MEG) module and bi-directional embedding bridging (BEB) module.","While MEG module represents objects/regions by combining embeddings of detailed information with context-aware ones, it also endows ControlCap the adaptability to specialized controls by utilizing them as linguistic guidance.","BEB module aligns the linguistic guidance with visual embeddings through borrowing/returning features from/to the visual domain and gathering such features to predict text descriptions.","Experiments on Visual Genome and VG-COCO datasets show that ControlCap respectively outperforms the state-of-the-art methods by 1.5% and 3.7% (mAP).","Last but not least, with the capability of converting region-category pairs to region-text pairs, ControlCap is able to act as a powerful data engine for dense captioning.","Code is available at https://github.com/callsys/ControlCap."],"url":"http://arxiv.org/abs/2401.17910v1","category":"cs.CV"}
{"created":"2024-01-31 15:12:58","title":"Duality of quantum geometries","abstract":"Quantum connections are defined by parallel transport operators acting on a Hilbert space. They transport tangent operators along paths in parameter space. The metric tensor of a Riemannian manifold is replaced by an inner product of pairs of operator fields, similar to the inner product of the Kubo-Mori formalism of Linear Response Theory. The metric is used to define the dual of a quantum connection. Directional derivatives of the parallel transport operators are the elements of the quantum vector potential. They define the covariant derivatives of operator fields. The covariant derivatives are used to quantify the holonomy of the quantum connection. It is shown that a quantum connection is holonomic if and only if its dual is holonomic. If the parallel transport operators are unitary then an alpha-family of quantum connections can be defined in a way similar to Amari's alpha family of connections in Information Geometry. The minus alpha connection is the dual of the alpha connection. In particular, the alpha equal zero connection is self-dual. An operator field can be combined with a path in parameter space to produce a path in operator space. A definition is given for such a path in operator space to be autoparallel. The path in parameter space is then a geodesic for the induced connection. If the quantum connection is self-dual then the induced connection is the metric connection.","sentences":["Quantum connections are defined by parallel transport operators acting on a Hilbert space.","They transport tangent operators along paths in parameter space.","The metric tensor of a Riemannian manifold is replaced by an inner product of pairs of operator fields, similar to the inner product of the Kubo-Mori formalism of Linear Response Theory.","The metric is used to define the dual of a quantum connection.","Directional derivatives of the parallel transport operators are the elements of the quantum vector potential.","They define the covariant derivatives of operator fields.","The covariant derivatives are used to quantify the holonomy of the quantum connection.","It is shown that a quantum connection is holonomic if and only if its dual is holonomic.","If the parallel transport operators are unitary then an alpha-family of quantum connections can be defined in a way similar to Amari's alpha family of connections in Information Geometry.","The minus alpha connection is the dual of the alpha connection.","In particular, the alpha equal zero connection is self-dual.","An operator field can be combined with a path in parameter space to produce a path in operator space.","A definition is given for such a path in operator space to be autoparallel.","The path in parameter space is then a geodesic for the induced connection.","If the quantum connection is self-dual then the induced connection is the metric connection."],"url":"http://arxiv.org/abs/2401.17908v1","category":"math-ph"}
{"created":"2024-01-31 15:11:20","title":"SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization","abstract":"This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation. SubPipe has been recorded using a \\gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors. The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand. The AUV's pose ground truth is estimated from the navigation sensors. The side-scan sonar and RGB images include object detection and segmentation annotations, respectively. State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms. To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario. The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset","sentences":["This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation.","SubPipe has been recorded using a \\gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors.","The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand.","The AUV's pose ground truth is estimated from the navigation sensors.","The side-scan sonar and RGB images include object detection and segmentation annotations, respectively.","State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms.","To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario.","The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset"],"url":"http://arxiv.org/abs/2401.17907v1","category":"cs.RO"}
{"created":"2024-01-31 15:10:29","title":"Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation","abstract":"The Segment Anything Model (SAM), a profound vision foundation model pre-trained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in text segmentation across four hierarchies, including stroke, word, text-line, and paragraph, while realizing layout analysis as well. Specifically, we first turn SAM into a high-quality text stroke segmentation (TSS) model through a parameter-efficient fine-tuning approach. We use this TSS model to iteratively generate the text stroke labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TSS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the promptable mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for text stroke segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring 20x fewer training epochs. The code is available at https://github.com/ymy-k/Hi-SAM.","sentences":["The Segment Anything Model (SAM), a profound vision foundation model pre-trained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications.","This paper introduces Hi-SAM, a unified model leveraging SAM for hierarchical text segmentation.","Hi-SAM excels in text segmentation across four hierarchies, including stroke, word, text-line, and paragraph, while realizing layout analysis as well.","Specifically, we first turn SAM into a high-quality text stroke segmentation (TSS) model through a parameter-efficient fine-tuning approach.","We use this TSS model to iteratively generate the text stroke labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset.","Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TSS architecture with a customized hierarchical mask decoder.","During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation mode.","In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing.","As for the promptable mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click.","Experimental results show the state-of-the-art performance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for text stroke segmentation.","Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring 20x fewer training epochs.","The code is available at https://github.com/ymy-k/Hi-SAM."],"url":"http://arxiv.org/abs/2401.17904v1","category":"cs.CV"}
{"created":"2024-01-31 15:07:46","title":"Gravitational effects in a superconducting film struck by a laser pulse","abstract":"We study the local interaction of the gravitational field with a superfluid condensate. To this end, we exploit the Ginzburg-Landau formalism with generalized Maxwell fields. The analysis shows that a slight local alteration of the gravitational field in a thin superconducting film can be achieved by laser pulses with particular characteristics.","sentences":["We study the local interaction of the gravitational field with a superfluid condensate.","To this end, we exploit the Ginzburg-Landau formalism with generalized Maxwell fields.","The analysis shows that a slight local alteration of the gravitational field in a thin superconducting film can be achieved by laser pulses with particular characteristics."],"url":"http://arxiv.org/abs/2401.17903v1","category":"gr-qc"}
{"created":"2024-01-31 15:04:01","title":"Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance","abstract":"The objective of legal text entailment is to ascertain whether the assertions in a legal query logically follow from the information provided in one or multiple legal articles. ChatGPT, a large language model, is robust in many natural language processing tasks, including legal text entailment: when we set the temperature = 0 (the ChatGPT answers are deterministic) and prompt the model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms the previous SOTA of 67.89%. On the other hand, if the temperature is larger than zero, ChatGPT answers are not deterministic, leading to inconsistent answers and fluctuating results. We propose to leverage label models (a fundamental component of weak supervision techniques) to integrate the provisional answers by ChatGPT into consolidated labels. By that way, we treat ChatGPT provisional answers as noisy predictions which can be consolidated by label models. The experimental results demonstrate that this approach can attain an accuracy of 76.15%, marking a significant improvement of 8.26% over the prior state-of-the-art benchmark. Additionally, we perform an analysis of the instances where ChatGPT produces incorrect answers, then we classify the errors, offering insights that could guide potential enhancements for future research endeavors.","sentences":["The objective of legal text entailment is to ascertain whether the assertions in a legal query logically follow from the information provided in one or multiple legal articles.","ChatGPT, a large language model, is robust in many natural language processing tasks, including legal text entailment: when we set the temperature = 0 (the ChatGPT answers are deterministic) and prompt the model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms the previous SOTA of 67.89%.","On the other hand, if the temperature is larger than zero, ChatGPT answers are not deterministic, leading to inconsistent answers and fluctuating results.","We propose to leverage label models (a fundamental component of weak supervision techniques) to integrate the provisional answers by ChatGPT into consolidated labels.","By that way, we treat ChatGPT provisional answers as noisy predictions which can be consolidated by label models.","The experimental results demonstrate that this approach can attain an accuracy of 76.15%, marking a significant improvement of 8.26% over the prior state-of-the-art benchmark.","Additionally, we perform an analysis of the instances where ChatGPT produces incorrect answers, then we classify the errors, offering insights that could guide potential enhancements for future research endeavors."],"url":"http://arxiv.org/abs/2401.17897v1","category":"cs.CL"}
{"created":"2024-01-31 15:02:58","title":"Photosynthetic properties assisted by the quantum entanglement in two adjacent pigment molecules","abstract":"The quantum dynamics of entanglement is widely revealed in photosynthetic light-harvesting complexes. Different from the previous work, we explore the properties of exciton transport and photosynthesis assisted by the quantum entanglement in two adjacent pigment molecules, which are measured by the population dynamics behaviors, the $j$-$V$ characteristics and by the output power via a photosynthetic quantum heat engine (QHE) model. A more robust exciton transport dynamic behavior is compared with those without quantum entanglement, and the photosynthetic characteristics evaluated by the output current and power were proved to be enhanced by the quantum entanglement at different ambient temperatures. These results may point toward the possibility for artificial photosynthetic nanostructures inspired by this quantum biological systems.","sentences":["The quantum dynamics of entanglement is widely revealed in photosynthetic light-harvesting complexes.","Different from the previous work, we explore the properties of exciton transport and photosynthesis assisted by the quantum entanglement in two adjacent pigment molecules, which are measured by the population dynamics behaviors, the $j$-$V$ characteristics and by the output power via a photosynthetic quantum heat engine (QHE) model.","A more robust exciton transport dynamic behavior is compared with those without quantum entanglement, and the photosynthetic characteristics evaluated by the output current and power were proved to be enhanced by the quantum entanglement at different ambient temperatures.","These results may point toward the possibility for artificial photosynthetic nanostructures inspired by this quantum biological systems."],"url":"http://arxiv.org/abs/2401.17896v1","category":"physics.app-ph"}
{"created":"2024-01-31 15:02:26","title":"ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields","abstract":"We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.","sentences":["We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene.","Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints.","We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity."],"url":"http://arxiv.org/abs/2401.17895v1","category":"cs.CV"}
{"created":"2024-01-31 14:59:29","title":"Cosmological solutions in the Brans-Dicke theory via invariants of symmetry groups","abstract":"We proceed to obtain an exact analytical solution of the Brans-Dicke (BD) equations for the spatially flat ($k=0$) Friedmann-Lamaitre-Robertson-Walker (FLRW) cosmological model in both cases of the absence and presence of the cosmological constant. The solution method that we use to solve the field equations of the BD equations is called the \"invariants of symmetry groups method\" (ISG-method). This method is based on the extended Prelle-Singer (PS) method and it employs the Lie point symmetry, $\\lambda$-symmetry, and Darboux polynomials (DPs). Indeed, the ISG-method tries to provide two independent first-order invariants associated to the one-parameter Lie groups of transformations keeping ordinary differential equations (ODEs) invariant, as solutions. It should be noted for integrable ODEs, the ISG-method guarantees the extraction of these two invariants. In this work for the BD equations in FLRW cosmological model, we find the Lie point symmetries, $\\lambda$-symmetries and DPs, and obtain the basic quantities of the extended PS method (which are the null forms and the integrating factors). By making use of the extended PS method we find two independent first-order invariants, in such a way appropriate cosmological solutions from solving these invariants as a system of algebraic equations are simultaneously obtained. These solutions are wealthy so that they include many known special solutions, such as O'Hanlon-Tupper vacuum solutions, Nariai's solutions, Brans-Dicke dust solutions, inflationary solutions, and etc.","sentences":["We proceed to obtain an exact analytical solution of the Brans-Dicke (BD) equations for the spatially flat ($k=0$) Friedmann-Lamaitre-Robertson-Walker (FLRW) cosmological model in both cases of the absence and presence of the cosmological constant.","The solution method that we use to solve the field equations of the BD equations is called the \"invariants of symmetry groups method\" (ISG-method).","This method is based on the extended Prelle-Singer (PS) method and it employs the Lie point symmetry, $\\lambda$-symmetry, and Darboux polynomials (DPs).","Indeed, the ISG-method tries to provide two independent first-order invariants associated to the one-parameter Lie groups of transformations keeping ordinary differential equations (ODEs) invariant, as solutions.","It should be noted for integrable ODEs, the ISG-method guarantees the extraction of these two invariants.","In this work for the BD equations in FLRW cosmological model, we find the Lie point symmetries, $\\lambda$-symmetries and DPs, and obtain the basic quantities of the extended PS method (which are the null forms and the integrating factors).","By making use of the extended PS method we find two independent first-order invariants, in such a way appropriate cosmological solutions from solving these invariants as a system of algebraic equations are simultaneously obtained.","These solutions are wealthy so that they include many known special solutions, such as O'Hanlon-Tupper vacuum solutions, Nariai's solutions, Brans-Dicke dust solutions, inflationary solutions, and etc."],"url":"http://arxiv.org/abs/2401.17893v1","category":"gr-qc"}
{"created":"2024-01-31 14:58:33","title":"Deep-learning density functional perturbation theory","abstract":"Calculating perturbation response properties of materials from first principles provides a vital link between theory and experiment, but is bottlenecked by the high computational cost. Here a general framework is proposed to perform density functional perturbation theory (DFPT) calculations by neural networks, greatly improving the computational efficiency. Automatic differentiation is applied on neural networks, facilitating accurate computation of derivatives. High efficiency and good accuracy of the approach are demonstrated by studying electron-phonon coupling and related physical quantities. This work brings deep-learning density functional theory and DFPT into a unified framework, creating opportunities for developing ab initio artificial intelligence.","sentences":["Calculating perturbation response properties of materials from first principles provides a vital link between theory and experiment, but is bottlenecked by the high computational cost.","Here a general framework is proposed to perform density functional perturbation theory (DFPT) calculations by neural networks, greatly improving the computational efficiency.","Automatic differentiation is applied on neural networks, facilitating accurate computation of derivatives.","High efficiency and good accuracy of the approach are demonstrated by studying electron-phonon coupling and related physical quantities.","This work brings deep-learning density functional theory and DFPT into a unified framework, creating opportunities for developing ab initio artificial intelligence."],"url":"http://arxiv.org/abs/2401.17892v1","category":"physics.comp-ph"}
{"created":"2024-01-31 14:56:17","title":"Periodic orbit theory of Bethe-integrable quantum systems: an $N$-particle Berry-Tabor trace formula","abstract":"One of the fundamental results of semiclassical theory is the existence of trace formulae showing how spectra of quantum mechanical systems emerge from massive interference among amplitudes related with time-periodic structures of the corresponding classical limit. If it displays the properties of Hamiltonian integrability, this connection is given by the celebrated Berry-Tabor trace formula, and the periodic structures it is built on are KAM tori supporting closed trajectories in phase space. Here we show how to extend this connection into the domain of quantum many-body systems displaying integrability in the sense of the Bethe ansatz, where a classical limit cannot be rigorously defined due to the presence of singular potentials. Formally following the original derivation of Berry and Tabor [1, 2], but applied to the Bethe equations without underlying classical structure, we obtain a many-particle trace formula for the density of states of N interacting bosons on a ring, the Lieb-Liniger model. Our semiclassical expressions are in excellent agreement with quantum mechanical results for $N$ = 2, 3 and 4 particles. For N = 2 we relate our results to the quantization of billiards with mixed boundary conditions. Our work paves the way towards the treatment of the important class of integrable many-body systems by means of semiclassical trace formulae pioneered by Michael Berry in the single-particle context.","sentences":["One of the fundamental results of semiclassical theory is the existence of trace formulae showing how spectra of quantum mechanical systems emerge from massive interference among amplitudes related with time-periodic structures of the corresponding classical limit.","If it displays the properties of Hamiltonian integrability, this connection is given by the celebrated Berry-Tabor trace formula, and the periodic structures it is built on are KAM tori supporting closed trajectories in phase space.","Here we show how to extend this connection into the domain of quantum many-body systems displaying integrability in the sense of the Bethe ansatz, where a classical limit cannot be rigorously defined due to the presence of singular potentials.","Formally following the original derivation of Berry and Tabor","[1, 2], but applied to the Bethe equations without underlying classical structure, we obtain a many-particle trace formula for the density of states of N interacting bosons on a ring, the Lieb-Liniger model.","Our semiclassical expressions are in excellent agreement with quantum mechanical results for $N$ = 2, 3 and 4 particles.","For N = 2 we relate our results to the quantization of billiards with mixed boundary conditions.","Our work paves the way towards the treatment of the important class of integrable many-body systems by means of semiclassical trace formulae pioneered by Michael Berry in the single-particle context."],"url":"http://arxiv.org/abs/2401.17891v1","category":"quant-ph"}
{"created":"2024-01-31 14:53:33","title":"The inherent randomness of news virality on social media","abstract":"Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics.","sentences":["Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting.","In this scenario, understanding the factors that capture audience attention and drive viral content is crucial.","Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media.","Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022.","We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet.","Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time.","We conclude the paper by presenting a statistical model replicating the observed growth dynamics."],"url":"http://arxiv.org/abs/2401.17890v1","category":"cs.SI"}
{"created":"2024-01-31 14:45:11","title":"Detecting Groups in Directed and Non-Directed Bipartite Networks","abstract":"Bipartite networks provide an effective resource for representing, characterizing, and modeling several abstract and real-world systems and structures involving binary relations, which include food webs, social interactions, and customer-product relationships. Of particular interest is the problem of, given a specific bipartite network, to identify possible respective groups or clusters characterized by similar interconnecting patterns. The present work approaches this issue by extending and complementing a previously described coincidence similarity methodology (Bioarxiv, doi.org/10.1101/2022.07.16.500294) in several manners, including the consideration of direct and non-directed bipartite networks, the characterization of groups in those networks, as well as considering synthetic bipartite networks presenting groups as a resource for studying the performance of the described methodology. Several interesting results are described and discussed, including the corroboration of the potential of the coincidence similarity methodology for achieving enhanced separation between the groups in bipartite networks.","sentences":["Bipartite networks provide an effective resource for representing, characterizing, and modeling several abstract and real-world systems and structures involving binary relations, which include food webs, social interactions, and customer-product relationships.","Of particular interest is the problem of, given a specific bipartite network, to identify possible respective groups or clusters characterized by similar interconnecting patterns.","The present work approaches this issue by extending and complementing a previously described coincidence similarity methodology (Bioarxiv, doi.org/10.1101/2022.07.16.500294) in several manners, including the consideration of direct and non-directed bipartite networks, the characterization of groups in those networks, as well as considering synthetic bipartite networks presenting groups as a resource for studying the performance of the described methodology.","Several interesting results are described and discussed, including the corroboration of the potential of the coincidence similarity methodology for achieving enhanced separation between the groups in bipartite networks."],"url":"http://arxiv.org/abs/2401.17887v1","category":"cs.SI"}
{"created":"2024-01-31 14:42:53","title":"Exact Dynamics and Shortcuts to Adiabaticity in the Tomonaga-Luttinger Liquid","abstract":"Controlling many-body quantum systems is a highly challenging task required to advance quantum technologies. Here, we report progress in controlling gapless many-body quantum systems described by the Tomonaga-Luttinger liquid (TLL). To do so, we investigate the exact dynamics of the TLL induced by an interaction quench, making use of the $SU(1,1)$ dynamical symmetry group and the Schr\\\"odinger picture. First, we demonstrate that this approach is useful to perform a shortcut to adiabaticity, that cancels the final non-adiabatic residual energy of the driven TLL and is experimentally implementable in the semiclassical limit of the sine-Gordon model. Second, we apply this framework to analyze various driving schemes in finite time, including linear ramps and smooth protocols.","sentences":["Controlling many-body quantum systems is a highly challenging task required to advance quantum technologies.","Here, we report progress in controlling gapless many-body quantum systems described by the Tomonaga-Luttinger liquid (TLL).","To do so, we investigate the exact dynamics of the TLL induced by an interaction quench, making use of the $SU(1,1)$ dynamical symmetry group and the Schr\\\"odinger picture.","First, we demonstrate that this approach is useful to perform a shortcut to adiabaticity, that cancels the final non-adiabatic residual energy of the driven TLL and is experimentally implementable in the semiclassical limit of the sine-Gordon model.","Second, we apply this framework to analyze various driving schemes in finite time, including linear ramps and smooth protocols."],"url":"http://arxiv.org/abs/2401.17884v1","category":"quant-ph"}
{"created":"2024-01-31 14:41:40","title":"Reimagining Reality: A Comprehensive Survey of Video Inpainting Techniques","abstract":"This paper offers a comprehensive analysis of recent advancements in video inpainting techniques, a critical subset of computer vision and artificial intelligence. As a process that restores or fills in missing or corrupted portions of video sequences with plausible content, video inpainting has evolved significantly with the advent of deep learning methodologies. Despite the plethora of existing methods and their swift development, the landscape remains complex, posing challenges to both novices and established researchers. Our study deconstructs major techniques, their underpinning theories, and their effective applications. Moreover, we conduct an exhaustive comparative study, centering on two often-overlooked dimensions: visual quality and computational efficiency. We adopt a human-centric approach to assess visual quality, enlisting a panel of annotators to evaluate the output of different video inpainting techniques. This provides a nuanced qualitative understanding that complements traditional quantitative metrics. Concurrently, we delve into the computational aspects, comparing inference times and memory demands across a standardized hardware setup. This analysis underscores the balance between quality and efficiency: a critical consideration for practical applications where resources may be constrained. By integrating human validation and computational resource comparison, this survey not only clarifies the present landscape of video inpainting techniques but also charts a course for future explorations in this vibrant and evolving field.","sentences":["This paper offers a comprehensive analysis of recent advancements in video inpainting techniques, a critical subset of computer vision and artificial intelligence.","As a process that restores or fills in missing or corrupted portions of video sequences with plausible content, video inpainting has evolved significantly with the advent of deep learning methodologies.","Despite the plethora of existing methods and their swift development, the landscape remains complex, posing challenges to both novices and established researchers.","Our study deconstructs major techniques, their underpinning theories, and their effective applications.","Moreover, we conduct an exhaustive comparative study, centering on two often-overlooked dimensions: visual quality and computational efficiency.","We adopt a human-centric approach to assess visual quality, enlisting a panel of annotators to evaluate the output of different video inpainting techniques.","This provides a nuanced qualitative understanding that complements traditional quantitative metrics.","Concurrently, we delve into the computational aspects, comparing inference times and memory demands across a standardized hardware setup.","This analysis underscores the balance between quality and efficiency: a critical consideration for practical applications where resources may be constrained.","By integrating human validation and computational resource comparison, this survey not only clarifies the present landscape of video inpainting techniques but also charts a course for future explorations in this vibrant and evolving field."],"url":"http://arxiv.org/abs/2401.17883v1","category":"cs.CV"}
{"created":"2024-01-31 14:41:23","title":"I Think, Therefore I am: Awareness in Large Language Models","abstract":"Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses. We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence. We identify four key dimensions of awareness: capability, mission, emotion, and perspective. To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness.","sentences":["Do large language models (LLMs) exhibit any forms of awareness similar to humans?","In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses.","We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence.","We identify four key dimensions of awareness: capability, mission, emotion, and perspective.","To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset.","Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness."],"url":"http://arxiv.org/abs/2401.17882v1","category":"cs.CL"}
{"created":"2024-01-31 14:39:11","title":"PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition","abstract":"Multi-label image recognition is a fundamental task in computer vision. Recently, vision-language models have made notable advancements in this area. However, previous methods often failed to effectively leverage the rich knowledge within language models and instead incorporated label semantics into visual features in a unidirectional manner. In this paper, we propose a Prompt-driven Visual-Linguistic Representation Learning (PVLR) framework to better leverage the capabilities of the linguistic modality. In PVLR, we first introduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP) and Context-Aware Prompting (CAP). KAP utilizes fixed prompts to capture the intrinsic semantic knowledge and relationships across all labels, while CAP employs learnable prompts to capture context-aware label semantics and relationships. Later, we propose an Interaction and Fusion Module (IFM) to interact and fuse the representations obtained from KAP and CAP. In contrast to the unidirectional fusion in previous works, we introduce a Dual-Modal Attention (DMA) that enables bidirectional interaction between textual and visual features, yielding context-aware label representations and semantic-related visual representations, which are subsequently used to calculate similarities and generate final predictions for all labels. Extensive experiments on three popular datasets including MS-COCO, Pascal VOC 2007, and NUS-WIDE demonstrate the superiority of PVLR.","sentences":["Multi-label image recognition is a fundamental task in computer vision.","Recently, vision-language models have made notable advancements in this area.","However, previous methods often failed to effectively leverage the rich knowledge within language models and instead incorporated label semantics into visual features in a unidirectional manner.","In this paper, we propose a Prompt-driven Visual-Linguistic Representation Learning (PVLR) framework to better leverage the capabilities of the linguistic modality.","In PVLR, we first introduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP) and Context-Aware Prompting (CAP).","KAP utilizes fixed prompts to capture the intrinsic semantic knowledge and relationships across all labels, while CAP employs learnable prompts to capture context-aware label semantics and relationships.","Later, we propose an Interaction and Fusion Module (IFM) to interact and fuse the representations obtained from KAP and CAP.","In contrast to the unidirectional fusion in previous works, we introduce a Dual-Modal Attention (DMA) that enables bidirectional interaction between textual and visual features, yielding context-aware label representations and semantic-related visual representations, which are subsequently used to calculate similarities and generate final predictions for all labels.","Extensive experiments on three popular datasets including MS-COCO, Pascal VOC 2007, and NUS-WIDE demonstrate the superiority of PVLR."],"url":"http://arxiv.org/abs/2401.17881v1","category":"cs.CV"}
{"created":"2024-01-31 14:37:06","title":"Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication","abstract":"In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments. The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem. Multi-agent reinforcement learning is a significant solution for the above decision-making. However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application. In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem. Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information. The attention mechanism provides additional weighting for conveyed information, so that the critic network can accurately evaluate the value of behavior for UAV BSs. This provides more reliable feedback signals and helps the actor network update the strategy more effectively. Ablation simulations indicate that the proposed approach attains improved convergence over the baselines. UAV BSs learn the optimal communication strategies to achieve their maximum cumulative rewards. Additionally, multi-agent trust region method with monotonic convergence provides an estimated Nash equilibrium for the multi-UAV assisted communication Markov game.","sentences":["In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments.","The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem.","Multi-agent reinforcement learning is a significant solution for the above decision-making.","However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application.","In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem.","Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information.","The attention mechanism provides additional weighting for conveyed information, so that the critic network can accurately evaluate the value of behavior for UAV BSs.","This provides more reliable feedback signals and helps the actor network update the strategy more effectively.","Ablation simulations indicate that the proposed approach attains improved convergence over the baselines.","UAV BSs learn the optimal communication strategies to achieve their maximum cumulative rewards.","Additionally, multi-agent trust region method with monotonic convergence provides an estimated Nash equilibrium for the multi-UAV assisted communication Markov game."],"url":"http://arxiv.org/abs/2401.17880v1","category":"cs.MA"}
{"created":"2024-01-31 14:36:49","title":"AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error","abstract":"With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions.","sentences":["With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation.","A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs).","In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space.","Despite their relevance, the forensic analysis of LDMs is still in its infancy.","In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space.","We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error.","Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training.","We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney.","Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions."],"url":"http://arxiv.org/abs/2401.17879v1","category":"cs.CV"}
{"created":"2024-01-31 14:36:44","title":"A Survey on Data-Centric Recommender Systems","abstract":"Recommender systems (RS) have become essential tools for mitigating information overload in a range of real-world scenarios. Recent trends in RS have seen a paradigm shift, moving the spotlight from model-centric innovations to the importance of data quality and quantity. This evolution has given rise to the concept of data-centric recommender systems (Data-Centric RS), marking a significant development in the field. This survey provides the first systematic overview of Data-Centric RS, covering 1) the foundational concepts of recommendation data and Data-Centric RS; 2) three primary issues in recommendation data; 3) recent research developed to address these issues; and 4) several potential future directions in Data-Centric RS.","sentences":["Recommender systems (RS) have become essential tools for mitigating information overload in a range of real-world scenarios.","Recent trends in RS have seen a paradigm shift, moving the spotlight from model-centric innovations to the importance of data quality and quantity.","This evolution has given rise to the concept of data-centric recommender systems (Data-Centric RS), marking a significant development in the field.","This survey provides the first systematic overview of Data-Centric RS, covering 1) the foundational concepts of recommendation data and Data-Centric RS; 2) three primary issues in recommendation data; 3) recent research developed to address these issues; and 4) several potential future directions in Data-Centric RS."],"url":"http://arxiv.org/abs/2401.17878v1","category":"cs.IR"}
{"created":"2024-01-31 14:33:20","title":"Perspective: Atomistic Simulations of Water and Aqueous Systems with Machine Learning Potentials","abstract":"As the most important solvent, water has been at the center of interest since the advent of computer simulations. While early molecular dynamics and Monte Carlo simulations had to make use of simple model potentials to describe the atomic interactions, accurate ab initio molecular dynamics simulations relying on the first-principles calculation of the energies and forces have opened the way to predictive simulations of aqueous systems. Still, these simulations are very demanding, which prevents the study of complex systems and their properties. Modern machine learning potentials (MLPs) have now reached a mature state, allowing to overcome these limitations by combining the high accuracy of electronic structure calculations with the efficiency of empirical force fields. In this Perspective we give a concise overview about the progress made in the simulation of water and aqueous systems employing MLPs, starting from early work on free molecules and clusters via bulk liquid water to electrolyte solutions and solid-liquid interfaces.","sentences":["As the most important solvent, water has been at the center of interest since the advent of computer simulations.","While early molecular dynamics and Monte Carlo simulations had to make use of simple model potentials to describe the atomic interactions, accurate ab initio molecular dynamics simulations relying on the first-principles calculation of the energies and forces have opened the way to predictive simulations of aqueous systems.","Still, these simulations are very demanding, which prevents the study of complex systems and their properties.","Modern machine learning potentials (MLPs) have now reached a mature state, allowing to overcome these limitations by combining the high accuracy of electronic structure calculations with the efficiency of empirical force fields.","In this Perspective we give a concise overview about the progress made in the simulation of water and aqueous systems employing MLPs, starting from early work on free molecules and clusters via bulk liquid water to electrolyte solutions and solid-liquid interfaces."],"url":"http://arxiv.org/abs/2401.17875v1","category":"cond-mat.soft"}
{"created":"2024-01-31 14:32:56","title":"VR-based generation of photorealistic synthetic data for training hand-object tracking models","abstract":"Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present \"blender-hoisynth\", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.","sentences":["Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training.","Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images.","To address these issues, we present \"blender-hoisynth\", an interactive synthetic data generator based on the Blender software.","Blender-hoisynth can scalably generate and automatically annotate visual HOI training data.","Other competing approaches usually generate synthetic HOI data compeletely without human input.","While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent.","With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware.","The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it.","We show that there is no significant degradation in the model performance despite the data replacement."],"url":"http://arxiv.org/abs/2401.17874v1","category":"cs.CV"}
{"created":"2024-01-31 14:27:35","title":"Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers","abstract":"Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enhances predictability on four surface and five upper-level atmospheric variables at a two-week lead time. Furthermore, the teleconnection-filtered features improve the spatial granularity of outputs significantly, indicating their potential physical consistency. Our research underscores the importance of atmospheric and oceanic teleconnections in driving future weather conditions. Besides, it presents a resource-efficient pathway for researchers to leverage existing foundation models on versatile downstream tasks.","sentences":["Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere.","Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models.","However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability.","Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning.","To deal with the aforementioned problems, we propose a teleconnection-informed transformer.","Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range.","Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enhances predictability on four surface and five upper-level atmospheric variables at a two-week lead time.","Furthermore, the teleconnection-filtered features improve the spatial granularity of outputs significantly, indicating their potential physical consistency.","Our research underscores the importance of atmospheric and oceanic teleconnections in driving future weather conditions.","Besides, it presents a resource-efficient pathway for researchers to leverage existing foundation models on versatile downstream tasks."],"url":"http://arxiv.org/abs/2401.17870v1","category":"cs.LG"}
{"created":"2024-01-31 14:27:31","title":"Deconfinement transition within the Curci-Ferrari model -- Renormalization scale and scheme dependences","abstract":"We analyze the confinement/deconfinement transition of pure Yang-Mills theories within the framework of the center-symmetric Landau gauge supplemented by a Curci-Ferrari mass term that models the effect of the associated Gribov copies in the infrared. In addition to providing details for earlier one-loop calculations in that framework, we explore how the results depend on the renormalization scale and/or on the renormalization scheme. We find that the predicted values for the transition temperatures of SU($2$) and SU($3$) Yang-Mills theories are similar in both schemes and are little sensitive to the renormalization scale $\\mu$ over a wide range of values including the standard range $\\smash{\\mu\\in[\\pi T,4\\pi T]}$. These values are also close both to those obtained from a minimal sensitivity principle and to those of lattice simulations, especially in the SU($3$) case. These results further confirm the good behavior of perturbative calculations within the Curci-Ferrari model and support the adequacy of the latter as an effective description of Yang-Mills theories in the infrared. We perform a similar analysis for the spinodal temperatures in the SU($3$) case and for the Polyakov loop, the order parameter associated to the breaking of center symmetry.","sentences":["We analyze the confinement/deconfinement transition of pure Yang-Mills theories within the framework of the center-symmetric Landau gauge supplemented by a Curci-Ferrari mass term that models the effect of the associated Gribov copies in the infrared.","In addition to providing details for earlier one-loop calculations in that framework, we explore how the results depend on the renormalization scale and/or on the renormalization scheme.","We find that the predicted values for the transition temperatures of SU($2$) and SU($3$) Yang-Mills theories are similar in both schemes and are little sensitive to the renormalization scale $\\mu$ over a wide range of values including the standard range $\\smash{\\mu\\in[\\pi T,4\\pi T]}$.","These values are also close both to those obtained from a minimal sensitivity principle and to those of lattice simulations, especially in the SU($3$) case.","These results further confirm the good behavior of perturbative calculations within the Curci-Ferrari model and support the adequacy of the latter as an effective description of Yang-Mills theories in the infrared.","We perform a similar analysis for the spinodal temperatures in the SU($3$) case and for the Polyakov loop, the order parameter associated to the breaking of center symmetry."],"url":"http://arxiv.org/abs/2401.17869v1","category":"hep-ph"}
{"created":"2024-01-31 14:27:07","title":"Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model","abstract":"The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks.","sentences":["The Segment Anything Model (SAM) stands as a foundational framework for image segmentation.","While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing.","To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach.","By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption.","Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining.","Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic segmentation tasks."],"url":"http://arxiv.org/abs/2401.17868v1","category":"cs.CV"}
{"created":"2024-01-31 14:25:05","title":"Making Sense of Knowledge Intensive Processes: an Oil & Gas Industry Scenario","abstract":"Sensemaking is a constant and ongoing process by which people associate meaning to experiences. It can be an individual process, known as abduction, or a group process by which people give meaning to collective experiences. The sensemaking of a group is influenced by the abduction process of each person about the experience. Every collaborative process needs some level of sensemaking to show results. For a knowledge intensive process, sensemaking is central and related to most of its tasks. We present findings from a fieldwork executed in knowledge intensive process from the Oil and Gas industry. Our findings indicated that different types of knowledge can be combined to compose the result of a sensemaking process (e.g. decision, the need for more discussion, etc.). This paper presents an initial set of knowledge types that can be combined to compose the result of the sensemaking of a collaborative decision making process. We also discuss ideas for using systems powered by Artificial Intelligence to support sensemaking processes.","sentences":["Sensemaking is a constant and ongoing process by which people associate meaning to experiences.","It can be an individual process, known as abduction, or a group process by which people give meaning to collective experiences.","The sensemaking of a group is influenced by the abduction process of each person about the experience.","Every collaborative process needs some level of sensemaking to show results.","For a knowledge intensive process, sensemaking is central and related to most of its tasks.","We present findings from a fieldwork executed in knowledge intensive process from the Oil and Gas industry.","Our findings indicated that different types of knowledge can be combined to compose the result of a sensemaking process (e.g. decision, the need for more discussion, etc.).","This paper presents an initial set of knowledge types that can be combined to compose the result of the sensemaking of a collaborative decision making process.","We also discuss ideas for using systems powered by Artificial Intelligence to support sensemaking processes."],"url":"http://arxiv.org/abs/2401.17866v1","category":"cs.HC"}
{"created":"2024-01-31 14:23:51","title":"Manipulating Predictions over Discrete Inputs in Machine Teaching","abstract":"Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have superior performance in effectively and efficiently manipulating the predictions of the model, surpassing conventional baselines.","sentences":["Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher.","While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited.","This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently.","We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm.","Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits.","Experimental results show that our proposed algorithm can have superior performance in effectively and efficiently manipulating the predictions of the model, surpassing conventional baselines."],"url":"http://arxiv.org/abs/2401.17865v1","category":"cs.LG"}
{"created":"2024-01-31 14:21:49","title":"Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis","abstract":"Multi-modal large language models (MLLMs) have demonstrated remarkable vision-language capabilities, primarily due to the exceptional in-context understanding and multi-task learning strengths of large language models (LLMs). The advent of visual instruction tuning has further enhanced MLLMs' performance in vision-language understanding. However, while existing MLLMs adeptly recognize \\textit{what} objects are in an image, they still face challenges in effectively discerning \\textit{where} these objects are, particularly along the distance (scene depth) axis. To overcome this limitation in MLLMs, we introduce Proximity Question Answering (Proximity QA), a novel framework designed to enable MLLMs to infer the proximity relationship between objects in images. The framework operates in two phases: the first phase focuses on guiding the models to understand the relative depth of objects, and the second phase further encourages the models to infer the proximity relationships between objects based on their depth perceptions. We also propose a VQA dataset called Proximity-110K, containing additional instructions that incorporate depth information and the proximity relationships of objects. We have conducted extensive experiments to validate Proximity QA's superior ability in depth perception and proximity analysis, outperforming other state-of-the-art MLLMs. Code and dataset will be released at \\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}.","sentences":["Multi-modal large language models (MLLMs) have demonstrated remarkable vision-language capabilities, primarily due to the exceptional in-context understanding and multi-task learning strengths of large language models (LLMs).","The advent of visual instruction tuning has further enhanced MLLMs' performance in vision-language understanding.","However, while existing MLLMs adeptly recognize \\textit{what} objects are in an image, they still face challenges in effectively discerning \\textit{where} these objects are, particularly along the distance (scene depth) axis.","To overcome this limitation in MLLMs, we introduce Proximity Question Answering (Proximity QA), a novel framework designed to enable MLLMs to infer the proximity relationship between objects in images.","The framework operates in two phases: the first phase focuses on guiding the models to understand the relative depth of objects, and the second phase further encourages the models to infer the proximity relationships between objects based on their depth perceptions.","We also propose a VQA dataset called Proximity-110K, containing additional instructions that incorporate depth information and the proximity relationships of objects.","We have conducted extensive experiments to validate Proximity QA's superior ability in depth perception and proximity analysis, outperforming other state-of-the-art MLLMs.","Code and dataset will be released at \\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}."],"url":"http://arxiv.org/abs/2401.17862v1","category":"cs.CV"}
{"created":"2024-01-31 14:19:08","title":"Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment","abstract":"In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA) is crucial for identifying identical entities across diverse modal attributes. However, semantic inconsistency, mainly due to missing modal attributes, poses a significant challenge. Traditional approaches rely on attribute interpolation, but this often introduces modality noise, distorting the original semantics. Moreover, the lack of a universal theoretical framework limits advancements in achieving semantic consistency. This study introduces a novel approach, DESAlign, which addresses these issues by applying a theoretical framework based on Dirichlet energy to ensure semantic consistency. We discover that semantic inconsistency leads to model overfitting to modality noise, causing performance fluctuations, particularly when modalities are missing. DESAlign innovatively combats over-smoothing and interpolates absent semantics using existing modalities. Our approach includes a multi-modal knowledge graph learning strategy and a propagation technique that employs existing semantic features to compensate for missing ones, providing explicit Euler solutions. Comprehensive evaluations across 18 benchmarks, including monolingual and bilingual scenarios, demonstrate that DESAlign surpasses existing methods, setting a new standard in performance. Further testing on 42 benchmarks with high rates of missing modalities confirms its robustness, offering an effective solution to semantic inconsistency in real-world MMKGs.","sentences":["In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA) is crucial for identifying identical entities across diverse modal attributes.","However, semantic inconsistency, mainly due to missing modal attributes, poses a significant challenge.","Traditional approaches rely on attribute interpolation, but this often introduces modality noise, distorting the original semantics.","Moreover, the lack of a universal theoretical framework limits advancements in achieving semantic consistency.","This study introduces a novel approach, DESAlign, which addresses these issues by applying a theoretical framework based on Dirichlet energy to ensure semantic consistency.","We discover that semantic inconsistency leads to model overfitting to modality noise, causing performance fluctuations, particularly when modalities are missing.","DESAlign innovatively combats over-smoothing and interpolates absent semantics using existing modalities.","Our approach includes a multi-modal knowledge graph learning strategy and a propagation technique that employs existing semantic features to compensate for missing ones, providing explicit Euler solutions.","Comprehensive evaluations across 18 benchmarks, including monolingual and bilingual scenarios, demonstrate that DESAlign surpasses existing methods, setting a new standard in performance.","Further testing on 42 benchmarks with high rates of missing modalities confirms its robustness, offering an effective solution to semantic inconsistency in real-world MMKGs."],"url":"http://arxiv.org/abs/2401.17859v1","category":"cs.IR"}
{"created":"2024-01-31 14:19:03","title":"Semantic Anything in 3D Gaussians","abstract":"3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed. Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain. Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc. In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters. We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians. Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods. We also propose a cross-view label-voting approach to assign labels from different views. In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks. Codes will be released soon.","sentences":["3D Gaussian Splatting has emerged as an alternative 3D representation of Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering results and real-time rendering speed.","Considering the 3D Gaussian representation remains unparsed, it is necessary first to execute object segmentation within this domain.","Subsequently, scene editing and collision detection can be performed, proving vital to a multitude of applications, such as virtual reality (VR), augmented reality (AR), game/movie production, etc.","In this paper, we propose a novel approach to achieve object segmentation in 3D Gaussian via an interactive procedure without any training process and learned parameters.","We refer to the proposed method as SA-GS, for Segment Anything in 3D Gaussians.","Given a set of clicked points in a single input view, SA-GS can generalize SAM to achieve 3D consistent segmentation via the proposed multi-view mask generation and view-wise label assignment methods.","We also propose a cross-view label-voting approach to assign labels from different views.","In addition, in order to address the boundary roughness issue of segmented objects resulting from the non-negligible spatial sizes of 3D Gaussian located at the boundary, SA-GS incorporates the simple but effective Gaussian Decomposition scheme.","Extensive experiments demonstrate that SA-GS achieves high-quality 3D segmentation results, which can also be easily applied for scene editing and collision detection tasks.","Codes will be released soon."],"url":"http://arxiv.org/abs/2401.17857v1","category":"cs.CV"}
{"created":"2024-01-31 14:19:03","title":"Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction","abstract":"The rise of Large Language Models (LLMs) has affected various disciplines that got beyond mere text generation. Going beyond their textual nature, this project proposal aims to investigate the interaction between LLMs and non-verbal communication, specifically focusing on gestures. The proposal sets out a plan to examine the proficiency of LLMs in deciphering both explicit and implicit non-verbal cues within textual prompts and their ability to associate these gestures with various contextual factors. The research proposes to test established psycholinguistic study designs to construct a comprehensive dataset that pairs textual prompts with detailed gesture descriptions, encompassing diverse regional variations, and semantic labels. To assess LLMs' comprehension of gestures, experiments are planned, evaluating their ability to simulate human behaviour in order to replicate psycholinguistic experiments. These experiments consider cultural dimensions and measure the agreement between LLM-identified gestures and the dataset, shedding light on the models' contextual interpretation of non-verbal cues (e.g. gestures).","sentences":["The rise of Large Language Models (LLMs) has affected various disciplines that got beyond mere text generation.","Going beyond their textual nature, this project proposal aims to investigate the interaction between LLMs and non-verbal communication, specifically focusing on gestures.","The proposal sets out a plan to examine the proficiency of LLMs in deciphering both explicit and implicit non-verbal cues within textual prompts and their ability to associate these gestures with various contextual factors.","The research proposes to test established psycholinguistic study designs to construct a comprehensive dataset that pairs textual prompts with detailed gesture descriptions, encompassing diverse regional variations, and semantic labels.","To assess LLMs' comprehension of gestures, experiments are planned, evaluating their ability to simulate human behaviour in order to replicate psycholinguistic experiments.","These experiments consider cultural dimensions and measure the agreement between LLM-identified gestures and the dataset, shedding light on the models' contextual interpretation of non-verbal cues (e.g. gestures)."],"url":"http://arxiv.org/abs/2401.17858v1","category":"cs.CL"}
{"created":"2024-01-31 14:17:52","title":"Beyond Numbers: Creating Analogies to Enhance Data Comprehension and Communication with Generative AI","abstract":"Unfamiliar measurements usually hinder readers from grasping the scale of the numerical data, understanding the content, and feeling engaged with the context. To enhance data comprehension and communication, we leverage analogies to bridge the gap between abstract data and familiar measurements. In this work, we first conduct semi-structured interviews with design experts to identify design problems and summarize design considerations. Then, we collect an analogy dataset of 138 cases from various online sources. Based on the collected dataset, we characterize a design space for creating data analogies. Next, we build a prototype system, AnalogyMate, that automatically suggests data analogies, their corresponding design solutions, and generated visual representations powered by generative AI. The study results show the usefulness of AnalogyMate in aiding the creation process of data analogies and the effectiveness of data analogy in enhancing data comprehension and communication.","sentences":["Unfamiliar measurements usually hinder readers from grasping the scale of the numerical data, understanding the content, and feeling engaged with the context.","To enhance data comprehension and communication, we leverage analogies to bridge the gap between abstract data and familiar measurements.","In this work, we first conduct semi-structured interviews with design experts to identify design problems and summarize design considerations.","Then, we collect an analogy dataset of 138 cases from various online sources.","Based on the collected dataset, we characterize a design space for creating data analogies.","Next, we build a prototype system, AnalogyMate, that automatically suggests data analogies, their corresponding design solutions, and generated visual representations powered by generative AI.","The study results show the usefulness of AnalogyMate in aiding the creation process of data analogies and the effectiveness of data analogy in enhancing data comprehension and communication."],"url":"http://arxiv.org/abs/2401.17856v1","category":"cs.HC"}
{"created":"2024-01-31 14:15:31","title":"Conformal invariants of curves via those for inscribed polygons with circular edges","abstract":"The conformal nature of smooth curves in $\\mathbb{R}^3$ is characterised by conformal length, curvature and torsion. We present a derivation of these conformal parameters via a limiting process using inscribed polygons with circular edges . The procedure is based on elementary geometry in $\\mathbb{R}^3$ only and similar to the rectification of curves in the metrical case. It seems to be not available in the literature so far.","sentences":["The conformal nature of smooth curves in $\\mathbb{R}^3$ is characterised by conformal length, curvature and torsion.","We present a derivation of these conformal parameters via a limiting process using inscribed polygons with circular edges .","The procedure is based on elementary geometry in $\\mathbb{R}^3$ only and similar to the rectification of curves in the metrical case.","It seems to be not available in the literature so far."],"url":"http://arxiv.org/abs/2401.17854v1","category":"math.DG"}
{"created":"2024-01-31 14:13:01","title":"Instruction-Guided Scene Text Recognition","abstract":"Multi-modal models have shown appealing performance in visual tasks recently, as instruction-guided training has evoked the ability to understand fine-grained visual content. However, current methods cannot be trivially applied to scene text recognition (STR) due to the gap between natural and text images. In this paper, we introduce a novel paradigm that formulates STR as an instruction learning problem, and propose instruction-guided scene text recognition (IGTR) to achieve effective cross-modal learning. IGTR first generates rich and diverse instruction triplets of <condition,question,answer>, serving as guidance for nuanced text image understanding. Then, we devise an architecture with dedicated cross-modal feature fusion module, and multi-task answer head to effectively fuse the required instruction and image features for answering questions. Built upon these designs, IGTR facilitates accurate text recognition by comprehending character attributes. Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins. Furthermore, by adjusting the instructions, IGTR enables various recognition schemes. These include zero-shot prediction, where the model is trained based on instructions not explicitly targeting character recognition, and the recognition of rarely appearing and morphologically similar characters, which were previous challenges for existing models.","sentences":["Multi-modal models have shown appealing performance in visual tasks recently, as instruction-guided training has evoked the ability to understand fine-grained visual content.","However, current methods cannot be trivially applied to scene text recognition (STR) due to the gap between natural and text images.","In this paper, we introduce a novel paradigm that formulates STR as an instruction learning problem, and propose instruction-guided scene text recognition (IGTR) to achieve effective cross-modal learning.","IGTR first generates rich and diverse instruction triplets of <condition,question,answer>, serving as guidance for nuanced text image understanding.","Then, we devise an architecture with dedicated cross-modal feature fusion module, and multi-task answer head to effectively fuse the required instruction and image features for answering questions.","Built upon these designs, IGTR facilitates accurate text recognition by comprehending character attributes.","Experiments on English and Chinese benchmarks show that IGTR outperforms existing models by significant margins.","Furthermore, by adjusting the instructions, IGTR enables various recognition schemes.","These include zero-shot prediction, where the model is trained based on instructions not explicitly targeting character recognition, and the recognition of rarely appearing and morphologically similar characters, which were previous challenges for existing models."],"url":"http://arxiv.org/abs/2401.17851v1","category":"cs.CV"}
{"created":"2024-01-31 14:04:47","title":"Drift Diffusion Model to understand (mis)information sharing dynamic in complex networks","abstract":"Sharing misinformation threatens societies as misleading news shapes the risk perception of individuals. We witnessed this during the COVID-19 pandemic, where misinformation undermined the effectiveness of stay-at-home orders, posing an additional obstacle in the fight against the virus. In this research, we study misinformation spreading, reanalyzing behavioral data on online sharing, and analyzing decision-making mechanisms using the Drift Diffusion Model (DDM). We find that subjects display an increased instinctive inclination towards sharing misleading news, but rational thinking significantly curbs this reaction, especially for more cautious and older individuals. Using an agent-based model, we expand this individual knowledge to a social network where individuals are exposed to misinformation through friends and share (or not) content with probabilities driven by DDM. The natural shape of the Twitter network provides a fertile ground for any news to rapidly become viral, yet we found that limiting users' followers proves to be an appropriate and feasible containment strategy.","sentences":["Sharing misinformation threatens societies as misleading news shapes the risk perception of individuals.","We witnessed this during the COVID-19 pandemic, where misinformation undermined the effectiveness of stay-at-home orders, posing an additional obstacle in the fight against the virus.","In this research, we study misinformation spreading, reanalyzing behavioral data on online sharing, and analyzing decision-making mechanisms using the Drift Diffusion Model (DDM).","We find that subjects display an increased instinctive inclination towards sharing misleading news, but rational thinking significantly curbs this reaction, especially for more cautious and older individuals.","Using an agent-based model, we expand this individual knowledge to a social network where individuals are exposed to misinformation through friends and share (or not) content with probabilities driven by DDM.","The natural shape of the Twitter network provides a fertile ground for any news to rapidly become viral, yet we found that limiting users' followers proves to be an appropriate and feasible containment strategy."],"url":"http://arxiv.org/abs/2401.17846v1","category":"physics.soc-ph"}
{"created":"2024-01-31 14:03:15","title":"On the breakdown of eikonal approximation and survival of Reggeization in presence of dimension-5 Higgs-gluon coupling","abstract":"We consider the one-loop effective vertex for the interaction of a gluon with a Reggeized gluon and a Higgs boson in the infinite-top-mass limit, which is described by a dimension-5 non-renormalizable operator. This vertex enters the calculation of differential cross sections for the forward inclusive production of a Higgs boson in high-energy proton-proton collisions, possibly in association with a backward jet or identified hadron, in a framework where next-to-leading logarithms of the energy are resummed to all orders. The effective vertex is extracted from the high-energy behavior of two-to-two amplitudes for the Higgs production in parton-parton collisions and relies on the validity of the Regge form for these amplitudes. We find that the usual eikonal approximation (Gribov prescription) for the Regge limit and the known region-expansion techniques in this limit lead to an incomplete result for the amplitude. The discrepancy is traced back to the non-renormalizable nature of the involved operator. However, the Regge limit of the exact QCD amplitude agrees with the Regge-pole exchange form at one loop, nontrivially supporting the Reggeization hypothesis.","sentences":["We consider the one-loop effective vertex for the interaction of a gluon with a Reggeized gluon and a Higgs boson in the infinite-top-mass limit, which is described by a dimension-5 non-renormalizable operator.","This vertex enters the calculation of differential cross sections for the forward inclusive production of a Higgs boson in high-energy proton-proton collisions, possibly in association with a backward jet or identified hadron, in a framework where next-to-leading logarithms of the energy are resummed to all orders.","The effective vertex is extracted from the high-energy behavior of two-to-two amplitudes for the Higgs production in parton-parton collisions and relies on the validity of the Regge form for these amplitudes.","We find that the usual eikonal approximation (Gribov prescription) for the Regge limit and the known region-expansion techniques in this limit lead to an incomplete result for the amplitude.","The discrepancy is traced back to the non-renormalizable nature of the involved operator.","However, the Regge limit of the exact QCD amplitude agrees with the Regge-pole exchange form at one loop, nontrivially supporting the Reggeization hypothesis."],"url":"http://arxiv.org/abs/2401.17843v1","category":"hep-ph"}
{"created":"2024-01-31 14:02:26","title":"Explainable Benchmarking for Iterative Optimization Heuristics","abstract":"Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, allowing for better benchmarking and algorithm design.","sentences":["Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well.","In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results.","This paper presents a novel approach we call explainable benchmarking.","Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters.","We showcase the framework in the context of two modular optimization frameworks.","Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios.","We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterative optimization heuristics in a more transparent and comprehensible manner, allowing for better benchmarking and algorithm design."],"url":"http://arxiv.org/abs/2401.17842v1","category":"cs.NE"}
{"created":"2024-01-31 13:58:23","title":"Propagation Dynamics of Rumor vs. Non-rumor across Multiple Social Media Platforms Driven by User Characteristics","abstract":"Studying information propagation dynamics in social media can elucidate user behaviors and patterns. However, previous research often focuses on single platforms and fails to differentiate between the nuanced roles of source users and other participants in cascades. To address these limitations, we analyze propagation cascades on Twitter and Weibo combined with a crawled dataset of nearly one million users with authentic attributes. Our preliminary findings from multiple platforms robustly indicate that rumors tend to spread more deeply, while non-rumors distribute more broadly. Interestingly, we discover that the spread of rumors is slower, persists longer, and, in most cases, involves fewer participants than that of non-rumors. And an undiscovered highlight is that reputable active users, termed `onlookers', inadvertently or unwittingly spread rumors due to their extensive online interactions and the allure of sensational fake news. Conversely, celebrities exhibit caution, mindful of releasing unverified information. Additionally, we identify cascade features aligning with exponential patterns, highlight the Credibility Erosion Effect (CEE) phenomenon in the propagation process, and discover the different contents and policies between the two platforms. Our findings enhance current understanding and provide a valuable statistical analysis for future research.","sentences":["Studying information propagation dynamics in social media can elucidate user behaviors and patterns.","However, previous research often focuses on single platforms and fails to differentiate between the nuanced roles of source users and other participants in cascades.","To address these limitations, we analyze propagation cascades on Twitter and Weibo combined with a crawled dataset of nearly one million users with authentic attributes.","Our preliminary findings from multiple platforms robustly indicate that rumors tend to spread more deeply, while non-rumors distribute more broadly.","Interestingly, we discover that the spread of rumors is slower, persists longer, and, in most cases, involves fewer participants than that of non-rumors.","And an undiscovered highlight is that reputable active users, termed `onlookers', inadvertently or unwittingly spread rumors due to their extensive online interactions and the allure of sensational fake news.","Conversely, celebrities exhibit caution, mindful of releasing unverified information.","Additionally, we identify cascade features aligning with exponential patterns, highlight the Credibility Erosion Effect (CEE) phenomenon in the propagation process, and discover the different contents and policies between the two platforms.","Our findings enhance current understanding and provide a valuable statistical analysis for future research."],"url":"http://arxiv.org/abs/2401.17840v1","category":"cs.SI"}
{"created":"2024-01-31 13:57:24","title":"Global-Liar: Factuality of LLMs over Time and Geographic Regions","abstract":"The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exacerbating existing informational inequities. Regions such as Africa and the Middle East are at a disadvantage, with much lower factual accuracy. The performance fluctuations over time suggest that model updates may not consistently benefit all regions equally.   Our study also offers insights into the impact of various LLM configuration settings, such as binary decision forcing, model re-runs and temperature, on model's factuality. Models constrained to binary (true/false) choices exhibit reduced factuality compared to those allowing an 'unclear' option. Single inference at a low temperature setting matches the reliability of majority voting across various configurations. The insights gained highlight the need for culturally diverse and geographically inclusive model training and evaluation. This approach is key to achieving global equity in technology, distributing AI benefits fairly worldwide.","sentences":["The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online.","Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   ","We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases.","Our analysis reveals that newer iterations of GPT models do not always equate to improved performance.","Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release.","Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exacerbating existing informational inequities.","Regions such as Africa and the Middle East are at a disadvantage, with much lower factual accuracy.","The performance fluctuations over time suggest that model updates may not consistently benefit all regions equally.   ","Our study also offers insights into the impact of various LLM configuration settings, such as binary decision forcing, model re-runs and temperature, on model's factuality.","Models constrained to binary (true/false) choices exhibit reduced factuality compared to those allowing an 'unclear' option.","Single inference at a low temperature setting matches the reliability of majority voting across various configurations.","The insights gained highlight the need for culturally diverse and geographically inclusive model training and evaluation.","This approach is key to achieving global equity in technology, distributing AI benefits fairly worldwide."],"url":"http://arxiv.org/abs/2401.17839v1","category":"cs.CL"}
{"created":"2024-01-31 13:56:08","title":"A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction","abstract":"The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply variations by incorporating historical demand-supply gaps. Extensive experiments on three real-world datasets demonstrate the superiority of the proposed framework compared to seven baselines and the effectiveness of the three modules.","sentences":["The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market.","Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem.","However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations.","In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction.","Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply variations by incorporating historical demand-supply gaps.","Extensive experiments on three real-world datasets demonstrate the superiority of the proposed framework compared to seven baselines and the effectiveness of the three modules."],"url":"http://arxiv.org/abs/2401.17838v1","category":"cs.LG"}
{"created":"2024-01-31 13:52:37","title":"Broadband biphoton source for quantum optical coherence tomography based on a Michelson interferometer","abstract":"Broadband correlated photon pairs (biphotons) are valuable in quantum metrology, but current generation methods either involve complex nonlinear structures or lack sufficient bandwidth and brightness. In this work, we theoretically describe and experimentally demonstrate a novel technique for generation of a bright collinear biphoton field with a broad spectrum, achieved by using a tightly focused pump in a bulk nonlinear crystal. As the most straightforward application of the source, we employ Michelson interferometer-based quantum optical coherence tomography (QOCT). Utilizing the source enables the demonstration of record resolution and dispersion cancellation for this QOCT scheme.","sentences":["Broadband correlated photon pairs (biphotons) are valuable in quantum metrology, but current generation methods either involve complex nonlinear structures or lack sufficient bandwidth and brightness.","In this work, we theoretically describe and experimentally demonstrate a novel technique for generation of a bright collinear biphoton field with a broad spectrum, achieved by using a tightly focused pump in a bulk nonlinear crystal.","As the most straightforward application of the source, we employ Michelson interferometer-based quantum optical coherence tomography (QOCT).","Utilizing the source enables the demonstration of record resolution and dispersion cancellation for this QOCT scheme."],"url":"http://arxiv.org/abs/2401.17836v1","category":"quant-ph"}
{"created":"2024-01-31 13:52:11","title":"Predicting the Future with Simple World Models","abstract":"World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment. However, the latent dynamics inferred by these models may still be highly complex. Abstracting the dynamics of the environment with simple models can have several benefits. If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states. We propose a regularization scheme that simplifies the world model's latent dynamics. Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them. This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable. We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning. We find that our regularization improves accuracy, generalization, and performance in downstream tasks.","sentences":["World models can represent potentially high-dimensional pixel observations in compact latent spaces, making it tractable to model the dynamics of the environment.","However, the latent dynamics inferred by these models may still be highly complex.","Abstracting the dynamics of the environment with simple models can have several benefits.","If the latent dynamics are simple, the model may generalize better to novel transitions, and discover useful latent representations of environment states.","We propose a regularization scheme that simplifies the world model's latent dynamics.","Our model, the Parsimonious Latent Space Model (PLSM), minimizes the mutual information between latent states and the dynamics that arise between them.","This makes the dynamics softly state-invariant, and the effects of the agent's actions more predictable.","We combine the PLSM with three different model classes used for i) future latent state prediction, ii) video prediction, and iii) planning.","We find that our regularization improves accuracy, generalization, and performance in downstream tasks."],"url":"http://arxiv.org/abs/2401.17835v1","category":"cs.LG"}
{"created":"2024-01-31 13:49:59","title":"SAT-Based Subsumption Resolution","abstract":"Subsumption resolution is an expensive but highly effective simplifying inference for first-order saturation theorem provers. We present a new SAT-based reasoning technique for subsumption resolution, without requiring radical changes to the underlying saturation algorithm. We implemented our work in the theorem prover Vampire, and show that it is noticeably faster than the state of the art.","sentences":["Subsumption resolution is an expensive but highly effective simplifying inference for first-order saturation theorem provers.","We present a new SAT-based reasoning technique for subsumption resolution, without requiring radical changes to the underlying saturation algorithm.","We implemented our work in the theorem prover Vampire, and show that it is noticeably faster than the state of the art."],"url":"http://arxiv.org/abs/2401.17832v1","category":"cs.LO"}
{"created":"2024-01-31 13:46:23","title":"Renormalised energy between boundary vortices in thin-film micromagnetics with Dzyaloshinskii-Moriya interaction","abstract":"We consider a three-dimensional micromagnetic model with Dzyaloshinskii-Moriya interaction in a thin-film regime for boundary vortices. In this regime, we prove a dimension reduction result: the nonlocal three-dimensional model reduces to a local two-dimensional Ginzburg-Landau type model in terms of the averaged magnetization in the thickness of the film. This reduced model captures the interaction between boundary vortices (so-called renormalised energy), that we determine by a $\\Gamma$-convergence result at the second order and then we analyse its minimisers. They nucleate two boundary vortices whose position depends on the Dzyaloshinskii-Moriya interaction.","sentences":["We consider a three-dimensional micromagnetic model with Dzyaloshinskii-Moriya interaction in a thin-film regime for boundary vortices.","In this regime, we prove a dimension reduction result: the nonlocal three-dimensional model reduces to a local two-dimensional Ginzburg-Landau type model in terms of the averaged magnetization in the thickness of the film.","This reduced model captures the interaction between boundary vortices (so-called renormalised energy), that we determine by a $\\Gamma$-convergence result at the second order and then we analyse its minimisers.","They nucleate two boundary vortices whose position depends on the Dzyaloshinskii-Moriya interaction."],"url":"http://arxiv.org/abs/2401.17830v1","category":"math.AP"}
{"created":"2024-01-31 13:41:17","title":"Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation","abstract":"In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing \"SWTformer\" to enhance the accuracy of the initial seed CAMs by bringing local and global views together. SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features. SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination. Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models. It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network. SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer.","sentences":["In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision.","Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs).","Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background.","Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs.","However, the use of hierarchical ViTs has not been extensively explored in this field.","This work explores the use of Swin Transformer by proposing \"SWTformer\" to enhance the accuracy of the initial seed CAMs by bringing local and global views together.","SWTformer-V1 generates class probabilities and CAMs using only the patch tokens as features.","SWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract additional information and utilizes a background-aware mechanism to generate more accurate localization maps with improved cross-object discrimination.","Based on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy, outperforming state-of-the-art models.","It also yields comparable performance by 0.82% mIoU on average higher than other methods in generating initial localization maps, depending only on the classification network.","SWTformer-V2 further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further proving the effectiveness of the local-to-global view provided by the Swin transformer."],"url":"http://arxiv.org/abs/2401.17828v1","category":"cs.CV"}
{"created":"2024-01-31 13:40:00","title":"Neural Machine Translation for Malayalam Paraphrase Generation","abstract":"This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models. We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation. Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment. This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages.","sentences":["This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models.","We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation.","Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment.","This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages."],"url":"http://arxiv.org/abs/2401.17827v1","category":"cs.CL"}
{"created":"2024-01-31 13:39:59","title":"PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation","abstract":"Accurately generating ground truth (GT) trajectories is essential for Simultaneous Localization and Mapping (SLAM) evaluation, particularly under varying environmental conditions. This study introduces a systematic approach employing a prior map-assisted framework for generating dense six-degree-of-freedom (6-DoF) GT poses for the first time, enhancing the fidelity of both indoor and outdoor SLAM datasets. Our method excels in handling degenerate and stationary conditions frequently encountered in SLAM datasets, thereby increasing robustness and precision. A significant aspect of our approach is the detailed derivation of covariances within the factor graph, enabling an in-depth analysis of pose uncertainty propagation. This analysis crucially contributes to demonstrating specific pose uncertainties and enhancing trajectory reliability from both theoretical and empirical perspectives. Additionally, we provide an open-source toolbox (https://github.com/JokerJohn/Cloud_Map_Evaluation) for map evaluation criteria, facilitating the indirect assessment of overall trajectory precision. Experimental results show at least a 30\\% improvement in map accuracy and a 20\\% increase in direct trajectory accuracy compared to the Iterative Closest Point (ICP) \\cite{sharp2002icp} algorithm across diverse campus environments, with substantially enhanced robustness. Our open-source solution (https://github.com/JokerJohn/PALoc), extensively applied in the FusionPortable\\cite{Jiao2022Mar} dataset, is geared towards SLAM benchmark dataset augmentation and represents a significant advancement in SLAM evaluations.","sentences":["Accurately generating ground truth (GT) trajectories is essential for Simultaneous Localization and Mapping (SLAM) evaluation, particularly under varying environmental conditions.","This study introduces a systematic approach employing a prior map-assisted framework for generating dense six-degree-of-freedom (6-DoF) GT poses for the first time, enhancing the fidelity of both indoor and outdoor SLAM datasets.","Our method excels in handling degenerate and stationary conditions frequently encountered in SLAM datasets, thereby increasing robustness and precision.","A significant aspect of our approach is the detailed derivation of covariances within the factor graph, enabling an in-depth analysis of pose uncertainty propagation.","This analysis crucially contributes to demonstrating specific pose uncertainties and enhancing trajectory reliability from both theoretical and empirical perspectives.","Additionally, we provide an open-source toolbox (https://github.com/JokerJohn/Cloud_Map_Evaluation) for map evaluation criteria, facilitating the indirect assessment of overall trajectory precision.","Experimental results show at least a 30\\% improvement in map accuracy and a 20\\% increase in direct trajectory accuracy compared to the Iterative Closest Point (ICP) \\cite{sharp2002icp} algorithm across diverse campus environments, with substantially enhanced robustness.","Our open-source solution (https://github.com/JokerJohn/PALoc), extensively applied in the FusionPortable\\cite{Jiao2022Mar} dataset, is geared towards SLAM benchmark dataset augmentation and represents a significant advancement in SLAM evaluations."],"url":"http://arxiv.org/abs/2401.17826v1","category":"cs.RO"}
{"created":"2024-01-31 13:35:07","title":"A Survey of Pre-trained Language Models for Processing Scientific Text","abstract":"The number of Language Models (LMs) dedicated to processing scientific text is on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs) has become a daunting task for researchers. To date, no comprehensive surveys on SciLMs have been undertaken, leaving this issue unaddressed. Given the constant stream of new SciLMs, appraising the state-of-the-art and how they compare to each other remain largely unknown. This work fills that gap and provides a comprehensive review of SciLMs, including an extensive analysis of their effectiveness across different domains, tasks and datasets, and a discussion on the challenges that lie ahead.","sentences":["The number of Language Models (LMs) dedicated to processing scientific text is on the rise.","Keeping pace with the rapid growth of scientific LMs (SciLMs) has become a daunting task for researchers.","To date, no comprehensive surveys on SciLMs have been undertaken, leaving this issue unaddressed.","Given the constant stream of new SciLMs, appraising the state-of-the-art and how they compare to each other remain largely unknown.","This work fills that gap and provides a comprehensive review of SciLMs, including an extensive analysis of their effectiveness across different domains, tasks and datasets, and a discussion on the challenges that lie ahead."],"url":"http://arxiv.org/abs/2401.17824v1","category":"cs.CL"}
{"created":"2024-01-31 13:28:07","title":"Privacy-preserving data release leveraging optimal transport and particle gradient descent","abstract":"We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.","sentences":["We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government.","Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals.","In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent.","Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints."],"url":"http://arxiv.org/abs/2401.17823v1","category":"cs.LG"}
{"created":"2024-01-31 13:24:51","title":"Do Object Detection Localization Errors Affect Human Performance and Trust?","abstract":"Bounding boxes are often used to communicate automatic object detection results to humans, aiding humans in a multitude of tasks. We investigate the relationship between bounding box localization errors and human task performance. We use observer performance studies on a visual multi-object counting task to measure both human trust and performance with different levels of bounding box accuracy. The results show that localization errors have no significant impact on human accuracy or trust in the system. Recall and precision errors impact both human performance and trust, suggesting that optimizing algorithms based on the F1 score is more beneficial in human-computer tasks. Lastly, the paper offers an improvement on bounding boxes in multi-object counting tasks with center dots, showing improved performance and better resilience to localization inaccuracy.","sentences":["Bounding boxes are often used to communicate automatic object detection results to humans, aiding humans in a multitude of tasks.","We investigate the relationship between bounding box localization errors and human task performance.","We use observer performance studies on a visual multi-object counting task to measure both human trust and performance with different levels of bounding box accuracy.","The results show that localization errors have no significant impact on human accuracy or trust in the system.","Recall and precision errors impact both human performance and trust, suggesting that optimizing algorithms based on the F1 score is more beneficial in human-computer tasks.","Lastly, the paper offers an improvement on bounding boxes in multi-object counting tasks with center dots, showing improved performance and better resilience to localization inaccuracy."],"url":"http://arxiv.org/abs/2401.17821v1","category":"cs.CV"}
{"created":"2024-01-31 13:23:26","title":"The 1/3-conjectures for domination in cubic graphs","abstract":"A set S of vertices in a graph G is a dominating set of G if every vertex not in S is adjacent to a vertex in S . The domination number of G, denoted by $\\gamma$(G), is the minimum cardinality of a dominating set in G. In a breakthrough paper in 2008, L{\\\"o}wenstein and Rautenbach proved that if G is a cubic graph of order n and girth at least 83, then $\\gamma$(G) $\\le$ n/3. A natural question is if this girth condition can be lowered. The question gave birth to two 1/3-conjectures for domination in cubic graphs. The first conjecture, posed by Verstraete in 2010, states that if G is a cubic graph on n vertices with girth at least 6, then $\\gamma$(G) $\\le$ n/3. The second conjecture, first posed as a question by Kostochka in 2009, states that if G is a cubic, bipartite graph of order n, then $\\gamma$(G) $\\le$n/3. In this paper, we prove Verstraete's conjecture when there is no 7-cycle and no 8-cycle, and we prove the Kostochka's related conjecture for bipartite graphs when there is no 4-cycle and no 8-cycle.","sentences":["A set S of vertices in a graph G is a dominating set of G if every vertex not in S is adjacent to a vertex in S .","The domination number of G, denoted by $\\gamma$(G), is the minimum cardinality of a dominating set in G. In a breakthrough paper in 2008, L{\\\"o}wenstein and Rautenbach proved that if G is a cubic graph of order n and girth at least 83, then $\\gamma$(G) $\\le$ n/3.","A natural question is if this girth condition can be lowered.","The question gave birth to two 1/3-conjectures for domination in cubic graphs.","The first conjecture, posed by Verstraete in 2010, states that if G is a cubic graph on n vertices with girth at least 6, then $\\gamma$(G) $\\le$ n/3.","The second conjecture, first posed as a question by Kostochka in 2009, states that if G is a cubic, bipartite graph of order n, then $\\gamma$(G) $\\le$n/3.","In this paper, we prove Verstraete's conjecture when there is no 7-cycle and no 8-cycle, and we prove the Kostochka's related conjecture for bipartite graphs when there is no 4-cycle and no 8-cycle."],"url":"http://arxiv.org/abs/2401.17820v1","category":"cs.DM"}
{"created":"2024-01-31 13:22:57","title":"QTFlow: Quantitative Timing-Sensitive Information Flow for Security-Aware Hardware Design on RTL","abstract":"In contemporary Electronic Design Automation (EDA) tools, security often takes a backseat to the primary goals of power, performance, and area optimization. Commonly, the security analysis is conducted by hand, leading to vulnerabilities in the design remaining unnoticed. Security-aware EDA tools assist the designer in the identification and removal of security threats while keeping performance and area in mind. Cutting-edge methods employ information flow analysis to identify inadvertent information leaks in design structures. Current information leakage detection methods use quantitative information flow analysis to quantify the leaks. However, handling sequential circuits poses challenges for state-of-the-art techniques due to their time-agnostic nature, overlooking timing channels, and introducing false positives. To address this, we introduce QTFlow, a timing-sensitive framework for quantifying hardware information leakages during the design phase. Illustrating its effectiveness on open-source benchmarks, QTFlow autonomously identifies timing channels and diminishes all false positives arising from time-agnostic analysis when contrasted with current state-of-the-art techniques.","sentences":["In contemporary Electronic Design Automation (EDA) tools, security often takes a backseat to the primary goals of power, performance, and area optimization.","Commonly, the security analysis is conducted by hand, leading to vulnerabilities in the design remaining unnoticed.","Security-aware EDA tools assist the designer in the identification and removal of security threats while keeping performance and area in mind.","Cutting-edge methods employ information flow analysis to identify inadvertent information leaks in design structures.","Current information leakage detection methods use quantitative information flow analysis to quantify the leaks.","However, handling sequential circuits poses challenges for state-of-the-art techniques due to their time-agnostic nature, overlooking timing channels, and introducing false positives.","To address this, we introduce QTFlow, a timing-sensitive framework for quantifying hardware information leakages during the design phase.","Illustrating its effectiveness on open-source benchmarks, QTFlow autonomously identifies timing channels and diminishes all false positives arising from time-agnostic analysis when contrasted with current state-of-the-art techniques."],"url":"http://arxiv.org/abs/2401.17819v1","category":"cs.CR"}
{"created":"2024-01-31 13:13:37","title":"Detection of Critical Events in Renewable Energy Production Time Series","abstract":"The introduction of more renewable energy sources into the energy system increases the variability and weather dependence of electricity generation. Power system simulations are used to assess the adequacy and reliability of the electricity grid over decades, but often become computational intractable for such long simulation periods with high technical detail. To alleviate this computational burden, we investigate the use of outlier detection algorithms to find periods of extreme renewable energy generation which enables detailed modelling of the performance of power systems under these circumstances. Specifically, we apply the Maximum Divergent Intervals (MDI) algorithm to power generation time series that have been derived from ERA5 historical climate reanalysis covering the period from 1950 through 2019. By applying the MDI algorithm on these time series, we identified intervals of extreme low and high energy production. To determine the outlierness of an interval different divergence measures can be used. Where the cross-entropy measure results in shorter and strongly peaking outliers, the unbiased Kullback-Leibler divergence tends to detect longer and more persistent intervals. These intervals are regarded as potential risks for the electricity grid by domain experts, showcasing the capability of the MDI algorithm to detect critical events in these time series. For the historical period analysed, we found no trend in outlier intensity, or shift and lengthening of the outliers that could be attributed to climate change. By applying MDI on climate model output, power system modellers can investigate the adequacy and possible changes of risk for the current and future electricity grid under a wider range of scenarios.","sentences":["The introduction of more renewable energy sources into the energy system increases the variability and weather dependence of electricity generation.","Power system simulations are used to assess the adequacy and reliability of the electricity grid over decades, but often become computational intractable for such long simulation periods with high technical detail.","To alleviate this computational burden, we investigate the use of outlier detection algorithms to find periods of extreme renewable energy generation which enables detailed modelling of the performance of power systems under these circumstances.","Specifically, we apply the Maximum Divergent Intervals (MDI) algorithm to power generation time series that have been derived from ERA5 historical climate reanalysis covering the period from 1950 through 2019.","By applying the MDI algorithm on these time series, we identified intervals of extreme low and high energy production.","To determine the outlierness of an interval different divergence measures can be used.","Where the cross-entropy measure results in shorter and strongly peaking outliers, the unbiased Kullback-Leibler divergence tends to detect longer and more persistent intervals.","These intervals are regarded as potential risks for the electricity grid by domain experts, showcasing the capability of the MDI algorithm to detect critical events in these time series.","For the historical period analysed, we found no trend in outlier intensity, or shift and lengthening of the outliers that could be attributed to climate change.","By applying MDI on climate model output, power system modellers can investigate the adequacy and possible changes of risk for the current and future electricity grid under a wider range of scenarios."],"url":"http://arxiv.org/abs/2401.17814v1","category":"physics.soc-ph"}
{"created":"2024-01-31 13:12:42","title":"Deterministic Computing Power Networking: Architecture, Technologies and Prospects","abstract":"With the development of new Internet services such as computation-intensive and delay-sensitive tasks, the traditional \"Best Effort\" network transmission mode has been greatly challenged. The network system is urgently required to provide end-to-end transmission determinacy and computing determinacy for new applications to ensure the safe and efficient operation of services. Based on the research of the convergence of computing and networking, a new network paradigm named deterministic computing power networking (Det-CPN) is proposed. In this article, we firstly introduce the research advance of computing power networking. And then the motivations and scenarios of Det-CPN are analyzed. Following that, we present the system architecture, technological capabilities, workflow as well as key technologies for Det-CPN. Finally, the challenges and future trends of Det-CPN are analyzed and discussed.","sentences":["With the development of new Internet services such as computation-intensive and delay-sensitive tasks, the traditional \"Best Effort\" network transmission mode has been greatly challenged.","The network system is urgently required to provide end-to-end transmission determinacy and computing determinacy for new applications to ensure the safe and efficient operation of services.","Based on the research of the convergence of computing and networking, a new network paradigm named deterministic computing power networking (Det-CPN) is proposed.","In this article, we firstly introduce the research advance of computing power networking.","And then the motivations and scenarios of Det-CPN are analyzed.","Following that, we present the system architecture, technological capabilities, workflow as well as key technologies for Det-CPN.","Finally, the challenges and future trends of Det-CPN are analyzed and discussed."],"url":"http://arxiv.org/abs/2401.17812v1","category":"cs.NI"}
{"created":"2024-01-31 13:08:45","title":"SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering","abstract":"Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS method for editing factual knowledge in LLMs. We demonstrate the state-of-the-art performance of SWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoning ability of SWEAOS in editing knowledge, we evaluate it on the more complex RIPPLEEDITS benchmark. The results on two subdatasets demonstrate that our SWEAOS possesses state-of-the-art reasoning ability.","sentences":["Model editing has recently gained widespread attention.","Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model.","However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable.","To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage.","SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead.","We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding.","We thus propose SWEAOS method for editing factual knowledge in LLMs.","We demonstrate the state-of-the-art performance of SWEAOS on the COUNTERFACT and zsRE datasets.","To further validate the reasoning ability of SWEAOS in editing knowledge, we evaluate it on the more complex RIPPLEEDITS benchmark.","The results on two subdatasets demonstrate that our SWEAOS possesses state-of-the-art reasoning ability."],"url":"http://arxiv.org/abs/2401.17809v1","category":"cs.CL"}
{"created":"2024-01-31 13:06:48","title":"Advances in 3D Generation: A Survey","abstract":"Generating 3D models lies at the core of computer graphics and has been the focus of decades of research. With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models. The rapid growth of this field makes it difficult to stay abreast of all recent developments. In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications. Specifically, we introduce the 3D representations that serve as the backbone for 3D generation. Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis. Lastly, we discuss available datasets, applications, and open challenges. We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation.","sentences":["Generating 3D models lies at the core of computer graphics and has been the focus of decades of research.","With the emergence of advanced neural representations and generative models, the field of 3D content generation is developing rapidly, enabling the creation of increasingly high-quality and diverse 3D models.","The rapid growth of this field makes it difficult to stay abreast of all recent developments.","In this survey, we aim to introduce the fundamental methodologies of 3D generation methods and establish a structured roadmap, encompassing 3D representation, generation methods, datasets, and corresponding applications.","Specifically, we introduce the 3D representations that serve as the backbone for 3D generation.","Furthermore, we provide a comprehensive overview of the rapidly growing literature on generation methods, categorized by the type of algorithmic paradigms, including feedforward generation, optimization-based generation, procedural generation, and generative novel view synthesis.","Lastly, we discuss available datasets, applications, and open challenges.","We hope this survey will help readers explore this exciting topic and foster further advancements in the field of 3D content generation."],"url":"http://arxiv.org/abs/2401.17807v1","category":"cs.CV"}
{"created":"2024-01-31 13:04:34","title":"Biospheric AI","abstract":"The dominant paradigm in AI ethics and value alignment is highly anthropocentric. The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights. Recently, attempts to expand to a sentientist perspective have been initiated. We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it. Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss hypothetical ways in which such an AI might be designed. Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests. All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere.","sentences":["The dominant paradigm in AI ethics and value alignment is highly anthropocentric.","The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights.","Recently, attempts to expand to a sentientist perspective have been initiated.","We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it.","Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective.","We discuss hypothetical ways in which such an AI might be designed.","Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests.","All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere."],"url":"http://arxiv.org/abs/2401.17805v1","category":"cs.CY"}
{"created":"2024-01-31 12:59:38","title":"An Efficient PGD Solver for Structural Dynamics Applications","abstract":"We propose in this paper a Proper Generalized Decomposition (PGD) solver for reduced-order modeling of linear elastodynamic problems. It primarily focuses on enhancing the computational efficiency of a previously introduced PGD solver based on the Hamiltonian formalism. The novelty of this work lies in the implementation of a solver that is halfway between Modal Decomposition and the conventional PGD framework, so as to accelerate the fixed-point iteration algorithm. Additional procedures such that Aitken's delta-squared process and mode-orthogonalization are incorporated to ensure convergence and stability of the algorithm. Numerical results regarding the ROM accuracy, time complexity, and scalability are provided to demonstrate the performance of the new solver when applied to dynamic simulation of a three-dimensional structure.","sentences":["We propose in this paper a Proper Generalized Decomposition (PGD) solver for reduced-order modeling of linear elastodynamic problems.","It primarily focuses on enhancing the computational efficiency of a previously introduced PGD solver based on the Hamiltonian formalism.","The novelty of this work lies in the implementation of a solver that is halfway between Modal Decomposition and the conventional PGD framework, so as to accelerate the fixed-point iteration algorithm.","Additional procedures such that Aitken's delta-squared process and mode-orthogonalization are incorporated to ensure convergence and stability of the algorithm.","Numerical results regarding the ROM accuracy, time complexity, and scalability are provided to demonstrate the performance of the new solver when applied to dynamic simulation of a three-dimensional structure."],"url":"http://arxiv.org/abs/2401.17804v1","category":"cs.CE"}
{"created":"2024-01-31 12:53:11","title":"SimAda: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes","abstract":"Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data. Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks. In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks. We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes. Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework. SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models. We conduct extensive experiments on nine datasets of six downstream tasks. The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs. Code is available at: https://github.com/zongzi13545329/SimAda","sentences":["Segment anything model (SAM) has demonstrated excellent generalization capabilities in common vision scenarios, yet lacking an understanding of specialized data.","Although numerous works have focused on optimizing SAM for downstream tasks, these task-specific approaches usually limit the generalizability to other downstream tasks.","In this paper, we aim to investigate the impact of the general vision modules on finetuning SAM and enable them to generalize across all downstream tasks.","We propose a simple unified framework called SimAda for adapting SAM in underperformed scenes.","Specifically, our framework abstracts the general modules of different methods into basic design elements, and we design four variants based on a shared theoretical framework.","SimAda is simple yet effective, which removes all dataset-specific designs and focuses solely on general optimization, ensuring that SimAda can be applied to all SAM-based and even Transformer-based models.","We conduct extensive experiments on nine datasets of six downstream tasks.","The results demonstrate that SimAda significantly improves the performance of SAM on multiple downstream tasks and achieves state-of-the-art performance on most of them, without requiring task-specific designs.","Code is available at: https://github.com/zongzi13545329/SimAda"],"url":"http://arxiv.org/abs/2401.17803v1","category":"cs.CV"}
{"created":"2024-01-31 12:52:10","title":"Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning","abstract":"Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two tasks. By developing model loss from multiple tasks, we can learn effective representations for downstream forecasting task. Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%.","sentences":["Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal.","However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data.","To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting.","Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences.","Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series.","Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process.","Finally, we jointly optimize the above two tasks.","By developing model loss from multiple tasks, we can learn effective representations for downstream forecasting task.","Extensive experiments, in comparison with state-of-the-arts, well demonstrate the effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%."],"url":"http://arxiv.org/abs/2401.17802v1","category":"cs.LG"}
{"created":"2024-01-31 12:52:05","title":"Weighted-Hamming Metric for Parallel Channels","abstract":"Independent parallel q-ary symmetric channels are a suitable transmission model for several applications. The proposed weighted-Hamming metric is tailored to this setting and enables optimal decoding performance. We show that some weighted-Hamming-metric codes exhibit the unusual property that all errors beyond half the minimum distance can be corrected. Nevertheless, a tight relation between the error-correction capability of a code and its minimum distance can be established. Generalizing their Hamming-metric counterparts, upper and lower bounds on the cardinality of a code with a given weighted-Hamming distance are obtained. Finally, we propose a simple code construction with optimal minimum distance for specific parameters.","sentences":["Independent parallel q-ary symmetric channels are a suitable transmission model for several applications.","The proposed weighted-Hamming metric is tailored to this setting and enables optimal decoding performance.","We show that some weighted-Hamming-metric codes exhibit the unusual property that all errors beyond half the minimum distance can be corrected.","Nevertheless, a tight relation between the error-correction capability of a code and its minimum distance can be established.","Generalizing their Hamming-metric counterparts, upper and lower bounds on the cardinality of a code with a given weighted-Hamming distance are obtained.","Finally, we propose a simple code construction with optimal minimum distance for specific parameters."],"url":"http://arxiv.org/abs/2401.17801v1","category":"cs.IT"}
{"created":"2024-01-31 12:51:26","title":"Dance-to-Music Generation with Encoder-based Textual Inversion of Diffusion Models","abstract":"The harmonious integration of music with dance movements is pivotal in vividly conveying the artistic essence of dance. This alignment also significantly elevates the immersive quality of gaming experiences and animation productions. While there has been remarkable advancement in creating high-fidelity music from textual descriptions, current methodologies mainly concentrate on modulating overarching characteristics such as genre and emotional tone. They often overlook the nuanced management of temporal rhythm, which is indispensable in crafting music for dance, since it intricately aligns the musical beats with the dancers' movements. Recognizing this gap, we propose an encoder-based textual inversion technique for augmenting text-to-music models with visual control, facilitating personalized music generation. Specifically, we develop dual-path rhythm-genre inversion to effectively integrate the rhythm and genre of a dance motion sequence into the textual space of a text-to-music model. Contrary to the classical textual inversion method, which directly updates text embeddings to reconstruct a single target object, our approach utilizes separate rhythm and genre encoders to obtain text embeddings for two pseudo-words, adapting to the varying rhythms and genres. To achieve a more accurate evaluation, we propose improved evaluation metrics for rhythm alignment. We demonstrate that our approach outperforms state-of-the-art methods across multiple evaluation metrics. Furthermore, our method seamlessly adapts to in-the-wild data and effectively integrates with the inherent text-guided generation capability of the pre-trained model. Samples are available at \\url{https://youtu.be/D7XDwtH1YwE}.","sentences":["The harmonious integration of music with dance movements is pivotal in vividly conveying the artistic essence of dance.","This alignment also significantly elevates the immersive quality of gaming experiences and animation productions.","While there has been remarkable advancement in creating high-fidelity music from textual descriptions, current methodologies mainly concentrate on modulating overarching characteristics such as genre and emotional tone.","They often overlook the nuanced management of temporal rhythm, which is indispensable in crafting music for dance, since it intricately aligns the musical beats with the dancers' movements.","Recognizing this gap, we propose an encoder-based textual inversion technique for augmenting text-to-music models with visual control, facilitating personalized music generation.","Specifically, we develop dual-path rhythm-genre inversion to effectively integrate the rhythm and genre of a dance motion sequence into the textual space of a text-to-music model.","Contrary to the classical textual inversion method, which directly updates text embeddings to reconstruct a single target object, our approach utilizes separate rhythm and genre encoders to obtain text embeddings for two pseudo-words, adapting to the varying rhythms and genres.","To achieve a more accurate evaluation, we propose improved evaluation metrics for rhythm alignment.","We demonstrate that our approach outperforms state-of-the-art methods across multiple evaluation metrics.","Furthermore, our method seamlessly adapts to in-the-wild data and effectively integrates with the inherent text-guided generation capability of the pre-trained model.","Samples are available at \\url{https://youtu.be/D7XDwtH1YwE}."],"url":"http://arxiv.org/abs/2401.17800v1","category":"cs.SD"}
{"created":"2024-01-31 12:48:28","title":"AI-enabled Cyber-Physical In-Orbit Factory -- AI approaches based on digital twin technology for robotic small satellite production","abstract":"With the ever increasing number of active satellites in space, the rising demand for larger formations of small satellites and the commercialization of the space industry (so-called New Space), the realization of manufacturing processes in orbit comes closer to reality. Reducing launch costs and risks, allowing for faster on-demand deployment of individually configured satellites as well as the prospect for possible on-orbit servicing for satellites makes the idea of realizing an in-orbit factory promising. In this paper, we present a novel approach to an in-orbit factory of small satellites covering a digital process twin, AI-based fault detection, and teleoperated robot-control, which are being researched as part of the \"AI-enabled Cyber-Physical In-Orbit Factory\" project. In addition to the integration of modern automation and Industry 4.0 production approaches, the question of how artificial intelligence (AI) and learning approaches can be used to make the production process more robust, fault-tolerant and autonomous is addressed. This lays the foundation for a later realisation of satellite production in space in the form of an in-orbit factory. Central aspect is the development of a robotic AIT (Assembly, Integration and Testing) system where a small satellite could be assembled by a manipulator robot from modular subsystems. Approaches developed to improving this production process with AI include employing neural networks for optical and electrical fault detection of components. Force sensitive measuring and motion training helps to deal with uncertainties and tolerances during assembly. An AI-guided teleoperated control of the robot arm allows for human intervention while a Digital Process Twin represents process data and provides supervision during the whole production process. Approaches and results towards automated satellite production are presented in detail.","sentences":["With the ever increasing number of active satellites in space, the rising demand for larger formations of small satellites and the commercialization of the space industry (so-called New Space), the realization of manufacturing processes in orbit comes closer to reality.","Reducing launch costs and risks, allowing for faster on-demand deployment of individually configured satellites as well as the prospect for possible on-orbit servicing for satellites makes the idea of realizing an in-orbit factory promising.","In this paper, we present a novel approach to an in-orbit factory of small satellites covering a digital process twin, AI-based fault detection, and teleoperated robot-control, which are being researched as part of the \"AI-enabled Cyber-Physical In-Orbit Factory\" project.","In addition to the integration of modern automation and Industry 4.0 production approaches, the question of how artificial intelligence (AI) and learning approaches can be used to make the production process more robust, fault-tolerant and autonomous is addressed.","This lays the foundation for a later realisation of satellite production in space in the form of an in-orbit factory.","Central aspect is the development of a robotic AIT (Assembly, Integration and Testing) system where a small satellite could be assembled by a manipulator robot from modular subsystems.","Approaches developed to improving this production process with AI include employing neural networks for optical and electrical fault detection of components.","Force sensitive measuring and motion training helps to deal with uncertainties and tolerances during assembly.","An AI-guided teleoperated control of the robot arm allows for human intervention while a Digital Process Twin represents process data and provides supervision during the whole production process.","Approaches and results towards automated satellite production are presented in detail."],"url":"http://arxiv.org/abs/2401.17799v1","category":"cs.RO"}
{"created":"2024-01-31 12:45:44","title":"M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval","abstract":"We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP. Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain. Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training. Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement. We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features. We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training. Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones. We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP.","sentences":["We present a Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards effective and efficient zero-shot video-text retrieval, dubbed M2-RAAP.","Upon popular image-text models like CLIP, most current adaptation-based video-text pre-training methods are confronted by three major issues, i.e., noisy data corpus, time-consuming pre-training, and limited performance gain.","Towards this end, we conduct a comprehensive study including four critical steps in video-text pre-training.","Specifically, we investigate 1) data filtering and refinement, 2) video input type selection, 3) temporal modeling, and 4) video feature enhancement.","We then summarize this empirical study into the M2-RAAP recipe, where our technical contributions lie in 1) the data filtering and text re-writing pipeline resulting in 1M high-quality bilingual video-text pairs, 2) the replacement of video inputs with key-frames to accelerate pre-training, and 3) the Auxiliary-Caption-Guided (ACG) strategy to enhance video features.","We conduct extensive experiments by adapting three image-text foundation models on two refined video-text datasets from different languages, validating the robustness and reproducibility of M2-RAAP for adaptation-based pre-training.","Results demonstrate that M2-RAAP yields superior performance with significantly reduced data (-90%) and time consumption (-95%), establishing a new SOTA on four English zero-shot retrieval datasets and two Chinese ones.","We are preparing our refined bilingual data annotations and codebase, which will be available at https://github.com/alipay/Ant-Multi-Modal-Framework/tree/main/prj/M2_RAAP."],"url":"http://arxiv.org/abs/2401.17797v1","category":"cs.CV"}
{"created":"2024-01-31 12:45:43","title":"Exploiting Audio-Visual Features with Pretrained AV-HuBERT for Multi-Modal Dysarthric Speech Reconstruction","abstract":"Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech by improving the intelligibility and naturalness. This is a challenging task especially for patients with severe dysarthria and speaking in complex, noisy acoustic environments. To address these challenges, we propose a novel multi-modal framework to utilize visual information, e.g., lip movements, in DSR as extra clues for reconstructing the highly abnormal pronunciations. The multi-modal framework consists of: (i) a multi-modal encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual features; (ii) a variance adaptor to infer the normal phoneme duration and pitch contour from the extracted phoneme embeddings; (iii) a speaker encoder to encode the speaker's voice characteristics; and (iv) a mel-decoder to generate the reconstructed mel-spectrogram based on the extracted phoneme embeddings, prosodic features and speaker embeddings. Both objective and subjective evaluations conducted on the commonly used UASpeech corpus show that our proposed approach can achieve significant improvements over baseline systems in terms of speech intelligibility and naturalness, especially for the speakers with more severe symptoms. Compared with original dysarthric speech, the reconstructed speech achieves 42.1\\% absolute word error rate reduction for patients with more severe dysarthria levels.","sentences":["Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech by improving the intelligibility and naturalness.","This is a challenging task especially for patients with severe dysarthria and speaking in complex, noisy acoustic environments.","To address these challenges, we propose a novel multi-modal framework to utilize visual information, e.g., lip movements, in DSR as extra clues for reconstructing the highly abnormal pronunciations.","The multi-modal framework consists of: (i) a multi-modal encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual features; (ii) a variance adaptor to infer the normal phoneme duration and pitch contour from the extracted phoneme embeddings; (iii) a speaker encoder to encode the speaker's voice characteristics; and (iv) a mel-decoder to generate the reconstructed mel-spectrogram based on the extracted phoneme embeddings, prosodic features and speaker embeddings.","Both objective and subjective evaluations conducted on the commonly used UASpeech corpus show that our proposed approach can achieve significant improvements over baseline systems in terms of speech intelligibility and naturalness, especially for the speakers with more severe symptoms.","Compared with original dysarthric speech, the reconstructed speech achieves 42.1\\% absolute word error rate reduction for patients with more severe dysarthria levels."],"url":"http://arxiv.org/abs/2401.17796v1","category":"cs.SD"}
{"created":"2024-01-31 12:43:38","title":"Quantum error correction realized by the renormalization group in scalar field theories","abstract":"We demonstrate that quantum error correction is realized by the renormalization group in scalar field theories. We construct $q$-level states by using coherent states in the IR region. By acting on them the inverse of the unitary operator $U$ that describes the renormalization group flow of the ground state, we encode them into states in the UV region. We find that the Knill-Laflamme condition is satisfied for operators that create coherent states. We verify this to the first order in the perturbation theory. This result suggests a general relationship between the renormalization group and quantum error correction and should give insights into understanding a role played by them in the gauge/gravity correspondence.","sentences":["We demonstrate that quantum error correction is realized by the renormalization group in scalar field theories.","We construct $q$-level states by using coherent states in the IR region.","By acting on them the inverse of the unitary operator $U$ that describes the renormalization group flow of the ground state, we encode them into states in the UV region.","We find that the Knill-Laflamme condition is satisfied for operators that create coherent states.","We verify this to the first order in the perturbation theory.","This result suggests a general relationship between the renormalization group and quantum error correction and should give insights into understanding a role played by them in the gauge/gravity correspondence."],"url":"http://arxiv.org/abs/2401.17795v1","category":"hep-th"}
{"created":"2024-01-31 12:33:31","title":"Graph Transformers without Positional Encodings","abstract":"Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show that it achieves performance comparable to SOTA MP-GNN architectures and Graph Transformers on a number of standard GNN benchmark datasets, even surpassing the SOTA on some datasets. We also find that our architecture is much faster to train in terms of number of epochs, presumably due to the innate graph inductive biases.","sentences":["Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs).","Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results.","However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc.","In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure.","We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show that it achieves performance comparable to SOTA MP-GNN architectures and Graph Transformers on a number of standard GNN benchmark datasets, even surpassing the SOTA on some datasets.","We also find that our architecture is much faster to train in terms of number of epochs, presumably due to the innate graph inductive biases."],"url":"http://arxiv.org/abs/2401.17791v1","category":"cs.LG"}
{"created":"2024-01-31 12:32:18","title":"RADIN: Souping on a Budget","abstract":"Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters. Yet, their adoption is hindered by computational challenges due to subset selection issues. In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances. Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet).","sentences":["Model Soups, extending Stochastic Weights Averaging (SWA), combine models fine-tuned with different hyperparameters.","Yet, their adoption is hindered by computational challenges due to subset selection issues.","In this paper, we propose to speed up model soups by approximating soups performance using averaged ensemble logits performances.","Theoretical insights validate the congruence between ensemble logits and weight averaging soups across any mixing ratios.","Our Resource ADjusted soups craftINg (RADIN) procedure stands out by allowing flexible evaluation budgets, enabling users to adjust his budget of exploration adapted to his resources while increasing performance at lower budget compared to previous greedy approach (up to 4% on ImageNet)."],"url":"http://arxiv.org/abs/2401.17790v1","category":"cs.LG"}
{"created":"2024-01-31 12:32:17","title":"Robustly overfitting latents for flexible neural image compression","abstract":"Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two-class rounding. Finally, we show how refinement of the latents with our best-performing method improves the compression performance on the Tecnick dataset and how it can be deployed to partly move along the rate-distortion curve.","sentences":["Neural image compression has made a great deal of progress.","State-of-the-art models are based on variational autoencoders and are outperforming classical models.","Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image.","While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity.","Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models.","We extend this idea by introducing SGA+, which contains three different methods that build upon SGA.","Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices.","Besides, we show how each method can be extended to three- instead of two-class rounding.","Finally, we show how refinement of the latents with our best-performing method improves the compression performance on the Tecnick dataset and how it can be deployed to partly move along the rate-distortion curve."],"url":"http://arxiv.org/abs/2401.17789v1","category":"cs.CV"}
{"created":"2024-01-31 12:31:42","title":"A Graph-Native Query Optimization Framework","abstract":"Graph queries that combine pattern matching with relational operations, referred as PatRelQuery, are widely used in many real-world applications. It allows users to identify arbitrary patterns in a graph and further perform in-depth relational analysis on the results. To effectively support PatRelQuery, two key challenges need to be addressed: (1) how to optimize PatRelQuery in a unified framework, and (2) how to handle the arbitrary type constraints in patterns in PatRelQuery. In this paper, we present a graph-native query optimization framework named GOpt, to tackle these issues. GOpt is built on top of a unified intermediate representation (IR) that is capable of capturing both graph and relational operations, thereby streamlining the optimization of PatRelQuery. To handle the arbitrary type constraints, GOpt employs an automatic type inference approach to identify implicit type constraints. Additionally, GOpt introduces a graph-native optimizer, which encompasses an extensive collection of optimization rules along with cost-based techniques tailored for arbitrary patterns, to optimize PatRelQuery. Through comprehensive experiments, we demonstrate that GOpt can achieve significant query performance improvements, in both crafted benchmarks and real-world applications.","sentences":["Graph queries that combine pattern matching with relational operations, referred as PatRelQuery, are widely used in many real-world applications.","It allows users to identify arbitrary patterns in a graph and further perform in-depth relational analysis on the results.","To effectively support PatRelQuery, two key challenges need to be addressed: (1) how to optimize PatRelQuery in a unified framework, and (2) how to handle the arbitrary type constraints in patterns in PatRelQuery.","In this paper, we present a graph-native query optimization framework named GOpt, to tackle these issues.","GOpt is built on top of a unified intermediate representation (IR) that is capable of capturing both graph and relational operations, thereby streamlining the optimization of PatRelQuery.","To handle the arbitrary type constraints, GOpt employs an automatic type inference approach to identify implicit type constraints.","Additionally, GOpt introduces a graph-native optimizer, which encompasses an extensive collection of optimization rules along with cost-based techniques tailored for arbitrary patterns, to optimize PatRelQuery.","Through comprehensive experiments, we demonstrate that GOpt can achieve significant query performance improvements, in both crafted benchmarks and real-world applications."],"url":"http://arxiv.org/abs/2401.17786v1","category":"cs.DB"}
{"created":"2024-01-31 12:26:59","title":"SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms","abstract":"SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm. The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed. All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats.","sentences":["SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm.","The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed.","All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats."],"url":"http://arxiv.org/abs/2401.17783v1","category":"cs.AI"}
{"created":"2024-01-31 12:23:55","title":"Vision-Assisted Digital Twin Creation for mmWave Beam Management","abstract":"In the context of communication networks, digital twin technology provides a means to replicate the radio frequency (RF) propagation environment as well as the system behaviour, allowing for a way to optimize the performance of a deployed system based on simulations. One of the key challenges in the application of Digital Twin technology to mmWave systems is the prevalent channel simulators' stringent requirements on the accuracy of the 3D Digital Twin, reducing the feasibility of the technology in real applications. We propose a practical Digital Twin creation pipeline and a channel simulator, that relies only on a single mounted camera and position information. We demonstrate the performance benefits compared to methods that do not explicitly model the 3D environment, on downstream sub-tasks in beam acquisition, using the real-world dataset of the DeepSense6G challenge","sentences":["In the context of communication networks, digital twin technology provides a means to replicate the radio frequency (RF) propagation environment as well as the system behaviour, allowing for a way to optimize the performance of a deployed system based on simulations.","One of the key challenges in the application of Digital Twin technology to mmWave systems is the prevalent channel simulators' stringent requirements on the accuracy of the 3D Digital Twin, reducing the feasibility of the technology in real applications.","We propose a practical Digital Twin creation pipeline and a channel simulator, that relies only on a single mounted camera and position information.","We demonstrate the performance benefits compared to methods that do not explicitly model the 3D environment, on downstream sub-tasks in beam acquisition, using the real-world dataset of the DeepSense6G challenge"],"url":"http://arxiv.org/abs/2401.17781v1","category":"eess.SP"}
