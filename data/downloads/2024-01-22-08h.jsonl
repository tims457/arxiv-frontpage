{"created":"2024-01-19 18:59:52","title":"Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data","abstract":"This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.","sentences":["This work presents Depth Anything, a highly practical solution for robust monocular depth estimation.","Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances.","To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error.","We investigate two simple yet effective strategies that make data scaling-up promising.","First, a more challenging optimization target is created by leveraging data augmentation tools.","It compels the model to actively seek extra visual knowledge and acquire robust representations.","Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders.","We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos.","It demonstrates impressive generalization ability.","Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set.","Our better depth model also results in a better depth-conditioned ControlNet.","Our models are released at https://github.com/LiheYoung/Depth-Anything."],"url":"http://arxiv.org/abs/2401.10891v1","category":"cs.CV"}
{"created":"2024-01-19 18:59:37","title":"Event detection from novel data sources: Leveraging satellite imagery alongside GPS traces","abstract":"Rapid identification and response to breaking events, particularly those that pose a threat to human life such as natural disasters or conflicts, is of paramount importance. The prevalence of mobile devices and the ubiquity of network connectivity has generated a massive amount of temporally- and spatially-stamped data. Numerous studies have used mobile data to derive individual human mobility patterns for various applications. Similarly, the increasing number of orbital satellites has made it easier to gather high-resolution images capturing a snapshot of a geographical area in sub-daily temporal frequency. We propose a novel data fusion methodology integrating satellite imagery with privacy-enhanced mobile data to augment the event inference task, whether in real-time or historical. In the absence of boots on the ground, mobile data is able to give an approximation of human mobility, proximity to one another, and the built environment. On the other hand, satellite imagery can provide visual information on physical changes to the built and natural environment. The expected use cases for our methodology include small-scale disaster detection (i.e., tornadoes, wildfires, and floods) in rural regions, search and rescue operation augmentation for lost hikers in remote wilderness areas, and identification of active conflict areas and population displacement in war-torn states. Our implementation is open-source on GitHub: https://github.com/ekinugurel/SatMobFusion.","sentences":["Rapid identification and response to breaking events, particularly those that pose a threat to human life such as natural disasters or conflicts, is of paramount importance.","The prevalence of mobile devices and the ubiquity of network connectivity has generated a massive amount of temporally- and spatially-stamped data.","Numerous studies have used mobile data to derive individual human mobility patterns for various applications.","Similarly, the increasing number of orbital satellites has made it easier to gather high-resolution images capturing a snapshot of a geographical area in sub-daily temporal frequency.","We propose a novel data fusion methodology integrating satellite imagery with privacy-enhanced mobile data to augment the event inference task, whether in real-time or historical.","In the absence of boots on the ground, mobile data is able to give an approximation of human mobility, proximity to one another, and the built environment.","On the other hand, satellite imagery can provide visual information on physical changes to the built and natural environment.","The expected use cases for our methodology include small-scale disaster detection (i.e., tornadoes, wildfires, and floods) in rural regions, search and rescue operation augmentation for lost hikers in remote wilderness areas, and identification of active conflict areas and population displacement in war-torn states.","Our implementation is open-source on GitHub: https://github.com/ekinugurel/SatMobFusion."],"url":"http://arxiv.org/abs/2401.10890v1","category":"cs.CV"}
{"created":"2024-01-19 18:59:11","title":"Synthesizing Moving People with 3D Control","abstract":"In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/.","sentences":["In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence.","Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture.","For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image.","We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint.","Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses.","This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions.","This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity.","In addition to that, the 3D control allows various synthetic camera trajectories to render a person.","Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods.","Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/."],"url":"http://arxiv.org/abs/2401.10889v1","category":"cs.CV"}
{"created":"2024-01-19 18:57:46","title":"SCENES: Subpixel Correspondence Estimation With Epipolar Supervision","abstract":"Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.","sentences":["Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion.","Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets.","However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors.","Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available.","We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry.","We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line.","While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data.","We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach.","We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision."],"url":"http://arxiv.org/abs/2401.10886v1","category":"cs.CV"}
{"created":"2024-01-19 18:56:35","title":"Thermodynamic limit for the magnetic uniform electron gas and representability of density-current pairs","abstract":"Although the concept of the uniform electron gas is essential to quantum physics, it has only been defined recently in a rigorous manner by Lewin, Lieb and Seiringer. We extend their approach to include the magnetic case, by which we mean that the vorticity of the gas is also held constant. Our definition involves the grand-canonical version of the universal functional introduced by Vignale and Rasolt in the context of current-density-functional theory. Besides establishing the existence of the thermodynamic limit, we derive an estimate on the kinetic energy functional that also gives a convenient answer to the (mixed) current-density representability problem.","sentences":["Although the concept of the uniform electron gas is essential to quantum physics, it has only been defined recently in a rigorous manner by Lewin, Lieb and Seiringer.","We extend their approach to include the magnetic case, by which we mean that the vorticity of the gas is also held constant.","Our definition involves the grand-canonical version of the universal functional introduced by Vignale and Rasolt in the context of current-density-functional theory.","Besides establishing the existence of the thermodynamic limit, we derive an estimate on the kinetic energy functional that also gives a convenient answer to the (mixed) current-density representability problem."],"url":"http://arxiv.org/abs/2401.10885v1","category":"math-ph"}
{"created":"2024-01-19 18:54:10","title":"RetinaVR: Democratizing Vitreoretinal Surgery Training with a Portable and Affordable Virtual Reality Simulator in the Metaverse","abstract":"We developed and validated RetinaVR, an affordable and immersive virtual reality simulator for vitreoretinal surgery training, using the Meta Quest 2 VR headset. We focused on four core fundamental skills: core vitrectomy, peripheral shaving, membrane peeling, and endolaser application. The validation study involved 10 novice ophthalmology residents and 10 expert vitreoretinal surgeons. We demonstrated construct validity, as shown by the varying user performance in a way that correlates with experimental runs, age, sex, and expertise. RetinaVR shows promise as a portable and affordable simulator, with potential to democratize surgical simulation access, especially in developing countries.","sentences":["We developed and validated RetinaVR, an affordable and immersive virtual reality simulator for vitreoretinal surgery training, using the Meta Quest 2 VR headset.","We focused on four core fundamental skills: core vitrectomy, peripheral shaving, membrane peeling, and endolaser application.","The validation study involved 10 novice ophthalmology residents and 10 expert vitreoretinal surgeons.","We demonstrated construct validity, as shown by the varying user performance in a way that correlates with experimental runs, age, sex, and expertise.","RetinaVR shows promise as a portable and affordable simulator, with potential to democratize surgical simulation access, especially in developing countries."],"url":"http://arxiv.org/abs/2401.10883v1","category":"cs.HC"}
{"created":"2024-01-19 18:49:36","title":"Reinforcement learning for question answering in programming domain using public community scoring as a human feedback","abstract":"In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accentuating the significance of context-aware evaluation, this study contributes to the ongoing efforts in refining Large Language Models through focused human feedback.","sentences":["In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow.","Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO).","Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant.","Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain.","Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods.","By elucidating the complexities involved in applying RLHF to programming CQA and accentuating the significance of context-aware evaluation, this study contributes to the ongoing efforts in refining Large Language Models through focused human feedback."],"url":"http://arxiv.org/abs/2401.10882v1","category":"cs.CL"}
{"created":"2024-01-19 18:49:03","title":"DynaVis: Dynamically Synthesized UI Widgets for Visualization Editing","abstract":"Users often rely on GUIs to edit and interact with visualizations - a daunting task due to the large space of editing options. As a result, users are either overwhelmed by a complex UI or constrained by a custom UI with a tailored, fixed subset of options with limited editing flexibility. Natural Language Interfaces (NLIs) are emerging as a feasible alternative for users to specify edits. However, NLIs forgo the advantages of traditional GUI: the ability to explore and repeat edits and see instant visual feedback.   We introduce DynaVis, which blends natural language and dynamically synthesized UI widgets. As the user describes an editing task in natural language, DynaVis performs the edit and synthesizes a persistent widget that the user can interact with to make further modifications. Study participants (n=24) preferred DynaVis over the NLI-only interface citing ease of further edits and editing confidence due to immediate visual feedback.","sentences":["Users often rely on GUIs to edit and interact with visualizations - a daunting task due to the large space of editing options.","As a result, users are either overwhelmed by a complex UI or constrained by a custom UI with a tailored, fixed subset of options with limited editing flexibility.","Natural Language Interfaces (NLIs) are emerging as a feasible alternative for users to specify edits.","However, NLIs forgo the advantages of traditional GUI: the ability to explore and repeat edits and see instant visual feedback.   ","We introduce DynaVis, which blends natural language and dynamically synthesized UI widgets.","As the user describes an editing task in natural language, DynaVis performs the edit and synthesizes a persistent widget that the user can interact with to make further modifications.","Study participants (n=24) preferred DynaVis over the NLI-only interface citing ease of further edits and editing confidence due to immediate visual feedback."],"url":"http://arxiv.org/abs/2401.10880v1","category":"cs.HC"}
{"created":"2024-01-19 18:45:14","title":"A novel model for direct numerical simulation of suspension dynamics with arbitrarily shaped convex particles","abstract":"This study presents an innovative direct numerical simulation approach for complex particle systems with irregular shapes and large numbers. Using partially saturated methods, it accurately models arbitrary shapes, albeit at considerable computational cost when integrating a compatible contact model. The introduction of a novel parallelization strategy significantly improves the performance of the contact model, enabling efficient four-way coupled simulations. Through hindered settling studies, the criticality of the explicit contact model for maintaining simulation accuracy is highlighted, especially at high particle volume fractions and low Archimedes numbers. The feasibility of simulating thousands of arbitrarily shaped convex particles is demonstrated with up to 1934 surface-resolved particles. The study also confirms the grid independence and linear convergence of the method. It shows for the first time that cube swarms settle 13 to 26% slower than swarms of volume-equivalent spheres across different Archimedes numbers (500 to 2000) and particle volume fractions (10 to 30%). These findings emphasize the shape dependence of particle systems and suggest avenues for exploring their nuanced dynamics.","sentences":["This study presents an innovative direct numerical simulation approach for complex particle systems with irregular shapes and large numbers.","Using partially saturated methods, it accurately models arbitrary shapes, albeit at considerable computational cost when integrating a compatible contact model.","The introduction of a novel parallelization strategy significantly improves the performance of the contact model, enabling efficient four-way coupled simulations.","Through hindered settling studies, the criticality of the explicit contact model for maintaining simulation accuracy is highlighted, especially at high particle volume fractions and low Archimedes numbers.","The feasibility of simulating thousands of arbitrarily shaped convex particles is demonstrated with up to 1934 surface-resolved particles.","The study also confirms the grid independence and linear convergence of the method.","It shows for the first time that cube swarms settle 13 to 26% slower than swarms of volume-equivalent spheres across different Archimedes numbers (500 to 2000) and particle volume fractions (10 to 30%).","These findings emphasize the shape dependence of particle systems and suggest avenues for exploring their nuanced dynamics."],"url":"http://arxiv.org/abs/2401.10878v1","category":"physics.flu-dyn"}
{"created":"2024-01-19 18:41:53","title":"The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology","abstract":"Motion capture systems, used across various domains, make body representations concrete through technical processes. We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices. By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time. Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements. We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction.","sentences":["Motion capture systems, used across various domains, make body representations concrete through technical processes.","We argue that the measurement of bodies and the validation of measurements for motion capture systems can be understood as social practices.","By analyzing the findings of a systematic literature review (N=278) through the lens of social practice theory, we show how these practices, and their varying attention to errors, become ingrained in motion capture design and innovation over time.","Moreover, we show how contemporary motion capture systems perpetuate assumptions about human bodies and their movements.","We suggest that social practices of measurement and validation are ubiquitous in the development of data- and sensor-driven systems more broadly, and provide this work as a basis for investigating hidden design assumptions and their potential negative consequences in human-computer interaction."],"url":"http://arxiv.org/abs/2401.10877v1","category":"cs.CY"}
{"created":"2024-01-19 18:40:30","title":"Spectral signatures of non-trivial topology in a superconducting circuit","abstract":"Topology, like symmetry, is a fundamental concept in understanding general properties of physical systems. In condensed matter systems, non-trivial topology may manifest itself as singular features in the energy spectrum or the quantization of observable quantities such as electrical conductance and magnetic flux. Using microwave spectroscopy, we show that a superconducting circuit with three Josephson tunnel junctions in parallel can possess energy degeneracies indicative of $\\textrm{\\emph{intrinsic}}$ non-trivial topology. We identify three topological invariants, one of which is related to a hidden quantum mechanical supersymmetry. Depending on fabrication parameters, devices are gapless or not, and fall on a simple phase diagram which is shown to be robust to perturbations including junction imperfections, asymmetry, and inductance. Josephson tunnel junction circuits, which are readily fabricated with conventional microlithography techniques, allow access to a wide range of topological systems which have no condensed matter analog. Notable spectral features of these circuits, such as degeneracies and flat bands, may be leveraged for quantum information applications, whereas quantized transport properties could be useful for metrology applications.","sentences":["Topology, like symmetry, is a fundamental concept in understanding general properties of physical systems.","In condensed matter systems, non-trivial topology may manifest itself as singular features in the energy spectrum or the quantization of observable quantities such as electrical conductance and magnetic flux.","Using microwave spectroscopy, we show that a superconducting circuit with three Josephson tunnel junctions in parallel can possess energy degeneracies indicative of $\\textrm{\\emph{intrinsic}}$ non-trivial topology.","We identify three topological invariants, one of which is related to a hidden quantum mechanical supersymmetry.","Depending on fabrication parameters, devices are gapless or not, and fall on a simple phase diagram which is shown to be robust to perturbations including junction imperfections, asymmetry, and inductance.","Josephson tunnel junction circuits, which are readily fabricated with conventional microlithography techniques, allow access to a wide range of topological systems which have no condensed matter analog.","Notable spectral features of these circuits, such as degeneracies and flat bands, may be leveraged for quantum information applications, whereas quantized transport properties could be useful for metrology applications."],"url":"http://arxiv.org/abs/2401.10876v1","category":"cond-mat.mes-hall"}
{"created":"2024-01-19 18:33:52","title":"Applications of flow models to the generation of correlated lattice QCD ensembles","abstract":"Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters. This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables. Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach. In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting.","sentences":["Machine-learned normalizing flows can be used in the context of lattice quantum field theory to generate statistically correlated ensembles of lattice gauge fields at different action parameters.","This work demonstrates how these correlations can be exploited for variance reduction in the computation of observables.","Three different proof-of-concept applications are demonstrated using a novel residual flow architecture: continuum limits of gauge theories, the mass dependence of QCD observables, and hadronic matrix elements based on the Feynman-Hellmann approach.","In all three cases, it is shown that statistical uncertainties are significantly reduced when machine-learned flows are incorporated as compared with the same calculations performed with uncorrelated ensembles or direct reweighting."],"url":"http://arxiv.org/abs/2401.10874v1","category":"hep-lat"}
{"created":"2024-01-19 18:33:30","title":"An AI-Resilient Text Rendering Technique for Reading and Skimming Documents","abstract":"Readers find text difficult to consume for many reasons. Summarization can address some of these difficulties, but introduce others, such as omitting, misrepresenting, or hallucinating information, which can be hard for a reader to notice. One approach to addressing this problem is to instead modify how the original text is rendered to make important information more salient. We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM), a text rendering method with a novel means of identifying what to de-emphasize. Specifically, GP-TSM uses a recursive sentence compression method to identify successive levels of detail beyond the core meaning of a passage, which are de-emphasized by rendering words in successively lighter but still legible gray text. In a lab study (n=18), participants preferred GP-TSM over pre-existing word-level text rendering methods and were able to answer GRE reading comprehension questions more efficiently.","sentences":["Readers find text difficult to consume for many reasons.","Summarization can address some of these difficulties, but introduce others, such as omitting, misrepresenting, or hallucinating information, which can be hard for a reader to notice.","One approach to addressing this problem is to instead modify how the original text is rendered to make important information more salient.","We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM), a text rendering method with a novel means of identifying what to de-emphasize.","Specifically, GP-TSM uses a recursive sentence compression method to identify successive levels of detail beyond the core meaning of a passage, which are de-emphasized by rendering words in successively lighter but still legible gray text.","In a lab study (n=18), participants preferred GP-TSM over pre-existing word-level text rendering methods and were able to answer GRE reading comprehension questions more efficiently."],"url":"http://arxiv.org/abs/2401.10873v1","category":"cs.HC"}
{"created":"2024-01-19 18:28:18","title":"Shadows and photon rings of binary black holes","abstract":"In this paper we present the images of binary black holes using the Majumdar-Papapetrou multiblack hole solution, depending on the parameters of the problem: the mass of black holes, the distance between them, and the inclination of the observer. The images consists of a shadows and photon rings. We find that a photon ring structure appears between black holes. The trajectories of the photons are calculated.","sentences":["In this paper we present the images of binary black holes using the Majumdar-Papapetrou multiblack hole solution, depending on the parameters of the problem: the mass of black holes, the distance between them, and the inclination of the observer.","The images consists of a shadows and photon rings.","We find that a photon ring structure appears between black holes.","The trajectories of the photons are calculated."],"url":"http://arxiv.org/abs/2401.10871v1","category":"gr-qc"}
{"created":"2024-01-19 18:05:34","title":"Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning","abstract":"Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100% success rate. These insights underline the potential of pruning as a generalizable approach for improving LLM safety, reliability, and potentially other desired behaviors.","sentences":["Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content.","In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks.","Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety.","Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts.","Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibility to jailbreaking attacks, with some categories achieving nearly 70-100% success rate.","These insights underline the potential of pruning as a generalizable approach for improving LLM safety, reliability, and potentially other desired behaviors."],"url":"http://arxiv.org/abs/2401.10862v1","category":"cs.LG"}
{"created":"2024-01-19 18:03:21","title":"Ensembler: Combating model inversion attacks using model ensemble during collaborative inference","abstract":"Deep learning models have exhibited remarkable performance across various domains. Nevertheless, the burgeoning model sizes compel edge devices to offload a significant portion of the inference process to the cloud. While this practice offers numerous advantages, it also raises critical concerns regarding user data privacy. In scenarios where the cloud server's trustworthiness is in question, the need for a practical and adaptable method to safeguard data privacy becomes imperative. In this paper, we introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties. Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference. Our experiments demonstrate that when combined with even basic Gaussian noise, Ensembler can effectively shield images from reconstruction attacks, achieving recognition levels that fall below human performance in some strict settings, significantly outperforming baseline methods lacking the Ensembler framework.","sentences":["Deep learning models have exhibited remarkable performance across various domains.","Nevertheless, the burgeoning model sizes compel edge devices to offload a significant portion of the inference process to the cloud.","While this practice offers numerous advantages, it also raises critical concerns regarding user data privacy.","In scenarios where the cloud server's trustworthiness is in question, the need for a practical and adaptable method to safeguard data privacy becomes imperative.","In this paper, we introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties.","Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference.","Our experiments demonstrate that when combined with even basic Gaussian noise, Ensembler can effectively shield images from reconstruction attacks, achieving recognition levels that fall below human performance in some strict settings, significantly outperforming baseline methods lacking the Ensembler framework."],"url":"http://arxiv.org/abs/2401.10859v1","category":"cs.CR"}
{"created":"2024-01-19 18:00:52","title":"Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning","abstract":"Deep learning algorithms have driven expressive progress in many complex tasks. The loss function is a core component of deep learning techniques, guiding the learning process of neural networks. This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches. The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips. Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark.","sentences":["Deep learning algorithms have driven expressive progress in many complex tasks.","The loss function is a core component of deep learning techniques, guiding the learning process of neural networks.","This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches.","The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips.","Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark."],"url":"http://arxiv.org/abs/2401.10857v1","category":"cs.CV"}
{"created":"2024-01-19 17:57:35","title":"Cactus Representation of Minimum Cuts: Derandomize and Speed up","abstract":"Given an undirected weighted graph with $n$ vertices and $m$ edges, we give the first deterministic $m^{1+o(1)}$-time algorithm for constructing the cactus representation of \\emph{all} global minimum cuts. This improves the current $n^{2+o(1)}$-time state-of-the-art deterministic algorithm, which can be obtained by combining ideas implicitly from three papers [Karger JACM'2000, Li STOC'2021, and Gabow TALG'2016] The known explicitly stated deterministic algorithm has a runtime of $\\tilde{O}(mn)$ [Fleischer 1999, Nagamochi and Nakao 2000]. Using our technique, we can even speed up the fastest randomized algorithm of [Karger and Panigrahi, SODA'2009] whose running time is at least $\\Omega(m\\log^4 n)$ to $O(m\\log^3 n)$.","sentences":["Given an undirected weighted graph with $n$ vertices and $m$ edges, we give the first deterministic $m^{1+o(1)}$-time algorithm for constructing the cactus representation of \\emph{all} global minimum cuts.","This improves the current $n^{2+o(1)}$-time state-of-the-art deterministic algorithm, which can be obtained by combining ideas implicitly from three papers [Karger JACM'2000, Li STOC'2021, and Gabow TALG'2016] The known explicitly stated deterministic algorithm has a runtime of $\\tilde{O}(mn)$ [Fleischer 1999, Nagamochi and Nakao 2000].","Using our technique, we can even speed up the fastest randomized algorithm of [Karger and Panigrahi, SODA'2009] whose running time is at least $\\Omega(m\\log^4 n)$ to $O(m\\log^3 n)$."],"url":"http://arxiv.org/abs/2401.10856v1","category":"cs.DS"}
{"created":"2024-01-19 17:56:51","title":"Photodissociation spectra of single trapped CaOH+ molecular ions","abstract":"Molecular ions that are generated by chemical reactions with trapped atomic ions can serve as an accessible and successful testbed for developing molecular quantum technologies. On the other hand, they are also a hindrance to scaling up quantum computers based on atomic ions as unavoidable reactions with background gas destroy the information carriers. Here, we investigate the single-photon and two-photon dissociation processes of single CaOH$^+$ molecular ions co-trapped in Ca$^+$ ion crystals using a femtosecond laser system. We report the photodissociation cross section spectra of CaOH$^+$ for single-photon processes at $\\lambda=245 - 275$ nm and for two-photon processes at $\\lambda=500 - 540$ nm. This result can serve as a basis for dissociation-based spectroscopy for studying the internal structure of CaOH$^+$. The result also gives a prescription for recycling Ca$^+$ ions in large-scale trapped Ca$^+$ quantum experiments from undesired CaOH$^+$ ions formed in the presence of background water vapor.","sentences":["Molecular ions that are generated by chemical reactions with trapped atomic ions can serve as an accessible and successful testbed for developing molecular quantum technologies.","On the other hand, they are also a hindrance to scaling up quantum computers based on atomic ions as unavoidable reactions with background gas destroy the information carriers.","Here, we investigate the single-photon and two-photon dissociation processes of single CaOH$^+$ molecular ions co-trapped in Ca$^+$ ion crystals using a femtosecond laser system.","We report the photodissociation cross section spectra of CaOH$^+$ for single-photon processes at $\\lambda=245 - 275$ nm and for two-photon processes at $\\lambda=500 - 540$ nm.","This result can serve as a basis for dissociation-based spectroscopy for studying the internal structure of CaOH$^+$. The result also gives a prescription for recycling Ca$^+$ ions in large-scale trapped Ca$^+$ quantum experiments from undesired CaOH$^+$ ions formed in the presence of background water vapor."],"url":"http://arxiv.org/abs/2401.10854v1","category":"physics.atom-ph"}
{"created":"2024-01-19 17:54:52","title":"Software Resource Disaggregation for HPC with Serverless Computing","abstract":"Aggregated HPC resources have rigid allocation systems and programming models which struggle to adapt to diverse and changing workloads. Consequently, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources. Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation. However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses. In this paper, we improve the utilization of supercomputers by employing the new cloud paradigm of serverless computing. We show how serverless functions provide fine-grained access to the resources of batch-managed cluster nodes. We present an HPC-oriented Function-as-a-Service (FaaS) that satisfies the requirements of high-performance applications. We demonstrate a \\emph{software resource disaggregation} approach where placing functions on unallocated and underutilized nodes allows idle cores and accelerators to be utilized while retaining near-native performance.","sentences":["Aggregated HPC resources have rigid allocation systems and programming models which struggle to adapt to diverse and changing workloads.","Consequently, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources.","Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation.","However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses.","In this paper, we improve the utilization of supercomputers by employing the new cloud paradigm of serverless computing.","We show how serverless functions provide fine-grained access to the resources of batch-managed cluster nodes.","We present an HPC-oriented Function-as-a-Service (FaaS) that satisfies the requirements of high-performance applications.","We demonstrate a \\emph{software resource disaggregation} approach where placing functions on unallocated and underutilized nodes allows idle cores and accelerators to be utilized while retaining near-native performance."],"url":"http://arxiv.org/abs/2401.10852v1","category":"cs.DC"}
{"created":"2024-01-19 17:51:11","title":"Advancements in eHealth Data Analytics through Natural Language Processing and Deep Learning","abstract":"The healthcare environment is commonly referred to as \"information-rich\" but also \"knowledge poor\". Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc. These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases. Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations. In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words. A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a specific meaning to a domain. All the terms of a domain create its terminology. This chapter offers a critical study of the current, most performant solutions for analyzing unstructured (image and textual) eHealth data. This study also provides a comparison of the current Natural Language Processing and Deep Learning techniques in the eHealth context. Finally, we examine and discuss some of the current issues, and we define a set of research directions in this area.","sentences":["The healthcare environment is commonly referred to as \"information-rich\" but also \"knowledge poor\".","Healthcare systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or programs, medical prescriptions, etc.","These massive sets of data can provide great knowledge and information that can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the patient's symptoms or disease prevention, by facilitating the discovery of behavioral factors for diseases.","Unfortunately, only a relatively small volume of the textual eHealth data is processed and interpreted, an important factor being the difficulty in efficiently performing Big Data operations.","In the medical field, detecting domain-specific multi-word terms is a crucial task as they can define an entire concept with a few words.","A term can be defined as a linguistic structure or a concept, and it is composed of one or more words with a specific meaning to a domain.","All the terms of a domain create its terminology.","This chapter offers a critical study of the current, most performant solutions for analyzing unstructured (image and textual) eHealth data.","This study also provides a comparison of the current Natural Language Processing and Deep Learning techniques in the eHealth context.","Finally, we examine and discuss some of the current issues, and we define a set of research directions in this area."],"url":"http://arxiv.org/abs/2401.10850v1","category":"cs.CL"}
{"created":"2024-01-19 17:48:20","title":"Exploring the role of structure in a time constrained decision task","abstract":"The structure of the basal ganglia is remarkably similar across a number of species (often described in terms of direct, indirect and hyperdirect pathways) and is deeply involved in decision making and action selection. In this article, we are interested in exploring the role of structure when solving a decision task while avoiding to make any strong assumption regarding the actual structure. To do so, we exploit the echo state network paradigm that allows to solve complex task based on a random architecture. Considering a temporal decision task, the question is whether a specific structure allows for better performance and if so, whether this structure shares some similarity with the basal ganglia. Our results highlight the advantage of having a slow (direct) and a fast (hyperdirect) pathway that allows to deal with late information during a decision making task.","sentences":["The structure of the basal ganglia is remarkably similar across a number of species (often described in terms of direct, indirect and hyperdirect pathways) and is deeply involved in decision making and action selection.","In this article, we are interested in exploring the role of structure when solving a decision task while avoiding to make any strong assumption regarding the actual structure.","To do so, we exploit the echo state network paradigm that allows to solve complex task based on a random architecture.","Considering a temporal decision task, the question is whether a specific structure allows for better performance and if so, whether this structure shares some similarity with the basal ganglia.","Our results highlight the advantage of having a slow (direct) and a fast (hyperdirect) pathway that allows to deal with late information during a decision making task."],"url":"http://arxiv.org/abs/2401.10849v1","category":"cs.NE"}
{"created":"2024-01-19 17:48:05","title":"Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation","abstract":"We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain even when the global pose is not correct. Our model is then trained in an EM fashion, alternating between updating the vertex features and the feature extractor. We show that our method simulates fine-tuning on a global pseudo-labeled dataset under mild assumptions, which converges to the target domain asymptotically. Through extensive empirical validation, including a complex extreme UDA setup which combines real nuisances, synthetic noise, and occlusion, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly improving pose estimation accuracy.","sentences":["We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation.","Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain.","We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data.","Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates.","We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering.","We focus on individual locally robust mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain even when the global pose is not correct.","Our model is then trained in an EM fashion, alternating between updating the vertex features and the feature extractor.","We show that our method simulates fine-tuning on a global pseudo-labeled dataset under mild assumptions, which converges to the target domain asymptotically.","Through extensive empirical validation, including a complex extreme UDA setup which combines real nuisances, synthetic noise, and occlusion, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly improving pose estimation accuracy."],"url":"http://arxiv.org/abs/2401.10848v1","category":"cs.CV"}
{"created":"2024-01-19 17:47:10","title":"Distributed Genetic Algorithm for Feature Selection","abstract":"We empirically show that process-based Parallelism speeds up the Genetic Algorithm (GA) for Feature Selection (FS) 2x to 25x, while additionally increasing the Machine Learning (ML) model performance on metrics such as F1-score, Accuracy, and Receiver Operating Characteristic Area Under the Curve (ROC-AUC).","sentences":["We empirically show that process-based Parallelism speeds up the Genetic Algorithm (GA) for Feature Selection (FS) 2x to 25x, while additionally increasing the Machine Learning (ML) model performance on metrics such as F1-score, Accuracy, and Receiver Operating Characteristic Area Under the Curve (ROC-AUC)."],"url":"http://arxiv.org/abs/2401.10846v1","category":"cs.DC"}
{"created":"2024-01-19 17:43:38","title":"Emotion Classification In Software Engineering Texts: A Comparative Analysis of Pre-trained Transformers Language Models","abstract":"Emotion recognition in software engineering texts is critical for understanding developer expressions and improving collaboration. This paper presents a comparative analysis of state-of-the-art Pre-trained Language Models (PTMs) for fine-grained emotion classification on two benchmark datasets from GitHub and Stack Overflow. We evaluate six transformer models - BERT, RoBERTa, ALBERT, DeBERTa, CodeBERT and GraphCodeBERT against the current best-performing tool SEntiMoji. Our analysis reveals consistent improvements ranging from 1.17\\% to 16.79\\% in terms of macro-averaged and micro-averaged F1 scores, with general domain models outperforming specialized ones. To further enhance PTMs, we incorporate polarity features in attention layer during training, demonstrating additional average gains of 1.0\\% to 10.23\\% over baseline PTMs approaches. Our work provides strong evidence for the advancements afforded by PTMs in recognizing nuanced emotions like Anger, Love, Fear, Joy, Sadness, and Surprise in software engineering contexts. Through comprehensive benchmarking and error analysis, we also outline scope for improvements to address contextual gaps.","sentences":["Emotion recognition in software engineering texts is critical for understanding developer expressions and improving collaboration.","This paper presents a comparative analysis of state-of-the-art Pre-trained Language Models (PTMs) for fine-grained emotion classification on two benchmark datasets from GitHub and Stack Overflow.","We evaluate six transformer models - BERT, RoBERTa, ALBERT, DeBERTa, CodeBERT and GraphCodeBERT against the current best-performing tool SEntiMoji.","Our analysis reveals consistent improvements ranging from 1.17\\% to 16.79\\% in terms of macro-averaged and micro-averaged F1 scores, with general domain models outperforming specialized ones.","To further enhance PTMs, we incorporate polarity features in attention layer during training, demonstrating additional average gains of 1.0\\% to 10.23\\% over baseline PTMs approaches.","Our work provides strong evidence for the advancements afforded by PTMs in recognizing nuanced emotions like Anger, Love, Fear, Joy, Sadness, and Surprise in software engineering contexts.","Through comprehensive benchmarking and error analysis, we also outline scope for improvements to address contextual gaps."],"url":"http://arxiv.org/abs/2401.10845v1","category":"cs.SE"}
{"created":"2024-01-19 17:40:50","title":"Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media","abstract":"Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic similarity to known antisemitic terminology using a fine-tuned large language model, and subsequent filtering out of the expressions that are too distant from known expressions of hatred. Emergent antisemitic expressions containing terms clearly relating to Jewish topics are then removed to return only coded expressions of hatred.","sentences":["Online hate speech proliferation has created a difficult problem for social media platforms.","A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection.","Coded language evolves quickly and its use varies over time.","This paper proposes a methodology for detecting emerging coded hate-laden terminology.","The methodology is tested in the context of online antisemitic discourse.","The approach considers posts scraped from social media platforms, often used by extremist users.","The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews.","The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus.","It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology.","This is followed by an assessment of semantic similarity to known antisemitic terminology using a fine-tuned large language model, and subsequent filtering out of the expressions that are too distant from known expressions of hatred.","Emergent antisemitic expressions containing terms clearly relating to Jewish topics are then removed to return only coded expressions of hatred."],"url":"http://arxiv.org/abs/2401.10841v1","category":"cs.CL"}
{"created":"2024-01-19 17:39:56","title":"Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation","abstract":"Dictation enables efficient text input on mobile devices. However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing. This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision. Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text. LLM-assisted macro revisions allow users to respeak, split, merge and transform dictated text without specifying precise editing locations. Together they pave the way for interactive dictation and revision that help close gaps between spontaneous spoken words and well-structured writing. In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies.","sentences":["Dictation enables efficient text input on mobile devices.","However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing.","This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision.","Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text.","LLM-assisted macro revisions allow users to respeak, split, merge and transform dictated text without specifying precise editing locations.","Together they pave the way for interactive dictation and revision that help close gaps between spontaneous spoken words and well-structured writing.","In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies."],"url":"http://arxiv.org/abs/2401.10838v1","category":"cs.HC"}
{"created":"2024-01-19 17:37:32","title":"Aerial Field Robotics","abstract":"Aerial field robotics research represents the domain of study that aims to equip unmanned aerial vehicles - and as it pertains to this chapter, specifically Micro Aerial Vehicles (MAVs)- with the ability to operate in real-life environments that present challenges to safe navigation. We present the key elements of autonomy for MAVs that are resilient to collisions and sensing degradation, while operating under constrained computational resources. We overview aspects of the state of the art, outline bottlenecks to resilient navigation autonomy, and overview the field-readiness of MAVs. We conclude with notable contributions and discuss considerations for future research that are essential for resilience in aerial robotics.","sentences":["Aerial field robotics research represents the domain of study that aims to equip unmanned aerial vehicles - and as it pertains to this chapter, specifically Micro Aerial Vehicles (MAVs)- with the ability to operate in real-life environments that present challenges to safe navigation.","We present the key elements of autonomy for MAVs that are resilient to collisions and sensing degradation, while operating under constrained computational resources.","We overview aspects of the state of the art, outline bottlenecks to resilient navigation autonomy, and overview the field-readiness of MAVs.","We conclude with notable contributions and discuss considerations for future research that are essential for resilience in aerial robotics."],"url":"http://arxiv.org/abs/2401.10837v1","category":"cs.RO"}
{"created":"2024-01-19 17:32:04","title":"Cppless: Productive and Performant Serverless Programming in C++","abstract":"The rise of serverless introduced a new class of scalable, elastic and highly available parallel workers in the cloud. Many systems and applications benefit from offloading computations and parallel tasks to dynamically allocated resources. However, the developers of C++ applications found it difficult to integrate functions due to complex deployment, lack of compatibility between client and cloud environments, and loosely typed input and output data. To enable single-source and efficient serverless acceleration in C++, we introduce Cppless, an end-to-end framework for implementing serverless functions which handles the creation, deployment, and invocation of functions. Cppless is built on top of LLVM and requires only two compiler extensions to automatically extract C++ function objects and deploy them to the cloud. We demonstrate that offloading parallel computations from a C++ application to serverless workers can provide up to 30x speedup, requiring only minor code modifications and costing less than one cent per computation.","sentences":["The rise of serverless introduced a new class of scalable, elastic and highly available parallel workers in the cloud.","Many systems and applications benefit from offloading computations and parallel tasks to dynamically allocated resources.","However, the developers of C++ applications found it difficult to integrate functions due to complex deployment, lack of compatibility between client and cloud environments, and loosely typed input and output data.","To enable single-source and efficient serverless acceleration in C++, we introduce Cppless, an end-to-end framework for implementing serverless functions which handles the creation, deployment, and invocation of functions.","Cppless is built on top of LLVM and requires only two compiler extensions to automatically extract C++ function objects and deploy them to the cloud.","We demonstrate that offloading parallel computations from a C++ application to serverless workers can provide up to 30x speedup, requiring only minor code modifications and costing less than one cent per computation."],"url":"http://arxiv.org/abs/2401.10834v1","category":"cs.DC"}
{"created":"2024-01-19 17:29:12","title":"Quality Requirements for Code: On the Untapped Potential in Maintainability Specifications","abstract":"Quality requirements are critical for successful software engineering, with maintainability being a key internal quality. Despite significant attention in software metrics research, maintainability has attracted surprisingly little focus in the Requirements Engineering (RE) community. This position paper proposes a synergistic approach, combining code-oriented research with RE expertise, to create meaningful industrial impact. We introduce six illustrative use cases and propose three future research directions. Preliminary findings indicate that the established QUPER model, designed for setting quality targets, does not adequately address the unique aspects of maintainability.","sentences":["Quality requirements are critical for successful software engineering, with maintainability being a key internal quality.","Despite significant attention in software metrics research, maintainability has attracted surprisingly little focus in the Requirements Engineering (RE) community.","This position paper proposes a synergistic approach, combining code-oriented research with RE expertise, to create meaningful industrial impact.","We introduce six illustrative use cases and propose three future research directions.","Preliminary findings indicate that the established QUPER model, designed for setting quality targets, does not adequately address the unique aspects of maintainability."],"url":"http://arxiv.org/abs/2401.10833v1","category":"cs.SE"}
{"created":"2024-01-19 17:27:21","title":"Understanding Video Transformers via Universal Concept Discovery","abstract":"This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models. Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers. Finally, we demonstrate that VTCDcan be used to improve model performance for fine-grained tasks.","sentences":["This paper studies the problem of concept-based interpretability of transformer representations for videos.","Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered.","Prior research on concept-based interpretability has concentrated solely on image-level tasks.","Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time.","In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm.","To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model.","The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models.","Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers.","Finally, we demonstrate that VTCDcan be used to improve model performance for fine-grained tasks."],"url":"http://arxiv.org/abs/2401.10831v1","category":"cs.CV"}
{"created":"2024-01-19 17:21:05","title":"A survey on recent advances in named entity recognition","abstract":"Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.","sentences":["Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations).","In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys.","Second, we focus on methods designed for datasets with scarce annotations.","Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes).","We thus provide a deep comparison of algorithms that are never considered together.","Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare."],"url":"http://arxiv.org/abs/2401.10825v1","category":"cs.CL"}
{"created":"2024-01-19 17:16:40","title":"Reconfigurable Intelligent Surface (RIS)-Assisted Entanglement Distribution in FSO Quantum Networks","abstract":"Quantum networks (QNs) relying on free-space optical (FSO) quantum channels can support quantum applications in environments wherein establishing an optical fiber infrastructure is challenging and costly. However, FSO-based QNs require a clear line-of-sight (LoS) between users, which is challenging due to blockages and natural obstacles. In this paper, a reconfigurable intelligent surface (RIS)-assisted FSO-based QN is proposed as a cost-efficient framework providing a virtual LoS between users for entanglement distribution. A novel modeling of the quantum noise and losses experienced by quantum states over FSO channels defined by atmospheric losses, turbulence, and pointing errors is derived. Then, the joint optimization of entanglement distribution and RIS placement problem is formulated, under heterogeneous entanglement rate and fidelity constraints. This problem is solved using a simulated annealing metaheuristic algorithm. Simulation results show that the proposed framework effectively meets the minimum fidelity requirements of all users' quantum applications. This is in stark contrast to baseline algorithms that lead to a drop of at least 83% in users' end-to-end fidelities. The proposed framework also achieves a 64% enhancement in the fairness level between users compared to baseline rate maximizing frameworks. Finally, the weather conditions, e.g., rain, are observed to have a more significant effect than pointing errors and turbulence.","sentences":["Quantum networks (QNs) relying on free-space optical (FSO) quantum channels can support quantum applications in environments wherein establishing an optical fiber infrastructure is challenging and costly.","However, FSO-based QNs require a clear line-of-sight (LoS) between users, which is challenging due to blockages and natural obstacles.","In this paper, a reconfigurable intelligent surface (RIS)-assisted FSO-based QN is proposed as a cost-efficient framework providing a virtual LoS between users for entanglement distribution.","A novel modeling of the quantum noise and losses experienced by quantum states over FSO channels defined by atmospheric losses, turbulence, and pointing errors is derived.","Then, the joint optimization of entanglement distribution and RIS placement problem is formulated, under heterogeneous entanglement rate and fidelity constraints.","This problem is solved using a simulated annealing metaheuristic algorithm.","Simulation results show that the proposed framework effectively meets the minimum fidelity requirements of all users' quantum applications.","This is in stark contrast to baseline algorithms that lead to a drop of at least 83% in users' end-to-end fidelities.","The proposed framework also achieves a 64% enhancement in the fairness level between users compared to baseline rate maximizing frameworks.","Finally, the weather conditions, e.g., rain, are observed to have a more significant effect than pointing errors and turbulence."],"url":"http://arxiv.org/abs/2401.10823v1","category":"cs.NI"}
{"created":"2024-01-19 17:16:16","title":"ActAnywhere: Subject-Aware Video Background Generation","abstract":"Generating video background that tailors to foreground subject motion is an important problem for the movie industry and visual effects community. This task involves synthesizing background that aligns with the motion and appearance of the foreground subject, while also complies with the artist's creative intention. We introduce ActAnywhere, a generative model that automates this process which traditionally requires tedious manual efforts. Our model leverages the power of large-scale video diffusion models, and is specifically tailored for this task. ActAnywhere takes a sequence of foreground subject segmentation as input and an image that describes the desired scene as condition, to produce a coherent video with realistic foreground-background interactions while adhering to the condition frame. We train our model on a large-scale dataset of human-scene interaction videos. Extensive evaluations demonstrate the superior performance of our model, significantly outperforming baselines. Moreover, we show that ActAnywhere generalizes to diverse out-of-distribution samples, including non-human subjects. Please visit our project webpage at https://actanywhere.github.io.","sentences":["Generating video background that tailors to foreground subject motion is an important problem for the movie industry and visual effects community.","This task involves synthesizing background that aligns with the motion and appearance of the foreground subject, while also complies with the artist's creative intention.","We introduce ActAnywhere, a generative model that automates this process which traditionally requires tedious manual efforts.","Our model leverages the power of large-scale video diffusion models, and is specifically tailored for this task.","ActAnywhere takes a sequence of foreground subject segmentation as input and an image that describes the desired scene as condition, to produce a coherent video with realistic foreground-background interactions while adhering to the condition frame.","We train our model on a large-scale dataset of human-scene interaction videos.","Extensive evaluations demonstrate the superior performance of our model, significantly outperforming baselines.","Moreover, we show that ActAnywhere generalizes to diverse out-of-distribution samples, including non-human subjects.","Please visit our project webpage at https://actanywhere.github.io."],"url":"http://arxiv.org/abs/2401.10822v1","category":"cs.CV"}
{"created":"2024-01-19 17:13:01","title":"Help Me Reflect: Leveraging Self-Reflection Interface Nudges to Enhance Deliberativeness on Online Deliberation Platforms","abstract":"The deliberative potential of online platforms has been widely examined. However, little is known about how various interface-based reflection nudges impact the quality of deliberation. This paper presents two user studies with 12 and 120 participants, respectively, to investigate the impacts of different reflective nudges on the quality of deliberation. In the first study, we examined five distinct reflective nudges: persona, temporal prompts, analogies and metaphors, cultural prompts and storytelling. Persona, temporal prompts, and storytelling emerged as the preferred nudges for implementation on online deliberation platforms. In the second study, we assess the impacts of these preferred reflectors more thoroughly. Results revealed a significant positive impact of these reflectors on deliberative quality. Specifically, persona promotes a deliberative environment for balanced and opinionated viewpoints while temporal prompts promote more individualised viewpoints. Our findings suggest that the choice of reflectors can significantly influence the dynamics and shape the nature of online discussions.","sentences":["The deliberative potential of online platforms has been widely examined.","However, little is known about how various interface-based reflection nudges impact the quality of deliberation.","This paper presents two user studies with 12 and 120 participants, respectively, to investigate the impacts of different reflective nudges on the quality of deliberation.","In the first study, we examined five distinct reflective nudges: persona, temporal prompts, analogies and metaphors, cultural prompts and storytelling.","Persona, temporal prompts, and storytelling emerged as the preferred nudges for implementation on online deliberation platforms.","In the second study, we assess the impacts of these preferred reflectors more thoroughly.","Results revealed a significant positive impact of these reflectors on deliberative quality.","Specifically, persona promotes a deliberative environment for balanced and opinionated viewpoints while temporal prompts promote more individualised viewpoints.","Our findings suggest that the choice of reflectors can significantly influence the dynamics and shape the nature of online discussions."],"url":"http://arxiv.org/abs/2401.10820v1","category":"cs.HC"}
{"created":"2024-01-19 17:09:32","title":"Optimisation in Neurosymbolic Learning Systems","abstract":"Neurosymbolic AI aims to integrate deep learning with symbolic AI. This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems. We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages. How do we connect the symbolic and neural components to communicate this knowledge? One option is fuzzy reasoning, which studies degrees of truth. For example, being tall is not a binary concept. Instead, probabilistic reasoning studies the probability that something is true or will happen. Our first research question studies how different forms of fuzzy reasoning combine with learning. We find surprising results like a connection to the Raven paradox stating we confirm \"ravens are black\" when we observe a green apple. In this study, we did not use the background knowledge when we deployed our models after training. In our second research question, we studied how to use background knowledge in deployed models. We developed a new neural network layer based on fuzzy reasoning. Probabilistic reasoning is a natural fit for neural networks, which we usually train to be probabilistic. However, they are expensive to compute and do not scale well to large tasks. In our third research question, we study how to connect probabilistic reasoning with neural networks by sampling to estimate averages, while in the final research question, we study scaling probabilistic neurosymbolic learning to much larger problems than before. Our insight is to train a neural network with synthetic data to predict the result of probabilistic reasoning.","sentences":["Neurosymbolic AI aims to integrate deep learning with symbolic AI.","This integration has many promises, such as decreasing the amount of data required to train a neural network, improving the explainability and interpretability of answers given by models and verifying the correctness of trained systems.","We study neurosymbolic learning, where we have both data and background knowledge expressed using symbolic languages.","How do we connect the symbolic and neural components to communicate this knowledge?","One option is fuzzy reasoning, which studies degrees of truth.","For example, being tall is not a binary concept.","Instead, probabilistic reasoning studies the probability that something is true or will happen.","Our first research question studies how different forms of fuzzy reasoning combine with learning.","We find surprising results like a connection to the Raven paradox stating we confirm \"ravens are black\" when we observe a green apple.","In this study, we did not use the background knowledge when we deployed our models after training.","In our second research question, we studied how to use background knowledge in deployed models.","We developed a new neural network layer based on fuzzy reasoning.","Probabilistic reasoning is a natural fit for neural networks, which we usually train to be probabilistic.","However, they are expensive to compute and do not scale well to large tasks.","In our third research question, we study how to connect probabilistic reasoning with neural networks by sampling to estimate averages, while in the final research question, we study scaling probabilistic neurosymbolic learning to much larger problems than before.","Our insight is to train a neural network with synthetic data to predict the result of probabilistic reasoning."],"url":"http://arxiv.org/abs/2401.10819v1","category":"cs.AI"}
{"created":"2024-01-19 17:04:41","title":"A Proof of the Pentagon Relation for Skeins","abstract":"In \\cite{HSZ23}, with Gus Schrader and Eric Zaslow we developed a skein-theoretic version of cluster theory, and made a conjecture on the pentagon relation for the skein dilogarithm. Here we give a topological proof of this conjecture. Combining \\cite{MS21} and \\cite{BCMN23}, we get a surjection from the skein algebra $\\mathrm{Sk}^+(T - D)$ to the positive part of the elliptic Hall algebra $\\mathcal{E}_{q, t}^+$. Hence our pentagon relation generalizes the ones in \\cite{Z23} and \\cite{GM19}.","sentences":["In \\cite{HSZ23}, with Gus Schrader and Eric Zaslow we developed a skein-theoretic version of cluster theory, and made a conjecture on the pentagon relation for the skein dilogarithm.","Here we give a topological proof of this conjecture.","Combining \\cite{MS21} and \\cite{BCMN23}, we get a surjection from the skein algebra $\\mathrm{Sk}^+(T - D)$ to the positive part of the elliptic Hall algebra $\\mathcal{E}_{q, t}^+$. Hence our pentagon relation generalizes the ones in \\cite{Z23} and \\cite{GM19}."],"url":"http://arxiv.org/abs/2401.10817v1","category":"math.QA"}
{"created":"2024-01-19 17:03:37","title":"Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes","abstract":"The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well received, with a 13.1% of nudges sent being opened (open rate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as not useful thereby demonstrating significant improvement in population level engagement metrics.","sentences":["The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes.","We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices.","Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore.","We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\\times10^{-2}$), compared to matched participants in control group who did not receive any nudges.","Further, such nudges were very well received, with a 13.1% of nudges sent being opened (open rate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as not useful thereby demonstrating significant improvement in population level engagement metrics."],"url":"http://arxiv.org/abs/2401.10816v1","category":"cs.HC"}
{"created":"2024-01-19 17:02:17","title":"RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision","abstract":"Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.","sentences":["Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains.","However, resulting features are limited by the information contained within the text.","This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information.","In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders.","We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks.","Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images).","To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports.","Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder."],"url":"http://arxiv.org/abs/2401.10815v1","category":"cs.CV"}
{"created":"2024-01-19 16:56:11","title":"Simulation Based Bayesian Optimization","abstract":"Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.   This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \\emph{sampling-based} access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO. In applications involving combinatorial optimization, we demonstrate empirically the effectiveness of SBBO method using various choices of surrogate models.","sentences":["Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations.","BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function.","For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions.","However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.   ","This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \\emph{sampling-based} access to posterior predictive distributions.","SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables.","Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO.","In applications involving combinatorial optimization, we demonstrate empirically the effectiveness of SBBO method using various choices of surrogate models."],"url":"http://arxiv.org/abs/2401.10811v1","category":"stat.ML"}
{"created":"2024-01-19 16:52:53","title":"Neglected Hessian component explains mysteries in Sharpness regularization","abstract":"Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.","sentences":["Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning.","Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits.","We show that these differences can be explained by the structure of the Hessian of the loss.","First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration.","The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation.","Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function.","Using this insight we design interventions to improve performance.","We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties.","This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning.","We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties."],"url":"http://arxiv.org/abs/2401.10809v1","category":"cs.LG"}
{"created":"2024-01-19 16:48:49","title":"Learning to Visually Connect Actions and their Effects","abstract":"In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.","sentences":["In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding.","CATE can have applications in areas like task planning and learning from demonstration.","We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels.","We observe that different formulations produce representations capturing intuitive action properties.","We also design various baseline models for action selection and action specification.","Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin.","The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models."],"url":"http://arxiv.org/abs/2401.10805v1","category":"cs.CV"}
{"created":"2024-01-19 16:43:57","title":"Endovascular Detection of Catheter-Thrombus Contact by Vacuum Excitation","abstract":"Objective: The objective of this work is to introduce and demonstrate the effectiveness of a novel sensing modality for contact detection between an off-the-shelf aspiration catheter and a thrombus. Methods: A custom robotic actuator with a pressure sensor was used to generate an oscillatory vacuum excitation and sense the pressure inside the extracorporeal portion of the catheter. Vacuum pressure profiles and robotic motion data were used to train a support vector machine (SVM) classification model to detect contact between the aspiration catheter tip and a mock thrombus. Validation consisted of benchtop accuracy verification, as well as user study comparison to the current standard of angiographic presentation. Results: Benchtop accuracy of the sensing modality was shown to be 99.67%. The user study demonstrated statistically significant improvement in identifying catheter-thrombus contact compared to the current standard. The odds ratio of successful detection of clot contact was 2.86 (p=0.03) when using the proposed sensory method compared to without it. Conclusion: The results of this work indicate that the proposed sensing modality can offer intraoperative feedback to interventionalists that can improve their ability to detect contact between the distal tip of a catheter and a thrombus. Significance: By offering a relatively low-cost technology that affords off-the-shelf aspiration catheters as clot-detecting sensors, interventionalists can improve the first-pass effect of the mechanical thrombectomy procedure while reducing procedural times and mental burden.","sentences":["Objective: The objective of this work is to introduce and demonstrate the effectiveness of a novel sensing modality for contact detection between an off-the-shelf aspiration catheter and a thrombus.","Methods: A custom robotic actuator with a pressure sensor was used to generate an oscillatory vacuum excitation and sense the pressure inside the extracorporeal portion of the catheter.","Vacuum pressure profiles and robotic motion data were used to train a support vector machine (SVM) classification model to detect contact between the aspiration catheter tip and a mock thrombus.","Validation consisted of benchtop accuracy verification, as well as user study comparison to the current standard of angiographic presentation.","Results: Benchtop accuracy of the sensing modality was shown to be 99.67%.","The user study demonstrated statistically significant improvement in identifying catheter-thrombus contact compared to the current standard.","The odds ratio of successful detection of clot contact was 2.86 (p=0.03) when using the proposed sensory method compared to without it.","Conclusion: The results of this work indicate that the proposed sensing modality can offer intraoperative feedback to interventionalists that can improve their ability to detect contact between the distal tip of a catheter and a thrombus.","Significance:","By offering a relatively low-cost technology that affords off-the-shelf aspiration catheters as clot-detecting sensors, interventionalists can improve the first-pass effect of the mechanical thrombectomy procedure while reducing procedural times and mental burden."],"url":"http://arxiv.org/abs/2401.10804v1","category":"cs.RO"}
{"created":"2024-01-19 16:36:27","title":"Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm","abstract":"The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function\" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those in the literature where a physically-informed score function was used. We show that coupling a rare-event algorithm with machine learning allows for a correct estimation of transition probabilities, transition times, and even transition paths for a wide range of model parameters. We then extend these results to the more difficult problem of S-transitions in the same model. In both cases of F- and S-transitions, we also show how the Next-Generation Reservoir Computing technique can be interpreted to retrieve an analytical estimate of the committor function.","sentences":["The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming.","The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS).","However, the efficiency and accuracy of TAMS depend on the choice of the score function.","Although the definition of the optimal score function, called ``committor function\" is known, it is impossible in general to compute it a priori.","Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm.","We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions.","Results for the F-transtions compare favorably with those in the literature where a physically-informed score function was used.","We show that coupling a rare-event algorithm with machine learning allows for a correct estimation of transition probabilities, transition times, and even transition paths for a wide range of model parameters.","We then extend these results to the more difficult problem of S-transitions in the same model.","In both cases of F- and S-transitions, we also show how the Next-Generation Reservoir Computing technique can be interpreted to retrieve an analytical estimate of the committor function."],"url":"http://arxiv.org/abs/2401.10800v1","category":"physics.ao-ph"}
{"created":"2024-01-19 16:34:37","title":"Novel Representation Learning Technique using Graphs for Performance Analytics","abstract":"The performance analytics domain in High Performance Computing (HPC) uses tabular data to solve regression problems, such as predicting the execution time. Existing Machine Learning (ML) techniques leverage the correlations among features given tabular datasets, not leveraging the relationships between samples directly. Moreover, since high-quality embeddings from raw features improve the fidelity of the downstream predictive models, existing methods rely on extensive feature engineering and pre-processing steps, costing time and manual effort. To fill these two gaps, we propose a novel idea of transforming tabular performance data into graphs to leverage the advancement of Graph Neural Network-based (GNN) techniques in capturing complex relationships between features and samples. In contrast to other ML application domains, such as social networks, the graph is not given; instead, we need to build it. To address this gap, we propose graph-building methods where nodes represent samples, and the edges are automatically inferred iteratively based on the similarity between the features in the samples. We evaluate the effectiveness of the generated embeddings from GNNs based on how well they make even a simple feed-forward neural network perform for regression tasks compared to other state-of-the-art representation learning techniques. Our evaluation demonstrates that even with up to 25% random missing values for each dataset, our method outperforms commonly used graph and Deep Neural Network (DNN)-based approaches and achieves up to 61.67% & 78.56% improvement in MSE loss over the DNN baseline respectively for HPC dataset and Machine Learning Datasets.","sentences":["The performance analytics domain in High Performance Computing (HPC) uses tabular data to solve regression problems, such as predicting the execution time.","Existing Machine Learning (ML) techniques leverage the correlations among features given tabular datasets, not leveraging the relationships between samples directly.","Moreover, since high-quality embeddings from raw features improve the fidelity of the downstream predictive models, existing methods rely on extensive feature engineering and pre-processing steps, costing time and manual effort.","To fill these two gaps, we propose a novel idea of transforming tabular performance data into graphs to leverage the advancement of Graph Neural Network-based (GNN) techniques in capturing complex relationships between features and samples.","In contrast to other ML application domains, such as social networks, the graph is not given; instead, we need to build it.","To address this gap, we propose graph-building methods where nodes represent samples, and the edges are automatically inferred iteratively based on the similarity between the features in the samples.","We evaluate the effectiveness of the generated embeddings from GNNs based on how well they make even a simple feed-forward neural network perform for regression tasks compared to other state-of-the-art representation learning techniques.","Our evaluation demonstrates that even with up to 25% random missing values for each dataset, our method outperforms commonly used graph and Deep Neural Network (DNN)-based approaches and achieves up to 61.67% & 78.56% improvement in MSE loss over the DNN baseline respectively for HPC dataset and Machine Learning Datasets."],"url":"http://arxiv.org/abs/2401.10799v1","category":"cs.LG"}
{"created":"2024-01-19 16:31:54","title":"Drawing a parallel between the trend of confirmed COVID-19 deaths in the winters of 2022/2023 and 2023/2024 in Italy, with a prediction","abstract":"We studied the weekly number and the growth decline rates of COVID-19 deaths of the period from October 31, 2022 to February 9, 2023, in Italy, finding that that COVID 19 winter wave reached its peak during the three holiday weeks from December 16, 2002 to January 5, 2023, and it was definitely trending downward, returning to the same number of deaths of the end of October 2022, in the first week February 2023. During this period of 15 weeks, that wave caused a number of deaths as large as 8,526. Its percentage average growth rate was +7.89 deaths per week (10 weeks), while the percentage average weekly decline rate was -12.32 (5 weeks). At the time of writing of this paper, Italy was experiencing a new COVID 19 wave, with the latest 7 weekly bulletins available at that date (from October 26, 2023 to December 14, 2023) showing that the deaths had climbed from 148 to 322. The percentage weekly growth rate had risen by +14.08 deaths, on average. In the hypothesis that this 2023-2024 wave will have a total duration similar to that of 2022 2023, with a comparable extension of both the growing period and of the decline period and similar growth decline rates, a prediction can be cast regarding the number of COVID 19 deaths of the period from the end of October 2023 to the beginning of February 2024. They should be in the neighbourhood of 4100,4200. A preliminary assessment of this forecast, based on 11 of the 15 weeks of the period, has confirmed the validity of the approach.","sentences":["We studied the weekly number and the growth decline rates of COVID-19 deaths of the period from October 31, 2022 to February 9, 2023, in Italy, finding that that COVID 19 winter wave reached its peak during the three holiday weeks from December 16, 2002 to January 5, 2023, and it was definitely trending downward, returning to the same number of deaths of the end of October 2022, in the first week February 2023.","During this period of 15 weeks, that wave caused a number of deaths as large as 8,526.","Its percentage average growth rate was +7.89 deaths per week (10 weeks), while the percentage average weekly decline rate was -12.32 (5 weeks).","At the time of writing of this paper, Italy was experiencing a new COVID 19 wave, with the latest 7 weekly bulletins available at that date (from October 26, 2023 to December 14, 2023) showing that the deaths had climbed from 148 to 322.","The percentage weekly growth rate had risen by +14.08 deaths, on average.","In the hypothesis that this 2023-2024 wave will have a total duration similar to that of 2022 2023, with a comparable extension of both the growing period and of the decline period and similar growth decline rates, a prediction can be cast regarding the number of COVID 19 deaths of the period from the end of October 2023 to the beginning of February 2024.","They should be in the neighbourhood of 4100,4200.","A preliminary assessment of this forecast, based on 11 of the 15 weeks of the period, has confirmed the validity of the approach."],"url":"http://arxiv.org/abs/2401.10798v1","category":"physics.soc-ph"}
{"created":"2024-01-19 16:26:35","title":"Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health Monitoring Systems","abstract":"In smart healthcare, health monitoring utilizes diverse tools and technologies to analyze patients' real-time biosignal data, enabling immediate actions and interventions. Existing monitoring approaches were designed on the premise that medical devices track several health metrics concurrently, tailored to their designated functional scope. This means that they report all relevant health values within that scope, which can result in excess resource use and the gathering of extraneous data due to monitoring irrelevant health metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring strategy (DActAHM) for striking a balance between optimal monitoring performance and cost efficiency, a novel framework based on Deep Reinforcement Learning (DRL) and SlowFast Model to ensure precise monitoring based on users' activities. Specifically, with the SlowFast Model, DActAHM efficiently identifies individual activities and captures these results for enhanced processing. Subsequently, DActAHM refines health metric monitoring in response to the identified activity by incorporating a DRL framework. Extensive experiments comparing DActAHM against three state-of-the-art approaches demonstrate it achieves 27.3% higher gain than the best-performing baseline that fixes monitoring actions over timeline.","sentences":["In smart healthcare, health monitoring utilizes diverse tools and technologies to analyze patients' real-time biosignal data, enabling immediate actions and interventions.","Existing monitoring approaches were designed on the premise that medical devices track several health metrics concurrently, tailored to their designated functional scope.","This means that they report all relevant health values within that scope, which can result in excess resource use and the gathering of extraneous data due to monitoring irrelevant health metrics.","In this context, we propose Dynamic Activity-Aware Health Monitoring strategy (DActAHM) for striking a balance between optimal monitoring performance and cost efficiency, a novel framework based on Deep Reinforcement Learning (DRL) and SlowFast Model to ensure precise monitoring based on users' activities.","Specifically, with the SlowFast Model, DActAHM efficiently identifies individual activities and captures these results for enhanced processing.","Subsequently, DActAHM refines health metric monitoring in response to the identified activity by incorporating a DRL framework.","Extensive experiments comparing DActAHM against three state-of-the-art approaches demonstrate it achieves 27.3% higher gain than the best-performing baseline that fixes monitoring actions over timeline."],"url":"http://arxiv.org/abs/2401.10794v1","category":"cs.LG"}
{"created":"2024-01-19 16:24:34","title":"Implications of the Weak Gravity Conjecture on Charge, Kinetic Mixing, the Photon Mass, and More","abstract":"We investigate several phenomenological implications of the Weak Gravity Conjecture (WGC). We find that the WGC implies that the SM neutrinos must be electrically neutral, that the electric charge in the SM must be quantized, and that the photon must be massless. In addition, we use the WGC to set lower bounds on the electric charge of milli-charged particles (mCP), the gauge coupling of several $U(1)$ extensions of the SM, their kinetic mixing parameter with the SM $U(1)_{\\text{EM}}$, and the axion couplings to photons and fermions. We also set an upper bound on the lifetime of the proton.","sentences":["We investigate several phenomenological implications of the Weak Gravity Conjecture (WGC).","We find that the WGC implies that the SM neutrinos must be electrically neutral, that the electric charge in the SM must be quantized, and that the photon must be massless.","In addition, we use the WGC to set lower bounds on the electric charge of milli-charged particles (mCP), the gauge coupling of several $U(1)$ extensions of the SM, their kinetic mixing parameter with the SM $U(1)_{\\text{EM}}$, and the axion couplings to photons and fermions.","We also set an upper bound on the lifetime of the proton."],"url":"http://arxiv.org/abs/2401.10792v1","category":"hep-ph"}
{"created":"2024-01-19 16:23:53","title":"Early alignment in two-layer networks training is a two-edged sword","abstract":"Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead.","sentences":["Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning.","The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions.","This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al.","(2018) .","For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions.","This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence.","This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead."],"url":"http://arxiv.org/abs/2401.10791v1","category":"cs.LG"}
{"created":"2024-01-19 16:21:55","title":"Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions","abstract":"Although accuracy and other common metrics can provide a useful window into the performance of an object detection model, they lack a deeper view of the model's decision process. Regardless of the quality of the training data and process, the features that an object detection model learns cannot be guaranteed. A model may learn a relationship between certain background context, i.e., scene level objects, and the presence of the labeled classes. Furthermore, standard performance verification and metrics would not identify this phenomenon. This paper presents a new black box explainability method for additional verification of object detection models by finding the impact of scene level objects on the identification of the objects within the image. By comparing the accuracies of a model on test data with and without certain scene level objects, the contributions of these objects to the model's performance becomes clearer. The experiment presented here will assess the impact of buildings and people in image context on the detection of emergency road vehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the presence of a scene level object will indicate the model's reliance on that object to make its detections. The results of this research lead to providing a quantitative explanation of the object detection model's decision process, enabling a deeper understanding of the model's performance.","sentences":["Although accuracy and other common metrics can provide a useful window into the performance of an object detection model, they lack a deeper view of the model's decision process.","Regardless of the quality of the training data and process, the features that an object detection model learns cannot be guaranteed.","A model may learn a relationship between certain background context, i.e., scene level objects, and the presence of the labeled classes.","Furthermore, standard performance verification and metrics would not identify this phenomenon.","This paper presents a new black box explainability method for additional verification of object detection models by finding the impact of scene level objects on the identification of the objects within the image.","By comparing the accuracies of a model on test data with and without certain scene level objects, the contributions of these objects to the model's performance becomes clearer.","The experiment presented here will assess the impact of buildings and people in image context on the detection of emergency road vehicles by a fine-tuned YOLOv8 model.","A large increase in accuracy in the presence of a scene level object will indicate the model's reliance on that object to make its detections.","The results of this research lead to providing a quantitative explanation of the object detection model's decision process, enabling a deeper understanding of the model's performance."],"url":"http://arxiv.org/abs/2401.10790v1","category":"cs.CV"}
{"created":"2024-01-19 16:21:50","title":"Probing polarization response of monolayer cell cultures with photon entanglement","abstract":"This study addresses the critical need for high signal-to-noise ratio in optical detection methods for biological sample discrimination under low-photon-flux conditions to ensure accuracy without compromising sample integrity. We explore polarization-based probing, which often excels over intensity modulation when assessing a specimen's morphology. Leveraging non-classical light sources, our approach capitalizes on sub-Poissonian photon statistics and quantum correlation-based measurements. We present a novel, highly sensitive method for probing single-layer cell cultures using entangled photon pairs. Our approach demonstrates capability in monolayer cell analysis, distinguishing between two types of monolayer cells and their host medium. The experimental results highlight our method's sensitivity, showcasing its potential for biological sample detection using quantum techniques, and paving the way for advanced diagnostic methodologies.","sentences":["This study addresses the critical need for high signal-to-noise ratio in optical detection methods for biological sample discrimination under low-photon-flux conditions to ensure accuracy without compromising sample integrity.","We explore polarization-based probing, which often excels over intensity modulation when assessing a specimen's morphology.","Leveraging non-classical light sources, our approach capitalizes on sub-Poissonian photon statistics and quantum correlation-based measurements.","We present a novel, highly sensitive method for probing single-layer cell cultures using entangled photon pairs.","Our approach demonstrates capability in monolayer cell analysis, distinguishing between two types of monolayer cells and their host medium.","The experimental results highlight our method's sensitivity, showcasing its potential for biological sample detection using quantum techniques, and paving the way for advanced diagnostic methodologies."],"url":"http://arxiv.org/abs/2401.10789v1","category":"physics.optics"}
{"created":"2024-01-19 16:16:11","title":"Hybrid Online Certificate Status Protocol with Certificate Revocation List for Smart Grid Public Key Infrastructure","abstract":"Hsu et al. (2022) proposed a cryptographic scheme within the public key infrastructure to bolster the security of smart grid meters. Their proposal involved developing the Certificate Management over CMS mechanism to establish Simple Certificate Enrollment Protocol and Enrollment over Secure Transport protocol. Additionally, they implemented Online Certificate Status Protocol (OCSP) services to independently query the status of certificates. However, their implementation featured a single OCSP server handling all query requests. Considering the typical scenario in smart grid PKI environments with over tens of thousands of end-meters, we introduced a Hybrid Online Certificate Status Protocol mechanism. This approach decreases demand of query resources from the client to OCSP servers collaborating with Certificate Revocation Lists. Our simulations, mimicking meter behavior, demonstrated increased efficiency, creating a more robust architecture tailored to the smart grid meter landscape.","sentences":["Hsu et al. (2022) proposed a cryptographic scheme within the public key infrastructure to bolster the security of smart grid meters.","Their proposal involved developing the Certificate Management over CMS mechanism to establish Simple Certificate Enrollment Protocol and Enrollment over Secure Transport protocol.","Additionally, they implemented Online Certificate Status Protocol (OCSP) services to independently query the status of certificates.","However, their implementation featured a single OCSP server handling all query requests.","Considering the typical scenario in smart grid PKI environments with over tens of thousands of end-meters, we introduced a Hybrid Online Certificate Status Protocol mechanism.","This approach decreases demand of query resources from the client to OCSP servers collaborating with Certificate Revocation Lists.","Our simulations, mimicking meter behavior, demonstrated increased efficiency, creating a more robust architecture tailored to the smart grid meter landscape."],"url":"http://arxiv.org/abs/2401.10787v1","category":"cs.CR"}
{"created":"2024-01-19 16:15:37","title":"Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion","abstract":"Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services. However, challenges arise from significant view changes and scene scale. Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views. Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery. To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques. Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner. The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency. Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.","sentences":["Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services.","However, challenges arise from significant view changes and scene scale.","Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views.","Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery.","To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques.","Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner.","The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency.","Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery."],"url":"http://arxiv.org/abs/2401.10786v1","category":"cs.CV"}
{"created":"2024-01-19 16:01:38","title":"Metric Dynamic Equilibrium Logic","abstract":"In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states. While this representation reflects their relative order, it abstracts away the specific times associated with each state. In many applications, however, timing constraints are important like, for instance, when planning and scheduling go hand in hand. We address this by developing a metric extension of linear-time Dynamic Equilibrium Logic, in which dynamic operators are constrained by intervals over integers. The resulting Metric Dynamic Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints. As such, it constitutes the most general among a whole spectrum of temporal extensions of Equilibrium Logic. In detail, we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium Logic, as well as its classic counterparts once the law of the excluded middle is added.","sentences":["In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states.","While this representation reflects their relative order, it abstracts away the specific times associated with each state.","In many applications, however, timing constraints are important like, for instance, when planning and scheduling go hand in hand.","We address this by developing a metric extension of linear-time Dynamic Equilibrium Logic, in which dynamic operators are constrained by intervals over integers.","The resulting Metric Dynamic Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints.","As such, it constitutes the most general among a whole spectrum of temporal extensions of Equilibrium Logic.","In detail, we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium Logic, as well as its classic counterparts once the law of the excluded middle is added."],"url":"http://arxiv.org/abs/2401.10781v1","category":"cs.AI"}
{"created":"2024-01-19 15:53:27","title":"HaliVer: Deductive Verification and Scheduling Languages Join Forces","abstract":"The HaliVer tool integrates deductive verification into the popular scheduling language Halide, used for image processing pipelines and array computations. HaliVer uses Vercors, a separation logic-based verifier, to verify the correctness of (1) the Halide algorithms and (2) the optimised parallel code produced by \\halide when an optimisation schedule is applied to the algorithm. This allows proving complex, optimised code correct while reducing the effort to provide the required verification annotations. For both approaches, the same specification is used. We evaluated the tool on several optimised programs generated from characteristic Halide algorithms, using all but one of the essential scheduling directives available in Halide. Without annotation effort, Haliver proves memory safety in almost all programs. With annotations Haliver, additionally, proves functional correctness properties. We show that the approach is viable and reduces the manual annotation effort by an order of magnitude.","sentences":["The HaliVer tool integrates deductive verification into the popular scheduling language Halide, used for image processing pipelines and array computations.","HaliVer uses Vercors, a separation logic-based verifier, to verify the correctness of (1) the Halide algorithms and (2) the optimised parallel code produced by \\halide when an optimisation schedule is applied to the algorithm.","This allows proving complex, optimised code correct while reducing the effort to provide the required verification annotations.","For both approaches, the same specification is used.","We evaluated the tool on several optimised programs generated from characteristic Halide algorithms, using all but one of the essential scheduling directives available in Halide.","Without annotation effort, Haliver proves memory safety in almost all programs.","With annotations Haliver, additionally, proves functional correctness properties.","We show that the approach is viable and reduces the manual annotation effort by an order of magnitude."],"url":"http://arxiv.org/abs/2401.10778v1","category":"cs.LO"}
{"created":"2024-01-19 15:51:34","title":"Determination of efficiency indicators of the stand for intelligent control of manual operations in industrial production","abstract":"Systems of intelligent control of manual operations in industrial production are being implemented in many industries nowadays. Such systems use high-resolution cameras and computer vision algorithms to automatically track the operator's manipulations and prevent technological errors in the assembly process. At the same time compliance with safety regulations in the workspace is monitored. As a result, the defect rate of manufactured products and the number of accidents during the manual assembly of any device are decreased. Before implementing an intelligent control system into a real production it is necessary to calculate its efficiency. In order to do it experiments on the stand for manual operations control systems were carried out. This paper proposes the methodology for calculating the efficiency indicators. This mathematical approach is based on the IoU calculation of real- and predicted-time intervals between assembly stages. The results show high precision in tracking the validity of manual assembly and do not depend on the duration of the assembly process.","sentences":["Systems of intelligent control of manual operations in industrial production are being implemented in many industries nowadays.","Such systems use high-resolution cameras and computer vision algorithms to automatically track the operator's manipulations and prevent technological errors in the assembly process.","At the same time compliance with safety regulations in the workspace is monitored.","As a result, the defect rate of manufactured products and the number of accidents during the manual assembly of any device are decreased.","Before implementing an intelligent control system into a real production it is necessary to calculate its efficiency.","In order to do it experiments on the stand for manual operations control systems were carried out.","This paper proposes the methodology for calculating the efficiency indicators.","This mathematical approach is based on the IoU calculation of real- and predicted-time intervals between assembly stages.","The results show high precision in tracking the validity of manual assembly and do not depend on the duration of the assembly process."],"url":"http://arxiv.org/abs/2401.10777v1","category":"cs.CV"}
{"created":"2024-01-19 15:48:40","title":"Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads","abstract":"The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.   We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.","sentences":["The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators.","While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model.","In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel.","Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step.","By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.   ","We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1:","Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration.","Medusa-2:","Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   ","Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality.","We evaluate Medusa on models of various sizes and training procedures.","Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x."],"url":"http://arxiv.org/abs/2401.10774v1","category":"cs.LG"}
{"created":"2024-01-19 15:47:16","title":"Multilevel lattice codes from Hurwitz integers","abstract":"This study extends the Construction $\\pi_A$ lattices proposed in \\cite{huang2017construction}, to Hurwitz integers. To this, we use a modified version of the Chinese remainder theorem applied to maximal orders. Exploiting the isomorphism guaranteed by this theorem, we construct multilevel lattice codes that effectively attain the Poltyrev-limit. The performance of the proposed lattice codes is evaluated via computer simulations, showing notably reduced computational complexity provided by the multistage decoding.","sentences":["This study extends the Construction $\\pi_A$ lattices proposed in \\cite{huang2017construction}, to Hurwitz integers.","To this, we use a modified version of the Chinese remainder theorem applied to maximal orders.","Exploiting the isomorphism guaranteed by this theorem, we construct multilevel lattice codes that effectively attain the Poltyrev-limit.","The performance of the proposed lattice codes is evaluated via computer simulations, showing notably reduced computational complexity provided by the multistage decoding."],"url":"http://arxiv.org/abs/2401.10773v1","category":"cs.IT"}
{"created":"2024-01-19 15:42:07","title":"Study of Orbital Dynamics in Singular and Regular Naked Singularity Space-times","abstract":"The universe is filled with various compact objects and the most attractive of them are the black holes and singularity. But it is also known that at the singularity density becomes so infinitely high that the present physics knowledge breaks down. Thus, the singularity remains a flaw of the present theories. Several methods have been exercised to resolve the singularity. One such mathematical method is through conformal transformations. This paper deals with regularizing a naked singularity space-time using conformal transformations, further studying and comparing its time-like orbits with that of the naked singularity space-time.","sentences":["The universe is filled with various compact objects and the most attractive of them are the black holes and singularity.","But it is also known that at the singularity density becomes so infinitely high that the present physics knowledge breaks down.","Thus, the singularity remains a flaw of the present theories.","Several methods have been exercised to resolve the singularity.","One such mathematical method is through conformal transformations.","This paper deals with regularizing a naked singularity space-time using conformal transformations, further studying and comparing its time-like orbits with that of the naked singularity space-time."],"url":"http://arxiv.org/abs/2401.10771v1","category":"gr-qc"}
{"created":"2024-01-19 15:41:00","title":"Thresholds for the distributed surface code in the presence of memory decoherence","abstract":"In the search for scalable, fault-tolerant quantum computing, distributed quantum computers are promising candidates. These systems can be realized in large-scale quantum networks or condensed onto a single chip with closely situated nodes. We present a framework for numerical simulations of a memory channel using the distributed toric surface code, where each data qubit of the code is part of a separate node, and the error-detection performance depends on the quality of four-qubit Greenberger-Horne-Zeilinger (GHZ) states generated between the nodes. We quantitatively investigate the effect of memory decoherence and evaluate the advantage of GHZ creation protocols tailored to the level of decoherence. We do this by applying our framework for the particular case of color centers in diamond, employing models developed from experimental characterization of nitrogen-vacancy centers. For diamond color centers, coherence times during entanglement generation are orders of magnitude lower than coherence times of idling qubits. These coherence times represent a limiting factor for applications, but previous surface code simulations did not treat them as such. Introducing limiting coherence times as a prominent noise factor makes it imperative to integrate realistic operation times into simulations and incorporate strategies for operation scheduling. Our model predicts error probability thresholds for gate and measurement reduced by at least a factor of three compared to prior work with more idealized noise models. We also find a threshold of $4\\cdot10^2$ in the ratio between the entanglement generation and the decoherence rates, setting a benchmark for experimental progress.","sentences":["In the search for scalable, fault-tolerant quantum computing, distributed quantum computers are promising candidates.","These systems can be realized in large-scale quantum networks or condensed onto a single chip with closely situated nodes.","We present a framework for numerical simulations of a memory channel using the distributed toric surface code, where each data qubit of the code is part of a separate node, and the error-detection performance depends on the quality of four-qubit Greenberger-Horne-Zeilinger (GHZ) states generated between the nodes.","We quantitatively investigate the effect of memory decoherence and evaluate the advantage of GHZ creation protocols tailored to the level of decoherence.","We do this by applying our framework for the particular case of color centers in diamond, employing models developed from experimental characterization of nitrogen-vacancy centers.","For diamond color centers, coherence times during entanglement generation are orders of magnitude lower than coherence times of idling qubits.","These coherence times represent a limiting factor for applications, but previous surface code simulations did not treat them as such.","Introducing limiting coherence times as a prominent noise factor makes it imperative to integrate realistic operation times into simulations and incorporate strategies for operation scheduling.","Our model predicts error probability thresholds for gate and measurement reduced by at least a factor of three compared to prior work with more idealized noise models.","We also find a threshold of $4\\cdot10^2$ in the ratio between the entanglement generation and the decoherence rates, setting a benchmark for experimental progress."],"url":"http://arxiv.org/abs/2401.10770v1","category":"quant-ph"}
{"created":"2024-01-19 15:39:49","title":"Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment","abstract":"While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation between knowledge inconsistency and hallucination, signifying the effectiveness of reducing knowledge inconsistency in alleviating hallucinations. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/KCA}.","sentences":["While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''.","In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment.","Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs.","For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing.","We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales.","Furthermore, we confirm the correlation between knowledge inconsistency and hallucination, signifying the effectiveness of reducing knowledge inconsistency in alleviating hallucinations.","Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/KCA}."],"url":"http://arxiv.org/abs/2401.10768v1","category":"cs.CL"}
{"created":"2024-01-19 15:38:16","title":"Semantic-Aware Resource Allocation in Constrained Networks with Limited User Participation","abstract":"Semantic communication has gained attention as a key enabler for intelligent and context-aware communication. However, one of the key challenges of semantic communications is the need to tailor the resource allocation to meet the specific requirements of semantic transmission. In this paper, we focus on networks with limited resources where devices are constrained to transmit with limited bandwidth and power over large distance. Specifically, we devise an efficient strategy to select the most pertinent semantic features and participating users, taking into account the channel quality, the transmission time, and the recovery accuracy. To this end, we formulate an optimization problem with the goal of selecting the most relevant and accurate semantic features over devices while satisfying constraints on transmission time and quality of the channel. This involves optimizing communication resources, identifying participating users, and choosing specific semantic information for transmission. The underlying problem is inherently complex due to its non-convex nature and combinatorial constraints. To overcome this challenge, we efficiently approximate the optimal solution by solving a series of integer linear programming problems. Our numerical findings illustrate the effectiveness and efficiency of our approach in managing semantic communications in networks with limited resources.","sentences":["Semantic communication has gained attention as a key enabler for intelligent and context-aware communication.","However, one of the key challenges of semantic communications is the need to tailor the resource allocation to meet the specific requirements of semantic transmission.","In this paper, we focus on networks with limited resources where devices are constrained to transmit with limited bandwidth and power over large distance.","Specifically, we devise an efficient strategy to select the most pertinent semantic features and participating users, taking into account the channel quality, the transmission time, and the recovery accuracy.","To this end, we formulate an optimization problem with the goal of selecting the most relevant and accurate semantic features over devices while satisfying constraints on transmission time and quality of the channel.","This involves optimizing communication resources, identifying participating users, and choosing specific semantic information for transmission.","The underlying problem is inherently complex due to its non-convex nature and combinatorial constraints.","To overcome this challenge, we efficiently approximate the optimal solution by solving a series of integer linear programming problems.","Our numerical findings illustrate the effectiveness and efficiency of our approach in managing semantic communications in networks with limited resources."],"url":"http://arxiv.org/abs/2401.10766v1","category":"cs.IT"}
{"created":"2024-01-19 15:37:11","title":"Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection","abstract":"Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange. However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations. They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts. This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations. It has various applications, such as enhancing financial fraud detection, mitigating terrorism, and enhancing digital health. We implemented Starlit and conducted a thorough performance analysis using synthetic data from a key player in global financial transactions. The evaluation indicates Starlit's scalability, efficiency, and accuracy.","sentences":["Federated Learning (FL) is a data-minimization approach enabling collaborative model training across diverse clients with local data, avoiding direct data exchange.","However, state-of-the-art FL solutions to identify fraudulent financial transactions exhibit a subset of the following limitations.","They (1) lack a formal security definition and proof, (2) assume prior freezing of suspicious customers' accounts by financial institutions (limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$ computationally expensive modular exponentiation (where $n$ is the total number of financial institutions) or highly inefficient fully homomorphic encryption, (4) assume the parties have already completed the identity alignment phase, hence excluding it from the implementation, performance evaluation, and security analysis, and (5) struggle to resist clients' dropouts.","This work introduces Starlit, a novel scalable privacy-preserving FL mechanism that overcomes these limitations.","It has various applications, such as enhancing financial fraud detection, mitigating terrorism, and enhancing digital health.","We implemented Starlit and conducted a thorough performance analysis using synthetic data from a key player in global financial transactions.","The evaluation indicates Starlit's scalability, efficiency, and accuracy."],"url":"http://arxiv.org/abs/2401.10765v1","category":"cs.LG"}
{"created":"2024-01-19 15:32:46","title":"Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models","abstract":"Large Language Models (LLMs) have upended decades of pedagogy in computing education. Students previously learned to code through \\textit{writing} many small problems with less emphasis on code reading and comprehension. Recent research has shown that free code generation tools powered by LLMs can solve introductory programming problems presented in natural language with ease. In this paper, we propose a new way to teach programming with Prompt Problems. Students receive a problem visually, indicating how input should be transformed to output, and must translate that to a prompt for an LLM to decipher. The problem is considered correct when the code that is generated by the student prompt can pass all test cases. In this paper we present the design of this tool, discuss student interactions with it as they learn, and provide insights into this new class of programming problems as well as the design tools that integrate LLMs.","sentences":["Large Language Models (LLMs) have upended decades of pedagogy in computing education.","Students previously learned to code through \\textit{writing} many small problems with less emphasis on code reading and comprehension.","Recent research has shown that free code generation tools powered by LLMs can solve introductory programming problems presented in natural language with ease.","In this paper, we propose a new way to teach programming with Prompt Problems.","Students receive a problem visually, indicating how input should be transformed to output, and must translate that to a prompt for an LLM to decipher.","The problem is considered correct when the code that is generated by the student prompt can pass all test cases.","In this paper we present the design of this tool, discuss student interactions with it as they learn, and provide insights into this new class of programming problems as well as the design tools that integrate LLMs."],"url":"http://arxiv.org/abs/2401.10759v1","category":"cs.HC"}
{"created":"2024-01-19 15:25:14","title":"Code Reviewer Recommendation Based on a Hypergraph with Multiplex Relationships","abstract":"Code review is an essential component of software development, playing a vital role in ensuring a comprehensive check of code changes. However, the continuous influx of pull requests and the limited pool of available reviewer candidates pose a significant challenge to the review process, making the task of assigning suitable reviewers to each review request increasingly difficult. To tackle this issue, we present MIRRec, a novel code reviewer recommendation method that leverages a hypergraph with multiplex relationships. MIRRec encodes high-order correlations that go beyond traditional pairwise connections using degree-free hyperedges among pull requests and developers. This way, it can capture high-order implicit connectivity and identify potential reviewers. To validate the effectiveness of MIRRec, we conducted experiments using a dataset comprising 48,374 pull requests from ten popular open-source software projects hosted on GitHub. The experiment results demonstrate that MIRRec, especially without PR-Review Commenters relationship, outperforms existing stateof-the-art code reviewer recommendation methods in terms of ACC and MRR, highlighting its significance in improving the code review process.","sentences":["Code review is an essential component of software development, playing a vital role in ensuring a comprehensive check of code changes.","However, the continuous influx of pull requests and the limited pool of available reviewer candidates pose a significant challenge to the review process, making the task of assigning suitable reviewers to each review request increasingly difficult.","To tackle this issue, we present MIRRec, a novel code reviewer recommendation method that leverages a hypergraph with multiplex relationships.","MIRRec encodes high-order correlations that go beyond traditional pairwise connections using degree-free hyperedges among pull requests and developers.","This way, it can capture high-order implicit connectivity and identify potential reviewers.","To validate the effectiveness of MIRRec, we conducted experiments using a dataset comprising 48,374 pull requests from ten popular open-source software projects hosted on GitHub.","The experiment results demonstrate that MIRRec, especially without PR-Review Commenters relationship, outperforms existing stateof-the-art code reviewer recommendation methods in terms of ACC and MRR, highlighting its significance in improving the code review process."],"url":"http://arxiv.org/abs/2401.10755v1","category":"cs.SE"}
{"created":"2024-01-19 15:22:28","title":"BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation","abstract":"Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow. Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency. This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs. A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space. The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale from small, simple training datasets to large, complex inference datasets. Finally, BoolGebra is integrated with existing synthesis tool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA baselines.","sentences":["Boolean algebraic manipulation is at the core of logic synthesis in Electronic Design Automation (EDA) design flow.","Existing methods struggle to fully exploit optimization opportunities, and often suffer from an explosive search space and limited scalability efficiency.","This work presents BoolGebra, a novel attributed graph-learning approach for Boolean algebraic manipulation that aims to improve fundamental logic synthesis.","BoolGebra incorporates Graph Neural Networks (GNNs) and takes initial feature embeddings from both structural and functional information as inputs.","A fully connected neural network is employed as the predictor for direct optimization result predictions, significantly reducing the search space and efficiently locating the optimization space.","The experiments involve training the BoolGebra model w.r.t design-specific and cross-design inferences using the trained model, where BoolGebra demonstrates generalizability for cross-design inference and its potential to scale from small, simple training datasets to large, complex inference datasets.","Finally, BoolGebra is integrated with existing synthesis tool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA baselines."],"url":"http://arxiv.org/abs/2401.10753v1","category":"cs.AR"}
{"created":"2024-01-19 15:21:51","title":"HiCD: Change Detection in Quality-Varied Images via Hierarchical Correlation Distillation","abstract":"Advanced change detection techniques primarily target image pairs of equal and high quality. However, variations in imaging conditions and platforms frequently lead to image pairs with distinct qualities: one image being high-quality, while the other being low-quality. These disparities in image quality present significant challenges for understanding image pairs semantically and extracting change features, ultimately resulting in a notable decline in performance. To tackle this challenge, we introduce an innovative training strategy grounded in knowledge distillation. The core idea revolves around leveraging task knowledge acquired from high-quality image pairs to guide the model's learning process when dealing with image pairs that exhibit differences in quality. Additionally, we develop a hierarchical correlation distillation approach (involving self-correlation, cross-correlation, and global correlation). This approach compels the student model to replicate the correlations inherent in the teacher model, rather than focusing solely on individual features. This ensures effective knowledge transfer while maintaining the student model's training flexibility.","sentences":["Advanced change detection techniques primarily target image pairs of equal and high quality.","However, variations in imaging conditions and platforms frequently lead to image pairs with distinct qualities: one image being high-quality, while the other being low-quality.","These disparities in image quality present significant challenges for understanding image pairs semantically and extracting change features, ultimately resulting in a notable decline in performance.","To tackle this challenge, we introduce an innovative training strategy grounded in knowledge distillation.","The core idea revolves around leveraging task knowledge acquired from high-quality image pairs to guide the model's learning process when dealing with image pairs that exhibit differences in quality.","Additionally, we develop a hierarchical correlation distillation approach (involving self-correlation, cross-correlation, and global correlation).","This approach compels the student model to replicate the correlations inherent in the teacher model, rather than focusing solely on individual features.","This ensures effective knowledge transfer while maintaining the student model's training flexibility."],"url":"http://arxiv.org/abs/2401.10752v1","category":"cs.CV"}
{"created":"2024-01-19 15:20:57","title":"EFO: the Emotion Frame Ontology","abstract":"Emotions are a subject of intense debate in various disciplines. Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them. In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience. EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology. EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations. EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge graph, and implementing a graph-based emotion detector from text. In addition, an EFO integration of multimodal datasets, including emotional speech and emotional face expressions, has been performed to enable further inquiry into crossmodal emotion semantics.","sentences":["Emotions are a subject of intense debate in various disciplines.","Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them.","In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO).","EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience.","EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology.","EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network.","In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations.","EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge graph, and implementing a graph-based emotion detector from text.","In addition, an EFO integration of multimodal datasets, including emotional speech and emotional face expressions, has been performed to enable further inquiry into crossmodal emotion semantics."],"url":"http://arxiv.org/abs/2401.10751v1","category":"cs.AI"}
{"created":"2024-01-19 15:19:04","title":"What can abelian gauge theories teach us about kinematic algebras?","abstract":"The phenomenon of BCJ duality implies that gauge theories possess an abstract kinematic algebra, mirroring the non-abelian Lie algebra underlying the colour information. Although the nature of the kinematic algebra is known in certain cases, a full understanding is missing for arbitrary non-abelian gauge theories, such that one typically works outwards from well-known examples. In this paper, we pursue an orthogonal approach, and argue that simpler abelian gauge theories can be used as a testing ground for clarifying our understanding of kinematic algebras. We first describe how classes of abelian gauge fields are associated with well-defined subgroups of the diffeomorphism algebra. By considering certain special subgroups, we show that one may construct interacting theories, whose kinematic algebras are inherited from those already appearing in a related abelian theory. Known properties of (anti-)self-dual Yang-Mills theory arise in this way, but so do new generalisations, including self-dual electromagnetism coupled to scalar matter. Furthermore, a recently obtained non-abelian generalisation of the Navier-Stokes equation fits into a similar scheme, as does Chern-Simons theory. Our results provide useful input to further conceptual studies of kinematic algebras.","sentences":["The phenomenon of BCJ duality implies that gauge theories possess an abstract kinematic algebra, mirroring the non-abelian Lie algebra underlying the colour information.","Although the nature of the kinematic algebra is known in certain cases, a full understanding is missing for arbitrary non-abelian gauge theories, such that one typically works outwards from well-known examples.","In this paper, we pursue an orthogonal approach, and argue that simpler abelian gauge theories can be used as a testing ground for clarifying our understanding of kinematic algebras.","We first describe how classes of abelian gauge fields are associated with well-defined subgroups of the diffeomorphism algebra.","By considering certain special subgroups, we show that one may construct interacting theories, whose kinematic algebras are inherited from those already appearing in a related abelian theory.","Known properties of (anti-)self-dual Yang-Mills theory arise in this way, but so do new generalisations, including self-dual electromagnetism coupled to scalar matter.","Furthermore, a recently obtained non-abelian generalisation of the Navier-Stokes equation fits into a similar scheme, as does Chern-Simons theory.","Our results provide useful input to further conceptual studies of kinematic algebras."],"url":"http://arxiv.org/abs/2401.10750v1","category":"hep-th"}
{"created":"2024-01-19 15:13:30","title":"A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding","abstract":"Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for each subject to use as a majority-voting ensemble classifier. In this scenario, using EA improved the 3-model ensemble accuracy by 3.7%. However, when compared to the shared model with EA, the ensemble accuracy was 3.62% lower.","sentences":["Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks.","While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements.","By leveraging data from multiple subjects, transfer learning enables more effective training of DL models.","A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models.","However, few studies evaluate its impact on the training performance of shared and individual DL models.","In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals.","We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects.","Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%.","We also trained individual models for each subject to use as a majority-voting ensemble classifier.","In this scenario, using EA improved the 3-model ensemble accuracy by 3.7%.","However, when compared to the shared model with EA, the ensemble accuracy was 3.62% lower."],"url":"http://arxiv.org/abs/2401.10746v1","category":"eess.SP"}
{"created":"2024-01-19 15:09:39","title":"FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models","abstract":"Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set. Our experiments demonstrate that synthetic data generated by FinLLMs effectively enhances the performance of several large-scale numerical reasoning models in the financial domain, outperforming two established benchmark financial question-answering datasets.","sentences":["Large Language models (LLMs) usually rely on extensive training datasets.","In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses.","To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models.","First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ.","We then augment the formula set by combining those that share identical variables as new elements.","Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph.","Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set.","Our experiments demonstrate that synthetic data generated by FinLLMs effectively enhances the performance of several large-scale numerical reasoning models in the financial domain, outperforming two established benchmark financial question-answering datasets."],"url":"http://arxiv.org/abs/2401.10744v1","category":"cs.AI"}
{"created":"2024-01-19 14:59:26","title":"Character Recognition in Byzantine Seals with Deep Neural Networks","abstract":"Seals are small coin-shaped artifacts, mostly made of lead, held with strings to seal letters. This work presents the first attempt towards automatic reading of text on Byzantine seal images.Byzantine seals are generally decorated with iconography on the obverse side and Greek text on the reverse side. Text may include the sender's name, position in the Byzantine aristocracy, and elements of prayers. Both text and iconography are precious literary sources that wait to be exploited electronically, so the development of computerized systems for interpreting seals images is of paramount importance. This work's contribution is hence a deep, two-stages, character reading pipeline for transcribing Byzantine seal images. A first deep convolutional neural network (CNN) detects characters in the seal (character localization). A second convolutional network reads the localized characters (character classification). Finally, a diplomatic transcription of the seal is provided by post-processing the two network outputs. We provide an experimental evaluation of each CNN in isolation and both CNNs in combination. All performances are evaluated by cross-validation. Character localization achieves a mean average precision (mAP@0.5) greater than 0.9. Classification of characters cropped from ground truth bounding boxes achieves Top-1 accuracy greater than 0.92. End-to-end evaluation shows the efficiency of the proposed approach when compared to the SoTA for similar tasks.","sentences":["Seals are small coin-shaped artifacts, mostly made of lead, held with strings to seal letters.","This work presents the first attempt towards automatic reading of text on Byzantine seal images.","Byzantine seals are generally decorated with iconography on the obverse side and Greek text on the reverse side.","Text may include the sender's name, position in the Byzantine aristocracy, and elements of prayers.","Both text and iconography are precious literary sources that wait to be exploited electronically, so the development of computerized systems for interpreting seals images is of paramount importance.","This work's contribution is hence a deep, two-stages, character reading pipeline for transcribing Byzantine seal images.","A first deep convolutional neural network (CNN) detects characters in the seal (character localization).","A second convolutional network reads the localized characters (character classification).","Finally, a diplomatic transcription of the seal is provided by post-processing the two network outputs.","We provide an experimental evaluation of each CNN in isolation and both CNNs in combination.","All performances are evaluated by cross-validation.","Character localization achieves a mean average precision (mAP@0.5) greater than 0.9.","Classification of characters cropped from ground truth bounding boxes achieves Top-1 accuracy greater than 0.92.","End-to-end evaluation shows the efficiency of the proposed approach when compared to the SoTA for similar tasks."],"url":"http://arxiv.org/abs/2401.10741v1","category":"cs.CV"}
{"created":"2024-01-19 14:55:51","title":"In-IDE Human-AI Experience in the Era of Large Language Models; A Literature Review","abstract":"IDEs, crucial in contemporary software development, have evolved with the integration of AI to boost programming efficiency and decision-making. Our focus on in-IDE Human-AI Experience delves into understanding how these AI tools reshape the software development process, impacting productivity and code quality. Our literature review aimed to comprehend the current state of in-IDE Human-AI Experience research, addressing a gap in understanding the nuanced interactions between programmers and AI assistants within IDEs. Analyzing 36 chosen papers, our study reveals three key research branches: Design, Impact, and Quality of Interaction. This paper sheds light on trends, challenges, and opportunities, underscoring the dynamic nature of software development. It serves as a guide for future research and development in this field, urging the community to explore three vital aspects of these interactions: designing task-specific user interfaces, fostering trust, and enhancing readability.","sentences":["IDEs, crucial in contemporary software development, have evolved with the integration of AI to boost programming efficiency and decision-making.","Our focus on in-IDE Human-AI Experience delves into understanding how these AI tools reshape the software development process, impacting productivity and code quality.","Our literature review aimed to comprehend the current state of in-IDE Human-AI Experience research, addressing a gap in understanding the nuanced interactions between programmers and AI assistants within IDEs.","Analyzing 36 chosen papers, our study reveals three key research branches: Design, Impact, and Quality of Interaction.","This paper sheds light on trends, challenges, and opportunities, underscoring the dynamic nature of software development.","It serves as a guide for future research and development in this field, urging the community to explore three vital aspects of these interactions: designing task-specific user interfaces, fostering trust, and enhancing readability."],"url":"http://arxiv.org/abs/2401.10739v1","category":"cs.SE"}
{"created":"2024-01-19 14:55:25","title":"Warehouse Problem with Multiple Vendors and Generalized Complementarity Constraints","abstract":"We study the warehouse problem, arising in the area of inventory management and production planning. Here, a merchant wants to decide an optimal trading policy that computes quantities of a single commodity to purchase, store and sell during each time period of a finite discrete time horizon. Motivated by recent applications in energy markets, we extend the models by Wolsey and Yaman (2018) and Bansal and G\\\"unl\\\"uk (2023) and consider markets with multiple vendors and a more general form of the complementarity constraints. We show that these extensions can capture various practical conditions such as surge pricing and discounted sales, ramp-up and ramp-down constraints and batch pricing. We analyze the extreme points of the underlying non-linear integer program and provide an algorithm that exactly solves the problem. Our algorithm runs in polynomial time under reasonable practical conditions. We also show that the absence of such conditions renders the problem NP-Hard.","sentences":["We study the warehouse problem, arising in the area of inventory management and production planning.","Here, a merchant wants to decide an optimal trading policy that computes quantities of a single commodity to purchase, store and sell during each time period of a finite discrete time horizon.","Motivated by recent applications in energy markets, we extend the models by Wolsey and Yaman (2018) and Bansal and G\\\"unl\\\"uk (2023) and consider markets with multiple vendors and a more general form of the complementarity constraints.","We show that these extensions can capture various practical conditions such as surge pricing and discounted sales, ramp-up and ramp-down constraints and batch pricing.","We analyze the extreme points of the underlying non-linear integer program and provide an algorithm that exactly solves the problem.","Our algorithm runs in polynomial time under reasonable practical conditions.","We also show that the absence of such conditions renders the problem NP-Hard."],"url":"http://arxiv.org/abs/2401.10738v1","category":"cs.DS"}
{"created":"2024-01-19 14:52:04","title":"A Survey and Comparative Analysis of Security Properties of CAN Authentication Protocols","abstract":"The large number of Electronic Control Units (ECUs) mounted on modern cars and their expansive communication capabilities create a substantial attack surface for potential exploitation. Despite the evolution of automotive technology, the continued use of the originally insecure Controller Area Network (CAN) bus leaves in-vehicle communications inherently non-secure. In response to the absence of standardized authentication protocols within the automotive domain, researchers propose diverse solutions, each with unique strengths and vulnerabilities. However, the continuous influx of new protocols and potential oversights in meeting security requirements and essential operational features further complicate the implementability of these protocols. This paper comprehensively reviews and compares the 15 most prominent authentication protocols for the CAN bus. Our analysis emphasizes their strengths and weaknesses, evaluating their alignment with critical security requirements for automotive authentication. Additionally, we evaluate protocols based on essential operational criteria that contribute to ease of implementation in predefined infrastructures, enhancing overall reliability and reducing the probability of successful attacks. Our study reveals a prevalent focus on defending against external attackers in existing protocols, exposing vulnerabilities to internal threats. Notably, authentication protocols employing hash chains, Mixed Message Authentication Codes, and asymmetric encryption techniques emerge as the most effective approaches. Through our comparative study, we classify the considered protocols based on their security attributes and suitability for implementation, providing valuable insights for future developments in the field.","sentences":["The large number of Electronic Control Units (ECUs) mounted on modern cars and their expansive communication capabilities create a substantial attack surface for potential exploitation.","Despite the evolution of automotive technology, the continued use of the originally insecure Controller Area Network (CAN) bus leaves in-vehicle communications inherently non-secure.","In response to the absence of standardized authentication protocols within the automotive domain, researchers propose diverse solutions, each with unique strengths and vulnerabilities.","However, the continuous influx of new protocols and potential oversights in meeting security requirements and essential operational features further complicate the implementability of these protocols.","This paper comprehensively reviews and compares the 15 most prominent authentication protocols for the CAN bus.","Our analysis emphasizes their strengths and weaknesses, evaluating their alignment with critical security requirements for automotive authentication.","Additionally, we evaluate protocols based on essential operational criteria that contribute to ease of implementation in predefined infrastructures, enhancing overall reliability and reducing the probability of successful attacks.","Our study reveals a prevalent focus on defending against external attackers in existing protocols, exposing vulnerabilities to internal threats.","Notably, authentication protocols employing hash chains, Mixed Message Authentication Codes, and asymmetric encryption techniques emerge as the most effective approaches.","Through our comparative study, we classify the considered protocols based on their security attributes and suitability for implementation, providing valuable insights for future developments in the field."],"url":"http://arxiv.org/abs/2401.10736v1","category":"cs.CR"}
{"created":"2024-01-19 14:51:12","title":"A Low-Frequency-Stable Higher-Order Spline-Based Integral Equation Method","abstract":"This contribution investigates the connection between Isogeometric Analysis and Integral Equation methods for full-wave electromagnetic problems. The proposed spline-based integral equation method allows for an exact representation of the model geometry described in terms of Non-Uniform Rational B-Splines without meshing. This is particularly useful when high accuracy is required or when meshing is cumbersome for instance during optimization of electric components. The Augmented Electric Field Integral Equation is adopted, so the low-frequency breakdown is avoided. The extension to higher-order basis functions is analyzed and the convergence rate discussed. The analogy with the Partial Element Equivalent Circuit method for the lowest-order case is established, allowing for a circuit interpretation while maintaining the exact representation of geometry even for coarse discretizations. Numerical experiments on academic and realistic test cases demonstrate the high accuracy of the proposed approach.","sentences":["This contribution investigates the connection between Isogeometric Analysis and Integral Equation methods for full-wave electromagnetic problems.","The proposed spline-based integral equation method allows for an exact representation of the model geometry described in terms of Non-Uniform Rational B-Splines without meshing.","This is particularly useful when high accuracy is required or when meshing is cumbersome for instance during optimization of electric components.","The Augmented Electric Field Integral Equation is adopted, so the low-frequency breakdown is avoided.","The extension to higher-order basis functions is analyzed and the convergence rate discussed.","The analogy with the Partial Element Equivalent Circuit method for the lowest-order case is established, allowing for a circuit interpretation while maintaining the exact representation of geometry even for coarse discretizations.","Numerical experiments on academic and realistic test cases demonstrate the high accuracy of the proposed approach."],"url":"http://arxiv.org/abs/2401.10735v1","category":"cs.CE"}
{"created":"2024-01-19 14:50:22","title":"Dynamic Q&A of Clinical Documents with Large Language Models","abstract":"Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.","sentences":["Electronic health records (EHRs) house crucial patient data in clinical notes.","As these notes grow in volume and complexity, manual extraction becomes challenging.","This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes.","Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes.","Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands.","Model optimization, including weight quantization, improves latency by approximately 48 times.","Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain.","Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making."],"url":"http://arxiv.org/abs/2401.10733v1","category":"cs.IR"}
{"created":"2024-01-19 14:49:42","title":"Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion","abstract":"Object detection in visible (RGB) and infrared (IR) images has been widely applied in recent years. Leveraging the complementary characteristics of RGB and IR images, the object detector provides reliable and robust object localization from day to night. Existing fusion strategies directly inject RGB and IR images into convolution neural networks, leading to inferior detection performance. Since the RGB and IR features have modality-specific noise, these strategies will worsen the fused features along with the propagation. Inspired by the mechanism of human brain processing multimodal information, this work introduces a new coarse-to-fine perspective to purify and fuse two modality features. Specifically, following this perspective, we design a Redundant Spectrum Removal module to coarsely remove interfering information within each modality and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called Removal and Selection Detector (RSDet). Extensive experiments on three RGB-IR object detection datasets verify the superior performance of our method.","sentences":["Object detection in visible (RGB) and infrared (IR) images has been widely applied in recent years.","Leveraging the complementary characteristics of RGB and IR images, the object detector provides reliable and robust object localization from day to night.","Existing fusion strategies directly inject RGB and IR images into convolution neural networks, leading to inferior detection performance.","Since the RGB and IR features have modality-specific noise, these strategies will worsen the fused features along with the propagation.","Inspired by the mechanism of human brain processing multimodal information, this work introduces a new coarse-to-fine perspective to purify and fuse two modality features.","Specifically, following this perspective, we design a Redundant Spectrum Removal module to coarsely remove interfering information within each modality and a Dynamic Feature Selection module to finely select the desired features for feature fusion.","To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called Removal and Selection Detector (RSDet).","Extensive experiments on three RGB-IR object detection datasets verify the superior performance of our method."],"url":"http://arxiv.org/abs/2401.10731v1","category":"cs.CV"}
{"created":"2024-01-19 14:46:14","title":"Network Design on Undirected Series-Parallel Graphs","abstract":"We study the single pair capacitated network design problem and the budget constrained max flow problem on undirected series-parallel graphs. These problems were well studied on directed series-parallel graphs, but little is known in the context of undirected graphs. The major difference between the cases is that the source and sink of the problem instance do not necessarily coincide with the terminals of the underlying series-parallel graph in the undirected case, thus creating certain complications. We provide pseudopolynomial time algorithms to solve both of the problems and provide an FPTAS for the budget constrained max flow problem. We also provide some extensions, arguing important cases when the problems are polynomial-time solvable, and describing a series-parallel gadget that captures an edge upgrade version of the problems.","sentences":["We study the single pair capacitated network design problem and the budget constrained max flow problem on undirected series-parallel graphs.","These problems were well studied on directed series-parallel graphs, but little is known in the context of undirected graphs.","The major difference between the cases is that the source and sink of the problem instance do not necessarily coincide with the terminals of the underlying series-parallel graph in the undirected case, thus creating certain complications.","We provide pseudopolynomial time algorithms to solve both of the problems and provide an FPTAS for the budget constrained max flow problem.","We also provide some extensions, arguing important cases when the problems are polynomial-time solvable, and describing a series-parallel gadget that captures an edge upgrade version of the problems."],"url":"http://arxiv.org/abs/2401.10729v1","category":"cs.DS"}
{"created":"2024-01-19 14:44:37","title":"Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning","abstract":"Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' perceiving tool-use ability is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the visual- or auditory-grounded instructions' information. Therefore, in this paper, we propose Tool-LMM, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featured by consisting of multi-modal input tools from HuggingFace. Another important feature of our dataset is that our dataset also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our LMM is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/Tool-LMM/Tool-LMM.","sentences":["Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems.","Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios.","However, the current LLMs' perceiving tool-use ability is limited to a single text query, which may result in ambiguity in understanding the users' real intentions.","LLMs are expected to eliminate that by perceiving the visual- or auditory-grounded instructions' information.","Therefore, in this paper, we propose Tool-LMM, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","To facilitate the evaluation of the model's capability, we collect a dataset featured by consisting of multi-modal input tools from HuggingFace.","Another important feature of our dataset is that our dataset also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query.","The experiments reveal that our LMM is capable of recommending appropriate tools for multi-modal instructions.","Codes and data are available at https://github.com/Tool-LMM/Tool-LMM."],"url":"http://arxiv.org/abs/2401.10727v1","category":"cs.CV"}
{"created":"2024-01-19 14:43:04","title":"Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response","abstract":"This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while the second approach carefully considers building occupants' thermal comfort profiles, while maximizing flexibility in case of existence of dedicated smart meters to the HVAC systems. Through the application of data-driven techniques and encompassing case studies from both industrial and residential buildings, this paper not only unveils pivotal opportunities for aggregators in the balancing and emerging flexibility markets but also successfully develops end-to-end practical tools for aggregators. Furthermore, the efficacy of this tool is validated through detailed case studies, substantiating its operational capability and contributing to the evolution of a resilient and efficient energy system.","sentences":["This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES).","Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns.","Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis.","The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while the second approach carefully considers building occupants' thermal comfort profiles, while maximizing flexibility in case of existence of dedicated smart meters to the HVAC systems.","Through the application of data-driven techniques and encompassing case studies from both industrial and residential buildings, this paper not only unveils pivotal opportunities for aggregators in the balancing and emerging flexibility markets but also successfully develops end-to-end practical tools for aggregators.","Furthermore, the efficacy of this tool is validated through detailed case studies, substantiating its operational capability and contributing to the evolution of a resilient and efficient energy system."],"url":"http://arxiv.org/abs/2401.10726v1","category":"eess.SY"}
{"created":"2024-01-19 14:42:08","title":"Proceedings 14th International Conference on Automated Deduction in Geometry","abstract":"ADG is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. The conference is held every two years. The previous editions of ADG were held in Hagenberg in 2021 (online, postponed from 2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996.   The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22, 2023. This edition of ADG had an additional special focus topic, Deduction in Education.   Invited Speakers: Julien Narboux, University of Strasbourg, France \"Formalisation, arithmetization and automatisation of geometry\"; Filip Mari\\'c, University of Belgrade, Serbia, \"Automatization, formalization and visualization of hyperbolic geometry\"; Zlatan Magajna, University of Ljubljana, Slovenia, \"Workshop OK Geometry\"","sentences":["ADG is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction.","The conference is held every two years.","The previous editions of ADG were held in Hagenberg in 2021 (online, postponed from 2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996.   ","The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22, 2023.","This edition of ADG had an additional special focus topic, Deduction in Education.   ","Invited Speakers: Julien Narboux, University of Strasbourg, France \"Formalisation, arithmetization and automatisation of geometry\"; Filip Mari\\'c, University of Belgrade, Serbia, \"Automatization, formalization and visualization of hyperbolic geometry\"; Zlatan Magajna, University of Ljubljana, Slovenia, \"Workshop OK Geometry\""],"url":"http://arxiv.org/abs/2401.10725v1","category":"cs.LO"}
{"created":"2024-01-19 14:36:01","title":"Real-Time Zero-Day Intrusion Detection System for Automotive Controller Area Network on FPGAs","abstract":"Increasing automation in vehicles enabled by increased connectivity to the outside world has exposed vulnerabilities in previously siloed automotive networks like controller area networks (CAN). Attributes of CAN such as broadcast-based communication among electronic control units (ECUs) that lowered deployment costs are now being exploited to carry out active injection attacks like denial of service (DoS), fuzzing, and spoofing attacks. Research literature has proposed multiple supervised machine learning models deployed as Intrusion detection systems (IDSs) to detect such malicious activity; however, these are largely limited to identifying previously known attack vectors. With the ever-increasing complexity of active injection attacks, detecting zero-day (novel) attacks in these networks in real-time (to prevent propagation) becomes a problem of particular interest. This paper presents an unsupervised-learning-based convolutional autoencoder architecture for detecting zero-day attacks, which is trained only on benign (attack-free) CAN messages. We quantise the model using Vitis-AI tools from AMD/Xilinx targeting a resource-constrained Zynq Ultrascale platform as our IDS-ECU system for integration. The proposed model successfully achieves equal or higher classification accuracy (> 99.5%) on unseen DoS, fuzzing, and spoofing attacks from a publicly available attack dataset when compared to the state-of-the-art unsupervised learning-based IDSs. Additionally, by cleverly overlapping IDS operation on a window of CAN messages with the reception, the model is able to meet line-rate detection (0.43 ms per window) of high-speed CAN, which when coupled with the low energy consumption per inference, makes this architecture ideally suited for detecting zero-day attacks on critical CAN networks.","sentences":["Increasing automation in vehicles enabled by increased connectivity to the outside world has exposed vulnerabilities in previously siloed automotive networks like controller area networks (CAN).","Attributes of CAN such as broadcast-based communication among electronic control units (ECUs) that lowered deployment costs are now being exploited to carry out active injection attacks like denial of service (DoS), fuzzing, and spoofing attacks.","Research literature has proposed multiple supervised machine learning models deployed as Intrusion detection systems (IDSs) to detect such malicious activity; however, these are largely limited to identifying previously known attack vectors.","With the ever-increasing complexity of active injection attacks, detecting zero-day (novel) attacks in these networks in real-time (to prevent propagation) becomes a problem of particular interest.","This paper presents an unsupervised-learning-based convolutional autoencoder architecture for detecting zero-day attacks, which is trained only on benign (attack-free) CAN messages.","We quantise the model using Vitis-AI tools from AMD/Xilinx targeting a resource-constrained Zynq Ultrascale platform as our IDS-ECU system for integration.","The proposed model successfully achieves equal or higher classification accuracy (> 99.5%) on unseen DoS, fuzzing, and spoofing attacks from a publicly available attack dataset when compared to the state-of-the-art unsupervised learning-based IDSs.","Additionally, by cleverly overlapping IDS operation on a window of CAN messages with the reception, the model is able to meet line-rate detection (0.43 ms per window) of high-speed CAN, which when coupled with the low energy consumption per inference, makes this architecture ideally suited for detecting zero-day attacks on critical CAN networks."],"url":"http://arxiv.org/abs/2401.10724v1","category":"cs.CR"}
{"created":"2024-01-19 14:35:35","title":"Collective rovibronic dynamics of a diatomic gas coupled by cavity","abstract":"We consider an ensemble of homonuclear diatomic molecules coupled to the two polarization directions of a Fabry-P\\'erot cavity via fully quantum simulations. Accompanied by analytical results, we identify a coupling mechanism mediated simultaneously by the two perpendicular polarizations, and inducing polaritonic relaxation towards molecular rotations. This mechanism is related to the concept of light-induced conical intersections (LICI). However, unlike LICIs, these non-adiabatic pathways are of collective nature, since they depend on the \\emph{relative} intermolecular orientation of all electronic transition dipoles in the polarization plane. Notably, this rotational mechanism directly couples the bright upper and lower polaritonic states, and it stays in direct competition with the collective relaxation towards dark-states. Our simulations indicate that the molecular rotational dynamics in gas-phase cavity-coupled systems can serve as a novel probe for non-radiative polaritonic decay towards the dark-states manifold.","sentences":["We consider an ensemble of homonuclear diatomic molecules coupled to the two polarization directions of a Fabry-P\\'erot cavity via fully quantum simulations.","Accompanied by analytical results, we identify a coupling mechanism mediated simultaneously by the two perpendicular polarizations, and inducing polaritonic relaxation towards molecular rotations.","This mechanism is related to the concept of light-induced conical intersections (LICI).","However, unlike LICIs, these non-adiabatic pathways are of collective nature, since they depend on the \\emph{relative} intermolecular orientation of all electronic transition dipoles in the polarization plane.","Notably, this rotational mechanism directly couples the bright upper and lower polaritonic states, and it stays in direct competition with the collective relaxation towards dark-states.","Our simulations indicate that the molecular rotational dynamics in gas-phase cavity-coupled systems can serve as a novel probe for non-radiative polaritonic decay towards the dark-states manifold."],"url":"http://arxiv.org/abs/2401.10723v1","category":"physics.chem-ph"}
{"created":"2024-01-19 14:32:50","title":"Generative Model for Constructing Reaction Path from Initial to Final States","abstract":"Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation. Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem. Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways. The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure. This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state. The application of this method extends to complex reaction pathways illustrated by organic reactions. Training was executed on the Transition1x dataset, an organic reaction pathway dataset. The results revealed generation of reactions that bore substantial similarities with the corresponding test data. The method's flexibility allows for reactions to be generated either to conform to predetermined conditions or in a randomized manner.","sentences":["Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation.","Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem.","Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways.","The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure.","This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state.","The application of this method extends to complex reaction pathways illustrated by organic reactions.","Training was executed on the Transition1x dataset, an organic reaction pathway dataset.","The results revealed generation of reactions that bore substantial similarities with the corresponding test data.","The method's flexibility allows for reactions to be generated either to conform to predetermined conditions or in a randomized manner."],"url":"http://arxiv.org/abs/2401.10721v1","category":"physics.comp-ph"}
{"created":"2024-01-19 14:27:44","title":"Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models","abstract":"Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text. This approach, however, overlooks the unambiguous structures inherent in programming languages. In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures. Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs. Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks. The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating program structures with plain-text representation even when working with backbone models that have not been pre-trained with structures.","sentences":["Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text.","This approach, however, overlooks the unambiguous structures inherent in programming languages.","In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures.","Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs.","Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks.","The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating program structures with plain-text representation even when working with backbone models that have not been pre-trained with structures."],"url":"http://arxiv.org/abs/2401.10716v1","category":"cs.CL"}
{"created":"2024-01-19 14:25:45","title":"Entanglement distribution in Bhabha scattering with entangled spectator particle","abstract":"We analyze how entanglement is generated and distributed in a Bhabha scattering process $(e^-e^+\\rightarrow e^-e^+)$ at tree level. In our setup an electron $A$ scatters with a positron $B$, which is initially entangled with another electron $C$ (spectator), that does not participate directly to the process. We find that the QED scattering generates and distributes entanglement in a non-trivial way among the three particles: the correlations in the output channels $AB$, $AC$ and $BC$ are studied in detail as functions of the scattering parameters and of the initial entanglement weight. Although derived in a specific case, our results exhibit some general features of other similar QED scattering processes, for which the extension of the present analysis is straightforward.","sentences":["We analyze how entanglement is generated and distributed in a Bhabha scattering process $(e^-e^+\\rightarrow e^-e^+)$ at tree level.","In our setup an electron $A$ scatters with a positron $B$, which is initially entangled with another electron $C$ (spectator), that does not participate directly to the process.","We find that the QED scattering generates and distributes entanglement in a non-trivial way among the three particles: the correlations in the output channels $AB$, $AC$ and $BC$ are studied in detail as functions of the scattering parameters and of the initial entanglement weight.","Although derived in a specific case, our results exhibit some general features of other similar QED scattering processes, for which the extension of the present analysis is straightforward."],"url":"http://arxiv.org/abs/2401.10715v1","category":"quant-ph"}
{"created":"2024-01-19 14:22:29","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge","abstract":"With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model. Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers. Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers. Experimental results show that, compared with state-of-the-art methods, our Q&A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA.","sentences":["With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever.","However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically.","In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer.","We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts.","We call the proposed method Q&A Prompts.","Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model.","Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers.","Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers.","Experimental results show that, compared with state-of-the-art methods, our Q&A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA."],"url":"http://arxiv.org/abs/2401.10712v1","category":"cs.CV"}
{"created":"2024-01-19 14:21:46","title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering","abstract":"Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure of the video, and sample question-critical frames as positive moments to be the visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks verify the effectiveness of our framework, and we achieve substantial improvements compared to previous state-of-the-art methods.","sentences":["Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos.","Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues.","Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets.","In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs.","Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels.","With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module.","GCG learns multiple Gaussian functions to characterize the temporal structure of the video, and sample question-critical frames as positive moments to be the visual inputs of LMMs.","Extensive experiments on several VideoQA benchmarks verify the effectiveness of our framework, and we achieve substantial improvements compared to previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2401.10711v1","category":"cs.CV"}
{"created":"2024-01-19 14:18:32","title":"Classification with neural networks with quadratic decision functions","abstract":"Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one. They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.","sentences":["Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one.","They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc.","In this paper we investigate the use of such ansatz functions for classification.","In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies.","We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively."],"url":"http://arxiv.org/abs/2401.10710v1","category":"cs.LG"}
{"created":"2024-01-19 14:14:21","title":"Demonstration of Cooperative Transport Interface using open-source 5G OpenRAN and virtualised PON network","abstract":"We demonstrate a real-time, converged 5G-PON through the Cooperative Transport Interface, synchronising 5G and PON-DBA upstream schedulers. This innovative approach, implemented using 5G and PON open network implementations, significantly enhances network resource allocation, reducing latency.","sentences":["We demonstrate a real-time, converged 5G-PON through the Cooperative Transport Interface, synchronising 5G and PON-DBA upstream schedulers.","This innovative approach, implemented using 5G and PON open network implementations, significantly enhances network resource allocation, reducing latency."],"url":"http://arxiv.org/abs/2401.10708v1","category":"cs.NI"}
{"created":"2024-01-19 14:08:08","title":"DRAT Proofs of Unsatisfiability for SAT Modulo Monotonic Theories","abstract":"Generating proofs of unsatisfiability is a valuable capability of most SAT solvers, and is an active area of research for SMT solvers. This paper introduces the first method to efficiently generate proofs of unsatisfiability specifically for an important subset of SMT: SAT Modulo Monotonic Theories (SMMT), which includes many useful finite-domain theories (e.g., bit vectors and many graph-theoretic properties) and is used in production at Amazon Web Services. Our method uses propositional definitions of the theory predicates, from which it generates compact Horn approximations of the definitions, which lead to efficient DRAT proofs, leveraging the large investment the SAT community has made in DRAT. In experiments on practical SMMT problems, our proof generation overhead is minimal (7.41% geometric mean slowdown, 28.8% worst-case), and we can generate and check proofs for many problems that were previously intractable.","sentences":["Generating proofs of unsatisfiability is a valuable capability of most SAT solvers, and is an active area of research for SMT solvers.","This paper introduces the first method to efficiently generate proofs of unsatisfiability specifically for an important subset of SMT: SAT Modulo Monotonic Theories (SMMT), which includes many useful finite-domain theories (e.g., bit vectors and many graph-theoretic properties) and is used in production at Amazon Web Services.","Our method uses propositional definitions of the theory predicates, from which it generates compact Horn approximations of the definitions, which lead to efficient DRAT proofs, leveraging the large investment the SAT community has made in DRAT.","In experiments on practical SMMT problems, our proof generation overhead is minimal (7.41% geometric mean slowdown, 28.8% worst-case), and we can generate and check proofs for many problems that were previously intractable."],"url":"http://arxiv.org/abs/2401.10703v1","category":"cs.LO"}
{"created":"2024-01-19 14:06:29","title":"G.O.G: A Versatile Gripper-On-Gripper Design for Bimanual Cloth Manipulation with a Single Robotic Arm","abstract":"The manipulation of garments poses research challenges due to their deformable nature and the extensive variability in shapes and sizes. Despite numerous attempts by researchers to address these via approaches involving robot perception and control, there has been a relatively limited interest in resolving it through the co-development of robot hardware. Consequently, the majority of studies employ off-the-shelf grippers in conjunction with dual robot arms to enable bimanual manipulation and high dexterity. However, this dual-arm system increases the overall cost of the robotic system as well as its control complexity in order to tackle robot collisions and other robot coordination issues. As an alternative approach, we propose to enable bimanual cloth manipulation using a single robot arm via novel end effector design -- sharing dexterity skills between manipulator and gripper rather than relying entirely on robot arm coordination. To this end, we introduce a new gripper, called G.O.G., based on a gripper-on-gripper structure where the first gripper independently regulates the span, up to 500mm, between its fingers which are in turn also grippers. These finger grippers consist of a variable friction module that enables two grasping modes: firm and sliding grasps. Household item and cloth object benchmarks are employed to evaluate the performance of the proposed design, encompassing both experiments on the gripper design itself and on cloth manipulation. Experimental results demonstrate the potential of the introduced ideas to undertake a range of bimanual cloth manipulation tasks with a single robot arm. Supplementary material is available at https://sites.google.com/view/gripperongripper.","sentences":["The manipulation of garments poses research challenges due to their deformable nature and the extensive variability in shapes and sizes.","Despite numerous attempts by researchers to address these via approaches involving robot perception and control, there has been a relatively limited interest in resolving it through the co-development of robot hardware.","Consequently, the majority of studies employ off-the-shelf grippers in conjunction with dual robot arms to enable bimanual manipulation and high dexterity.","However, this dual-arm system increases the overall cost of the robotic system as well as its control complexity in order to tackle robot collisions and other robot coordination issues.","As an alternative approach, we propose to enable bimanual cloth manipulation using a single robot arm via novel end effector design -- sharing dexterity skills between manipulator and gripper rather than relying entirely on robot arm coordination.","To this end, we introduce a new gripper, called G.O.G., based on a gripper-on-gripper structure where the first gripper independently regulates the span, up to 500mm, between its fingers which are in turn also grippers.","These finger grippers consist of a variable friction module that enables two grasping modes: firm and sliding grasps.","Household item and cloth object benchmarks are employed to evaluate the performance of the proposed design, encompassing both experiments on the gripper design itself and on cloth manipulation.","Experimental results demonstrate the potential of the introduced ideas to undertake a range of bimanual cloth manipulation tasks with a single robot arm.","Supplementary material is available at https://sites.google.com/view/gripperongripper."],"url":"http://arxiv.org/abs/2401.10702v1","category":"cs.RO"}
{"created":"2024-01-19 14:05:09","title":"Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model","abstract":"Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning. Thus, we propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training. We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks.","sentences":["Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning.","Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined.","This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios.","An alternative is to enforce the hard constraint of zero violation.","However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets.","Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset.","This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region.","Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability.","In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning.","Thus, we propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training.","We compare FISOR against baselines on DSRL benchmark for safe offline RL.","Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks."],"url":"http://arxiv.org/abs/2401.10700v1","category":"cs.LG"}
{"created":"2024-01-19 14:03:33","title":"Navigating Expertise in Configurable Software Systems through the Maze of Variability","abstract":"The understanding of source code in large-scale software systems poses a challenge for developers. The role of expertise in source code becomes critical for identifying developers accountable for substantial changes. However, in the context of configurable software systems (CSS) using pre-processing and conditional compilation, conventional expertise metrics may encounter limitations due to the non-alignment of variability implementation with the natural module structure. This early research study investigates the distribution of development efforts in CSS, specifically focusing on variable and mandatory code. It also examines the engagement of designated experts with variable code in their assigned files. The findings provide insights into task allocation dynamics and raise questions about the applicability of existing metrics, laying the groundwork for alternative approaches to assess developer expertise in handling variable code. This research aims to contribute to a comprehensive understanding of challenges within CSS, marking initial steps toward advancing the evaluation of expertise in this context.","sentences":["The understanding of source code in large-scale software systems poses a challenge for developers.","The role of expertise in source code becomes critical for identifying developers accountable for substantial changes.","However, in the context of configurable software systems (CSS) using pre-processing and conditional compilation, conventional expertise metrics may encounter limitations due to the non-alignment of variability implementation with the natural module structure.","This early research study investigates the distribution of development efforts in CSS, specifically focusing on variable and mandatory code.","It also examines the engagement of designated experts with variable code in their assigned files.","The findings provide insights into task allocation dynamics and raise questions about the applicability of existing metrics, laying the groundwork for alternative approaches to assess developer expertise in handling variable code.","This research aims to contribute to a comprehensive understanding of challenges within CSS, marking initial steps toward advancing the evaluation of expertise in this context."],"url":"http://arxiv.org/abs/2401.10699v1","category":"cs.SE"}
{"created":"2024-01-19 14:01:51","title":"Reconfigurable entanglement distribution network based on pump management of spontaneous four-wave mixing source","abstract":"Leveraging the unique properties of quantum entanglement, quantum entanglement distribution networks support multiple quantum information applications and are essential to the development of quantum networks. However, its practical implementation poses significant challenges to network scalability and flexibility. In this work, we propose a novel reconfigurable entanglement distribution network based on tunable multi-pump excitation of a spontaneous four-wave mixing (SFWM) source and a time-sharing method. We characterize the two-photon correlation under different pump conditions to demonstrate the effect of pump degenerate and pump non-degenerate SFWM processes on the two-photon correlation, and its tunability. Then as a benchmark application, a 10-user fully-connected quantum key distribution (QKD) network is established in a time-sharing way with triple pump lights. Each user receives one frequency channel thus it shows a linear scaling between the number of frequency channels and the user number in despite of the network topology. Our results thus provide a promising networking scheme for large-scale entanglement distribution networks owing to its scalability, functionality, and reconfigurability.","sentences":["Leveraging the unique properties of quantum entanglement, quantum entanglement distribution networks support multiple quantum information applications and are essential to the development of quantum networks.","However, its practical implementation poses significant challenges to network scalability and flexibility.","In this work, we propose a novel reconfigurable entanglement distribution network based on tunable multi-pump excitation of a spontaneous four-wave mixing (SFWM) source and a time-sharing method.","We characterize the two-photon correlation under different pump conditions to demonstrate the effect of pump degenerate and pump non-degenerate SFWM processes on the two-photon correlation, and its tunability.","Then as a benchmark application, a 10-user fully-connected quantum key distribution (QKD) network is established in a time-sharing way with triple pump lights.","Each user receives one frequency channel thus it shows a linear scaling between the number of frequency channels and the user number in despite of the network topology.","Our results thus provide a promising networking scheme for large-scale entanglement distribution networks owing to its scalability, functionality, and reconfigurability."],"url":"http://arxiv.org/abs/2401.10697v1","category":"quant-ph"}
{"created":"2024-01-19 14:00:19","title":"LangBridge: Multilingual Reasoning Without Multilingual Supervision","abstract":"We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.","sentences":["We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision.","LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2).","LangBridge connects the two models by introducing minimal trainable parameters between them.","Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning.","Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations.","We publicly release our code and models."],"url":"http://arxiv.org/abs/2401.10695v1","category":"cs.CL"}
{"created":"2024-01-19 13:47:25","title":"Large violation of Leggett-Garg inequalities with coherent-state projectors for a harmonic oscillator and chiral scalar field","abstract":"We investigate violations of Leggett-Garg inequalities (LGIs) for a harmonic oscillator and a (1+1)-dimensional chiral scalar field with coherent-state projectors, which is equivalent to a heterodyne-type measurement scheme. For the harmonic oscillator, we found that the vacuum and thermal states violated the LGIs by evaluating the two-time quasi-probability distribution function. In particular, we demonstrate that the value of the two-time quasi-probability reaches -0.123 for a squeezed coherent-state projector, which is equivalent to 98% of the L\\\"uders bound corresponding to the maximal violation of the LGIs. We also find a violation of the LGIs for the local mode of a quantum chiral scalar field by constructing a coherent-state projector similar to the harmonic oscillator case. In contrast to the harmonic oscillator, the periodicity in the time direction of the quasi-probability disappears, which is related to the existence of quantum entanglement between the local mode and its complementary degrees of freedom.","sentences":["We investigate violations of Leggett-Garg inequalities (LGIs) for a harmonic oscillator and a (1+1)-dimensional chiral scalar field with coherent-state projectors, which is equivalent to a heterodyne-type measurement scheme.","For the harmonic oscillator, we found that the vacuum and thermal states violated the LGIs by evaluating the two-time quasi-probability distribution function.","In particular, we demonstrate that the value of the two-time quasi-probability reaches -0.123 for a squeezed coherent-state projector, which is equivalent to 98% of the L\\\"uders bound corresponding to the maximal violation of the LGIs.","We also find a violation of the LGIs for the local mode of a quantum chiral scalar field by constructing a coherent-state projector similar to the harmonic oscillator case.","In contrast to the harmonic oscillator, the periodicity in the time direction of the quasi-probability disappears, which is related to the existence of quantum entanglement between the local mode and its complementary degrees of freedom."],"url":"http://arxiv.org/abs/2401.10692v1","category":"quant-ph"}
{"created":"2024-01-19 13:43:09","title":"Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors","abstract":"espite being widely used in network intrusion detection systems (NIDSs), machine learning (ML) has proven to be highly vulnerable to adversarial attacks. White-box and black-box adversarial attacks of NIDS have been explored in several studies. However, white-box attacks unrealistically assume that the attackers have full knowledge of the target NIDSs. Meanwhile, existing black-box attacks can not achieve high attack success rate due to the weak adversarial transferability between models (e.g., neural networks and tree models). Additionally, neither of them explains why adversarial examples exist and why they can transfer across models. To address these challenges, this paper introduces ETA, an Explainable Transfer-based Black-Box Adversarial Attack framework. ETA aims to achieve two primary objectives: 1) create transferable adversarial examples applicable to various ML models and 2) provide insights into the existence of adversarial examples and their transferability within NIDSs. Specifically, we first provide a general transfer-based adversarial attack method applicable across the entire ML space. Following that, we exploit a unique insight based on cooperative game theory and perturbation interpretations to explain adversarial examples and adversarial transferability. On this basis, we propose an Important-Sensitive Feature Selection (ISFS) method to guide the search for adversarial examples, achieving stronger transferability and ensuring traffic-space constraints.","sentences":["espite being widely used in network intrusion detection systems (NIDSs), machine learning (ML) has proven to be highly vulnerable to adversarial attacks.","White-box and black-box adversarial attacks of NIDS have been explored in several studies.","However, white-box attacks unrealistically assume that the attackers have full knowledge of the target NIDSs.","Meanwhile, existing black-box attacks can not achieve high attack success rate due to the weak adversarial transferability between models (e.g., neural networks and tree models).","Additionally, neither of them explains why adversarial examples exist and why they can transfer across models.","To address these challenges, this paper introduces ETA, an Explainable Transfer-based Black-Box Adversarial Attack framework.","ETA aims to achieve two primary objectives: 1) create transferable adversarial examples applicable to various ML models and 2) provide insights into the existence of adversarial examples and their transferability within NIDSs.","Specifically, we first provide a general transfer-based adversarial attack method applicable across the entire ML space.","Following that, we exploit a unique insight based on cooperative game theory and perturbation interpretations to explain adversarial examples and adversarial transferability.","On this basis, we propose an Important-Sensitive Feature Selection (ISFS) method to guide the search for adversarial examples, achieving stronger transferability and ensuring traffic-space constraints."],"url":"http://arxiv.org/abs/2401.10691v1","category":"cs.CR"}
{"created":"2024-01-19 13:41:08","title":"Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models","abstract":"Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models and datasets. We also prove the adequateness of EAUC by using naive de-biasing corrections to demonstrate that a lower model bias correlates with a lower EAUC and vice-versa. This work contributes a bias-aware evaluation of dyadic regression models to avoid potential unfairness and risks in critical real-world applications of such systems.","sentences":["Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology).","In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases.","We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models and datasets.","We also prove the adequateness of EAUC by using naive de-biasing corrections to demonstrate that a lower model bias correlates with a lower EAUC and vice-versa.","This work contributes a bias-aware evaluation of dyadic regression models to avoid potential unfairness and risks in critical real-world applications of such systems."],"url":"http://arxiv.org/abs/2401.10690v1","category":"cs.LG"}
{"created":"2024-01-19 13:39:05","title":"A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid FPGAs","abstract":"Rising connectivity in vehicles is enabling new capabilities like connected autonomous driving and advanced driver assistance systems (ADAS) for improving the safety and reliability of next-generation vehicles. This increased access to in-vehicle functions compromises critical capabilities that use legacy invehicle networks like Controller Area Network (CAN), which has no inherent security or authentication mechanism. Intrusion detection and mitigation approaches, particularly using machine learning models, have shown promising results in detecting multiple attack vectors in CAN through their ability to generalise to new vectors. However, most deployments require dedicated computing units like GPUs to perform line-rate detection, consuming much higher power. In this paper, we present a lightweight multi-attack quantised machine learning model that is deployed using Xilinx's Deep Learning Processing Unit IP on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the public CAN Intrusion Detection dataset. The quantised model detects denial of service and fuzzing attacks with an accuracy of above 99 % and a false positive rate of 0.07%, which are comparable to the state-of-the-art techniques in the literature. The Intrusion Detection System (IDS) execution consumes just 2.0 W with software tasks running on the ECU and achieves a 25 % reduction in per-message processing latency over the state-of-the-art implementations. This deployment allows the ECU function to coexist with the IDS with minimal changes to the tasks, making it ideal for real-time IDS in in-vehicle systems.","sentences":["Rising connectivity in vehicles is enabling new capabilities like connected autonomous driving and advanced driver assistance systems (ADAS) for improving the safety and reliability of next-generation vehicles.","This increased access to in-vehicle functions compromises critical capabilities that use legacy invehicle networks like Controller Area Network (CAN), which has no inherent security or authentication mechanism.","Intrusion detection and mitigation approaches, particularly using machine learning models, have shown promising results in detecting multiple attack vectors in CAN through their ability to generalise to new vectors.","However, most deployments require dedicated computing units like GPUs to perform line-rate detection, consuming much higher power.","In this paper, we present a lightweight multi-attack quantised machine learning model that is deployed using Xilinx's Deep Learning Processing Unit IP on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the public CAN Intrusion Detection dataset.","The quantised model detects denial of service and fuzzing attacks with an accuracy of above 99 % and a false positive rate of 0.07%, which are comparable to the state-of-the-art techniques in the literature.","The Intrusion Detection System (IDS) execution consumes just 2.0 W with software tasks running on the ECU and achieves a 25 % reduction in per-message processing latency over the state-of-the-art implementations.","This deployment allows the ECU function to coexist with the IDS with minimal changes to the tasks, making it ideal for real-time IDS in in-vehicle systems."],"url":"http://arxiv.org/abs/2401.10689v1","category":"cs.CR"}
{"created":"2024-01-19 13:34:49","title":"Unraveling codes: fast, robust, beyond-bound error correction for DRAM","abstract":"Generalized Reed-Solomon (RS) codes are a common choice for efficient, reliable error correction in memory and communications systems. These codes add $2t$ extra parity symbols to a block of memory, and can efficiently and reliably correct up to $t$ symbol errors in that block. Decoding is possible beyond this bound, but it is imperfectly reliable and often computationally expensive. Beyond-bound decoding is an important problem to solve for error-correcting Dynamic Random Access Memory (DRAM). These memories are often designed so that each access touches two extra memory devices, so that a failure in any one device can be corrected. But system architectures increasingly require DRAM to store metadata in addition to user data. When the metadata replaces parity data, a single-device failure is then beyond-bound. An error-correction system can either protect each access with a single RS code, or divide it into several segments protected with a shorter code, usually in an Interleaved Reed-Solomon (IRS) configuration. The full-block RS approach is more reliable, both at correcting errors and at preventing silent data corruption (SDC). The IRS option is faster, and is especially efficient at beyond-bound correction of single- or double-device failures. Here we describe a new family of \"unraveling\" Reed-Solomon codes that bridges the gap between these options. Our codes are full-block generalized RS codes, but they can also be decoded using an IRS decoder. As a result, they combine the speed and beyond-bound correction capabilities of interleaved codes with the robustness of full-block codes, including the ability of the latter to reliably correct failures across multiple devices. We show that unraveling codes are an especially good fit for high-reliability DRAM error correction.","sentences":["Generalized Reed-Solomon (RS) codes are a common choice for efficient, reliable error correction in memory and communications systems.","These codes add $2t$ extra parity symbols to a block of memory, and can efficiently and reliably correct up to $t$ symbol errors in that block.","Decoding is possible beyond this bound, but it is imperfectly reliable and often computationally expensive.","Beyond-bound decoding is an important problem to solve for error-correcting Dynamic Random Access Memory (DRAM).","These memories are often designed so that each access touches two extra memory devices, so that a failure in any one device can be corrected.","But system architectures increasingly require DRAM to store metadata in addition to user data.","When the metadata replaces parity data, a single-device failure is then beyond-bound.","An error-correction system can either protect each access with a single RS code, or divide it into several segments protected with a shorter code, usually in an Interleaved Reed-Solomon (IRS) configuration.","The full-block RS approach is more reliable, both at correcting errors and at preventing silent data corruption (SDC).","The IRS option is faster, and is especially efficient at beyond-bound correction of single- or double-device failures.","Here we describe a new family of \"unraveling\" Reed-Solomon codes that bridges the gap between these options.","Our codes are full-block generalized RS codes, but they can also be decoded using an IRS decoder.","As a result, they combine the speed and beyond-bound correction capabilities of interleaved codes with the robustness of full-block codes, including the ability of the latter to reliably correct failures across multiple devices.","We show that unraveling codes are an especially good fit for high-reliability DRAM error correction."],"url":"http://arxiv.org/abs/2401.10688v1","category":"cs.IT"}
{"created":"2024-01-19 13:33:23","title":"Manipulating Sparse Double Descent","abstract":"This paper investigates the double descent phenomenon in two-layer neural networks, focusing on the role of L1 regularization and representation dimensions. It explores an alternative double descent phenomenon, named sparse double descent. The study emphasizes the complex relationship between model complexity, sparsity, and generalization, and suggests further research into more diverse models and datasets. The findings contribute to a deeper understanding of neural network training and optimization.","sentences":["This paper investigates the double descent phenomenon in two-layer neural networks, focusing on the role of L1 regularization and representation dimensions.","It explores an alternative double descent phenomenon, named sparse double descent.","The study emphasizes the complex relationship between model complexity, sparsity, and generalization, and suggests further research into more diverse models and datasets.","The findings contribute to a deeper understanding of neural network training and optimization."],"url":"http://arxiv.org/abs/2401.10686v1","category":"cs.LG"}
{"created":"2024-01-19 13:32:55","title":"Towards End-to-End GPS Localization with Neural Pseudorange Correction","abstract":"Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.","sentences":["Pseudorange errors are the root cause of localization inaccuracy in GPS.","Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels.","Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states.","The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet.","The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods."],"url":"http://arxiv.org/abs/2401.10685v1","category":"cs.LG"}
{"created":"2024-01-19 13:31:29","title":"QuantumReservoirPy: A Software Package for Time Series Prediction","abstract":"In recent times, quantum reservoir computing has emerged as a potential resource for time series prediction. Hence, there is a need for a flexible framework to test quantum circuits as nonlinear dynamical systems. We have developed a software package to allow for quantum reservoirs to fit a common structure, similar to that of reservoirpy which is advertised as \"a python tool designed to easily define, train and use (classical) reservoir computing architectures\". Our package results in simplified development and logical methods of comparison between quantum reservoir architectures. Examples are provided to demonstrate the resulting simplicity of executing quantum reservoir computing using our software package.","sentences":["In recent times, quantum reservoir computing has emerged as a potential resource for time series prediction.","Hence, there is a need for a flexible framework to test quantum circuits as nonlinear dynamical systems.","We have developed a software package to allow for quantum reservoirs to fit a common structure, similar to that of reservoirpy which is advertised as \"a python tool designed to easily define, train and use (classical) reservoir computing architectures\".","Our package results in simplified development and logical methods of comparison between quantum reservoir architectures.","Examples are provided to demonstrate the resulting simplicity of executing quantum reservoir computing using our software package."],"url":"http://arxiv.org/abs/2401.10683v1","category":"quant-ph"}
{"created":"2024-01-19 13:28:42","title":"Discrete-coordinate crypto-Hermitian quantum system controlled by time-dependent Robin boundary conditions","abstract":"Non-stationary version of unitary quantum mechanics formulated in non-Hermitian (or, more precisely, in hiddenly Hermitian) interaction-picture representation is illustrated via a preselected elementary $N$ by $N$ matrix Hamiltonian $H(t)$ mimicking a 1D-box system with physics controlled by general time-dependent boundary conditions. The model is presented as analytically solvable at $N=2$. {\\it Expressis verbis} this means that for both of the underlying Heisenberg and Schr\\\"{o}dinger evolution equations the generators (i.e., in our notation, the respective operators $\\Sigma(t)$ and $G(t)$) become available in closed form. The key message is that contrary to the conventional beliefs and in spite of the unitarity of the evolution of the system, neither its ``Heisenbergian ``Hamiltonian'' $\\Sigma(t)$ nor its ``Schr\\\"{o}dingerian ``Hamiltonian'' $G(t)$ possesses a real spectrum (or even some spectrum containing the conjugate pairs of complex eigenvalues).","sentences":["Non-stationary version of unitary quantum mechanics formulated in non-Hermitian (or, more precisely, in hiddenly Hermitian) interaction-picture representation is illustrated via a preselected elementary $N$ by $N$ matrix Hamiltonian $H(t)$ mimicking a 1D-box system with physics controlled by general time-dependent boundary conditions.","The model is presented as analytically solvable at $N=2$. {\\it Expressis verbis} this means that for both of the underlying Heisenberg and Schr\\\"{o}dinger evolution equations the generators (i.e., in our notation, the respective operators $\\Sigma(t)$ and $G(t)$) become available in closed form.","The key message is that contrary to the conventional beliefs and in spite of the unitarity of the evolution of the system, neither its ``Heisenbergian ``Hamiltonian'' $\\Sigma(t)$ nor its ``Schr\\\"{o}dingerian ``Hamiltonian'' $G(t)$ possesses a real spectrum (or even some spectrum containing the conjugate pairs of complex eigenvalues)."],"url":"http://arxiv.org/abs/2401.10682v1","category":"quant-ph"}
{"created":"2024-01-19 13:28:11","title":"Maximizing Real-Time Video QoE via Bandwidth Sharing under Markovian setting","abstract":"We consider the problem of optimizing Quality of Experience (QoE) of clients streaming real-time video, served by networks managed by different operators that can share bandwidth with each other. The abundance of real-time video traffic is evident in the popularity of applications like video conferencing and video streaming of live events, which have increased significantly since the recent pandemic. We model the problem as a joint optimization of resource allocation for the clients and bandwidth sharing across the operators, with special attention to how the resource allocation impacts clients' perceived video quality. We propose an online policy as a solution, which involves dynamically sharing a portion of one operator's bandwidth with another operator. We provide strong theoretical optimality guarantees for the policy. We also use extensive simulations to demonstrate the policy's substantial performance improvements (of up to ninety percent), and identify insights into key system parameters (e.g., imbalance in arrival rates or channel conditions of the operators) that dictate the improvements.","sentences":["We consider the problem of optimizing Quality of Experience (QoE) of clients streaming real-time video, served by networks managed by different operators that can share bandwidth with each other.","The abundance of real-time video traffic is evident in the popularity of applications like video conferencing and video streaming of live events, which have increased significantly since the recent pandemic.","We model the problem as a joint optimization of resource allocation for the clients and bandwidth sharing across the operators, with special attention to how the resource allocation impacts clients' perceived video quality.","We propose an online policy as a solution, which involves dynamically sharing a portion of one operator's bandwidth with another operator.","We provide strong theoretical optimality guarantees for the policy.","We also use extensive simulations to demonstrate the policy's substantial performance improvements (of up to ninety percent), and identify insights into key system parameters (e.g., imbalance in arrival rates or channel conditions of the operators) that dictate the improvements."],"url":"http://arxiv.org/abs/2401.10681v1","category":"cs.NI"}
{"created":"2024-01-19 13:24:33","title":"The PTA Hellings and Downs Correlation Unmasked by Symmetries","abstract":"The Hellings and Downs correlation curve describes the correlation of the timing residuals from pairs of pulsars as a function of their angular separation on the sky and is a smoking-gun signature for the detection of an isotropic stochastic background of gravitational waves. We show that it can be easily obtained from realizing that Lorentz transformations are conformal transformations on the celestial sphere and from the conformal properties of the two-point correlation of the timing residuals. This result allows several generalizations, e.g. the calculation of the three-point correlator of the time residuals and the inclusion of additional polarization modes (vector and/or scalar) arising in alternative theories of gravity.","sentences":["The Hellings and Downs correlation curve describes the correlation of the timing residuals from pairs of pulsars as a function of their angular separation on the sky and is a smoking-gun signature for the detection of an isotropic stochastic background of gravitational waves.","We show that it can be easily obtained from realizing that Lorentz transformations are conformal transformations on the celestial sphere and from the conformal properties of the two-point correlation of the timing residuals.","This result allows several generalizations, e.g. the calculation of the three-point correlator of the time residuals and the inclusion of additional polarization modes (vector and/or scalar) arising in alternative theories of gravity."],"url":"http://arxiv.org/abs/2401.10680v1","category":"gr-qc"}
{"created":"2024-01-19 13:22:27","title":"Coherent Control of the Fine-Structure Qubit in a Single Alkaline-Earth Atom","abstract":"We report on the first realization of a novel neutral atom qubit encoded in the metastable fine-structure states ${^3\\rm{P}_0}$ and ${^3\\rm{P}_2}$ of single $^{88}$Sr atoms trapped in an optical tweezer. Raman coupling of the qubit states promises rapid single-qubit rotations on par with the fast Rydberg-mediated two-body gates. We demonstrate preparation, read-out, and coherent control of the qubit. In addition to driving Rabi oscillations bridging an energy gap of more than 17 THz using a pair of phase-locked clock lasers, we also carry out Ramsey spectroscopy to extract the transverse qubit coherence time $T_2$. When the tweezer is tuned into magic trapping conditions, which is achieved in our setup by tuning the tensor polarizability of the ${^3\\rm{P}_2}$ state via an external control magnetic field, we measure $T_2 = 1.2$ ms. A microscopic quantum mechanical model is used to simulate our experiments including dominant noise sources. We identify the main constraints limiting the observed coherence time and project improvements to our system in the immediate future. Our work opens the door for a so far unexplored qubit encoding concept for neutral atom based quantum computing.","sentences":["We report on the first realization of a novel neutral atom qubit encoded in the metastable fine-structure states ${^3\\rm{P}_0}$ and ${^3\\rm{P}_2}$ of single $^{88}$Sr atoms trapped in an optical tweezer.","Raman coupling of the qubit states promises rapid single-qubit rotations on par with the fast Rydberg-mediated two-body gates.","We demonstrate preparation, read-out, and coherent control of the qubit.","In addition to driving Rabi oscillations bridging an energy gap of more than 17 THz using a pair of phase-locked clock lasers, we also carry out Ramsey spectroscopy to extract the transverse qubit coherence time $T_2$. When the tweezer is tuned into magic trapping conditions, which is achieved in our setup by tuning the tensor polarizability of the ${^3\\rm{P}_2}$ state via an external control magnetic field, we measure $T_2 = 1.2$ ms.","A microscopic quantum mechanical model is used to simulate our experiments including dominant noise sources.","We identify the main constraints limiting the observed coherence time and project improvements to our system in the immediate future.","Our work opens the door for a so far unexplored qubit encoding concept for neutral atom based quantum computing."],"url":"http://arxiv.org/abs/2401.10679v1","category":"quant-ph"}
{"created":"2024-01-19 13:19:40","title":"Multipole and fracton topological order via gauging foliated SPT phases","abstract":"Spurred by recent development of fracton topological phases, unusual topological phases possessing fractionalized quasi-particles with mobility constraints, the concept of symmetries has been renewed. In particular, in accordance with the progress of multipole symmetries, associated with conservation of multipoles, such as dipole or quadruple moments as well as global charges, there have been proposed topological phases with such symmetries. These topological phases are unconventional as excitations are subject to mobility constraints corresponding to the multipole symmetries. We demonstrate a way to construct such phases by preparing layers of symmetry protected topological (SPT) phases and implementing gauging a global symmetry. After gauging, the statistics of a fractional excitation is altered when crossing the SPT phases, resulting in topological phases with the multipole symmetries. The way we construct the phases allows us to have a comprehensive understanding of field theories of topological phases with the multipole symmetries and other fracton models.","sentences":["Spurred by recent development of fracton topological phases, unusual topological phases possessing fractionalized quasi-particles with mobility constraints, the concept of symmetries has been renewed.","In particular, in accordance with the progress of multipole symmetries, associated with conservation of multipoles, such as dipole or quadruple moments as well as global charges, there have been proposed topological phases with such symmetries.","These topological phases are unconventional as excitations are subject to mobility constraints corresponding to the multipole symmetries.","We demonstrate a way to construct such phases by preparing layers of symmetry protected topological (SPT) phases and implementing gauging a global symmetry.","After gauging, the statistics of a fractional excitation is altered when crossing the SPT phases, resulting in topological phases with the multipole symmetries.","The way we construct the phases allows us to have a comprehensive understanding of field theories of topological phases with the multipole symmetries and other fracton models."],"url":"http://arxiv.org/abs/2401.10677v1","category":"cond-mat.str-el"}
{"created":"2024-01-19 13:13:38","title":"Deep Learning-based Embedded Intrusion Detection System for Automotive CAN","abstract":"Rising complexity of in-vehicle electronics is enabling new capabilities like autonomous driving and active safety. However, rising automation also increases risk of security threats which is compounded by lack of in-built security measures in legacy networks like CAN, allowing attackers to observe, tamper and modify information shared over such broadcast networks. Various intrusion detection approaches have been proposed to detect and tackle such threats, with machine learning models proving highly effective. However, deploying machine learning models will require high processing power through high-end processors or GPUs to perform them close to line rate. In this paper, we propose a hybrid FPGA-based ECU approach that can transparently integrate IDS functionality through a dedicated off-the-shelf hardware accelerator that implements a deep-CNN intrusion detection model. Our results show that the proposed approach provides an average accuracy of over 99% across multiple attack datasets with 0.64% false detection rates while consuming 94% less energy and achieving 51.8% reduction in per-message processing latency when compared to IDS implementations on GPUs.","sentences":["Rising complexity of in-vehicle electronics is enabling new capabilities like autonomous driving and active safety.","However, rising automation also increases risk of security threats which is compounded by lack of in-built security measures in legacy networks like CAN, allowing attackers to observe, tamper and modify information shared over such broadcast networks.","Various intrusion detection approaches have been proposed to detect and tackle such threats, with machine learning models proving highly effective.","However, deploying machine learning models will require high processing power through high-end processors or GPUs to perform them close to line rate.","In this paper, we propose a hybrid FPGA-based ECU approach that can transparently integrate IDS functionality through a dedicated off-the-shelf hardware accelerator that implements a deep-CNN intrusion detection model.","Our results show that the proposed approach provides an average accuracy of over 99% across multiple attack datasets with 0.64% false detection rates while consuming 94% less energy and achieving 51.8% reduction in per-message processing latency when compared to IDS implementations on GPUs."],"url":"http://arxiv.org/abs/2401.10674v1","category":"cs.CR"}
{"created":"2024-01-19 13:00:45","title":"Time synchronization for deterministic communication","abstract":"Deterministic communication is required for applications of several industry verticals including manufacturing, automotive, financial, and health care, etc. These applications rely on reliable and time-synchronized delivery of information among the communicating devices. Therefore, large delay variations in packet delivery or inaccuracies in time synchronization cannot be tolerated. In particular, the industrial revolution on digitization, connectivity of digital and physical systems, and flexible production design require deterministic and time-synchronized communication. A network supporting deterministic communication guarantees data delivery in a specified time with high reliability. The IEEE 802.1 TSN task group is developing standards to provide deterministic communication through IEEE 802 networks. The IEEE 802.1AS standard defines time synchronization mechanism for accurate distribution of time among the communicating devices. The time synchronization accuracy depends on the accurate calculation of the residence time which is the time between the ingress and the egress ports of the bridge and includes the processing, queuing, transmission, and link latency of the timing information. This paper discusses time synchronization mechanisms supported in current wired and wireless integrated systems.","sentences":["Deterministic communication is required for applications of several industry verticals including manufacturing, automotive, financial, and health care, etc.","These applications rely on reliable and time-synchronized delivery of information among the communicating devices.","Therefore, large delay variations in packet delivery or inaccuracies in time synchronization cannot be tolerated.","In particular, the industrial revolution on digitization, connectivity of digital and physical systems, and flexible production design require deterministic and time-synchronized communication.","A network supporting deterministic communication guarantees data delivery in a specified time with high reliability.","The IEEE 802.1 TSN task group is developing standards to provide deterministic communication through IEEE 802 networks.","The IEEE 802.1AS standard defines time synchronization mechanism for accurate distribution of time among the communicating devices.","The time synchronization accuracy depends on the accurate calculation of the residence time which is the time between the ingress and the egress ports of the bridge and includes the processing, queuing, transmission, and link latency of the timing information.","This paper discusses time synchronization mechanisms supported in current wired and wireless integrated systems."],"url":"http://arxiv.org/abs/2401.10670v1","category":"cs.NI"}
{"created":"2024-01-19 13:00:36","title":"A Room With an Overview: Towards Meaningful Transparency for the Consumer Internet of Things","abstract":"As our physical environments become ever-more connected, instrumented and automated, it can be increasingly difficult for users to understand what is happening within them and why. This warrants attention; with the pervasive and physical nature of the IoT comes risks of data misuse, privacy, surveillance, and even physical harm. Such concerns come amid increasing calls for more transparency surrounding technologies (in general), as a means for supporting scrutiny and accountability.   This paper explores the practical dimensions to transparency mechanisms within the consumer IoT. That is, we consider how smart homes might be made more meaningfully transparent, so as to support users in gaining greater understanding, oversight, and control. Through a series of three user-centric studies, we (i) survey prospective smart home users to gain a general understanding of what meaningful transparency within smart homes might entail; (ii) identify categories of user-derived requirements and design elements (design features for supporting smart home transparency) that have been created through two co-design workshops; and (iii) validate these through an evaluation with an altogether new set of participants. In all, these categories of requirements and interface design elements provide a foundation for understanding how meaningful transparency might be achieved within smart homes, and introduces several wider considerations for doing so.","sentences":["As our physical environments become ever-more connected, instrumented and automated, it can be increasingly difficult for users to understand what is happening within them and why.","This warrants attention; with the pervasive and physical nature of the IoT comes risks of data misuse, privacy, surveillance, and even physical harm.","Such concerns come amid increasing calls for more transparency surrounding technologies (in general), as a means for supporting scrutiny and accountability.   ","This paper explores the practical dimensions to transparency mechanisms within the consumer IoT. That is, we consider how smart homes might be made more meaningfully transparent, so as to support users in gaining greater understanding, oversight, and control.","Through a series of three user-centric studies, we (i) survey prospective smart home users to gain a general understanding of what meaningful transparency within smart homes might entail; (ii) identify categories of user-derived requirements and design elements (design features for supporting smart home transparency) that have been created through two co-design workshops; and (iii) validate these through an evaluation with an altogether new set of participants.","In all, these categories of requirements and interface design elements provide a foundation for understanding how meaningful transparency might be achieved within smart homes, and introduces several wider considerations for doing so."],"url":"http://arxiv.org/abs/2401.10669v1","category":"cs.HC"}
{"created":"2024-01-19 12:50:44","title":"Physically viable rotating mass solutions surrounding Kerr black hole","abstract":"There exists in literature an increasing interest in the study of mass distributions surrounding black holes as describing dark matter halo in spiral galaxies. Motivated by this interest, we study a very recent new class of rotating solutions that are suitable to build anisotropic matter sources surrounding rotating black holes. Contrary to the mainstream approach, instead of use the so called regular black holes as central objects, we perform a smooth matching between the aforementioned anisotropic matter and a central vacuum Kerr black hole. In this framework, we study in full generality energy conditions near the matching surface. As a result, we found that, after imposing the vanishing of the energy density $E$ at the matching surface, if weak and dominant energy conditions (WEC,DEC) are satisfied, then unavoidable strong energy conditions is violated, i.e. near the event horizon only matter with dark energy-like features is allowed. As an application, we present two solutions everywhere satisfying DEC. The first one is asymptotically flat and equipped with a non vanishing electric charge, while the second solution presented is equipped with a non-vanishing energy flow around the symmetry axis and it is not asymptotically flat","sentences":["There exists in literature an increasing interest in the study of mass distributions surrounding black holes as describing dark matter halo in spiral galaxies.","Motivated by this interest, we study a very recent new class of rotating solutions that are suitable to build anisotropic matter sources surrounding rotating black holes.","Contrary to the mainstream approach, instead of use the so called regular black holes as central objects, we perform a smooth matching between the aforementioned anisotropic matter and a central vacuum Kerr black hole.","In this framework, we study in full generality energy conditions near the matching surface.","As a result, we found that, after imposing the vanishing of the energy density $E$ at the matching surface, if weak and dominant energy conditions (WEC,DEC) are satisfied, then unavoidable strong energy conditions is violated, i.e. near the event horizon only matter with dark energy-like features is allowed.","As an application, we present two solutions everywhere satisfying DEC.","The first one is asymptotically flat and equipped with a non vanishing electric charge, while the second solution presented is equipped with a non-vanishing energy flow around the symmetry axis and it is not asymptotically flat"],"url":"http://arxiv.org/abs/2401.10668v1","category":"gr-qc"}
{"created":"2024-01-19 12:40:54","title":"MixNet: Towards Effective and Efficient UHD Low-Light Image Enhancement","abstract":"With the continuous advancement of imaging devices, the prevalence of Ultra-High-Definition (UHD) images is rising. Although many image restoration methods have achieved promising results, they are not directly applicable to UHD images on devices with limited computational resources due to the inherently high computational complexity of UHD images. In this paper, we focus on the task of low-light image enhancement (LLIE) and propose a novel LLIE method called MixNet, which is designed explicitly for UHD images. To capture the long-range dependency of features without introducing excessive computational complexity, we present the Global Feature Modulation Layer (GFML). GFML associates features from different views by permuting the feature maps, enabling efficient modeling of long-range dependency. In addition, we also design the Local Feature Modulation Layer (LFML) and Feed-forward Layer (FFL) to capture local features and transform features into a compact representation. This way, our MixNet achieves effective LLIE with few model parameters and low computational complexity. We conducted extensive experiments on both synthetic and real-world datasets, and the comprehensive results demonstrate that our proposed method surpasses the performance of current state-of-the-art methods. The code will be available at \\url{https://github.com/zzr-idam/MixNet}.","sentences":["With the continuous advancement of imaging devices, the prevalence of Ultra-High-Definition (UHD) images is rising.","Although many image restoration methods have achieved promising results, they are not directly applicable to UHD images on devices with limited computational resources due to the inherently high computational complexity of UHD images.","In this paper, we focus on the task of low-light image enhancement (LLIE) and propose a novel LLIE method called MixNet, which is designed explicitly for UHD images.","To capture the long-range dependency of features without introducing excessive computational complexity, we present the Global Feature Modulation Layer (GFML).","GFML associates features from different views by permuting the feature maps, enabling efficient modeling of long-range dependency.","In addition, we also design the Local Feature Modulation Layer (LFML) and Feed-forward Layer (FFL) to capture local features and transform features into a compact representation.","This way, our MixNet achieves effective LLIE with few model parameters and low computational complexity.","We conducted extensive experiments on both synthetic and real-world datasets, and the comprehensive results demonstrate that our proposed method surpasses the performance of current state-of-the-art methods.","The code will be available at \\url{https://github.com/zzr-idam/MixNet}."],"url":"http://arxiv.org/abs/2401.10666v1","category":"cs.CV"}
{"created":"2024-01-19 12:38:15","title":"Optoacoustic entanglement in a continuous Brillouin-active solid state system","abstract":"Entanglement in hybrid quantum systems comprised of fundamentally different degrees of freedom, such as light and mechanics is of interest for a wide range of applications in quantum technologies. Here, we propose to engineer bipartite entanglement between traveling acoustic phonons in a Brillouin active solid state system and the accompanying light wave. The effect is achieved by applying optical pump pulses to state-of-the-art waveguides, exciting a Brillouin Stokes process. This pulsed approach, in a system operating in a regime orthogonal to standard optomechanical setups, allows for the generation of entangled photon-phonon pairs, resilient to thermal fluctuations. We propose an experimental platform where readout of the optoacoustics entanglement is done by the simultaneous detection of Stokes and Anti-Stokes photons in a two-pump configuration. The proposed mechanism presents an important feature in that it does not require initial preparation of the quantum ground state of the phonon mode.","sentences":["Entanglement in hybrid quantum systems comprised of fundamentally different degrees of freedom, such as light and mechanics is of interest for a wide range of applications in quantum technologies.","Here, we propose to engineer bipartite entanglement between traveling acoustic phonons in a Brillouin active solid state system and the accompanying light wave.","The effect is achieved by applying optical pump pulses to state-of-the-art waveguides, exciting a Brillouin Stokes process.","This pulsed approach, in a system operating in a regime orthogonal to standard optomechanical setups, allows for the generation of entangled photon-phonon pairs, resilient to thermal fluctuations.","We propose an experimental platform where readout of the optoacoustics entanglement is done by the simultaneous detection of Stokes and Anti-Stokes photons in a two-pump configuration.","The proposed mechanism presents an important feature in that it does not require initial preparation of the quantum ground state of the phonon mode."],"url":"http://arxiv.org/abs/2401.10665v1","category":"quant-ph"}
{"created":"2024-01-19 12:35:00","title":"PTPsec: Securing the Precision Time Protocol Against Time Delay Attacks Using Cyclic Path Asymmetry Analysis","abstract":"High-precision time synchronization is a vital prerequisite for many modern applications and technologies, including Smart Grids, Time-Sensitive Networking (TSN), and 5G networks. Although the Precision Time Protocol (PTP) can accomplish this requirement in trusted environments, it becomes unreliable in the presence of specific cyber attacks. Mainly, time delay attacks pose the highest threat to the protocol, enabling attackers to diverge targeted clocks undetected. With the increasing danger of cyber attacks, especially against critical infrastructure, there is a great demand for effective countermeasures to secure both time synchronization and the applications that depend on it. However, current solutions are not sufficiently capable of mitigating sophisticated delay attacks. For example, they lack proper integration into the PTP protocol, scalability, or sound evaluation with the required microsecond-level accuracy. This work proposes an approach to detect and counteract delay attacks against PTP based on cyclic path asymmetry measurements over redundant paths. For that, we provide a method to find redundant paths in arbitrary networks and show how this redundancy can be exploited to reveal and mitigate undesirable asymmetries on the synchronization path that cause the malicious clock divergence. Furthermore, we propose PTPsec, a secure PTP protocol and its implementation based on the latest IEEE 1588-2019 standard. With PTPsec, we advance the conventional PTP to support reliable delay attack detection and mitigation. We validate our approach on a hardware testbed, which includes an attacker capable of performing static and incremental delay attacks at a microsecond precision. Our experimental results show that all attack scenarios can be reliably detected and mitigated with minimal detection time.","sentences":["High-precision time synchronization is a vital prerequisite for many modern applications and technologies, including Smart Grids, Time-Sensitive Networking (TSN), and 5G networks.","Although the Precision Time Protocol (PTP) can accomplish this requirement in trusted environments, it becomes unreliable in the presence of specific cyber attacks.","Mainly, time delay attacks pose the highest threat to the protocol, enabling attackers to diverge targeted clocks undetected.","With the increasing danger of cyber attacks, especially against critical infrastructure, there is a great demand for effective countermeasures to secure both time synchronization and the applications that depend on it.","However, current solutions are not sufficiently capable of mitigating sophisticated delay attacks.","For example, they lack proper integration into the PTP protocol, scalability, or sound evaluation with the required microsecond-level accuracy.","This work proposes an approach to detect and counteract delay attacks against PTP based on cyclic path asymmetry measurements over redundant paths.","For that, we provide a method to find redundant paths in arbitrary networks and show how this redundancy can be exploited to reveal and mitigate undesirable asymmetries on the synchronization path that cause the malicious clock divergence.","Furthermore, we propose PTPsec, a secure PTP protocol and its implementation based on the latest IEEE 1588-2019 standard.","With PTPsec, we advance the conventional PTP to support reliable delay attack detection and mitigation.","We validate our approach on a hardware testbed, which includes an attacker capable of performing static and incremental delay attacks at a microsecond precision.","Our experimental results show that all attack scenarios can be reliably detected and mitigated with minimal detection time."],"url":"http://arxiv.org/abs/2401.10664v1","category":"cs.CR"}
{"created":"2024-01-19 12:26:57","title":"A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation","abstract":"Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages. However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages. This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation. To address this, our study introduces a novel framework designed to expedite text generation in these languages. This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required. Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-trained multilingual model on monolingual tasks.","sentences":["Recent advancements in large language models have facilitated the execution of complex language tasks, not only in English but also in non-English languages.","However, the tokenizers of most language models, such as Llama, trained on English-centric corpora, tend to excessively fragment tokens in non-English languages.","This issue is especially pronounced in non-roman alphabetic languages, which are often divided at a character or even Unicode level, leading to slower text generation.","To address this, our study introduces a novel framework designed to expedite text generation in these languages.","This framework predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required.","Our empirical results demonstrate that the proposed framework increases the generation speed by a factor of 1.9 compared to standard decoding while maintaining the performance of a pre-trained multilingual model on monolingual tasks."],"url":"http://arxiv.org/abs/2401.10660v1","category":"cs.CL"}
{"created":"2024-01-19 12:26:51","title":"BadODD: Bangladeshi Autonomous Driving Object Detection Dataset","abstract":"We propose a comprehensive dataset for object detection in diverse driving environments across 9 districts in Bangladesh. The dataset, collected exclusively from smartphone cameras, provided a realistic representation of real-world scenarios, including day and night conditions. Most existing datasets lack suitable classes for autonomous navigation on Bangladeshi roads, making it challenging for researchers to develop models that can handle the intricacies of road scenarios. To address this issue, the authors proposed a new set of classes based on characteristics rather than local vehicle names. The dataset aims to encourage the development of models that can handle the unique challenges of Bangladeshi road scenarios for the effective deployment of autonomous vehicles. The dataset did not consist of any online images to simulate real-world conditions faced by autonomous vehicles. The classification of vehicles is challenging because of the diverse range of vehicles on Bangladeshi roads, including those not found elsewhere in the world. The proposed classification system is scalable and can accommodate future vehicles, making it a valuable resource for researchers in the autonomous vehicle sector.","sentences":["We propose a comprehensive dataset for object detection in diverse driving environments across 9 districts in Bangladesh.","The dataset, collected exclusively from smartphone cameras, provided a realistic representation of real-world scenarios, including day and night conditions.","Most existing datasets lack suitable classes for autonomous navigation on Bangladeshi roads, making it challenging for researchers to develop models that can handle the intricacies of road scenarios.","To address this issue, the authors proposed a new set of classes based on characteristics rather than local vehicle names.","The dataset aims to encourage the development of models that can handle the unique challenges of Bangladeshi road scenarios for the effective deployment of autonomous vehicles.","The dataset did not consist of any online images to simulate real-world conditions faced by autonomous vehicles.","The classification of vehicles is challenging because of the diverse range of vehicles on Bangladeshi roads, including those not found elsewhere in the world.","The proposed classification system is scalable and can accommodate future vehicles, making it a valuable resource for researchers in the autonomous vehicle sector."],"url":"http://arxiv.org/abs/2401.10659v1","category":"cs.CV"}
{"created":"2024-01-19 12:04:31","title":"FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks","abstract":"With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes. This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets. We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance. Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model. Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives. Furthermore, we analyze the resulting adversarial samples via spectral analysis yielding conclusions for countermeasures against such attacks.","sentences":["With the steady rise of the use of AI in bio-technical applications and the widespread adoption of genomics sequencing, an increasing amount of AI-based algorithms and tools is entering the research and production stage affecting critical decision-making streams like drug discovery and clinical outcomes.","This paper demonstrates the vulnerability of AI models often utilized downstream tasks on recognized public genomics datasets.","We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance.","Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model.","Our empirical findings unequivocally demonstrate a decline in model performance, underscored by diminished accuracy and an upswing in false positives and false negatives.","Furthermore, we analyze the resulting adversarial samples via spectral analysis yielding conclusions for countermeasures against such attacks."],"url":"http://arxiv.org/abs/2401.10657v1","category":"cs.LG"}
{"created":"2024-01-19 11:59:13","title":"Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech Detection","abstract":"With the recent surge and exponential growth of social media usage, scrutinizing social media content for the presence of any hateful content is of utmost importance. Researchers have been diligently working since the past decade on distinguishing between content that promotes hatred and content that does not. Traditionally, the main focus has been on analyzing textual content. However, recent research attempts have also commenced into the identification of audio-based content. Nevertheless, studies have shown that relying solely on audio or text-based content may be ineffective, as recent upsurge indicates that individuals often employ sarcasm in their speech and writing. To overcome these challenges, we present an approach to identify whether a speech promotes hate or not utilizing both audio and textual representations. Our methodology is based on the Transformer framework that incorporates both audio and text sampling, accompanied by our very own layer called \"Attentive Fusion\". The results of our study surpassed previous state-of-the-art techniques, achieving an impressive macro F1 score of 0.927 on the Test Set.","sentences":["With the recent surge and exponential growth of social media usage, scrutinizing social media content for the presence of any hateful content is of utmost importance.","Researchers have been diligently working since the past decade on distinguishing between content that promotes hatred and content that does not.","Traditionally, the main focus has been on analyzing textual content.","However, recent research attempts have also commenced into the identification of audio-based content.","Nevertheless, studies have shown that relying solely on audio or text-based content may be ineffective, as recent upsurge indicates that individuals often employ sarcasm in their speech and writing.","To overcome these challenges, we present an approach to identify whether a speech promotes hate or not utilizing both audio and textual representations.","Our methodology is based on the Transformer framework that incorporates both audio and text sampling, accompanied by our very own layer called \"Attentive Fusion\".","The results of our study surpassed previous state-of-the-art techniques, achieving an impressive macro F1 score of 0.927 on the Test Set."],"url":"http://arxiv.org/abs/2401.10653v1","category":"cs.CL"}
{"created":"2024-01-19 11:58:13","title":"AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference","abstract":"Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The experiments demonstrate that AutoChunk can reduce over 80\\% of activation memory while maintaining speed loss within 10%, extend max sequence length by 3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.","sentences":["Large deep learning models have achieved impressive performance across a range of applications.","However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving.","While existing methods mainly address parameter memory, the importance of activation memory has been overlooked.","Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases.","In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies.","The proposed system generates chunk plans by optimizing through multiple stages.","In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one.","At runtime, AutoChunk employs code generation to automatically apply chunk strategies.","The experiments demonstrate that AutoChunk can reduce over 80\\% of activation memory while maintaining speed loss within 10%, extend max sequence length by 3.2x to 11.7x, and outperform state-of-the-art methods by a large margin."],"url":"http://arxiv.org/abs/2401.10652v1","category":"cs.PF"}
{"created":"2024-01-19 11:55:59","title":"Low-loss, compact, fibre-integrated cell for quantum memories","abstract":"We present a low-loss, compact, hollow core optical fibre (HCF) cell integrated with single mode fibre (SMF). The cell is designed to be filled with atomic vapour and used as a component in photonic quantum technologies, with applications in quantum memory and optical switching. We achieve a total insertion loss of 0.6(2) dB at 780 nm wavelength via graded index fibre to ensure efficient mode matching coupled with anti-reflection coatings to minimise loss at the SMF-HCF interfaces. We also present numerical modelling of these interfaces, which can be undertaken efficiently without the need for finite element simulation. We encapsulate the HCF core by coupling to the SMF inside a support capillary, enhancing durability and facilitating seamless integration into existing fibre platforms.","sentences":["We present a low-loss, compact, hollow core optical fibre (HCF) cell integrated with single mode fibre (SMF).","The cell is designed to be filled with atomic vapour and used as a component in photonic quantum technologies, with applications in quantum memory and optical switching.","We achieve a total insertion loss of 0.6(2) dB at 780 nm wavelength via graded index fibre to ensure efficient mode matching coupled with anti-reflection coatings to minimise loss at the SMF-HCF interfaces.","We also present numerical modelling of these interfaces, which can be undertaken efficiently without the need for finite element simulation.","We encapsulate the HCF core by coupling to the SMF inside a support capillary, enhancing durability and facilitating seamless integration into existing fibre platforms."],"url":"http://arxiv.org/abs/2401.10651v1","category":"quant-ph"}
{"created":"2024-01-19 11:48:52","title":"Area Modeling using Stay Information for Large-Scale Users and Analysis for Influence of COVID-19","abstract":"Understanding how people use area in a city can be a valuable information in a wide range of fields, from marketing to urban planning. Area usage is subject to change over time due to various events including seasonal shifts and pandemics. Before the spread of smartphones, this data had been collected through questionnaire survey. However, this is not a sustainable approach in terms of time to results and cost. There are many existing studies on area modeling, which characterize an area with some kind of information, using Point of Interest (POI) or inter-area movement data. However, since POI is data that is statically tied to space, and inter-area movement data ignores the behavior of people within an area, existing methods are not sufficient in terms of capturing area usage changes. In this paper, we propose a novel area modeling method named Area2Vec, inspired by Word2Vec, which models areas based on people's location data. This method is based on the discovery that it is possible to characterize an area based on its usage by using people's stay information in the area. And it is a novel method that can reflect the dynamically changing people's behavior in an area in the modeling results. We validated Area2vec by performing a functional classification of areas in a district of Japan. The results show that Area2Vec can be usable in general area analysis. We also investigated area usage changes due to COVID-19 in two districts in Japan. We could find that COVID-19 made people refrain from unnecessary going out, such as visiting entertainment areas.","sentences":["Understanding how people use area in a city can be a valuable information in a wide range of fields, from marketing to urban planning.","Area usage is subject to change over time due to various events including seasonal shifts and pandemics.","Before the spread of smartphones, this data had been collected through questionnaire survey.","However, this is not a sustainable approach in terms of time to results and cost.","There are many existing studies on area modeling, which characterize an area with some kind of information, using Point of Interest (POI) or inter-area movement data.","However, since POI is data that is statically tied to space, and inter-area movement data ignores the behavior of people within an area, existing methods are not sufficient in terms of capturing area usage changes.","In this paper, we propose a novel area modeling method named Area2Vec, inspired by Word2Vec, which models areas based on people's location data.","This method is based on the discovery that it is possible to characterize an area based on its usage by using people's stay information in the area.","And it is a novel method that can reflect the dynamically changing people's behavior in an area in the modeling results.","We validated Area2vec by performing a functional classification of areas in a district of Japan.","The results show that Area2Vec can be usable in general area analysis.","We also investigated area usage changes due to COVID-19 in two districts in Japan.","We could find that COVID-19 made people refrain from unnecessary going out, such as visiting entertainment areas."],"url":"http://arxiv.org/abs/2401.10648v1","category":"cs.LG"}
{"created":"2024-01-19 11:48:09","title":"Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models","abstract":"In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior","sentences":["In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study.","This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models.","This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity.","Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors.","Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain.","This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails.","Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior"],"url":"http://arxiv.org/abs/2401.10647v1","category":"cs.CL"}
{"created":"2024-01-19 11:47:49","title":"Empowering HWNs with Efficient Data Labeling: A Clustered Federated Semi-Supervised Learning Approach","abstract":"Clustered Federated Multitask Learning (CFL) has gained considerable attention as an effective strategy for overcoming statistical challenges, particularly when dealing with non independent and identically distributed (non IID) data across multiple users. However, much of the existing research on CFL operates under the unrealistic premise that devices have access to accurate ground truth labels. This assumption becomes especially problematic in hierarchical wireless networks (HWNs), where edge networks contain a large amount of unlabeled data, resulting in slower convergence rates and increased processing times, particularly when dealing with two layers of model aggregation. To address these issues, we introduce a novel framework, Clustered Federated Semi-Supervised Learning (CFSL), designed for more realistic HWN scenarios. Our approach leverages a best-performing specialized model algorithm, wherein each device is assigned a specialized model that is highly adept at generating accurate pseudo-labels for unlabeled data, even when the data stems from diverse environments. We validate the efficacy of CFSL through extensive experiments, comparing it with existing methods highlighted in recent literature. Our numerical results demonstrate that CFSL significantly improves upon key metrics such as testing accuracy, labeling accuracy, and labeling latency under varying proportions of labeled and unlabeled data while also accommodating the non-IID nature of the data and the unique characteristics of wireless edge networks.","sentences":["Clustered Federated Multitask Learning (CFL) has gained considerable attention as an effective strategy for overcoming statistical challenges, particularly when dealing with non independent and identically distributed (non IID) data across multiple users.","However, much of the existing research on CFL operates under the unrealistic premise that devices have access to accurate ground truth labels.","This assumption becomes especially problematic in hierarchical wireless networks (HWNs), where edge networks contain a large amount of unlabeled data, resulting in slower convergence rates and increased processing times, particularly when dealing with two layers of model aggregation.","To address these issues, we introduce a novel framework, Clustered Federated Semi-Supervised Learning (CFSL), designed for more realistic HWN scenarios.","Our approach leverages a best-performing specialized model algorithm, wherein each device is assigned a specialized model that is highly adept at generating accurate pseudo-labels for unlabeled data, even when the data stems from diverse environments.","We validate the efficacy of CFSL through extensive experiments, comparing it with existing methods highlighted in recent literature.","Our numerical results demonstrate that CFSL significantly improves upon key metrics such as testing accuracy, labeling accuracy, and labeling latency under varying proportions of labeled and unlabeled data while also accommodating the non-IID nature of the data and the unique characteristics of wireless edge networks."],"url":"http://arxiv.org/abs/2401.10646v1","category":"cs.NI"}
{"created":"2024-01-19 11:45:10","title":"A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges","abstract":"Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments. This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives. Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years. Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable. This paper extensively explores deep learning techniques applied to vehicle ReID. It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research directions. This comprehensive assessment examines the landscape of deep learning in vehicle ReID and establishes a foundation and starting point for future works. It aims to serve as a complete reference by highlighting challenges and emerging trends, fostering advancements and applications in vehicle ReID utilizing deep learning models.","sentences":["Vehicle re-identification (ReID) endeavors to associate vehicle images collected from a distributed network of cameras spanning diverse traffic environments.","This task assumes paramount importance within the spectrum of vehicle-centric technologies, playing a pivotal role in deploying Intelligent Transportation Systems (ITS) and advancing smart city initiatives.","Rapid advancements in deep learning have significantly propelled the evolution of vehicle ReID technologies in recent years.","Consequently, undertaking a comprehensive survey of methodologies centered on deep learning for vehicle re-identification has become imperative and inescapable.","This paper extensively explores deep learning techniques applied to vehicle ReID.","It outlines the categorization of these methods, encompassing supervised and unsupervised approaches, delves into existing research within these categories, introduces datasets and evaluation criteria, and delineates forthcoming challenges and potential research directions.","This comprehensive assessment examines the landscape of deep learning in vehicle ReID and establishes a foundation and starting point for future works.","It aims to serve as a complete reference by highlighting challenges and emerging trends, fostering advancements and applications in vehicle ReID utilizing deep learning models."],"url":"http://arxiv.org/abs/2401.10643v1","category":"cs.CV"}
{"created":"2024-01-19 11:44:09","title":"Fast Butterfly-Core Community Search For Large Labeled Graphs","abstract":"Community Search (CS) aims to identify densely interconnected subgraphs corresponding to query vertices within a graph. However, existing heterogeneous graph-based community search methods need help identifying cross-group communities and suffer from efficiency issues, making them unsuitable for large graphs. This paper presents a fast community search model based on the Butterfly-Core Community (BCC) structure for heterogeneous graphs. The Random Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate the importance of vertices within communities, allowing leader vertices to be rapidly updated to maintain cross-group cohesion. Moreover, we devised a more efficient method for updating vertex distances, which minimizes vertex visits and enhances operational efficiency. Extensive experiments on several real-world temporal graphs demonstrate the effectiveness and efficiency of this solution.","sentences":["Community Search (CS) aims to identify densely interconnected subgraphs corresponding to query vertices within a graph.","However, existing heterogeneous graph-based community search methods need help identifying cross-group communities and suffer from efficiency issues, making them unsuitable for large graphs.","This paper presents a fast community search model based on the Butterfly-Core Community (BCC) structure for heterogeneous graphs.","The Random Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate the importance of vertices within communities, allowing leader vertices to be rapidly updated to maintain cross-group cohesion.","Moreover, we devised a more efficient method for updating vertex distances, which minimizes vertex visits and enhances operational efficiency.","Extensive experiments on several real-world temporal graphs demonstrate the effectiveness and efficiency of this solution."],"url":"http://arxiv.org/abs/2401.10642v1","category":"cs.SI"}
{"created":"2024-01-19 11:37:30","title":"An Effective Index for Truss-based Community Search on Large Directed Graphs","abstract":"Community search is a derivative of community detection that enables online and personalized discovery of communities and has found extensive applications in massive real-world networks. Recently, there needs to be more focus on the community search issue within directed graphs, even though substantial research has been carried out on undirected graphs. The recently proposed D-truss model has achieved good results in the quality of retrieved communities. However, existing D-truss-based work cannot perform efficient community searches on large graphs because it consumes too many computing resources to retrieve the maximal D-truss. To overcome this issue, we introduce an innovative merge relation known as D-truss-connected to capture the inherent density and cohesiveness of edges within D-truss. This relation allows us to partition all the edges in the original graph into a series of D-truss-connected classes. Then, we construct a concise and compact index, ConDTruss, based on D-truss-connected. Using ConDTruss, the efficiency of maximum D-truss retrieval will be greatly improved, making it a theoretically optimal approach. Experimental evaluations conducted on large directed graph certificate the effectiveness of our proposed method.","sentences":["Community search is a derivative of community detection that enables online and personalized discovery of communities and has found extensive applications in massive real-world networks.","Recently, there needs to be more focus on the community search issue within directed graphs, even though substantial research has been carried out on undirected graphs.","The recently proposed D-truss model has achieved good results in the quality of retrieved communities.","However, existing D-truss-based work cannot perform efficient community searches on large graphs because it consumes too many computing resources to retrieve the maximal D-truss.","To overcome this issue, we introduce an innovative merge relation known as D-truss-connected to capture the inherent density and cohesiveness of edges within D-truss.","This relation allows us to partition all the edges in the original graph into a series of D-truss-connected classes.","Then, we construct a concise and compact index, ConDTruss, based on D-truss-connected.","Using ConDTruss, the efficiency of maximum D-truss retrieval will be greatly improved, making it a theoretically optimal approach.","Experimental evaluations conducted on large directed graph certificate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2401.10641v1","category":"cs.SI"}
{"created":"2024-01-19 11:35:52","title":"A comprehensive study on fidelity metrics for XAI","abstract":"The use of eXplainable Artificial Intelligence (XAI) systems has introduced a set of challenges that need resolution. Herein, we focus on how to correctly select an XAI method, an open questions within the field. The inherent difficulty of this task is due to the lack of a ground truth. Several authors have proposed metrics to approximate the fidelity of different XAI methods. These metrics lack verification and have concerning disagreements. In this study, we proposed a novel methodology to verify fidelity metrics, using a well-known transparent model, namely a decision tree. This model allowed us to obtain explanations with perfect fidelity. Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods. We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images. The images from these datasets had a size a 128 by 128 pixels and were synthetic data that simplified the training process. All metric values, indicated a lack of fidelity, with the best one showing a 30 \\% deviation from the expected values for perfect explanation. Our experimentation led us to conclude that the current fidelity metrics are not reliable enough to be used in real scenarios. From this finding, we deemed it necessary to development new metrics, to avoid the detected problems, and we recommend the usage of our proposal as a benchmark within the scientific community to address these limitations.","sentences":["The use of eXplainable Artificial Intelligence (XAI) systems has introduced a set of challenges that need resolution.","Herein, we focus on how to correctly select an XAI method, an open questions within the field.","The inherent difficulty of this task is due to the lack of a ground truth.","Several authors have proposed metrics to approximate the fidelity of different XAI methods.","These metrics lack verification and have concerning disagreements.","In this study, we proposed a novel methodology to verify fidelity metrics, using a well-known transparent model, namely a decision tree.","This model allowed us to obtain explanations with perfect fidelity.","Our proposal constitutes the first objective benchmark for these metrics, facilitating a comparison of existing proposals, and surpassing existing methods.","We applied our benchmark to assess the existing fidelity metrics in two different experiments, each using public datasets comprising 52,000 images.","The images from these datasets had a size a 128 by 128 pixels and were synthetic data that simplified the training process.","All metric values, indicated a lack of fidelity, with the best one showing a 30 \\% deviation from the expected values for perfect explanation.","Our experimentation led us to conclude that the current fidelity metrics are not reliable enough to be used in real scenarios.","From this finding, we deemed it necessary to development new metrics, to avoid the detected problems, and we recommend the usage of our proposal as a benchmark within the scientific community to address these limitations."],"url":"http://arxiv.org/abs/2401.10640v1","category":"cs.CV"}
{"created":"2024-01-19 11:35:14","title":"Accurately Computing Expected Visiting Times and Stationary Distributions in Markov Chains","abstract":"We study the accurate and efficient computation of the expected number of times each state is visited in discrete- and continuous-time Markov chains. To obtain sound accuracy guarantees efficiently, we lift interval iteration and topological approaches known from the computation of reachability probabilities and expected rewards. We further study applications of expected visiting times, including the sound computation of the stationary distribution and expected rewards conditioned on reaching multiple goal states. The implementation of our methods in the probabilistic model checker Storm scales to large systems with millions of states. Our experiments on the quantitative verification benchmark set show that the computation of stationary distributions via expected visiting times consistently outperforms existing approaches - sometimes by several orders of magnitude.","sentences":["We study the accurate and efficient computation of the expected number of times each state is visited in discrete- and continuous-time Markov chains.","To obtain sound accuracy guarantees efficiently, we lift interval iteration and topological approaches known from the computation of reachability probabilities and expected rewards.","We further study applications of expected visiting times, including the sound computation of the stationary distribution and expected rewards conditioned on reaching multiple goal states.","The implementation of our methods in the probabilistic model checker Storm scales to large systems with millions of states.","Our experiments on the quantitative verification benchmark set show that the computation of stationary distributions via expected visiting times consistently outperforms existing approaches - sometimes by several orders of magnitude."],"url":"http://arxiv.org/abs/2401.10638v1","category":"cs.LO"}
{"created":"2024-01-19 11:35:07","title":"Towards Universal Unsupervised Anomaly Detection in Medical Imaging","abstract":"The increasing complexity of medical imaging data underscores the need for advanced anomaly detection methods to automatically identify diverse pathologies. Current methods face challenges in capturing the broad spectrum of anomalies, often limiting their use to specific lesion types in brain scans. To address this challenge, we introduce a novel unsupervised approach, termed \\textit{Reversed Auto-Encoders (RA)}, designed to create realistic pseudo-healthy reconstructions that enable the detection of a wider range of pathologies. We evaluate the proposed method across various imaging modalities, including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray, and chest X-ray, and demonstrate superior performance in detecting anomalies compared to existing state-of-the-art methods. Our unsupervised anomaly detection approach may enhance diagnostic accuracy in medical imaging by identifying a broader range of unknown pathologies. Our code is publicly available at: \\url{https://github.com/ci-ber/RA}.","sentences":["The increasing complexity of medical imaging data underscores the need for advanced anomaly detection methods to automatically identify diverse pathologies.","Current methods face challenges in capturing the broad spectrum of anomalies, often limiting their use to specific lesion types in brain scans.","To address this challenge, we introduce a novel unsupervised approach, termed \\textit{Reversed Auto-Encoders (RA)}, designed to create realistic pseudo-healthy reconstructions that enable the detection of a wider range of pathologies.","We evaluate the proposed method across various imaging modalities, including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray, and chest X-ray, and demonstrate superior performance in detecting anomalies compared to existing state-of-the-art methods.","Our unsupervised anomaly detection approach may enhance diagnostic accuracy in medical imaging by identifying a broader range of unknown pathologies.","Our code is publicly available at: \\url{https://github.com/ci-ber/RA}."],"url":"http://arxiv.org/abs/2401.10637v1","category":"eess.IV"}
{"created":"2024-01-19 11:27:34","title":"Catch the Butterfly: Peeking into the Terms and Conflicts among SPDX Licenses","abstract":"The widespread adoption of third-party libraries (TPLs) in software development has accelerated the creation of modern software. However, this convenience comes with potential legal risks. Developers may inadvertently violate the licenses of TPLs, leading to legal issues. While existing studies have explored software licenses and potential incompatibilities, these studies often focus on a limited set of licenses or rely on low-quality license data, which may affect their conclusions. To address this gap, there is a need for a high-quality license dataset that encompasses a broad range of mainstream licenses to help developers navigate the complex landscape of software licenses, avoid potential legal pitfalls, and guide solutions for managing license compliance and compatibility in software development. To this end, we conduct the first work to understand the mainstream software licenses based on term granularity and obtain a high-quality dataset of 453 SPDX licenses with well-labeled terms and conflicts. Specifically, we first conduct a differential analysis of the mainstream platforms to understand the terms and attitudes of each license. Next, we propose a standardized set of license terms to capture and label existing mainstream licenses with high quality. Moreover, we include copyleft conflicts and conclude the three major types of license conflicts among the 453 SPDX licenses. Based on these, we carry out two empirical studies to reveal the concerns and threats from the perspectives of both licensors and licensees. One study provides an in-depth analysis of the similarities, differences, and conflicts among SPDX licenses, revisits the usage and conflicts of licenses in the NPM ecosystem, and draws conclusions that differ from previous work. Our studies reveal some insightful findings and disclose relevant analytical data, which set the stage for further research.","sentences":["The widespread adoption of third-party libraries (TPLs) in software development has accelerated the creation of modern software.","However, this convenience comes with potential legal risks.","Developers may inadvertently violate the licenses of TPLs, leading to legal issues.","While existing studies have explored software licenses and potential incompatibilities, these studies often focus on a limited set of licenses or rely on low-quality license data, which may affect their conclusions.","To address this gap, there is a need for a high-quality license dataset that encompasses a broad range of mainstream licenses to help developers navigate the complex landscape of software licenses, avoid potential legal pitfalls, and guide solutions for managing license compliance and compatibility in software development.","To this end, we conduct the first work to understand the mainstream software licenses based on term granularity and obtain a high-quality dataset of 453 SPDX licenses with well-labeled terms and conflicts.","Specifically, we first conduct a differential analysis of the mainstream platforms to understand the terms and attitudes of each license.","Next, we propose a standardized set of license terms to capture and label existing mainstream licenses with high quality.","Moreover, we include copyleft conflicts and conclude the three major types of license conflicts among the 453 SPDX licenses.","Based on these, we carry out two empirical studies to reveal the concerns and threats from the perspectives of both licensors and licensees.","One study provides an in-depth analysis of the similarities, differences, and conflicts among SPDX licenses, revisits the usage and conflicts of licenses in the NPM ecosystem, and draws conclusions that differ from previous work.","Our studies reveal some insightful findings and disclose relevant analytical data, which set the stage for further research."],"url":"http://arxiv.org/abs/2401.10636v1","category":"cs.SE"}
{"created":"2024-01-19 11:22:04","title":"Automatic Construction of Multi-faceted User Profiles using Text Clustering and its Application to Expert Recommendation and Filtering Problems","abstract":"In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc. but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun. Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile. In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested. The experts will then be represented by means of multi-faceted profiles. Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering.","sentences":["In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc.","but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun.","Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile.","In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested.","The experts will then be represented by means of multi-faceted profiles.","Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering."],"url":"http://arxiv.org/abs/2401.10634v1","category":"cs.IR"}
{"created":"2024-01-19 11:20:31","title":"Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach","abstract":"Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge. The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate the effectiveness of this method.","sentences":["Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race.","In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects.","However, current methods assume that the true causal graph is given, which is often not true in real-world applications.","To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known.","The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge.","The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy.","Results on both simulated and real-world datasets demonstrate the effectiveness of this method."],"url":"http://arxiv.org/abs/2401.10632v1","category":"cs.LG"}
{"created":"2024-01-19 11:13:30","title":"A Critical Reflection on the Use of Toxicity Detection Algorithms in Proactive Content Moderation Systems","abstract":"Toxicity detection algorithms, originally designed with reactive content moderation in mind, are increasingly being deployed into proactive end-user interventions to moderate content. Through a socio-technical lens and focusing on contexts in which they are applied, we explore the use of these algorithms in proactive moderation systems. Placing a toxicity detection algorithm in an imagined virtual mobile keyboard, we critically explore how such algorithms could be used to proactively reduce the sending of toxic content. We present findings from design workshops conducted with four distinct stakeholder groups and find concerns around how contextual complexities may exasperate inequalities around content moderation processes. Whilst only specific user groups are likely to directly benefit from these interventions, we highlight the potential for other groups to misuse them to circumvent detection, validate and gamify hate, and manipulate algorithmic models to exasperate harm.","sentences":["Toxicity detection algorithms, originally designed with reactive content moderation in mind, are increasingly being deployed into proactive end-user interventions to moderate content.","Through a socio-technical lens and focusing on contexts in which they are applied, we explore the use of these algorithms in proactive moderation systems.","Placing a toxicity detection algorithm in an imagined virtual mobile keyboard, we critically explore how such algorithms could be used to proactively reduce the sending of toxic content.","We present findings from design workshops conducted with four distinct stakeholder groups and find concerns around how contextual complexities may exasperate inequalities around content moderation processes.","Whilst only specific user groups are likely to directly benefit from these interventions, we highlight the potential for other groups to misuse them to circumvent detection, validate and gamify hate, and manipulate algorithmic models to exasperate harm."],"url":"http://arxiv.org/abs/2401.10629v1","category":"cs.HC"}
{"created":"2024-01-19 11:11:40","title":"Catastrophe Theory for $\u0393$-invariant Unfoldings with Applications to Quantum Many-body Theory","abstract":"The theory of singularities and its broad ramifications, especially catastrophe theory, have found fertile ground in some areas of physics (e.g., caustics, wave optics) for their applications. In the context of quantum many-body theory, however, their results, despite being useful, are not generally known by the scientific community and often require a non-trivial adaptation, mainly due to the effects of symmetries in the associated physical system, which are not encompassed by the original theory. In this article, we provide an extension of the main results of Ren\\'e Thom's catastrophe theory for the case of germs and unfoldings possessing special symmetries. In a more mathematically precise language, we provide a proof of the determinacy theorems for germs that are invariant under the action of an arbitrary compact Lie group, and of the transversality and stability theorems for the case of invariant unfoldings. The results obtained can be seen as an extension and adaptation of the works of [7] and [8] on singularity theory of $\\mathbb{R}^n$ to $\\mathbb{R}^n$ mappings to the particular scenario of catastrophe theory. Finally, we also provide a classification theorem for unfoldings invariant under the action of $\\mathbb{Z}_2$, and present some of the possible applications of the theory to the study of phase transitions in quantum many-body systems.","sentences":["The theory of singularities and its broad ramifications, especially catastrophe theory, have found fertile ground in some areas of physics (e.g., caustics, wave optics) for their applications.","In the context of quantum many-body theory, however, their results, despite being useful, are not generally known by the scientific community and often require a non-trivial adaptation, mainly due to the effects of symmetries in the associated physical system, which are not encompassed by the original theory.","In this article, we provide an extension of the main results of Ren\\'e Thom's catastrophe theory for the case of germs and unfoldings possessing special symmetries.","In a more mathematically precise language, we provide a proof of the determinacy theorems for germs that are invariant under the action of an arbitrary compact Lie group, and of the transversality and stability theorems for the case of invariant unfoldings.","The results obtained can be seen as an extension and adaptation of the works of [7] and [8] on singularity theory of $\\mathbb{R}^n$ to $\\mathbb{R}^n$ mappings to the particular scenario of catastrophe theory.","Finally, we also provide a classification theorem for unfoldings invariant under the action of $\\mathbb{Z}_2$, and present some of the possible applications of the theory to the study of phase transitions in quantum many-body systems."],"url":"http://arxiv.org/abs/2401.10628v1","category":"math-ph"}
{"created":"2024-01-19 11:11:03","title":"Key to Kindness: Reducing Toxicity In Online Discourse Through Proactive Content Moderation in a Mobile Keyboard","abstract":"Growing evidence shows that proactive content moderation supported by AI can help improve online discourse. However, we know little about designing these systems, how design impacts efficacy and user experience, and how people perceive proactive moderation across public and private platforms. We developed a mobile keyboard with built-in proactive content moderation which we tested (N=575) within a semi-functional simulation of a public and private communication platform. Where toxic content was detected, we used different interventions that embedded three design factors: timing, friction, and the presentation of the AI model output. We found moderation to be effective, regardless of the design. However, friction was a source of annoyance while prompts with no friction that occurred during typing were more effective. Follow-up interviews highlight the differences in how these systems are perceived across public and private platforms, and how they can offer more than moderation by acting as educational and communication support tools.","sentences":["Growing evidence shows that proactive content moderation supported by AI can help improve online discourse.","However, we know little about designing these systems, how design impacts efficacy and user experience, and how people perceive proactive moderation across public and private platforms.","We developed a mobile keyboard with built-in proactive content moderation which we tested (N=575) within a semi-functional simulation of a public and private communication platform.","Where toxic content was detected, we used different interventions that embedded three design factors: timing, friction, and the presentation of the AI model output.","We found moderation to be effective, regardless of the design.","However, friction was a source of annoyance while prompts with no friction that occurred during typing were more effective.","Follow-up interviews highlight the differences in how these systems are perceived across public and private platforms, and how they can offer more than moderation by acting as educational and communication support tools."],"url":"http://arxiv.org/abs/2401.10627v1","category":"cs.HC"}
{"created":"2024-01-19 11:07:47","title":"Long-Lived Circular Rydberg Qubits of Alkaline-Earth Atoms in Optical Tweezers","abstract":"Coherence time and gate fidelities in Rydberg atom quantum simulators and computers are fundamentally limited by the Rydberg state lifetime. Circular Rydberg states are highly promising candidates to overcome this limitation by orders of magnitude, as they can be effectively protected from decay due to their maximum angular momentum. We report the first realization of alkaline-earth circular Rydberg atoms trapped in optical tweezers, which provide unique and novel control possibilities due to the optically active ionic core. Specifically, we demonstrate creation of very high-$n$ ($n=79$) circular states of $^{88}$Sr. We measure lifetimes as long as 2.55 ms at room temperature, which are achieved via cavity-assisted suppression of black-body radiation. We show coherent control of a microwave qubit encoded in circular states of nearby manifolds, and characterize the qubit coherence time via Ramsey and spin-echo spectroscopy. Finally, circular state tweezer trapping exploiting the Sr$^+$ core polarizability is quantified via measurements of the trap-induced light shift on the qubit. Our work opens routes for quantum simulations with circular Rydberg states of divalent atoms, exploiting the emergent toolbox associated with the optically active core ion.","sentences":["Coherence time and gate fidelities in Rydberg atom quantum simulators and computers are fundamentally limited by the Rydberg state lifetime.","Circular Rydberg states are highly promising candidates to overcome this limitation by orders of magnitude, as they can be effectively protected from decay due to their maximum angular momentum.","We report the first realization of alkaline-earth circular Rydberg atoms trapped in optical tweezers, which provide unique and novel control possibilities due to the optically active ionic core.","Specifically, we demonstrate creation of very high-$n$ ($n=79$) circular states of $^{88}$Sr.","We measure lifetimes as long as 2.55 ms at room temperature, which are achieved via cavity-assisted suppression of black-body radiation.","We show coherent control of a microwave qubit encoded in circular states of nearby manifolds, and characterize the qubit coherence time via Ramsey and spin-echo spectroscopy.","Finally, circular state tweezer trapping exploiting the Sr$^+$ core polarizability is quantified via measurements of the trap-induced light shift on the qubit.","Our work opens routes for quantum simulations with circular Rydberg states of divalent atoms, exploiting the emergent toolbox associated with the optically active core ion."],"url":"http://arxiv.org/abs/2401.10625v1","category":"physics.atom-ph"}
{"created":"2024-01-19 11:04:14","title":"Quantum Computing Enhanced Service Ecosystem for Simulation in Manufacturing","abstract":"Quantum computing (QC) and machine learning (ML), taken individually or combined into quantum-assisted ML (QML), are ascending computing paradigms whose calculations come with huge potential for speedup, increase in precision, and resource reductions. Likely improvements for numerical simulations in engineering imply the possibility of a strong economic impact on the manufacturing industry. In this project report, we propose a framework for a quantum computing-enhanced service ecosystem for simulation in manufacturing, consisting of various layers ranging from hardware to algorithms to service and organizational layers. In addition, we give insight into the current state of the art of applications research based on QC and QML, both from a scientific and an industrial point of view. We further analyse two high-value use cases with the aim of a quantitative evaluation of these new computing paradigms for industrially-relevant settings.","sentences":["Quantum computing (QC) and machine learning (ML), taken individually or combined into quantum-assisted ML (QML), are ascending computing paradigms whose calculations come with huge potential for speedup, increase in precision, and resource reductions.","Likely improvements for numerical simulations in engineering imply the possibility of a strong economic impact on the manufacturing industry.","In this project report, we propose a framework for a quantum computing-enhanced service ecosystem for simulation in manufacturing, consisting of various layers ranging from hardware to algorithms to service and organizational layers.","In addition, we give insight into the current state of the art of applications research based on QC and QML, both from a scientific and an industrial point of view.","We further analyse two high-value use cases with the aim of a quantitative evaluation of these new computing paradigms for industrially-relevant settings."],"url":"http://arxiv.org/abs/2401.10623v1","category":"quant-ph"}
{"created":"2024-01-19 10:54:40","title":"Digital-Analog Quantum Computing and Algorithms","abstract":"This Thesis delves into the development and implementation of quantum algorithms using the digital-analog quantum computing (DAQC) paradigm. It provides a comparative analysis of the performance of DAQC versus traditional digital approaches, particularly in the presence of noise sources from current noisy intermediate-scale quantum (NISQ) devices. The DAQC paradigm combines the strengths of digital and analog quantum computing, offering greater efficiency and precision for implementing quantum algorithms on real hardware. The Thesis focuses on the comparison of four relevant quantum algorithms using digital and digital-analog approaches, and the results show significant advantages in favor of the latter. Furthermore, the Thesis investigates the cross-resonance effect to achieve efficient and high-precision Hamiltonian simulations. The findings indicate that the digital-analog paradigm is promising for practical quantum computing applications. Its ability to deliver greater efficiency and accuracy in implementing quantum algorithms on real hardware is a significant advantage over traditional digital approaches.","sentences":["This Thesis delves into the development and implementation of quantum algorithms using the digital-analog quantum computing (DAQC) paradigm.","It provides a comparative analysis of the performance of DAQC versus traditional digital approaches, particularly in the presence of noise sources from current noisy intermediate-scale quantum (NISQ) devices.","The DAQC paradigm combines the strengths of digital and analog quantum computing, offering greater efficiency and precision for implementing quantum algorithms on real hardware.","The Thesis focuses on the comparison of four relevant quantum algorithms using digital and digital-analog approaches, and the results show significant advantages in favor of the latter.","Furthermore, the Thesis investigates the cross-resonance effect to achieve efficient and high-precision Hamiltonian simulations.","The findings indicate that the digital-analog paradigm is promising for practical quantum computing applications.","Its ability to deliver greater efficiency and accuracy in implementing quantum algorithms on real hardware is a significant advantage over traditional digital approaches."],"url":"http://arxiv.org/abs/2401.10622v1","category":"quant-ph"}
{"created":"2024-01-19 10:52:57","title":"Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows","abstract":"With the advancement of neural networks, there has been a notable increase, both in terms of quantity and variety, in research publications concerning the application of autoencoders to reduced-order models. We propose a polytopic autoencoder architecture that includes a lightweight nonlinear encoder, a convex combination decoder, and a smooth clustering network. Supported by several proofs, the model architecture ensures that all reconstructed states lie within a polytope, accompanied by a metric indicating the quality of the constructed polytopes, referred to as polytope error. Additionally, it offers a minimal number of convex coordinates for polytopic linear-parameter varying systems while achieving acceptable reconstruction errors compared to proper orthogonal decomposition (POD). To validate our proposed model, we conduct simulations involving two flow scenarios with the incompressible Navier-Stokes equation. Numerical results demonstrate the guaranteed properties of the model, low reconstruction errors compared to POD, and the improvement in error using a clustering network.","sentences":["With the advancement of neural networks, there has been a notable increase, both in terms of quantity and variety, in research publications concerning the application of autoencoders to reduced-order models.","We propose a polytopic autoencoder architecture that includes a lightweight nonlinear encoder, a convex combination decoder, and a smooth clustering network.","Supported by several proofs, the model architecture ensures that all reconstructed states lie within a polytope, accompanied by a metric indicating the quality of the constructed polytopes, referred to as polytope error.","Additionally, it offers a minimal number of convex coordinates for polytopic linear-parameter varying systems while achieving acceptable reconstruction errors compared to proper orthogonal decomposition (POD).","To validate our proposed model, we conduct simulations involving two flow scenarios with the incompressible Navier-Stokes equation.","Numerical results demonstrate the guaranteed properties of the model, low reconstruction errors compared to POD, and the improvement in error using a clustering network."],"url":"http://arxiv.org/abs/2401.10620v1","category":"cs.LG"}
{"created":"2024-01-19 10:49:31","title":"LDA-based Term Profiles for Expert Finding in a Political Setting","abstract":"A common task in many political institutions (i.e. Parliament) is to find politicians who are experts in a particular field. In order to tackle this problem, the first step is to obtain politician profiles which include their interests, and these can be automatically learned from their speeches. As a politician may have various areas of expertise, one alternative is to use a set of subprofiles, each of which covers a different subject. In this study, we propose a novel approach for this task by using latent Dirichlet allocation (LDA) to determine the main underlying topics of each political speech, and to distribute the related terms among the different topic-based subprofiles. With this objective, we propose the use of fifteen distance and similarity measures to automatically determine the optimal number of topics discussed in a document, and to demonstrate that every measure converges into five strategies: Euclidean, Dice, Sorensen, Cosine and Overlap. Our experimental results showed that the scores of the different accuracy metrics of the proposed strategies tended to be higher than those of the baselines for expert recommendation tasks, and that the use of an appropriate number of topics has proved relevant.","sentences":["A common task in many political institutions (i.e. Parliament) is to find politicians who are experts in a particular field.","In order to tackle this problem, the first step is to obtain politician profiles which include their interests, and these can be automatically learned from their speeches.","As a politician may have various areas of expertise, one alternative is to use a set of subprofiles, each of which covers a different subject.","In this study, we propose a novel approach for this task by using latent Dirichlet allocation (LDA) to determine the main underlying topics of each political speech, and to distribute the related terms among the different topic-based subprofiles.","With this objective, we propose the use of fifteen distance and similarity measures to automatically determine the optimal number of topics discussed in a document, and to demonstrate that every measure converges into five strategies: Euclidean, Dice, Sorensen, Cosine and Overlap.","Our experimental results showed that the scores of the different accuracy metrics of the proposed strategies tended to be higher than those of the baselines for expert recommendation tasks, and that the use of an appropriate number of topics has proved relevant."],"url":"http://arxiv.org/abs/2401.10617v1","category":"cs.IR"}
{"created":"2024-01-19 10:46:21","title":"Goal-Oriented Multiple Access Connectivity for Networked Intelligent Systems","abstract":"We design a self-decision goal-oriented multiple access scheme, where sensing agents observe a common event and individually decide to communicate the event's attributes to the monitoring agents, to satisfy a certain goal. Decisions are based on the usefulness of contents, which are generated under uniform, change- and semantics-aware content acquisition, as well as statistics and contents of other agents. We obtain optimal activation probabilities and threshold criteria for decision-making under all schemes, maximizing a grade of effectiveness metric. Combined with a semantics-aware acquisition scheme, the self-decision scheme offers, on average, 29.52% higher effectiveness, 25.13% fewer drop-offs, and 67.21% fewer transmissions.","sentences":["We design a self-decision goal-oriented multiple access scheme, where sensing agents observe a common event and individually decide to communicate the event's attributes to the monitoring agents, to satisfy a certain goal.","Decisions are based on the usefulness of contents, which are generated under uniform, change- and semantics-aware content acquisition, as well as statistics and contents of other agents.","We obtain optimal activation probabilities and threshold criteria for decision-making under all schemes, maximizing a grade of effectiveness metric.","Combined with a semantics-aware acquisition scheme, the self-decision scheme offers, on average, 29.52% higher effectiveness, 25.13% fewer drop-offs, and 67.21% fewer transmissions."],"url":"http://arxiv.org/abs/2401.10614v1","category":"cs.IT"}
{"created":"2024-01-19 10:42:36","title":"Introducing the comfort performance gap in new educational buildings: a case study","abstract":"Providing adequate indoor environmental quality is crucial in educational settings. In this paper, we implemented and tested a framework that collects occupant feedback and investigated correlations between teachers comfort and the operational characteristics of an Austrian school building in September and October 2022. Initial results show that the measured average temperatures (23.1 t-25.1 deg C) in all rooms are on the upper limit of various recommendations, such as comfort guidelines for building operation or workplace regulations. This assessment is in line with the feedback we received from the teachers. A literature review demonstrated that childrens comfort temperatures are lower compared to adults. Hence, it is reasonable to conclude that indoor temperatures during the survey period were inadequate for the pupils either, even without direct feedback. An analysis of the CO2 measurements showed that, during school hours, approximately 20% of all measurement values were above 1000 ppm, with 2% above 1500 ppm. CO2 levels above 1000 ppm are considered hygienically critical, with the latest research proposing to lower the limits below 800 ppm to ensure a healthy and effective learning environment. While we only assessed the challenges of providing a healthy indoor environment for an educational building in Austria, our literature review shows similar challenges and research efforts worldwide. Our analysis demonstrates the need for adapting design requirements, especially for school buildings, acknowledging the different comfort needs of adults and children and the importance of high indoor air quality for providing an optimum learning environment. Future research should focus on testing adapted indoor environmental quality requirements for schools, especially in urban areas, and how to integrate real-time occupant feedback in the heating, ventilation and air conditioning systems.","sentences":["Providing adequate indoor environmental quality is crucial in educational settings.","In this paper, we implemented and tested a framework that collects occupant feedback and investigated correlations between teachers comfort and the operational characteristics of an Austrian school building in September and October 2022.","Initial results show that the measured average temperatures (23.1 t-25.1 deg C) in all rooms are on the upper limit of various recommendations, such as comfort guidelines for building operation or workplace regulations.","This assessment is in line with the feedback we received from the teachers.","A literature review demonstrated that childrens comfort temperatures are lower compared to adults.","Hence, it is reasonable to conclude that indoor temperatures during the survey period were inadequate for the pupils either, even without direct feedback.","An analysis of the CO2 measurements showed that, during school hours, approximately 20% of all measurement values were above 1000 ppm, with 2% above 1500 ppm.","CO2 levels above 1000 ppm are considered hygienically critical, with the latest research proposing to lower the limits below 800 ppm to ensure a healthy and effective learning environment.","While we only assessed the challenges of providing a healthy indoor environment for an educational building in Austria, our literature review shows similar challenges and research efforts worldwide.","Our analysis demonstrates the need for adapting design requirements, especially for school buildings, acknowledging the different comfort needs of adults and children and the importance of high indoor air quality for providing an optimum learning environment.","Future research should focus on testing adapted indoor environmental quality requirements for schools, especially in urban areas, and how to integrate real-time occupant feedback in the heating, ventilation and air conditioning systems."],"url":"http://arxiv.org/abs/2401.10612v1","category":"physics.soc-ph"}
{"created":"2024-01-19 10:42:29","title":"Publication venue recommendation using profiles based on clustering","abstract":"In this paper we study the venue recommendation problem in order to help researchers to identify a journal or conference to submit a given paper. A common approach to tackle this problem is to build profiles defining the scope of each venue. Then, these profiles are compared against the target paper. In our approach we will study how clustering techniques can be used to construct topic-based profiles and use an Information Retrieval based approach to obtain the final recommendations. Additionally, we will explore how the use of authorship, representing a complementary piece of information, helps to improve the recommendations.","sentences":["In this paper we study the venue recommendation problem in order to help researchers to identify a journal or conference to submit a given paper.","A common approach to tackle this problem is to build profiles defining the scope of each venue.","Then, these profiles are compared against the target paper.","In our approach we will study how clustering techniques can be used to construct topic-based profiles and use an Information Retrieval based approach to obtain the final recommendations.","Additionally, we will explore how the use of authorship, representing a complementary piece of information, helps to improve the recommendations."],"url":"http://arxiv.org/abs/2401.10611v1","category":"cs.IR"}
{"created":"2024-01-19 10:37:27","title":"M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images","abstract":"The advancement of Spatial Transcriptomics (ST) has facilitated the spatially-aware profiling of gene expressions based on histopathology images. Although ST data offers valuable insights into the micro-environment of tumors, its acquisition cost remains expensive. Therefore, directly predicting the ST expressions from digital pathology images is desired. Current methods usually adopt existing regression backbones for this task, which ignore the inherent multi-scale hierarchical data structure of digital pathology images. To address this limit, we propose M2ORT, a many-to-one regression Transformer that can accommodate the hierarchical structure of the pathology images through a decoupled multi-scale feature extractor. Different from traditional models that are trained with one-to-one image-label pairs, M2ORT accepts multiple pathology images of different magnifications at a time to jointly predict the gene expressions at their corresponding common ST spot, aiming at learning a many-to-one relationship through training. We have tested M2ORT on three public ST datasets and the experimental results show that M2ORT can achieve state-of-the-art performance with fewer parameters and floating-point operations (FLOPs). The code is available at: https://github.com/Dootmaan/M2ORT/.","sentences":["The advancement of Spatial Transcriptomics (ST) has facilitated the spatially-aware profiling of gene expressions based on histopathology images.","Although ST data offers valuable insights into the micro-environment of tumors, its acquisition cost remains expensive.","Therefore, directly predicting the ST expressions from digital pathology images is desired.","Current methods usually adopt existing regression backbones for this task, which ignore the inherent multi-scale hierarchical data structure of digital pathology images.","To address this limit, we propose M2ORT, a many-to-one regression Transformer that can accommodate the hierarchical structure of the pathology images through a decoupled multi-scale feature extractor.","Different from traditional models that are trained with one-to-one image-label pairs, M2ORT accepts multiple pathology images of different magnifications at a time to jointly predict the gene expressions at their corresponding common ST spot, aiming at learning a many-to-one relationship through training.","We have tested M2ORT on three public ST datasets and the experimental results show that M2ORT can achieve state-of-the-art performance with fewer parameters and floating-point operations (FLOPs).","The code is available at: https://github.com/Dootmaan/M2ORT/."],"url":"http://arxiv.org/abs/2401.10608v1","category":"cs.CV"}
{"created":"2024-01-19 10:32:28","title":"Use of topical and temporal profiles and their hybridisation for content-based recommendation","abstract":"In the context of content-based recommender systems, the aim of this paper is to determine how better profiles can be built and how these affect the recommendation process based on the incorporation of temporality, i.e. the inclusion of time in the recommendation process, and topicality, i.e. the representation of texts associated with users and items using topics and their combination. The main contribution of the paper is to present two different ways of hybridising these two dimensions and to evaluate and compare them with other alternatives.","sentences":["In the context of content-based recommender systems, the aim of this paper is to determine how better profiles can be built and how these affect the recommendation process based on the incorporation of temporality, i.e. the inclusion of time in the recommendation process, and topicality, i.e. the representation of texts associated with users and items using topics and their combination.","The main contribution of the paper is to present two different ways of hybridising these two dimensions and to evaluate and compare them with other alternatives."],"url":"http://arxiv.org/abs/2401.10607v1","category":"cs.IR"}
{"created":"2024-01-19 10:21:27","title":"ZnTrack -- Data as Code","abstract":"The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon. Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation. However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking. In this work, we introduce ZnTrack, a Python-driven data versioning tool. ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data. From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve. ZnTrack offers an open-source, FAIR data compatible Python package to enable users to harness these concepts of the future.","sentences":["The past decade has seen tremendous breakthroughs in computation and there is no indication that this will slow any time soon.","Machine learning, large-scale computing resources, and increased industry focus have resulted in rising investments in computer-driven solutions for data management, simulations, and model generation.","However, with this growth in computation has come an even larger expansion of data and with it, complexity in data storage, sharing, and tracking.","In this work, we introduce ZnTrack, a Python-driven data versioning tool.","ZnTrack builds upon established version control systems to provide a user-friendly and easy-to-use interface for tracking parameters in experiments, designing workflows, and storing and sharing data.","From this ability to reduce large datasets to a simple Python script emerges the concept of Data as Code, a core component of the work presented here and an undoubtedly important concept as the age of computation continues to evolve.","ZnTrack offers an open-source, FAIR data compatible Python package to enable users to harness these concepts of the future."],"url":"http://arxiv.org/abs/2401.10603v1","category":"cs.SE"}
{"created":"2024-01-19 10:20:57","title":"Fractional Conformal Map, Qubit Dynamics and the Leggett-Garg Inequality","abstract":"Any pure state of a qubit can be geometrically represented as a point on the extended complex plane through stereographic projection. By employing successive conformal maps on the extended complex plane, we can generate an effective discrete-time evolution of the pure states of the qubit. This work focuses on a subset of analytic maps known as fractional linear conformal maps. We show that these maps serve as a unifying framework for a diverse range of quantum-inspired conceivable dynamics, including (i) unitary dynamics,(ii) non-unitary but linear dynamics and (iii) non-unitary and non-linear dynamics where linearity (non-linearity) refers to the action of the discrete time evolution operator on the Hilbert space. We provide a characterization of these maps in terms of Leggett-Garg Inequality complemented with No-signaling in Time (NSIT) and Arrow of Time (AoT) conditions.","sentences":["Any pure state of a qubit can be geometrically represented as a point on the extended complex plane through stereographic projection.","By employing successive conformal maps on the extended complex plane, we can generate an effective discrete-time evolution of the pure states of the qubit.","This work focuses on a subset of analytic maps known as fractional linear conformal maps.","We show that these maps serve as a unifying framework for a diverse range of quantum-inspired conceivable dynamics, including (i) unitary dynamics,(ii) non-unitary but linear dynamics and (iii) non-unitary and non-linear dynamics where linearity (non-linearity) refers to the action of the discrete time evolution operator on the Hilbert space.","We provide a characterization of these maps in terms of Leggett-Garg Inequality complemented with No-signaling in Time (NSIT) and Arrow of Time (AoT) conditions."],"url":"http://arxiv.org/abs/2401.10602v1","category":"quant-ph"}
{"created":"2024-01-19 10:19:34","title":"Influential Slot and Tag Selection in Billboard Advertisement","abstract":"The selection of influential billboard slots remains an important problem in billboard advertisements. Existing studies on this problem have not considered the case of context-specific influence probability. To bridge this gap, in this paper, we introduce the Context Dependent Influential Billboard Slot Selection Problem. First, we show that the problem is NP-hard. We also show that the influence function holds the bi-monotonicity, bi-submodularity, and non-negativity properties. We propose an orthant-wise Stochastic Greedy approach to solve this problem. We show that this method leads to a constant factor approximation guarantee. Subsequently, we propose an orthant-wise Incremental and Lazy Greedy approach. In a generic sense, this is a method for maximizing a bi-submodular function under the cardinality constraint, which may also be of independent interest. We analyze the performance guarantee of this algorithm as well as time and space complexity. The proposed solution approaches have been implemented with real-world billboard and trajectory datasets. We compare the performance of our method with many baseline methods, and the results are reported. Our proposed orthant-wise stochastic greedy approach leads to significant results when the parameters are set properly with reasonable computational overhead.","sentences":["The selection of influential billboard slots remains an important problem in billboard advertisements.","Existing studies on this problem have not considered the case of context-specific influence probability.","To bridge this gap, in this paper, we introduce the Context Dependent Influential Billboard Slot Selection Problem.","First, we show that the problem is NP-hard.","We also show that the influence function holds the bi-monotonicity, bi-submodularity, and non-negativity properties.","We propose an orthant-wise Stochastic Greedy approach to solve this problem.","We show that this method leads to a constant factor approximation guarantee.","Subsequently, we propose an orthant-wise Incremental and Lazy Greedy approach.","In a generic sense, this is a method for maximizing a bi-submodular function under the cardinality constraint, which may also be of independent interest.","We analyze the performance guarantee of this algorithm as well as time and space complexity.","The proposed solution approaches have been implemented with real-world billboard and trajectory datasets.","We compare the performance of our method with many baseline methods, and the results are reported.","Our proposed orthant-wise stochastic greedy approach leads to significant results when the parameters are set properly with reasonable computational overhead."],"url":"http://arxiv.org/abs/2401.10601v1","category":"cs.DS"}
{"created":"2024-01-19 10:02:20","title":"Adversarially Robust Signed Graph Contrastive Learning from Balance Augmentation","abstract":"Signed graphs consist of edges and signs, which can be separated into structural information and balance-related information, respectively. Existing signed graph neural networks (SGNNs) typically rely on balance-related information to generate embeddings. Nevertheless, the emergence of recent adversarial attacks has had a detrimental impact on the balance-related information. Similar to how structure learning can restore unsigned graphs, balance learning can be applied to signed graphs by improving the balance degree of the poisoned graph. However, this approach encounters the challenge \"Irreversibility of Balance-related Information\" - while the balance degree improves, the restored edges may not be the ones originally affected by attacks, resulting in poor defense effectiveness. To address this challenge, we propose a robust SGNN framework called Balance Augmented-Signed Graph Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning principles with balance augmentation techniques. Experimental results demonstrate that BA-SGCL not only enhances robustness against existing adversarial attacks but also achieves superior performance on link sign prediction task across various datasets.","sentences":["Signed graphs consist of edges and signs, which can be separated into structural information and balance-related information, respectively.","Existing signed graph neural networks (SGNNs) typically rely on balance-related information to generate embeddings.","Nevertheless, the emergence of recent adversarial attacks has had a detrimental impact on the balance-related information.","Similar to how structure learning can restore unsigned graphs, balance learning can be applied to signed graphs by improving the balance degree of the poisoned graph.","However, this approach encounters the challenge \"Irreversibility of Balance-related Information\" - while the balance degree improves, the restored edges may not be the ones originally affected by attacks, resulting in poor defense effectiveness.","To address this challenge, we propose a robust SGNN framework called Balance Augmented-Signed Graph Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning principles with balance augmentation techniques.","Experimental results demonstrate that BA-SGCL not only enhances robustness against existing adversarial attacks but also achieves superior performance on link sign prediction task across various datasets."],"url":"http://arxiv.org/abs/2401.10590v1","category":"cs.LG"}
{"created":"2024-01-19 09:59:02","title":"Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local Search Solvers","abstract":"MaxSAT is an optimization version of the famous NP-complete Satisfiability problem (SAT). Algorithms for MaxSAT mainly include complete solvers and local search incomplete solvers. In many complete solvers, once a better solution is found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to enforce the algorithm to find better solutions. In many local search algorithms, clause weighting is a key technique for effectively guiding the search directions. In this paper, we propose to transfer the SPB constraint into the clause weighting system of the local search method, leading the algorithm to better solutions. We further propose an adaptive clause weighting strategy that breaks the tradition of using constant values to adjust clause weights. Based on the above methods, we propose a new local search algorithm called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT local search solvers. Extensive experiments demonstrate the excellent performance of the proposed methods.","sentences":["MaxSAT is an optimization version of the famous NP-complete Satisfiability problem (SAT).","Algorithms for MaxSAT mainly include complete solvers and local search incomplete solvers.","In many complete solvers, once a better solution is found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to enforce the algorithm to find better solutions.","In many local search algorithms, clause weighting is a key technique for effectively guiding the search directions.","In this paper, we propose to transfer the SPB constraint into the clause weighting system of the local search method, leading the algorithm to better solutions.","We further propose an adaptive clause weighting strategy that breaks the tradition of using constant values to adjust clause weights.","Based on the above methods, we propose a new local search algorithm called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT local search solvers.","Extensive experiments demonstrate the excellent performance of the proposed methods."],"url":"http://arxiv.org/abs/2401.10589v1","category":"cs.AI"}
{"created":"2024-01-19 09:58:06","title":"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval","abstract":"Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL","sentences":["Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query.","Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem.","To address this challenge, prompt tuning has emerged as an alternative.","However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information.","(2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap.","To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention.","In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction.","Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning.","Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.","Code will be available at https://github.com/knightyxp/DGL"],"url":"http://arxiv.org/abs/2401.10588v1","category":"cs.CV"}
{"created":"2024-01-19 09:54:23","title":"PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks","abstract":"Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defense mechanism, demonstrating significant improvements in robustness against query-based attacks.","sentences":["Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters.","Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs.","To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost.","These models leverage the local implicit function and rebuild the natural image manifold.","Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications.","Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defense mechanism, demonstrating significant improvements in robustness against query-based attacks."],"url":"http://arxiv.org/abs/2401.10586v1","category":"cs.CR"}
{"created":"2024-01-19 09:51:15","title":"Fast winning strategies for the attacker in eternal domination","abstract":"Dominating sets in graphs are often used to model some monitoring of the graph: guards are posted on the vertices of the dominating set, and they can thus react to attacks occurring on the unguarded vertices by moving there (yielding a new set of guards, which may not be dominating anymore). A dominating set is eternal if it can endlessly resist to attacks. From the attacker's perspective, if we are given a non-eternal dominating set, the question is to determine how fast can we provoke an attack that cannot be handled by a neighboring guard. We investigate this question from a computational complexity point of view, by showing that this question is PSPACE-hard, even for graph classes where finding a minimum eternal dominating set is in P. We then complement this result by giving polynomial time algorithms for cographs and trees, and showing a connection with tree-depth for the latter. We also investigate the problem from a parameterized complexity perspective, mainly considering two parameters: the number of guards and the number of steps.","sentences":["Dominating sets in graphs are often used to model some monitoring of the graph: guards are posted on the vertices of the dominating set, and they can thus react to attacks occurring on the unguarded vertices by moving there (yielding a new set of guards, which may not be dominating anymore).","A dominating set is eternal if it can endlessly resist to attacks.","From the attacker's perspective, if we are given a non-eternal dominating set, the question is to determine how fast can we provoke an attack that cannot be handled by a neighboring guard.","We investigate this question from a computational complexity point of view, by showing that this question is PSPACE-hard, even for graph classes where finding a minimum eternal dominating set is in P.","We then complement this result by giving polynomial time algorithms for cographs and trees, and showing a connection with tree-depth for the latter.","We also investigate the problem from a parameterized complexity perspective, mainly considering two parameters: the number of guards and the number of steps."],"url":"http://arxiv.org/abs/2401.10584v1","category":"cs.DM"}
{"created":"2024-01-19 09:49:53","title":"Exploiting Kubernetes' Image Pull Implementation to Deny Node Availability","abstract":"Kubernetes (K8s) has grown in popularity over the past few years to become the de-facto standard for container orchestration in cloud-native environments. While research is not new to topics such as containerization and access control security, the Application Programming Interface (API) interactions between K8s and its runtime interfaces have not been studied thoroughly. In particular, the CRI-API is responsible for abstracting the container runtime, managing the creation and lifecycle of containers along with the downloads of the respective images. However, this decoupling of concerns and the abstraction of the container runtime renders K8s unaware of the status of the downloading process of the container images, obstructing the monitoring of the resources allocated to such process. In this paper, we discuss how this lack of status information can be exploited as a Denial of Service attack in a K8s cluster. We show that such attacks can generate up to 95% average CPU usage, prevent downloading new container images, and increase I/O and network usage for a potentially unlimited amount of time. Finally, we propose two possible mitigation strategies: one, implemented as a stopgap solution, and another, requiring more radical architectural changes in the relationship between K8s and the CRI-API.","sentences":["Kubernetes (K8s) has grown in popularity over the past few years to become the de-facto standard for container orchestration in cloud-native environments.","While research is not new to topics such as containerization and access control security, the Application Programming Interface (API) interactions between K8s and its runtime interfaces have not been studied thoroughly.","In particular, the CRI-API is responsible for abstracting the container runtime, managing the creation and lifecycle of containers along with the downloads of the respective images.","However, this decoupling of concerns and the abstraction of the container runtime renders K8s unaware of the status of the downloading process of the container images, obstructing the monitoring of the resources allocated to such process.","In this paper, we discuss how this lack of status information can be exploited as a Denial of Service attack in a K8s cluster.","We show that such attacks can generate up to 95% average CPU usage, prevent downloading new container images, and increase I/O and network usage for a potentially unlimited amount of time.","Finally, we propose two possible mitigation strategies: one, implemented as a stopgap solution, and another, requiring more radical architectural changes in the relationship between K8s and the CRI-API."],"url":"http://arxiv.org/abs/2401.10582v1","category":"cs.CR"}
{"created":"2024-01-19 09:47:44","title":"Co-propagation of Classical and Continuous-variable QKD Signals over a Turbulent Optical Channel with a Real-time QKD Receiver","abstract":"We demonstrate classical and quantum signal co-propagation over a turbulent free-space channel with 3 Tbit/s throughput and record 2.7 Mbit/s secret-key rate. Our real-time GPU-based receiver assessed quantum signal integrity under different turbulence scenarios for the first time.","sentences":["We demonstrate classical and quantum signal co-propagation over a turbulent free-space channel with 3 Tbit/s throughput and record 2.7 Mbit/s secret-key rate.","Our real-time GPU-based receiver assessed quantum signal integrity under different turbulence scenarios for the first time."],"url":"http://arxiv.org/abs/2401.10581v1","category":"quant-ph"}
