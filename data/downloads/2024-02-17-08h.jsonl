{"created":"2024-02-14 18:59:33","title":"AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability","abstract":"This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.","sentences":["This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS).","The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves.","We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs.","Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs.","(2) Naively providing interactive examples may inadvertently hurt few-shot performance.","(3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance.","(4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend.","We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning.","The code is available at https://github.com/UCSC-VLAA/AQA-Bench."],"url":"http://arxiv.org/abs/2402.09404v1","category":"cs.CL"}
{"created":"2024-02-14 18:58:40","title":"Reinforcement Learning from Human Feedback with Active Queries","abstract":"Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.","sentences":["Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF).","Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect.","In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods.","We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\\Delta$ is the sub-optimality gap over all the contexts.","We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs.","Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method."],"url":"http://arxiv.org/abs/2402.09401v1","category":"cs.LG"}
{"created":"2024-02-14 18:54:56","title":"Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference","abstract":"Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.","sentences":["Many computational factors limit broader deployment of large language models.","In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding.","While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens.","To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps.","Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient."],"url":"http://arxiv.org/abs/2402.09398v1","category":"cs.LG"}
{"created":"2024-02-14 18:43:19","title":"LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning","abstract":"Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users' quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time. However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality. Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption. In this paper, we propose LL-GABR, a deep reinforcement learning approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics like latency, rebuffering events, and smoothness. LL-GABR makes no assumptions about the underlying video, environment, or network settings and can operate flexibly on different video titles, each having a different bitrate encoding ladder without additional re-training, unlike existing learning-based ABRs. Trace-driven experimental results show that LL-GABR outperforms the state-of-the-art approaches by up to 44% in terms of perceptual QoE and a 73% increase in energy efficiency as a result of reducing net energy consumption by 11%.","sentences":["Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users' quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time.","However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality.","Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption.","In this paper, we propose LL-GABR, a deep reinforcement learning approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics like latency, rebuffering events, and smoothness.","LL-GABR makes no assumptions about the underlying video, environment, or network settings and can operate flexibly on different video titles, each having a different bitrate encoding ladder without additional re-training, unlike existing learning-based ABRs.","Trace-driven experimental results show that LL-GABR outperforms the state-of-the-art approaches by up to 44% in terms of perceptual QoE and a 73% increase in energy efficiency as a result of reducing net energy consumption by 11%."],"url":"http://arxiv.org/abs/2402.09392v1","category":"cs.MM"}
{"created":"2024-02-14 18:42:25","title":"LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset","abstract":"Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. We further conduct analysis on the impact of trainable parameters, providing insights for future research.","sentences":["Chemistry plays a crucial role in many domains, such as drug discovery and material science.","While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low.","In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models.","The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct.","It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry.","Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks.","We further conduct analysis on the impact of trainable parameters, providing insights for future research."],"url":"http://arxiv.org/abs/2402.09391v1","category":"cs.AI"}
{"created":"2024-02-14 18:41:19","title":"HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation","abstract":"With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts. Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module's ranking. Experiments reveal that HGOT outperforms other retrieval-augmented in-context learning methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its efficacy in enhancing the factuality of LLMs.","sentences":["With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns.","To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning.","The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries.","It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality.","This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts.","Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module's ranking.","Experiments reveal that HGOT outperforms other retrieval-augmented in-context learning methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7\\%$, demonstrating its efficacy in enhancing the factuality of LLMs."],"url":"http://arxiv.org/abs/2402.09390v1","category":"cs.AI"}
{"created":"2024-02-14 18:37:47","title":"Entropy-regularized Point-based Value Iteration","abstract":"Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference. However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior. Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems. Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary. We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains. Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference.","sentences":["Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference.","However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior.","Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems.","Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary.","We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains.","Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference."],"url":"http://arxiv.org/abs/2402.09388v1","category":"cs.AI"}
{"created":"2024-02-14 18:32:30","title":"Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions","abstract":"A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a \"human-in-the-loop\" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.","sentences":["A principal designs an algorithm that generates a publicly observable prediction of a binary state.","She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment.","We study the optimal design of the prediction algorithm and the delegation rule in such environments.","Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information.","(2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction.","Instead, the optimal algorithm may provide more information about one state and restrict information about the other.","(3) Common restrictions on algorithms, such as keeping a \"human-in-the-loop\" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals.","These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers."],"url":"http://arxiv.org/abs/2402.09384v1","category":"econ.TH"}
{"created":"2024-02-14 18:24:27","title":"Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints","abstract":"Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources. Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results. In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented. After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected. With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented. The solution was submitted to the FaaSDom benchmark and time metrics were collected. Additionally, the solution was characterized in terms of the Serverless Trilemma. The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture.","sentences":["Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources.","Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results.","In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented.","After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected.","With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented.","The solution was submitted to the FaaSDom benchmark and time metrics were collected.","Additionally, the solution was characterized in terms of the Serverless Trilemma.","The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture."],"url":"http://arxiv.org/abs/2402.09377v1","category":"cs.DC"}
{"created":"2024-02-14 18:18:33","title":"Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge","abstract":"Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts. Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future. As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website. As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques. The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis.","sentences":["Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans.","While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms.","To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental).","The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric.","During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary.","The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts.","Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future.","As an active benchmark and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website.","As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques.","The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis."],"url":"http://arxiv.org/abs/2402.09372v1","category":"eess.IV"}
{"created":"2024-02-14 18:18:29","title":"Transformers Can Achieve Length Generalization But Not Robustly","abstract":"Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.","sentences":["Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models.","This issue persists even with large-scale Transformers handling relatively straightforward tasks.","In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers.","We show that the success of length generalization is intricately linked to the data format and the type of position encoding.","Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length.","Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds."],"url":"http://arxiv.org/abs/2402.09371v1","category":"cs.LG"}
{"created":"2024-02-14 18:17:45","title":"Pseudorandom Error-Correcting Codes","abstract":"We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.   Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.","sentences":["We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary.","Efficient decoding of corrupted codewords is possible with the help of a decoding key.   ","We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions.","Specifically, pseudorandomness is based on either $2^{O(\\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   ","As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions.","The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model.","This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.   ","Our second application is to steganography, where a secret message is hidden in innocent-looking content.","We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions.","Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors."],"url":"http://arxiv.org/abs/2402.09370v1","category":"cs.CR"}
{"created":"2024-02-14 18:16:54","title":"Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking","abstract":"Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.","sentences":["Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions.","Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition.","Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages.","Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction.","Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models.","Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain."],"url":"http://arxiv.org/abs/2402.09369v1","category":"cs.CL"}
{"created":"2024-02-14 18:13:51","title":"Magic-Me: Identity-Specific Video Customized Diffusion","abstract":"Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.","sentences":["Creating content for a specific identity (ID) has shown significant interest in the field of generative models.","In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable.","However, extending it to video generation is not well explored.","In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD).","With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent.","To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   ","Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines.","Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability.","The codes are available at https://github.com/Zhen-Dong/Magic-Me."],"url":"http://arxiv.org/abs/2402.09368v1","category":"cs.CV"}
{"created":"2024-02-14 18:13:37","title":"Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning","abstract":"Microbial communities play a key role in biological wastewater treatment processes. Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs). Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts. This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images. Implementing the transfer learning of deep convolutional neural network (CNN) models, this approach aims to overcome the limitations of existing quantitative image analysis techniques. The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium. Multiple data augmentation techniques were employed to enhance the generalizability of the CNN models. Various CNN architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics. The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice. The results showed that the suggested CNN-based approach provides less labour-intensive, objective, and consistent assessments, while transfer learning notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications.","sentences":["Microbial communities play a key role in biological wastewater treatment processes.","Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs).","Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts.","This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images.","Implementing the transfer learning of deep convolutional neural network (CNN) models, this approach aims to overcome the limitations of existing quantitative image analysis techniques.","The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium.","Multiple data augmentation techniques were employed to enhance the generalizability of the CNN models.","Various CNN architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics.","The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice.","The results showed that the suggested CNN-based approach provides less labour-intensive, objective, and consistent assessments, while transfer learning notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications."],"url":"http://arxiv.org/abs/2402.09367v1","category":"cs.CV"}
{"created":"2024-02-14 18:04:36","title":"HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference","abstract":"Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\\times$ on a single TPUv5e device.","sentences":["Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache.","On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where","$k \\approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency.","However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains.","To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation).","HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator.","We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\\times$ on a single TPUv5e device."],"url":"http://arxiv.org/abs/2402.09360v1","category":"cs.LG"}
{"created":"2024-02-14 18:03:58","title":"Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy","abstract":"Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.","sentences":["Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns.","ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream.","Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes.","ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually.","Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements.","Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D.","We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information.","When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks.","In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings.","We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time."],"url":"http://arxiv.org/abs/2402.09359v1","category":"eess.IV"}
{"created":"2024-02-14 18:02:24","title":"Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis","abstract":"This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.","sentences":["This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy.","By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies.","The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators.","These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision."],"url":"http://arxiv.org/abs/2402.09358v1","category":"cs.AI"}
{"created":"2024-02-14 17:59:47","title":"Single-Reset Divide & Conquer Imitation Learning","abstract":"Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets. To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states. In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state. Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II. In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption.","sentences":["Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms.","To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration.","In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration.","The latest version, DCIL-II demonstrates remarkable sample efficiency.","This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration.","However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems.","In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets.","To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states.","In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state.","Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II.","In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption."],"url":"http://arxiv.org/abs/2402.09355v1","category":"cs.RO"}
{"created":"2024-02-14 17:49:31","title":"Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop","abstract":"As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. The criteria for generating and applying auditing probes is generalizable to various LLMs regardless of the underlying structure or training mechanism.","sentences":["As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential.","Examples include bias, inconsistencies, and hallucination.","Although auditing the LLM for these problems is desirable, it is far from being easy or solved.","An effective method is to probe the LLM using different versions of the same question.","This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination.","However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically.","In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop.","This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability.","Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes.","Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM.","The criteria for generating and applying auditing probes is generalizable to various LLMs regardless of the underlying structure or training mechanism."],"url":"http://arxiv.org/abs/2402.09346v1","category":"cs.AI"}
{"created":"2024-02-14 17:49:07","title":"Mitigating Reward Hacking via Information-Theoretic Reward Modeling","abstract":"Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of RLHF. Code will be released upon acceptance.","sentences":["Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset.","In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation.","Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization.","Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies.","Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM.","Further analyses reveal that InfoRM's overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of RLHF.","Code will be released upon acceptance."],"url":"http://arxiv.org/abs/2402.09345v2","category":"cs.LG"}
{"created":"2024-02-14 17:31:04","title":"AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach","abstract":"As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by different people. Based on this assumption, AuditLLM produces easily interpretable results regarding the LLM's consistencies from a single question that the user enters. A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues. One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM. To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis. This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform.","sentences":["As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand.","This may require probing or auditing them.","Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality.","However, a tool for performing such audits with simple workflow and low technical threshold is lacking.","In this demo, we introduce \"AuditLLM,\" a novel tool designed to evaluate the performance of various LLMs in a methodical way.","AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation.","A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by different people.","Based on this assumption, AuditLLM produces easily interpretable results regarding the LLM's consistencies from a single question that the user enters.","A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues.","One could then use the output of AuditLLM to further investigate issues with the aforementioned LLM.","To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of LLMs by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive LLM auditing by processing multiple queries at once for in-depth analysis.","This tool is beneficial for both researchers and general users, as it enhances our understanding of LLMs' capabilities in generating responses, using a standardized auditing platform."],"url":"http://arxiv.org/abs/2402.09334v1","category":"cs.AI"}
{"created":"2024-02-14 17:14:34","title":"ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization","abstract":"Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.","sentences":["Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content.","Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods.","However, these methods do not essentially enhance the LLM itself.","In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL).","Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO).","It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance.","ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits.","Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA.","We also conduct detailed analyses to offer comprehensive insights into ICDPO."],"url":"http://arxiv.org/abs/2402.09320v1","category":"cs.CL"}
{"created":"2024-02-14 17:13:36","title":"Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio","abstract":"We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve most of the performance achieved with the autoencoder embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier.","sentences":["We present PECMAE, an interpretable model for music audio classification based on prototype learning.","Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network.","Instead, we propose to decouple both training processes.","This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization.","APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples.","In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency.","We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before.","We find that the prototype-based models preserve most of the performance achieved with the autoencoder embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier."],"url":"http://arxiv.org/abs/2402.09318v1","category":"cs.SD"}
{"created":"2024-02-14 16:49:13","title":"Embracing the black box: Heading towards foundation models for causal discovery from time series data","abstract":"Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We argue that this hints at the possibility of a foundation model for causal discovery.","sentences":["Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques.","However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning.","To address this gap, we explore what we call Causal Pretraining.","A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner.","Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics.","More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics.","Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits.","We argue that this hints at the possibility of a foundation model for causal discovery."],"url":"http://arxiv.org/abs/2402.09305v1","category":"cs.LG"}
{"created":"2024-02-14 16:47:20","title":"Immediate generalisation in humans but a generalisation lag in deep neural networks$\\unicode{x2014}$evidence for representational divergence?","abstract":"Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\\unicode{x2014}$and sometimes even exceeding that$\\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field. However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data.","sentences":["Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification.","Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed.","However, the process of how these representations emerge$\\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\\unicode{x2014}$is less often directly and empirically compared.   ","Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs.","We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided.","Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data.   ","Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to$\\unicode{x2014}$and sometimes even exceeding that$\\unicode{x2014}$of human learners, challenging some prevailing assumptions in the field.","However, comparisons across the entire learning process reveal significant representational differences: while DNNs' learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data."],"url":"http://arxiv.org/abs/2402.09303v1","category":"cs.CV"}
{"created":"2024-02-14 16:41:35","title":"Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code","abstract":"Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.","sentences":["Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources.","The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing.","The dataset for training these models is mainly collected from publicly available sources.","This raises the issue of intellectual property infringement as developers' codes are already included in the dataset.","Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models.","Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement.","To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM's training dataset.","We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion.","In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM.","In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%.","In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets."],"url":"http://arxiv.org/abs/2402.09299v1","category":"cs.SE"}
{"created":"2024-02-14 16:23:23","title":"Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning","abstract":"Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed.","sentences":["Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery.","Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state.","We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework.","At the heart of PSRL is the fusion of both supervised and unsupervised learning.","The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time.","This yields more interpretable policies that compose state predictions with control.","In parallel, it captures an unsupervised latent representation.","These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network.","This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights.","Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed."],"url":"http://arxiv.org/abs/2402.09290v1","category":"cs.LG"}
{"created":"2024-02-14 16:19:09","title":"Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research","abstract":"Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans. In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population. Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values. This framework allows general users to assess the validity and biases of a model without diving into technical model documentation. Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model. We demonstrate the ease of accessing the appropriate information when the data is structured appropriately. Discussion: The Model Facts template is limited in its current form to human based data and biases. Like nutrition facts, it also will require some educational resources for users to grasp its full utility. Human computer interaction experiments should be conducted to ensure that the interaction between user interface and model interface is as desired. Conclusion: The Model Facts label is the first framework dedicated to establishing trust with end users and general population consumers. Implementation of Model Facts into firearm injury research will provide public health practitioners and those impacted by firearm injury greater faith in the tools the research provides.","sentences":["Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans.","In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population.","Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values.","This framework allows general users to assess the validity and biases of a model without diving into technical model documentation.","Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model.","We demonstrate the ease of accessing the appropriate information when the data is structured appropriately.","Discussion:","The Model Facts template is limited in its current form to human based data and biases.","Like nutrition facts, it also will require some educational resources for users to grasp its full utility.","Human computer interaction experiments should be conducted to ensure that the interaction between user interface and model interface is as desired.","Conclusion: The Model Facts label is the first framework dedicated to establishing trust with end users and general population consumers.","Implementation of Model Facts into firearm injury research will provide public health practitioners and those impacted by firearm injury greater faith in the tools the research provides."],"url":"http://arxiv.org/abs/2402.09286v1","category":"cs.AI"}
{"created":"2024-02-14 16:10:42","title":"Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification","abstract":"Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms established methods. Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each. This comprehensive approach captures intricate patterns and relationships, enhancing classification performance. Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions. Our method also surpasses kernel-based methods and manifold learning techniques in performance. Additionally, our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space.","sentences":["Covariance and Hessian matrices have been analyzed separately in the literature for classification problems.","However, integrating these matrices has the potential to enhance their combined power in improving classification performance.","We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks.","Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances.","By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria.","Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our method outperforms established methods.","Our method stands out by addressing both LDA criteria, unlike PCA and the Hessian method, which predominantly emphasize one criterion each.","This comprehensive approach captures intricate patterns and relationships, enhancing classification performance.","Furthermore, through the utilization of both LDA criteria, our method outperforms LDA itself by leveraging higher-dimensional feature spaces, in accordance with Cover's theorem, which favors linear separability in higher dimensions.","Our method also surpasses kernel-based methods and manifold learning techniques in performance.","Additionally, our approach sheds light on complex DNN decision-making, rendering them comprehensible within a 2D space."],"url":"http://arxiv.org/abs/2402.09281v1","category":"cs.LG"}
{"created":"2024-02-14 16:09:34","title":"Nonreciprocal collective dynamics in a mixture of phoretic Janus colloids","abstract":"A multicomponent mixture of Janus colloids with distinct catalytic coats and phoretic mobilities is a promising theoretical system to explore the collective behavior arising from nonreciprocal interactions. An active colloid produces (or consumes) chemicals, self-propels, drifts along chemical gradients, and rotates its intrinsic polarity to align with a gradient. As a result the connection from microscopics to continuum theories through coarse-graining couples densities and polarization fields in unique ways. Focusing on a binary mixture, we show that these couplings render the unpatterned reference state unstable to small perturbations through a variety of instabilities including oscillatory ones which arise on crossing an exceptional point or through a Hopf bifurcation. For fast relaxation of the polar fields, they can be eliminated in favor of the density fields to obtain a microscopic realization of the Nonreciprocal Cahn-Hilliard model for two conserved species with two distinct sources of non-reciprocity, one in the interaction coefficient and the other in the interfacial tension. Our work establishes Janus colloids as a versatile model for a bottom-up approach to both scalar and polar active mixtures.","sentences":["A multicomponent mixture of Janus colloids with distinct catalytic coats and phoretic mobilities is a promising theoretical system to explore the collective behavior arising from nonreciprocal interactions.","An active colloid produces (or consumes) chemicals, self-propels, drifts along chemical gradients, and rotates its intrinsic polarity to align with a gradient.","As a result the connection from microscopics to continuum theories through coarse-graining couples densities and polarization fields in unique ways.","Focusing on a binary mixture, we show that these couplings render the unpatterned reference state unstable to small perturbations through a variety of instabilities including oscillatory ones which arise on crossing an exceptional point or through a Hopf bifurcation.","For fast relaxation of the polar fields, they can be eliminated in favor of the density fields to obtain a microscopic realization of the Nonreciprocal Cahn-Hilliard model for two conserved species with two distinct sources of non-reciprocity, one in the interaction coefficient and the other in the interfacial tension.","Our work establishes Janus colloids as a versatile model for a bottom-up approach to both scalar and polar active mixtures."],"url":"http://arxiv.org/abs/2402.09279v1","category":"cond-mat.soft"}
{"created":"2024-02-14 15:55:30","title":"Personalized Large Language Models","abstract":"Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.","sentences":["Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years.","However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots.","This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks.","Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models.","Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures.","These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks."],"url":"http://arxiv.org/abs/2402.09269v1","category":"cs.CL"}
{"created":"2024-02-14 15:52:42","title":"Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation","abstract":"Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.","sentences":["Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. \"hallucinations\", even when they hold relevant knowledge.","To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations.","In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality.","Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge.","Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration.","We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm.","We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN."],"url":"http://arxiv.org/abs/2402.09267v1","category":"cs.CL"}
{"created":"2024-02-14 15:51:58","title":"Machine Learning in management of precautionary closures caused by lipophilic biotoxins","abstract":"Mussel farming is one of the most important aquaculture industries. The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption. In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program. In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied. These decisions are made by experts without the support or formalisation of the experience on which they are based. Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures. Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results. This allows the creation of a system capable of helping in complex situations where forecast errors are more common.","sentences":["Mussel farming is one of the most important aquaculture industries.","The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption.","In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program.","In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied.","These decisions are made by experts without the support or formalisation of the experience on which they are based.","Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures.","Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results.","This allows the creation of a system capable of helping in complex situations where forecast errors are more common."],"url":"http://arxiv.org/abs/2402.09266v1","category":"cs.AI"}
{"created":"2024-02-14 15:51:55","title":"Computational Complexity of Preferred Subset Repairs on Data-Graphs","abstract":"The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data. However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it. Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently. In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints. We present several preference criteria based on the standard subset repair semantics, incorporating weights, multisets, and set-based priority levels. We study the most common repairing tasks, showing that it is possible to maintain the same computational complexity as in the case where no preference criterion is available for exploitation. To complete the picture, we explore the complexity of consistent query answering in this setting and obtain tight lower and upper bounds for all the preference criteria introduced.","sentences":["The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data.","However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it.","Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently.","In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints.","We present several preference criteria based on the standard subset repair semantics, incorporating weights, multisets, and set-based priority levels.","We study the most common repairing tasks, showing that it is possible to maintain the same computational complexity as in the case where no preference criterion is available for exploitation.","To complete the picture, we explore the complexity of consistent query answering in this setting and obtain tight lower and upper bounds for all the preference criteria introduced."],"url":"http://arxiv.org/abs/2402.09265v1","category":"cs.DB"}
{"created":"2024-02-14 15:45:56","title":"SyntaxShap: Syntax-aware Explainability Method for Text Generation","abstract":"To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models.","sentences":["To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions.","However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data.","This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data.","The presented work extends Shapley values to account for parsing-based syntactic dependencies.","Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree.","We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model.","We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models."],"url":"http://arxiv.org/abs/2402.09259v1","category":"cs.CL"}
{"created":"2024-02-14 15:40:31","title":"On collection schemes and Gaifman's splitting theorem","abstract":"We study model theoretic characterizations of various collection schemes over $\\mathbf{PA}^-$ from the viewpoint of Gaifman's splitting theorem.","sentences":["We study model theoretic characterizations of various collection schemes over $\\mathbf{PA}^-$ from the viewpoint of Gaifman's splitting theorem."],"url":"http://arxiv.org/abs/2402.09255v1","category":"math.LO"}
{"created":"2024-02-14 15:38:56","title":"Universal Machine Learning Kohn-Sham Hamiltonian for Materials","abstract":"While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist. Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations. Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist. Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project. We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems. By offering a reliable efficient framework for computing electronic properties, this universal Hamiltonian model lays the groundwork for advancements in diverse fields related to electronic structures.","sentences":["While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist.","Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations.","Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist.","Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project.","We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems.","By offering a reliable efficient framework for computing electronic properties, this universal Hamiltonian model lays the groundwork for advancements in diverse fields related to electronic structures."],"url":"http://arxiv.org/abs/2402.09251v1","category":"physics.comp-ph"}
{"created":"2024-02-14 15:34:38","title":"Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots","abstract":"We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium.","sentences":["We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game.","We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations.","To solve the problem, we introduce Branch and Play (B&P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium.","As a subroutine for B&P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play.","We demonstrate the practical utility of B&P to coordinate air traffic control, swarm formation, and delivery vehicle fleets.","We find that B&P consistently outperforms various baselines, and computes the socially optimal equilibrium."],"url":"http://arxiv.org/abs/2402.09246v1","category":"cs.RO"}
{"created":"2024-02-14 15:32:35","title":"Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection","abstract":"Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.","sentences":["Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health.","As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants.","Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations.","Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable.","The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories.","To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes.","Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features.","Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion.","Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector.","Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS.","Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD.","Code and dataset are available at https://github.com/LanceZPF/KEFS."],"url":"http://arxiv.org/abs/2402.09242v1","category":"cs.CV"}
{"created":"2024-02-14 15:23:59","title":"Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models","abstract":"To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.","sentences":["To build intelligent machine learning systems, there are two broad approaches.","One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning.","The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work.","In this work, we relate these two approaches and study how to learn human-interpretable concepts from data.","Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data.","Experiments on synthetic data and large language models show the utility of our unified approach."],"url":"http://arxiv.org/abs/2402.09236v1","category":"cs.LG"}
{"created":"2024-02-14 15:22:24","title":"Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms","abstract":"Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives. The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions. This paper introduces a testbed for evaluating and benchmarking platooning algorithms on 1/10th scale vehicles with onboard sensors. To demonstrate the testbed's utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times. We validate our algorithms in simulation to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in simulation.","sentences":["Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives.","The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions.","This paper introduces a testbed for evaluating and benchmarking platooning algorithms on 1/10th scale vehicles with onboard sensors.","To demonstrate the testbed's utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times.","We validate our algorithms in simulation to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in simulation."],"url":"http://arxiv.org/abs/2402.09233v1","category":"cs.RO"}
{"created":"2024-02-14 15:09:01","title":"Is my Data in your AI Model? Membership Inference Test with Application to Face Images","abstract":"This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.","sentences":["This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models.","Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process.","The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs).","The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models.","Experiments are carried out using six publicly available databases, comprising over 22 million face images in total.","Also, different experimental scenarios are considered depending on the context available of the AI model to test.","Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data."],"url":"http://arxiv.org/abs/2402.09225v1","category":"cs.CV"}
{"created":"2024-02-14 15:01:07","title":"Spectral Filters, Dark Signals, and Attention Sinks","abstract":"Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.","sentences":["Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens.","We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands.","We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation.","We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved.","Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum."],"url":"http://arxiv.org/abs/2402.09221v1","category":"cs.AI"}
{"created":"2024-02-14 14:57:25","title":"A case study of university student networks and the COVID-19 pandemic using a social network analysis approach in halls of residence","abstract":"The COVID-19 pandemic has meant that young university students have had to adapt their learning and have a reduced relational context. Adversity contexts build models of human behaviour based on relationships. However, there is a lack of studies that analyse the behaviour of university students based on their social structure in the context of a pandemic. This information could be useful in making decisions on how to plan collective responses to adversities. The Social Network Analysis (SNA) method has been chosen to address this structural perspective. The aim of our research is to describe the structural behaviour of students in university residences during the COVID-19 pandemic with a more in-depth analysis of student leaders. A descriptive cross-sectional study was carried out at one Spanish Public University, Le\\'on, from 23th October 2020 to 20th November 2020. The participation was of 93 students, from four halls of residence. The data were collected from a database created specifically at the university to \"track\" contacts in the COVID-19 pandemic, SiVeUle. We applied the SNA for the analysis of the data. The leadership on the university residence was measured using centrality measures. The top leaders were analyzed using the Egonetwork and an assessment of the key players. Students with higher social reputations experience higher levels of pandemic contagion in relation to COVID-19 infection. The results were statistically significant between the centrality in the network and the results of the COVID-19 infection. The most leading students showed a high degree of Betweenness, and three students had the key player structure in the network. Networking behaviour of university students in halls of residence could be related to contagion in the COVID-19 pandemic.","sentences":["The COVID-19 pandemic has meant that young university students have had to adapt their learning and have a reduced relational context.","Adversity contexts build models of human behaviour based on relationships.","However, there is a lack of studies that analyse the behaviour of university students based on their social structure in the context of a pandemic.","This information could be useful in making decisions on how to plan collective responses to adversities.","The Social Network Analysis (SNA) method has been chosen to address this structural perspective.","The aim of our research is to describe the structural behaviour of students in university residences during the COVID-19 pandemic with a more in-depth analysis of student leaders.","A descriptive cross-sectional study was carried out at one Spanish Public University, Le\\'on, from 23th October 2020 to 20th November 2020.","The participation was of 93 students, from four halls of residence.","The data were collected from a database created specifically at the university to \"track\" contacts in the COVID-19 pandemic, SiVeUle.","We applied the SNA for the analysis of the data.","The leadership on the university residence was measured using centrality measures.","The top leaders were analyzed using the Egonetwork and an assessment of the key players.","Students with higher social reputations experience higher levels of pandemic contagion in relation to COVID-19 infection.","The results were statistically significant between the centrality in the network and the results of the COVID-19 infection.","The most leading students showed a high degree of Betweenness, and three students had the key player structure in the network.","Networking behaviour of university students in halls of residence could be related to contagion in the COVID-19 pandemic."],"url":"http://arxiv.org/abs/2402.09219v1","category":"cs.CY"}
{"created":"2024-02-14 14:53:56","title":"Scaling the Authoring of AutoTutors with Large Language Models","abstract":"Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches. Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow","sentences":["Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation.","In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems.","A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees.","We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results.","Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer.","This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches.","Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, GPT-4.","MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow"],"url":"http://arxiv.org/abs/2402.09216v1","category":"cs.CL"}
{"created":"2024-02-14 14:46:10","title":"Machine classification of quantum correlations for entanglement distribution networks","abstract":"The paper suggest employing machine learning for resource-efficient classification of quantum correlations in entanglement distribution networks. Specifically, artificial neural networks (ANN) are utilized to classify quantum correlations based on collective measurements conducted in the geometry of entanglement swapping. ANNs are trained to categorize two-qubit quantum states into five mutually exclusive classes depending on the strength of quantum correlations exhibited by the states. The precision and recall of the ANN models are analyzed as functions of the quantum resources consumed, i.e. the number of collective measurements performed.","sentences":["The paper suggest employing machine learning for resource-efficient classification of quantum correlations in entanglement distribution networks.","Specifically, artificial neural networks (ANN) are utilized to classify quantum correlations based on collective measurements conducted in the geometry of entanglement swapping.","ANNs are trained to categorize two-qubit quantum states into five mutually exclusive classes depending on the strength of quantum correlations exhibited by the states.","The precision and recall of the ANN models are analyzed as functions of the quantum resources consumed, i.e. the number of collective measurements performed."],"url":"http://arxiv.org/abs/2402.09212v1","category":"quant-ph"}
{"created":"2024-02-14 14:46:03","title":"DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers","abstract":"Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality. However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers). Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population. In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities. We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction. We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model. We further stabilize the inferred full-body pose in a wide range of configurations by learning to blend predictions that are computed in two reference frames, each of which is designed for different types of motions. We demonstrate the effectiveness of our design on a large dataset that captures 22 subjects performing challenging locomotion for three-point tracking, including lunges, hula-hooping, and sitting. As shown in a live demo using the Meta VR headset and Xsens IMUs, our method runs in real-time while accurately tracking a user's motion when they perform a diverse set of movements.","sentences":["Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality.","However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers).","Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population.","In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities.","We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction.","We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model.","We further stabilize the inferred full-body pose in a wide range of configurations by learning to blend predictions that are computed in two reference frames, each of which is designed for different types of motions.","We demonstrate the effectiveness of our design on a large dataset that captures 22 subjects performing challenging locomotion for three-point tracking, including lunges, hula-hooping, and sitting.","As shown in a live demo using the Meta VR headset and Xsens IMUs, our method runs in real-time while accurately tracking a user's motion when they perform a diverse set of movements."],"url":"http://arxiv.org/abs/2402.09211v1","category":"cs.CV"}
{"created":"2024-02-14 18:45:14","title":"Long-form evaluation of model editing","abstract":"Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.","sentences":["Evaluations of model editing currently only use the `next few token' completions after a prompt.","As a result, the impact of these methods on longer natural language generation is largely unknown.","We introduce long-form evaluation of model editing (\\textbf{\\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings.","Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings.","Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods.","Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods.","Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues."],"url":"http://arxiv.org/abs/2402.09394v1","category":"cs.CL"}
{"created":"2024-02-14 18:44:48","title":"A superstatistical measure of distance from canonical equilibrium","abstract":"Non-equilibrium systems in steady states are commonly described by generalized statistical mechanical theories such as non-extensive statistics and superstatistics. Superstatistics assumes that the inverse temperature $\\beta = 1/(k_B T)$ follows some pre-established statistical distribution, however, it has been previously proved (Physica A 505, 864-870 [2018]) that $\\beta$ cannot be associated to an observable function $B(\\boldsymbol{\\Gamma})$ of the microstates $\\boldsymbol{\\Gamma}$. In this work, we provide an information-theoretical interpretation of this theorem by introducing a new quantity $\\mathcal{D}$, the mutual information between $\\beta$ and $\\boldsymbol{\\Gamma}$. Our results show that $\\mathcal{D}$ is also a measure of departure from canonical equilibrium, and reveal a minimum, non-zero uncertainty about $\\beta$ given $\\boldsymbol{\\Gamma}$ for every non-canonical superstatistical ensemble. This supports the use of the mutual information as a descriptor of complexity and correlation in complex systems, also providing in some cases a sound basis for the use of Tsallis' entropic index $q$ as a measure of distance from equilibrium, being in those cases a proxy for $\\mathcal{D}$.","sentences":["Non-equilibrium systems in steady states are commonly described by generalized statistical mechanical theories such as non-extensive statistics and superstatistics.","Superstatistics assumes that the inverse temperature $\\beta = 1/(k_B T)$ follows some pre-established statistical distribution, however, it has been previously proved (Physica A 505, 864-870 [2018]) that $\\beta$ cannot be associated to an observable function $B(\\boldsymbol{\\Gamma})$ of the microstates $\\boldsymbol{\\Gamma}$.","In this work, we provide an information-theoretical interpretation of this theorem by introducing a new quantity $\\mathcal{D}$, the mutual information between $\\beta$ and $\\boldsymbol{\\Gamma}$. Our results show that $\\mathcal{D}$ is also a measure of departure from canonical equilibrium, and reveal a minimum, non-zero uncertainty about $\\beta$ given $\\boldsymbol{\\Gamma}$ for every non-canonical superstatistical ensemble.","This supports the use of the mutual information as a descriptor of complexity and correlation in complex systems, also providing in some cases a sound basis for the use of Tsallis' entropic index $q$ as a measure of distance from equilibrium, being in those cases a proxy for $\\mathcal{D}$."],"url":"http://arxiv.org/abs/2402.09393v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-14 18:37:40","title":"Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning","abstract":"The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance. This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions. The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed. To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training. The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits. We also address the crucial issue of safety criticality by demonstrating that a constraint-conditioned policy can be used as a trajectory design assistant to design a library of feed-forward trajectories to handle different physics conditions and user settings. As a library of trajectories is more interpretable and verifiable offline, we argue such an approach is a promising path for leveraging the capabilities of reinforcement learning in the safety-critical context of burning plasma tokamaks. Finally, we demonstrate how the training environment can be a useful platform for other feed-forward optimization approaches by using an evolutionary algorithm to perform optimization of feed-forward trajectories that are robust to physics uncertainty","sentences":["The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance.","This work develops a reinforcement learning approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions.","The policy training environment is a hybrid physics and machine learning model trained on simulations of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed.","To address physics uncertainty and model inaccuracies, the simulation environment is massively parallelized on GPU with randomized physics parameters during policy training.","The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits.","We also address the crucial issue of safety criticality by demonstrating that a constraint-conditioned policy can be used as a trajectory design assistant to design a library of feed-forward trajectories to handle different physics conditions and user settings.","As a library of trajectories is more interpretable and verifiable offline, we argue such an approach is a promising path for leveraging the capabilities of reinforcement learning in the safety-critical context of burning plasma tokamaks.","Finally, we demonstrate how the training environment can be a useful platform for other feed-forward optimization approaches by using an evolutionary algorithm to perform optimization of feed-forward trajectories that are robust to physics uncertainty"],"url":"http://arxiv.org/abs/2402.09387v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 18:26:58","title":"Safe Distributed Control of Multi-Robot Systems with Communication Delays","abstract":"Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction. We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays. We formalize a notion of distributed control barrier function (CBF) for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using graph neural networks to learn safe distributed controllers. Further, we observe that learning a distributed controller ignoring delays can severely degrade safety. Our main contribution is a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information. Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays","sentences":["Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction.","We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays.","We formalize a notion of distributed control barrier function (CBF) for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using graph neural networks to learn safe distributed controllers.","Further, we observe that learning a distributed controller ignoring delays can severely degrade safety.","Our main contribution is a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information.","Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays"],"url":"http://arxiv.org/abs/2402.09382v1","category":"cs.RO"}
{"created":"2024-02-14 18:21:17","title":"Varentropy Estimation via Nearest Neighbor Graphs","abstract":"The Varentropy is a measure of the variability of the information content of random vector and it is invariant under affine transformations. We introduce the statistical estimate of varentropy of random vector based on the nearest neighbor graphs (distances). The asymptotic unbiasedness and L2-consistency of the estimates are established.","sentences":["The Varentropy is a measure of the variability of the information content of random vector and it is invariant under affine transformations.","We introduce the statistical estimate of varentropy of random vector based on the nearest neighbor graphs (distances).","The asymptotic unbiasedness and L2-consistency of the estimates are established."],"url":"http://arxiv.org/abs/2402.09374v1","category":"math.ST"}
{"created":"2024-02-14 18:09:27","title":"Long-Distance Nuclear Matrix Elements for Neutrinoless Double-Beta Decay from Lattice QCD","abstract":"Neutrinoless double-beta ($0\\nu\\beta\\beta$) decay is a heretofore unobserved process which, if observed, would imply that neutrinos are Majorana particles. Interpretations of the stringent experimental constraints on $0\\nu\\beta\\beta$-decay half-lives require calculations of nuclear matrix elements. This work presents the first lattice quantum-chromodynamics (LQCD) calculation of the matrix element for $0\\nu\\beta\\beta$ decay in a multi-nucleon system, specifically the $nn \\rightarrow pp ee$ transition, mediated by a light left-handed Majorana neutrino propagating over nuclear-scale distances. This calculation is performed with quark masses corresponding to a pion mass of $m_\\pi = 806$ MeV at a single lattice spacing and volume. The statistically cleaner $\\Sigma^- \\rightarrow \\Sigma^+ ee$ transition is also computed in order to investigate various systematic uncertainties. The prospects for matching the results of LQCD calculations onto a nuclear effective field theory to determine a leading-order low-energy constant relevant for $0\\nu\\beta\\beta$ decay with a light Majorana neutrino are investigated. This work, therefore, sets the stage for future calculations at physical values of the quark masses that, combined with effective field theory and nuclear many-body studies, will provide controlled theoretical inputs to experimental searches of $0\\nu\\beta\\beta$ decay.","sentences":["Neutrinoless double-beta ($0\\nu\\beta\\beta$) decay is a heretofore unobserved process which, if observed, would imply that neutrinos are Majorana particles.","Interpretations of the stringent experimental constraints on $0\\nu\\beta\\beta$-decay half-lives require calculations of nuclear matrix elements.","This work presents the first lattice quantum-chromodynamics (LQCD) calculation of the matrix element for $0\\nu\\beta\\beta$ decay in a multi-nucleon system, specifically the $nn \\rightarrow pp ee$ transition, mediated by a light left-handed Majorana neutrino propagating over nuclear-scale distances.","This calculation is performed with quark masses corresponding to a pion mass of $m_\\pi = 806$ MeV at a single lattice spacing and volume.","The statistically cleaner $\\Sigma^- \\rightarrow \\Sigma^+ ee$ transition is also computed in order to investigate various systematic uncertainties.","The prospects for matching the results of LQCD calculations onto a nuclear effective field theory to determine a leading-order low-energy constant relevant for $0\\nu\\beta\\beta$ decay with a light Majorana neutrino are investigated.","This work, therefore, sets the stage for future calculations at physical values of the quark masses that, combined with effective field theory and nuclear many-body studies, will provide controlled theoretical inputs to experimental searches of $0\\nu\\beta\\beta$ decay."],"url":"http://arxiv.org/abs/2402.09362v1","category":"hep-lat"}
{"created":"2024-02-14 18:01:28","title":"On the Impact of Spatial Covariance Matrix Ordering on Tile Low-Rank Estimation of Mat\u00e9rn Parameters","abstract":"Spatial statistical modeling and prediction involve generating and manipulating an n*n symmetric positive definite covariance matrix, where n denotes the number of spatial locations. However, when n is large, processing this covariance matrix using traditional methods becomes prohibitive. Thus, coupling parallel processing with approximation can be an elegant solution to this challenge by relying on parallel solvers that deal with the matrix as a set of small tiles instead of the full structure. Each processing unit can process a single tile, allowing better performance. The approximation can also be performed at the tile level for better compression and faster execution. The Tile Low-Rank (TLR) approximation, a tile-based approximation algorithm, has recently been used in spatial statistics applications. However, the quality of TLR algorithms mainly relies on ordering the matrix elements. This order can impact the compression quality and, therefore, the efficiency of the underlying linear solvers, which highly depends on the individual ranks of each tile. Thus, herein, we aim to investigate the accuracy and performance of some existing ordering algorithms that are used to order the geospatial locations before generating the spatial covariance matrix. Furthermore, we highlight the pros and cons of each ordering algorithm in the context of spatial statistics applications and give hints to practitioners on how to choose the ordering algorithm carefully. We assess the quality of the compression and the accuracy of the statistical parameter estimates of the Mat\\'ern covariance function using TLR approximation under various ordering algorithms and settings of correlations.","sentences":["Spatial statistical modeling and prediction involve generating and manipulating an n*n symmetric positive definite covariance matrix, where n denotes the number of spatial locations.","However, when n is large, processing this covariance matrix using traditional methods becomes prohibitive.","Thus, coupling parallel processing with approximation can be an elegant solution to this challenge by relying on parallel solvers that deal with the matrix as a set of small tiles instead of the full structure.","Each processing unit can process a single tile, allowing better performance.","The approximation can also be performed at the tile level for better compression and faster execution.","The Tile Low-Rank (TLR) approximation, a tile-based approximation algorithm, has recently been used in spatial statistics applications.","However, the quality of TLR algorithms mainly relies on ordering the matrix elements.","This order can impact the compression quality and, therefore, the efficiency of the underlying linear solvers, which highly depends on the individual ranks of each tile.","Thus, herein, we aim to investigate the accuracy and performance of some existing ordering algorithms that are used to order the geospatial locations before generating the spatial covariance matrix.","Furthermore, we highlight the pros and cons of each ordering algorithm in the context of spatial statistics applications and give hints to practitioners on how to choose the ordering algorithm carefully.","We assess the quality of the compression and the accuracy of the statistical parameter estimates of the Mat\\'ern covariance function using TLR approximation under various ordering algorithms and settings of correlations."],"url":"http://arxiv.org/abs/2402.09356v1","category":"stat.CO"}
{"created":"2024-02-14 17:59:38","title":"Investigation of Ga interstitial and vacancy diffusion in $\u03b2$-Ga$_2$O$_3$ via split defects: a direct approach via master diffusion equations","abstract":"The low symmetry of monoclinic $\\beta$-Ga$_2$O$_3$ leads to elaborate intrinsic defects, such as Ga vacancies split amongst multiple lattice sites. These defects contribute to fast, anisotropic Ga diffusion, yet their complexity makes it challenging to understand dominant diffusion mechanisms. Here, we predict the 3D diffusivity tensors for Ga interstitials (Ga${_i^{3+}}$) and vacancies (V${_{Ga}^{3-}}$) via first principles and direct solution of the master diffusion equations. We first explore the maximum extent of configurationally complex ''$N$-split'' Ga interstitials and vacancies. With dominant low-energy defects identified, we enumerate all possible elementary hops connecting defect configurations to each other, including interstitialcy hops. Hopping barriers are obtained from nudged elastic band simulations. Finally, the comprehensive sets of (i) defect configurations and their energies and (ii) the hopping barriers that connect them are used to construct the master diffusion equations for both Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$. The solution to these equations yields the Onsager transport coefficients, i.e. the components of the 3D diffusivity tensors $D_{{Ga}_i}$ and $D_{V_{Ga}}$ for Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$, respectively. It further reveals the active diffusion paths along all crystallographic directions. We find that both Ga${_i^{3+}}$ and V${_{{Ga}}^{3-}}$ diffusion are fastest along the $c$-axis, due to 3-split defects that bridge neighboring unit cells along the $c$-axis and divert diffusing species around high-energy bottlenecks. Although isolated Ga${_i^{3+}}$ diffuse faster than isolated V${_{Ga}^{3-}}$, self-diffusion of Ga is predominantly mediated by V$_{Ga}^{3-}$ due to the higher V$_{Ga}^{3-}$ defect concentration under most thermodynamic environments.","sentences":["The low symmetry of monoclinic $\\beta$-Ga$_2$O$_3$ leads to elaborate intrinsic defects, such as Ga vacancies split amongst multiple lattice sites.","These defects contribute to fast, anisotropic Ga diffusion, yet their complexity makes it challenging to understand dominant diffusion mechanisms.","Here, we predict the 3D diffusivity tensors for Ga interstitials (Ga${_i^{3+}}$) and vacancies (V${_{Ga}^{3-}}$) via first principles and direct solution of the master diffusion equations.","We first explore the maximum extent of configurationally complex ''$N$-split'' Ga interstitials and vacancies.","With dominant low-energy defects identified, we enumerate all possible elementary hops connecting defect configurations to each other, including interstitialcy hops.","Hopping barriers are obtained from nudged elastic band simulations.","Finally, the comprehensive sets of (i) defect configurations and their energies and (ii) the hopping barriers that connect them are used to construct the master diffusion equations for both Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$.","The solution to these equations yields the Onsager transport coefficients, i.e. the components of the 3D diffusivity tensors $D_{{Ga}_i}$ and $D_{V_{Ga}}$ for Ga${_i^{3+}}$ and V${_{Ga}^{3-}}$, respectively.","It further reveals the active diffusion paths along all crystallographic directions.","We find that both Ga${_i^{3+}}$ and V${_{{Ga}}^{3-}}$ diffusion are fastest along the $c$-axis, due to 3-split defects that bridge neighboring unit cells along the $c$-axis and divert diffusing species around high-energy bottlenecks.","Although isolated Ga${_i^{3+}}$ diffuse faster than isolated V${_{Ga}^{3-}}$, self-diffusion of Ga is predominantly mediated by V$_{Ga}^{3-}$ due to the higher V$_{Ga}^{3-}$ defect concentration under most thermodynamic environments."],"url":"http://arxiv.org/abs/2402.09354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 17:53:48","title":"Monoclinic LaSb$_2$ Superconducting Thin Films","abstract":"Rare-earth diantimondes exhibit coupling between structural and electronic orders which are tunable under pressure and temperature. Here we present the discovery of a new polymorph of LaSb$_2$ stabilized in thin films synthesized using molecular beam epitaxy. Using diffraction, electron microscopy, and first principles calculations we identify a YbSb$_2$-type monoclinic lattice as a yet-uncharacterized stacking configuration. The material hosts superconductivity with a $T_\\mathrm{c}$ = 2 K, which is enhanced relative to the bulk ambient phase, and a long superconducting coherence length of 140 nm. This result highlights the potential thin film growth has in stabilizing novel stacking configurations in quasi-two dimensional compounds with competing layered structures.","sentences":["Rare-earth diantimondes exhibit coupling between structural and electronic orders which are tunable under pressure and temperature.","Here we present the discovery of a new polymorph of LaSb$_2$ stabilized in thin films synthesized using molecular beam epitaxy.","Using diffraction, electron microscopy, and first principles calculations we identify a YbSb$_2$-type monoclinic lattice as a yet-uncharacterized stacking configuration.","The material hosts superconductivity with a $T_\\mathrm{c}$ = 2 K, which is enhanced relative to the bulk ambient phase, and a long superconducting coherence length of 140 nm.","This result highlights the potential thin film growth has in stabilizing novel stacking configurations in quasi-two dimensional compounds with competing layered structures."],"url":"http://arxiv.org/abs/2402.09349v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-14 17:45:11","title":"Floor and fractional part statistics and arithmetic functions","abstract":"We apply statistical properties of the floor and fractional part functions to give several results describing the M\\\"obius function $\\mu(.)$ and the summatory M\\\"obius function $M(.)$. These results include the equalities $$ \\sum_{n}\\frac{\\mu(n)}{n^{2}}\\sum_{m<n}\\mu(m)=\\sum_{n}\\mu(n)\\sum_{m>n}\\frac{\\mu(m)}{m^{2}} $$ and $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du=\\sum_{n\\leq N}\\mu(n)\\int_{n}^{N}\\frac{M(u)}{u^{2}}du, $$ the latter of which implies that, under the Riemann hypothesis and simplicity of the nontrivial zeta zeros $\\rho=1/2+i\\gamma$, $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du\\sim\\sum_{\\rho}\\frac{N^{2i\\gamma}}{2i\\gamma}\\left(\\frac{1}{\\rho\\zeta'(\\rho)}\\right)^{2}+\\sum_{\\rho\\neq \\rho'}\\frac{N^{i(\\gamma+\\gamma')}}{i(\\gamma+\\gamma')}\\frac{1}{\\rho \\rho' \\zeta'(\\rho)\\zeta'(\\rho')} $$ as $N\\rightarrow \\infty$, where $\\zeta(.)$ denotes the zeta function. The integral on the left-hand side diverges at least logarithmically with $N$, hence this result demonstrates constructive interference properties of the ordinates $\\gamma$.","sentences":["We apply statistical properties of the floor and fractional part functions to give several results describing the M\\\"obius function $\\mu(.)$ and the summatory M\\\"obius function $M(.)$. These results include the equalities $$ \\sum_{n}\\frac{\\mu(n)}{n^{2}}\\sum_{m<n}\\mu(m)=\\sum_{n}\\mu(n)\\sum_{m>n}\\frac{\\mu(m)}{m^{2}} $$ and $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du=\\sum_{n\\leq N}\\mu(n)\\int_{n}^{N}\\frac{M(u)}{u^{2}}du, $$ the latter of which implies that, under the Riemann hypothesis and simplicity of the nontrivial zeta zeros $\\rho=1/2+i\\gamma$, $$ \\int_{1}^{N}\\frac{M^{2}(u)}{u^{2}}du\\sim\\sum_{\\rho}\\frac{N^{2i\\gamma}}{2i\\gamma}\\left(\\frac{1}{\\rho\\zeta'(\\rho)}\\right)^{2}+\\sum_{\\rho\\neq \\rho'}\\frac{N^{i(\\gamma+\\gamma')}}{i(\\gamma+\\gamma')}\\frac{1}{\\rho \\rho' \\zeta'(\\rho)\\zeta'(\\rho')} $$ as $N\\rightarrow \\infty$, where $\\zeta(.)$ denotes the zeta function.","The integral on the left-hand side diverges at least logarithmically with $N$, hence this result demonstrates constructive interference properties of the ordinates $\\gamma$."],"url":"http://arxiv.org/abs/2402.09343v1","category":"math.NT"}
{"created":"2024-02-14 17:32:57","title":"A Modern Approach to Electoral Delimitation using the Quadtree Data Structure","abstract":"The boundaries of electoral constituencies for assembly and parliamentary seats are drafted using a process referred to as delimitation, which ensures fair and equal representation of all citizens. The current delimitation exercise suffers from a number of drawbacks viz. inefficiency, gerrymandering and an uneven seat-to-population ratio, owing to existing legal and constitutional dictates. The existing methods allocate seats to every state but remain silent about their actual shape and location within the state. The main purpose of this research is to study and analyse the performance of existing delimitation algorithms and further propose a potential solution, along with its merits, that involves using a computational model based on the quadtree data structure to automate the districting process by optimizing objective population criteria. The paper presents an approach to electoral delimitation using the quadtree data structure, which is used to partition a two-dimensional geographical space by recursively subdividing it into four quadrants or regions on the basis of population as a parameter value associated with the node. The quadtree makes use of a quadrant schema of the geographical space for representing constituencies, which not only keeps count of the allocated constituencies but also holds their location-specific information. The performance of the proposed algorithm is analysed and evaluated against existing techniques and proves to be an efficient solution in terms of algorithmic complexity and boundary visualisation to the process of political districting.","sentences":["The boundaries of electoral constituencies for assembly and parliamentary seats are drafted using a process referred to as delimitation, which ensures fair and equal representation of all citizens.","The current delimitation exercise suffers from a number of drawbacks viz.","inefficiency, gerrymandering and an uneven seat-to-population ratio, owing to existing legal and constitutional dictates.","The existing methods allocate seats to every state but remain silent about their actual shape and location within the state.","The main purpose of this research is to study and analyse the performance of existing delimitation algorithms and further propose a potential solution, along with its merits, that involves using a computational model based on the quadtree data structure to automate the districting process by optimizing objective population criteria.","The paper presents an approach to electoral delimitation using the quadtree data structure, which is used to partition a two-dimensional geographical space by recursively subdividing it into four quadrants or regions on the basis of population as a parameter value associated with the node.","The quadtree makes use of a quadrant schema of the geographical space for representing constituencies, which not only keeps count of the allocated constituencies but also holds their location-specific information.","The performance of the proposed algorithm is analysed and evaluated against existing techniques and proves to be an efficient solution in terms of algorithmic complexity and boundary visualisation to the process of political districting."],"url":"http://arxiv.org/abs/2402.09336v1","category":"cs.DS"}
{"created":"2024-02-14 17:22:03","title":"3D-based RNA function prediction tools in rnaglib","abstract":"Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures.","sentences":["Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design.","However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization.","In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures."],"url":"http://arxiv.org/abs/2402.09330v1","category":"q-bio.BM"}
{"created":"2024-02-14 17:17:30","title":"Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization","abstract":"In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.","sentences":["In this work, we investigate the interplay between memorization and learning in the context of \\emph{stochastic convex optimization} (SCO).","We define memorization via the information a learning algorithm reveals about its training data points.","We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020).","Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023).","We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\varepsilon$ has CMI bounded below by $\\Omega(1/\\varepsilon^2)$ and $\\Omega(1/\\varepsilon)$, respectively.","We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems.","Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems."],"url":"http://arxiv.org/abs/2402.09327v1","category":"cs.LG"}
{"created":"2024-02-14 17:17:05","title":"Stability and Multigroup Fairness in Ranking with Uncertain Predictions","abstract":"Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve multigroup fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.","sentences":["Rankings are ubiquitous across many applications, from search engines to hiring committees.","In practice, many rankings are derived from the output of predictors.","However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings.","Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings.","We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups.","Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012).","While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al.","(2021) are stable.","Our main result is that UA rankings also achieve multigroup fairness through successful composition with multiaccurate or multicalibrated predictors.","Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used."],"url":"http://arxiv.org/abs/2402.09326v1","category":"cs.LG"}
{"created":"2024-02-14 17:06:45","title":"Bolometric detection of Josephson radiation","abstract":"A Josephson junction (JJ) has been under intensive study ever since 1960's. Yet even in the present era of building quantum information processing devices based on many JJs, open questions regarding a single junction remain unsolved, such as quantum phase transitions, coupling of the JJ to an environment and improving coherence of a superconducting qubit. Here we design and build an engineered on-chip reservoir that acts as an efficient bolometer for detecting the Josephson radiation under non-equilibrium (biased) conditions. The bolometer converts ac Josephson current at microwave frequencies, up to about $100\\,$GHz, into a measurable dc temperature rise. The present experiment demonstrates an efficient, wide-band, thermal detection scheme of microwave photons and provides a sensitive detector of Josephson dynamics beyond the standard conductance measurements. Using a circuit model, we capture both the current-voltage characteristics and the measured power quantitatively.","sentences":["A Josephson junction (JJ) has been under intensive study ever since 1960's.","Yet even in the present era of building quantum information processing devices based on many JJs, open questions regarding a single junction remain unsolved, such as quantum phase transitions, coupling of the JJ to an environment and improving coherence of a superconducting qubit.","Here we design and build an engineered on-chip reservoir that acts as an efficient bolometer for detecting the Josephson radiation under non-equilibrium (biased) conditions.","The bolometer converts ac Josephson current at microwave frequencies, up to about $100\\,$GHz, into a measurable dc temperature rise.","The present experiment demonstrates an efficient, wide-band, thermal detection scheme of microwave photons and provides a sensitive detector of Josephson dynamics beyond the standard conductance measurements.","Using a circuit model, we capture both the current-voltage characteristics and the measured power quantitatively."],"url":"http://arxiv.org/abs/2402.09314v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 16:59:06","title":"Topologies of maximally extended non-Hausdorff Misner Space","abstract":"Misner (1967) space is a portion of 2-dimensional Minkowski spacetime, identified under a boost $\\mathcal B$. It is well known that the maximal analytic extension of Misner space that is Hausdorff consists of one half of Minkowski spacetime, identified under $\\mathcal B$; and Hawking and Ellis (1973) have shown that the maximal analytic extension that is non-Hausdorff is equal to the full Minkowski spacetime with the point $Q$ at the origin removed, identifed under $\\mathcal B$. In this paper I show that, in fact, there is an infinite set of non-Hausdorff maximal analytic extensions, each with a different causal structure. The extension constructed by Hawking and Ellis is the simplest of these. Another extension is obtained by wrapping an infinite number of copies of Minkowski spacetime around the removed $Q$ as a helicoid or Riemann surface and then identifying events under the boost $\\mathcal B$. The other extensions are obtained by wrapping some number $n$ of successive copies of Minkowski spacetime around the missing $Q$ as a helicoid, then identifying the end of the $n$'th copy with the beginning of the initial copy, and then identifying events under $\\mathcal B$. I discuss the causal structure and covering spaces of each of these extensions.","sentences":["Misner (1967) space is a portion of 2-dimensional Minkowski spacetime, identified under a boost $\\mathcal B$.","It is well known that the maximal analytic extension of Misner space that is Hausdorff consists of one half of Minkowski spacetime, identified under $\\mathcal B$; and Hawking and Ellis (1973) have shown that the maximal analytic extension that is non-Hausdorff is equal to the full Minkowski spacetime with the point $Q$ at the origin removed, identifed under $\\mathcal B$. In this paper I show that, in fact, there is an infinite set of non-Hausdorff maximal analytic extensions, each with a different causal structure.","The extension constructed by Hawking and Ellis is the simplest of these.","Another extension is obtained by wrapping an infinite number of copies of Minkowski spacetime around the removed $Q$ as a helicoid or Riemann surface and then identifying events under the boost $\\mathcal B$.","The other extensions are obtained by wrapping some number $n$ of successive copies of Minkowski spacetime around the missing $Q$ as a helicoid, then identifying the end of the $n$'th copy with the beginning of the initial copy, and then identifying events under $\\mathcal B$. I discuss the causal structure and covering spaces of each of these extensions."],"url":"http://arxiv.org/abs/2402.09312v1","category":"gr-qc"}
{"created":"2024-02-14 16:51:54","title":"Wave-particle correlations in multiphoton resonances of coherent light-matter interaction","abstract":"We discuss the conditional measurement of field amplitudes by a nonclassical photon sequence in the Jaynes-Cummings (JC) model under multiphoton operation. We do so by employing a correlator of immediate experimental relevance to reveal a distinct nonclassical evolution in the spirit of [G. T. Foster et al., Phys. Rev. Lett. 85 3149 (2000)]. The correlator relies on the complementary nature of the pictures obtained from different unravelings of a JC source master equation. We demonstrate that direct photodetection entails a conditioned separation of timescales, a quantum beat and a semiclassical oscillation, produced by the coherent light-matter interaction in its strong-coupling limit. We single the quantum beat out in the analytical expression for the waiting-time distribution, pertaining to the particle nature of the scattered light, and find a negative spectrum of quadrature amplitude squeezing, characteristic of its wave nature. Finally, we jointly detect the dual aspects through the wave-particle correlator, showing an asymmetric regression of fluctuations to the steady state which depends on the quadrature amplitude being measured.","sentences":["We discuss the conditional measurement of field amplitudes by a nonclassical photon sequence in the Jaynes-Cummings (JC) model under multiphoton operation.","We do so by employing a correlator of immediate experimental relevance to reveal a distinct nonclassical evolution in the spirit of [G. T. Foster et al., Phys.","Rev. Lett. 85 3149 (2000)].","The correlator relies on the complementary nature of the pictures obtained from different unravelings of a JC source master equation.","We demonstrate that direct photodetection entails a conditioned separation of timescales, a quantum beat and a semiclassical oscillation, produced by the coherent light-matter interaction in its strong-coupling limit.","We single the quantum beat out in the analytical expression for the waiting-time distribution, pertaining to the particle nature of the scattered light, and find a negative spectrum of quadrature amplitude squeezing, characteristic of its wave nature.","Finally, we jointly detect the dual aspects through the wave-particle correlator, showing an asymmetric regression of fluctuations to the steady state which depends on the quadrature amplitude being measured."],"url":"http://arxiv.org/abs/2402.09308v1","category":"quant-ph"}
{"created":"2024-02-14 16:50:48","title":"Optimal design of equilibrium solutions of the Vlasov-Poisson system by an external electric field","abstract":"A new optimization framework to design steady equilibrium solutions of the Vlasov-Poisson system by means of external electric fields is presented. This optimization framework requires the minimization of an ensemble functional with Tikhonov regularization of the control field under the differential constraint of a nonlinear elliptic equation that models equilibrium solutions of the Vlasov-Poisson system. Existence of optimal control fields and their characterization as solutions to first-order optimality conditions are discussed. Numerical approximations and optimization schemes are developed to validate the proposed framework.","sentences":["A new optimization framework to design steady equilibrium solutions of the Vlasov-Poisson system by means of external electric fields is presented.","This optimization framework requires the minimization of an ensemble functional with Tikhonov regularization of the control field under the differential constraint of a nonlinear elliptic equation that models equilibrium solutions of the Vlasov-Poisson system.","Existence of optimal control fields and their characterization as solutions to first-order optimality conditions are discussed.","Numerical approximations and optimization schemes are developed to validate the proposed framework."],"url":"http://arxiv.org/abs/2402.09306v1","category":"math.OC"}
{"created":"2024-02-14 16:21:47","title":"EcoVal: An Efficient Data Valuation Framework for Machine Learning","abstract":"Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \\textit{production function}, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models.","sentences":["Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives.","The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value.","In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner.","Instead of directly working with individual data sample, we determine the value of a cluster of similar data points.","This value is further propagated amongst all the member cluster points.","We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data.","This is enabled by formulating the performance of a model as a \\textit{production function}, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market.","We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance.","We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data.","This work addresses one of the core challenges of efficient data valuation at scale in machine learning models."],"url":"http://arxiv.org/abs/2402.09288v2","category":"cs.LG"}
{"created":"2024-02-14 16:18:56","title":"Smart Cities and Villages: Concept Review and Implementation Perspectives in Developing Cities","abstract":"The \"Smart City\" (SC) concept has been around for decades with deployment scenarios revealed in major cities of developed countries. However, while SC has enhanced the living conditions of city dwellers in the developed world, the concept is still either missing or poorly deployed in the developing world. This paper presents a review of the SC concept from the perspective of its application to cities in developing nations, the opportunities it avails, and challenges related to its applicability to these cities. Building upon a systematic review of literature, this paper shows that there are neither canonical definitions, models or frameworks of references for the SC concept. This paper also aims to bridge the gap between the \"smart city\" and \"smart village\" concepts, with the expectation of providing a holistic approach to solving common issues in cities around the world. Drawing inspiration from other authors, we propose a conceptual model for a SC initiative in Africa and demonstrate the need to prioritize research and capacity development. We also discuss the potential opportunities for such SC implementations in sub-Saharan Africa. As a case study, we consider the city of Lubumbashi in the Democratic Republic of Congo and discuss ways of making it a smart city by building around successful smart city initiatives. It is our belief that for Lubumbashi, as with any other city in Sub-Saharan Africa, the first step to developing a smart city is to build knowledge and create an intellectual capital.","sentences":["The \"Smart City\" (SC) concept has been around for decades with deployment scenarios revealed in major cities of developed countries.","However, while SC has enhanced the living conditions of city dwellers in the developed world, the concept is still either missing or poorly deployed in the developing world.","This paper presents a review of the SC concept from the perspective of its application to cities in developing nations, the opportunities it avails, and challenges related to its applicability to these cities.","Building upon a systematic review of literature, this paper shows that there are neither canonical definitions, models or frameworks of references for the SC concept.","This paper also aims to bridge the gap between the \"smart city\" and \"smart village\" concepts, with the expectation of providing a holistic approach to solving common issues in cities around the world.","Drawing inspiration from other authors, we propose a conceptual model for a SC initiative in Africa and demonstrate the need to prioritize research and capacity development.","We also discuss the potential opportunities for such SC implementations in sub-Saharan Africa.","As a case study, we consider the city of Lubumbashi in the Democratic Republic of Congo and discuss ways of making it a smart city by building around successful smart city initiatives.","It is our belief that for Lubumbashi, as with any other city in Sub-Saharan Africa, the first step to developing a smart city is to build knowledge and create an intellectual capital."],"url":"http://arxiv.org/abs/2402.09284v1","category":"cs.DC"}
{"created":"2024-02-14 18:25:13","title":"Fixed-sparsity matrix approximation from matrix-vector products","abstract":"We study the problem of approximating a matrix $\\mathbf{A}$ with a matrix that has a fixed sparsity pattern (e.g., diagonal, banded, etc.), when $\\mathbf{A}$ is accessed only by matrix-vector products. We describe a simple randomized algorithm that returns an approximation with the given sparsity pattern with Frobenius-norm error at most $(1+\\varepsilon)$ times the best possible error. When each row of the desired sparsity pattern has at most $s$ nonzero entries, this algorithm requires $O(s/\\varepsilon)$ non-adaptive matrix-vector products with $\\mathbf{A}$. We also prove a matching lower-bound, showing that, for any sparsity pattern with $\\Theta(s)$ nonzeros per row and column, any algorithm achieving $(1+\\epsilon)$ approximation requires $\\Omega(s/\\varepsilon)$ matrix-vector products in the worst case. We thus resolve the matrix-vector product query complexity of the problem up to constant factors, even for the well-studied case of diagonal approximation, for which no previous lower bounds were known.","sentences":["We study the problem of approximating a matrix $\\mathbf{A}$ with a matrix that has a fixed sparsity pattern (e.g., diagonal, banded, etc.), when $\\mathbf{A}$ is accessed only by matrix-vector products.","We describe a simple randomized algorithm that returns an approximation with the given sparsity pattern with Frobenius-norm error at most $(1+\\varepsilon)$ times the best possible error.","When each row of the desired sparsity pattern has at most $s$ nonzero entries, this algorithm requires $O(s/\\varepsilon)$ non-adaptive matrix-vector products with $\\mathbf{A}$. We also prove a matching lower-bound, showing that, for any sparsity pattern with $\\Theta(s)$ nonzeros per row and column, any algorithm achieving $(1+\\epsilon)$ approximation requires $\\Omega(s/\\varepsilon)$ matrix-vector products in the worst case.","We thus resolve the matrix-vector product query complexity of the problem up to constant factors, even for the well-studied case of diagonal approximation, for which no previous lower bounds were known."],"url":"http://arxiv.org/abs/2402.09379v2","category":"cs.DS"}
{"created":"2024-02-14 17:59:34","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","abstract":"Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding.","sentences":["Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs.","However, there still often exists an accuracy gap between these methods and full fine-tuning (FT).","In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA.","Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA).","DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters.","By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.","DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding."],"url":"http://arxiv.org/abs/2402.09353v1","category":"cs.CL"}
{"created":"2024-02-14 17:11:52","title":"Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models","abstract":"Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.","sentences":["Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical.","Image data, if not protected, can be exploited to infer personal or contextual information.","Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans.","Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation.","This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps.","The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification.","Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively."],"url":"http://arxiv.org/abs/2402.09316v1","category":"cs.CV"}
{"created":"2024-02-14 16:31:58","title":"Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems","abstract":"The purpose of this paper is to develop a practical strategy to accelerate Newton's method in the vicinity of singular points. We do this by developing an adaptive safeguarding scheme, which we call gamma-safeguarding, that one can apply to Anderson accelerated Newton's method when solving problems near singular points. The key features of adaptive gamma-safeguarding are that it converges locally for singular problems, and it can detect nonsingular problems, in which case the Newton-Anderson iterates are scaled towards a standard Newton step. This leads to faster local convergence compared to both Newton's method and Newton-Anderson without safeguarding, at no additional computational cost. We demonstrate three strategies one can use when implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve parameter-dependent problems near singular points. For our benchmark problems, we take two parameter-dependent incompressible flow systems: flow in a channel and Rayleigh-Benard convection.","sentences":["The purpose of this paper is to develop a practical strategy to accelerate Newton's method in the vicinity of singular points.","We do this by developing an adaptive safeguarding scheme, which we call gamma-safeguarding, that one can apply to Anderson accelerated Newton's method when solving problems near singular points.","The key features of adaptive gamma-safeguarding are that it converges locally for singular problems, and it can detect nonsingular problems, in which case the Newton-Anderson iterates are scaled towards a standard Newton step.","This leads to faster local convergence compared to both Newton's method and Newton-Anderson without safeguarding, at no additional computational cost.","We demonstrate three strategies one can use when implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve parameter-dependent problems near singular points.","For our benchmark problems, we take two parameter-dependent incompressible flow systems: flow in a channel and Rayleigh-Benard convection."],"url":"http://arxiv.org/abs/2402.09295v1","category":"math.NA"}
{"created":"2024-02-14 16:05:29","title":"Persistence of steady-states for dynamical systems on large networks","abstract":"The goal of this work is to identify steady-state solutions to dynamical systems defined on large, random families of networks. We do so by passing to a continuum limit where the adjacency matrix is replaced by a non-local operator with kernel called a graphon. This graphon equation is often more amenable to analysis and provides a single equation to study instead of the infinitely many variations of networks that lead to the limit. Our work establishes a rigorous connection between steady-states of the continuum and network systems. Precisely, we show that if the graphon equation has a steady-state solution whose linearization is invertible, there exists related steady-state solutions to the finite-dimensional networked dynamical system over all sufficiently large graphs converging to the graphon. The proof involves setting up a Newton--Kantorovich type iteration scheme which is shown to be a contraction on a suitable metric space. Interestingly, we show that the first iterate of our defined operator in general fails to be a contraction mapping, but the second iterate is proven to contract on the space. We extend our results to show that linear stability properties further carry over from the graphon system to the graph dynamical system. Our results are applied to twisted states in a Kuramoto model of coupled oscillators, steady-states in a model of neuronal network activity, and a Lotka--Volterra model of ecological interaction.","sentences":["The goal of this work is to identify steady-state solutions to dynamical systems defined on large, random families of networks.","We do so by passing to a continuum limit where the adjacency matrix is replaced by a non-local operator with kernel called a graphon.","This graphon equation is often more amenable to analysis and provides a single equation to study instead of the infinitely many variations of networks that lead to the limit.","Our work establishes a rigorous connection between steady-states of the continuum and network systems.","Precisely, we show that if the graphon equation has a steady-state solution whose linearization is invertible, there exists related steady-state solutions to the finite-dimensional networked dynamical system over all sufficiently large graphs converging to the graphon.","The proof involves setting up a Newton--Kantorovich type iteration scheme which is shown to be a contraction on a suitable metric space.","Interestingly, we show that the first iterate of our defined operator in general fails to be a contraction mapping, but the second iterate is proven to contract on the space.","We extend our results to show that linear stability properties further carry over from the graphon system to the graph dynamical system.","Our results are applied to twisted states in a Kuramoto model of coupled oscillators, steady-states in a model of neuronal network activity, and a Lotka--Volterra model of ecological interaction."],"url":"http://arxiv.org/abs/2402.09276v1","category":"math.DS"}
{"created":"2024-02-14 15:41:07","title":"TDViT: Temporal Dilated Video Transformer for Dense Video Tasks","abstract":"Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.","sentences":["Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video.","However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame.","Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations.","To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB).","TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy.","Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics.","Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation.","Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method.","The code is available at https://github.com/guanxiongsun/vfe.pytorch."],"url":"http://arxiv.org/abs/2402.09257v1","category":"cs.CV"}
{"created":"2024-02-14 15:37:58","title":"Exploring the Relationship: Transformative Adaptive Activation Functions in Comparison to Other Activation Functions","abstract":"Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance. Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed. This work sets the TAAF into the context of other activation functions. It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs. This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks.","sentences":["Neural networks are the state-of-the-art approach for many tasks and the activation function is one of the main building blocks that allow such performance.","Recently, a novel transformative adaptive activation function (TAAF) allowing for any vertical and horizontal translation and scaling was proposed.","This work sets the TAAF into the context of other activation functions.","It shows that the TAAFs generalize over 50 existing activation functions and utilize similar concepts as over 70 other activation functions, underscoring the versatility of TAAFs.","This comprehensive exploration positions TAAFs as a promising and adaptable addition to neural networks."],"url":"http://arxiv.org/abs/2402.09249v1","category":"cs.LG"}
{"created":"2024-02-14 15:32:07","title":"Efficient One-stage Video Object Detection by Exploiting Temporal Consistency","abstract":"Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.","sentences":["Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data.","However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors.","Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs.","In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD.","Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames.","Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames.","We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset.","Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method.","The code is available at https://github.com/guanxiongsun/vfe.pytorch."],"url":"http://arxiv.org/abs/2402.09241v1","category":"cs.CV"}
{"created":"2024-02-14 15:21:17","title":"Investigating Premature Convergence in Co-optimization of Morphology and Control in Evolved Virtual Soft Robots","abstract":"Evolving virtual creatures is a field with a rich history and recently it has been getting more attention, especially in the soft robotics domain. The compliance of soft materials endows soft robots with complex behavior, but it also makes their design process unintuitive and in need of automated design. Despite the great interest, evolved virtual soft robots lack the complexity, and co-optimization of morphology and control remains a challenging problem. Prior work identifies and investigates a major issue with the co-optimization process -- fragile co-adaptation of brain and body resulting in premature convergence of morphology. In this work, we expand the investigation of this phenomenon by comparing learnable controllers with proprioceptive observations and fixed controllers without any observations, whereas in the latter case, we only have the optimization of the morphology. Our experiments in two morphology spaces and two environments that vary in complexity show, concrete examples of the existence of high-performing regions in the morphology space that are not able to be discovered during the co-optimization of the morphology and control, yet exist and are easily findable when optimizing morphologies alone. Thus this work clearly demonstrates and characterizes the challenges of optimizing morphology during co-optimization. Based on these results, we propose a new body-centric framework to think about the co-optimization problem which helps us understand the issue from a search perspective. We hope the insights we share with this work attract more attention to the problem and help us to enable efficient brain-body co-optimization.","sentences":["Evolving virtual creatures is a field with a rich history and recently it has been getting more attention, especially in the soft robotics domain.","The compliance of soft materials endows soft robots with complex behavior, but it also makes their design process unintuitive and in need of automated design.","Despite the great interest, evolved virtual soft robots lack the complexity, and co-optimization of morphology and control remains a challenging problem.","Prior work identifies and investigates a major issue with the co-optimization process -- fragile co-adaptation of brain and body resulting in premature convergence of morphology.","In this work, we expand the investigation of this phenomenon by comparing learnable controllers with proprioceptive observations and fixed controllers without any observations, whereas in the latter case, we only have the optimization of the morphology.","Our experiments in two morphology spaces and two environments that vary in complexity show, concrete examples of the existence of high-performing regions in the morphology space that are not able to be discovered during the co-optimization of the morphology and control, yet exist and are easily findable when optimizing morphologies alone.","Thus this work clearly demonstrates and characterizes the challenges of optimizing morphology during co-optimization.","Based on these results, we propose a new body-centric framework to think about the co-optimization problem which helps us understand the issue from a search perspective.","We hope the insights we share with this work attract more attention to the problem and help us to enable efficient brain-body co-optimization."],"url":"http://arxiv.org/abs/2402.09231v1","category":"cs.RO"}
{"created":"2024-02-14 14:35:57","title":"Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration","abstract":"Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy. Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model. However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios. To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration. Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set. We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties. A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets. Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.","sentences":["Although deep neural networks yield high classification accuracy given sufficient training data, their predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy.","Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model.","However, current approaches assume congruence between test and validation data distributions, limiting their applicability to out-of-distribution scenarios.","To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration.","Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through data augmentation on the validation set.","We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties.","A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets.","Extensive experimental results on MNIST, CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2402.09204v1","category":"cs.CV"}
{"created":"2024-02-14 13:08:25","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis","abstract":"In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT.","sentences":["In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings.","This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations.","There is a recognized need for models capable of efficient analysis.","While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology.","To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries.","To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism.","Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain.","We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions.","Due to concerns regarding data privacy, the dataset will not be made publicly available.","However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT."],"url":"http://arxiv.org/abs/2402.09151v1","category":"cs.CL"}
{"created":"2024-02-14 12:55:28","title":"ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks","abstract":"In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications.","sentences":["In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them.","Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability.","Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs.","However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers.","To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers.","By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance.","Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs.","Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training.","Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs.","Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications."],"url":"http://arxiv.org/abs/2402.09146v1","category":"cs.LG"}
{"created":"2024-02-14 11:26:23","title":"Localization engineering by resonant driving in dissipative polariton arrays","abstract":"Arrays of microcavity polaritons are very versatile systems that allow for broad possibilities for the engineering of multi-orbital lattice geometries using different state preparation schemes. One of these schemes, spatially modulated resonant driving, can be used to selectively localize the polariton field on a particular region of the lattice. Both the frequency and the spatial amplitude distribution (module and phase) of the driven laser field are important and serve as a knob to control the extend of the spatial localization. Here, we analyse both the linear and nonlinear regimes using the lattice Green function formalism that is particularly suitable for the case of polariton arrays described in a tight-binding approximation. We identify the conditions for maximum localization on arbitrary lattice's geometries and discuss some experimentally relevant cases. We find that the polariton-polariton interaction leads to a frequency shift of the optimal localization condition that could be used to further control it.","sentences":["Arrays of microcavity polaritons are very versatile systems that allow for broad possibilities for the engineering of multi-orbital lattice geometries using different state preparation schemes.","One of these schemes, spatially modulated resonant driving, can be used to selectively localize the polariton field on a particular region of the lattice.","Both the frequency and the spatial amplitude distribution (module and phase) of the driven laser field are important and serve as a knob to control the extend of the spatial localization.","Here, we analyse both the linear and nonlinear regimes using the lattice Green function formalism that is particularly suitable for the case of polariton arrays described in a tight-binding approximation.","We identify the conditions for maximum localization on arbitrary lattice's geometries and discuss some experimentally relevant cases.","We find that the polariton-polariton interaction leads to a frequency shift of the optimal localization condition that could be used to further control it."],"url":"http://arxiv.org/abs/2402.09104v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-14 11:20:47","title":"Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs","abstract":"Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.","sentences":["Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup.","This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions.","In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence.","Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames.","By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames.","We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs.","Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results."],"url":"http://arxiv.org/abs/2402.09100v1","category":"cs.CV"}
{"created":"2024-02-14 11:14:52","title":"Unity is Strength: Enhancing Precision in Reentrancy Vulnerability Detection of Smart Contract Analysis Tools","abstract":"Reentrancy is one of the most notorious vulnerabilities in smart contracts, resulting in significant digital asset losses. However, many previous works indicate that current Reentrancy detection tools suffer from high false positive rates. Even worse, recent years have witnessed the emergence of new Reentrancy attack patterns fueled by intricate and diverse vulnerability exploit mechanisms. Unfortunately, current tools face a significant limitation in their capacity to adapt and detect these evolving Reentrancy patterns. Consequently, ensuring precise and highly extensible Reentrancy vulnerability detection remains critical challenges for existing tools. To address this issue, we propose a tool named ReEP, designed to reduce the false positives for Reentrancy vulnerability detection. Additionally, ReEP can integrate multiple tools, expanding its capacity for vulnerability detection. It evaluates results from existing tools to verify vulnerability likelihood and reduce false positives. ReEP also offers excellent extensibility, enabling the integration of different detection tools to enhance precision and cover different vulnerability attack patterns. We perform ReEP to eight existing state-of-the-art Reentrancy detection tools. The average precision of these eight tools increased from the original 0.5% to 73% without sacrificing recall. Furthermore, ReEP exhibits robust extensibility. By integrating multiple tools, the precision further improved to a maximum of 83.6%. These results demonstrate that ReEP effectively unites the strengths of existing works, enhances the precision of Reentrancy vulnerability detection tools.","sentences":["Reentrancy is one of the most notorious vulnerabilities in smart contracts, resulting in significant digital asset losses.","However, many previous works indicate that current Reentrancy detection tools suffer from high false positive rates.","Even worse, recent years have witnessed the emergence of new Reentrancy attack patterns fueled by intricate and diverse vulnerability exploit mechanisms.","Unfortunately, current tools face a significant limitation in their capacity to adapt and detect these evolving Reentrancy patterns.","Consequently, ensuring precise and highly extensible Reentrancy vulnerability detection remains critical challenges for existing tools.","To address this issue, we propose a tool named ReEP, designed to reduce the false positives for Reentrancy vulnerability detection.","Additionally, ReEP can integrate multiple tools, expanding its capacity for vulnerability detection.","It evaluates results from existing tools to verify vulnerability likelihood and reduce false positives.","ReEP also offers excellent extensibility, enabling the integration of different detection tools to enhance precision and cover different vulnerability attack patterns.","We perform ReEP to eight existing state-of-the-art Reentrancy detection tools.","The average precision of these eight tools increased from the original 0.5% to 73% without sacrificing recall.","Furthermore, ReEP exhibits robust extensibility.","By integrating multiple tools, the precision further improved to a maximum of 83.6%.","These results demonstrate that ReEP effectively unites the strengths of existing works, enhances the precision of Reentrancy vulnerability detection tools."],"url":"http://arxiv.org/abs/2402.09094v2","category":"cs.CR"}
{"created":"2024-02-14 11:11:41","title":"Software in the natural world: A computational approach to emergence in complex multi-level systems","abstract":"Understanding the functional architecture of complex systems is crucial to illuminate their inner workings and enable effective methods for their prediction and control. Recent advances have introduced tools to characterise emergent macroscopic levels; however, while these approaches are successful in identifying when emergence takes place, they are limited in the extent they can determine how it does. Here we address this limitation by developing a computational approach to emergence, which characterises macroscopic processes in terms of their computational capabilities. Concretely, we articulate a view on emergence based on how software works, which is rooted on a mathematical formalism that articulates how macroscopic processes can express self-contained informational, interventional, and computational properties. This framework establishes a hierarchy of nested self-contained processes that determines what computations take place at what level, which in turn delineates the functional architecture of a complex system. This approach is illustrated on paradigmatic models from the statistical physics and computational neuroscience literature, which are shown to exhibit macroscopic processes that are akin to software in human-engineered systems. Overall, this framework enables a deeper understanding of the multi-level structure of complex systems, revealing specific ways in which they can be efficiently simulated, predicted, and controlled.","sentences":["Understanding the functional architecture of complex systems is crucial to illuminate their inner workings and enable effective methods for their prediction and control.","Recent advances have introduced tools to characterise emergent macroscopic levels; however, while these approaches are successful in identifying when emergence takes place, they are limited in the extent they can determine how it does.","Here we address this limitation by developing a computational approach to emergence, which characterises macroscopic processes in terms of their computational capabilities.","Concretely, we articulate a view on emergence based on how software works, which is rooted on a mathematical formalism that articulates how macroscopic processes can express self-contained informational, interventional, and computational properties.","This framework establishes a hierarchy of nested self-contained processes that determines what computations take place at what level, which in turn delineates the functional architecture of a complex system.","This approach is illustrated on paradigmatic models from the statistical physics and computational neuroscience literature, which are shown to exhibit macroscopic processes that are akin to software in human-engineered systems.","Overall, this framework enables a deeper understanding of the multi-level structure of complex systems, revealing specific ways in which they can be efficiently simulated, predicted, and controlled."],"url":"http://arxiv.org/abs/2402.09090v1","category":"nlin.AO"}
{"created":"2024-02-14 10:39:16","title":"Preserving system activity while controlling epidemic spreading in adaptive temporal networks","abstract":"Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population. Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations. We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state. We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness. We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism. Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives. We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations.","sentences":["Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population.","Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations.","We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state.","We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness.","We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism.","Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives.","We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations."],"url":"http://arxiv.org/abs/2402.09076v1","category":"physics.soc-ph"}
{"created":"2024-02-14 10:35:26","title":"Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards","abstract":"The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes steady-state errors but also results in smoother variations in system states.","sentences":["The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance.","Issues of steady-state error often manifest when quadratic reward functions are employed.","Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes.","In response to this challenge, this study proposes an approach that introduces an integral term.","By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors.","Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes steady-state errors but also results in smoother variations in system states."],"url":"http://arxiv.org/abs/2402.09075v1","category":"eess.SY"}
{"created":"2024-02-14 10:34:06","title":"Stable-to-unstable transition in quantum friction","abstract":"We investigate the frictional force arising from quantum fluctuations when two dissipative metallic plates are set in a shear motion. While early studies showed that the electromagnetic fields in the quantum friction setup reach nonequilibrium steady states, yielding a time-independent force, other works have demonstrated the failure to attain steady states, leading to instability and time-varying friction under sufficiently low-loss conditions. Here, we develop a fully quantum-mechanical theory without perturbative approximations and unveil the transition from stable to unstable regimes of the quantum friction setup. Due to the relative motion of the plates, their electromagnetic response may be active in some conditions, resulting in optical gain. We prove that the standard fluctuation-dissipation leads to inconsistent results when applied to our system, and, in particular, it predicts a vanishing frictional force. Using a modified fluctuation-dissipation relation tailored for gain media, we calculate the frictional force in terms of the system Green's function, thereby recovering early works on quantum friction. Remarkably, we also find that the frictional force diverges to infinity as the relative velocity of the plates approaches a threshold. This threshold is determined by the damping strength and the distance between the metal surfaces. Beyond this critical velocity, the system exhibits instability, akin to the behaviour of a laser cavity, where no steady state exists. In such a scenario, the frictional force escalates exponentially. Our findings pave the way for experimental exploration of the frictional force in proximity to this critical regime.","sentences":["We investigate the frictional force arising from quantum fluctuations when two dissipative metallic plates are set in a shear motion.","While early studies showed that the electromagnetic fields in the quantum friction setup reach nonequilibrium steady states, yielding a time-independent force, other works have demonstrated the failure to attain steady states, leading to instability and time-varying friction under sufficiently low-loss conditions.","Here, we develop a fully quantum-mechanical theory without perturbative approximations and unveil the transition from stable to unstable regimes of the quantum friction setup.","Due to the relative motion of the plates, their electromagnetic response may be active in some conditions, resulting in optical gain.","We prove that the standard fluctuation-dissipation leads to inconsistent results when applied to our system, and, in particular, it predicts a vanishing frictional force.","Using a modified fluctuation-dissipation relation tailored for gain media, we calculate the frictional force in terms of the system Green's function, thereby recovering early works on quantum friction.","Remarkably, we also find that the frictional force diverges to infinity as the relative velocity of the plates approaches a threshold.","This threshold is determined by the damping strength and the distance between the metal surfaces.","Beyond this critical velocity, the system exhibits instability, akin to the behaviour of a laser cavity, where no steady state exists.","In such a scenario, the frictional force escalates exponentially.","Our findings pave the way for experimental exploration of the frictional force in proximity to this critical regime."],"url":"http://arxiv.org/abs/2402.09074v1","category":"quant-ph"}
{"created":"2024-02-14 10:33:33","title":"Selective decision making and collective behavior of fish by the motion of visual attention","abstract":"Collective motion provides a spectacular example of self-organization in Nature. Visual information plays a crucial role among various types of information in determining interactions. Recently, experiments have revealed that organisms such as fish and insects selectively utilize a portion, rather than the entirety, of visual information. Here, focusing on fish, we propose an agent-based model where the direction of attention is guided by visual stimuli received from the images of nearby fish. Our model reproduces a branching phenomenon where a fish selectively follows a specific individual as the distance between two or three nearby fish increases. Furthermore, our model replicates various patterns of collective motion in a group of agents, such as vortex, polarized school, swarm, and turning. We also discuss the topological nature of visual interaction, as well as the positional distribution of nearby fish and the map of pairwise and three-body interactions induced by them. Through a comprehensive comparison with existing experimental results, we clarify the roles of visual interactions and issues to be resolved by other forms of interactions.","sentences":["Collective motion provides a spectacular example of self-organization in Nature.","Visual information plays a crucial role among various types of information in determining interactions.","Recently, experiments have revealed that organisms such as fish and insects selectively utilize a portion, rather than the entirety, of visual information.","Here, focusing on fish, we propose an agent-based model where the direction of attention is guided by visual stimuli received from the images of nearby fish.","Our model reproduces a branching phenomenon where a fish selectively follows a specific individual as the distance between two or three nearby fish increases.","Furthermore, our model replicates various patterns of collective motion in a group of agents, such as vortex, polarized school, swarm, and turning.","We also discuss the topological nature of visual interaction, as well as the positional distribution of nearby fish and the map of pairwise and three-body interactions induced by them.","Through a comprehensive comparison with existing experimental results, we clarify the roles of visual interactions and issues to be resolved by other forms of interactions."],"url":"http://arxiv.org/abs/2402.09073v1","category":"nlin.AO"}
{"created":"2024-02-14 09:16:06","title":"Ecient spatial designs for targeted regions or level set detection","abstract":"Acquiring information on spatial phenomena can be costly and time-consuming. In this context, to obtain reliable global knowledge, the choice of measurement location is a crucial issue. Space-lling designs are often used to control variability uniformly across the whole space. However, in a monitoring context, it is more relevant to focus on crucial regions, especially when dealing with sensitive areas such as the environment, climate or public health. It is therefore important to choose a relevant optimality criterion to build models adapted to the purpose of the experiment. In this article, we propose two new optimality criteria: the rst aims to focus on areas where the response exceeds a given threshold, while the second is suitable for estimating sets of levels. We introduce several algorithms for constructing optimal designs. We also focus on cost-eective algorithms that produce non-optimal but ecient designs. For both sequential and non-sequential contexts, we compare our designs with existing ones through extensive simulation studies.","sentences":["Acquiring information on spatial phenomena can be costly and time-consuming.","In this context, to obtain reliable global knowledge, the choice of measurement location is a crucial issue.","Space-lling designs are often used to control variability uniformly across the whole space.","However, in a monitoring context, it is more relevant to focus on crucial regions, especially when dealing with sensitive areas such as the environment, climate or public health.","It is therefore important to choose a relevant optimality criterion to build models adapted to the purpose of the experiment.","In this article, we propose two new optimality criteria: the rst aims to focus on areas where the response exceeds a given threshold, while the second is suitable for estimating sets of levels.","We introduce several algorithms for constructing optimal designs.","We also focus on cost-eective algorithms that produce non-optimal but ecient designs.","For both sequential and non-sequential contexts, we compare our designs with existing ones through extensive simulation studies."],"url":"http://arxiv.org/abs/2402.09032v1","category":"stat.AP"}
{"created":"2024-02-14 09:01:55","title":"Using Fricke modular polynomials to compute isogenies","abstract":"Let $\\mathcal{E}$ be an elliptic curve over a field $\\mathbf{K}$ and $\\ell$ a prime. There exists an elliptic curve $\\mathcal{E}^*$ related to $\\mathcal{E}$ by an isogeny of degree $\\ell$ only if $\\Phi_\\ell^t(X, j(\\mathcal{E})) = 0$, where $\\Phi_\\ell^t(X, Y)$ is the traditional modular polynomial. Moreover, $\\Phi_\\ell^t$ gives the coefficients of $\\mathcal{E}^*$, together with parameters needed to build the isogeny explicitly. Since $\\Phi_\\ell^t$ has very large coefficients, many families with smaller coefficients can be used instead, as described by Elkies, Atkin and others. In this work, we concentrate on the computation of the family of modular polynomials introduced by Fricke and more recently used by Charlap, Coley and Robbins. In some cases, the resulting polynomials are small, which justifies the interest of this study. We review and adapt the known algorithms to perform the computations of these polynomials. After describing the use of series computations, we investigate fast algorithms using floating point numbers based on fast numerical evaluation of Eisenstein series. We also explain how to use isogeny volcanoes as an alternative. The last part is concerned with finding explicit formulas for computing the coefficients of $\\mathcal{E}^*$. To this we add tables of numerical examples.","sentences":["Let $\\mathcal{E}$ be an elliptic curve over a field $\\mathbf{K}$ and $\\ell$ a prime.","There exists an elliptic curve $\\mathcal{E}^*$ related to $\\mathcal{E}$ by an isogeny of degree $\\ell$ only if $\\Phi_\\ell^t(X, j(\\mathcal{E}))","= 0$, where $\\Phi_\\ell^t(X, Y)$ is the traditional modular polynomial.","Moreover, $\\Phi_\\ell^t$ gives the coefficients of $\\mathcal{E}^*$, together with parameters needed to build the isogeny explicitly.","Since $\\Phi_\\ell^t$ has very large coefficients, many families with smaller coefficients can be used instead, as described by Elkies, Atkin and others.","In this work, we concentrate on the computation of the family of modular polynomials introduced by Fricke and more recently used by Charlap, Coley and Robbins.","In some cases, the resulting polynomials are small, which justifies the interest of this study.","We review and adapt the known algorithms to perform the computations of these polynomials.","After describing the use of series computations, we investigate fast algorithms using floating point numbers based on fast numerical evaluation of Eisenstein series.","We also explain how to use isogeny volcanoes as an alternative.","The last part is concerned with finding explicit formulas for computing the coefficients of $\\mathcal{E}^*$. To this we add tables of numerical examples."],"url":"http://arxiv.org/abs/2402.09027v1","category":"math.NT"}
{"created":"2024-02-14 08:50:14","title":"Neural Operators Meet Energy-based Theory: Operator Learning for Hamiltonian and Dissipative PDEs","abstract":"The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces. Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs). However, these works still struggle to learn dynamics that obeys the laws of physics. This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories. We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs. Experiments on multiple physical systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings.","sentences":["The operator learning has received significant attention in recent years, with the aim of learning a mapping between function spaces.","Prior works have proposed deep neural networks (DNNs) for learning such a mapping, enabling the learning of solution operators of partial differential equations (PDEs).","However, these works still struggle to learn dynamics that obeys the laws of physics.","This paper proposes Energy-consistent Neural Operators (ENOs), a general framework for learning solution operators of PDEs that follows the energy conservation or dissipation law from observed solution trajectories.","We introduce a novel penalty function inspired by the energy-based theory of physics for training, in which the energy functional is modeled by another DNN, allowing one to bias the outputs of the DNN-based solution operators to ensure energetic consistency without explicit PDEs.","Experiments on multiple physical systems show that ENO outperforms existing DNN models in predicting solutions from data, especially in super-resolution settings."],"url":"http://arxiv.org/abs/2402.09018v1","category":"stat.ML"}
{"created":"2024-02-14 18:59:27","title":"Auditing Private Prediction","abstract":"Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist forDP training algorithms. However machine learning can also be made private at inference. We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poisoningand query capabilities. This enables us to study the privacy leakage of four private prediction algorithms:PATE [Papernot et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE [Duan et al., 2023],and Private-kNN [Zhu et al., 2020]. To conduct our audit, we introduce novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis ofprivate prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacyleakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than thosewith full control.","sentences":["Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound.","Auditing techniques exist forDP training algorithms.","However machine learning can also be made private at inference.","We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poisoningand query capabilities.","This enables us to study the privacy leakage of four private prediction algorithms:PATE","[Papernot et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE","[Duan et al., 2023],and Private-kNN [Zhu et al., 2020].","To conduct our audit, we introduce novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP.","Our experiments show that (i) the privacy analysis ofprivate prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacyleakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than thosewith full control."],"url":"http://arxiv.org/abs/2402.09403v1","category":"cs.CR"}
{"created":"2024-02-14 18:59:16","title":"From Architectures to Applications: A Review of Neural Quantum States","abstract":"Due to the exponential growth of the Hilbert space dimension with system size, the simulation of quantum many-body systems has remained a persistent challenge until today. Here, we review a relatively new class of variational states for the simulation of such systems, namely neural quantum states (NQS), which overcome the exponential scaling by compressing the state in terms of the network parameters rather than storing all exponentially many coefficients needed for an exact parameterization of the state. We introduce the commonly used NQS architectures and their various applications for the simulation of ground and excited states, finite temperature and open system states as well as NQS approaches to simulate the dynamics of quantum states. Furthermore, we discuss NQS in the context of quantum state tomography.","sentences":["Due to the exponential growth of the Hilbert space dimension with system size, the simulation of quantum many-body systems has remained a persistent challenge until today.","Here, we review a relatively new class of variational states for the simulation of such systems, namely neural quantum states (NQS), which overcome the exponential scaling by compressing the state in terms of the network parameters rather than storing all exponentially many coefficients needed for an exact parameterization of the state.","We introduce the commonly used NQS architectures and their various applications for the simulation of ground and excited states, finite temperature and open system states as well as NQS approaches to simulate the dynamics of quantum states.","Furthermore, we discuss NQS in the context of quantum state tomography."],"url":"http://arxiv.org/abs/2402.09402v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-14 18:26:58","title":"GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly","abstract":"Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes. In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection. We evaluate our method using simulated and synthetic metagenomic datasets. The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences. Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall.","sentences":["Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment.","This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities.","Detecting repeats is a crucial first step in overcoming these challenges.","To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories.","Specifically, we frame this problem as a node classification task within a metagenomic assembly graph.","In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes.","We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes.","In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection.","We evaluate our method using simulated and synthetic metagenomic datasets.","The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences.","Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance.","Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall."],"url":"http://arxiv.org/abs/2402.09381v1","category":"cs.LG"}
{"created":"2024-02-14 17:18:15","title":"YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection","abstract":"Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist. With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD). In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body. Attention mechanism is one of the hottest methods to improve the model performance. This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture. Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement. Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.","sentences":["Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases.","Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist.","With the development of neural networks, You Only Look Once (YOLO) series models have been widely used in fracture detection as computer-assisted diagnosis (CAD).","In 2023, Ultralytics presented the latest version of the YOLO models, which has been employed for detecting fractures across various parts of the body.","Attention mechanism is one of the hottest methods to improve the model performance.","This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture.","Specifically, we respectively employ four attention modules, Convolutional Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset.","Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance.","Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement.","Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%."],"url":"http://arxiv.org/abs/2402.09329v1","category":"cs.CV"}
{"created":"2024-02-14 17:16:39","title":"PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames in Autonomous Driving Environments","abstract":"Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf.","sentences":["Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames.","However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution.","Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored.","To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF).","Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels.","The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation.","With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes.","Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs.","Our approach implementation and the pre-trained models are available at https://github.com/biter0088/pc-nerf."],"url":"http://arxiv.org/abs/2402.09325v1","category":"cs.CV"}
{"created":"2024-02-14 17:15:59","title":"Development of a gyrokinetic-MHD energetic particle simulation code Part I: MHD version","abstract":"A new magnetohydrodynamics (MHD) code based on initial value approach, GMEC_I, has been developed for simulating various MHD physics in tokamak plasmas, as the MHD foundation of the gyrokinetic-MHD energetic particle simulation code (GMEC) family. GMEC_I solves multi-level reduced-MHD models that form a hierarchy of physics complexity, which provide conveniences for the cross-code verification and the identification of key physics effect in tokamak geometry. The field-aligned coordinates are used to represent mode structure efficiently. High-order finite difference methods are used for spatial discretization. The shifted metric methods are used for numerical stability. The discrete expansion forms of physics equations in the code are generated symbolically using the compile-time symbolic solver (CSS), which is specifically developed to reduce the complexity of the high-order finite difference form of the MHD equations. Advanced computational techniques have been implemented for optimizing memory access and code parallelization that show a good efficiency using both Thread Building Block (TBB) and Message Passing Interface (MPI). Benchmarks between GMEC_I and the eigenvalue code MAS are presented for ballooning modes without and with diamagnetic drift effects, and tearing modes, which show excellent agreements.","sentences":["A new magnetohydrodynamics (MHD) code based on initial value approach, GMEC_I, has been developed for simulating various MHD physics in tokamak plasmas, as the MHD foundation of the gyrokinetic-MHD energetic particle simulation code (GMEC) family.","GMEC_I solves multi-level reduced-MHD models that form a hierarchy of physics complexity, which provide conveniences for the cross-code verification and the identification of key physics effect in tokamak geometry.","The field-aligned coordinates are used to represent mode structure efficiently.","High-order finite difference methods are used for spatial discretization.","The shifted metric methods are used for numerical stability.","The discrete expansion forms of physics equations in the code are generated symbolically using the compile-time symbolic solver (CSS), which is specifically developed to reduce the complexity of the high-order finite difference form of the MHD equations.","Advanced computational techniques have been implemented for optimizing memory access and code parallelization that show a good efficiency using both Thread Building Block (TBB) and Message Passing Interface (MPI).","Benchmarks between GMEC_I and the eigenvalue code MAS are presented for ballooning modes without and with diamagnetic drift effects, and tearing modes, which show excellent agreements."],"url":"http://arxiv.org/abs/2402.09324v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 17:14:24","title":"Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling","abstract":"The development of blood-handling medical devices, such as ventricular assist devices, requires the analysis of their biocompatibility. Among other aspects, this includes hemolysis, i.e., red blood cell damage. For this purpose, computational fluid dynamics (CFD) methods are employed to predict blood flow in prototypes. The most basic hemolysis models directly estimate red blood cell damage from fluid stress in the resulting flow field. More advanced models explicitly resolve cell deformation. On the downside, these models are typically written in a Lagrangian formulation, i.e., they require pathline tracking. We present a new Eulerian description of cell deformation, enabling the evaluation of the solution across the whole domain. The resulting hemolysis model can be applied to any converged CFD simulation due to one-way coupling with the fluid velocity field. We discuss the efficient numerical treatment of the model equations in a stabilized finite element context. We validate the model by comparison to the original Lagrangian formulation in selected benchmark flows. Two more complex test cases demonstrate the method's capabilities in real-world applications. The results highlight the advantages over previous hemolysis models. In conclusion, the model holds great potential for the design process of future generations of medical devices.","sentences":["The development of blood-handling medical devices, such as ventricular assist devices, requires the analysis of their biocompatibility.","Among other aspects, this includes hemolysis, i.e., red blood cell damage.","For this purpose, computational fluid dynamics (CFD) methods are employed to predict blood flow in prototypes.","The most basic hemolysis models directly estimate red blood cell damage from fluid stress in the resulting flow field.","More advanced models explicitly resolve cell deformation.","On the downside, these models are typically written in a Lagrangian formulation, i.e., they require pathline tracking.","We present a new Eulerian description of cell deformation, enabling the evaluation of the solution across the whole domain.","The resulting hemolysis model can be applied to any converged CFD simulation due to one-way coupling with the fluid velocity field.","We discuss the efficient numerical treatment of the model equations in a stabilized finite element context.","We validate the model by comparison to the original Lagrangian formulation in selected benchmark flows.","Two more complex test cases demonstrate the method's capabilities in real-world applications.","The results highlight the advantages over previous hemolysis models.","In conclusion, the model holds great potential for the design process of future generations of medical devices."],"url":"http://arxiv.org/abs/2402.09319v1","category":"physics.flu-dyn"}
{"created":"2024-02-14 17:03:04","title":"Mixture to Mixture: Leveraging Close-talk Mixtures as Weak-supervision for Speech Separation","abstract":"We propose mixture to mixture (M2M) training, a weakly-supervised neural speech separation algorithm that leverages close-talk mixtures as a weak supervision for training discriminative models to separate far-field mixtures. Our idea is that, for a target speaker, its close-talk mixture has a much higher signal-to-noise ratio (SNR) of the target speaker than any far-field mixtures, and hence could be utilized to design a weak supervision for separation. To realize this, at each training step we feed a far-field mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, and, for each of considered close-talk and far-field microphones, we linearly filter the DNN estimates and optimize a loss so that the filtered estimates of all the speakers can sum up to the mixture captured by each of the considered microphones. Evaluation results on a 2-speaker separation task in simulated reverberant conditions show that M2M can effectively leverage close-talk mixtures as a weak supervision for separating far-field mixtures.","sentences":["We propose mixture to mixture (M2M) training, a weakly-supervised neural speech separation algorithm that leverages close-talk mixtures as a weak supervision for training discriminative models to separate far-field mixtures.","Our idea is that, for a target speaker, its close-talk mixture has a much higher signal-to-noise ratio (SNR) of the target speaker than any far-field mixtures, and hence could be utilized to design a weak supervision for separation.","To realize this, at each training step we feed a far-field mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, and, for each of considered close-talk and far-field microphones, we linearly filter the DNN estimates and optimize a loss so that the filtered estimates of all the speakers can sum up to the mixture captured by each of the considered microphones.","Evaluation results on a 2-speaker separation task in simulated reverberant conditions show that M2M can effectively leverage close-talk mixtures as a weak supervision for separating far-field mixtures."],"url":"http://arxiv.org/abs/2402.09313v1","category":"eess.AS"}
{"created":"2024-02-14 16:10:31","title":"Proton-proton and proton-cluster femtoscopy at the HADES experiment","abstract":"This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI. Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3). For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed. Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics. This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI. Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3). For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed. Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics. This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.","sentences":["This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI.","Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3).","For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed.","Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics.","This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics.","This work explores femtoscopic correlations in proton-proton and proton-cluster systems at the HADES experiment, GSI.","Through high-precision correlation functions, it reveals the impact of strong interactions between protons and light nuclei (deuteron, triton, Helium-3).","For proton-proton interactions, measurements in two centralities and five $k_T$ bins were analyzed.","Extracted radii were compared with different Equations of State in UrQMD, providing valuable insights into collision dynamics.","This research enhances our understanding of particle interactions and contributes to refining theoretical models in nuclear and particle physics."],"url":"http://arxiv.org/abs/2402.09280v1","category":"nucl-ex"}
{"created":"2024-02-14 16:09:22","title":"Disentangling the origin of chemical differences using GHOST","abstract":"Aims. We explore different scenarios to explain the chemical difference found in the remarkable giant-giant binary system HD 138202 + CD-30 12303. For the first time, we suggest how to distinguish these scenarios by taking advantage of the extensive convective envelopes of giant stars. Methods. We carried out a high-precision determination of stellar parameters and abundances by applying a full line-by-line differential analysis on GHOST high-resolution spectra. Results. We found a significant chemical difference between the two stars (0.08 dex), which is largely unexpected considering the insensitivity of giant stars to planetary ingestion and diffusion effects. We tested the possibility of engulfment events by using several different combinations of stellar mass, ingested mass, metallicity of the engulfed object and different convective envelopes. However, the planetary ingestion scenario does not seem to explain the observed differences. For the first time, we distinguished the source of chemical differences using a giant-giant binary system. By ruling out other possible scenarios such as planet formation and evolutionary effects between the two stars, we suggest that primordial inhomogeneities might explain the observed differences. This remarkable result implies that the metallicity differences that were observed in at least some main-sequence binary systems might be related to primordial inhomogeneities rather than engulfment events. We also discuss the important implications of finding primordial inhomogeneities, which affect chemical tagging and other fields such as planet formation. We strongly encourage the use of giant-giant pairs. They are a relevant complement to main-sequence pairs for determining the origin of the observed chemical differences in multiple systems. [abridged]","sentences":["Aims.","We explore different scenarios to explain the chemical difference found in the remarkable giant-giant binary system HD 138202 + CD-30 12303.","For the first time, we suggest how to distinguish these scenarios by taking advantage of the extensive convective envelopes of giant stars.","Methods.","We carried out a high-precision determination of stellar parameters and abundances by applying a full line-by-line differential analysis on GHOST high-resolution spectra. Results.","We found a significant chemical difference between the two stars (0.08 dex), which is largely unexpected considering the insensitivity of giant stars to planetary ingestion and diffusion effects.","We tested the possibility of engulfment events by using several different combinations of stellar mass, ingested mass, metallicity of the engulfed object and different convective envelopes.","However, the planetary ingestion scenario does not seem to explain the observed differences.","For the first time, we distinguished the source of chemical differences using a giant-giant binary system.","By ruling out other possible scenarios such as planet formation and evolutionary effects between the two stars, we suggest that primordial inhomogeneities might explain the observed differences.","This remarkable result implies that the metallicity differences that were observed in at least some main-sequence binary systems might be related to primordial inhomogeneities rather than engulfment events.","We also discuss the important implications of finding primordial inhomogeneities, which affect chemical tagging and other fields such as planet formation.","We strongly encourage the use of giant-giant pairs.","They are a relevant complement to main-sequence pairs for determining the origin of the observed chemical differences in multiple systems.","[abridged]"],"url":"http://arxiv.org/abs/2402.09278v1","category":"astro-ph.SR"}
{"created":"2024-02-14 15:59:22","title":"Hybrid Machine Learning techniques in the management of harmful algal blooms impact","abstract":"Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit. This new option is the most similar to the actual functioning of the control of shellfish production areas. For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas. The study has been carried out in several estuaries with different levels of complexity in the episodes of algal blooms to demonstrate the generalization capacity of the models in bloom detection. As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness.","sentences":["Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption.","Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues.","To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected.","At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible.","Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models.","This is largely due to the irregularity of the data due to the established sampling programs.","As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit.","This new option is the most similar to the actual functioning of the control of shellfish production areas.","For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas.","The study has been carried out in several estuaries with different levels of complexity in the episodes of algal blooms to demonstrate the generalization capacity of the models in bloom detection.","As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness."],"url":"http://arxiv.org/abs/2402.09271v1","category":"cs.LG"}
{"created":"2024-02-14 15:54:55","title":"Transformers, parallel computation, and logarithmic depth","abstract":"We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.","sentences":["We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation.","As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations.","We thus establish parallelism as a key distinguishing property of transformers."],"url":"http://arxiv.org/abs/2402.09268v1","category":"cs.LG"}
{"created":"2024-02-14 15:51:01","title":"Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach","abstract":"Transient stability-constrained preventive redispatch plays a crucial role in ensuring power system security and stability. Since redispatch strategies need to simultaneously satisfy complex transient constraints and the economic need, model-based formulation and optimization become extremely challenging. In addition, the increasing uncertainty and variability introduced by renewable sources start to drive the system stability consideration from deterministic to probabilistic, which further exaggerates the complexity. In this paper, a Graph neural network guided Distributional Deep Reinforcement Learning (GD2RL) method is proposed, for the first time, to solve the uncertainty-aware transient stability-constrained preventive redispatch problem. First, a graph neural network-based transient simulator is trained by supervised learning to efficiently generate post-contingency rotor angle curves with the steady-state and contingency as inputs, which serves as a feature extractor for operating states and a surrogate time-domain simulator during the environment interaction for reinforcement learning. Distributional deep reinforcement learning with explicit uncertainty distribution of system operational conditions is then applied to generate the redispatch strategy to balance the user-specified probabilistic stability performance and economy preferences. The full distribution of the post-control transient stability index is directly provided as the output. Case studies on the modified New England 39-bus system validate the proposed method.","sentences":["Transient stability-constrained preventive redispatch plays a crucial role in ensuring power system security and stability.","Since redispatch strategies need to simultaneously satisfy complex transient constraints and the economic need, model-based formulation and optimization become extremely challenging.","In addition, the increasing uncertainty and variability introduced by renewable sources start to drive the system stability consideration from deterministic to probabilistic, which further exaggerates the complexity.","In this paper, a Graph neural network guided Distributional Deep Reinforcement Learning (GD2RL) method is proposed, for the first time, to solve the uncertainty-aware transient stability-constrained preventive redispatch problem.","First, a graph neural network-based transient simulator is trained by supervised learning to efficiently generate post-contingency rotor angle curves with the steady-state and contingency as inputs, which serves as a feature extractor for operating states and a surrogate time-domain simulator during the environment interaction for reinforcement learning.","Distributional deep reinforcement learning with explicit uncertainty distribution of system operational conditions is then applied to generate the redispatch strategy to balance the user-specified probabilistic stability performance and economy preferences.","The full distribution of the post-control transient stability index is directly provided as the output.","Case studies on the modified New England 39-bus system validate the proposed method."],"url":"http://arxiv.org/abs/2402.09263v1","category":"eess.SY"}
{"created":"2024-02-14 15:39:25","title":"An asymptotic-preserving scheme for Euler equations I: non-ideal gases","abstract":"{We analyze a general Implicit-Explicit (IMEX) time discretization for the compressible Euler equations of gas dynamics, showing that they are asymptotic-preserving (AP) in the low Mach number limit. The analysis is carried out for a general equation of state (EOS). We consider both a single asymptotic length scale and two length scales. We then show that, when coupling these time discretizations with a Discontinuous Galerkin (DG) space discretization with appropriate fluxes, an all Mach number numerical method is obtained. A number of relevant benchmarks for ideal gases and their non-trivial extension to non-ideal EOS validate the performed analysis.","sentences":["{We analyze a general Implicit-Explicit (IMEX) time discretization for the compressible Euler equations of gas dynamics, showing that they are asymptotic-preserving (AP) in the low Mach number limit.","The analysis is carried out for a general equation of state (EOS).","We consider both a single asymptotic length scale and two length scales.","We then show that, when coupling these time discretizations with a Discontinuous Galerkin (DG) space discretization with appropriate fluxes, an all Mach number numerical method is obtained.","A number of relevant benchmarks for ideal gases and their non-trivial extension to non-ideal EOS validate the performed analysis."],"url":"http://arxiv.org/abs/2402.09252v1","category":"math.NA"}
{"created":"2024-02-14 15:38:05","title":"Validity of using Els\u00e4sser variables to study the interaction of compressible solar wind fluctuations with a coronal mass ejection","abstract":"Alfv\\'enic fluctuations, as modelled by the non-linear interactions of Alfv\\'en waves of various scales, are seen to dominate solar wind turbulence. However, there is also a non-negligible component of non-Alfv\\'enic fluctuations. The Els\\\"asser formalism, which is central to the study of Alfv\\'enic turbulence due to its ability to differentiate between parallel and anti-parallel Alfv\\'en waves, cannot strictly separate wavemodes in the presence of compressive magnetoacoustic waves. In this study, we analyse the deviations generated in the Els\\\"asser formalism as density fluctuations are naturally generated through the propagation of a linearly polarised Alfv\\'en wave. The study was performed in the context of a coronal mass ejection (CME) propagating through the solar wind, which enables the creation of two solar wind regimes, pristine wind and a shocked CME sheath, where the Els\\\"asser formalism can be evaluated. In these two regimes we studied the deviations of the Els\\\"asser formalism in separating parallel and anti-parallel components of Alfv\\'enic solar wind perturbations generated by small-amplitude density fluctuations. We used an ideal 2.5D magnetohydrodynamic (MHD) model with an adiabatic equation of state. An Alfv\\'en pump wave was injected into the quiet solar wind by perturbing the transverse magnetic field and velocity components. This wave subsequently generates density fluctuations through the ponderomotive force. A CME was injected by inserting a flux-rope modelled as a magnetic island into the quasi-steady solar wind. The presence of density perturbations creates an approximately 10% deviation in the Els\\\"asser variables and reflection coefficient for the Alfv\\'en waves as well as a deviation of approximately 0.1 in the cross helicity in regions containing both parallel and anti-parallel fluctuations.","sentences":["Alfv\\'enic fluctuations, as modelled by the non-linear interactions of Alfv\\'en waves of various scales, are seen to dominate solar wind turbulence.","However, there is also a non-negligible component of non-Alfv\\'enic fluctuations.","The Els\\\"asser formalism, which is central to the study of Alfv\\'enic turbulence due to its ability to differentiate between parallel and anti-parallel Alfv\\'en waves, cannot strictly separate wavemodes in the presence of compressive magnetoacoustic waves.","In this study, we analyse the deviations generated in the Els\\\"asser formalism as density fluctuations are naturally generated through the propagation of a linearly polarised Alfv\\'en wave.","The study was performed in the context of a coronal mass ejection (CME) propagating through the solar wind, which enables the creation of two solar wind regimes, pristine wind and a shocked CME sheath, where the Els\\\"asser formalism can be evaluated.","In these two regimes we studied the deviations of the Els\\\"asser formalism in separating parallel and anti-parallel components of Alfv\\'enic solar wind perturbations generated by small-amplitude density fluctuations.","We used an ideal 2.5D magnetohydrodynamic (MHD) model with an adiabatic equation of state.","An Alfv\\'en pump wave was injected into the quiet solar wind by perturbing the transverse magnetic field and velocity components.","This wave subsequently generates density fluctuations through the ponderomotive force.","A CME was injected by inserting a flux-rope modelled as a magnetic island into the quasi-steady solar wind.","The presence of density perturbations creates an approximately 10% deviation in the Els\\\"asser variables and reflection coefficient for the Alfv\\'en waves as well as a deviation of approximately 0.1 in the cross helicity in regions containing both parallel and anti-parallel fluctuations."],"url":"http://arxiv.org/abs/2402.09250v1","category":"astro-ph.SR"}
{"created":"2024-02-14 15:35:53","title":"Momentum Approximation in Asynchronous Private Federated Learning","abstract":"Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum approximation can achieve $1.15 \\textrm{--}4\\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum.","sentences":["Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients.","Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL.","However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance.","It is still unclear how to effective combinie these two techniques together to achieve a win-win.","In this paper, we find that asynchrony introduces implicit bias to momentum updates.","In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates.","Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost.","We empirically demonstrate that on benchmark FL datasets, momentum approximation can achieve $1.15 \\textrm{--}4\\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum."],"url":"http://arxiv.org/abs/2402.09247v1","category":"cs.LG"}
{"created":"2024-02-14 15:28:42","title":"Switch EMA: A Free Lunch for Better Flatness and Sharpness","abstract":"Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.","sentences":["Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization.","Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations.","This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA).","From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness.","To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling.","Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds."],"url":"http://arxiv.org/abs/2402.09240v1","category":"cs.LG"}
{"created":"2024-02-14 15:27:53","title":"Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives","abstract":"Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.","sentences":["Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks.","Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss.","During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance.","In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling.","We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples.","Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance."],"url":"http://arxiv.org/abs/2402.09239v1","category":"cs.LG"}
{"created":"2024-02-14 15:17:37","title":"Context Composing for Full Line Code Completion","abstract":"Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.","sentences":["Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer.","Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks.","This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself.","At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer.","We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users.","The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation.","In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area."],"url":"http://arxiv.org/abs/2402.09230v1","category":"cs.SE"}
{"created":"2024-02-14 15:16:35","title":"Invariant conformal Killing forms on almost abelian Lie groups","abstract":"We describe completely conformal Killing or conformal Killing-Yano (CKY) $p$-forms on almost abelian metric Lie algebras. In particular we prove that if a $n$-dimensional almost abelian metric Lie algebra admits a non-parallel CKY $p$-form, then $p=1$ or $p=n-1$. In other words, any CKY $p$-form on a metric almost abelian Lie algebra is parallel for $2\\leq p\\leq n-2$. Moreover, we characterize almost abelian Lie algebras admitting non-parallel CKY $p$-forms, and we classify all Lie algebras with this property up to dimension $5$, distinguishing also those cases where the associated simply connected Lie group admits lattices.","sentences":["We describe completely conformal Killing or conformal Killing-Yano (CKY)","$p$-forms on almost abelian metric Lie algebras.","In particular we prove that if a $n$-dimensional almost abelian metric Lie algebra admits a non-parallel CKY $p$-form, then $p=1$ or $p=n-1$. In other words, any CKY $p$-form on a metric almost abelian Lie algebra is parallel for $2\\leq p\\leq","n-2$. Moreover, we characterize almost abelian Lie algebras admitting non-parallel CKY $p$-forms, and we classify all Lie algebras with this property up to dimension $5$, distinguishing also those cases where the associated simply connected Lie group admits lattices."],"url":"http://arxiv.org/abs/2402.09229v1","category":"math.DG"}
{"created":"2024-02-14 15:10:37","title":"Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks","abstract":"This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.","sentences":["This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin.","For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set.","For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin.","Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points."],"url":"http://arxiv.org/abs/2402.09226v1","category":"cs.LG"}
{"created":"2024-02-14 15:06:44","title":"UV-complete 4-derivative scalar field theory","abstract":"A scalar field theory with 4-derivative kinetic terms and 4-derivative cubic and quartic couplings is presented as a proxy for quantum quadratic gravity (QQG). The scalar theory is renormalizable and asymptotically free and the remaining key issue is unitarity, or more precisely positivity, just as it is in QQG. We have extended calculations for the optical theorem and for a differential cross section, both in the high energy limit, to show how positivity constrains the theory. The results also show how it is that differential cross sections can have good high energy behavior. Finally we use the scalar theory to extend the Stuckelberg theory of a massive U(1) gauge boson to a renormalizable theory of a self-interacting gauge boson.","sentences":["A scalar field theory with 4-derivative kinetic terms and 4-derivative cubic and quartic couplings is presented as a proxy for quantum quadratic gravity (QQG).","The scalar theory is renormalizable and asymptotically free and the remaining key issue is unitarity, or more precisely positivity, just as it is in QQG.","We have extended calculations for the optical theorem and for a differential cross section, both in the high energy limit, to show how positivity constrains the theory.","The results also show how it is that differential cross sections can have good high energy behavior.","Finally we use the scalar theory to extend the Stuckelberg theory of a massive U(1) gauge boson to a renormalizable theory of a self-interacting gauge boson."],"url":"http://arxiv.org/abs/2402.09223v1","category":"hep-th"}
{"created":"2024-02-14 14:56:05","title":"The Mumford Dynamical System and Hyperelliptic Kleinian Functions","abstract":"We establish differential-algebraic theory of the Mumford dynamical system. In the framework of this theory, we introduce the $(P,Q)$-recursion, which defines a sequence of functions $P_1,P_2,\\ldots$ given the first function of this sequence $P_1$ and a sequence of parameters $h_1,h_2,\\ldots$. The general solution of the $(P,Q)$-recursion is shown to give a solution for the parametric graded Korteweg--de Vries hierarchy. We prove that all solutions of the Mumford dynamical $g$-system are determined by the $(P,Q)$-recursion under the condition $P_{g+1} = 0$, which is equivalent to an ordinary nonlinear differential equation of order $2g$ for the function $P_1$. Reduction of the $g$-system of Mumford to the Buchstaber--Enolskii--Leykin dynamical system is described explicitly, and its explicit $2g$-parameter solution in hyperelliptic Klein functions is presented.","sentences":["We establish differential-algebraic theory of the Mumford dynamical system.","In the framework of this theory, we introduce the $(P,Q)$-recursion, which defines a sequence of functions $P_1,P_2,\\ldots$ given the first function of this sequence $P_1$ and a sequence of parameters $h_1,h_2,\\ldots$. The general solution of the $(P,Q)$-recursion is shown to give a solution for the parametric graded Korteweg--de Vries hierarchy.","We prove that all solutions of the Mumford dynamical $g$-system are determined by the $(P,Q)$-recursion under the condition $P_{g+1} = 0$, which is equivalent to an ordinary nonlinear differential equation of order $2g$ for the function $P_1$. Reduction of the $g$-system of Mumford to the Buchstaber--Enolskii--Leykin dynamical system is described explicitly, and its explicit $2g$-parameter solution in hyperelliptic Klein functions is presented."],"url":"http://arxiv.org/abs/2402.09218v1","category":"nlin.SI"}
{"created":"2024-02-14 14:53:46","title":"Modeling of groundwater flow in porous medium layered over inclined impermeable bed","abstract":"We propose a new mathematical model of groundwater flow in porous medium layered over inclined impermeable bed. In its full generality, this is a free-surface problem. To obtain analytically tractable model, we use generalized Dupuit-Forchheimer assumption for inclined impermeable bed. In this way, we arrive at parabolic partial differential equation which is a generalization of the classical Boussinesq equation. Novelty of our approach consists in considering nonlinear constitutive law of the power type. Thus introducing $p$-Laplacian-like differential operator into the Boussinesq equation. Unlike in the classical case of the Boussinesq equation, the convective term cannot be set aside from the main part of the diffusive term and remains incorporated within it.   In the sequel of the paper, we analyze qualitative properties of the stationary solutions of our model. In particular, we study existence and regularity of weak solutions for the following boundary value problem \\begin{equation*} \\begin{aligned}   &   -   \\frac{\\rm d}{{\\rm d} x}   \\left[   (u(x) + H) \\left|\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) + \\sin(\\varphi) \\right|^{p - 2}   \\left(\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) + \\sin(\\varphi)\\right)   \\right]   &   \\begin{aligned}   &   =   f(x)\\,, & \\qquad\\qquad x \\in (-1,1)\\,,   &   u(-1) = u(1) = 0\\,,&   \\end{aligned} \\end{aligned} \\end{equation*} where $p>1$, $H>0$, $\\varphi\\in (0, \\pi/2)$, $f\\geq 0$, $f\\in L^{1}(-1,1)$. In the case of $p>2$, we study validity of Weak and Strong Maximum Principles as well. We use methods based on the linearization of the $p$-Laplacian-type problems in the vicinity of known solution, error estimates, and analysis of Green's function of the linearized problem.","sentences":["We propose a new mathematical model of groundwater flow in porous medium layered over inclined impermeable bed.","In its full generality, this is a free-surface problem.","To obtain analytically tractable model, we use generalized Dupuit-Forchheimer assumption for inclined impermeable bed.","In this way, we arrive at parabolic partial differential equation which is a generalization of the classical Boussinesq equation.","Novelty of our approach consists in considering nonlinear constitutive law of the power type.","Thus introducing $p$-Laplacian-like differential operator into the Boussinesq equation.","Unlike in the classical case of the Boussinesq equation, the convective term cannot be set aside from the main part of the diffusive term and remains incorporated within it.   ","In the sequel of the paper, we analyze qualitative properties of the stationary solutions of our model.","In particular, we study existence and regularity of weak solutions for the following boundary value problem \\begin{equation*} \\begin{aligned}   &   -   \\frac{\\rm d}{{\\rm d} x}   \\left[   (u(x)","+ H) \\left|\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi)","+ \\sin(\\varphi) \\right|^{p - 2}   \\left(\\frac{{\\rm d} u}{{\\rm d} x}(x) \\cos(\\varphi) +","\\sin(\\varphi)\\right)   \\right]   &   \\begin{aligned}   &   =   f(x)\\,, & \\qquad\\qquad x \\in (-1,1)\\,,   &   u(-1) = u(1) = 0\\,,&   \\end{aligned} \\end{aligned} \\end{equation*} where $p>1$, $H>0$, $\\varphi\\in (0, \\pi/2)$, $f\\geq 0$, $f\\in L^{1}(-1,1)$. In the case of $p>2$, we study validity of Weak and Strong Maximum Principles as well.","We use methods based on the linearization of the $p$-Laplacian-type problems in the vicinity of known solution, error estimates, and analysis of Green's function of the linearized problem."],"url":"http://arxiv.org/abs/2402.09215v1","category":"math.AP"}
{"created":"2024-02-14 14:40:29","title":"Monoidal model structures on filtered chain complexes relating to spectral sequences","abstract":"We establish monoidal model structures on model categories of filtered chain complexes constructed by Cirici, Egas Santander, Livernet and Whitehouse whose weak equivalences are the quasi-isomorphisms on the $r$-page of the associated spectral sequences. In doing so we provide a partial classification of cofibrant objects and cofibrations of the model structures involving a boundedness restriction on the filtration. As a consequence we also obtain, by results of Schwede and Shipley, cofibrantly generated model structures on the categories of filtered differential graded algebras as well as their modules.","sentences":["We establish monoidal model structures on model categories of filtered chain complexes constructed by Cirici, Egas Santander, Livernet and Whitehouse whose weak equivalences are the quasi-isomorphisms on the $r$-page of the associated spectral sequences.","In doing so we provide a partial classification of cofibrant objects and cofibrations of the model structures involving a boundedness restriction on the filtration.","As a consequence we also obtain, by results of Schwede and Shipley, cofibrantly generated model structures on the categories of filtered differential graded algebras as well as their modules."],"url":"http://arxiv.org/abs/2402.09207v1","category":"math.AT"}
{"created":"2024-02-14 14:33:39","title":"Better-than-KL PAC-Bayes Bounds","abstract":"Let $f(\\theta, X_1),$ $ \\dots,$ $ f(\\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \\dots, X_n$ are independent random variables (data), and $\\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence. An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function. Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem. Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the KL divergence. However, the tightness of this choice has rarely been questioned.   In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound. In particular, we demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022). Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities. Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL. Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds.","sentences":["Let $f(\\theta, X_1),$ $ \\dots,$ $ f(\\theta, X_n)$ be a sequence of random elements, where $f$ is a fixed scalar function, $X_1, \\dots, X_n$ are independent random variables (data), and $\\theta$ is a random parameter distributed according to some data-dependent posterior distribution $P_n$. In this paper, we consider the problem of proving concentration inequalities to estimate the mean of the sequence.","An example of such a problem is the estimation of the generalization error of some predictor trained by a stochastic algorithm, such as a neural network where $f$ is a loss function.","Classically, this problem is approached through a PAC-Bayes analysis where, in addition to the posterior, we choose a prior distribution which captures our belief about the inductive bias of the learning problem.","Then, the key quantity in PAC-Bayes concentration bounds is a divergence that captures the complexity of the learning problem where the de facto standard choice is the KL divergence.","However, the tightness of this choice has rarely been questioned.   ","In this paper, we challenge the tightness of the KL-divergence-based bounds by showing that it is possible to achieve a strictly tighter bound.","In particular, we demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022).","Our proof is inspired by recent advances in regret analysis of gambling algorithms, and its use to derive concentration inequalities.","Our result is first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are not known to be strictly better than KL.","Thus, we believe our work marks the first step towards identifying optimal rates of PAC-Bayes bounds."],"url":"http://arxiv.org/abs/2402.09201v1","category":"cs.LG"}
{"created":"2024-02-14 14:08:06","title":"Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process","abstract":"Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure. It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data. To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states. Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures. Moreover, we replace the conventional $\\mathrm{SE}(3)$ state representation with the combination of $\\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement. Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the community, we will make our source code publicly available.","sentences":["Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure.","It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data.","To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of Gaussian Process (GP) that predicts a non-parametric continuous-time trajectory to capture sensors' spatial-temporal movement with limited control states.","Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures.","Moreover, we replace the conventional $\\mathrm{SE}(3)$ state representation with the combination of $\\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement.","Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator.","To contribute to the community, we will make our source code publicly available."],"url":"http://arxiv.org/abs/2402.09189v1","category":"cs.RO"}
{"created":"2024-02-14 13:17:56","title":"On the Thomas-Fermi model: Gabor J. Kalman's contribution and numerical approximations","abstract":"In this article, we would like to pay tribute to Gabor Kalman, outlining his contribution to a model widely used in dense plasma physics: the high-temperature Thomas-Fermi model. The approach of Ruoxian Ying and Kalman relies on the separation of the bound and free electrons, a physically reasonable definition of the bound electrons, a description of the source density in the Poisson equation through the electron-ion and ion-ion pair correlation functions and a determination of the degree of ionization from the minimization of the total free energy. We also report on different approximations of the function $\\Phi$, which is a cornerstone of the original Thomas-Femi model.","sentences":["In this article, we would like to pay tribute to Gabor Kalman, outlining his contribution to a model widely used in dense plasma physics: the high-temperature Thomas-Fermi model.","The approach of Ruoxian Ying and Kalman relies on the separation of the bound and free electrons, a physically reasonable definition of the bound electrons, a description of the source density in the Poisson equation through the electron-ion and ion-ion pair correlation functions and a determination of the degree of ionization from the minimization of the total free energy.","We also report on different approximations of the function $\\Phi$, which is a cornerstone of the original Thomas-Femi model."],"url":"http://arxiv.org/abs/2402.09157v1","category":"physics.plasm-ph"}
{"created":"2024-02-14 13:13:26","title":"Attacking Large Language Models with Projected Gradient Descent","abstract":"Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.","sentences":["Current LLM alignment methods are readily broken through specifically crafted adversarial prompts.","While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls.","This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training.","To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt.","Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy.","Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results."],"url":"http://arxiv.org/abs/2402.09154v1","category":"cs.LG"}
{"created":"2024-02-14 12:57:59","title":"Static, Cylindrically Symmetric Spacetimes in the Coincident f(Q) Gravity","abstract":"In this paper we consider a static, cylindrically symmetric spacetime with coincident f(Q) gravity. Since the field equation of this spacetime in symmetric teleparallel gravity is suitable for choosing the function of f(Q) in the form of power series and exponential forms, perfect fluid solutions of these forms are discussed. Energy densities, directional pressures and energy conditions are plotted and analysed for a few different metric potentials. Although cosmic strings violate all energy conditions in both f(Q) functions, the Levi-Civita solution violates all energy conditions in the power law function of f(Q), but for exponential f(Q) gravity they are satisfied in small regions.","sentences":["In this paper we consider a static, cylindrically symmetric spacetime with coincident f(Q) gravity.","Since the field equation of this spacetime in symmetric teleparallel gravity is suitable for choosing the function of f(Q) in the form of power series and exponential forms, perfect fluid solutions of these forms are discussed.","Energy densities, directional pressures and energy conditions are plotted and analysed for a few different metric potentials.","Although cosmic strings violate all energy conditions in both f(Q) functions, the Levi-Civita solution violates all energy conditions in the power law function of f(Q), but for exponential f(Q) gravity they are satisfied in small regions."],"url":"http://arxiv.org/abs/2402.09149v1","category":"gr-qc"}
{"created":"2024-02-14 12:48:27","title":"On groups and fields interpretable in differentially closed valued fields and in various $\\mathrm{NTP_2}$ fields","abstract":"This paper aims at developing model-theoretic tools to study interpretable fields and definably amenable groups, mainly in $\\mathrm{NIP}$ or $\\mathrm{NTP_2}$ settings. An abstract theorem constructing definable group homomorphisms from generic data is proved. It relies heavily on a stabilizer theorem of Montenegro, Onshuus and Simon. The main application is a structure theorem for definably amenable groups that are interpretable in algebraically bounded perfect $\\mathrm{NTP_2}$ fields with bounded Galois group (under some mild assumption on the imaginaries involved), or in algebraically bounded theories of (differential) NIP fields. These imply a classification of the fields interpretable in differentially closed valued fields, and structure theorems for fields interpretable in finitely ramified henselian valued fields of characteristic $0$, or in NIP algebraically bounded differential fields.","sentences":["This paper aims at developing model-theoretic tools to study interpretable fields and definably amenable groups, mainly in $\\mathrm{NIP}$ or $\\mathrm{NTP_2}$ settings.","An abstract theorem constructing definable group homomorphisms from generic data is proved.","It relies heavily on a stabilizer theorem of Montenegro, Onshuus and Simon.","The main application is a structure theorem for definably amenable groups that are interpretable in algebraically bounded perfect $\\mathrm{NTP_2}$ fields with bounded Galois group (under some mild assumption on the imaginaries involved), or in algebraically bounded theories of (differential) NIP fields.","These imply a classification of the fields interpretable in differentially closed valued fields, and structure theorems for fields interpretable in finitely ramified henselian valued fields of characteristic $0$, or in NIP algebraically bounded differential fields."],"url":"http://arxiv.org/abs/2402.09143v1","category":"math.LO"}
{"created":"2024-02-14 12:48:17","title":"When Representations Align: Universality in Representation Learning Dynamics","abstract":"Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the \"rich\" and \"lazy\" regime. While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible.","sentences":["Deep neural networks come in many sizes and architectures.","The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations.","Yet, recent results have shown that different architectures learn representations with striking qualitative similarities.","Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions.","This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization.","We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the \"rich\" and \"lazy\" regime.","While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible."],"url":"http://arxiv.org/abs/2402.09142v1","category":"cs.LG"}
{"created":"2024-02-14 12:38:22","title":"Unifying Graded Linear Logic and Differential Operators","abstract":"Linear Logic refines Intuitionnistic Logic by taking into account the resources used during the proof and program computation. In the past decades, it has been extended to various frameworks. The most famous are indexed linear logics which can describe the resource management or the complexity analysis of a program. From an other perspective, Differential Linear Logic is an extension which allows the linearization of proofs. In this article, we merge these two directions by first defining a differential version of Graded linear logic: this is made by indexing exponential connectives with a monoid of differential operators. We prove that it is equivalent to a graded version of previously defined extension of finitary differential linear logic. We give a denotational model of our logic, based on distribution theory and linear partial differential operators with constant coefficients.","sentences":["Linear Logic refines Intuitionnistic Logic by taking into account the resources used during the proof and program computation.","In the past decades, it has been extended to various frameworks.","The most famous are indexed linear logics which can describe the resource management or the complexity analysis of a program.","From an other perspective, Differential Linear Logic is an extension which allows the linearization of proofs.","In this article, we merge these two directions by first defining a differential version of Graded linear logic: this is made by indexing exponential connectives with a monoid of differential operators.","We prove that it is equivalent to a graded version of previously defined extension of finitary differential linear logic.","We give a denotational model of our logic, based on distribution theory and linear partial differential operators with constant coefficients."],"url":"http://arxiv.org/abs/2402.09138v1","category":"cs.LO"}
{"created":"2024-02-14 12:27:54","title":"Optimal Automated Market Makers: Differentiable Economics and Strong Duality","abstract":"The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices. An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals. The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation. More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022). Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood. In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on the transport plan in the dual. We show that optimal mechanisms for multiple goods and under adverse selection can take advantage of bundling, both improved prices for bundled purchases and sales as well as sometimes accepting payment \"in kind.\" We present conjectures of optimal mechanisms in additional settings which show further complex behavior. From a methodological perspective, we make essential use of the tools of differentiable economics to generate conjectures of optimal mechanisms, and give a proof-of-concept for the use of such tools in guiding theoretical investigations.","sentences":["The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices.","An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals.","The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation.","More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022).","Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood.","In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on the transport plan in the dual.","We show that optimal mechanisms for multiple goods and under adverse selection can take advantage of bundling, both improved prices for bundled purchases and sales as well as sometimes accepting payment \"in kind.\"","We present conjectures of optimal mechanisms in additional settings which show further complex behavior.","From a methodological perspective, we make essential use of the tools of differentiable economics to generate conjectures of optimal mechanisms, and give a proof-of-concept for the use of such tools in guiding theoretical investigations."],"url":"http://arxiv.org/abs/2402.09129v1","category":"cs.GT"}
{"created":"2024-02-14 12:26:10","title":"1D stochastic pressure equation with log-correlated Gaussian coefficients","abstract":"We study unique solvability for one dimensional stochastic pressure equation with diffusion coefficient given by the Wick exponential of log-correlated Gaussian fields. We prove well-posedness for Dirichlet, Neumann and periodic boundary data, and the initial value problem, covering the cases of both the Wick renormalization of the diffusion and of point-wise multiplication. We provide explicit representations for the solutions in both cases, characterized by the $S$-transform and the Gaussian multiplicative chaos measure.","sentences":["We study unique solvability for one dimensional stochastic pressure equation with diffusion coefficient given by the Wick exponential of log-correlated Gaussian fields.","We prove well-posedness for Dirichlet, Neumann and periodic boundary data, and the initial value problem, covering the cases of both the Wick renormalization of the diffusion and of point-wise multiplication.","We provide explicit representations for the solutions in both cases, characterized by the $S$-transform and the Gaussian multiplicative chaos measure."],"url":"http://arxiv.org/abs/2402.09127v1","category":"math.PR"}
{"created":"2024-02-14 12:15:05","title":"Classical and generalized solutions of an alarm-taxis model","abstract":"In bounded, spatially two-dimensional domains, the system \\begin{equation*}   \\left\\lbrace\\begin{alignedat}{3}   u_t &= d_1 \\Delta u && &&+ u(\\lambda_1 - \\mu_1 u - a_1 v - a_2 w), \\\\   v_t &= d_2 \\Delta v &&- \\xi \\nabla \\cdot (v \\nabla u) &&+ v(\\lambda_2 - \\mu_2 v + b_1 u - a_3 w),\\\\   w_t &= d_3 \\Delta w &&- \\chi \\nabla \\cdot (w \\nabla (uv)) &&+ w(\\lambda_3 - \\mu_3 w + b_2 u + b_3 v),   \\end{alignedat}\\right. \\end{equation*} complemented with initial and homogeneous Neumann boundary conditions, models the interaction between prey (with density $u$), predator (with density $v$) and superpredator (with density $w$), which preys on both other populations. Apart from random motion and prey-tactical behavior of the primary predator, the key aspect of this system is that the secondary predator reacts to alarm calls of the prey, issued by the latter whenever attacked by the primary predator.   We first show in the pure alarm-taxis model, i.e. if $\\xi = 0$, that global classical solutions exist.   For the full model (with $\\xi > 0$), the taxis terms and the presence of the term $-a_2 uw$ in the first equation apparently hinder certain bootstrap procedures, meaning that the available regularity information is rather limited. Nonetheless, we are able to obtain global generalized solutions. An important technical challenge is to guarantee strong convergence of (weighted) gradients of the first two solution components in order to conclude that approximate solutions converge to a generalized solution of the limit problem.","sentences":["In bounded, spatially two-dimensional domains, the system \\begin{equation*}   \\left\\lbrace\\begin{alignedat}{3}   u_t &= d_1 \\Delta u && &&+ u(\\lambda_1 - \\mu_1 u - a_1 v - a_2 w), \\\\   v_t &= d_2 \\Delta v &&- \\xi \\nabla \\cdot (v \\nabla u) &&+ v(\\lambda_2 - \\mu_2 v + b_1 u - a_3","w),\\\\   w_t &= d_3 \\Delta w &&- \\chi \\nabla \\cdot (w \\nabla (uv)) &&+ w(\\lambda_3 - \\mu_3 w + b_2 u + b_3 v),   \\end{alignedat}\\right.","\\end{equation*} complemented with initial and homogeneous Neumann boundary conditions, models the interaction between prey (with density $u$), predator (with density $v$) and superpredator (with density $w$), which preys on both other populations.","Apart from random motion and prey-tactical behavior of the primary predator, the key aspect of this system is that the secondary predator reacts to alarm calls of the prey, issued by the latter whenever attacked by the primary predator.   ","We first show in the pure alarm-taxis model, i.e. if $\\xi = 0$, that global classical solutions exist.   ","For the full model (with $\\xi > 0$), the taxis terms and the presence of the term $-a_2 uw$ in the first equation apparently hinder certain bootstrap procedures, meaning that the available regularity information is rather limited.","Nonetheless, we are able to obtain global generalized solutions.","An important technical challenge is to guarantee strong convergence of (weighted) gradients of the first two solution components in order to conclude that approximate solutions converge to a generalized solution of the limit problem."],"url":"http://arxiv.org/abs/2402.09119v1","category":"math.AP"}
{"created":"2024-02-14 11:56:03","title":"Non-Volatile Analog Control and Reconfiguration of a Vortex Nano-Oscillator Frequency","abstract":"Magnetic tunnel junctions are nanoscale devices which have recently attracted interested in the context of frequency multiplexed spintronic neural networks, due to their interesting dynamical properties, which are defined during the fabrication process, and depend on the material parameters and geometry. This paper proposes an approach to extending the functionality of a standard magnetic tunnel junction (MTJ) by introducing an additional ferromagnet/antiferromagnet (FM/AFM) storage layer (SL) vertically integrated with the standard Vortex MTJ stack into the nanopillar. The magnetostatic field created by this storage layer acts on the free layer and can be used to change its static and dynamic properties. To tune the magnitude and direction of this magnetostatic field, magnetic reconfiguration is carried out through a thermally assisted switching mechanism using a voltage pulse that heats the AFM layer in the SL above the Neel temperature in the presence of an external field. It is experimentally shown that using an MTJ based on a 600 nm diameter nanopillar with a vortex in the free layer, reconfiguration of the SL allows to continuously change the core precession frequency in the 15 MHz range. The reconfigurable analogue storage layer locally affects both the static and dynamic properties of the MTJ free layer, demonstrating vertical 3D integration of additional functionalities into a single MTJ nanopillar.","sentences":["Magnetic tunnel junctions are nanoscale devices which have recently attracted interested in the context of frequency multiplexed spintronic neural networks, due to their interesting dynamical properties, which are defined during the fabrication process, and depend on the material parameters and geometry.","This paper proposes an approach to extending the functionality of a standard magnetic tunnel junction (MTJ) by introducing an additional ferromagnet/antiferromagnet (FM/AFM) storage layer (SL) vertically integrated with the standard Vortex MTJ stack into the nanopillar.","The magnetostatic field created by this storage layer acts on the free layer and can be used to change its static and dynamic properties.","To tune the magnitude and direction of this magnetostatic field, magnetic reconfiguration is carried out through a thermally assisted switching mechanism using a voltage pulse that heats the AFM layer in the SL above the Neel temperature in the presence of an external field.","It is experimentally shown that using an MTJ based on a 600 nm diameter nanopillar with a vortex in the free layer, reconfiguration of the SL allows to continuously change the core precession frequency in the 15 MHz range.","The reconfigurable analogue storage layer locally affects both the static and dynamic properties of the MTJ free layer, demonstrating vertical 3D integration of additional functionalities into a single MTJ nanopillar."],"url":"http://arxiv.org/abs/2402.09114v1","category":"physics.app-ph"}
{"created":"2024-02-14 11:47:19","title":"Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks","abstract":"Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$). We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\\times$ lower latency as compared to a GPU implementation, while consuming $15\\times$ less power.","sentences":["Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency.","Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient.","In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers.","We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$).","We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design.","We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\\times$ lower latency as compared to a GPU implementation, while consuming $15\\times$ less power."],"url":"http://arxiv.org/abs/2402.09109v1","category":"cs.AR"}
{"created":"2024-02-14 11:13:33","title":"Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks","abstract":"Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide the most comprehensive overview and systematization of previously published activation functions with links to their original sources. The secondary aim is to update the current understanding of this family of functions.","sentences":["Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life.","Recently, their importance and practical usability have further been reinforced with the advent of deep learning.","One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model.","Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview.","The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions.","To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys.","Our comprehensive compilation also references these surveys; however, its main goal is to provide the most comprehensive overview and systematization of previously published activation functions with links to their original sources.","The secondary aim is to update the current understanding of this family of functions."],"url":"http://arxiv.org/abs/2402.09092v1","category":"cs.LG"}
{"created":"2024-02-14 11:11:33","title":"Vapor equilibrium models of accreting rocky planets demonstrate direct core growth by pebble accretion","abstract":"The gaseous envelope of an accreting rocky planet becomes hot enough to sublimate silicates and other refractory minerals.   % aims heading (mandatory) For this work, we studied the effect of the resulting envelope enrichment with a heavy vapor species on the composition and temperature of the envelope. For simplification, we used the gas-phase molecule SiO to represent the sublimation of silicate material. We solved the equilibrium structure equations in 1D for planets in the mass range of $0.1$ to $3\\,M_\\oplus$. The convective stability criterion was extended to take the stabilizing effect of the condensation of SiO clouds into account. We assumed that the envelope is both in hydrostatic equilibrium and in vapor equilibrium with the underlying magma ocean. This means that pebbles do not undergo sublimation in the envelope and therefore survive until they plunge into the magma ocean. We find that the emergence of an inner radiative region, where SiO condensation suppresses convection, increases the pressure and temperature in the inner envelope compared to pure H$_2$/He envelopes once $M_\\mathrm{pl} \\gtrsim 0.3\\,M_\\oplus$. For $M_\\mathrm{pl}>0.75\\,M_\\oplus$, the temperature and pressure close to the surface reach the supercritical point of SiO. The amount of SiO stored in the envelope is lower than the total planet mass for low mass planets. However, for $M_\\mathrm{pl}>2.0\\,M_\\oplus$, all accreted pebble material must contribute to maintain the vapor equilibrium in the envelope. Therefore, the non-vapor mass of the planet ceases to increase beyond this threshold. Overall, our vapor equilibrium model of the planetary envelope allows for direct core growth by pebble accretion up to much higher masses than previously thought.","sentences":["The gaseous envelope of an accreting rocky planet becomes hot enough to sublimate silicates and other refractory minerals.   ","% aims heading (mandatory) For this work, we studied the effect of the resulting envelope enrichment with a heavy vapor species on the composition and temperature of the envelope.","For simplification, we used the gas-phase molecule SiO to represent the sublimation of silicate material.","We solved the equilibrium structure equations in 1D for planets in the mass range of $0.1$ to $3\\,M_\\oplus$. The convective stability criterion was extended to take the stabilizing effect of the condensation of SiO clouds into account.","We assumed that the envelope is both in hydrostatic equilibrium and in vapor equilibrium with the underlying magma ocean.","This means that pebbles do not undergo sublimation in the envelope and therefore survive until they plunge into the magma ocean.","We find that the emergence of an inner radiative region, where SiO condensation suppresses convection, increases the pressure and temperature in the inner envelope compared to pure H$_2$/He envelopes once $M_\\mathrm{pl} \\gtrsim 0.3\\,M_\\oplus$. For $M_\\mathrm{pl}>0.75\\,M_\\oplus$, the temperature and pressure close to the surface reach the supercritical point of SiO. The amount of SiO stored in the envelope is lower than the total planet mass for low mass planets.","However, for $M_\\mathrm{pl}>2.0\\,M_\\oplus$, all accreted pebble material must contribute to maintain the vapor equilibrium in the envelope.","Therefore, the non-vapor mass of the planet ceases to increase beyond this threshold.","Overall, our vapor equilibrium model of the planetary envelope allows for direct core growth by pebble accretion up to much higher masses than previously thought."],"url":"http://arxiv.org/abs/2402.09089v1","category":"astro-ph.EP"}
{"created":"2024-02-14 11:10:21","title":"Differential Sensitivity of the KM3NeT/ARCA detector to a diffuse neutrino flux and to point-like source emission: exploring the case of the Starburst Galaxies","abstract":"KM3NeT/ARCA is a Cherenkov neutrino telescope under construction in the Mediterranean sea, optimised for the detection of astrophysical neutrinos with energies above $\\sim$1~TeV. In this work, using Monte Carlo simulations including all-flavour neutrinos, the integrated and differential sensitivities for KM3NeT/ARCA are presented considering the case of a diffuse neutrino flux as well as extended and point-like neutrino sources. This analysis is applied to Starburst Galaxies demonstrating that the detector has the capability of tracing TeV neutrinos from these sources. Remarkably, after eight years, a hard power-law spectrum from the nearby Small Magellanic Cloud can be constrained. The sensitivity and discovery potential for NGC 1068 is also evaluated showing that KM3NeT/ARCA will discriminate between different astrophysical components of the measured neutrino flux after 3 years of data taking.","sentences":["KM3NeT/ARCA is a Cherenkov neutrino telescope under construction in the Mediterranean sea, optimised for the detection of astrophysical neutrinos with energies above $\\sim$1~TeV.","In this work, using Monte Carlo simulations including all-flavour neutrinos, the integrated and differential sensitivities for KM3NeT/ARCA are presented considering the case of a diffuse neutrino flux as well as extended and point-like neutrino sources.","This analysis is applied to Starburst Galaxies demonstrating that the detector has the capability of tracing TeV neutrinos from these sources.","Remarkably, after eight years, a hard power-law spectrum from the nearby Small Magellanic Cloud can be constrained.","The sensitivity and discovery potential for NGC 1068 is also evaluated showing that KM3NeT/ARCA will discriminate between different astrophysical components of the measured neutrino flux after 3 years of data taking."],"url":"http://arxiv.org/abs/2402.09088v1","category":"astro-ph.HE"}
{"created":"2024-02-14 17:13:35","title":"Extended mean-field games with multi-dimensional singular controls and non-linear jump impact","abstract":"We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs. Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones. We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function. The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations. Parametrisations are laws of state/control processes that continuously interpolate jumps. We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide. This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact.","sentences":["We establish a probabilistic framework for analysing extended mean-field games with multi-dimensional singular controls and state-dependent jump dynamics and costs.","Two key challenges arise when analysing such games: the state dynamics may not depend continuously on the control and the reward function may not be u.s.c.~Both problems can be overcome by restricting the set of admissible singular controls to controls that can be approximated by continuous ones.","We prove that the corresponding set of admissible weak controls is given by the weak solutions to a Marcus-type SDE and provide an explicit characterisation of the reward function.","The reward function will in general only be u.s.c.~To address the lack of continuity we introduce a novel class of MFGs with a broader set of admissible controls, called MFGs of parametrisations.","Parametrisations are laws of state/control processes that continuously interpolate jumps.","We prove that the reward functional is continuous on the set of parametrisations, establish the existence of equilibria in MFGs of parametrisations, and show that the set of Nash equilibria in MFGs of parametrisations and in the underlying MFG with singular controls coincide.","This shows that MFGs of parametrisations provide a canonical framework for analysing MFGs with singular controls and non-linear jump impact."],"url":"http://arxiv.org/abs/2402.09317v1","category":"math.OC"}
{"created":"2024-02-14 16:44:08","title":"Manipulating a beam of barium fluoride molecules using an electrostatic hexapole","abstract":"An electrostatic hexapole lens is used to manipulate the transverse properties of a beam of barium fluoride molecules from a cryogenic buffer gas source. The spatial distribution of the beam is measured by recording state-selective laser-induced fluorescence on an emccd camera, providing insight into the intensity and transverse position spread of the molecular beam. Although the high mass and unfavorable Stark shift of barium fluoride pose a considerable challenge, the number of molecules in the low-field seeking component of the N=1 state that pass a 4 mm diameter aperture 712 mm behind the source is increased by a factor of 12. Furthermore, it is demonstrated that the molecular beam can be displaced by up to +/-5 mm by moving the hexapole lens. Our measurements agree well with numerical trajectory simulations. We discuss how electrostatic lenses may be used to increase the sensitivity of beam experiments such as the search for the electric dipole moment of the electron.","sentences":["An electrostatic hexapole lens is used to manipulate the transverse properties of a beam of barium fluoride molecules from a cryogenic buffer gas source.","The spatial distribution of the beam is measured by recording state-selective laser-induced fluorescence on an emccd camera, providing insight into the intensity and transverse position spread of the molecular beam.","Although the high mass and unfavorable Stark shift of barium fluoride pose a considerable challenge, the number of molecules in the low-field seeking component of the N=1 state that pass a 4 mm diameter aperture 712 mm behind the source is increased by a factor of 12.","Furthermore, it is demonstrated that the molecular beam can be displaced by up to +/-5 mm by moving the hexapole lens.","Our measurements agree well with numerical trajectory simulations.","We discuss how electrostatic lenses may be used to increase the sensitivity of beam experiments such as the search for the electric dipole moment of the electron."],"url":"http://arxiv.org/abs/2402.09300v1","category":"physics.atom-ph"}
{"created":"2024-02-14 16:37:09","title":"Reconstructing a state-independent cost function in a mean-field game model","abstract":"In this short note, we consider an inverse problem to a mean-field games system where we are interested in reconstructing the state-independent running cost function from observed value-function data. We provide an elementary proof of a uniqueness result for the inverse problem using the standard multilinearization technique. One of the main features of our work is that we insist that the population distribution be a probability measure, a requirement that is not enforced in some of the existing literature on theoretical inverse mean-field games.","sentences":["In this short note, we consider an inverse problem to a mean-field games system where we are interested in reconstructing the state-independent running cost function from observed value-function data.","We provide an elementary proof of a uniqueness result for the inverse problem using the standard multilinearization technique.","One of the main features of our work is that we insist that the population distribution be a probability measure, a requirement that is not enforced in some of the existing literature on theoretical inverse mean-field games."],"url":"http://arxiv.org/abs/2402.09297v1","category":"math.AP"}
{"created":"2024-02-14 16:30:30","title":"The impact of load placement on grid resonances during grid restoration","abstract":"As inverter-based generation is being massively deployed in the grid, these type of units have to take over the current roles of conventional generation, including the capability of restoring the grid. In this context, the resonances of the grid during the first steps of a black start can be concerning, given that the grid is lightly loaded. Especially relevant are the low frequency resonances, that may be excited by the harmonic components of the inverter. A typical strategy to avoid or minimize the effect of such resonances relies on connecting load banks. This was fairly feasible with conventional generation, but given the limited ratings of inverters, the amount of load that can be connected at the beginning is very limited. In this paper we consider the energization of a transmission line, and investigate the optimal location of a load along a line in order to maximize the damping in the system. By analysing the spectral properties as a function of the load location, we formally prove that placing the load in the middle of the transmission line maximizes the damping ratio of the first resonance of the system.","sentences":["As inverter-based generation is being massively deployed in the grid, these type of units have to take over the current roles of conventional generation, including the capability of restoring the grid.","In this context, the resonances of the grid during the first steps of a black start can be concerning, given that the grid is lightly loaded.","Especially relevant are the low frequency resonances, that may be excited by the harmonic components of the inverter.","A typical strategy to avoid or minimize the effect of such resonances relies on connecting load banks.","This was fairly feasible with conventional generation, but given the limited ratings of inverters, the amount of load that can be connected at the beginning is very limited.","In this paper we consider the energization of a transmission line, and investigate the optimal location of a load along a line in order to maximize the damping in the system.","By analysing the spectral properties as a function of the load location, we formally prove that placing the load in the middle of the transmission line maximizes the damping ratio of the first resonance of the system."],"url":"http://arxiv.org/abs/2402.09294v1","category":"eess.SY"}
{"created":"2024-02-14 16:19:04","title":"GraphiQ: Quantum circuit design for photonic graph states","abstract":"GraphiQ is a versatile open-source framework for designing photonic graph state generation schemes, with a particular emphasis on photon-emitter hybrid circuits. Built in Python, GraphiQ consists of a suite of design tools, including multiple simulation backends and optimization methods. The library supports scheme optimization in the presence of circuit imperfections, as well as user-defined optimization goals. Our framework thus represents a valuable tool for the development of practical schemes adhering to experimentally-relevant constraints. As graph states are a key resource for measurement-based quantum computing, all-photonic quantum repeaters, and robust quantum metrology, among others, we envision GraphiQ's broad impact for advancing quantum technologies.","sentences":["GraphiQ is a versatile open-source framework for designing photonic graph state generation schemes, with a particular emphasis on photon-emitter hybrid circuits.","Built in Python, GraphiQ consists of a suite of design tools, including multiple simulation backends and optimization methods.","The library supports scheme optimization in the presence of circuit imperfections, as well as user-defined optimization goals.","Our framework thus represents a valuable tool for the development of practical schemes adhering to experimentally-relevant constraints.","As graph states are a key resource for measurement-based quantum computing, all-photonic quantum repeaters, and robust quantum metrology, among others, we envision GraphiQ's broad impact for advancing quantum technologies."],"url":"http://arxiv.org/abs/2402.09285v1","category":"quant-ph"}
{"created":"2024-02-14 16:10:45","title":"Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies","abstract":"The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection. Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications.","sentences":["The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations.","This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks.","Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data.","The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings.","The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection.","Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications."],"url":"http://arxiv.org/abs/2402.09282v1","category":"cs.CL"}
{"created":"2024-02-14 15:56:42","title":"Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement","abstract":"Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events. In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned convolutional sparse coding to optimize the objective function. Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet. The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes. Extensive experimental results verify the effectiveness and robustness of our MSDNet. Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks.","sentences":["Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs.","In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time.","Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability.","In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events.","In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned convolutional sparse coding to optimize the objective function.","Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet.","The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes.","Extensive experimental results verify the effectiveness and robustness of our MSDNet.","Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks."],"url":"http://arxiv.org/abs/2402.09270v1","category":"cs.CV"}
{"created":"2024-02-14 15:51:28","title":"UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers","abstract":"Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different event models; (iii) optimize the deployment of the model and MCU library for system efficiency. We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets. Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance.   UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications.","sentences":["Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases.","This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare.","Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output.","However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs).","This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   ","In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs.","Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different event models; (iii) optimize the deployment of the model and MCU library for system efficiency.","We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets.","Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance.   ","UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications."],"url":"http://arxiv.org/abs/2402.09264v1","category":"cs.LG"}
{"created":"2024-02-14 15:39:56","title":"Max-Min Fair Energy-Efficient Beam Design for Quantized ISAC LEO Satellite Systems: A Rate-Splitting Approach","abstract":"Low earth orbit (LEO) satellite systems with sensing functionality is envisioned to facilitate global-coverage service and emerging applications in 6G. Currently, two fundamental challenges, namely, inter-beam interference among users and power limitation at the LEO satellites, limit the full potential of the joint design of sensing and communication. To effectively control the interference, rate-splitting multiple access (RSMA) scheme is employed as the interference management strategy in the system design. On the other hand, to address the limited power supply at the LEO satellites, we consider low-resolution quantization digital-to-analog converters (DACs) at the transmitter to reduce power consumption, which grows exponentially with the number of quantization bits. Additionally, optimizing the total energy efficiency (EE) of the system is a common practice to save the power. However, this metric lacks fairness among users. To ensure this fairness and further enhance EE, we investigate the max-min fairness EE of the RSMA-assisted integrated sensing and communications (ISAC)-LEO satellite system. In this system, the satellite transmits a quantized dual-functional signal serving downlink users while detecting a target. Specifically, we optimize the precoders for maximizing the minimal EE among all users, considering the power consumption of each radio frequency (RF) chain under communication and sensing constraints. To tackle this optimization problem, we proposed an iterative algorithm based on successive convex approximation (SCA) and Dinkelbach's method. Numerical results illustrate that the proposed design outperforms the strategies that aim to maximize the total EE of the system and conventional space-division multiple access (SDMA) in terms of max-min fairness EE and the communication-sensing trade-off.","sentences":["Low earth orbit (LEO) satellite systems with sensing functionality is envisioned to facilitate global-coverage service and emerging applications in 6G. Currently, two fundamental challenges, namely, inter-beam interference among users and power limitation at the LEO satellites, limit the full potential of the joint design of sensing and communication.","To effectively control the interference, rate-splitting multiple access (RSMA) scheme is employed as the interference management strategy in the system design.","On the other hand, to address the limited power supply at the LEO satellites, we consider low-resolution quantization digital-to-analog converters (DACs) at the transmitter to reduce power consumption, which grows exponentially with the number of quantization bits.","Additionally, optimizing the total energy efficiency (EE) of the system is a common practice to save the power.","However, this metric lacks fairness among users.","To ensure this fairness and further enhance EE, we investigate the max-min fairness EE of the RSMA-assisted integrated sensing and communications (ISAC)-LEO satellite system.","In this system, the satellite transmits a quantized dual-functional signal serving downlink users while detecting a target.","Specifically, we optimize the precoders for maximizing the minimal EE among all users, considering the power consumption of each radio frequency (RF) chain under communication and sensing constraints.","To tackle this optimization problem, we proposed an iterative algorithm based on successive convex approximation (SCA) and Dinkelbach's method.","Numerical results illustrate that the proposed design outperforms the strategies that aim to maximize the total EE of the system and conventional space-division multiple access (SDMA) in terms of max-min fairness EE and the communication-sensing trade-off."],"url":"http://arxiv.org/abs/2402.09253v1","category":"eess.SP"}
{"created":"2024-02-14 14:18:14","title":"The Boosted Difference of Convex Functions Algorithm for Value-at-Risk Constrained Portfolio Optimization","abstract":"A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios. Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market and operational risks. For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex (DC) functions representation of the VaR can be derived. Wozabal (2012) showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm (DCA). A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm (BDCA) which accelerates the convergence due to an additional line search step. It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints. In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-{\\L}ojasiewicz property. An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data. We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA. Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR constrained portfolio selection problems in Python.","sentences":["A highly relevant problem of modern finance is the design of Value-at-Risk (VaR) optimal portfolios.","Due to contemporary financial regulations, banks and other financial institutions are tied to use the risk measure to control their credit, market and operational risks.","For a portfolio with a discrete return distribution and finitely many scenarios, a Difference of Convex (DC) functions representation of the VaR can be derived.","Wozabal (2012) showed that this yields a solution to a VaR constrained Markowitz style portfolio selection problem using the Difference of Convex Functions Algorithm (DCA).","A recent algorithmic extension is the so-called Boosted Difference of Convex Functions Algorithm (BDCA) which accelerates the convergence due to an additional line search step.","It has been shown that the BDCA converges linearly for solving non-smooth quadratic problems with linear inequality constraints.","In this paper, we prove that the linear rate of convergence is also guaranteed for a piecewise linear objective function with linear equality and inequality constraints using the Kurdyka-{\\L}ojasiewicz property.","An extended case study under consideration of best practices for comparing optimization algorithms demonstrates the superiority of the BDCA over the DCA for real-world financial market data.","We are able to show that the results of the BDCA are significantly closer to the efficient frontier compared to the DCA.","Due to the open availability of all data sets and code, this paper further provides a practical guide for transparent and easily reproducible comparisons of VaR constrained portfolio selection problems in Python."],"url":"http://arxiv.org/abs/2402.09194v1","category":"math.OC"}
{"created":"2024-02-14 13:51:56","title":"Approximating maximum independent set on Rydberg atom arrays using local detunings","abstract":"Rydberg atom arrays are among the most promising quantum simulating platforms due to their scalability and long coherence time. From the perspective of combinatorial optimization, they are intrinsic solver for the maximum independent set problem because of the resemblance between the Rydberg Hamiltonian and the cost function of the maximum independent set problem. In this paper, we suggest a strategy to approximate maximum independent sets by adjusting local detunings on the Rydberg Hamiltonian according to each vertex's vertex support, which is a quantity that represents connectivity between vertices. By doing so, we explicitly reflect on the Rydberg Hamiltonian the potential probability that each vertex will be included in maximum independent sets. Our strategy reduces an error rate three times for the checkerboard graphs with defects when the adiabaticity is enough. Our strategy also decreases the error rate for random graphs of density 3.0, even when the adiabaticity is relatively insufficient. Moreover, we harness our strategy to raise the fidelity between the evolved quantum state and a 2D cat state on a square lattice, showing that our strategy helps to prepare a quantum many-body ground state.","sentences":["Rydberg atom arrays are among the most promising quantum simulating platforms due to their scalability and long coherence time.","From the perspective of combinatorial optimization, they are intrinsic solver for the maximum independent set problem because of the resemblance between the Rydberg Hamiltonian and the cost function of the maximum independent set problem.","In this paper, we suggest a strategy to approximate maximum independent sets by adjusting local detunings on the Rydberg Hamiltonian according to each vertex's vertex support, which is a quantity that represents connectivity between vertices.","By doing so, we explicitly reflect on the Rydberg Hamiltonian the potential probability that each vertex will be included in maximum independent sets.","Our strategy reduces an error rate three times for the checkerboard graphs with defects when the adiabaticity is enough.","Our strategy also decreases the error rate for random graphs of density 3.0, even when the adiabaticity is relatively insufficient.","Moreover, we harness our strategy to raise the fidelity between the evolved quantum state and a 2D cat state on a square lattice, showing that our strategy helps to prepare a quantum many-body ground state."],"url":"http://arxiv.org/abs/2402.09180v1","category":"quant-ph"}
{"created":"2024-02-14 13:44:16","title":"Nearly Optimal Regret for Decentralized Online Convex Optimization","abstract":"We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$. The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners. Furthermore, by carefully exploiting the spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\\Omega(n\\rho^{-1/4}\\sqrt{T})$ and $\\Omega(n\\rho^{-1/2})$, respectively. These lower bounds suggest that our algorithms are nearly optimal in terms of $T$, $n$, and $\\rho$.","sentences":["We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications.","Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon.","However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions.","To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$.","The primary technique is to design an online accelerated gossip strategy that enjoys a faster average consensus among local learners.","Furthermore, by carefully exploiting the spectral properties of a specific network topology, we enhance the lower bounds for convex and strongly convex functions to $\\Omega(n\\rho^{-1/4}\\sqrt{T})$ and $\\Omega(n\\rho^{-1/2})$, respectively.","These lower bounds suggest that our algorithms are nearly optimal in terms of $T$, $n$, and $\\rho$."],"url":"http://arxiv.org/abs/2402.09173v1","category":"cs.LG"}
{"created":"2024-02-14 13:14:00","title":"Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems","abstract":"Integrated sensing and communication (ISAC) is widely recognized as a fundamental enabler for future wireless communications. In this paper, we present a joint communication and radar beamforming framework for maximizing a sum spectral efficiency (SE) while guaranteeing desired radar performance with imperfect channel state information (CSI) in multi-user and multi-target ISAC systems. To this end, we adopt either a radar transmit beam mean square error (MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar performance constraint of a sum SE maximization problem. To resolve inherent challenges such as non-convexity and imperfect CSI, we reformulate the problems and identify first-order optimality conditions for the joint radar and communication beamformer. Turning the condition to a nonlinear eigenvalue problem with eigenvector dependency (NEPv), we develop an alternating method which finds the joint beamformer through power iteration and a Lagrangian multiplier through binary search. The proposed framework encompasses both the radar metrics and is robust to channel estimation error with low complexity. Simulations validate the proposed methods. In particular, we observe that the MSE and SCNR constraints exhibit complementary performance depending on the operating environment, which manifests the importance of the proposed comprehensive and robust optimization framework.","sentences":["Integrated sensing and communication (ISAC) is widely recognized as a fundamental enabler for future wireless communications.","In this paper, we present a joint communication and radar beamforming framework for maximizing a sum spectral efficiency (SE) while guaranteeing desired radar performance with imperfect channel state information (CSI) in multi-user and multi-target ISAC systems.","To this end, we adopt either a radar transmit beam mean square error (MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar performance constraint of a sum SE maximization problem.","To resolve inherent challenges such as non-convexity and imperfect CSI, we reformulate the problems and identify first-order optimality conditions for the joint radar and communication beamformer.","Turning the condition to a nonlinear eigenvalue problem with eigenvector dependency (NEPv), we develop an alternating method which finds the joint beamformer through power iteration and a Lagrangian multiplier through binary search.","The proposed framework encompasses both the radar metrics and is robust to channel estimation error with low complexity.","Simulations validate the proposed methods.","In particular, we observe that the MSE and SCNR constraints exhibit complementary performance depending on the operating environment, which manifests the importance of the proposed comprehensive and robust optimization framework."],"url":"http://arxiv.org/abs/2402.09155v1","category":"eess.SP"}
{"created":"2024-02-14 13:08:26","title":"Improved Regret for Bandit Convex Optimization with Delayed Feedback","abstract":"We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly convex functions, and prove that the proposed algorithm can enjoy a better regret bound of $O(T^{2/3}\\log^{1/3}T+d\\log T)$. Finally, we show that in a special case with unconstrained action sets, it can be simply extended to achieve a regret bound of $O(\\sqrt{T\\log T}+d\\log T)$ for strongly convex and smooth functions.","sentences":["We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay.","Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm.","In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism.","Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\\sqrt{dT})$ for convex functions.","Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly convex functions, and prove that the proposed algorithm can enjoy a better regret bound of $O(T^{2/3}\\log^{1/3}T+d\\log T)$. Finally, we show that in a special case with unconstrained action sets, it can be simply extended to achieve a regret bound of $O(\\sqrt{T\\log T}+d\\log T)$ for strongly convex and smooth functions."],"url":"http://arxiv.org/abs/2402.09152v1","category":"cs.LG"}
{"created":"2024-02-14 12:59:43","title":"Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity","abstract":"We study the \\emph{sensitivity oracles problem for subgraph connectivity} in the \\emph{decremental} and \\emph{fully dynamic} settings. In the fully dynamic setting, we preprocess an $n$-vertices $m$-edges undirected graph $G$ with $n_{\\rm off}$ deactivated vertices initially and the others are activated. Then we receive a single update $D\\subseteq V(G)$ of size $|D| = d \\leq d_{\\star}$, representing vertices whose states will be switched. Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices $u$ and $v$ in the activated subgraph. The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \\emph{vertex-failure connectivity oracles} problem.   We present a better deterministic vertex-failure connectivity oracle with $\\widehat{O}(d_{\\star}m)$ preprocessing time, $\\widetilde{O}(m)$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from $\\widehat{O}(d^{2})$ to $\\widetilde{O}(d^{2})$.   We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with $\\widehat{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{\\omega}\\})$ preprocessing time, $\\widetilde{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{2}\\})$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from $\\widetilde{O}(d^{4})$ to $\\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures.","sentences":["We study the \\emph{sensitivity oracles problem for subgraph connectivity} in the \\emph{decremental} and \\emph{fully dynamic} settings.","In the fully dynamic setting, we preprocess an $n$-vertices $m$-edges undirected graph $G$ with $n_{\\rm off}$ deactivated vertices initially and the others are activated.","Then we receive a single update $D\\subseteq V(G)$ of size $|D| = d \\leq d_{\\star}$, representing vertices whose states will be switched.","Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices $u$ and $v$ in the activated subgraph.","The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \\emph{vertex-failure connectivity oracles} problem.   ","We present a better deterministic vertex-failure connectivity oracle with $\\widehat{O}(d_{\\star}m)$ preprocessing time, $\\widetilde{O}(m)$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from $\\widehat{O}(d^{2})$ to $\\widetilde{O}(d^{2})$.   We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with $\\widehat{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{\\omega}\\})$ preprocessing time, $\\widetilde{O}(\\min\\{m(n_{\\rm off} + d_{\\star}),n^{2}\\})$ space, $\\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from $\\widetilde{O}(d^{4})$ to $\\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures."],"url":"http://arxiv.org/abs/2402.09150v1","category":"cs.DS"}
{"created":"2024-02-14 12:41:09","title":"Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies","abstract":"This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks. The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in text augmentation strategies in NLP.","sentences":["This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods.","It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection.","The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training.","A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field.","Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance.","These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks.","The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in text augmentation strategies in NLP."],"url":"http://arxiv.org/abs/2402.09141v1","category":"cs.CL"}
{"created":"2024-02-14 12:39:48","title":"The Photochemistry of Rydberg Excited Cyclobutanone: Photoinduced Processes and Ground State Dynamics","abstract":"Owing to ring-strain, cyclic ketones exhibit complex excited-state dynamics with multiple competing photochemical channels active on the ultrafast timescale. While the excited-state dynamics of cyclobutanone after $\\pi^{\\ast}\\leftarrow n$ excitation into the lowest-energy excited singlet state (S$_1$) has been extensively studied, the dynamics following 3$s\\leftarrow n$ excitation into the higher-lying singlet Rydberg (S$_2$) state are less well understood. Herein, we couple quantum and excited-state trajectory surface-hopping molecular dynamics simulations to study the relaxation of cyclobutanone following 3s$\\leftarrow n$ excitation and to predict the ultrafast electron diffraction scattering signal that we anticipate to arise from the relaxation dynamics that we observe. Our simulations indicate that relaxation from the initially-populated singlet Rydberg state occurs on the hundreds-of-femtosecond to picosecond timescale consistent with the symmetry-forbidden nature of the state-to-state transition involved. Once cyclobutanone has relaxed non-radiatively to the electronic ground state (S$_0$), the vibrationally hot molecules have sufficient energy to form multiple fragmentory products on the electronic ground-state surface including C$_2$H$_4$ + CH$_2$CO (C2; 20%), and C$_3$H$_6$ + CO (C3; 2.5%). We discuss the limitations of our simulations, how these may influence the outcome of the excited-state dynamics we observe, and -- ultimately -- the predictive power of the simulated experimental observable.","sentences":["Owing to ring-strain, cyclic ketones exhibit complex excited-state dynamics with multiple competing photochemical channels active on the ultrafast timescale.","While the excited-state dynamics of cyclobutanone after $\\pi^{\\ast}\\leftarrow n$ excitation into the lowest-energy excited singlet state (S$_1$) has been extensively studied, the dynamics following 3$s\\leftarrow n$ excitation into the higher-lying singlet Rydberg (S$_2$) state are less well understood.","Herein, we couple quantum and excited-state trajectory surface-hopping molecular dynamics simulations to study the relaxation of cyclobutanone following 3s$\\leftarrow n$ excitation and to predict the ultrafast electron diffraction scattering signal that we anticipate to arise from the relaxation dynamics that we observe.","Our simulations indicate that relaxation from the initially-populated singlet Rydberg state occurs on the hundreds-of-femtosecond to picosecond timescale consistent with the symmetry-forbidden nature of the state-to-state transition involved.","Once cyclobutanone has relaxed non-radiatively to the electronic ground state (S$_0$), the vibrationally hot molecules have sufficient energy to form multiple fragmentory products on the electronic ground-state surface including C$_2$H$_4$ + CH$_2$CO (C2; 20%), and C$_3$H$_6$ + CO (C3; 2.5%).","We discuss the limitations of our simulations, how these may influence the outcome of the excited-state dynamics we observe, and -- ultimately -- the predictive power of the simulated experimental observable."],"url":"http://arxiv.org/abs/2402.09140v1","category":"physics.chem-ph"}
{"created":"2024-02-14 12:24:21","title":"MPIrigen: MPI Code Generation through Domain-Specific Language Models","abstract":"The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen","sentences":["The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration.","The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored.","This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs.","Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs.","In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models.","Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI.","We call the resulting model as MPIrigen.","We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context.","Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions.","The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools.","The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen"],"url":"http://arxiv.org/abs/2402.09126v1","category":"cs.DC"}
{"created":"2024-02-14 11:59:30","title":"Deterministic identification over channels with finite output: a dimensional perspective on superlinear rates","abstract":"Following initial work by JaJa and Ahlswede/Cai, and inspired by a recent renewed surge in interest in deterministic identification via noisy channels, we consider the problem in its generality for memoryless channels with finite output, but arbitrary input alphabets.   Such a channel is essentially given by (the closure of) the subset of its output distributions in the probability simplex. Our main findings are that the maximum number of messages thus identifiable scales super-exponentially as $2^{R\\,n\\log n}$ with the block length $n$, and that the optimal rate $R$ is upper and lower bounded in terms of the covering (aka Minkowski, or Kolmogorov, or entropy) dimension $d$ of the output set: $\\frac14 d \\leq R \\leq d$. Leading up to the general case, we treat the important special case of the so-called Bernoulli channel with input alphabet $[0;1]$ and binary output, which has $d=1$, to gain intuition. Along the way, we show a certain Hypothesis Testing Lemma (generalising an earlier insight of Ahlswede regarding the intersection of typical sets) that implies that for the construction of a deterministic identification code, it is sufficient to ensure pairwise reliable distinguishability of the output distributions.   These results are then shown to generalise directly to classical-quantum channels with finite-dimensional output quantum system (but arbitrary input alphabet), and in particular to quantum channels on finite-dimensional quantum systems under the constraint that the identification code can only use tensor product inputs.","sentences":["Following initial work by JaJa and Ahlswede/Cai, and inspired by a recent renewed surge in interest in deterministic identification via noisy channels, we consider the problem in its generality for memoryless channels with finite output, but arbitrary input alphabets.   ","Such a channel is essentially given by (the closure of) the subset of its output distributions in the probability simplex.","Our main findings are that the maximum number of messages thus identifiable scales super-exponentially as $2^{R\\,n\\log n}$ with the block length $n$, and that the optimal rate $R$ is upper and lower bounded in terms of the covering (aka Minkowski, or Kolmogorov, or entropy) dimension $d$ of the output set: $\\frac14 d \\leq R \\leq d$. Leading up to the general case, we treat the important special case of the so-called Bernoulli channel with input alphabet $[0;1]$ and binary output, which has $d=1$, to gain intuition.","Along the way, we show a certain Hypothesis Testing Lemma (generalising an earlier insight of Ahlswede regarding the intersection of typical sets) that implies that for the construction of a deterministic identification code, it is sufficient to ensure pairwise reliable distinguishability of the output distributions.   ","These results are then shown to generalise directly to classical-quantum channels with finite-dimensional output quantum system (but arbitrary input alphabet), and in particular to quantum channels on finite-dimensional quantum systems under the constraint that the identification code can only use tensor product inputs."],"url":"http://arxiv.org/abs/2402.09117v1","category":"cs.IT"}
{"created":"2024-02-14 11:55:50","title":"Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space","abstract":"Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour of any RL algorithm, and also allows us to compare the exploratory behaviours of different RL algorithms.","sentences":["Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning.","Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm.","Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution.","The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space.","We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour of any RL algorithm, and also allows us to compare the exploratory behaviours of different RL algorithms."],"url":"http://arxiv.org/abs/2402.09113v1","category":"cs.LG"}
{"created":"2024-02-14 11:18:48","title":"Multiple-output composite quantile regression through an optimal transport lens","abstract":"Composite quantile regression has been used to obtain robust estimators of regression coefficients in linear models with good statistical efficiency. By revealing an intrinsic link between the composite quantile regression loss function and the Wasserstein distance from the residuals to the set of quantiles, we establish a generalization of the composite quantile regression to the multiple-output settings. Theoretical convergence rates of the proposed estimator are derived both under the setting where the additive error possesses only a finite $\\ell$-th moment (for $\\ell > 2$) and where it exhibits a sub-Weibull tail. In doing so, we develop novel techniques for analyzing the M-estimation problem that involves Wasserstein-distance in the loss. Numerical studies confirm the practical effectiveness of our proposed procedure.","sentences":["Composite quantile regression has been used to obtain robust estimators of regression coefficients in linear models with good statistical efficiency.","By revealing an intrinsic link between the composite quantile regression loss function and the Wasserstein distance from the residuals to the set of quantiles, we establish a generalization of the composite quantile regression to the multiple-output settings.","Theoretical convergence rates of the proposed estimator are derived both under the setting where the additive error possesses only a finite $\\ell$-th moment (for $\\ell > 2$) and where it exhibits a sub-Weibull tail.","In doing so, we develop novel techniques for analyzing the M-estimation problem that involves Wasserstein-distance in the loss.","Numerical studies confirm the practical effectiveness of our proposed procedure."],"url":"http://arxiv.org/abs/2402.09098v1","category":"math.ST"}
{"created":"2024-02-14 11:16:50","title":"FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning","abstract":"In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student models and addressing device constraints. FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25\\% and 18\\% for highly skewed data at $\\alpha = {0.1,0.5}$ on the HAR and MNIST datasets, respectively. Its faster convergence is illustrated by a 17\\% and 20\\% increase in accuracy within the first five rounds on the HAR and MNIST datasets, respectively, highlighting its early-stage learning proficiency. Code is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)","sentences":["In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy.","The non-independent and identically distributed (non-i.i.d.)","nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL.","Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation.","Traditional FL approaches may suffer from these challenges.","Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework.","As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity.","This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student models and addressing device constraints.","FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25\\% and 18\\% for highly skewed data at $\\alpha = {0.1,0.5}$ on the HAR and MNIST datasets, respectively.","Its faster convergence is illustrated by a 17\\% and 20\\% increase in accuracy within the first five rounds on the HAR and MNIST datasets, respectively, highlighting its early-stage learning proficiency.","Code is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)"],"url":"http://arxiv.org/abs/2402.09095v1","category":"cs.LG"}
{"created":"2024-02-14 10:52:39","title":"Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?","abstract":"The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things. Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors.","sentences":["The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections.","Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed.","In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection.","Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector.","We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things.","Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors."],"url":"http://arxiv.org/abs/2402.09082v1","category":"cs.CR"}
{"created":"2024-02-14 10:48:00","title":"Low-Rank Extragradient Methods for Scalable Semidefinite Optimization","abstract":"We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in worst-case. Our approach is supported by numerical experiments conducted with a dataset of Max-Cut instances.","sentences":["We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains.","We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition.","We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in worst-case.","Our approach is supported by numerical experiments conducted with a dataset of Max-Cut instances."],"url":"http://arxiv.org/abs/2402.09081v1","category":"math.OC"}
{"created":"2024-02-14 10:40:09","title":"DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform","abstract":"In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform. DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment. Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose. This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements. Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\\% and 98.2\\%, respectively. As executed on a GPU, our two-stage method can ensure the requirement for real-time computation. Codes are released at https://github.com/FLAMEZZ5201/DisGNet.","sentences":["In this paper, we propose a graph neural network, DisGNet, for learning the graph distance matrix to address the forward kinematics problem of the Gough-Stewart platform.","DisGNet employs the k-FWL algorithm for message-passing, providing high expressiveness with a small parameter count, making it suitable for practical deployment.","Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet's output poses, achieving ultra-high-precision pose.","This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements.","Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8\\% and 98.2\\%, respectively.","As executed on a GPU, our two-stage method can ensure the requirement for real-time computation.","Codes are released at https://github.com/FLAMEZZ5201/DisGNet."],"url":"http://arxiv.org/abs/2402.09077v1","category":"cs.RO"}
{"created":"2024-02-14 10:33:17","title":"Trace Ratio Based Manifold Learning with Tensor Data","abstract":"In this paper, we propose an extension of trace ratio based Manifold learning methods to deal with multidimensional data sets. Based on recent progress on the tensor-tensor product, we present a generalization of the trace ratio criterion by using the properties of the t-product. This will conduct us to introduce some new concepts such as Laplacian tensor and we will study formally the trace ratio problem by discuting the conditions for the exitence of solutions and optimality. Next, we will present a tensor Newton QR decomposition algorithm for solving the trace ratio problem. Manifold learning methods such as Laplacian eigenmaps, linear discriminant analysis and locally linear embedding will be formulated in a tensor representation and optimized by the proposed algorithm. Lastly, we will evaluate the performance of the different studied dimension reduction methods on several synthetic and real world data sets.","sentences":["In this paper, we propose an extension of trace ratio based Manifold learning methods to deal with multidimensional data sets.","Based on recent progress on the tensor-tensor product, we present a generalization of the trace ratio criterion by using the properties of the t-product.","This will conduct us to introduce some new concepts such as Laplacian tensor and we will study formally the trace ratio problem by discuting the conditions for the exitence of solutions and optimality.","Next, we will present a tensor Newton QR decomposition algorithm for solving the trace ratio problem.","Manifold learning methods such as Laplacian eigenmaps, linear discriminant analysis and locally linear embedding will be formulated in a tensor representation and optimized by the proposed algorithm.","Lastly, we will evaluate the performance of the different studied dimension reduction methods on several synthetic and real world data sets."],"url":"http://arxiv.org/abs/2402.09072v1","category":"math.NA"}
{"created":"2024-02-14 10:28:43","title":"Using quantum annealing to design lattice proteins","abstract":"Quantum annealing has shown promise for finding solutions to difficult optimization problems, including protein folding. Recently, we used the D-Wave Advantage quantum annealer to explore the folding problem in a coarse-grained lattice model, the HP model, in which amino acids are classified into two broad groups: hydrophobic (H) and polar (P). Using a set of 22 HP sequences with up to 64 amino acids, we demonstrated the fast and consistent identification of the correct HP model ground states using the D-Wave hybrid quantum-classical solver. An equally relevant biophysical challenge, called the protein design problem, is the inverse of the above, where the task is to predict protein sequences that fold to a given structure. Here, we approach the design problem by a two-step procedure, implemented and executed on a D-Wave machine. In the first step, we perform a pure sequence-space search by varying the type of amino acid at each sequence position, and seek sequences which minimize the HP-model energy of the target structure. After mapping this task onto an Ising spin glass representation, we employ a hybrid quantum-classical solver to deliver energy-optimal sequences for structures with 30-64 amino acids, with a 100% success rate. In the second step, we filter the optimized sequences from the first step according to their ability to fold to the intended structure. In addition, we try solving the sequence optimization problem using only the QPU, which confines us to sizes $\\le$20, due to exponentially decreasing success rates. To shed light on the pure QPU results, we investigate the effects of control errors caused by an imperfect implementation of the intended Hamiltonian on the QPU, by numerically analyzing the Schr\\\"odinger equation. We find that the simulated success rates in the presence of control noise semi-quantitatively reproduce the modest pure QPU results for larger chains.","sentences":["Quantum annealing has shown promise for finding solutions to difficult optimization problems, including protein folding.","Recently, we used the D-Wave Advantage quantum annealer to explore the folding problem in a coarse-grained lattice model, the HP model, in which amino acids are classified into two broad groups: hydrophobic (H) and polar (P).","Using a set of 22 HP sequences with up to 64 amino acids, we demonstrated the fast and consistent identification of the correct HP model ground states using the D-Wave hybrid quantum-classical solver.","An equally relevant biophysical challenge, called the protein design problem, is the inverse of the above, where the task is to predict protein sequences that fold to a given structure.","Here, we approach the design problem by a two-step procedure, implemented and executed on a D-Wave machine.","In the first step, we perform a pure sequence-space search by varying the type of amino acid at each sequence position, and seek sequences which minimize the HP-model energy of the target structure.","After mapping this task onto an Ising spin glass representation, we employ a hybrid quantum-classical solver to deliver energy-optimal sequences for structures with 30-64 amino acids, with a 100% success rate.","In the second step, we filter the optimized sequences from the first step according to their ability to fold to the intended structure.","In addition, we try solving the sequence optimization problem using only the QPU, which confines us to sizes $\\le$20, due to exponentially decreasing success rates.","To shed light on the pure QPU results, we investigate the effects of control errors caused by an imperfect implementation of the intended Hamiltonian on the QPU, by numerically analyzing the Schr\\\"odinger equation.","We find that the simulated success rates in the presence of control noise semi-quantitatively reproduce the modest pure QPU results for larger chains."],"url":"http://arxiv.org/abs/2402.09069v1","category":"quant-ph"}
{"created":"2024-02-14 10:07:05","title":"Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?","abstract":"Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures.","sentences":["Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty.","Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years.","The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted.","This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures.","With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-order loss minimization, and the relative (rather than absolute) nature of epistemic uncertainty measures."],"url":"http://arxiv.org/abs/2402.09056v1","category":"cs.AI"}
{"created":"2024-02-14 09:46:53","title":"End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training","abstract":"End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning. Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain. Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality. Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap. In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors. On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate representations based on the Hilbert-Schmidt independence criterion (HSIC). The results of our normalized HSIC value analysis reveal the E2E training ability to exhibit different information dynamics across layers, in addition to efficient information propagation. Furthermore, we show that this layer-role differentiation leads to the final representation following the information bottleneck principle. It suggests the need to consider the cooperative interactions between layers, not just the final layer when analyzing the information bottleneck of deep learning.","sentences":["End-to-end (E2E) training, optimizing the entire model through error backpropagation, fundamentally supports the advancements of deep learning.","Despite its high performance, E2E training faces the problems of memory consumption, parallel computing, and discrepancy with the functionalities of the actual brain.","Various alternative methods have been proposed to overcome these difficulties; however, no one can yet match the performance of E2E training, thereby falling short in practicality.","Furthermore, there is no deep understanding regarding differences in the trained model properties beyond the performance gap.","In this paper, we reconsider why E2E training demonstrates a superior performance through a comparison with layer-wise training, a non-E2E method that locally sets errors.","On the basis of the observation that E2E training has an advantage in propagating input information, we analyze the information plane dynamics of intermediate representations based on the Hilbert-Schmidt independence criterion (HSIC).","The results of our normalized HSIC value analysis reveal the E2E training ability to exhibit different information dynamics across layers, in addition to efficient information propagation.","Furthermore, we show that this layer-role differentiation leads to the final representation following the information bottleneck principle.","It suggests the need to consider the cooperative interactions between layers, not just the final layer when analyzing the information bottleneck of deep learning."],"url":"http://arxiv.org/abs/2402.09050v1","category":"cs.LG"}
{"created":"2024-02-14 09:01:13","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks","abstract":"Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB","sentences":["Large language models (LLMs) have proven to be highly effective across various natural language processing tasks.","However, their large number of parameters poses significant challenges for practical deployment.","Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network.","Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup.","In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks.","We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks.","This choice allows us to effectively enhance the processing speed of LLMs.","Our experimental results demonstrate that SLEB successfully accelerates LLM inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of LLMs.","The code is available at: https://github.com/leapingjagg-dev/SLEB"],"url":"http://arxiv.org/abs/2402.09025v1","category":"cs.CL"}
{"created":"2024-02-14 08:56:41","title":"Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems","abstract":"Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility.","sentences":["Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks.","Understanding attack tactics helps improve the robustness of RSs.","We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors.","In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles.","Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced.","Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility."],"url":"http://arxiv.org/abs/2402.09023v1","category":"cs.CR"}
{"created":"2024-02-14 08:51:53","title":"Bayesian reliability acceptance sampling plan with optional warranty under hybrid censoring","abstract":"This work considers design of Bayesian reliability acceptance sampling plan (RASP) under hybrid censored life test for the products sold under optional warranty. The consumer and manufacturer agree on a common lifetime distribution of the product. However, they differ in the assessment of the prior distributions because of the adversarial nature of the consumer and manufacturer. The consumer takes decision based on his/her utility and prior belief without warranty offer by the manufacturer. If the decision is rejection, manufacturer provides warranty offer to the consumer. If the consumer rejects the lot with a warranty, the manufacturer conducts life test under hybrid censoring scheme (HCS) and provide lifetime information to the consumer. The consumer updates his/her belief based on lifetime information provided by the manufacturer. The consumer then takes decision of acceptance or rejection of lot based on updated belief. Task of the manufacturer is to determine the optimal life testing plan.","sentences":["This work considers design of Bayesian reliability acceptance sampling plan (RASP) under hybrid censored life test for the products sold under optional warranty.","The consumer and manufacturer agree on a common lifetime distribution of the product.","However, they differ in the assessment of the prior distributions because of the adversarial nature of the consumer and manufacturer.","The consumer takes decision based on his/her utility and prior belief without warranty offer by the manufacturer.","If the decision is rejection, manufacturer provides warranty offer to the consumer.","If the consumer rejects the lot with a warranty, the manufacturer conducts life test under hybrid censoring scheme (HCS) and provide lifetime information to the consumer.","The consumer updates his/her belief based on lifetime information provided by the manufacturer.","The consumer then takes decision of acceptance or rejection of lot based on updated belief.","Task of the manufacturer is to determine the optimal life testing plan."],"url":"http://arxiv.org/abs/2402.09020v1","category":"stat.AP"}
{"created":"2024-02-14 08:44:26","title":"The Order Oracle: a New Concept in The Black Box Optimization Problems","abstract":"Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. In this paper, we provide the new concept of the \"Order Oracle\" as a novel approach to solving such problems. The Order Oracle offers a unique perspective, using only access to the order between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we provide estimates of the convergence rates of the algorithms (obtained by integrating the Order Oracle into existing optimization \"tools\") in the non-convex, convex, and strongly convex settings. Our theoretical results demonstrate the effectiveness of the Order Oracle through numerical experiments. Finally, we show the possibility of accelerating the convergence of such algorithms.","sentences":["Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function.","In this paper, we provide the new concept of the \"Order Oracle\" as a novel approach to solving such problems.","The Order Oracle offers a unique perspective, using only access to the order between function values (possibly with some bounded noise), but without assuming access to their values.","As theoretical results, we provide estimates of the convergence rates of the algorithms (obtained by integrating the Order Oracle into existing optimization \"tools\") in the non-convex, convex, and strongly convex settings.","Our theoretical results demonstrate the effectiveness of the Order Oracle through numerical experiments.","Finally, we show the possibility of accelerating the convergence of such algorithms."],"url":"http://arxiv.org/abs/2402.09014v1","category":"math.OC"}
{"created":"2024-02-14 08:28:04","title":"A Practical and Online Trajectory Planner for Autonomous Ships' Berthing, Incorporating Speed Control","abstract":"Autonomous ships are essentially designed and equipped to perceive their internal and external environment and subsequently perform appropriate actions depending on the predetermined objective(s) without human intervention. Consequently, trajectory planning algorithms for autonomous berthing must consider factors such as system dynamics, ship actuators, environmental disturbances, and the safety of the ship, other ships, and port structures, among others. In this study, basing the ship dynamics on the low-speed MMG model, trajectory planning for an autonomous ship is modeled as an optimal control problem (OCP) that is transcribed into a nonlinear programming problem (NLP) using the direct multiple shooting technique. To enhance berthing safety, besides considering wind disturbances, speed control, actuators' limitations, and collision avoidance features are incorporated as constraints in the NLP, which is then solved using the Sequential Quadratic Programming (SQP) algorithm in MATLAB. Finally, the performance of the proposed planner is evaluated through (i) comparison with solutions obtained using CMA-ES for two different model ships, (ii) trajectory planning for different harbor entry and berth approach scenarios, and (iii) feasibility study using stochastically generated initial conditions and positions within the port boundaries. Simulation results indicate enhanced berthing safety as well as practical and computational feasibility making the planner suitable for real-time applications.","sentences":["Autonomous ships are essentially designed and equipped to perceive their internal and external environment and subsequently perform appropriate actions depending on the predetermined objective(s) without human intervention.","Consequently, trajectory planning algorithms for autonomous berthing must consider factors such as system dynamics, ship actuators, environmental disturbances, and the safety of the ship, other ships, and port structures, among others.","In this study, basing the ship dynamics on the low-speed MMG model, trajectory planning for an autonomous ship is modeled as an optimal control problem (OCP) that is transcribed into a nonlinear programming problem (NLP) using the direct multiple shooting technique.","To enhance berthing safety, besides considering wind disturbances, speed control, actuators' limitations, and collision avoidance features are incorporated as constraints in the NLP, which is then solved using the Sequential Quadratic Programming (SQP) algorithm in MATLAB.","Finally, the performance of the proposed planner is evaluated through (i) comparison with solutions obtained using CMA-ES for two different model ships, (ii) trajectory planning for different harbor entry and berth approach scenarios, and (iii) feasibility study using stochastically generated initial conditions and positions within the port boundaries.","Simulation results indicate enhanced berthing safety as well as practical and computational feasibility making the planner suitable for real-time applications."],"url":"http://arxiv.org/abs/2402.09009v1","category":"eess.SY"}
{"created":"2024-02-14 08:20:49","title":"DRL-Based Orchestration of Multi-User MISO Systems with Stacked Intelligent Metasurfaces","abstract":"Stacked intelligent metasurfaces (SIM) represents an advanced signal processing paradigm that enables over-the-air processing of electromagnetic waves at the speed of light. Its multi-layer structure exhibits customizable increased computational capability compared to conventional single-layer reconfigurable intelligent surfaces and metasurface lenses. In this paper, we deploy SIM to improve the performance of multi-user multiple-input single-output (MISO) wireless systems with low complexity transmit radio frequency (RF) chains. In particular, an optimization formulation for the joint design of the SIM phase shifts and the transmit power allocation is presented, which is efficiently solved via a customized deep reinforcement learning (DRL) approach that continuously observes pre-designed states of the SIM-parametrized smart wireless environment. The presented performance evaluation results showcase the proposed method's capability to effectively learn from the wireless environment while outperforming conventional precoding schemes under low transmit power conditions. Finally, a whitening process is presented to further augment the robustness of the proposed scheme.","sentences":["Stacked intelligent metasurfaces (SIM) represents an advanced signal processing paradigm that enables over-the-air processing of electromagnetic waves at the speed of light.","Its multi-layer structure exhibits customizable increased computational capability compared to conventional single-layer reconfigurable intelligent surfaces and metasurface lenses.","In this paper, we deploy SIM to improve the performance of multi-user multiple-input single-output (MISO) wireless systems with low complexity transmit radio frequency (RF) chains.","In particular, an optimization formulation for the joint design of the SIM phase shifts and the transmit power allocation is presented, which is efficiently solved via a customized deep reinforcement learning (DRL) approach that continuously observes pre-designed states of the SIM-parametrized smart wireless environment.","The presented performance evaluation results showcase the proposed method's capability to effectively learn from the wireless environment while outperforming conventional precoding schemes under low transmit power conditions.","Finally, a whitening process is presented to further augment the robustness of the proposed scheme."],"url":"http://arxiv.org/abs/2402.09006v1","category":"eess.SP"}
