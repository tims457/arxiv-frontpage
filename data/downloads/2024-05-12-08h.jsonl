{"created":"2024-05-09 17:59:56","title":"Gravitational wave signatures of departures from classical black hole scattering","abstract":"We initiate a general investigation into gravitational wave signatures of modifications to scattering of gravitational radiation from black holes. Such modifications may be present due to the quantum dynamics that makes black holes consistent with quantum mechanics, or in other models for departures from classical black hole behavior. We propose a parameterization of the corrections to scattering as a physically meaningful, model-independent, and practical bridge between theoretical and observational aspects of the problem; this parameterization can incorporate different models in the literature. We then describe how these corrections influence the gravitational wave signal, e.g. of a body orbiting a much more massive black hole. In particular, they generically change the rate of energy emission; this effect can be leveraged over many orbits of inspiral to enhance the sensitivity to small corrections, as has been noticed in simple models. We provide preliminary estimates of the sensitivity of future gravitational wave observations to these corrections, and outline further work to be done to connect both to a more fundamental theory of quantum black holes, and to realistic observational situations.","sentences":["We initiate a general investigation into gravitational wave signatures of modifications to scattering of gravitational radiation from black holes.","Such modifications may be present due to the quantum dynamics that makes black holes consistent with quantum mechanics, or in other models for departures from classical black hole behavior.","We propose a parameterization of the corrections to scattering as a physically meaningful, model-independent, and practical bridge between theoretical and observational aspects of the problem; this parameterization can incorporate different models in the literature.","We then describe how these corrections influence the gravitational wave signal, e.g. of a body orbiting a much more massive black hole.","In particular, they generically change the rate of energy emission; this effect can be leveraged over many orbits of inspiral to enhance the sensitivity to small corrections, as has been noticed in simple models.","We provide preliminary estimates of the sensitivity of future gravitational wave observations to these corrections, and outline further work to be done to connect both to a more fundamental theory of quantum black holes, and to realistic observational situations."],"url":"http://arxiv.org/abs/2405.05970v1","category":"gr-qc"}
{"created":"2024-05-09 17:59:55","title":"A Universal Growth Rate for Learning with Smooth Surrogate Losses","abstract":"This paper presents a comprehensive analysis of the growth rate of $H$-consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth comp-sum and constrained losses, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how $H$-consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify minimizability gaps as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds.","sentences":["This paper presents a comprehensive analysis of the growth rate of $H$-consistency bounds (and excess error bounds) for various surrogate losses used in classification.","We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions.","This result also translates to excess error bounds.","Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel.","Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth comp-sum and constrained losses, covering common choices for training neural networks in multi-class classification.","Given this universal rate, we turn to the question of choosing among different surrogate losses.","We first examine how $H$-consistency bounds vary across surrogates based on the number of classes.","Next, ignoring constants and focusing on behavior near zero, we identify minimizability gaps as the key differentiating factor in these bounds.","Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps.","Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds."],"url":"http://arxiv.org/abs/2405.05968v1","category":"cs.LG"}
{"created":"2024-05-09 17:59:40","title":"Distilling Diffusion Models into Conditional GANs","abstract":"We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark.","sentences":["We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality.","Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory.","For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations.","Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation.","E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs.","We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark."],"url":"http://arxiv.org/abs/2405.05967v1","category":"cs.CV"}
{"created":"2024-05-09 17:59:32","title":"Natural Language Processing RELIES on Linguistics","abstract":"Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language.","sentences":["Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence.","What does this mean for the future of linguistic expertise in NLP?","We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions.","We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language.","This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language."],"url":"http://arxiv.org/abs/2405.05966v1","category":"cs.CL"}
{"created":"2024-05-09 17:59:05","title":"Quantum Communication and Mixed-State Order in Decohered Symmetry-Protected Topological States","abstract":"Certain pure-state symmetry-protected topological orders (SPT) can be used as a resource for transmitting quantum information. Here, we investigate the ability to transmit quantum information using decohered SPT states, and relate this property to the \"strange correlation functions\" which diagnose quantum many-body orders in these mixed-states. This perspective leads to the identification of a class of quantum channels -- termed symmetry-decoupling channels -- which do not necessarily preserve any weak or strong symmetries of the SPT state, but nevertheless protect quantum many-body order in the decohered mixed-state. We quantify the ability to transmit quantum information in decohered SPT states through the coherent quantum information, whose behavior is generally related to a decoding problem, whereby local measurements in the system are used to attempt to \"learn\" the symmetry charge of the SPT state before decoherence.","sentences":["Certain pure-state symmetry-protected topological orders (SPT) can be used as a resource for transmitting quantum information.","Here, we investigate the ability to transmit quantum information using decohered SPT states, and relate this property to the \"strange correlation functions\" which diagnose quantum many-body orders in these mixed-states.","This perspective leads to the identification of a class of quantum channels -- termed symmetry-decoupling channels -- which do not necessarily preserve any weak or strong symmetries of the SPT state, but nevertheless protect quantum many-body order in the decohered mixed-state.","We quantify the ability to transmit quantum information in decohered SPT states through the coherent quantum information, whose behavior is generally related to a decoding problem, whereby local measurements in the system are used to attempt to \"learn\" the symmetry charge of the SPT state before decoherence."],"url":"http://arxiv.org/abs/2405.05965v1","category":"quant-ph"}
{"created":"2024-05-09 17:59:00","title":"Lattice Models for Phases and Transitions with Non-Invertible Symmetries","abstract":"Non-invertible categorical symmetries have emerged as a powerful tool to uncover new beyond-Landau phases of matter, both gapped and gapless, along with second order phase transitions between them. The general theory of such phases in (1+1)d has been studied using the Symmetry Topological Field Theory (SymTFT), also known as topological holography. This has unearthed the infrared (IR) structure of these phases and transitions. In this paper, we describe how the SymTFT information can be converted into an ultraviolet (UV) anyonic chain lattice model realizing in the IR limit these phases and transitions. In many cases, the Hilbert space of the anyonic chain is tensor product decomposable and the model can be realized as a quantum spin-chain Hamiltonian. We also describe operators acting on the lattice models that are charged under non-invertible symmetries and act as order parameters for the phases and transitions. In order to fully describe the action of non-invertible symmetries, it is crucial to understand the symmetry twisted sectors of the lattice models, which we describe in detail. Throughout the paper, we illustrate the general concepts using the symmetry category $\\mathsf{Rep}(S_3)$ formed by representations of the permutation group $S_3$, but our procedure can be applied to any fusion category symmetry.","sentences":["Non-invertible categorical symmetries have emerged as a powerful tool to uncover new beyond-Landau phases of matter, both gapped and gapless, along with second order phase transitions between them.","The general theory of such phases in (1+1)d has been studied using the Symmetry Topological Field Theory (SymTFT), also known as topological holography.","This has unearthed the infrared (IR) structure of these phases and transitions.","In this paper, we describe how the SymTFT information can be converted into an ultraviolet (UV) anyonic chain lattice model realizing in the IR limit these phases and transitions.","In many cases, the Hilbert space of the anyonic chain is tensor product decomposable and the model can be realized as a quantum spin-chain Hamiltonian.","We also describe operators acting on the lattice models that are charged under non-invertible symmetries and act as order parameters for the phases and transitions.","In order to fully describe the action of non-invertible symmetries, it is crucial to understand the symmetry twisted sectors of the lattice models, which we describe in detail.","Throughout the paper, we illustrate the general concepts using the symmetry category $\\mathsf{Rep}(S_3)$ formed by representations of the permutation group $S_3$, but our procedure can be applied to any fusion category symmetry."],"url":"http://arxiv.org/abs/2405.05964v1","category":"cond-mat.str-el"}
{"created":"2024-05-09 17:58:40","title":"Wormhole restrictions from quantum energy inequalities","abstract":"Wormhole solutions, bridges that connect different parts of spacetime, were proposed early in the history of General Relativity. Soon after, it was shown that all wormholes violate classical energy conditions, non-negativity constraints on contractions of the stress-energy tensor. Since these are violated by quantum fields, it was believed that wormholes can be constructed in the context of semiclassical gravity. But negative energies in quantum field theory are not without restriction: quantum energy inequalities (QEIs) control renormalized negative energies averaged over a geodesic. Thus, QEIs provide restrictions on the construction of wormholes. This work is a review of the relevant literature, focusing on results where QEIs restrict traversable wormholes. Both 'short' and 'long' (without causality violations) wormhole solutions in the context of semiclassical gravity are examined. A new result is presented on constraints on the Maldacena, Milekhin and Popov 'long' wormhole from the recently derived doubled smeared null energy condition.","sentences":["Wormhole solutions, bridges that connect different parts of spacetime, were proposed early in the history of General Relativity.","Soon after, it was shown that all wormholes violate classical energy conditions, non-negativity constraints on contractions of the stress-energy tensor.","Since these are violated by quantum fields, it was believed that wormholes can be constructed in the context of semiclassical gravity.","But negative energies in quantum field theory are not without restriction: quantum energy inequalities (QEIs) control renormalized negative energies averaged over a geodesic.","Thus, QEIs provide restrictions on the construction of wormholes.","This work is a review of the relevant literature, focusing on results where QEIs restrict traversable wormholes.","Both 'short' and 'long' (without causality violations) wormhole solutions in the context of semiclassical gravity are examined.","A new result is presented on constraints on the Maldacena, Milekhin and Popov 'long' wormhole from the recently derived doubled smeared null energy condition."],"url":"http://arxiv.org/abs/2405.05963v1","category":"gr-qc"}
{"created":"2024-05-09 17:57:46","title":"Towards comprehensive coverage of chemical space: Quantum mechanical properties of 836k constitutional and conformational closed shell neutral isomers consisting of HCNOFSiPSClBr","abstract":"The Vector-QM24 (VQM24) dataset attempts to more comprehensively cover all possible neutral closed shell small organic and inorganic molecules and their conformers at state of the art level of theory. We have used density functional theory ($\\omega$B97X-D3/cc-pVDZ) to optimize 577k conformational isomers corresponding to 258k constitutional isomers.Isomers included contain up to five heavy atoms (non-hydrogen) consisting of $p$-block elements C, N, O, F, Si, P, S, Cl, Br. Single point diffusion quantum Monte Carlo (DMC@PBE0(ccECP/cc-pVQZ)) energies are reported for the sub-set of the lowest conformers of 10,793 molecules with up to 4 heavy atoms.This dataset has been systematically generated by considering all combinatorially possible stoichiometries, and graphs (according to Lewis rules as implemented in the {\\tt SURGE} package), along with all stable conformers identified by GFN2-xTB. Apart from graphs, geometries, rotational constants, and vibrational normal modes, VQM24 includes internal, atomization, electron-electron repulsion, exchange correlation, dispersion, vibrational frequency, Gibbs free, enthalpy, ZPV, molecular orbital energies; as well as entropy, and heat capacities. Electronic properties include multipole moments (dipole, quadrupole, octupole, hexadecapole), electrostatic potentials at nuclei (alchemical potential), Mulliken charges, and molecular wavefunctions. VQM24 represents a highly accurate and unbiased dataset of molecules, ideal for testing and training transferable, scalable, and generative ML models of real quantum systems.","sentences":["The Vector-QM24 (VQM24) dataset attempts to more comprehensively cover all possible neutral closed shell small organic and inorganic molecules and their conformers at state of the art level of theory.","We have used density functional theory ($\\omega$B97X-D3/cc-pVDZ) to optimize 577k conformational isomers corresponding to 258k constitutional isomers.","Isomers included contain up to five heavy atoms (non-hydrogen) consisting of $p$-block elements C, N, O, F, Si, P, S, Cl, Br.","Single point diffusion quantum Monte Carlo (DMC@PBE0(ccECP/cc-pVQZ))","energies are reported for the sub-set of the lowest conformers of 10,793 molecules with up to 4 heavy atoms.","This dataset has been systematically generated by considering all combinatorially possible stoichiometries, and graphs (according to Lewis rules as implemented in the {\\tt SURGE} package), along with all stable conformers identified by GFN2-xTB.","Apart from graphs, geometries, rotational constants, and vibrational normal modes, VQM24 includes internal, atomization, electron-electron repulsion, exchange correlation, dispersion, vibrational frequency, Gibbs free, enthalpy, ZPV, molecular orbital energies; as well as entropy, and heat capacities.","Electronic properties include multipole moments (dipole, quadrupole, octupole, hexadecapole), electrostatic potentials at nuclei (alchemical potential), Mulliken charges, and molecular wavefunctions.","VQM24 represents a highly accurate and unbiased dataset of molecules, ideal for testing and training transferable, scalable, and generative ML models of real quantum systems."],"url":"http://arxiv.org/abs/2405.05961v1","category":"physics.chem-ph"}
{"created":"2024-05-09 17:55:16","title":"Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask","abstract":"Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.","sentences":["Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks.","Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances.","Recently, diffusion-based methods have shown advanced generative capabilities.","However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL.","Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach.","TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask.","It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part.","We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part.","Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering.","We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data."],"url":"http://arxiv.org/abs/2405.05959v1","category":"cs.LG"}
{"created":"2024-05-09 17:53:41","title":"Stability of slow Hamiltonian dynamics from Lieb-Robinson bounds","abstract":"We rigorously show that a local spin system giving rise to a slow Hamiltonian dynamics is stable against generic, even time-dependent, local perturbations. The sum of these perturbations can cover a significant amount of the system's size. The stability of the slow dynamics follows from proving that the Lieb-Robinson bound for the dynamics of the total Hamiltonian is the sum of two contributions: the Lieb-Robinson bound of the unperturbed dynamics and an additional term coming from the Lieb-Robinson bound of the perturbations with respect to the unperturbed Hamiltonian. Our results are particularly relevant in the context of the study of the stability of Many-Body-Localized systems, implying that if a so called ergodic region is present in the system, to spread across a certain distance it takes a time proportional to the exponential of such distance. The non-perturbative nature of our result allows us to develop a dual description of the dynamics of a system. As a consequence we are able to prove that the presence of a region of disorder in a ergodic system implies the slowing down of the dynamics in the vicinity of that region.","sentences":["We rigorously show that a local spin system giving rise to a slow Hamiltonian dynamics is stable against generic, even time-dependent, local perturbations.","The sum of these perturbations can cover a significant amount of the system's size.","The stability of the slow dynamics follows from proving that the Lieb-Robinson bound for the dynamics of the total Hamiltonian is the sum of two contributions: the Lieb-Robinson bound of the unperturbed dynamics and an additional term coming from the Lieb-Robinson bound of the perturbations with respect to the unperturbed Hamiltonian.","Our results are particularly relevant in the context of the study of the stability of Many-Body-Localized systems, implying that if a so called ergodic region is present in the system, to spread across a certain distance it takes a time proportional to the exponential of such distance.","The non-perturbative nature of our result allows us to develop a dual description of the dynamics of a system.","As a consequence we are able to prove that the presence of a region of disorder in a ergodic system implies the slowing down of the dynamics in the vicinity of that region."],"url":"http://arxiv.org/abs/2405.05958v1","category":"quant-ph"}
{"created":"2024-05-09 17:52:42","title":"Probing Multimodal LLMs as World Models for Driving","abstract":"We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment. Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration. We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera. Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior. The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data. To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving. Additionally, we contribute the full open-source code and a new dataset, \"Eval-LLM-Drive\", for evaluating MLLMs in driving. Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments.","sentences":["We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment.","Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration.","We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera.","Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior.","The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data.","To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving.","Additionally, we contribute the full open-source code and a new dataset, \"Eval-LLM-Drive\", for evaluating MLLMs in driving.","Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments."],"url":"http://arxiv.org/abs/2405.05956v1","category":"cs.RO"}
{"created":"2024-05-09 17:46:22","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","abstract":"Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement.","sentences":["Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames.","Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations.","Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times.","The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large.","This makes the sampling trajectory random, resulting in diverse rather than deterministic generations.","To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion.","Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations.","Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement."],"url":"http://arxiv.org/abs/2405.05953v1","category":"cs.CV"}
{"created":"2024-05-09 17:44:29","title":"New Algorithms and Lower Bounds for Streaming Tournaments","abstract":"We study fundamental directed graph (digraph) problems in the streaming model. An initial investigation by Chakrabarti, Ghosh, McGregor, and Vorotnikova [SODA'20] on streaming digraphs showed that while most of these problems are provably hard in general, some of them become tractable when restricted to the well-studied class of tournament graphs where every pair of nodes shares exactly one directed edge. Thus, we focus on tournaments and improve the state of the art for multiple problems in terms of both upper and lower bounds.   Our primary upper bound is a deterministic single-pass semi-streaming algorithm (using $\\tilde{O}(n)$ space for $n$-node graphs, where $\\tilde{O}(.)$ hides polylog$(n)$ factors) for decomposing a tournament into strongly connected components (SCC). it improves upon the previously best-known algorithm by Baweja, Jia, and Woodruff [ITCS'22] in terms of both space and passes: for $p\\geq 1$, they used $(p+1)$-passes and $\\tilde{O}(n^{1+1/p})$-space. We further extend our algorithm to digraphs that are close to tournaments and establish tight bounds demonstrating that the problem's complexity grows smoothly with the \"distance\" from tournaments. Applying our framework, we obtain improved tournament algorithms for $s,t$-reachability, strong connectivity, Hamiltonian paths and cycles, and feedback arc set.   On the other hand, we prove the first $\\Omega(n^2)$-space lower bounds for this class, exhibiting that some well-studied problems -- such as (exact) feedback arc set on tournaments (FAST) and $s,t$-distance -- remain hard here. We obtain a generalized lower bound on space-approximation tradeoffs for FAST: any single-pass $(1\\pm \\varepsilon)$-approximation algorithm requires $\\Omega(n/\\sqrt{\\varepsilon})$ space. As a whole, our collection of results contributes significantly to the growing literature on streaming digraphs.","sentences":["We study fundamental directed graph (digraph) problems in the streaming model.","An initial investigation by Chakrabarti, Ghosh, McGregor, and Vorotnikova [SODA'20] on streaming digraphs showed that while most of these problems are provably hard in general, some of them become tractable when restricted to the well-studied class of tournament graphs where every pair of nodes shares exactly one directed edge.","Thus, we focus on tournaments and improve the state of the art for multiple problems in terms of both upper and lower bounds.   ","Our primary upper bound is a deterministic single-pass semi-streaming algorithm (using $\\tilde{O}(n)$ space for $n$-node graphs, where $\\tilde{O}(.)$ hides polylog$(n)$ factors) for decomposing a tournament into strongly connected components (SCC).","it improves upon the previously best-known algorithm by Baweja, Jia, and Woodruff","[ITCS'22] in terms of both space and passes: for $p\\geq 1$, they used $(p+1)$-passes and $\\tilde{O}(n^{1+1/p})$-space.","We further extend our algorithm to digraphs that are close to tournaments and establish tight bounds demonstrating that the problem's complexity grows smoothly with the \"distance\" from tournaments.","Applying our framework, we obtain improved tournament algorithms for $s,t$-reachability, strong connectivity, Hamiltonian paths and cycles, and feedback arc set.   ","On the other hand, we prove the first $\\Omega(n^2)$-space lower bounds for this class, exhibiting that some well-studied problems -- such as (exact) feedback arc set on tournaments (FAST) and $s,t$-distance -- remain hard here.","We obtain a generalized lower bound on space-approximation tradeoffs for FAST: any single-pass $(1\\pm \\varepsilon)$-approximation algorithm requires $\\Omega(n/\\sqrt{\\varepsilon})$ space.","As a whole, our collection of results contributes significantly to the growing literature on streaming digraphs."],"url":"http://arxiv.org/abs/2405.05952v1","category":"cs.DS"}
{"created":"2024-05-09 17:44:25","title":"$\\mathcal{H}_2$ optimal model reduction of linear systems with multiple quadratic outputs","abstract":"In this work, we consider the $\\mathcal{H}_2$ optimal model reduction of dynamical systems that are linear in the state equation and up to quadratic nonlinearity in the output equation. As our primary theoretical contributions, we derive gradients of the squared $\\mathcal{H}_2$ system error with respect to the reduced model quantities and, from the stationary points of these gradients, introduce Gramian-based first-order necessary conditions for the $\\mathcal{H}_2$ optimal approximation of a linear quadratic output (LQO) system. The resulting $\\mathcal{H}_2$ optimality framework neatly generalizes the analogous Gramian-based optimality framework for purely linear systems. Computationally, we show how to enforce the necessary optimality conditions using Petrov-Galerkin projection; the corresponding projection matrices are obtained from a pair of Sylvester equations. Based on this result, we propose an iteratively corrected algorithm for the $\\mathcal{H}_2$ model reduction of LQO systems, which we refer to as LQO-TSIA (linear quadratic output two-sided iteration algorithm). Numerical examples are included to illustrate the effectiveness of the proposed computational method against other existing approaches.","sentences":["In this work, we consider the $\\mathcal{H}_2$ optimal model reduction of dynamical systems that are linear in the state equation and up to quadratic nonlinearity in the output equation.","As our primary theoretical contributions, we derive gradients of the squared $\\mathcal{H}_2$ system error with respect to the reduced model quantities and, from the stationary points of these gradients, introduce Gramian-based first-order necessary conditions for the $\\mathcal{H}_2$ optimal approximation of a linear quadratic output (LQO) system.","The resulting $\\mathcal{H}_2$ optimality framework neatly generalizes the analogous Gramian-based optimality framework for purely linear systems.","Computationally, we show how to enforce the necessary optimality conditions using Petrov-Galerkin projection; the corresponding projection matrices are obtained from a pair of Sylvester equations.","Based on this result, we propose an iteratively corrected algorithm for the $\\mathcal{H}_2$ model reduction of LQO systems, which we refer to as LQO-TSIA (linear quadratic output two-sided iteration algorithm).","Numerical examples are included to illustrate the effectiveness of the proposed computational method against other existing approaches."],"url":"http://arxiv.org/abs/2405.05951v1","category":"math.NA"}
{"created":"2024-05-09 17:40:09","title":"Federated Combinatorial Multi-Agent Multi-Armed Bandits","abstract":"This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback. In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals. Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of $\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$, into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents. Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees. We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios.","sentences":["This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback.","In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals.","Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of $\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$, into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents.","Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$.","Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees.","We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios."],"url":"http://arxiv.org/abs/2405.05950v1","category":"cs.LG"}
{"created":"2024-05-09 17:37:02","title":"Uniformly global observables for 1D maps with an indifferent fixed point","abstract":"We study the property of global-local mixing for full-branched expanding maps of either the half-line or the interval, with one indifferent fixed point. Global-local mixing expresses the decorrelation of global vs local observables w.r.t. to an infinite measure $\\mu$. Global observables are essentially bounded functions admitting an infinite-volume average, i.e., a limit for the average of the function over bigger and bigger intervals; local observables are integrable functions (both notions are relative to $\\mu$). Of course, the definition of global observable depends on the exact definition of infinite-volume average. The first choice for it would be to consider averages over the entire space minus a neighborhood of the indifferent fixed point (a.k.a. the \"point at infinity\"), in the limit where such neighborhood vanishes. This is the choice that was made in previous papers on the subject. The classes of systems for which global-local mixing was proved, with this natural choice of global observables, are ample but not really general. In this paper we consider uniformly global observables, i.e., $L^\\infty$ functions whose averages over any interval $V$ converges to a limit, uniformly as $\\mu(V) \\to \\infty$. Uniformly global observables form quite an extensive subclass of all global observables. We prove global-local mixing in the sense of uniformly global observables, for two truly general classes of expanding maps with one indifferent fixed point, respectively on $\\mathbb{R}_0^+$ and on $(0,1]$. The technical core of the proofs is rather different from previous work.","sentences":["We study the property of global-local mixing for full-branched expanding maps of either the half-line or the interval, with one indifferent fixed point.","Global-local mixing expresses the decorrelation of global vs local observables w.r.t.","to an infinite measure $\\mu$. Global observables are essentially bounded functions admitting an infinite-volume average, i.e., a limit for the average of the function over bigger and bigger intervals; local observables are integrable functions (both notions are relative to $\\mu$).","Of course, the definition of global observable depends on the exact definition of infinite-volume average.","The first choice for it would be to consider averages over the entire space minus a neighborhood of the indifferent fixed point (a.k.a. the \"point at infinity\"), in the limit where such neighborhood vanishes.","This is the choice that was made in previous papers on the subject.","The classes of systems for which global-local mixing was proved, with this natural choice of global observables, are ample but not really general.","In this paper we consider uniformly global observables, i.e., $L^\\infty$ functions whose averages over any interval $V$ converges to a limit, uniformly as $\\mu(V) \\to \\infty$. Uniformly global observables form quite an extensive subclass of all global observables.","We prove global-local mixing in the sense of uniformly global observables, for two truly general classes of expanding maps with one indifferent fixed point, respectively on $\\mathbb{R}_0^+$ and on $(0,1]$. The technical core of the proofs is rather different from previous work."],"url":"http://arxiv.org/abs/2405.05948v1","category":"math.DS"}
{"created":"2024-05-09 17:35:49","title":"Motion from Measurement: The Role of Symmetry of Quantum Measurements","abstract":"In quantum mechanics, measurements are dynamical processes and thus they should be capable of inducing currents. The symmetries of the Hamiltonian and measurement operator provide an organizing principle for understanding the conditions for such currents to emerge. The central role is played by the inversion and time-reversal symmetries. We classify the distinct behaviors that emerge from single and repeated measurements, with and without coupling to a dissipative bath. While the breaking of inversion symmetry alone is sufficient to generate currents through measurements, the breaking of time-reversal symmetry by the measurement operator leads to a dramatic increase in the magnitude of the currents. We consider the dependence on the measurement rate and find that the current is non-monotonic. Furthermore, nondegenerate measurements can lead to current loops within the steady state even in the Zeno limit.","sentences":["In quantum mechanics, measurements are dynamical processes and thus they should be capable of inducing currents.","The symmetries of the Hamiltonian and measurement operator provide an organizing principle for understanding the conditions for such currents to emerge.","The central role is played by the inversion and time-reversal symmetries.","We classify the distinct behaviors that emerge from single and repeated measurements, with and without coupling to a dissipative bath.","While the breaking of inversion symmetry alone is sufficient to generate currents through measurements, the breaking of time-reversal symmetry by the measurement operator leads to a dramatic increase in the magnitude of the currents.","We consider the dependence on the measurement rate and find that the current is non-monotonic.","Furthermore, nondegenerate measurements can lead to current loops within the steady state even in the Zeno limit."],"url":"http://arxiv.org/abs/2405.05946v1","category":"quant-ph"}
{"created":"2024-05-09 17:35:16","title":"Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers","abstract":"Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference. Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community.","sentences":["Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details.","In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions.","By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions.","This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference.","Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens.","This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model.","Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions.","We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community."],"url":"http://arxiv.org/abs/2405.05945v1","category":"cs.CV"}
{"created":"2024-05-09 17:32:50","title":"Quantitative fluid approximation in fractional regimes of transport equations with more invariants","abstract":"We present an extension of results in a previous paper by the first and the last author [PMP, 2022] about macroscopic limits of linear kinetic equations in (potentially) fractional regimes. More precisely, we develop a unified framework inspired by Ellis and Pinsky [J. Math. Pures Appl., 1975] for operators that preserve mass, momentum and energy, and have microscopic equilibrium with heavy tails (typically polynomial). This paper also generalizes one of Hittmeir and Merino [KRM, 2016] in a related framework. The main difficulty, that leads to our main contribution, is the understanding of the spectrum of the generator in the Fourier space, which is significantly complicated by the lack of spectral gap and the fat tails of the equilibrium. Indeed, the scaling of the eigenelements in the suitable macroscopic rescaling is subtle to handle. In particular, our study uncovered an interesting difference in scaling in the fractional regime, where the transversal wave eigenvalues converge faster to zero than the Boussinesq and acoustic wave eigenvalues.","sentences":["We present an extension of results in a previous paper by the first and the last author","[PMP, 2022] about macroscopic limits of linear kinetic equations in (potentially) fractional regimes.","More precisely, we develop a unified framework inspired by Ellis and Pinsky","[J. Math.","Pures Appl., 1975] for operators that preserve mass, momentum and energy, and have microscopic equilibrium with heavy tails (typically polynomial).","This paper also generalizes one of Hittmeir and Merino [KRM, 2016] in a related framework.","The main difficulty, that leads to our main contribution, is the understanding of the spectrum of the generator in the Fourier space, which is significantly complicated by the lack of spectral gap and the fat tails of the equilibrium.","Indeed, the scaling of the eigenelements in the suitable macroscopic rescaling is subtle to handle.","In particular, our study uncovered an interesting difference in scaling in the fractional regime, where the transversal wave eigenvalues converge faster to zero than the Boussinesq and acoustic wave eigenvalues."],"url":"http://arxiv.org/abs/2405.05943v1","category":"math.AP"}
{"created":"2024-05-09 17:30:16","title":"Evaluating Real-World Robot Manipulation Policies in Simulation","abstract":"The field of robotics has made significant advances towards generalist robot manipulation policies. However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform. We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments. We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups. Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world. Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts. We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks.","sentences":["The field of robotics has made significant advances towards generalist robot manipulation policies.","However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform.","We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments.","We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups.","Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world.","Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts.","We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks."],"url":"http://arxiv.org/abs/2405.05941v1","category":"cs.RO"}
{"created":"2024-05-09 17:29:50","title":"Generalized Campanato Space Over Non-homogeneous Space and Its Applications","abstract":"The authors introduce generalized Campanato space with regularized condition over non-homogeneous space, and study its basic properties including the John-Nirenberg inequality and equivalent characterizations. As applications, the boundedness of fractional type Marcinkiewicz integral operator and its commutator on generalized Morrey space over non-homogeneous space is obtained.","sentences":["The authors introduce generalized Campanato space with regularized condition over non-homogeneous space, and study its basic properties including the John-Nirenberg inequality and equivalent characterizations.","As applications, the boundedness of fractional type Marcinkiewicz integral operator and its commutator on generalized Morrey space over non-homogeneous space is obtained."],"url":"http://arxiv.org/abs/2405.05940v1","category":"math.FA"}
{"created":"2024-05-09 17:26:41","title":"Bounded Generation of Submonoids of Heisenberg Groups","abstract":"If $G$ is a nilpotent group and $[G,G]$ has Hirsch length $1$, then every f.g. submonoid of $G$ is boundedly generated, i.e. a product of cyclic submonoids. Using a reduction of Bodart, this implies the decidability of the submonoid membership problem for nilpotent groups $G$ where $[G,G]$ has Hirsch length $2$.","sentences":["If $G$ is a nilpotent group and $[G,G]$ has Hirsch length $1$, then every f.g. submonoid of $G$ is boundedly generated, i.e. a product of cyclic submonoids.","Using a reduction of Bodart, this implies the decidability of the submonoid membership problem for nilpotent groups $G$ where $[G,G]$","has Hirsch length $2$."],"url":"http://arxiv.org/abs/2405.05939v1","category":"math.GR"}
{"created":"2024-05-09 17:25:31","title":"DOLOMITES: Domain-Specific Long-Form Methodical Tasks","abstract":"Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work. From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input. We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields. Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task. We use these examples to evaluate contemporary language models highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge.","sentences":["Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work.","From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input.","We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields.","Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task.","We use these examples to evaluate contemporary language models highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge."],"url":"http://arxiv.org/abs/2405.05938v1","category":"cs.CL"}
{"created":"2024-05-09 17:21:17","title":"Scalar Perturbations in Nonsingular Universes from Interacting Vacuum","abstract":"In this paper we examine the stability of scalar perturbations in nonsingular models which emerge from an interacting vacuum component. The analysis developed in this paper relies on two phenomenological choices for the energy exchange between a nonrelativistic fluid and a vacuum component. In both scenarios it can be shown that closed models may furnish nonsingular orbits of physical interest in phase space once a decelerated past era is connected to a graceful exit to late-time acceleration. Regarding such configurations as background spacetimes we introduce scalar perturbations in order to examine the stability of these models in a high energy domain. We explicitly show that the vacuum perturbation is not an independent variable and diverges as dynamics approaches the bounce. This feature assigns a rather unstable signature to the dynamics making the choices for the energy transfer ill defined at least for nonsingular configurations at the bounce scale.","sentences":["In this paper we examine the stability of scalar perturbations in nonsingular models which emerge from an interacting vacuum component.","The analysis developed in this paper relies on two phenomenological choices for the energy exchange between a nonrelativistic fluid and a vacuum component.","In both scenarios it can be shown that closed models may furnish nonsingular orbits of physical interest in phase space once a decelerated past era is connected to a graceful exit to late-time acceleration.","Regarding such configurations as background spacetimes we introduce scalar perturbations in order to examine the stability of these models in a high energy domain.","We explicitly show that the vacuum perturbation is not an independent variable and diverges as dynamics approaches the bounce.","This feature assigns a rather unstable signature to the dynamics making the choices for the energy transfer ill defined at least for nonsingular configurations at the bounce scale."],"url":"http://arxiv.org/abs/2405.05936v1","category":"gr-qc"}
{"created":"2024-05-09 17:16:36","title":"Non-symplectic automorphisms of prime order of O'Grady's tenfolds and cubic fourfolds","abstract":"We give a lattice-theoretic classification of non-symplectic automorphisms of prime order of irreducible holomorphic symplectic manifolds of OG10 type. We determine which automorphisms are induced by a non-symplectic automorphism of prime order of a cubic fourfold on the associated LSV manifolds, giving a geometric and lattice-theoretic description of the algebraic and transcendental lattices of the cubic fourfold. As an application we discuss the rationality conjecture for a general cubic fourfold with a non-symplectic automorphism of prime order.","sentences":["We give a lattice-theoretic classification of non-symplectic automorphisms of prime order of irreducible holomorphic symplectic manifolds of OG10 type.","We determine which automorphisms are induced by a non-symplectic automorphism of prime order of a cubic fourfold on the associated LSV manifolds, giving a geometric and lattice-theoretic description of the algebraic and transcendental lattices of the cubic fourfold.","As an application we discuss the rationality conjecture for a general cubic fourfold with a non-symplectic automorphism of prime order."],"url":"http://arxiv.org/abs/2405.05932v1","category":"math.AG"}
{"created":"2024-05-09 17:16:20","title":"Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness","abstract":"AI-generated content (AIGC) models, represented by large language models (LLM), have brought revolutionary changes to the content generation fields. The high-speed and extensive 6G technology is an ideal platform for providing powerful AIGC mobile service applications, while future 6G mobile networks also need to support intelligent and personalized mobile generation services. However, the significant ethical and security issues of current AIGC models, such as adversarial attacks, privacy, and fairness, greatly affect the credibility of 6G intelligent networks, especially in ensuring secure, private, and fair AIGC applications. In this paper, we propose TrustGAIN, a novel paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale AIGC services in future 6G networks. We first discuss the adversarial attacks and privacy threats faced by AIGC systems in 6G networks, as well as the corresponding protection issues. Subsequently, we emphasize the importance of ensuring the unbiasedness and fairness of the mobile generative service in future intelligent networks. In particular, we conduct a use case to demonstrate that TrustGAIN can effectively guide the resistance against malicious or generated false information. We believe that TrustGAIN is a necessary paradigm for intelligent and trustworthy 6G networks to support AIGC services, ensuring the security, privacy, and fairness of AIGC network services.","sentences":["AI-generated content (AIGC) models, represented by large language models (LLM), have brought revolutionary changes to the content generation fields.","The high-speed and extensive 6G technology is an ideal platform for providing powerful AIGC mobile service applications, while future 6G mobile networks also need to support intelligent and personalized mobile generation services.","However, the significant ethical and security issues of current AIGC models, such as adversarial attacks, privacy, and fairness, greatly affect the credibility of 6G intelligent networks, especially in ensuring secure, private, and fair AIGC applications.","In this paper, we propose TrustGAIN, a novel paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale AIGC services in future 6G networks.","We first discuss the adversarial attacks and privacy threats faced by AIGC systems in 6G networks, as well as the corresponding protection issues.","Subsequently, we emphasize the importance of ensuring the unbiasedness and fairness of the mobile generative service in future intelligent networks.","In particular, we conduct a use case to demonstrate that TrustGAIN can effectively guide the resistance against malicious or generated false information.","We believe that TrustGAIN is a necessary paradigm for intelligent and trustworthy 6G networks to support AIGC services, ensuring the security, privacy, and fairness of AIGC network services."],"url":"http://arxiv.org/abs/2405.05930v1","category":"cs.CR"}
{"created":"2024-05-09 17:15:24","title":"On the Effect of Geometry on Scaling Laws for a Class of Martensitic Phase Transformations","abstract":"We study scaling laws for singular perturbation problems associated with a class of two-dimensional martensitic phase transformations and deduce a domain dependence of the scaling law in the singular perturbation parameter. In these settings the respective scaling laws give rise to a selection principle for specific, highly symmetric domain geometries for the associated nucleation microstructure. More precisely, firstly, we prove a general lower bound estimate illustrating that in settings in which the domain and well geometry are incompatible in the sense of the Hadamard-jump condition, then necessarily at least logarithmic losses in the singular perturbation parameter occur in the associated scaling laws. Secondly, for specific phase transformations in two-dimensional settings we prove that this gives rise to a dichotomy involving logarithmic losses in the scaling law for generic domains and optimal linear scaling laws for very specific, highly compatible polygonal domains. In these situations the scaling law thus gives important insight into optimal isoperimetric domains. We discuss both the geometrically linearized and nonlinear settings.","sentences":["We study scaling laws for singular perturbation problems associated with a class of two-dimensional martensitic phase transformations and deduce a domain dependence of the scaling law in the singular perturbation parameter.","In these settings the respective scaling laws give rise to a selection principle for specific, highly symmetric domain geometries for the associated nucleation microstructure.","More precisely, firstly, we prove a general lower bound estimate illustrating that in settings in which the domain and well geometry are incompatible in the sense of the Hadamard-jump condition, then necessarily at least logarithmic losses in the singular perturbation parameter occur in the associated scaling laws.","Secondly, for specific phase transformations in two-dimensional settings we prove that this gives rise to a dichotomy involving logarithmic losses in the scaling law for generic domains and optimal linear scaling laws for very specific, highly compatible polygonal domains.","In these situations the scaling law thus gives important insight into optimal isoperimetric domains.","We discuss both the geometrically linearized and nonlinear settings."],"url":"http://arxiv.org/abs/2405.05927v1","category":"math.AP"}
{"created":"2024-05-09 17:15:09","title":"FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting","abstract":"Ensemble weather forecasting is essential for weather predictions and mitigating the impacts of extreme weather events. Constructing an ensemble prediction system (EPS) based on conventional numerical weather prediction (NWP) models is highly computationally expensive. Machine learning (ML) models have emerged as valuable tools for deterministic weather forecasts, providing forecasts with significantly reduced computational requirements and even surpassing the forecast performance of traditional NWP models. However, challenges arise when applying ML models to ensemble forecasting. Recent ML models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data Assimilations (EDA) or two operational NWP ensemble members for forecast generation. The spatial resolution of 1{\\deg} or 2{\\deg} in these models is often considered too coarse for many applications. To overcome these limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global ensemble weather forecasts up to 15 days. This model runs at a significantly improved spatial resolution of 0.25{\\deg}, incorporating 5 upper-air atmospheric variables at 13 pressure levels, along with 13 surface variables. By leveraging the inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the continuous ranked probability score (CRPS) and the KL divergence between the predicted and target distribution. This innovative approach represents an advancement over the traditional use of L1 loss combined with the KL loss in standard VAE models when VAE for ensemble weather forecasts. Evaluation results demonstrate that FuXi-ENS outperforms ensemble forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model, on 98.1% of 360 variable and forecast lead time combinations on CRPS.","sentences":["Ensemble weather forecasting is essential for weather predictions and mitigating the impacts of extreme weather events.","Constructing an ensemble prediction system (EPS) based on conventional numerical weather prediction (NWP) models is highly computationally expensive.","Machine learning (ML) models have emerged as valuable tools for deterministic weather forecasts, providing forecasts with significantly reduced computational requirements and even surpassing the forecast performance of traditional NWP models.","However, challenges arise when applying ML models to ensemble forecasting.","Recent ML models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data Assimilations (EDA) or two operational NWP ensemble members for forecast generation.","The spatial resolution of 1{\\deg} or 2{\\deg} in these models is often considered too coarse for many applications.","To overcome these limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global ensemble weather forecasts up to 15 days.","This model runs at a significantly improved spatial resolution of 0.25{\\deg}, incorporating 5 upper-air atmospheric variables at 13 pressure levels, along with 13 surface variables.","By leveraging the inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the continuous ranked probability score (CRPS) and the KL divergence between the predicted and target distribution.","This innovative approach represents an advancement over the traditional use of L1 loss combined with the KL loss in standard VAE models when VAE for ensemble weather forecasts.","Evaluation results demonstrate that FuXi-ENS outperforms ensemble forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model, on 98.1% of 360 variable and forecast lead time combinations on CRPS."],"url":"http://arxiv.org/abs/2405.05925v1","category":"cs.LG"}
{"created":"2024-05-09 17:11:00","title":"Generalized R\u00e9nyi entropy accumulation theorem and generalized quantum probability estimation","abstract":"The entropy accumulation theorem, and its subsequent generalized version, is a powerful tool in the security analysis of many device-dependent and device-independent cryptography protocols. However, it has the drawback that the finite-size bounds it yields are not necessarily optimal, and furthermore it relies on the construction of an affine min-tradeoff function, which can often be challenging to construct optimally in practice. In this work, we address both of these challenges simultaneously by deriving a new entropy accumulation bound. Our bound yields significantly better finite-size performance, and can be computed as an intuitively interpretable convex optimization, without any specification of affine min-tradeoff functions. Furthermore, it can be applied directly at the level of R\\'enyi entropies if desired, yielding fully-R\\'enyi security proofs. Our proof techniques are based on elaborating on a connection between entropy accumulation and the frameworks of quantum probability estimation or $f$-weighted R\\'enyi entropies, and in the process we obtain some new results with respect to those frameworks as well.","sentences":["The entropy accumulation theorem, and its subsequent generalized version, is a powerful tool in the security analysis of many device-dependent and device-independent cryptography protocols.","However, it has the drawback that the finite-size bounds it yields are not necessarily optimal, and furthermore it relies on the construction of an affine min-tradeoff function, which can often be challenging to construct optimally in practice.","In this work, we address both of these challenges simultaneously by deriving a new entropy accumulation bound.","Our bound yields significantly better finite-size performance, and can be computed as an intuitively interpretable convex optimization, without any specification of affine min-tradeoff functions.","Furthermore, it can be applied directly at the level of R\\'enyi entropies if desired, yielding fully-R\\'enyi security proofs.","Our proof techniques are based on elaborating on a connection between entropy accumulation and the frameworks of quantum probability estimation or $f$-weighted R\\'enyi entropies, and in the process we obtain some new results with respect to those frameworks as well."],"url":"http://arxiv.org/abs/2405.05912v1","category":"quant-ph"}
{"created":"2024-05-09 17:10:56","title":"Small-Scale Testbed for Evaluating C-V2X Applications on 5G Cellular Networks","abstract":"In this work, we present a small-scale testbed for evaluating the real-life performance of cellular V2X (C-V2X) applications on 5G cellular networks. Despite the growing interest and rapid technology development for V2X applications, researchers still struggle to prototype V2X applications with real wireless networks, hardware, and software in the loop in a controlled environment. To help alleviate this challenge, we present a testbed designed to accelerate development and evaluation of C-V2X applications on 5G cellular networks. By including a small-scale vehicle platform into the testbed design, we significantly reduce the time and effort required to test new C-V2X applications on 5G cellular networks. With a focus around the integration of small-scale vehicle platforms, we detail the design decisions behind the full software and hardware setup of commonly needed intelligent transport system agents (e.g. sensors, servers, vehicles). Moreover, to showcase the testbed's capability to produce industrially-relevant, real world performance evaluations, we present an evaluation of a simple test case inspired from shared situational awareness. Finally, we discuss the upcoming use of the testbed for evaluating 5G cellular network-based shared situational awareness and other C-V2X applications.","sentences":["In this work, we present a small-scale testbed for evaluating the real-life performance of cellular V2X (C-V2X) applications on 5G cellular networks.","Despite the growing interest and rapid technology development for V2X applications, researchers still struggle to prototype V2X applications with real wireless networks, hardware, and software in the loop in a controlled environment.","To help alleviate this challenge, we present a testbed designed to accelerate development and evaluation of C-V2X applications on 5G cellular networks.","By including a small-scale vehicle platform into the testbed design, we significantly reduce the time and effort required to test new C-V2X applications on 5G cellular networks.","With a focus around the integration of small-scale vehicle platforms, we detail the design decisions behind the full software and hardware setup of commonly needed intelligent transport system agents (e.g. sensors, servers, vehicles).","Moreover, to showcase the testbed's capability to produce industrially-relevant, real world performance evaluations, we present an evaluation of a simple test case inspired from shared situational awareness.","Finally, we discuss the upcoming use of the testbed for evaluating 5G cellular network-based shared situational awareness and other C-V2X applications."],"url":"http://arxiv.org/abs/2405.05911v1","category":"eess.SY"}
{"created":"2024-05-09 17:06:51","title":"Diag2Diag: Multi modal super resolution for physics discovery with application to fusion","abstract":"This paper introduces a groundbreaking multi-modal neural network model designed for resolution enhancement, which innovatively leverages inter-diagnostic correlations within a system. Traditional approaches have primarily focused on uni-modal enhancement strategies, such as pixel-based image enhancement or heuristic signal interpolation. In contrast, our model employs a novel methodology by harnessing the diagnostic relationships within the physics of fusion plasma. Initially, we establish the correlation among diagnostics within the tokamak. Subsequently, we utilize these correlations to substantially enhance the temporal resolution of the Thomson Scattering diagnostic, which assesses plasma density and temperature. By increasing its resolution from conventional 200Hz to 500kHz, we facilitate a new level of insight into plasma behavior, previously attainable only through computationally intensive simulations. This enhancement goes beyond simple interpolation, offering novel perspectives on the underlying physical phenomena governing plasma dynamics.","sentences":["This paper introduces a groundbreaking multi-modal neural network model designed for resolution enhancement, which innovatively leverages inter-diagnostic correlations within a system.","Traditional approaches have primarily focused on uni-modal enhancement strategies, such as pixel-based image enhancement or heuristic signal interpolation.","In contrast, our model employs a novel methodology by harnessing the diagnostic relationships within the physics of fusion plasma.","Initially, we establish the correlation among diagnostics within the tokamak.","Subsequently, we utilize these correlations to substantially enhance the temporal resolution of the Thomson Scattering diagnostic, which assesses plasma density and temperature.","By increasing its resolution from conventional 200Hz to 500kHz, we facilitate a new level of insight into plasma behavior, previously attainable only through computationally intensive simulations.","This enhancement goes beyond simple interpolation, offering novel perspectives on the underlying physical phenomena governing plasma dynamics."],"url":"http://arxiv.org/abs/2405.05908v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 17:03:04","title":"On the Ground State Energies of Discrete and Semiclassical Schr\u00f6dinger Operators","abstract":"We study the infimum of the spectrum, or ground state energy, of a discrete Schr\\\"odinger operator on $\\theta\\mathbb{Z}^d$ parameterized by a potential $V:\\mathbb{R}^d\\rightarrow\\mathbb{R}_{\\ge 0}$ and a frequency parameter $\\theta\\in (0,1)$. We prove a general comparison result relating this ground state energy to that of a corresponding semiclassical Schr\\\"odinger operator on $\\mathbb{R}^d$ with parameter $\\theta$, arising from the same choice of potential. We show that for smooth periodic potentials these ground state energies are multiplicatively related by a factor approaching $1$ as $\\theta\\rightarrow 0$, and that they are multiplicatively related by a dimension-dependent constant for a more general class of bounded potentials and all irrational $\\theta$.","sentences":["We study the infimum of the spectrum, or ground state energy, of a discrete Schr\\\"odinger operator on $\\theta\\mathbb{Z}^d$ parameterized by a potential $V:\\mathbb{R}^d\\rightarrow\\mathbb{R}_{\\ge 0}$ and a frequency parameter $\\theta\\in (0,1)$. We prove a general comparison result relating this ground state energy to that of a corresponding semiclassical Schr\\\"odinger operator on $\\mathbb{R}^d$ with parameter $\\theta$, arising from the same choice of potential.","We show that for smooth periodic potentials these ground state energies are multiplicatively related by a factor approaching $1$ as $\\theta\\rightarrow 0$, and that they are multiplicatively related by a dimension-dependent constant for a more general class of bounded potentials and all irrational $\\theta$."],"url":"http://arxiv.org/abs/2405.05907v1","category":"math.SP"}
{"created":"2024-05-09 17:02:06","title":"Deep Multi-Task Learning for Malware Image Classification","abstract":"Malicious software is a pernicious global problem. A novel multi-task learning framework is proposed in this paper for malware image classification for accurate and fast malware detection. We generate bitmap (BMP) and (PNG) images from malware features, which we feed to a deep learning classifier. Our state-of-the-art multi-task learning approach has been tested on a new dataset, for which we have collected approximately 100,000 benign and malicious PE, APK, Mach-o, and ELF examples. Experiments with seven tasks tested with 4 activation functions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLU gives the highest accuracy of more than 99.87% on all tasks. Our model can effectively detect a variety of obfuscation methods like packing, encryption, and instruction overlapping, strengthing the beneficial claims of our model, in addition to achieving the state-of-art methods in terms of accuracy.","sentences":["Malicious software is a pernicious global problem.","A novel multi-task learning framework is proposed in this paper for malware image classification for accurate and fast malware detection.","We generate bitmap (BMP) and (PNG) images from malware features, which we feed to a deep learning classifier.","Our state-of-the-art multi-task learning approach has been tested on a new dataset, for which we have collected approximately 100,000 benign and malicious PE, APK, Mach-o, and ELF examples.","Experiments with seven tasks tested with 4 activation functions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLU gives the highest accuracy of more than 99.87% on all tasks.","Our model can effectively detect a variety of obfuscation methods like packing, encryption, and instruction overlapping, strengthing the beneficial claims of our model, in addition to achieving the state-of-art methods in terms of accuracy."],"url":"http://arxiv.org/abs/2405.05906v1","category":"cs.CR"}
{"created":"2024-05-09 17:01:31","title":"Truthful Aggregation of LLMs with an Application to Online Advertising","abstract":"We address the challenge of aggregating the preferences of multiple agents over LLM-generated replies to user queries, where agents might modify or exaggerate their preferences. New agents may participate for each new query, making fine-tuning LLMs on these preferences impractical. To overcome these challenges, we propose an auction mechanism that operates without fine-tuning or access to model weights. This mechanism is designed to provably converge to the ouput of the optimally fine-tuned LLM as computational resources are increased. The mechanism can also incorporate contextual information about the agents when avaiable, which significantly accelerates its convergence. A well-designed payment rule ensures that truthful reporting is the optimal strategy for all agents, while also promoting an equity property by aligning each agent's utility with her contribution to social welfare - an essential feature for the mechanism's long-term viability. While our approach can be applied whenever monetary transactions are permissible, our flagship application is in online advertising. In this context, advertisers try to steer LLM-generated responses towards their brand interests, while the platform aims to maximize advertiser value and ensure user satisfaction. Experimental results confirm that our mechanism not only converges efficiently to the optimally fine-tuned LLM but also significantly boosts advertiser value and platform revenue, all with minimal computational overhead.","sentences":["We address the challenge of aggregating the preferences of multiple agents over LLM-generated replies to user queries, where agents might modify or exaggerate their preferences.","New agents may participate for each new query, making fine-tuning LLMs on these preferences impractical.","To overcome these challenges, we propose an auction mechanism that operates without fine-tuning or access to model weights.","This mechanism is designed to provably converge to the ouput of the optimally fine-tuned LLM as computational resources are increased.","The mechanism can also incorporate contextual information about the agents when avaiable, which significantly accelerates its convergence.","A well-designed payment rule ensures that truthful reporting is the optimal strategy for all agents, while also promoting an equity property by aligning each agent's utility with her contribution to social welfare - an essential feature for the mechanism's long-term viability.","While our approach can be applied whenever monetary transactions are permissible, our flagship application is in online advertising.","In this context, advertisers try to steer LLM-generated responses towards their brand interests, while the platform aims to maximize advertiser value and ensure user satisfaction.","Experimental results confirm that our mechanism not only converges efficiently to the optimally fine-tuned LLM but also significantly boosts advertiser value and platform revenue, all with minimal computational overhead."],"url":"http://arxiv.org/abs/2405.05905v1","category":"cs.GT"}
{"created":"2024-05-09 17:00:22","title":"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?","abstract":"When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.","sentences":["When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training.","It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge.","In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge.","To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge.","We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge.","However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate.","Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently."],"url":"http://arxiv.org/abs/2405.05904v1","category":"cs.CL"}
{"created":"2024-05-09 16:59:59","title":"The Other Side of the Coin: Recipient Norms and Their Impact on Indirect Reciprocity and Cooperation","abstract":"Human cooperation depends on indirect reciprocity. In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population. In particular, we examine how updating the reputations of recipients influences cooperation. Our work adds a time-scale parameter for updating donor and recipient reputations. We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms. `Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors. Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse.","sentences":["Human cooperation depends on indirect reciprocity.","In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population.","In particular, we examine how updating the reputations of recipients influences cooperation.","Our work adds a time-scale parameter for updating donor and recipient reputations.","We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms.","`Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors.","Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse."],"url":"http://arxiv.org/abs/2405.05903v1","category":"physics.soc-ph"}
{"created":"2024-05-09 16:54:47","title":"The largest subgraph without a forbidden induced subgraph","abstract":"We initiate the systematic study of the following Tur\\'an-type question. Suppose $\\Gamma$ is a graph with $n$ vertices such that the edge density between any pair of subsets of vertices of size at least $t$ is at most $1 - c$, for some $t$ and $c > 0$. What is the largest number of edges in a subgraph $G \\subseteq \\Gamma$ which does not contain a fixed graph $H$ as an induced subgraph or, more generally, which belongs to a hereditary property $\\mathcal{P}$? This provides a common generalization of two recently studied cases, namely $\\Gamma$ being a (pseudo-)random graph and a graph without a large complete bipartite subgraph. We focus on the interesting case where $H$ is a bipartite graph.   We determine the answer up to a constant factor with respect to $n$ and $t$, for certain bipartite $H$ and for $\\Gamma$ either a dense random graph or a Paley graph with a square number of vertices. In particular, our bounds match if $H$ is a tree, or if one part of $H$ has $d$ vertices complete to the other part, all other vertices in that part have degree at most $d$, and the other part has sufficiently many vertices. As applications of the latter result, we answer a question of Alon, Krivelevich, and Samotij on the largest subgraph with a hereditary property which misses a bipartite graph, and determine up to a constant factor the largest number of edges in a string subgraph of $\\Gamma$. The proofs are based on a variant of the dependent random choice and a novel approach for finding induced copies by inductively defining probability distributions supported on induced copies of smaller subgraphs.","sentences":["We initiate the systematic study of the following Tur\\'an-type question.","Suppose $\\Gamma$ is a graph with $n$ vertices such that the edge density between any pair of subsets of vertices of size at least $t$ is at most $1 - c$, for some $t$ and $c >","0$.","What is the largest number of edges in a subgraph $G \\subseteq \\Gamma$ which does not contain a fixed graph $H$ as an induced subgraph or, more generally, which belongs to a hereditary property $\\mathcal{P}$?","This provides a common generalization of two recently studied cases, namely $\\Gamma$ being a (pseudo-)random graph and a graph without a large complete bipartite subgraph.","We focus on the interesting case where $H$ is a bipartite graph.   ","We determine the answer up to a constant factor with respect to $n$ and $t$, for certain bipartite $H$ and for $\\Gamma$ either a dense random graph or a Paley graph with a square number of vertices.","In particular, our bounds match if $H$ is a tree, or if one part of $H$ has $d$ vertices complete to the other part, all other vertices in that part have degree at most $d$, and the other part has sufficiently many vertices.","As applications of the latter result, we answer a question of Alon, Krivelevich, and Samotij on the largest subgraph with a hereditary property which misses a bipartite graph, and determine up to a constant factor the largest number of edges in a string subgraph of $\\Gamma$.","The proofs are based on a variant of the dependent random choice and a novel approach for finding induced copies by inductively defining probability distributions supported on induced copies of smaller subgraphs."],"url":"http://arxiv.org/abs/2405.05902v1","category":"math.CO"}
{"created":"2024-05-09 16:45:27","title":"Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons","abstract":"LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.","sentences":["LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion.","However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations.","This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment.","Here individual comparisons are considered experts that provide information on a pair's score difference.","The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed.","When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking.","Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used.","We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment.","When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used."],"url":"http://arxiv.org/abs/2405.05894v1","category":"cs.CL"}
{"created":"2024-05-09 16:44:23","title":"Financial knowledge and borrower discouragement","abstract":"This study provides first empirical evidence on the impact of entrepreneurs' financial knowledge on borrower discouragement. Using novel survey data on Italian micro-enterprises, we find that less financially knowledgeable entrepreneurs are more likely to be discouraged from applying for new financing, due to higher application costs and expected rejection. Our main results are robust to several sensitivity checks, including accounting for potential endogeneity. Furthermore, we show that the observed self-rationing mechanism is rather inefficient, suggesting that financial knowledge might play a key role in reducing credit market imperfections.","sentences":["This study provides first empirical evidence on the impact of entrepreneurs' financial knowledge on borrower discouragement.","Using novel survey data on Italian micro-enterprises, we find that less financially knowledgeable entrepreneurs are more likely to be discouraged from applying for new financing, due to higher application costs and expected rejection.","Our main results are robust to several sensitivity checks, including accounting for potential endogeneity.","Furthermore, we show that the observed self-rationing mechanism is rather inefficient, suggesting that financial knowledge might play a key role in reducing credit market imperfections."],"url":"http://arxiv.org/abs/2405.05891v1","category":"econ.GN"}
{"created":"2024-05-09 16:42:39","title":"Safe Exploration Using Bayesian World Models and Log-Barrier Optimization","abstract":"A major challenge in deploying reinforcement learning in online tasks is ensuring that safety is maintained throughout the learning process. In this work, we propose CERL, a new method for solving constrained Markov decision processes while keeping the policy safe during learning. Our method leverages Bayesian world models and suggests policies that are pessimistic w.r.t. the model's epistemic uncertainty. This makes CERL robust towards model inaccuracies and leads to safe exploration during learning. In our experiments, we demonstrate that CERL outperforms the current state-of-the-art in terms of safety and optimality in solving CMDPs from image observations.","sentences":["A major challenge in deploying reinforcement learning in online tasks is ensuring that safety is maintained throughout the learning process.","In this work, we propose CERL, a new method for solving constrained Markov decision processes while keeping the policy safe during learning.","Our method leverages Bayesian world models and suggests policies that are pessimistic w.r.t.","the model's epistemic uncertainty.","This makes CERL robust towards model inaccuracies and leads to safe exploration during learning.","In our experiments, we demonstrate that CERL outperforms the current state-of-the-art in terms of safety and optimality in solving CMDPs from image observations."],"url":"http://arxiv.org/abs/2405.05890v1","category":"cs.LG"}
{"created":"2024-05-09 16:40:42","title":"Perturbing Dynamics of Active Emulsions and Their Collectives","abstract":"Controlling fluidic flows in active droplets is crucial in developing intelligent models to understand and mimic single-celled microorganisms. Typically, these fluidic flows are affected by the interfacial dynamics of chemical agents. We found that these flows can be reconfigured by the mere presence of anisotropic solid boundary embedded within active droplets. Spontaneous fluidic flows dynamically orient an embedded magnetic cluster and the magnetic cluster, when realigned, causes these flows to reorient. Thus, providing an unprecedented control over the propulsion dynamics of chemotactic emulsions. When continuously perturbed, achiral emulsions exhibit emergent chiral motion with rotating fluidic flows. Such solid-fluid interactions removes barriers of specific emulsion chemistries and complements their inherent abilities thereby also enabling control over emergent collective behaviors of active droplets.","sentences":["Controlling fluidic flows in active droplets is crucial in developing intelligent models to understand and mimic single-celled microorganisms.","Typically, these fluidic flows are affected by the interfacial dynamics of chemical agents.","We found that these flows can be reconfigured by the mere presence of anisotropic solid boundary embedded within active droplets.","Spontaneous fluidic flows dynamically orient an embedded magnetic cluster and the magnetic cluster, when realigned, causes these flows to reorient.","Thus, providing an unprecedented control over the propulsion dynamics of chemotactic emulsions.","When continuously perturbed, achiral emulsions exhibit emergent chiral motion with rotating fluidic flows.","Such solid-fluid interactions removes barriers of specific emulsion chemistries and complements their inherent abilities thereby also enabling control over emergent collective behaviors of active droplets."],"url":"http://arxiv.org/abs/2405.05889v1","category":"cond-mat.soft"}
{"created":"2024-05-09 16:26:55","title":"Convergence Rates of Online Critic Value Function Approximation in Native Spaces","abstract":"In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs). Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases. The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning. The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic.","sentences":["In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs).","Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases.","The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning.","The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic."],"url":"http://arxiv.org/abs/2405.05887v1","category":"math.OC"}
{"created":"2024-05-09 16:22:24","title":"Exploiting Autoencoder's Weakness to Generate Pseudo Anomalies","abstract":"Due to the rare occurrence of anomalous events, a typical approach to anomaly detection is to train an autoencoder (AE) with normal data only so that it learns the patterns or representations of the normal training data. At test time, the trained AE is expected to well reconstruct normal but to poorly reconstruct anomalous data. However, contrary to the expectation, anomalous data is often well reconstructed as well. In order to further separate the reconstruction quality between normal and anomalous data, we propose creating pseudo anomalies from learned adaptive noise by exploiting the aforementioned weakness of AE, i.e., reconstructing anomalies too well. The generated noise is added to the normal data to create pseudo anomalies. Extensive experiments on Ped2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the effectiveness and generic applicability of our approach in improving the discriminative capability of AEs for anomaly detection.","sentences":["Due to the rare occurrence of anomalous events, a typical approach to anomaly detection is to train an autoencoder (AE) with normal data only so that it learns the patterns or representations of the normal training data.","At test time, the trained AE is expected to well reconstruct normal but to poorly reconstruct anomalous data.","However, contrary to the expectation, anomalous data is often well reconstructed as well.","In order to further separate the reconstruction quality between normal and anomalous data, we propose creating pseudo anomalies from learned adaptive noise by exploiting the aforementioned weakness of AE, i.e., reconstructing anomalies too well.","The generated noise is added to the normal data to create pseudo anomalies.","Extensive experiments on Ped2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the effectiveness and generic applicability of our approach in improving the discriminative capability of AEs for anomaly detection."],"url":"http://arxiv.org/abs/2405.05886v1","category":"cs.LG"}
{"created":"2024-05-09 16:16:34","title":"A meta inspiral-merger-ringdown consistency test of general relativity with gravitational wave signals from compact binaries","abstract":"The observation of gravitational waves from compact binary coalescences is a promising tool to test the validity of general relativity (GR) in a highly dynamical strong-field regime. There are now a variety of tests of GR performed on the observed compact binary signals. In this paper, we propose a new test of GR that compares the results of these individual tests. This meta inspiral-merger-ringdown consistency test (IMRCT) involves inferring the final mass and spin of the remnant black hole obtained from the analyses of two different tests of GR and checking for consistency. If there is a deviation from GR, we expect that different tests of GR will recover different values for the final mass and spin, in general. We check the performance of the meta IMRCT using a standard set of null tests used in various gravitational-wave analyses: the original IMRCT, parameterized phasing tests (TIGER and FTI) and the modified dispersion test. However, the meta IMRCT is applicable to any tests of GR that infer the initial masses and spins or the final mass and spin, including ones that are applied to binary neutron star or neutron star--black hole signals. We apply the meta IMRCT to simulated quasi-circular GR and non-GR binary black hole (BBH) signals as well as to eccentric BBH signals in GR (analyzed with quasicircular waveforms). We find that the meta IMRCT gives consistency with GR for the quasi-circular GR signals and picks up a deviation from GR in the other cases, as do other tests. In some cases, the meta IMRCT finds a significant GR deviation for a given pair of tests (and specific testing parameters) while the individual tests do not, showing that it is more sensitive than the individual tests to certain types of deviations. In addition, we also apply this test to a few selected real compact binary signals and find them consistent with GR.","sentences":["The observation of gravitational waves from compact binary coalescences is a promising tool to test the validity of general relativity (GR) in a highly dynamical strong-field regime.","There are now a variety of tests of GR performed on the observed compact binary signals.","In this paper, we propose a new test of GR that compares the results of these individual tests.","This meta inspiral-merger-ringdown consistency test (IMRCT) involves inferring the final mass and spin of the remnant black hole obtained from the analyses of two different tests of GR and checking for consistency.","If there is a deviation from GR, we expect that different tests of GR will recover different values for the final mass and spin, in general.","We check the performance of the meta IMRCT using a standard set of null tests used in various gravitational-wave analyses: the original IMRCT, parameterized phasing tests (TIGER and FTI) and the modified dispersion test.","However, the meta IMRCT is applicable to any tests of GR that infer the initial masses and spins or the final mass and spin, including ones that are applied to binary neutron star or neutron star--black hole signals.","We apply the meta IMRCT to simulated quasi-circular GR and non-GR binary black hole (BBH) signals as well as to eccentric BBH signals in GR (analyzed with quasicircular waveforms).","We find that the meta IMRCT gives consistency with GR for the quasi-circular GR signals and picks up a deviation from GR in the other cases, as do other tests.","In some cases, the meta IMRCT finds a significant GR deviation for a given pair of tests (and specific testing parameters) while the individual tests do not, showing that it is more sensitive than the individual tests to certain types of deviations.","In addition, we also apply this test to a few selected real compact binary signals and find them consistent with GR."],"url":"http://arxiv.org/abs/2405.05884v1","category":"gr-qc"}
{"created":"2024-05-09 16:16:14","title":"supDQN: Supervised Rewarding Strategy Driven Deep Q-Network for sEMG Signal Decontamination","abstract":"The presence of muscles throughout the active parts of the body such as the upper and lower limbs, makes electromyography-based human-machine interaction prevalent. However, muscle signals are stochastic and noisy. These noises can be regular and irregular. Irregular noises due to movements or electrical switching require dynamic filtering. Conventionally, filters are stacked, which trims and delays the signal unnecessarily. This study introduces a decontamination technique involving a supervised rewarding strategy to drive a deep Q-network-based agent (supDQN). It applies one of three filters to decontaminate a 1sec long surface electromyography signal, which is dynamically contaminated. A machine learning agent identifies whether the signal after filtering is clean or noisy. Accordingly, a reward is generated. The identification accuracy is enhanced by using a local interpretable model-agnostic explanation. The deep Q-network is guided by this reward to select filter optimally while decontaminating a signal. The proposed filtering strategy is tested on four noise levels (-5 dB, -1 dB, +1 dB, +5 dB). supDQN filters the signal desirably when the signal-to-noise ratio (SNR) is between -5 dB to +1 dB. It filters less desirably at high SNR (+5 dB). A normalized root mean square (nRMSE) is formulated to depict the difference of filtered signal from ground truth. This is used to compare supDQN and conventional methods including wavelet denoising with debauchies and symlet wavelet, high order low pass filter, notch filter, and high pass filter. The proposed filtering strategy gives an average value nRMSE of 1.1974, which is lower than the conventional filters.","sentences":["The presence of muscles throughout the active parts of the body such as the upper and lower limbs, makes electromyography-based human-machine interaction prevalent.","However, muscle signals are stochastic and noisy.","These noises can be regular and irregular.","Irregular noises due to movements or electrical switching require dynamic filtering.","Conventionally, filters are stacked, which trims and delays the signal unnecessarily.","This study introduces a decontamination technique involving a supervised rewarding strategy to drive a deep Q-network-based agent (supDQN).","It applies one of three filters to decontaminate a 1sec long surface electromyography signal, which is dynamically contaminated.","A machine learning agent identifies whether the signal after filtering is clean or noisy.","Accordingly, a reward is generated.","The identification accuracy is enhanced by using a local interpretable model-agnostic explanation.","The deep Q-network is guided by this reward to select filter optimally while decontaminating a signal.","The proposed filtering strategy is tested on four noise levels (-5 dB, -1 dB, +1 dB, +5 dB).","supDQN filters the signal desirably when the signal-to-noise ratio (SNR) is between -5 dB to +1 dB. It filters less desirably at high SNR (+5 dB).","A normalized root mean square (nRMSE) is formulated to depict the difference of filtered signal from ground truth.","This is used to compare supDQN and conventional methods including wavelet denoising with debauchies and symlet wavelet, high order low pass filter, notch filter, and high pass filter.","The proposed filtering strategy gives an average value nRMSE of 1.1974, which is lower than the conventional filters."],"url":"http://arxiv.org/abs/2405.05883v1","category":"eess.SP"}
{"created":"2024-05-09 16:04:14","title":"Composable Part-Based Manipulation","abstract":"In this paper, we propose composable part-based manipulation (CPM), a novel approach that leverages object-part decomposition and part-part correspondences to improve learning and generalization of robotic manipulation skills. By considering the functional correspondences between object parts, we conceptualize functional actions, such as pouring and constrained placing, as combinations of different correspondence constraints. CPM comprises a collection of composable diffusion models, where each model captures a different inter-object correspondence. These diffusion models can generate parameters for manipulation skills based on the specific object parts. Leveraging part-based correspondences coupled with the task decomposition into distinct constraints enables strong generalization to novel objects and object categories. We validate our approach in both simulated and real-world scenarios, demonstrating its effectiveness in achieving robust and generalized manipulation capabilities.","sentences":["In this paper, we propose composable part-based manipulation (CPM), a novel approach that leverages object-part decomposition and part-part correspondences to improve learning and generalization of robotic manipulation skills.","By considering the functional correspondences between object parts, we conceptualize functional actions, such as pouring and constrained placing, as combinations of different correspondence constraints.","CPM comprises a collection of composable diffusion models, where each model captures a different inter-object correspondence.","These diffusion models can generate parameters for manipulation skills based on the specific object parts.","Leveraging part-based correspondences coupled with the task decomposition into distinct constraints enables strong generalization to novel objects and object categories.","We validate our approach in both simulated and real-world scenarios, demonstrating its effectiveness in achieving robust and generalized manipulation capabilities."],"url":"http://arxiv.org/abs/2405.05876v1","category":"cs.RO"}
{"created":"2024-05-09 16:03:41","title":"A Genetic Approach to Minimising Gate and Qubit Teleportations for Multi-Processor Quantum Circuit Distribution","abstract":"Distributed Quantum Computing (DQC) provides a means for scaling available quantum computation by interconnecting multiple quantum processor units (QPUs). A key challenge in this domain is efficiently allocating logical qubits from quantum circuits to the physical qubits within QPUs, a task known to be NP-hard. Traditional approaches, primarily focused on graph partitioning strategies, have sought to reduce the number of required Bell pairs for executing non-local CNOT operations, a form of gate teleportation. However, these methods have limitations in terms of efficiency and scalability. Addressing this, our work jointly considers gate and qubit teleportations introducing a novel meta-heuristic algorithm to minimise the network cost of executing a quantum circuit. By allowing dynamic reallocation of qubits along with gate teleportations during circuit execution, our method significantly enhances the overall efficacy and potential scalability of DQC frameworks. In our numerical analysis, we demonstrate that integrating qubit teleportations into our genetic algorithm for optimising circuit blocking reduces the required resources, specifically the number of EPR pairs, compared to traditional graph partitioning methods. Our results, derived from both benchmark and randomly generated circuits, show that as circuit complexity increases - demanding more qubit teleportations - our approach effectively optimises these teleportations throughout the execution, thereby enhancing performance through strategic circuit partitioning. This is a step forward in the pursuit of a global quantum compiler which will ultimately enable the efficient use of a 'quantum data center' in the future.","sentences":["Distributed Quantum Computing (DQC) provides a means for scaling available quantum computation by interconnecting multiple quantum processor units (QPUs).","A key challenge in this domain is efficiently allocating logical qubits from quantum circuits to the physical qubits within QPUs, a task known to be NP-hard.","Traditional approaches, primarily focused on graph partitioning strategies, have sought to reduce the number of required Bell pairs for executing non-local CNOT operations, a form of gate teleportation.","However, these methods have limitations in terms of efficiency and scalability.","Addressing this, our work jointly considers gate and qubit teleportations introducing a novel meta-heuristic algorithm to minimise the network cost of executing a quantum circuit.","By allowing dynamic reallocation of qubits along with gate teleportations during circuit execution, our method significantly enhances the overall efficacy and potential scalability of DQC frameworks.","In our numerical analysis, we demonstrate that integrating qubit teleportations into our genetic algorithm for optimising circuit blocking reduces the required resources, specifically the number of EPR pairs, compared to traditional graph partitioning methods.","Our results, derived from both benchmark and randomly generated circuits, show that as circuit complexity increases - demanding more qubit teleportations - our approach effectively optimises these teleportations throughout the execution, thereby enhancing performance through strategic circuit partitioning.","This is a step forward in the pursuit of a global quantum compiler which will ultimately enable the efficient use of a 'quantum data center' in the future."],"url":"http://arxiv.org/abs/2405.05875v1","category":"quant-ph"}
{"created":"2024-05-09 16:03:23","title":"Performance Parameters of Infra-red and Visible-active MXene Photocatalysts for Water Splitting","abstract":"Water splitting reactions through photocatalysis is an efficient and sustainable technique for the generation of green energy. The photocatalyst's ability to effect simultaneous generation of hydrogen and oxygen, along with efficiency in utilisation of charged carriers, conversion of solar energy to hydrogen, fast migration, and low recombination rates of carriers, are the parameters to decide its suitability in water splitting. In literature, comprehensive calculation and analysis of all these performance parameters for a potential photocatalyst are rare. In this work, we have performed first-principles-based computations to find new efficient photocatalysts from the family of Janus MXenes and assessed their performance parameters. Strain engineering has been invoked in search of new materials. Out of 14 studied materials, we find 5 materials: Sc$_{2}$COS, Zr$_{2}$COS, Hf$_{2}$COS, and ZrHfCO$_{2}$ under zero or finite tensile strain and Hf$_{2}$COSe at 6\\% tensile strain meeting the requirements of simultaneous reactions to split water. The computations of various efficiency-related parameters demonstrate that Zr$_{2}$COS, Hf$_{2}$COS, and Hf$_{2}$COSe have excellent efficiencies, significantly better than the well-known photocatalysts. The origin of such performances lies in their electronic and optical properties, which are analysed systematically.","sentences":["Water splitting reactions through photocatalysis is an efficient and sustainable technique for the generation of green energy.","The photocatalyst's ability to effect simultaneous generation of hydrogen and oxygen, along with efficiency in utilisation of charged carriers, conversion of solar energy to hydrogen, fast migration, and low recombination rates of carriers, are the parameters to decide its suitability in water splitting.","In literature, comprehensive calculation and analysis of all these performance parameters for a potential photocatalyst are rare.","In this work, we have performed first-principles-based computations to find new efficient photocatalysts from the family of Janus MXenes and assessed their performance parameters.","Strain engineering has been invoked in search of new materials.","Out of 14 studied materials, we find 5 materials: Sc$_{2}$COS, Zr$_{2}$COS, Hf$_{2}$COS, and ZrHfCO$_{2}$ under zero or finite tensile strain and Hf$_{2}$COSe at 6\\% tensile strain meeting the requirements of simultaneous reactions to split water.","The computations of various efficiency-related parameters demonstrate that Zr$_{2}$COS, Hf$_{2}$COS, and Hf$_{2}$COSe have excellent efficiencies, significantly better than the well-known photocatalysts.","The origin of such performances lies in their electronic and optical properties, which are analysed systematically."],"url":"http://arxiv.org/abs/2405.05874v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 16:00:54","title":"FlockGPT: Guiding UAV Flocking with Linguistic Orchestration","abstract":"This article presents the world's first rapid drone flocking control using natural language through generative AI. The described approach enables the intuitive orchestration of a flock of any size to achieve the desired geometry. The key feature of the method is the development of a new interface based on Large Language Models to communicate with the user and to generate the target geometry descriptions. Users can interactively modify or provide comments during the construction of the flock geometry model. By combining flocking technology and defining the target surface using a signed distance function, smooth and adaptive movement of the drone swarm between target states is achieved.   Our user study on FlockGPT confirmed a high level of intuitive control over drone flocking by users. Subjects who had never previously controlled a swarm of drones were able to construct complex figures in just a few iterations and were able to accurately distinguish the formed swarm drone figures. The results revealed a high recognition rate for six different geometric patterns generated through the LLM-based interface and performed by a simulated drone flock (mean of 80% with a maximum of 93\\% for cube and tetrahedron patterns). Users commented on low temporal demand (19.2 score in NASA-TLX), high performance (26 score in NASA-TLX), attractiveness (1.94 UEQ score), and hedonic quality (1.81 UEQ score) of the developed system. The FlockGPT demo code repository can be found at: coming soon","sentences":["This article presents the world's first rapid drone flocking control using natural language through generative AI.","The described approach enables the intuitive orchestration of a flock of any size to achieve the desired geometry.","The key feature of the method is the development of a new interface based on Large Language Models to communicate with the user and to generate the target geometry descriptions.","Users can interactively modify or provide comments during the construction of the flock geometry model.","By combining flocking technology and defining the target surface using a signed distance function, smooth and adaptive movement of the drone swarm between target states is achieved.   ","Our user study on FlockGPT confirmed a high level of intuitive control over drone flocking by users.","Subjects who had never previously controlled a swarm of drones were able to construct complex figures in just a few iterations and were able to accurately distinguish the formed swarm drone figures.","The results revealed a high recognition rate for six different geometric patterns generated through the LLM-based interface and performed by a simulated drone flock (mean of 80% with a maximum of 93\\% for cube and tetrahedron patterns).","Users commented on low temporal demand (19.2 score in NASA-TLX), high performance (26 score in NASA-TLX), attractiveness (1.94 UEQ score), and hedonic quality (1.81 UEQ score) of the developed system.","The FlockGPT demo code repository can be found at: coming soon"],"url":"http://arxiv.org/abs/2405.05872v1","category":"cs.RO"}
{"created":"2024-05-09 16:00:20","title":"Selecting the Most Conflicting Pair of Candidates","abstract":"We study committee elections from a perspective of finding the most conflicting candidates, that is, candidates that imply the largest amount of conflict, as per voter preferences. By proposing basic axioms to capture this objective, we show that none of the prominent multiwinner voting rules meet them. Consequently, we design committee voting rules compliant with our desiderata, introducing conflictual voting rules. A subsequent deepened analysis sheds more light on how they operate. Our investigation identifies various aspects of conflict, for which we come up with relevant axioms and quantitative measures, which may be of independent interest. We support our theoretical study with experiments on both real-life and synthetic data.","sentences":["We study committee elections from a perspective of finding the most conflicting candidates, that is, candidates that imply the largest amount of conflict, as per voter preferences.","By proposing basic axioms to capture this objective, we show that none of the prominent multiwinner voting rules meet them.","Consequently, we design committee voting rules compliant with our desiderata, introducing conflictual voting rules.","A subsequent deepened analysis sheds more light on how they operate.","Our investigation identifies various aspects of conflict, for which we come up with relevant axioms and quantitative measures, which may be of independent interest.","We support our theoretical study with experiments on both real-life and synthetic data."],"url":"http://arxiv.org/abs/2405.05870v1","category":"cs.GT"}
{"created":"2024-05-09 15:52:40","title":"SMARTINI3: Systematic Parametrization of Realistic Multi-Scale Membrane Models via Unsupervised Learning and Multi-Objective Evolutionary Algorithms","abstract":"In this study, we utilize genetic algorithms to develop a realistic implicit solvent ultra-coarse-grained (PC) membrane model comprising only three interaction sites. The key philosophy of the ultra-CG membrane model SMARTINI3 is its compatibility with realistic membrane proteins, for example, modeled within the Martini coarse-grained (CG) model, as well as with the widely used GROMACS software for molecular simulations. Our objective is to parameterize this ultra-CG model to accurately reproduce the experimentally observed structural and thermodynamic properties of PC membranes in real units, including properties such as area per lipid, area compressibility, bending modulus, line tension, phase transition temperature, density profile, and radial distribution function. In our example, we specifically focus on the properties of a POPC membrane, although the developed membrane model could be perceived as a generic model of lipid membranes. To optimize the performance of the model (the fitness), we conduct a series of evolutionary runs with diverse random initial population sizes (ranging from 96 to 384). We demonstrate that the ultra-CG membrane model we developed exhibits authentic lipid membrane behaviors, encompassing self-assembly into bilayers, vesicle formation, membrane fusion, and gel phase formation. Moreover, we demonstrate compatibility with the Martini coarse-grained model by successfully reproducing the behavior of a transmembrane domain embedded within a lipid bilayer. This facilitates the simulation of realistic membrane proteins within an ultra-CG bilayer membrane, enhancing the accuracy and applicability of our model in biophysical studies.","sentences":["In this study, we utilize genetic algorithms to develop a realistic implicit solvent ultra-coarse-grained (PC) membrane model comprising only three interaction sites.","The key philosophy of the ultra-CG membrane model SMARTINI3 is its compatibility with realistic membrane proteins, for example, modeled within the Martini coarse-grained (CG) model, as well as with the widely used GROMACS software for molecular simulations.","Our objective is to parameterize this ultra-CG model to accurately reproduce the experimentally observed structural and thermodynamic properties of PC membranes in real units, including properties such as area per lipid, area compressibility, bending modulus, line tension, phase transition temperature, density profile, and radial distribution function.","In our example, we specifically focus on the properties of a POPC membrane, although the developed membrane model could be perceived as a generic model of lipid membranes.","To optimize the performance of the model (the fitness), we conduct a series of evolutionary runs with diverse random initial population sizes (ranging from 96 to 384).","We demonstrate that the ultra-CG membrane model we developed exhibits authentic lipid membrane behaviors, encompassing self-assembly into bilayers, vesicle formation, membrane fusion, and gel phase formation.","Moreover, we demonstrate compatibility with the Martini coarse-grained model by successfully reproducing the behavior of a transmembrane domain embedded within a lipid bilayer.","This facilitates the simulation of realistic membrane proteins within an ultra-CG bilayer membrane, enhancing the accuracy and applicability of our model in biophysical studies."],"url":"http://arxiv.org/abs/2405.05864v1","category":"q-bio.BM"}
{"created":"2024-05-09 15:48:39","title":"ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers","abstract":"Excavators are crucial for diverse tasks such as construction and mining, while autonomous excavator systems enhance safety and efficiency, address labor shortages, and improve human working conditions. Different from the existing modularized approaches, this paper introduces ExACT, an end-to-end autonomous excavator system that processes raw LiDAR, camera data, and joint positions to control excavator valves directly. Utilizing the Action Chunking with Transformers (ACT) architecture, ExACT employs imitation learning to take observations from multi-modal sensors as inputs and generate actionable sequences. In our experiment, we build a simulator based on the captured real-world data to model the relations between excavator valve states and joint velocities. With a few human-operated demonstration data trajectories, ExACT demonstrates the capability of completing different excavation tasks, including reaching, digging and dumping through imitation learning in validations with the simulator. To the best of our knowledge, ExACT represents the first instance towards building an end-to-end autonomous excavator system via imitation learning methods with a minimal set of human demonstrations. The video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.","sentences":["Excavators are crucial for diverse tasks such as construction and mining, while autonomous excavator systems enhance safety and efficiency, address labor shortages, and improve human working conditions.","Different from the existing modularized approaches, this paper introduces ExACT, an end-to-end autonomous excavator system that processes raw LiDAR, camera data, and joint positions to control excavator valves directly.","Utilizing the Action Chunking with Transformers (ACT) architecture, ExACT employs imitation learning to take observations from multi-modal sensors as inputs and generate actionable sequences.","In our experiment, we build a simulator based on the captured real-world data to model the relations between excavator valve states and joint velocities.","With a few human-operated demonstration data trajectories, ExACT demonstrates the capability of completing different excavation tasks, including reaching, digging and dumping through imitation learning in validations with the simulator.","To the best of our knowledge, ExACT represents the first instance towards building an end-to-end autonomous excavator system via imitation learning methods with a minimal set of human demonstrations.","The video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk."],"url":"http://arxiv.org/abs/2405.05861v1","category":"cs.RO"}
{"created":"2024-05-09 15:45:08","title":"Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera","abstract":"We propose an approach for reconstructing free-moving object from a monocular RGB video. Most existing methods either assume scene prior, hand pose prior, object category pose prior, or rely on local optimization with multiple sequence segments. We propose a method that allows free interaction with the object in front of a moving camera without relying on any prior, and optimizes the sequence globally without any segments. We progressively optimize the object shape and pose simultaneously based on an implicit neural representation. A key aspect of our method is a virtual camera system that reduces the search space of the optimization significantly. We evaluate our method on the standard HO3D dataset and a collection of egocentric RGB sequences captured with a head-mounted device. We demonstrate that our approach outperforms most methods significantly, and is on par with recent techniques that assume prior information.","sentences":["We propose an approach for reconstructing free-moving object from a monocular RGB video.","Most existing methods either assume scene prior, hand pose prior, object category pose prior, or rely on local optimization with multiple sequence segments.","We propose a method that allows free interaction with the object in front of a moving camera without relying on any prior, and optimizes the sequence globally without any segments.","We progressively optimize the object shape and pose simultaneously based on an implicit neural representation.","A key aspect of our method is a virtual camera system that reduces the search space of the optimization significantly.","We evaluate our method on the standard HO3D dataset and a collection of egocentric RGB sequences captured with a head-mounted device.","We demonstrate that our approach outperforms most methods significantly, and is on par with recent techniques that assume prior information."],"url":"http://arxiv.org/abs/2405.05858v1","category":"cs.CV"}
{"created":"2024-05-09 15:44:15","title":"Fukaya Categories of Hyperplane Arrangements","abstract":"To a simple polarized hyperplane arrangement (not necessarily cyclic) $\\mathbb{V}$, one can associate a stopped Liouville manifold (equivalently, a Liouville sector) $\\left(M(\\mathbb{V}),\\xi\\right)$, where $M(\\mathbb{V})$ is the complement of finitely many hyperplanes in $\\mathbb{C}^d$, obtained as the complexifications of the real hyperplanes in $\\mathbb{V}$. The Liouville structure on $M(\\mathbb{V})$ comes from a very affine embedding, and the stop $\\xi$ is determined by the polarization. In this article, we study the symplectic topology of $\\left(M(\\mathbb{V}),\\xi\\right)$. In particular, we prove that their partially wrapped Fukaya categories are generated by Lagrangian submanifolds associated to the bounded and feasible chambers of $\\mathbb{V}$. A computation of the Fukaya $A_\\infty$-algebra of these Lagrangians then enables us to identity these wrapped Fukaya categories with the $\\mathbb{G}_m^d$-equivariant hypertoric convolution algebras $\\widetilde{B}(\\mathbb{V})$ associated to $\\mathbb{V}$. This confirms a conjecture of Lauda-Licata-Manion and provides evidence for the general conjecture of Lekili-Segal on the equivariant Fukaya categories of symplectic manifolds with Hamiltonian torus actions.","sentences":["To a simple polarized hyperplane arrangement (not necessarily cyclic) $\\mathbb{V}$, one can associate a stopped Liouville manifold (equivalently, a Liouville sector) $\\left(M(\\mathbb{V}),\\xi\\right)$, where $M(\\mathbb{V})$ is the complement of finitely many hyperplanes in $\\mathbb{C}^d$, obtained as the complexifications of the real hyperplanes in $\\mathbb{V}$. The Liouville structure on $M(\\mathbb{V})$ comes from a very affine embedding, and the stop $\\xi$ is determined by the polarization.","In this article, we study the symplectic topology of $\\left(M(\\mathbb{V}),\\xi\\right)$. In particular, we prove that their partially wrapped Fukaya categories are generated by Lagrangian submanifolds associated to the bounded and feasible chambers of $\\mathbb{V}$. A computation of the Fukaya $A_\\infty$-algebra of these Lagrangians then enables us to identity these wrapped Fukaya categories with the $\\mathbb{G}_m^d$-equivariant hypertoric convolution algebras $\\widetilde{B}(\\mathbb{V})$ associated to $\\mathbb{V}$. This confirms a conjecture of Lauda-Licata-Manion and provides evidence for the general conjecture of Lekili-Segal on the equivariant Fukaya categories of symplectic manifolds with Hamiltonian torus actions."],"url":"http://arxiv.org/abs/2405.05856v1","category":"math.SG"}
{"created":"2024-05-09 15:44:11","title":"Compressed Bayesian Federated Learning for Reliable Passive Radio Sensing in Industrial IoT","abstract":"Bayesian Federated Learning (FL) has been recently introduced to provide well-calibrated Machine Learning (ML) models quantifying the uncertainty of their predictions. Despite their advantages compared to frequentist FL setups, Bayesian FL tools implemented over decentralized networks are subject to high communication costs due to the iterated exchange of local posterior distributions among cooperating devices. Therefore, this paper proposes a communication-efficient decentralized Bayesian FL policy to reduce the communication overhead without sacrificing final learning accuracy and calibration. The proposed method integrates compression policies and allows devices to perform multiple optimization steps before sending the local posterior distributions. We integrate the developed tool in an Industrial Internet of Things (IIoT) use case where collaborating nodes equipped with autonomous radar sensors are tasked to reliably localize human operators in a workplace shared with robots. Numerical results show that the developed approach obtains highly accurate yet well-calibrated ML models compatible with the ones provided by conventional (uncompressed) Bayesian FL tools while substantially decreasing the communication overhead (i.e., up to 99%). Furthermore, the proposed approach is advantageous when compared with state-of-the-art compressed frequentist FL setups in terms of calibration, especially when the statistical distribution of the testing dataset changes.","sentences":["Bayesian Federated Learning (FL) has been recently introduced to provide well-calibrated Machine Learning (ML) models quantifying the uncertainty of their predictions.","Despite their advantages compared to frequentist FL setups, Bayesian FL tools implemented over decentralized networks are subject to high communication costs due to the iterated exchange of local posterior distributions among cooperating devices.","Therefore, this paper proposes a communication-efficient decentralized Bayesian FL policy to reduce the communication overhead without sacrificing final learning accuracy and calibration.","The proposed method integrates compression policies and allows devices to perform multiple optimization steps before sending the local posterior distributions.","We integrate the developed tool in an Industrial Internet of Things (IIoT) use case where collaborating nodes equipped with autonomous radar sensors are tasked to reliably localize human operators in a workplace shared with robots.","Numerical results show that the developed approach obtains highly accurate yet well-calibrated ML models compatible with the ones provided by conventional (uncompressed) Bayesian FL tools while substantially decreasing the communication overhead (i.e., up to 99%).","Furthermore, the proposed approach is advantageous when compared with state-of-the-art compressed frequentist FL setups in terms of calibration, especially when the statistical distribution of the testing dataset changes."],"url":"http://arxiv.org/abs/2405.05855v1","category":"cs.LG"}
{"created":"2024-05-09 15:39:54","title":"Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control","abstract":"Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.","sentences":["Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs.","Such capabilities are difficult to learn solely from task-specific data.","This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains.","However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control.","To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information.","Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments.","We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks.","Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark."],"url":"http://arxiv.org/abs/2405.05852v1","category":"cs.CV"}
{"created":"2024-05-09 15:34:15","title":"Learned feature representations are biased by complexity, learning order, position, and more","abstract":"Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. These results also highlight a key challenge for interpretability $-$ or for comparing the representations of models and brains $-$ disentangling extraneous biases from the computationally important aspects of a system's internal representations.","sentences":["Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience.","Both fields generally use representations as a means to understand or improve a system's computations.","In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts.","We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data.","We train various deep learning architectures to compute these multiple abstract features about their inputs.","We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs.","For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well.","We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly).","Our results help to characterize the inductive biases of gradient-based representation learning.","These results also highlight a key challenge for interpretability $-$ or for comparing the representations of models and brains $-$ disentangling extraneous biases from the computationally important aspects of a system's internal representations."],"url":"http://arxiv.org/abs/2405.05847v1","category":"cs.LG"}
{"created":"2024-05-09 15:32:00","title":"Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models","abstract":"The past few years have witnessed substantial advancement in text-guided image generation powered by diffusion models. However, it was shown that text-to-image diffusion models are vulnerable to training image memorization, raising concerns on copyright infringement and privacy invasion. In this work, we perform practical analysis of memorization in text-to-image diffusion models. Targeting a set of images to protect, we conduct quantitive analysis on them without need to collect any prompts. Specifically, we first formally define the memorization of image and identify three necessary conditions of memorization, respectively similarity, existence and probability. We then reveal the correlation between the model's prediction error and image replication. Based on the correlation, we propose to utilize inversion techniques to verify the safety of target images against memorization and measure the extent to which they are memorized. Model developers can utilize our analysis method to discover memorized images or reliably claim safety against memorization. Extensive experiments on the Stable Diffusion, a popular open-source text-to-image diffusion model, demonstrate the effectiveness of our analysis method.","sentences":["The past few years have witnessed substantial advancement in text-guided image generation powered by diffusion models.","However, it was shown that text-to-image diffusion models are vulnerable to training image memorization, raising concerns on copyright infringement and privacy invasion.","In this work, we perform practical analysis of memorization in text-to-image diffusion models.","Targeting a set of images to protect, we conduct quantitive analysis on them without need to collect any prompts.","Specifically, we first formally define the memorization of image and identify three necessary conditions of memorization, respectively similarity, existence and probability.","We then reveal the correlation between the model's prediction error and image replication.","Based on the correlation, we propose to utilize inversion techniques to verify the safety of target images against memorization and measure the extent to which they are memorized.","Model developers can utilize our analysis method to discover memorized images or reliably claim safety against memorization.","Extensive experiments on the Stable Diffusion, a popular open-source text-to-image diffusion model, demonstrate the effectiveness of our analysis method."],"url":"http://arxiv.org/abs/2405.05846v1","category":"cs.CR"}
{"created":"2024-05-09 15:30:28","title":"Non-Binary Covering Codes for Low-Access Computations","abstract":"Given a real dataset and a computation family, we wish to encode and store the dataset in a distributed system so that any computation from the family can be performed by accessing a small number of nodes. In this work, we focus on the families of linear computations where the coefficients are restricted to a finite set of real values. For two-valued computations, a recent work presented a scheme that gives good feasible points on the access-redundancy tradeoff. This scheme is based on binary covering codes having a certain closure property. In a follow-up work, this scheme was extended to all finite coefficient sets, using a new additive-combinatorics notion called coefficient complexity. In the present paper, we explore non-binary covering codes and develop schemes that outperform the state-of-the-art for some coefficient sets. We provide a more general coefficient complexity definition and show its applicability to the access-redundancy tradeoff.","sentences":["Given a real dataset and a computation family, we wish to encode and store the dataset in a distributed system so that any computation from the family can be performed by accessing a small number of nodes.","In this work, we focus on the families of linear computations where the coefficients are restricted to a finite set of real values.","For two-valued computations, a recent work presented a scheme that gives good feasible points on the access-redundancy tradeoff.","This scheme is based on binary covering codes having a certain closure property.","In a follow-up work, this scheme was extended to all finite coefficient sets, using a new additive-combinatorics notion called coefficient complexity.","In the present paper, we explore non-binary covering codes and develop schemes that outperform the state-of-the-art for some coefficient sets.","We provide a more general coefficient complexity definition and show its applicability to the access-redundancy tradeoff."],"url":"http://arxiv.org/abs/2405.05845v1","category":"cs.IT"}
{"created":"2024-05-09 15:24:36","title":"The Correlated Spatial Structure of the Proton: Two-body densities as a framework for dynamical imaging","abstract":"We present results on two-parton densities in coordinate space, which capture a fuller dynamical picture of the proton's internal structure, including information on the relative position between quarks and gluons in the transverse plane. The connection of such two body densities to observables proceeds in QCD via the definition of double generalized parton distributions.","sentences":["We present results on two-parton densities in coordinate space, which capture a fuller dynamical picture of the proton's internal structure, including information on the relative position between quarks and gluons in the transverse plane.","The connection of such two body densities to observables proceeds in QCD via the definition of double generalized parton distributions."],"url":"http://arxiv.org/abs/2405.05842v1","category":"hep-ph"}
{"created":"2024-05-09 15:23:38","title":"Self-Supervised Pre-training with Symmetric Superimposition Modeling for Scene Text Recognition","abstract":"In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data. Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning. However, they omit modeling the linguistic information in text images, which is crucial for recognizing text. To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM). The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input. Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs. At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context. At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination. In our design, we disrupt the character shape and linguistic rules. Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics. Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks.","sentences":["In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data.","Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning.","However, they omit modeling the linguistic information in text images, which is crucial for recognizing text.","To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM).","The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input.","Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs.","At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context.","At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination.","In our design, we disrupt the character shape and linguistic rules.","Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics.","Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks."],"url":"http://arxiv.org/abs/2405.05841v1","category":"cs.CV"}
{"created":"2024-05-09 15:21:22","title":"FREmu: Power Spectrum Emulator for $f(R)$ Gravity","abstract":"To investigate gravity in the non-linear regime of cosmic structure using measurements from Stage-IV surveys, it is imperative to accurately compute large-scale structure observables, such as non-linear matter power spectra, for gravity models that extend beyond general relativity. However, the theoretical predictions of non-linear observables are typically derived from N-body simulations, which demand substantial computational resources. In this study, we introduce a novel public emulator, termed FREmu, designed to provide rapid and precise forecasts of non-linear power spectra specifically for the Hu-Sawicki $f(R)$ gravity model across scales $0.0089 h \\mathrm{Mpc}^{-1}<k<0.5 h \\mathrm{Mpc}^{-1}$ and redshifts $0<z<3$. FREmu leverages Principal Component Analysis and Artificial Neural Networks to establish a mapping from parameters to power spectra, utilizing training data derived from the Quijote-MG simulation suite. With a parameter space encompassing 7 dimensions, including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $M_{\\nu}$ and $f_{R_0}$, the emulator achieves an accuracy exceeding 95% for the majority of cases, thus proving to be highly efficient for constraining parameters.","sentences":["To investigate gravity in the non-linear regime of cosmic structure using measurements from Stage-IV surveys, it is imperative to accurately compute large-scale structure observables, such as non-linear matter power spectra, for gravity models that extend beyond general relativity.","However, the theoretical predictions of non-linear observables are typically derived from N-body simulations, which demand substantial computational resources.","In this study, we introduce a novel public emulator, termed FREmu, designed to provide rapid and precise forecasts of non-linear power spectra specifically for the Hu-Sawicki $f(R)$ gravity model across scales $0.0089 h \\mathrm{Mpc}^{-1}<k<0.5 h \\mathrm{Mpc}^{-1}$ and redshifts $0<z<3$. FREmu leverages Principal Component Analysis and Artificial Neural Networks to establish a mapping from parameters to power spectra, utilizing training data derived from the Quijote-MG simulation suite.","With a parameter space encompassing 7 dimensions, including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $M_{\\nu}$ and $f_{R_0}$, the emulator achieves an accuracy exceeding 95% for the majority of cases, thus proving to be highly efficient for constraining parameters."],"url":"http://arxiv.org/abs/2405.05840v1","category":"astro-ph.CO"}
{"created":"2024-05-09 15:18:41","title":"Exploring solutions to the muon g-2 anomaly in a THDM-like model under flavor constraints","abstract":"The magnetic moment of the muon can receive significant two-loop contributions from the scalar spectrum in Two Higgs Doublet Models (THDM), particularly for a light pseudoscalar. It is established that the symmetry breaking from $SU(3)_L \\times U(1)_N$ to $SU(2)_L \\times U(1)_Y$ generates an effective Type-III THDM, which invariably leads to flavor-changing neutral current (FCNC) processes. In this study, we examine whether such an effective THDM framework can account for the anomalous magnetic moment of the muon, considering the constraints imposed by $B$-meson decays, meson mixing, and invisible Higgs decays. Our principal finding reveals that a pseudoscalar with a mass of 66 GeV and a $\\tan \\beta$ value of 58 can account for the $g-2$ anomaly without conflicting with existing experimental limits.","sentences":["The magnetic moment of the muon can receive significant two-loop contributions from the scalar spectrum in Two Higgs Doublet Models (THDM), particularly for a light pseudoscalar.","It is established that the symmetry breaking from $SU(3)_L \\times U(1)_N$ to $SU(2)_L \\times U(1)_Y$ generates an effective Type-III THDM, which invariably leads to flavor-changing neutral current (FCNC) processes.","In this study, we examine whether such an effective THDM framework can account for the anomalous magnetic moment of the muon, considering the constraints imposed by $B$-meson decays, meson mixing, and invisible Higgs decays.","Our principal finding reveals that a pseudoscalar with a mass of 66 GeV and a $\\tan \\beta$ value of 58 can account for the $g-2$ anomaly without conflicting with existing experimental limits."],"url":"http://arxiv.org/abs/2405.05839v1","category":"hep-ph"}
{"created":"2024-05-09 15:15:51","title":"Role of coupled electrochemistry and stress on the Li-anode instability: A continuum approach","abstract":"We present a coupled mechanistic approach that elucidates the intricate interplay between stress and electrochemistry, enabling the prediction of the onset of instabilities in Li-metal anodes and the solid electrolyte interphase (SEI) in liquid-electrolyte Li-metal batteries. Our continuum theory considers a two-way coupling between stress and electrochemistry, includes Li and electron transport through SEI, incorporates effects of Li viscoplasticity, includes SEI and electrolyte interface surface energy and evaluates crucial roles of these mechanistic effects on the continuously evolving anode surface due to the viscoplastic deformation of lithium. In the model, spatial current density evolves with the stress-induced potential across the deformed anode/SEI interface. We assume SEI as a homogeneous, artificial layer on the Li-anode, which allows the investigation of the mechanical and electrochemical properties of the SEI systematically. Subsequently, we solve a set of coupled electrochemistry and displacement equations within the SEI and anode domains. The model is implemented numerically by writing a user element subroutine in Abaqus/Standard. We conduct numerical simulations under various galvanostatic conditions and SEI properties and predict conditions for anode instability. We find that Li viscoplasticity is one of the key attributes that drives instability in the Li-anode and show that applying a soft artificial SEI layer on the Li-anode to minimize viscoplastic deformation can be an effective method. We also report the role of artificial SEI elasticity and thickness on anode stability. Selected stability maps are provided as a design aid for artificial SEI.","sentences":["We present a coupled mechanistic approach that elucidates the intricate interplay between stress and electrochemistry, enabling the prediction of the onset of instabilities in Li-metal anodes and the solid electrolyte interphase (SEI) in liquid-electrolyte Li-metal batteries.","Our continuum theory considers a two-way coupling between stress and electrochemistry, includes Li and electron transport through SEI, incorporates effects of Li viscoplasticity, includes SEI and electrolyte interface surface energy and evaluates crucial roles of these mechanistic effects on the continuously evolving anode surface due to the viscoplastic deformation of lithium.","In the model, spatial current density evolves with the stress-induced potential across the deformed anode/SEI interface.","We assume SEI as a homogeneous, artificial layer on the Li-anode, which allows the investigation of the mechanical and electrochemical properties of the SEI systematically.","Subsequently, we solve a set of coupled electrochemistry and displacement equations within the SEI and anode domains.","The model is implemented numerically by writing a user element subroutine in Abaqus/Standard.","We conduct numerical simulations under various galvanostatic conditions and SEI properties and predict conditions for anode instability.","We find that Li viscoplasticity is one of the key attributes that drives instability in the Li-anode and show that applying a soft artificial SEI layer on the Li-anode to minimize viscoplastic deformation can be an effective method.","We also report the role of artificial SEI elasticity and thickness on anode stability.","Selected stability maps are provided as a design aid for artificial SEI."],"url":"http://arxiv.org/abs/2405.05837v1","category":"math.NA"}
{"created":"2024-05-09 15:00:39","title":"Measurement-based Verification of Quantum Markov Chains","abstract":"Model-checking techniques have been extended to analyze quantum programs and communication protocols represented as quantum Markov chains, an extension of classical Markov chains. To specify qualitative temporal properties, a subspace-based quantum temporal logic is used, which is built on Birkhoff-von Neumann atomic propositions. These propositions determine whether a quantum state is within a subspace of the entire state space. In this paper, we propose the measurement-based linear-time temporal logic MLTL to check quantitative properties. MLTL builds upon classical linear-time temporal logic (LTL) but introduces quantum atomic propositions that reason about the probability distribution after measuring a quantum state. To facilitate verification, we extend the symbolic dynamics-based techniques for stochastic matrices described by Agrawal et al. (JACM 2015) to handle more general quantum linear operators (super-operators) through eigenvalue analysis. This extension enables the development of an efficient algorithm for approximately model checking a quantum Markov chain against an MLTL formula. To demonstrate the utility of our model-checking algorithm, we use it to simultaneously verify linear-time properties of both quantum and classical random walks. Through this verification, we confirm the previously established advantages discovered by Ambainis et al. (STOC 2001) of quantum walks over classical random walks and discover new phenomena unique to quantum walks.","sentences":["Model-checking techniques have been extended to analyze quantum programs and communication protocols represented as quantum Markov chains, an extension of classical Markov chains.","To specify qualitative temporal properties, a subspace-based quantum temporal logic is used, which is built on Birkhoff-von Neumann atomic propositions.","These propositions determine whether a quantum state is within a subspace of the entire state space.","In this paper, we propose the measurement-based linear-time temporal logic MLTL to check quantitative properties.","MLTL builds upon classical linear-time temporal logic (LTL) but introduces quantum atomic propositions that reason about the probability distribution after measuring a quantum state.","To facilitate verification, we extend the symbolic dynamics-based techniques for stochastic matrices described by Agrawal et al.","(JACM 2015)","to handle more general quantum linear operators (super-operators) through eigenvalue analysis.","This extension enables the development of an efficient algorithm for approximately model checking a quantum Markov chain against an MLTL formula.","To demonstrate the utility of our model-checking algorithm, we use it to simultaneously verify linear-time properties of both quantum and classical random walks.","Through this verification, we confirm the previously established advantages discovered by Ambainis et al.","(STOC 2001) of quantum walks over classical random walks and discover new phenomena unique to quantum walks."],"url":"http://arxiv.org/abs/2405.05825v1","category":"quant-ph"}
{"created":"2024-05-09 14:57:30","title":"Equivariant formality in complex-oriented theories","abstract":"Let $G$ be a product of unitary groups and let $(M,\\omega)$ be a compact symplectic manifold with Hamiltonian $G$-action. We prove an equivariant formality result for any complex-oriented cohomology theory $\\mathbb{E}^*$ (in particular, integral cohomology). This generalizes the celebrated result of Atiyah-Bott-Kirwan for rational cohomology. The proof does not use classical ideas but instead relies on a recent cohomological splitting result of Abouzaid-McLean-Smith for Hamiltonian fibrations over $\\mathbb{CP}^1$. Moreover, we establish analogues of the \"localization\" and \"injectivity to fixed points\" theorems for certain cohomology theories studied by Hopkins-Kuhn-Ravenel. As an application of these results, we establish a Goresky-Kottwitz-MacPherson theorem with Morava $K$-theory coefficients for Hamiltonian $T$-manifolds.","sentences":["Let $G$ be a product of unitary groups and let $(M,\\omega)$ be a compact symplectic manifold with Hamiltonian $G$-action.","We prove an equivariant formality result for any complex-oriented cohomology theory $\\mathbb{E}^*$ (in particular, integral cohomology).","This generalizes the celebrated result of Atiyah-Bott-Kirwan for rational cohomology.","The proof does not use classical ideas but instead relies on a recent cohomological splitting result of Abouzaid-McLean-Smith for Hamiltonian fibrations over $\\mathbb{CP}^1$. Moreover, we establish analogues of the \"localization\" and \"injectivity to fixed points\" theorems for certain cohomology theories studied by Hopkins-Kuhn-Ravenel.","As an application of these results, we establish a Goresky-Kottwitz-MacPherson theorem with Morava $K$-theory coefficients for Hamiltonian $T$-manifolds."],"url":"http://arxiv.org/abs/2405.05821v1","category":"math.SG"}
{"created":"2024-05-09 14:56:11","title":"Semi-Autonomous Laparoscopic Robot Docking with Learned Hand-Eye Information Fusion","abstract":"In this study, we introduce a novel shared-control system for key-hole docking operations, combining a commercial camera with occlusion-robust pose estimation and a hand-eye information fusion technique. This system is used to enhance docking precision and force-compliance safety. To train a hand-eye information fusion network model, we generated a self-supervised dataset using this docking system. After training, our pose estimation method showed improved accuracy compared to traditional methods, including observation-only approaches, hand-eye calibration, and conventional state estimation filters. In real-world phantom experiments, our approach demonstrated its effectiveness with reduced position dispersion (1.23\\pm 0.81 mm vs. 2.47 \\pm 1.22 mm) and force dispersion (0.78\\pm 0.57 N vs. 1.15 \\pm 0.97 N) compared to the control group. These advancements in semi-autonomy co-manipulation scenarios enhance interaction and stability. The study presents an anti-interference, steady, and precision solution with potential applications extending beyond laparoscopic surgery to other minimally invasive procedures.","sentences":["In this study, we introduce a novel shared-control system for key-hole docking operations, combining a commercial camera with occlusion-robust pose estimation and a hand-eye information fusion technique.","This system is used to enhance docking precision and force-compliance safety.","To train a hand-eye information fusion network model, we generated a self-supervised dataset using this docking system.","After training, our pose estimation method showed improved accuracy compared to traditional methods, including observation-only approaches, hand-eye calibration, and conventional state estimation filters.","In real-world phantom experiments, our approach demonstrated its effectiveness with reduced position dispersion (1.23\\pm 0.81 mm vs. 2.47 \\pm 1.22 mm) and force dispersion (0.78\\pm 0.57 N vs. 1.15 \\pm 0.97 N) compared to the control group.","These advancements in semi-autonomy co-manipulation scenarios enhance interaction and stability.","The study presents an anti-interference, steady, and precision solution with potential applications extending beyond laparoscopic surgery to other minimally invasive procedures."],"url":"http://arxiv.org/abs/2405.05817v1","category":"cs.RO"}
{"created":"2024-05-09 14:52:32","title":"MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction","abstract":"Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction. Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles. Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics. Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques. Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively. By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively. Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets.","sentences":["Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction.","Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles.","Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics.","Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques.","Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively.","By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively.","Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets."],"url":"http://arxiv.org/abs/2405.05814v1","category":"eess.IV"}
{"created":"2024-05-09 14:50:17","title":"The $cd$-index of semi-Eulerian posets","abstract":"We generalize the definition of the $cd$-index of an Eulerian poset to the class of semi-Eulerian posets. For simplicial semi-Eulerian Buchsbaum posets, we show that all coefficients of the $cd$-index are non-negative. This proves a conjecture of Novik for odd dimensional manifolds and extends it to the even dimensional case.","sentences":["We generalize the definition of the $cd$-index of an Eulerian poset to the class of semi-Eulerian posets.","For simplicial semi-Eulerian Buchsbaum posets, we show that all coefficients of the $cd$-index are non-negative.","This proves a conjecture of Novik for odd dimensional manifolds and extends it to the even dimensional case."],"url":"http://arxiv.org/abs/2405.05812v1","category":"math.CO"}
{"created":"2024-05-09 14:49:31","title":"Series involving rational, factorial and power functions","abstract":"This is an anthology of series involving rational, factorial, and power functions expressed in terms of special functions. New finite expansions involving quotient functions expressed in terms of the Hurwitz-Lerch zeta function are given. These results represent a new form of expressing this special function as a finite series where contour integration is required for derivation. Extended series previously known and derived are extended using differential equations and algebraic methods.","sentences":["This is an anthology of series involving rational, factorial, and power functions expressed in terms of special functions.","New finite expansions involving quotient functions expressed in terms of the Hurwitz-Lerch zeta function are given.","These results represent a new form of expressing this special function as a finite series where contour integration is required for derivation.","Extended series previously known and derived are extended using differential equations and algebraic methods."],"url":"http://arxiv.org/abs/2405.05810v1","category":"math.GM"}
{"created":"2024-05-09 14:48:17","title":"Aequitas Flow: Streamlining Fair ML Experimentation","abstract":"Aequitas Flow is an open-source framework for end-to-end Fair Machine Learning (ML) experimentation in Python. This package fills the existing integration gaps in other Fair ML packages of complete and accessible experimentation. It provides a pipeline for fairness-aware model training, hyperparameter optimization, and evaluation, enabling rapid and simple experiments and result analysis. Aimed at ML practitioners and researchers, the framework offers implementations of methods, datasets, metrics, and standard interfaces for these components to improve extensibility. By facilitating the development of fair ML practices, Aequitas Flow seeks to enhance the adoption of these concepts in AI technologies.","sentences":["Aequitas Flow is an open-source framework for end-to-end Fair Machine Learning (ML) experimentation in Python.","This package fills the existing integration gaps in other Fair ML packages of complete and accessible experimentation.","It provides a pipeline for fairness-aware model training, hyperparameter optimization, and evaluation, enabling rapid and simple experiments and result analysis.","Aimed at ML practitioners and researchers, the framework offers implementations of methods, datasets, metrics, and standard interfaces for these components to improve extensibility.","By facilitating the development of fair ML practices, Aequitas Flow seeks to enhance the adoption of these concepts in AI technologies."],"url":"http://arxiv.org/abs/2405.05809v1","category":"cs.LG"}
{"created":"2024-05-09 14:42:16","title":"MasterWeaver: Taming Editability and Identity for Personalized Text-to-Image Generation","abstract":"Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver.","sentences":["Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images.","Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues.","The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces.","In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability.","Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention.","To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model.","Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability.","Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability.","Our code will be publicly available at https://github.com/csyxwei/MasterWeaver."],"url":"http://arxiv.org/abs/2405.05806v1","category":"cs.CV"}
{"created":"2024-05-09 14:39:03","title":"Attochaos I: The classically chaotic postcursor of high harmonic generation","abstract":"Attosecond physics provides unique insights into light-matter interaction on ultrafast time scales. Its core phenomenon, High Harmonic Generation (HHG), is often described by a classical recollision model, the simple-man or three-step model, where the atomic potential is disregarded. Many features are already well explained using this model; however, the simplicity of the model does not allow the possibility of classical chaotic motion. We show that beyond this model, classical chaotic motion does exist albeit on timescales that are generally longer than the first recollision time. Chaos is analyzed using tools from the theory of dynamical systems, such as Lyapunov exponents and stroboscopic maps. The calculations are done for a one-dimensional Coulomb potential subjected to a linearly polarized electric field.","sentences":["Attosecond physics provides unique insights into light-matter interaction on ultrafast time scales.","Its core phenomenon, High Harmonic Generation (HHG), is often described by a classical recollision model, the simple-man or three-step model, where the atomic potential is disregarded.","Many features are already well explained using this model; however, the simplicity of the model does not allow the possibility of classical chaotic motion.","We show that beyond this model, classical chaotic motion does exist albeit on timescales that are generally longer than the first recollision time.","Chaos is analyzed using tools from the theory of dynamical systems, such as Lyapunov exponents and stroboscopic maps.","The calculations are done for a one-dimensional Coulomb potential subjected to a linearly polarized electric field."],"url":"http://arxiv.org/abs/2405.05804v1","category":"physics.class-ph"}
{"created":"2024-05-09 14:38:53","title":"Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference","abstract":"Multimodal large language models (MLLMs) demand considerable computations for inference due to the extensive parameters and the additional input tokens needed for visual information representation. Herein, we introduce Visual Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid inference. Our approach is inspired by two intriguing phenomena we have observed: (1) the attention sink phenomenon that is prevalent in LLMs also persists in MLLMs, suggesting that initial tokens and nearest tokens receive the majority of attention, while middle vision tokens garner minimal attention in deep layers; (2) the presence of information migration, which implies that visual information is transferred to subsequent text tokens within the first few layers of MLLMs. As per our findings, we conclude that vision tokens are not necessary in the deep layers of MLLMs. Thus, we strategically withdraw them at a certain layer, enabling only text tokens to engage in subsequent layers. To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze a limited set of tiny datasets and choose the first layer that meets the Kullback-Leibler divergence criterion. Our VTW approach can cut computational overhead by over 40\\% across diverse multimodal tasks while maintaining performance. Our code is released at https://github.com/lzhxmu/VTW.","sentences":["Multimodal large language models (MLLMs) demand considerable computations for inference due to the extensive parameters and the additional input tokens needed for visual information representation.","Herein, we introduce Visual Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid inference.","Our approach is inspired by two intriguing phenomena we have observed: (1) the attention sink phenomenon that is prevalent in LLMs also persists in MLLMs, suggesting that initial tokens and nearest tokens receive the majority of attention, while middle vision tokens garner minimal attention in deep layers; (2) the presence of information migration, which implies that visual information is transferred to subsequent text tokens within the first few layers of MLLMs.","As per our findings, we conclude that vision tokens are not necessary in the deep layers of MLLMs.","Thus, we strategically withdraw them at a certain layer, enabling only text tokens to engage in subsequent layers.","To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze a limited set of tiny datasets and choose the first layer that meets the Kullback-Leibler divergence criterion.","Our VTW approach can cut computational overhead by over 40\\% across diverse multimodal tasks while maintaining performance.","Our code is released at https://github.com/lzhxmu/VTW."],"url":"http://arxiv.org/abs/2405.05803v1","category":"cs.CV"}
{"created":"2024-05-09 14:37:08","title":"Deploying Graph Neural Networks in Wireless Networks: A Link Stability Viewpoint","abstract":"As an emerging artificial intelligence technology, graph neural networks (GNNs) have exhibited promising performance across a wide range of graph-related applications. However, information exchanges among neighbor nodes in GNN pose new challenges in the resource-constrained scenario, especially in wireless systems. In practical wireless systems, the communication links among nodes are usually unreliable due to wireless fading and receiver noise, consequently resulting in performance degradation of GNNs. To improve the learning performance of GNNs, we aim to maximize the number of long-term average (LTA) communication links by the optimized power control under energy consumption constraints. Using the Lyapunov optimization method, we first transform the intractable long-term problem into a deterministic problem in each time slot by converting the long-term energy constraints into the objective function. In spite of this non-convex combinatorial optimization problem, we address this problem via equivalently solving a sequence of convex feasibility problems together with a greedy based solver. Simulation results demonstrate the superiority of our proposed scheme over the baselines.","sentences":["As an emerging artificial intelligence technology, graph neural networks (GNNs) have exhibited promising performance across a wide range of graph-related applications.","However, information exchanges among neighbor nodes in GNN pose new challenges in the resource-constrained scenario, especially in wireless systems.","In practical wireless systems, the communication links among nodes are usually unreliable due to wireless fading and receiver noise, consequently resulting in performance degradation of GNNs.","To improve the learning performance of GNNs, we aim to maximize the number of long-term average (LTA) communication links by the optimized power control under energy consumption constraints.","Using the Lyapunov optimization method, we first transform the intractable long-term problem into a deterministic problem in each time slot by converting the long-term energy constraints into the objective function.","In spite of this non-convex combinatorial optimization problem, we address this problem via equivalently solving a sequence of convex feasibility problems together with a greedy based solver.","Simulation results demonstrate the superiority of our proposed scheme over the baselines."],"url":"http://arxiv.org/abs/2405.05802v1","category":"cs.DC"}
{"created":"2024-05-09 14:34:05","title":"DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation","abstract":"User-friendly 3D object editing is a challenging task that has attracted significant attention recently. The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing. While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing. In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks. Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input. This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing. Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments.","sentences":["User-friendly 3D object editing is a challenging task that has attracted significant attention recently.","The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing.","While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing.","In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks.","Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input.","This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing.","Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments."],"url":"http://arxiv.org/abs/2405.05800v1","category":"cs.GR"}
{"created":"2024-05-09 14:25:25","title":"Enhancing Suicide Risk Detection on Social Media through Semi-Supervised Deep Label Smoothing","abstract":"Suicide is a prominent issue in society. Unfortunately, many people at risk for suicide do not receive the support required. Barriers to people receiving support include social stigma and lack of access to mental health care. With the popularity of social media, people have turned to online forums, such as Reddit to express their feelings and seek support. This provides the opportunity to support people with the aid of artificial intelligence. Social media posts can be classified, using text classification, to help connect people with professional help. However, these systems fail to account for the inherent uncertainty in classifying mental health conditions. Unlike other areas of healthcare, mental health conditions have no objective measurements of disease often relying on expert opinion. Thus when formulating deep learning problems involving mental health, using hard, binary labels does not accurately represent the true nature of the data. In these settings, where human experts may disagree, fuzzy or soft labels may be more appropriate. The current work introduces a novel label smoothing method which we use to capture any uncertainty within the data. We test our approach on a five-label multi-class classification problem. We show, our semi-supervised deep label smoothing method improves classification accuracy above the existing state of the art. Where existing research reports an accuracy of 43\\% on the Reddit C-SSRS dataset, using empirical experiments to evaluate our novel label smoothing method, we improve upon this existing benchmark to 52\\%. These improvements in model performance have the potential to better support those experiencing mental distress. Future work should explore the use of probabilistic methods in both natural language processing and quantifying contributions of both epistemic and aleatoric uncertainty in noisy datasets.","sentences":["Suicide is a prominent issue in society.","Unfortunately, many people at risk for suicide do not receive the support required.","Barriers to people receiving support include social stigma and lack of access to mental health care.","With the popularity of social media, people have turned to online forums, such as Reddit to express their feelings and seek support.","This provides the opportunity to support people with the aid of artificial intelligence.","Social media posts can be classified, using text classification, to help connect people with professional help.","However, these systems fail to account for the inherent uncertainty in classifying mental health conditions.","Unlike other areas of healthcare, mental health conditions have no objective measurements of disease often relying on expert opinion.","Thus when formulating deep learning problems involving mental health, using hard, binary labels does not accurately represent the true nature of the data.","In these settings, where human experts may disagree, fuzzy or soft labels may be more appropriate.","The current work introduces a novel label smoothing method which we use to capture any uncertainty within the data.","We test our approach on a five-label multi-class classification problem.","We show, our semi-supervised deep label smoothing method improves classification accuracy above the existing state of the art.","Where existing research reports an accuracy of 43\\% on the Reddit C-SSRS dataset, using empirical experiments to evaluate our novel label smoothing method, we improve upon this existing benchmark to 52\\%.","These improvements in model performance have the potential to better support those experiencing mental distress.","Future work should explore the use of probabilistic methods in both natural language processing and quantifying contributions of both epistemic and aleatoric uncertainty in noisy datasets."],"url":"http://arxiv.org/abs/2405.05795v1","category":"cs.LG"}
{"created":"2024-05-09 14:20:54","title":"Quantum vs. classical $P$-divisibility","abstract":"$P$-divisibility is a central concept in both classical and quantum non-Markovian processes; in particular, it is strictly related to the notion of information backflow. When restricted to a fixed commutative algebra generated by a complete set of orthogonal projections, any quantum dynamics naturally provides a classical stochastic process. It is indeed well known that a quantum generator gives rise to a $P$-divisible quantum dynamics if and only if all its possible classical reductions give rise to divisible classical stochastic processes. Yet, this property does not hold if one classically reduces the quantum dynamical maps instead of their generators: for a unitary dynamics, as an example, $P$-divisibility of its classical reduction is inevitably lost, which is thus, non-Markovian and exhibits information backflow. Instead, for some important classes of purely dissipative evolutions, quantum $P$-divisibility always implies classical $P$-divisibility and thus lack of information backflow both in the quantum and classical scenarios. On the contrary, for a wide class of orthogonally covariant qubit dynamics, we show that loss of classical $P$-divisibility can originate from the classical reduction of a purely dissipative $P$-divisible quantum dynamics as in the unitary case. Moreover, such an effect can be interpreted in terms of information backflow, the information coming in being stored in the coherences of the time-evolving quantum state.","sentences":["$P$-divisibility is a central concept in both classical and quantum non-Markovian processes; in particular, it is strictly related to the notion of information backflow.","When restricted to a fixed commutative algebra generated by a complete set of orthogonal projections, any quantum dynamics naturally provides a classical stochastic process.","It is indeed well known that a quantum generator gives rise to a $P$-divisible quantum dynamics if and only if all its possible classical reductions give rise to divisible classical stochastic processes.","Yet, this property does not hold if one classically reduces the quantum dynamical maps instead of their generators: for a unitary dynamics, as an example, $P$-divisibility of its classical reduction is inevitably lost, which is thus, non-Markovian and exhibits information backflow.","Instead, for some important classes of purely dissipative evolutions, quantum $P$-divisibility always implies classical $P$-divisibility and thus lack of information backflow both in the quantum and classical scenarios.","On the contrary, for a wide class of orthogonally covariant qubit dynamics, we show that loss of classical $P$-divisibility can originate from the classical reduction of a purely dissipative $P$-divisible quantum dynamics as in the unitary case.","Moreover, such an effect can be interpreted in terms of information backflow, the information coming in being stored in the coherences of the time-evolving quantum state."],"url":"http://arxiv.org/abs/2405.05794v1","category":"quant-ph"}
{"created":"2024-05-09 14:17:26","title":"RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation","abstract":"Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on \"image segments\", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a \"continuous sense of a place\", defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of \"hops over segments\" and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/","sentences":["Mapping is crucial for spatial reasoning, planning and robot navigation.","Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity.","In this paper, we propose a novel topological representation of an environment based on \"image segments\", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features.","Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids.","This unveils a \"continuous sense of a place\", defined by inter-image persistence of segments along with their intra-image neighbours.","It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval.","Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of \"hops over segments\" and ii) search for target objects using natural language queries describing spatial relations of objects.","Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place.","Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation.","Project page with supplementary details: oravus.github.io/RoboHop/"],"url":"http://arxiv.org/abs/2405.05792v1","category":"cs.RO"}
{"created":"2024-05-09 14:15:00","title":"A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties","abstract":"In this paper, we present a robust version of the well-known exact low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA, to localize brain sources in the presence of different forward model uncertainties. Methods: We first assume that the true lead field matrix is a transformation of the existing lead field matrix distorted by uncertainties and propose an iterative approach to estimate this transformation accurately. Major sources of the forward model uncertainties, including differences in geometry, conductivity, and source space resolution between the real and simulated head models, and misaligned electrode positions, are then simulated to test the proposed method. Results: ReLORETA and eLORETA are applied to simulated focal sources in different regions of the brain and the presence of various noise levels as well as real data from a patient with focal epilepsy. The results show that ReLORETA is considerably more robust and accurate than eLORETA in all cases. Conclusion: Having successfully dealt with the forward model uncertainties, ReLORETA proved to be a promising method for real-world clinical applications. Significance: eLORETA is one of the localization techniques that could be used to study brain activity for medical applications such as determining the epileptogenic zone in patients with medically refractory epilepsy. However, the major limitation of eLORETA is sensitivity to the uncertainties in the forward model. Since this problem can substantially undermine its performance in real-world applications where the exact lead field matrix is unknown, developing a more robust method capable of dealing with these uncertainties is of significant interest.","sentences":["In this paper, we present a robust version of the well-known exact low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA, to localize brain sources in the presence of different forward model uncertainties.","Methods: We first assume that the true lead field matrix is a transformation of the existing lead field matrix distorted by uncertainties and propose an iterative approach to estimate this transformation accurately.","Major sources of the forward model uncertainties, including differences in geometry, conductivity, and source space resolution between the real and simulated head models, and misaligned electrode positions, are then simulated to test the proposed method.","Results: ReLORETA and eLORETA are applied to simulated focal sources in different regions of the brain and the presence of various noise levels as well as real data from a patient with focal epilepsy.","The results show that ReLORETA is considerably more robust and accurate than eLORETA in all cases.","Conclusion: Having successfully dealt with the forward model uncertainties, ReLORETA proved to be a promising method for real-world clinical applications.","Significance: eLORETA is one of the localization techniques that could be used to study brain activity for medical applications such as determining the epileptogenic zone in patients with medically refractory epilepsy.","However, the major limitation of eLORETA is sensitivity to the uncertainties in the forward model.","Since this problem can substantially undermine its performance in real-world applications where the exact lead field matrix is unknown, developing a more robust method capable of dealing with these uncertainties is of significant interest."],"url":"http://arxiv.org/abs/2405.05790v1","category":"cs.CE"}
{"created":"2024-05-09 14:11:20","title":"Autonomous Robotic Ultrasound System for Liver Follow-up Diagnosis: Pilot Phantom Study","abstract":"The paper introduces a novel autonomous robot ultrasound (US) system targeting liver follow-up scans for outpatients in local communities. Given a computed tomography (CT) image with specific target regions of interest, the proposed system carries out the autonomous follow-up scan in three steps: (i) initial robot contact to surface, (ii) coordinate mapping between CT image and robot, and (iii) target US scan. Utilizing 3D US-CT registration and deep learning-based segmentation networks, we can achieve precise imaging of 3D hepatic veins, facilitating accurate coordinate mapping between CT and the robot. This enables the automatic localization of follow-up targets within the CT image, allowing the robot to navigate precisely to the target's surface. Evaluation of the ultrasound phantom confirms the quality of the US-CT registration and shows the robot reliably locates the targets in repeated trials. The proposed framework holds the potential to significantly reduce time and costs for healthcare providers, clinicians, and follow-up patients, thereby addressing the increasing healthcare burden associated with chronic disease in local communities.","sentences":["The paper introduces a novel autonomous robot ultrasound (US) system targeting liver follow-up scans for outpatients in local communities.","Given a computed tomography (CT) image with specific target regions of interest, the proposed system carries out the autonomous follow-up scan in three steps: (i) initial robot contact to surface, (ii) coordinate mapping between CT image and robot, and (iii) target US scan.","Utilizing 3D US-CT registration and deep learning-based segmentation networks, we can achieve precise imaging of 3D hepatic veins, facilitating accurate coordinate mapping between CT and the robot.","This enables the automatic localization of follow-up targets within the CT image, allowing the robot to navigate precisely to the target's surface.","Evaluation of the ultrasound phantom confirms the quality of the US-CT registration and shows the robot reliably locates the targets in repeated trials.","The proposed framework holds the potential to significantly reduce time and costs for healthcare providers, clinicians, and follow-up patients, thereby addressing the increasing healthcare burden associated with chronic disease in local communities."],"url":"http://arxiv.org/abs/2405.05787v1","category":"cs.RO"}
{"created":"2024-05-09 14:09:36","title":"FusionTransNet for Smart Urban Mobility: Spatiotemporal Traffic Forecasting Through Multimodal Network Integration","abstract":"This study develops FusionTransNet, a framework designed for Origin-Destination (OD) flow predictions within smart and multimodal urban transportation systems. Urban transportation complexity arises from the spatiotemporal interactions among various traffic modes. Motivated by analyzing multimodal data from Shenzhen, a framework that can dissect complicated spatiotemporal interactions between these modes, from the microscopic local level to the macroscopic city-wide perspective, is essential. The framework contains three core components: the Intra-modal Learning Module, the Inter-modal Learning Module, and the Prediction Decoder. The Intra-modal Learning Module is designed to analyze spatial dependencies within individual transportation modes, facilitating a granular understanding of single-mode spatiotemporal dynamics. The Inter-modal Learning Module extends this analysis, integrating data across different modes to uncover cross-modal interdependencies, by breaking down the interactions at both local and global scales. Finally, the Prediction Decoder synthesizes insights from the preceding modules to generate accurate OD flow predictions, translating complex multimodal interactions into forecasts. Empirical evaluations conducted in metropolitan contexts, including Shenzhen and New York, demonstrate FusionTransNet's superior predictive accuracy compared to existing state-of-the-art methods. The implication of this study extends beyond urban transportation, as the method for transferring information across different spatiotemporal graphs at both local and global scales can be instrumental in other spatial systems, such as supply chain logistics and epidemics spreading.","sentences":["This study develops FusionTransNet, a framework designed for Origin-Destination (OD) flow predictions within smart and multimodal urban transportation systems.","Urban transportation complexity arises from the spatiotemporal interactions among various traffic modes.","Motivated by analyzing multimodal data from Shenzhen, a framework that can dissect complicated spatiotemporal interactions between these modes, from the microscopic local level to the macroscopic city-wide perspective, is essential.","The framework contains three core components: the Intra-modal Learning Module, the Inter-modal Learning Module, and the Prediction Decoder.","The Intra-modal Learning Module is designed to analyze spatial dependencies within individual transportation modes, facilitating a granular understanding of single-mode spatiotemporal dynamics.","The Inter-modal Learning Module extends this analysis, integrating data across different modes to uncover cross-modal interdependencies, by breaking down the interactions at both local and global scales.","Finally, the Prediction Decoder synthesizes insights from the preceding modules to generate accurate OD flow predictions, translating complex multimodal interactions into forecasts.","Empirical evaluations conducted in metropolitan contexts, including Shenzhen and New York, demonstrate FusionTransNet's superior predictive accuracy compared to existing state-of-the-art methods.","The implication of this study extends beyond urban transportation, as the method for transferring information across different spatiotemporal graphs at both local and global scales can be instrumental in other spatial systems, such as supply chain logistics and epidemics spreading."],"url":"http://arxiv.org/abs/2405.05786v1","category":"cs.LG"}
{"created":"2024-05-09 14:03:52","title":"Link Stealing Attacks Against Inductive Graph Neural Networks","abstract":"A graph neural network (GNN) is a type of neural network that is specifically designed to process graph-structured data. Typically, GNNs can be implemented in two settings, including the transductive setting and the inductive setting. In the transductive setting, the trained model can only predict the labels of nodes that were observed at the training time. In the inductive setting, the trained model can be generalized to new nodes/graphs. Due to its flexibility, the inductive setting is the most popular GNN setting at the moment. Previous work has shown that transductive GNNs are vulnerable to a series of privacy attacks. However, a comprehensive privacy analysis of inductive GNN models is still missing. This paper fills the gap by conducting a systematic privacy analysis of inductive GNNs through the lens of link stealing attacks, one of the most popular attacks that are specifically designed for GNNs. We propose two types of link stealing attacks, i.e., posterior-only attacks and combined attacks. We define threat models of the posterior-only attacks with respect to node topology and the combined attacks by considering combinations of posteriors, node attributes, and graph features. Extensive evaluation on six real-world datasets demonstrates that inductive GNNs leak rich information that enables link stealing attacks with advantageous properties. Even attacks with no knowledge about graph structures can be effective. We also show that our attacks are robust to different node similarities and different graph features. As a counterpart, we investigate two possible defenses and discover they are ineffective against our attacks, which calls for more effective defenses.","sentences":["A graph neural network (GNN) is a type of neural network that is specifically designed to process graph-structured data.","Typically, GNNs can be implemented in two settings, including the transductive setting and the inductive setting.","In the transductive setting, the trained model can only predict the labels of nodes that were observed at the training time.","In the inductive setting, the trained model can be generalized to new nodes/graphs.","Due to its flexibility, the inductive setting is the most popular GNN setting at the moment.","Previous work has shown that transductive GNNs are vulnerable to a series of privacy attacks.","However, a comprehensive privacy analysis of inductive GNN models is still missing.","This paper fills the gap by conducting a systematic privacy analysis of inductive GNNs through the lens of link stealing attacks, one of the most popular attacks that are specifically designed for GNNs.","We propose two types of link stealing attacks, i.e., posterior-only attacks and combined attacks.","We define threat models of the posterior-only attacks with respect to node topology and the combined attacks by considering combinations of posteriors, node attributes, and graph features.","Extensive evaluation on six real-world datasets demonstrates that inductive GNNs leak rich information that enables link stealing attacks with advantageous properties.","Even attacks with no knowledge about graph structures can be effective.","We also show that our attacks are robust to different node similarities and different graph features.","As a counterpart, we investigate two possible defenses and discover they are ineffective against our attacks, which calls for more effective defenses."],"url":"http://arxiv.org/abs/2405.05784v1","category":"cs.CR"}
{"created":"2024-05-09 13:54:22","title":"Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\u00e1mi Language","abstract":"S\\'ami, an indigenous language group comprising multiple languages, faces digital marginalization due to the limited availability of data and sophisticated language models designed for its linguistic intricacies. This work focuses on increasing technological participation for the S\\'ami language. We draw the attention of the ML community towards the language modeling problem of Ultra Low Resource (ULR) languages. ULR languages are those for which the amount of available textual resources is very low, and the speaker count for them is also very low. ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging. Mainstream AI foundational model development has given less attention to this category of languages. Generally, these languages have very few speakers, making it hard to find them. However, it is important to develop foundational models for these ULR languages to promote inclusion and the tangible abilities and impact of LLMs. To this end, we have compiled the available S\\'ami language resources from the web to create a clean dataset for training language models. In order to study the behavior of modern LLM models with ULR languages (S\\'ami), we have experimented with different kinds of LLMs, mainly at the order of $\\sim$ seven billion parameters. We have also explored the effect of multilingual LLM training for ULRLs. We found that the decoder-only models under a sequential multilingual training scenario perform better than joint multilingual training, whereas multilingual training with high semantic overlap, in general, performs better than training from scratch.This is the first study on the S\\'ami language for adapting non-statistical language models that use the latest developments in the field of natural language processing (NLP).","sentences":["S\\'ami, an indigenous language group comprising multiple languages, faces digital marginalization due to the limited availability of data and sophisticated language models designed for its linguistic intricacies.","This work focuses on increasing technological participation for the S\\'ami language.","We draw the attention of the ML community towards the language modeling problem of Ultra Low Resource (ULR) languages.","ULR languages are those for which the amount of available textual resources is very low, and the speaker count for them is also very low.","ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging.","Mainstream AI foundational model development has given less attention to this category of languages.","Generally, these languages have very few speakers, making it hard to find them.","However, it is important to develop foundational models for these ULR languages to promote inclusion and the tangible abilities and impact of LLMs.","To this end, we have compiled the available S\\'ami language resources from the web to create a clean dataset for training language models.","In order to study the behavior of modern LLM models with ULR languages (S\\'ami), we have experimented with different kinds of LLMs, mainly at the order of $\\sim$ seven billion parameters.","We have also explored the effect of multilingual LLM training for ULRLs.","We found that the decoder-only models under a sequential multilingual training scenario perform better than joint multilingual training, whereas multilingual training with high semantic overlap, in general, performs better than training from scratch.","This is the first study on the S\\'ami language for adapting non-statistical language models that use the latest developments in the field of natural language processing (NLP)."],"url":"http://arxiv.org/abs/2405.05777v1","category":"cs.CL"}
{"created":"2024-05-09 13:46:54","title":"Exploration of morphological coherence in open clusters with \"core-shell'' structure","abstract":"The study of their morphological coherence allows for a better understanding of the morphological evolution of open clusters. We employ the ellipsoid fitting method to delineate the 3D spatial structure of the sample clusters while using the morphological dislocation (MD) defined in our previous work and the ellipticity ratio (ER) of the clusters' inner and outer structures to characterize the morphological coherence of the sample clusters. The results show an inverse correlation between the ER of the sample clusters and the number of their members, indicating that sample clusters with much more elliptical external morphology than internal shape have generally a large number of members. Meanwhile, a slight shrinking of the MD of the sample clusters with their members' number may shed light on the significant role of the gravitational binding of the sample clusters in maintaining their morphological stability. Moreover, there are no correlations between the MD and ER of the sample clusters and their age. They are also not significantly correlated with the X-axis, the Y-axis, their orbital eccentricities, and the radial and vertical forces on them. However, the ER of the sample clusters displays some fluctuations in the distributions between it and the above covariates, implying that the morphologies of the sample clusters are sensitive to the external environment if sample effects are not taken into account. Finally, the analysis of the 3D spatial shapes of sample clusters with a small ER or a large ER demonstrates that the number of members lays an important foundation for forming a dense internal system for sample clusters. At the same time, the MD of the sample clusters can serve well as an indicator of their morphological stability, which is built on a certain amount of member stars.","sentences":["The study of their morphological coherence allows for a better understanding of the morphological evolution of open clusters.","We employ the ellipsoid fitting method to delineate the 3D spatial structure of the sample clusters while using the morphological dislocation (MD) defined in our previous work and the ellipticity ratio (ER) of the clusters' inner and outer structures to characterize the morphological coherence of the sample clusters.","The results show an inverse correlation between the ER of the sample clusters and the number of their members, indicating that sample clusters with much more elliptical external morphology than internal shape have generally a large number of members.","Meanwhile, a slight shrinking of the MD of the sample clusters with their members' number may shed light on the significant role of the gravitational binding of the sample clusters in maintaining their morphological stability.","Moreover, there are no correlations between the MD and ER of the sample clusters and their age.","They are also not significantly correlated with the X-axis, the Y-axis, their orbital eccentricities, and the radial and vertical forces on them.","However, the ER of the sample clusters displays some fluctuations in the distributions between it and the above covariates, implying that the morphologies of the sample clusters are sensitive to the external environment if sample effects are not taken into account.","Finally, the analysis of the 3D spatial shapes of sample clusters with a small ER or a large ER demonstrates that the number of members lays an important foundation for forming a dense internal system for sample clusters.","At the same time, the MD of the sample clusters can serve well as an indicator of their morphological stability, which is built on a certain amount of member stars."],"url":"http://arxiv.org/abs/2405.05771v1","category":"astro-ph.GA"}
{"created":"2024-05-09 13:45:12","title":"A minimal dynamical system and analog circuit for non-associative learning","abstract":"Learning in living organisms is typically associated with networks of neurons. The use of large numbers of adjustable units has also been a crucial factor in the continued success of artificial neural networks. In light of the complexity of both living and artificial neural networks, it is surprising to see that very simple organisms -- even unicellular organisms that do not possess a nervous system -- are capable of certain forms of learning. Since in these cases learning may be implemented with much simpler structures than neural networks, it is natural to ask how simple the building blocks required for basic forms of learning may be. The purpose of this study is to discuss the simplest dynamical systems that model a fundamental form of non-associative learning, habituation, and to elucidate technical implementations of such systems, which may be used to implement non-associative learning in neuromorphic computing and related applications.","sentences":["Learning in living organisms is typically associated with networks of neurons.","The use of large numbers of adjustable units has also been a crucial factor in the continued success of artificial neural networks.","In light of the complexity of both living and artificial neural networks, it is surprising to see that very simple organisms -- even unicellular organisms that do not possess a nervous system -- are capable of certain forms of learning.","Since in these cases learning may be implemented with much simpler structures than neural networks, it is natural to ask how simple the building blocks required for basic forms of learning may be.","The purpose of this study is to discuss the simplest dynamical systems that model a fundamental form of non-associative learning, habituation, and to elucidate technical implementations of such systems, which may be used to implement non-associative learning in neuromorphic computing and related applications."],"url":"http://arxiv.org/abs/2405.05770v1","category":"q-bio.NC"}
{"created":"2024-05-09 13:45:04","title":"Exploring Text-Guided Single Image Editing for Remote Sensing Images","abstract":"Artificial Intelligence Generative Content (AIGC) technologies have significantly influenced the remote sensing domain, particularly in the realm of image generation. However, remote sensing image editing, an equally vital research area, has not garnered sufficient attention. Different from text-guided editing in natural images, which relies on extensive text-image paired data for semantic correlation, the application scenarios of remote sensing image editing are often extreme, such as forest on fire, so it is difficult to obtain sufficient paired samples. At the same time, the lack of remote sensing semantics and the ambiguity of text also restrict the further application of image editing in remote sensing field. To solve above problems, this letter proposes a diffusion based method to fulfill stable and controllable remote sensing image editing with text guidance. Our method avoids the use of a large number of paired image, and can achieve good image editing results using only a single image. The quantitative evaluation system including CLIP score and subjective evaluation metrics shows that our method has better editing effect on remote sensing images than the existing image editing model.","sentences":["Artificial Intelligence Generative Content (AIGC) technologies have significantly influenced the remote sensing domain, particularly in the realm of image generation.","However, remote sensing image editing, an equally vital research area, has not garnered sufficient attention.","Different from text-guided editing in natural images, which relies on extensive text-image paired data for semantic correlation, the application scenarios of remote sensing image editing are often extreme, such as forest on fire, so it is difficult to obtain sufficient paired samples.","At the same time, the lack of remote sensing semantics and the ambiguity of text also restrict the further application of image editing in remote sensing field.","To solve above problems, this letter proposes a diffusion based method to fulfill stable and controllable remote sensing image editing with text guidance.","Our method avoids the use of a large number of paired image, and can achieve good image editing results using only a single image.","The quantitative evaluation system including CLIP score and subjective evaluation metrics shows that our method has better editing effect on remote sensing images than the existing image editing model."],"url":"http://arxiv.org/abs/2405.05769v1","category":"cs.CV"}
{"created":"2024-05-09 13:44:16","title":"FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic Gaussian Splatting","abstract":"Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation.","sentences":["Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications.","Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences.","However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users.","Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality.","To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency.","Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints.","To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality.","Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction.","Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency.","Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation."],"url":"http://arxiv.org/abs/2405.05768v1","category":"cs.CV"}
{"created":"2024-05-09 13:44:04","title":"Large Language Model-Aided Evolutionary Search for Constrained Multiobjective Optimization","abstract":"Evolutionary algorithms excel in solving complex optimization problems, especially those with multiple objectives. However, their stochastic nature can sometimes hinder rapid convergence to the global optima, particularly in scenarios involving constraints. In this study, we employ a large language model (LLM) to enhance evolutionary search for solving constrained multi-objective optimization problems. Our aim is to speed up the convergence of the evolutionary population. To achieve this, we finetune the LLM through tailored prompt engineering, integrating information concerning both objective values and constraint violations of solutions. This process enables the LLM to grasp the relationship between well-performing and poorly performing solutions based on the provided input data. Solution's quality is assessed based on their constraint violations and objective-based performance. By leveraging the refined LLM, it can be used as a search operator to generate superior-quality solutions. Experimental evaluations across various test benchmarks illustrate that LLM-aided evolutionary search can significantly accelerate the population's convergence speed and stands out competitively against cutting-edge evolutionary algorithms.","sentences":["Evolutionary algorithms excel in solving complex optimization problems, especially those with multiple objectives.","However, their stochastic nature can sometimes hinder rapid convergence to the global optima, particularly in scenarios involving constraints.","In this study, we employ a large language model (LLM) to enhance evolutionary search for solving constrained multi-objective optimization problems.","Our aim is to speed up the convergence of the evolutionary population.","To achieve this, we finetune the LLM through tailored prompt engineering, integrating information concerning both objective values and constraint violations of solutions.","This process enables the LLM to grasp the relationship between well-performing and poorly performing solutions based on the provided input data.","Solution's quality is assessed based on their constraint violations and objective-based performance.","By leveraging the refined LLM, it can be used as a search operator to generate superior-quality solutions.","Experimental evaluations across various test benchmarks illustrate that LLM-aided evolutionary search can significantly accelerate the population's convergence speed and stands out competitively against cutting-edge evolutionary algorithms."],"url":"http://arxiv.org/abs/2405.05767v1","category":"cs.NE"}
{"created":"2024-05-09 13:42:54","title":"On the mixing between flavor singlets in lattice gauge theories coupled to matter fields in multiple representations","abstract":"We provide the first extensive, numerical study of the non-trivial problem of mixing between flavor-singlet composite states emerging in strongly coupled lattice field theories with matter field content consisting of fermions transforming in different representations of the gauge group. The theory of interest is the minimal candidate for a composite Higgs model that also accommodates a mechanism for top partial compositeness: the $Sp(4)$ gauge theory coupled to two (Dirac) fermions transforming as the fundamental and three as the two-index antisymmetric representation of the gauge group, respectively. We apply an admixture of APE and Wuppertal smearings, as well as the generalized eigenvalue problem approach, to two-point functions involving flavor-singlet mesons, for ensembles having time extent longer than the space extent. We demonstrate that, in the region of lattice parameter space accessible to this study, both masses and mixing angles can be measured effectively, despite the presence of (numerically noisy) contributions from disconnected diagrams.","sentences":["We provide the first extensive, numerical study of the non-trivial problem of mixing between flavor-singlet composite states emerging in strongly coupled lattice field theories with matter field content consisting of fermions transforming in different representations of the gauge group.","The theory of interest is the minimal candidate for a composite Higgs model that also accommodates a mechanism for top partial compositeness: the $Sp(4)$ gauge theory coupled to two (Dirac) fermions transforming as the fundamental and three as the two-index antisymmetric representation of the gauge group, respectively.","We apply an admixture of APE and Wuppertal smearings, as well as the generalized eigenvalue problem approach, to two-point functions involving flavor-singlet mesons, for ensembles having time extent longer than the space extent.","We demonstrate that, in the region of lattice parameter space accessible to this study, both masses and mixing angles can be measured effectively, despite the presence of (numerically noisy) contributions from disconnected diagrams."],"url":"http://arxiv.org/abs/2405.05765v1","category":"hep-lat"}
{"created":"2024-05-09 13:42:54","title":"To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems","abstract":"The increasing reliance on Deep Learning models, combined with their inherent lack of transparency, has spurred the development of a novel field of study known as eXplainable AI (XAI) methods. These methods seek to enhance the trust of end-users in automated systems by providing insights into the rationale behind their decisions. This paper presents a novel approach for measuring user trust in XAI systems, allowing their refinement. Our proposed metric combines both performance metrics and trust indicators from an objective perspective. To validate this novel methodology, we conducted a case study in a realistic medical scenario: the usage of XAI system for the detection of pneumonia from x-ray images.","sentences":["The increasing reliance on Deep Learning models, combined with their inherent lack of transparency, has spurred the development of a novel field of study known as eXplainable AI (XAI) methods.","These methods seek to enhance the trust of end-users in automated systems by providing insights into the rationale behind their decisions.","This paper presents a novel approach for measuring user trust in XAI systems, allowing their refinement.","Our proposed metric combines both performance metrics and trust indicators from an objective perspective.","To validate this novel methodology, we conducted a case study in a realistic medical scenario: the usage of XAI system for the detection of pneumonia from x-ray images."],"url":"http://arxiv.org/abs/2405.05766v1","category":"cs.CV"}
{"created":"2024-05-09 13:37:18","title":"DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models","abstract":"Detail features of magnetic resonance images play a cru-cial role in accurate medical diagnosis and treatment, as they capture subtle changes that pose challenges for doc-tors when performing precise judgments. However, the widely utilized naive diffusion model has limitations, as it fails to accurately capture more intricate details. To en-hance the quality of MRI reconstruction, we propose a comprehensive detail-preserving reconstruction method using multiple diffusion models to extract structure and detail features in k-space domain instead of image do-main. Moreover, virtual binary modal masks are utilized to refine the range of values in k-space data through highly adaptive center windows, which allows the model to focus its attention more efficiently. Last but not least, an inverted pyramid structure is employed, where the top-down image information gradually decreases, ena-bling a cascade representation. The framework effective-ly represents multi-scale sampled data, taking into ac-count the sparsity of the inverted pyramid architecture, and utilizes cascade training data distribution to repre-sent multi-scale data. Through a step-by-step refinement approach, the method refines the approximation of de-tails. Finally, the proposed method was evaluated by con-ducting experiments on clinical and public datasets. The results demonstrate that the proposed method outper-forms other methods.","sentences":["Detail features of magnetic resonance images play a cru-cial role in accurate medical diagnosis and treatment, as they capture subtle changes that pose challenges for doc-tors when performing precise judgments.","However, the widely utilized naive diffusion model has limitations, as it fails to accurately capture more intricate details.","To en-hance the quality of MRI reconstruction, we propose a comprehensive detail-preserving reconstruction method using multiple diffusion models to extract structure and detail features in k-space domain instead of image do-main.","Moreover, virtual binary modal masks are utilized to refine the range of values in k-space data through highly adaptive center windows, which allows the model to focus its attention more efficiently.","Last but not least, an inverted pyramid structure is employed, where the top-down image information gradually decreases, ena-bling a cascade representation.","The framework effective-ly represents multi-scale sampled data, taking into ac-count the sparsity of the inverted pyramid architecture, and utilizes cascade training data distribution to repre-sent multi-scale data.","Through a step-by-step refinement approach, the method refines the approximation of de-tails.","Finally, the proposed method was evaluated by con-ducting experiments on clinical and public datasets.","The results demonstrate that the proposed method outper-forms other methods."],"url":"http://arxiv.org/abs/2405.05763v1","category":"cs.CV"}
{"created":"2024-05-09 13:36:23","title":"Introducing two improved methods for approximating radiative cooling in hydrodynamical simulations of accretion discs","abstract":"The evolution of many astrophysical systems depends strongly on the balance between heating and cooling, in particular star formation in giant molecular clouds and the evolution of young protostellar systems. Protostellar discs are susceptible to the gravitational instability, which can play a key role in their evolution and in planet formation. The strength of the instability depends on the rate at which the system loses thermal energy. To study the evolution of these systems, we require radiative cooling approximations because full radiative transfer is generally too expensive to be coupled to hydrodynamical models. Here we present two new approximate methods for computing radiative cooling that make use of the polytropic cooling approximation. This approach invokes the assumption that each parcel of gas is located within a spherical pseudo-cloud which can then be used to approximate the optical depth. The first method combines the methods introduced by Stamatellos et al. and Lombardi et al. to overcome the limitations of each method at low and high optical depths respectively. The second, the \"Modified Lombardi\" method, is specifically tailored for self-gravitating discs. This modifies the scale height estimate from the method of Lombardi et al. using the analytical scale height for a self-gravitating disc. We show that the Modified Lombardi method provides an excellent approximation for the column density in a fragmenting disc, a regime in which the existing methods fail to recover the clumps and spiral structures. We therefore recommend this improved radiative cooling method for more realistic simulations of self-gravitating discs.","sentences":["The evolution of many astrophysical systems depends strongly on the balance between heating and cooling, in particular star formation in giant molecular clouds and the evolution of young protostellar systems.","Protostellar discs are susceptible to the gravitational instability, which can play a key role in their evolution and in planet formation.","The strength of the instability depends on the rate at which the system loses thermal energy.","To study the evolution of these systems, we require radiative cooling approximations because full radiative transfer is generally too expensive to be coupled to hydrodynamical models.","Here we present two new approximate methods for computing radiative cooling that make use of the polytropic cooling approximation.","This approach invokes the assumption that each parcel of gas is located within a spherical pseudo-cloud which can then be used to approximate the optical depth.","The first method combines the methods introduced by Stamatellos et al. and Lombardi et al. to overcome the limitations of each method at low and high optical depths respectively.","The second, the \"Modified Lombardi\" method, is specifically tailored for self-gravitating discs.","This modifies the scale height estimate from the method of Lombardi et al. using the analytical scale height for a self-gravitating disc.","We show that the Modified Lombardi method provides an excellent approximation for the column density in a fragmenting disc, a regime in which the existing methods fail to recover the clumps and spiral structures.","We therefore recommend this improved radiative cooling method for more realistic simulations of self-gravitating discs."],"url":"http://arxiv.org/abs/2405.05762v1","category":"astro-ph.SR"}
{"created":"2024-05-09 13:28:07","title":"Advancing Distribution Decomposition Methods Beyond Common Supports: Applications to Racial Wealth Disparities","abstract":"I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion. The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support. I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables. Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region. On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution.","sentences":["I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion.","The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support.","I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables.","Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region.","On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution."],"url":"http://arxiv.org/abs/2405.05759v1","category":"econ.EM"}
{"created":"2024-05-09 13:27:22","title":"Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma","abstract":"Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI). Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored. To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research. The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights. We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions. We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond.","sentences":["Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI).","Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored.","To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research.","The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights.","We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions.","We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond."],"url":"http://arxiv.org/abs/2405.05758v1","category":"cs.HC"}
{"created":"2024-05-09 13:23:09","title":"Design and Implementation of Energy-Efficient Wireless Tire Sensing System with Delay Analysis for Intelligent Vehicles","abstract":"The growing prevalence of Internet of Things (IoT) technologies has led to a rise in the popularity of intelligent vehicles that incorporate a range of sensors to monitor various aspects, such as driving speed, fuel usage, distance proximity and tire anomalies. Nowadays, real-time tire sensing systems play important roles for intelligent vehicles in increasing mileage, reducing fuel consumption, improving driving safety, and reducing the potential for traffic accidents. However, the current tire sensing system drains a significant vehicle' energy and lacks effective collection of sensing data, which may not guarantee the immediacy of driving safety. Thus, this paper designs an energy-efficient wireless tire sensing system (WTSS), which leverages energy-saving techniques to significantly reduce power consumption while ensuring data retrieval delays during real-time monitoring. Additionally, we mathematically analyze the worst-case transmission delay and sensor reception ratio of the system to ensure the immediacy based on the collision probabilities of sensor transmissions. This system has been implemented and verified by the simulation and field train experiments. These results show that the proposed scheme provides enhanced performance in energy efficiency up to 76.5% in average and identifies the worst transmission delay accurately.","sentences":["The growing prevalence of Internet of Things (IoT) technologies has led to a rise in the popularity of intelligent vehicles that incorporate a range of sensors to monitor various aspects, such as driving speed, fuel usage, distance proximity and tire anomalies.","Nowadays, real-time tire sensing systems play important roles for intelligent vehicles in increasing mileage, reducing fuel consumption, improving driving safety, and reducing the potential for traffic accidents.","However, the current tire sensing system drains a significant vehicle' energy and lacks effective collection of sensing data, which may not guarantee the immediacy of driving safety.","Thus, this paper designs an energy-efficient wireless tire sensing system (WTSS), which leverages energy-saving techniques to significantly reduce power consumption while ensuring data retrieval delays during real-time monitoring.","Additionally, we mathematically analyze the worst-case transmission delay and sensor reception ratio of the system to ensure the immediacy based on the collision probabilities of sensor transmissions.","This system has been implemented and verified by the simulation and field train experiments.","These results show that the proposed scheme provides enhanced performance in energy efficiency up to 76.5% in average and identifies the worst transmission delay accurately."],"url":"http://arxiv.org/abs/2405.05757v1","category":"cs.ET"}
{"created":"2024-05-09 13:22:10","title":"Everything is Entangled in Quantum Mechanics: Are the Orthodox Measures Physically Meaningful?","abstract":"Even though quantum entanglement is today's most essential concept within the new technological era of quantum information processing, we do not only lack a consistent definition of this kernel notion, we are also far from understanding its physical meaning [35]. These failures have lead to many problems when attempting to provide a consistent measure or quantification of entanglement. In fact, the two main lines of contemporary research within the orthodox literature have created mazes where inconsistencies and problems are found everywhere. While the operational-instrumentalist approach has failed to explain how inequalities are able to distinguish the classical from the quantum, the geometrical approach has failed to provide a consistent meaningful account of their entropic measure. Taking distance from orthodoxy, in this work we address the quantification and measure of quantum entanglement by considering a recently presented objective-invariant definition in terms of the coding of intensive relations [21] which allows to escape the widespread relativist account of bases and factorizations [24, 25]. Going beyond the orthodox dualistic reference to \"quantum particles\" and \"clicks\" in detectors, we will argue that this new line of research is capable not only to evade the many open problems which appear within the mainstream literature, but is also able to present a consistent and coherent physical understanding of entanglement. The main conclusion of this work is that in quantum mechanics -- contrary to what is generally presupposed -- all operational expressions found within the laboratory are intrinsically entangled.","sentences":["Even though quantum entanglement is today's most essential concept within the new technological era of quantum information processing, we do not only lack a consistent definition of this kernel notion, we are also far from understanding its physical meaning [35].","These failures have lead to many problems when attempting to provide a consistent measure or quantification of entanglement.","In fact, the two main lines of contemporary research within the orthodox literature have created mazes where inconsistencies and problems are found everywhere.","While the operational-instrumentalist approach has failed to explain how inequalities are able to distinguish the classical from the quantum, the geometrical approach has failed to provide a consistent meaningful account of their entropic measure.","Taking distance from orthodoxy, in this work we address the quantification and measure of quantum entanglement by considering a recently presented objective-invariant definition in terms of the coding of intensive relations","[21] which allows to escape the widespread relativist account of bases and factorizations","[24, 25].","Going beyond the orthodox dualistic reference to \"quantum particles\" and \"clicks\" in detectors, we will argue that this new line of research is capable not only to evade the many open problems which appear within the mainstream literature, but is also able to present a consistent and coherent physical understanding of entanglement.","The main conclusion of this work is that in quantum mechanics -- contrary to what is generally presupposed -- all operational expressions found within the laboratory are intrinsically entangled."],"url":"http://arxiv.org/abs/2405.05756v1","category":"quant-ph"}
{"created":"2024-05-09 13:21:03","title":"CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks","abstract":"In recent years, convolutional neural networks (CNNs) with channel-wise feature refining mechanisms have brought noticeable benefits to modelling channel dependencies. However, current attention paradigms fail to infer an optimal channel descriptor capable of simultaneously exploiting statistical and spatial relationships among feature maps. In this paper, to overcome this shortcoming, we present a novel channel-wise spatially autocorrelated (CSA) attention mechanism. Inspired by geographical analysis, the proposed CSA exploits the spatial relationships between channels of feature maps to produce an effective channel descriptor. To the best of our knowledge, this is the f irst time that the concept of geographical spatial analysis is utilized in deep CNNs. The proposed CSA imposes negligible learning parameters and light computational overhead to the deep model, making it a powerful yet efficient attention module of choice. We validate the effectiveness of the proposed CSA networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and MS COCO benchmark datasets for image classification, object detection, and instance segmentation. The experimental results demonstrate that CSA-Nets are able to consistently achieve competitive performance and superior generalization than several state-of-the-art attention-based CNNs over different benchmark tasks and datasets.","sentences":["In recent years, convolutional neural networks (CNNs) with channel-wise feature refining mechanisms have brought noticeable benefits to modelling channel dependencies.","However, current attention paradigms fail to infer an optimal channel descriptor capable of simultaneously exploiting statistical and spatial relationships among feature maps.","In this paper, to overcome this shortcoming, we present a novel channel-wise spatially autocorrelated (CSA) attention mechanism.","Inspired by geographical analysis, the proposed CSA exploits the spatial relationships between channels of feature maps to produce an effective channel descriptor.","To the best of our knowledge, this is the f irst time that the concept of geographical spatial analysis is utilized in deep CNNs.","The proposed CSA imposes negligible learning parameters and light computational overhead to the deep model, making it a powerful yet efficient attention module of choice.","We validate the effectiveness of the proposed CSA networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and MS COCO benchmark datasets for image classification, object detection, and instance segmentation.","The experimental results demonstrate that CSA-Nets are able to consistently achieve competitive performance and superior generalization than several state-of-the-art attention-based CNNs over different benchmark tasks and datasets."],"url":"http://arxiv.org/abs/2405.05755v1","category":"cs.CV"}
{"created":"2024-05-09 13:18:22","title":"Manipulating Topological Polaritons in Optomechanical Ladders","abstract":"We propose to manipulate topological polaritons in optomechanical ladders consisting of an optical Su-Schrieffer-Heeger (SSH) chain and a mechanical SSH chain connected through optomechanical (interchain) interactions. We show that the topological phase diagrams are divided into six areas by four boundaries and that there are four topological phases characterized by the Berry phases. We find that a topologically nontrivial phase of the polaritons is generated by the optomechanical interaction between the optical and mechanical SSH chains even though they are both in the topologically trivial phases. Counter-intuitively, six edge states appear in one of the topological phases with only two topological nontrivial bands, and some edge states are localized near but not at the boundaries of an open-boundary ladder. Moreover, a two-dimensional Chern insulator with higher Chern numbers is simulated by introducing proper periodical adiabatic modulations of the driving amplitude and frequency. Our work not only opens a route towards topological polaritons manipulation by optomachanical interactions, but also will exert a far-reaching influence on designing topologically protected polaritonic devices.","sentences":["We propose to manipulate topological polaritons in optomechanical ladders consisting of an optical Su-Schrieffer-Heeger (SSH) chain and a mechanical SSH chain connected through optomechanical (interchain) interactions.","We show that the topological phase diagrams are divided into six areas by four boundaries and that there are four topological phases characterized by the Berry phases.","We find that a topologically nontrivial phase of the polaritons is generated by the optomechanical interaction between the optical and mechanical SSH chains even though they are both in the topologically trivial phases.","Counter-intuitively, six edge states appear in one of the topological phases with only two topological nontrivial bands, and some edge states are localized near but not at the boundaries of an open-boundary ladder.","Moreover, a two-dimensional Chern insulator with higher Chern numbers is simulated by introducing proper periodical adiabatic modulations of the driving amplitude and frequency.","Our work not only opens a route towards topological polaritons manipulation by optomachanical interactions, but also will exert a far-reaching influence on designing topologically protected polaritonic devices."],"url":"http://arxiv.org/abs/2405.05753v1","category":"quant-ph"}
{"created":"2024-05-09 13:17:07","title":"Refinements and Extensions of Ziv's Model of Perfect Secrecy for Individual Sequences","abstract":"We refine and extend Ziv's model and results regarding perfectly secure encryption of individual sequences. According to this model, the encrypter and the legitimate decrypter share in common a secret key, not shared with the unauthorized eavesdropper, who is aware of the encryption scheme and has some prior knowledge concerning the individual plaintext source sequence. This prior knowledge, combined with the cryptogram, is harnessed by eavesdropper which implements a finite-state machine as a mechanism for accepting or rejecting attempted guesses of the source plaintext. The encryption is considered perfectly secure if the cryptogram does not provide any new information to the eavesdropper that may enhance its knowledge concerning the plaintext beyond his prior knowledge. Ziv has shown that the key rate needed for perfect secrecy is essentially lower bounded by the finite-state compressibility of the plaintext sequence, a bound which is clearly asymptotically attained by Lempel-Ziv compression followed by one-time pad encryption. In this work, we consider some more general classes of finite-state eavesdroppers and derive the respective lower bounds on the key rates needed for perfect secrecy. These bounds are tighter and more refined than Ziv's bound and they are attained by encryption schemes that are based on different universal lossless compression schemes. We also extend our findings to the case where side information is available to the eavesdropper and the legitimate decrypter, but may or may not be available to the encrypter as well.","sentences":["We refine and extend Ziv's model and results regarding perfectly secure encryption of individual sequences.","According to this model, the encrypter and the legitimate decrypter share in common a secret key, not shared with the unauthorized eavesdropper, who is aware of the encryption scheme and has some prior knowledge concerning the individual plaintext source sequence.","This prior knowledge, combined with the cryptogram, is harnessed by eavesdropper which implements a finite-state machine as a mechanism for accepting or rejecting attempted guesses of the source plaintext.","The encryption is considered perfectly secure if the cryptogram does not provide any new information to the eavesdropper that may enhance its knowledge concerning the plaintext beyond his prior knowledge.","Ziv has shown that the key rate needed for perfect secrecy is essentially lower bounded by the finite-state compressibility of the plaintext sequence, a bound which is clearly asymptotically attained by Lempel-Ziv compression followed by one-time pad encryption.","In this work, we consider some more general classes of finite-state eavesdroppers and derive the respective lower bounds on the key rates needed for perfect secrecy.","These bounds are tighter and more refined than Ziv's bound and they are attained by encryption schemes that are based on different universal lossless compression schemes.","We also extend our findings to the case where side information is available to the eavesdropper and the legitimate decrypter, but may or may not be available to the encrypter as well."],"url":"http://arxiv.org/abs/2405.05752v1","category":"cs.IT"}
{"created":"2024-05-09 13:15:40","title":"A Multi-Level Superoptimizer for Tensor Programs","abstract":"We introduce Mirage, the first multi-level superoptimizer for tensor programs. A key idea in Mirage is $\\mu$Graphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized $\\mu$Graph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees. Our evaluation shows that Mirage outperforms existing approaches by up to 3.5$\\times$ even for DNNs that are widely used and heavily optimized. Mirage is publicly available at https://github.com/mirage-project/mirage.","sentences":["We introduce Mirage, the first multi-level superoptimizer for tensor programs.","A key idea in Mirage is $\\mu$Graphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy.","$\\mu$Graphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels.","To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee.","To ensure that the optimized $\\mu$Graph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees.","Our evaluation shows that Mirage outperforms existing approaches by up to 3.5$\\times$ even for DNNs that are widely used and heavily optimized.","Mirage is publicly available at https://github.com/mirage-project/mirage."],"url":"http://arxiv.org/abs/2405.05751v1","category":"cs.LG"}
{"created":"2024-05-09 13:14:42","title":"Quantum Fluctuations in the Interior of Black Holes and Backreactions","abstract":"We study the propagation of the quantum field perturbations in the interior of the Schwarzschild black hole. The interior of the black hole is like an anisotropic cosmological background which expands in one extended direction while contracting in azimuthal directions. Solving the quantum mode functions approximately, we calculate the expectation values of physical quantities such as the energy density $\\langle \\rho \\rangle$ and pressure $\\langle P\\rangle$ as measured by an observer in the interior of the black hole. We employ a combination of the $\\zeta$ function regularization and dimensional regularization schemes to regularize the UV divergences in the energy momentum tensor associated to these quantum perturbations. By solving the Einstein field equations, we calculate the quantum backreactions induced in the background geometry. We speculate that the effects of quantum fluctuations in the interior of the black hole can be measured by the exterior observer.","sentences":["We study the propagation of the quantum field perturbations in the interior of the Schwarzschild black hole.","The interior of the black hole is like an anisotropic cosmological background which expands in one extended direction while contracting in azimuthal directions.","Solving the quantum mode functions approximately, we calculate the expectation values of physical quantities such as the energy density $\\langle \\rho \\rangle$ and pressure $\\langle P\\rangle$ as measured by an observer in the interior of the black hole.","We employ a combination of the $\\zeta$ function regularization and dimensional regularization schemes to regularize the UV divergences in the energy momentum tensor associated to these quantum perturbations.","By solving the Einstein field equations, we calculate the quantum backreactions induced in the background geometry.","We speculate that the effects of quantum fluctuations in the interior of the black hole can be measured by the exterior observer."],"url":"http://arxiv.org/abs/2405.05750v1","category":"gr-qc"}
{"created":"2024-05-09 13:14:06","title":"NeRFFaceSpeech: One-shot Audio-diven 3D Talking Head Synthesis via Generative Prior","abstract":"Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively.","sentences":["Audio-driven talking head generation is advancing from 2D to 3D content.","Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs.","Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method.","Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image.","In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives.","We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head.","Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image.","Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion.","Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image.","The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data.","The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches.","In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively."],"url":"http://arxiv.org/abs/2405.05749v1","category":"cs.CV"}
{"created":"2024-05-09 13:13:34","title":"Learning to Slice Wi-Fi Networks: A State-Augmented Primal-Dual Approach","abstract":"Network slicing is a key feature in 5G/NG cellular networks that creates customized slices for different service types with various quality-of-service (QoS) requirements, which can achieve service differentiation and guarantee service-level agreement (SLA) for each service type. In Wi-Fi networks, there is limited prior work on slicing, and a potential solution is based on a multi-tenant architecture on a single access point (AP) that dedicates different channels to different slices. In this paper, we define a flexible, constrained learning framework to enable slicing in Wi-Fi networks subject to QoS requirements. We specifically propose an unsupervised learning-based network slicing method that leverages a state-augmented primal-dual algorithm, where a neural network policy is trained offline to optimize a Lagrangian function and the dual variable dynamics are updated online in the execution phase. We show that state augmentation is crucial for generating slicing decisions that meet the ergodic QoS requirements.","sentences":["Network slicing is a key feature in 5G/NG cellular networks that creates customized slices for different service types with various quality-of-service (QoS) requirements, which can achieve service differentiation and guarantee service-level agreement (SLA) for each service type.","In Wi-Fi networks, there is limited prior work on slicing, and a potential solution is based on a multi-tenant architecture on a single access point (AP) that dedicates different channels to different slices.","In this paper, we define a flexible, constrained learning framework to enable slicing in Wi-Fi networks subject to QoS requirements.","We specifically propose an unsupervised learning-based network slicing method that leverages a state-augmented primal-dual algorithm, where a neural network policy is trained offline to optimize a Lagrangian function and the dual variable dynamics are updated online in the execution phase.","We show that state augmentation is crucial for generating slicing decisions that meet the ergodic QoS requirements."],"url":"http://arxiv.org/abs/2405.05748v1","category":"eess.SP"}
{"created":"2024-05-09 13:10:54","title":"Efficient Pretraining Model based on Multi-Scale Local Visual Field Feature Reconstruction for PCB CT Image Element Segmentation","abstract":"Element segmentation is a key step in nondestructive testing of Printed Circuit Boards (PCB) based on Computed Tomography (CT) technology. In recent years, the rapid development of self-supervised pretraining technology can obtain general image features without labeled samples, and then use a small amount of labeled samples to solve downstream tasks, which has a good potential in PCB element segmentation. At present, Masked Image Modeling (MIM) pretraining model has been initially applied in PCB CT image element segmentation. However, due to the small and regular size of PCB elements such as vias, wires, and pads, the global visual field has redundancy for a single element reconstruction, which may damage the performance of the model. Based on this issue, we propose an efficient pretraining model based on multi-scale local visual field feature reconstruction for PCB CT image element segmentation (EMLR-seg). In this model, the teacher-guided MIM pretraining model is introduced into PCB CT image element segmentation for the first time, and a multi-scale local visual field extraction (MVE) module is proposed to reduce redundancy by focusing on local visual fields. At the same time, a simple 4-Transformer-blocks decoder is used. Experiments show that EMLR-seg can achieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2% by the baseline model, and the training time is reduced by 29.6 hours, a reduction of 17.4% under the same experimental condition, which reflects the advantage of EMLR-seg in terms of performance and efficiency.","sentences":["Element segmentation is a key step in nondestructive testing of Printed Circuit Boards (PCB) based on Computed Tomography (CT) technology.","In recent years, the rapid development of self-supervised pretraining technology can obtain general image features without labeled samples, and then use a small amount of labeled samples to solve downstream tasks, which has a good potential in PCB element segmentation.","At present, Masked Image Modeling (MIM) pretraining model has been initially applied in PCB CT image element segmentation.","However, due to the small and regular size of PCB elements such as vias, wires, and pads, the global visual field has redundancy for a single element reconstruction, which may damage the performance of the model.","Based on this issue, we propose an efficient pretraining model based on multi-scale local visual field feature reconstruction for PCB CT image element segmentation (EMLR-seg).","In this model, the teacher-guided MIM pretraining model is introduced into PCB CT image element segmentation for the first time, and a multi-scale local visual field extraction (MVE) module is proposed to reduce redundancy by focusing on local visual fields.","At the same time, a simple 4-Transformer-blocks decoder is used.","Experiments show that EMLR-seg can achieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2% by the baseline model, and the training time is reduced by 29.6 hours, a reduction of 17.4% under the same experimental condition, which reflects the advantage of EMLR-seg in terms of performance and efficiency."],"url":"http://arxiv.org/abs/2405.05745v1","category":"cs.CV"}
{"created":"2024-05-09 12:58:22","title":"Can large language models understand uncommon meanings of common words?","abstract":"Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear, fostering numerous studies and sparking heated debates. Prevailing research mainly focuses on surface-level NLU, neglecting fine-grained explorations. However, such explorations are crucial for understanding their unique comprehension mechanisms, aligning with human cognition, and finally enhancing LLMs' general NLU capacities. To address this gap, our study delves into LLMs' nuanced semantic comprehension capabilities, particularly regarding common words with uncommon meanings. The idea stems from foundational principles of human communication within psychology, which underscore accurate shared understandings of word semantics. Specifically, this paper presents the innovative construction of a Lexical Semantic Comprehension (LeSC) dataset with novel evaluation metrics, the first benchmark encompassing both fine-grained and cross-lingual dimensions. Introducing models of both open-source and closed-source, varied scales and architectures, our extensive empirical experiments demonstrate the inferior performance of existing models in this basic lexical-meaning understanding task. Notably, even the state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9% and 22.3%, respectively. Additionally, multiple advanced prompting techniques and retrieval-augmented generation are also introduced to help alleviate this trouble, yet limitations persist. By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs.","sentences":["Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents.","Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear, fostering numerous studies and sparking heated debates.","Prevailing research mainly focuses on surface-level NLU, neglecting fine-grained explorations.","However, such explorations are crucial for understanding their unique comprehension mechanisms, aligning with human cognition, and finally enhancing LLMs' general NLU capacities.","To address this gap, our study delves into LLMs' nuanced semantic comprehension capabilities, particularly regarding common words with uncommon meanings.","The idea stems from foundational principles of human communication within psychology, which underscore accurate shared understandings of word semantics.","Specifically, this paper presents the innovative construction of a Lexical Semantic Comprehension (LeSC) dataset with novel evaluation metrics, the first benchmark encompassing both fine-grained and cross-lingual dimensions.","Introducing models of both open-source and closed-source, varied scales and architectures, our extensive empirical experiments demonstrate the inferior performance of existing models in this basic lexical-meaning understanding task.","Notably, even the state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9% and 22.3%, respectively.","Additionally, multiple advanced prompting techniques and retrieval-augmented generation are also introduced to help alleviate this trouble, yet limitations persist.","By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs."],"url":"http://arxiv.org/abs/2405.05741v1","category":"cs.CL"}
{"created":"2024-05-09 12:55:32","title":"Preliminary Exploration on the Low-Pressure Ar-O2 Plasma Generated by Low-Frequency Alternating Current (AC) Power Supply","abstract":"This study reports a low-frequency alternating current (AC) power supply as a novel approach for generating low-pressure capacitively coupled Ar-O2 plasma, offering advantages in cost, compactness, and operational simplicity, which are crucial for both material science and biological applications. The effectiveness of low-frequency AC-generated plasma against traditional RF systems by examining key plasma parameters such as electron density, electron temperature, and electron energy distribution function (EEDF), are investigated. Experimental results revealed that AC power supply could effectively produce low pressure Ar-O2 plasma with comparable properties to RF systems. Most notably, the AC-generated plasma achieved a significant reduction in bacterial growth, suggesting its potential as a more economical and flexible alternative for enhancing plasma-assisted applications in sterilization and material processing.","sentences":["This study reports a low-frequency alternating current (AC) power supply as a novel approach for generating low-pressure capacitively coupled Ar-O2 plasma, offering advantages in cost, compactness, and operational simplicity, which are crucial for both material science and biological applications.","The effectiveness of low-frequency AC-generated plasma against traditional RF systems by examining key plasma parameters such as electron density, electron temperature, and electron energy distribution function (EEDF), are investigated.","Experimental results revealed that AC power supply could effectively produce low pressure Ar-O2 plasma with comparable properties to RF systems.","Most notably, the AC-generated plasma achieved a significant reduction in bacterial growth, suggesting its potential as a more economical and flexible alternative for enhancing plasma-assisted applications in sterilization and material processing."],"url":"http://arxiv.org/abs/2405.05739v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 12:55:03","title":"End-to-End Generative Semantic Communication Powered by Shared Semantic Knowledge Base","abstract":"Semantic communication has drawn substantial attention as a promising paradigm to achieve effective and intelligent communications. However, efficient image semantic communication encounters challenges with a lower testing compression ratio (CR) compared to the training phase. To tackle this issue, we propose an innovative semantic knowledge base (SKB)-enabled generative semantic communication system for image classification and image generation tasks. Specifically, a lightweight SKB, comprising class-level information, is exploited to guide the semantic communication process, which enables us to transmit only the relevant indices. This approach promotes the completion of the image classification task at the source end and significantly reduces the transmission load. Meanwhile, the category-level knowledge in the SKB facilitates the image generation task by allowing controllable generation, making it possible to generate favorable images in resource-constrained scenarios. Additionally, semantic accuracy is introduced as a new metric to validate the performance of semantic transmission powered by the SKB. Evaluation results indicate that the proposed method outperforms the benchmarks and achieves superior performance with minimal transmission overhead, especially in the low SNR regime.","sentences":["Semantic communication has drawn substantial attention as a promising paradigm to achieve effective and intelligent communications.","However, efficient image semantic communication encounters challenges with a lower testing compression ratio (CR) compared to the training phase.","To tackle this issue, we propose an innovative semantic knowledge base (SKB)-enabled generative semantic communication system for image classification and image generation tasks.","Specifically, a lightweight SKB, comprising class-level information, is exploited to guide the semantic communication process, which enables us to transmit only the relevant indices.","This approach promotes the completion of the image classification task at the source end and significantly reduces the transmission load.","Meanwhile, the category-level knowledge in the SKB facilitates the image generation task by allowing controllable generation, making it possible to generate favorable images in resource-constrained scenarios.","Additionally, semantic accuracy is introduced as a new metric to validate the performance of semantic transmission powered by the SKB.","Evaluation results indicate that the proposed method outperforms the benchmarks and achieves superior performance with minimal transmission overhead, especially in the low SNR regime."],"url":"http://arxiv.org/abs/2405.05738v1","category":"cs.IT"}
{"created":"2024-05-09 12:53:14","title":"Neural Network Approach for Predicting Infrared Spectra from 3D Molecular Structure","abstract":"Accurately predicting infrared (IR) spectra in computational chemistry using ab initio methods remains a challenge. Current approaches often rely on an empirical approach or on tedious anharmonic calculations, mainly adapted to semi-rigid molecules. This limitation motivates us to explore alternative methodologies. Previous studies explored machine-learning techniques for potential and dipolar surface generation, followed by IR spectra calculation using classical molecular dynamics. However, these methods are computationally expensive and require molecule-by-molecule processing. Our article introduces a new approach to improve IR spectra prediction accuracy within a significantly reduced computing time. We developed a machine learning (ML) model to directly predict IR spectra from three-dimensional (3D) molecular structures. The spectra predicted by our model significantly outperform those from density functional theory (DFT) calculations, even after scaling. In a test set of 200 molecules, our model achieves a Spectral Information Similarity Metric of 0.92, surpassing the value achieved by DFT scaled frequencies, which is 0.57. Additionally, our model considers anharmonic effects, offering a fast alternative to laborious anharmonic calculations. Moreover, our model can be used to predict various types of spectra (Ultraviolet or Nuclear Magnetic Resonance for example) as a function of molecular structure. All it needs is a database of 3D structures and their associated spectra.","sentences":["Accurately predicting infrared (IR) spectra in computational chemistry using ab initio methods remains a challenge.","Current approaches often rely on an empirical approach or on tedious anharmonic calculations, mainly adapted to semi-rigid molecules.","This limitation motivates us to explore alternative methodologies.","Previous studies explored machine-learning techniques for potential and dipolar surface generation, followed by IR spectra calculation using classical molecular dynamics.","However, these methods are computationally expensive and require molecule-by-molecule processing.","Our article introduces a new approach to improve IR spectra prediction accuracy within a significantly reduced computing time.","We developed a machine learning (ML) model to directly predict IR spectra from three-dimensional (3D) molecular structures.","The spectra predicted by our model significantly outperform those from density functional theory (DFT) calculations, even after scaling.","In a test set of 200 molecules, our model achieves a Spectral Information Similarity Metric of 0.92, surpassing the value achieved by DFT scaled frequencies, which is 0.57.","Additionally, our model considers anharmonic effects, offering a fast alternative to laborious anharmonic calculations.","Moreover, our model can be used to predict various types of spectra (Ultraviolet or Nuclear Magnetic Resonance for example) as a function of molecular structure.","All it needs is a database of 3D structures and their associated spectra."],"url":"http://arxiv.org/abs/2405.05737v1","category":"physics.chem-ph"}
{"created":"2024-05-09 12:52:22","title":"Optimal Baseline Corrections for Off-Policy Contextual Bandits","abstract":"The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings.","sentences":["The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric.","With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance.","These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation).","Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios.","The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates.","Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it.","This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements.","Empirical observations corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2405.05736v1","category":"cs.LG"}
{"created":"2024-05-09 12:48:29","title":"Constraints on primordial black holes from LIGO-Virgo-KAGRA O3 events","abstract":"Primordial black holes (PBH) can efficiently form black hole binaries in the early universe. We update the resulting constraints on PBH abundance using data from the third observational run (O3) of LIGO-Virgo-KAGRA. To capture a wide range of PBH scenarios, we consider a variety of mass functions, including critical collapse in the QCD epoch in the presence of non-Gaussianities. Applying hierarchical Bayesian analysis to a population binaries consisting of primordial and astrophysical black holes, we find that, in every scenario, the PBHs can make up at most $f_{\\rm PBH} \\lesssim 10^{-3}$ of dark matter in the mass range $1-200~M_\\odot$. The shape and strength of the constraints are insensitive to the type of non-Gaussianities, the modifications to the mass function during the QCD epoch, or the modelling of the astrophysical PBH population.","sentences":["Primordial black holes (PBH) can efficiently form black hole binaries in the early universe.","We update the resulting constraints on PBH abundance using data from the third observational run (O3) of LIGO-Virgo-KAGRA.","To capture a wide range of PBH scenarios, we consider a variety of mass functions, including critical collapse in the QCD epoch in the presence of non-Gaussianities.","Applying hierarchical Bayesian analysis to a population binaries consisting of primordial and astrophysical black holes, we find that, in every scenario, the PBHs can make up at most $f_{\\rm PBH} \\lesssim 10^{-3}$ of dark matter in the mass range $1-200~M_\\odot$. The shape and strength of the constraints are insensitive to the type of non-Gaussianities, the modifications to the mass function during the QCD epoch, or the modelling of the astrophysical PBH population."],"url":"http://arxiv.org/abs/2405.05732v1","category":"astro-ph.CO"}
{"created":"2024-05-09 12:48:18","title":"Robustness of the Hedgehog Skyrmion","abstract":"We investigate the radial profile function of the hedgehog Skyrmion with unit baryon number in generic EFTs (effective field theories) of pions. The analysis assumes chiral symmetry and ignores the pion mass term. The Skyrmion is always smooth, because it has no point source at the origin, and terms in the EFT with higher numbers of pion derivatives do not result in uncontrolled large corrections or singularities there. The profile varies in quite a limited way as the terms in the EFT change, and a universal profile function is proposed.","sentences":["We investigate the radial profile function of the hedgehog Skyrmion with unit baryon number in generic EFTs (effective field theories) of pions.","The analysis assumes chiral symmetry and ignores the pion mass term.","The Skyrmion is always smooth, because it has no point source at the origin, and terms in the EFT with higher numbers of pion derivatives do not result in uncontrolled large corrections or singularities there.","The profile varies in quite a limited way as the terms in the EFT change, and a universal profile function is proposed."],"url":"http://arxiv.org/abs/2405.05731v1","category":"hep-th"}
{"created":"2024-05-09 12:42:36","title":"Nonlinear vibrational spectrometer for bioapplications featuring narrowband 1-$\u03bc$m pulses and a recycled OPA pump beam","abstract":"Moving the detection wavelength in vibrational sum-frequency generation (VSFG) spectroscopy to the near-infrared (> 700 nm) can potentially enable the study of molecular interfaces absorbing in the visible and give access to buried bio-interfaces at minimal absorption, reduced scattering, and negligible autofluorescence. Here, we employ an ultra-narrow bandpass thin-film optical interference filter on 180-fs, 1.03-$\\mu$m laser pulses to generate an upconversion beam yielding a spectral resolution of 5 cm$^{-1}$ and VSFG wavelengths between 890 and 980 nm for molecular vibrations in the fingerprint region. We demonstrate that the beam rejected by the filter can be utilized for driving a supercontinuum-seeded near-infrared optical parametric amplifier serving as the front-end of a broadband LiGaS$_{2}$-based mid-infrared amplifier. Benchmark data on a phospholipid monolayer at the air-water interface acquired using the resulting VSFG spectrometer show the possibility of achieving high resolution and signal-to-noise ratio at short acquisition times. The scheme can also be utilized in other types of vibrational spectroscopy that derive their spectral resolution from bandpass-filtering of femtosecond near-infrared laser pulses, such as stimulated Raman scattering (SRS) and coherent anti-Stokes Raman scattering (CARS) spectroscopy.","sentences":["Moving the detection wavelength in vibrational sum-frequency generation (VSFG) spectroscopy to the near-infrared (> 700 nm) can potentially enable the study of molecular interfaces absorbing in the visible and give access to buried bio-interfaces at minimal absorption, reduced scattering, and negligible autofluorescence.","Here, we employ an ultra-narrow bandpass thin-film optical interference filter on 180-fs, 1.03-$\\mu$m laser pulses to generate an upconversion beam yielding a spectral resolution of 5 cm$^{-1}$ and VSFG wavelengths between 890 and 980 nm for molecular vibrations in the fingerprint region.","We demonstrate that the beam rejected by the filter can be utilized for driving a supercontinuum-seeded near-infrared optical parametric amplifier serving as the front-end of a broadband LiGaS$_{2}$-based mid-infrared amplifier.","Benchmark data on a phospholipid monolayer at the air-water interface acquired using the resulting VSFG spectrometer show the possibility of achieving high resolution and signal-to-noise ratio at short acquisition times.","The scheme can also be utilized in other types of vibrational spectroscopy that derive their spectral resolution from bandpass-filtering of femtosecond near-infrared laser pulses, such as stimulated Raman scattering (SRS) and coherent anti-Stokes Raman scattering (CARS) spectroscopy."],"url":"http://arxiv.org/abs/2405.05729v1","category":"physics.optics"}
{"created":"2024-05-09 12:35:33","title":"Computational lexical analysis of Flamenco genres","abstract":"Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of Humanity, is a profound expression of cultural identity rooted in Andalusia, Spain. However, there is a lack of quantitative studies that help identify characteristic patterns in this long-lived music tradition. In this work, we present a computational analysis of Flamenco lyrics, employing natural language processing and machine learning to categorize over 2000 lyrics into their respective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial Naive Bayes classifier, we find that lexical variation across styles enables to accurately identify distinct $\\textit{palos}$. More importantly, from an automatic method of word usage, we obtain the semantic fields that characterize each style. Further, applying a metric that quantifies the inter-genre distance we perform a network analysis that sheds light on the relationship between Flamenco styles. Remarkably, our results suggest historical connections and $\\textit{palo}$ evolutions. Overall, our work illuminates the intricate relationships and cultural significance embedded within Flamenco lyrics, complementing previous qualitative discussions with quantitative analyses and sparking new discussions on the origin and development of traditional music genres.","sentences":["Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of Humanity, is a profound expression of cultural identity rooted in Andalusia, Spain.","However, there is a lack of quantitative studies that help identify characteristic patterns in this long-lived music tradition.","In this work, we present a computational analysis of Flamenco lyrics, employing natural language processing and machine learning to categorize over 2000 lyrics into their respective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial Naive Bayes classifier, we find that lexical variation across styles enables to accurately identify distinct $\\textit{palos}$. More importantly, from an automatic method of word usage, we obtain the semantic fields that characterize each style.","Further, applying a metric that quantifies the inter-genre distance we perform a network analysis that sheds light on the relationship between Flamenco styles.","Remarkably, our results suggest historical connections and $\\textit{palo}$ evolutions.","Overall, our work illuminates the intricate relationships and cultural significance embedded within Flamenco lyrics, complementing previous qualitative discussions with quantitative analyses and sparking new discussions on the origin and development of traditional music genres."],"url":"http://arxiv.org/abs/2405.05723v1","category":"cs.CL"}
{"created":"2024-05-09 12:34:45","title":"A Framework of SO(3)-equivariant Non-linear Representation Learning and its Application to Electronic-Structure Hamiltonian Prediction","abstract":"We present both a theoretical and a methodological framework that addresses a critical challenge in applying deep learning to physical systems: the reconciliation of non-linear expressiveness with SO(3)-equivariance in predictions of SO(3)-equivariant quantities, such as the electronic-structure Hamiltonian. Inspired by covariant theory in physics, we address this problem by exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations. We first construct theoretical SO(3)-invariant quantities derived from the SO(3)-equivariant regression targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features. Given that SO(3)-invariance is preserved under non-linear operations, the encoding process for invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems. Building on this foundation, we propose a gradient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features. This mechanism can incorporate non-linear expressive capabilities into SO(3)-equivariant representations, while theoretically preserving their equivariant properties as we prove. Our approach offers a promising general solution to the critical dilemma between equivariance and non-linear expressiveness in deep learning methodologies. We apply our theory and method to the electronic-structure Hamiltonian prediction tasks, demonstrating state-of-the-art performance across six benchmark databases.","sentences":["We present both a theoretical and a methodological framework that addresses a critical challenge in applying deep learning to physical systems: the reconciliation of non-linear expressiveness with SO(3)-equivariance in predictions of SO(3)-equivariant quantities, such as the electronic-structure Hamiltonian.","Inspired by covariant theory in physics, we address this problem by exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations.","We first construct theoretical SO(3)-invariant quantities derived from the SO(3)-equivariant regression targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features.","Given that SO(3)-invariance is preserved under non-linear operations, the encoding process for invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems.","Building on this foundation, we propose a gradient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features.","This mechanism can incorporate non-linear expressive capabilities into SO(3)-equivariant representations, while theoretically preserving their equivariant properties as we prove.","Our approach offers a promising general solution to the critical dilemma between equivariance and non-linear expressiveness in deep learning methodologies.","We apply our theory and method to the electronic-structure Hamiltonian prediction tasks, demonstrating state-of-the-art performance across six benchmark databases."],"url":"http://arxiv.org/abs/2405.05722v1","category":"cs.LG"}
{"created":"2024-05-09 12:34:34","title":"A Newton Method for Hausdorff Approximations of the Pareto Front within Multi-objective Evolutionary Algorithms","abstract":"A common goal in evolutionary multi-objective optimization is to find suitable finite-size approximations of the Pareto front of a given multi-objective optimization problem. While many multi-objective evolutionary algorithms have proven to be very efficient in finding good Pareto front approximations, they may need quite a few resources or may even fail to obtain optimal or nearly approximations. Hereby, optimality is implicitly defined by the chosen performance indicator. In this work, we propose a set-based Newton method for Hausdorff approximations of the Pareto front to be used within multi-objective evolutionary algorithms. To this end, we first generalize the previously proposed Newton step for the performance indicator for the treatment of constrained problems for general reference sets. To approximate the target Pareto front, we propose a particular strategy for generating the reference set that utilizes the data gathered by the evolutionary algorithm during its run. Finally, we show the benefit of the Newton method as a post-processing step on several benchmark test functions and different base evolutionary algorithms.","sentences":["A common goal in evolutionary multi-objective optimization is to find suitable finite-size approximations of the Pareto front of a given multi-objective optimization problem.","While many multi-objective evolutionary algorithms have proven to be very efficient in finding good Pareto front approximations, they may need quite a few resources or may even fail to obtain optimal or nearly approximations.","Hereby, optimality is implicitly defined by the chosen performance indicator.","In this work, we propose a set-based Newton method for Hausdorff approximations of the Pareto front to be used within multi-objective evolutionary algorithms.","To this end, we first generalize the previously proposed Newton step for the performance indicator for the treatment of constrained problems for general reference sets.","To approximate the target Pareto front, we propose a particular strategy for generating the reference set that utilizes the data gathered by the evolutionary algorithm during its run.","Finally, we show the benefit of the Newton method as a post-processing step on several benchmark test functions and different base evolutionary algorithms."],"url":"http://arxiv.org/abs/2405.05721v1","category":"cs.NE"}
{"created":"2024-05-09 12:26:47","title":"A note on Jacquet modules of general linear groups","abstract":"Let F be a non-Archimedean local field. Consider G_n:= GL_n(F) and let M:= G_l * G_{n-l} be a maximal Levi subgroup of G_n. In this article, we compute the semisimplified Jacquet module of representations of G_n with respect to the maximal Levi subgroup M, belonging to a particular category of representations. Utilizing our results, we prove that the Jacquet module is multiplicity-free for a specific subcategory of representations. Our findings are based on the Zelevinsky classification.","sentences":["Let F be a non-Archimedean local field.","Consider G_n:= GL_n(F) and let M:= G_l * G_{n-l} be a maximal Levi subgroup of G_n.","In this article, we compute the semisimplified Jacquet module of representations of G_n with respect to the maximal Levi subgroup M, belonging to a particular category of representations.","Utilizing our results, we prove that the Jacquet module is multiplicity-free for a specific subcategory of representations.","Our findings are based on the Zelevinsky classification."],"url":"http://arxiv.org/abs/2405.05719v1","category":"math.RT"}
{"created":"2024-05-09 12:23:58","title":"A review on two types of sonic interfaces","abstract":"In this paper, two examples of sonic interfaces are presented. The first example shows the case of sonic interfaces as weak discontinuities in self-similar shock configurations of unsteady Euler system. The second example shows the case of sonic interfaces as regular interfaces in accelerating transonic flows governed by the steady Euler-Poisson system with self-generated electric forces. And, we discuss analytic differences of the two examples, and introduce an open problem on decelerating transonic solution to the steady Euler-Poisson system.","sentences":["In this paper, two examples of sonic interfaces are presented.","The first example shows the case of sonic interfaces as weak discontinuities in self-similar shock configurations of unsteady Euler system.","The second example shows the case of sonic interfaces as regular interfaces in accelerating transonic flows governed by the steady Euler-Poisson system with self-generated electric forces.","And, we discuss analytic differences of the two examples, and introduce an open problem on decelerating transonic solution to the steady Euler-Poisson system."],"url":"http://arxiv.org/abs/2405.05717v1","category":"math.AP"}
{"created":"2024-05-09 12:12:00","title":"Zeta-functions of Curves over Finite Fields","abstract":"Curves over finite fields are of great importance in cryptography and coding theory. Through studying their zeta-functions, we would be able to find out vital arithmetic and geometric information about them and their Jacobians, including the number of rational points on this kind of curves. In this paper, I investigate if it is possible to construct a curve over finite fields of a given genus $g$ whose zeta-function is given as a product of zeta-functions of $g$ elliptic curves, and find out alternative methods if it is not possible. Basically, I look for conditions which those $g$ elliptic curves should satisfy such that their product (of their Jacobians) is isogenous to the Jacobian of a curve of a given genus $g$. Then from this isogenous relationship I can determine the characteristic polynomial of the Frobenius endomorphism of the Jacobian of the new curve and by this characteristic polynomial I can thus determine the zeta-function of this new curve. By using the zeta-functions of curves in the form as generating functions, the number of rational points on curves can even be found out, which may lead to further researches relating to some applications in cryptography, coding theory and even information theory.","sentences":["Curves over finite fields are of great importance in cryptography and coding theory.","Through studying their zeta-functions, we would be able to find out vital arithmetic and geometric information about them and their Jacobians, including the number of rational points on this kind of curves.","In this paper, I investigate if it is possible to construct a curve over finite fields of a given genus $g$ whose zeta-function is given as a product of zeta-functions of $g$ elliptic curves, and find out alternative methods if it is not possible.","Basically, I look for conditions which those $g$ elliptic curves should satisfy such that their product (of their Jacobians) is isogenous to the Jacobian of a curve of a given genus $g$. Then from this isogenous relationship I can determine the characteristic polynomial of the Frobenius endomorphism of the Jacobian of the new curve and by this characteristic polynomial I can thus determine the zeta-function of this new curve.","By using the zeta-functions of curves in the form as generating functions, the number of rational points on curves can even be found out, which may lead to further researches relating to some applications in cryptography, coding theory and even information theory."],"url":"http://arxiv.org/abs/2405.05711v1","category":"math.NT"}
{"created":"2024-05-09 12:11:28","title":"On the applicability of Kolmogorov's theory of probability to the description of quantum phenomena. Part I","abstract":"It is a common view that von Neumann laid the foundations of a \"non-commutative probability theory\" with his axiomatization of quantum mechanics (QM). As such, it is regarded a generalization of the \"classical probability theory\" due to Kolmogorov. Outside of quantum physics, however, Kolmogorov's axioms enjoy universal applicability. This raises the question of whether quantum physics indeed requires such a generalization of our conception of probability or if von Neumann's axiomatization of QM was contingent on the absence of a general theory of probability in the 1920s.   In this work I argue in favor of the latter position. In particular, I show that for non-relativistic $N$-body quantum systems subject to a time-independent scalar potential, it is possible to construct a mathematically rigorous theory based on Kolmogorov's axioms and physically natural random variables, which reproduces central predictions of QM. The respective theories are distinct, so that an empirical comparison may be possible. Moreover, the approach can in principle be adapted to other classes of quantum-mechanical models.   Part II of this series will address the projection postulate and the question of measurement in this approach.","sentences":["It is a common view that von Neumann laid the foundations of a \"non-commutative probability theory\" with his axiomatization of quantum mechanics (QM).","As such, it is regarded a generalization of the \"classical probability theory\" due to Kolmogorov.","Outside of quantum physics, however, Kolmogorov's axioms enjoy universal applicability.","This raises the question of whether quantum physics indeed requires such a generalization of our conception of probability or if von Neumann's axiomatization of QM was contingent on the absence of a general theory of probability in the 1920s.   ","In this work I argue in favor of the latter position.","In particular, I show that for non-relativistic $N$-body quantum systems subject to a time-independent scalar potential, it is possible to construct a mathematically rigorous theory based on Kolmogorov's axioms and physically natural random variables, which reproduces central predictions of QM.","The respective theories are distinct, so that an empirical comparison may be possible.","Moreover, the approach can in principle be adapted to other classes of quantum-mechanical models.   ","Part II of this series will address the projection postulate and the question of measurement in this approach."],"url":"http://arxiv.org/abs/2405.05710v1","category":"quant-ph"}
{"created":"2024-05-09 12:06:06","title":"LatentColorization: Latent Diffusion-Based Speaker Video Colorization","abstract":"While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM.","sentences":["While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored.","Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames.","This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames.","To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods.","Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art.","Our dataset encompasses a combination of conventional datasets and videos from television/movies.","In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency.","A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM."],"url":"http://arxiv.org/abs/2405.05707v1","category":"cs.CV"}
{"created":"2024-05-09 12:03:53","title":"An Observation on the Beta Functions in Quadratic Gravity","abstract":"We study the beta functions for the dimensionless couplings in quadratic curvature gravity, and find that there is a simple argument to restrict the possible form of the beta functions as derived from the counterterms at an arbitrary loop. The relation to the recent different results on beta functions is also commented on.","sentences":["We study the beta functions for the dimensionless couplings in quadratic curvature gravity, and find that there is a simple argument to restrict the possible form of the beta functions as derived from the counterterms at an arbitrary loop.","The relation to the recent different results on beta functions is also commented on."],"url":"http://arxiv.org/abs/2405.05706v1","category":"hep-th"}
{"created":"2024-05-09 12:01:11","title":"Ising's roots and the transfer-matrix eigenvalues","abstract":"Today, the Ising model is an archetype describing collective ordering processes. And, as such, it is widely known in physics and far beyond. Less known is the fact that the thesis defended by Ernst Ising 100 years ago (in 1924) contained not only the solution of what we call now the `classical 1D Ising model' but also other problems. Some of these problems, as well as the method of their solution, are the subject of this note. In particular, we discuss the combinatorial method Ernst Ising used to calculate the partition function for a chain of elementary magnets. In the thermodynamic limit, this method leads to the result that the partition function is given by the roots of a certain polynomial. We explicitly show that `Ising's roots' that arise within the combinatorial treatment are also recovered by the eigenvalues of the transfer matrix, a concept that was introduced much later. Moreover, we discuss the generalization of the two-state model to a three-state one presented in Ising's thesis, but not included in his famous paper of 1925 ( \\i E. Ising, Z. Physik {\\bf 31} (1925) 253}). The latter model can be considered as a forerunner of the now abundant models with many-component order parameters.","sentences":["Today, the Ising model is an archetype describing collective ordering processes.","And, as such, it is widely known in physics and far beyond.","Less known is the fact that the thesis defended by Ernst Ising 100 years ago (in 1924) contained not only the solution of what we call now the `classical 1D Ising model' but also other problems.","Some of these problems, as well as the method of their solution, are the subject of this note.","In particular, we discuss the combinatorial method Ernst Ising used to calculate the partition function for a chain of elementary magnets.","In the thermodynamic limit, this method leads to the result that the partition function is given by the roots of a certain polynomial.","We explicitly show that `Ising's roots' that arise within the combinatorial treatment are also recovered by the eigenvalues of the transfer matrix, a concept that was introduced much later.","Moreover, we discuss the generalization of the two-state model to a three-state one presented in Ising's thesis, but not included in his famous paper of 1925 ( \\i E. Ising, Z. Physik {\\bf 31} (1925) 253}).","The latter model can be considered as a forerunner of the now abundant models with many-component order parameters."],"url":"http://arxiv.org/abs/2405.05703v1","category":"physics.hist-ph"}
{"created":"2024-05-09 11:57:42","title":"NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap","abstract":"Gaussian Splatting has garnered widespread attention due to its exceptional performance. Consequently, SLAM systems based on Gaussian Splatting have emerged, leveraging its capabilities for rapid real-time rendering and high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure adjustments and scene generalization capabilities. To address these issues, we introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting. We have developed neural implicit submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps. Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate gap filling and high-quality scene expression, supporting both monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance.","sentences":["Gaussian Splatting has garnered widespread attention due to its exceptional performance.","Consequently, SLAM systems based on Gaussian Splatting have emerged, leveraging its capabilities for rapid real-time rendering and high-fidelity mapping.","However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure adjustments and scene generalization capabilities.","To address these issues, we introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting.","We have developed neural implicit submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps.","Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate gap filling and high-quality scene expression, supporting both monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance."],"url":"http://arxiv.org/abs/2405.05702v1","category":"cs.RO"}
{"created":"2024-05-09 11:54:10","title":"Detecting environmental effects in gravitational waves from binaries perturbed by periodic forces","abstract":"We study the gravitational wave (GW) emission of sources perturbed by periodic dynamical forces which do not cause secular evolution in the orbital elements. We construct a corresponding post-Newtonian waveform model and provide estimates for the detectability of the resulting GW phase perturbations, for both space-based and future ground-based detectors. We validate our results by performing a set of Bayesian parameter recovery experiments with post-Newtonian waveforms. We find that, in stark contrast to the more commonly studied secular dephasing, periodic phase perturbations do not suffer from degeneracies with any of the tested vacuum binary parameters. We discuss the applications of our findings to a range of possible astrophysical scenarios, finding that such periodic perturbations may be detectable for massive black hole binaries embedded in circum-binary discs, extreme mass-ratio inspirals in accretion discs, as well as stellar-mass compact objects perturbed by tidal fields. We argue that modelling conservative sub-orbital dynamics opens up a promising new avenue to detect environmental effects in binary sources of GWs that should be included in state-of-the-art waveform templates.","sentences":["We study the gravitational wave (GW) emission of sources perturbed by periodic dynamical forces which do not cause secular evolution in the orbital elements.","We construct a corresponding post-Newtonian waveform model and provide estimates for the detectability of the resulting GW phase perturbations, for both space-based and future ground-based detectors.","We validate our results by performing a set of Bayesian parameter recovery experiments with post-Newtonian waveforms.","We find that, in stark contrast to the more commonly studied secular dephasing, periodic phase perturbations do not suffer from degeneracies with any of the tested vacuum binary parameters.","We discuss the applications of our findings to a range of possible astrophysical scenarios, finding that such periodic perturbations may be detectable for massive black hole binaries embedded in circum-binary discs, extreme mass-ratio inspirals in accretion discs, as well as stellar-mass compact objects perturbed by tidal fields.","We argue that modelling conservative sub-orbital dynamics opens up a promising new avenue to detect environmental effects in binary sources of GWs that should be included in state-of-the-art waveform templates."],"url":"http://arxiv.org/abs/2405.05698v1","category":"gr-qc"}
{"created":"2024-05-09 11:50:19","title":"Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost","abstract":"We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference. Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence. These connections can be removed during the primary task inference, resulting in a single-task inference cost. We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections. Moreover, our method can be incorporated with optimization-based auxiliary learning approaches. Extensive experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG, ResNet, and ViT backbones validate the promising performance. The codes are available at https://github.com/ethanygao/Aux-NAS.","sentences":["We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task.","While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference.","Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence.","These connections can be removed during the primary task inference, resulting in a single-task inference cost.","We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections.","Moreover, our method can be incorporated with optimization-based auxiliary learning approaches.","Extensive experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG, ResNet, and ViT backbones validate the promising performance.","The codes are available at https://github.com/ethanygao/Aux-NAS."],"url":"http://arxiv.org/abs/2405.05695v1","category":"cs.LG"}
{"created":"2024-05-09 11:45:21","title":"Meta Algebras and Biorthogonal Rational Functions: The Hahn Case","abstract":"The finite families of biorthogonal rational functions and orthogonal polynomials of Hahn type are interpreted algebraically in a unified way by considering the three-generated meta Hahn algebra and its finite-dimensional representations. The functions of interest arise as overlaps between eigensolutions of generalized and ordinary eigenvalue problems on the representation space. The orthogonality relations and bispectral properties naturally follow from the framework.","sentences":["The finite families of biorthogonal rational functions and orthogonal polynomials of Hahn type are interpreted algebraically in a unified way by considering the three-generated meta Hahn algebra and its finite-dimensional representations.","The functions of interest arise as overlaps between eigensolutions of generalized and ordinary eigenvalue problems on the representation space.","The orthogonality relations and bispectral properties naturally follow from the framework."],"url":"http://arxiv.org/abs/2405.05692v1","category":"math-ph"}
{"created":"2024-05-09 11:41:27","title":"StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework","abstract":"Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation. Existing diffusion-based methods employ disparate network architectures and training strategies. The effect of the design of each component is still unclear. In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots. For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs. Based on the profound analysis, we tailor each component for efficient high-quality human motion generation. Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process. By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation. Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods. Project page: https://h-y1heng.github.io/StableMoFusion-page/","sentences":["Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation.","Existing diffusion-based methods employ disparate network architectures and training strategies.","The effect of the design of each component is still unclear.","In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots.","For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs.","Based on the profound analysis, we tailor each component for efficient high-quality human motion generation.","Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions.","To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process.","By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation.","Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods.","Project page: https://h-y1heng.github.io/StableMoFusion-page/"],"url":"http://arxiv.org/abs/2405.05691v1","category":"cs.CV"}
{"created":"2024-05-09 11:38:23","title":"Evaluating Dialect Robustness of Language Models via Conversation Understanding","abstract":"With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained. Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of `taboo'. We formulate two evaluative tasks: target word prediction (TWP) (i.e.predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words). Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng subsets. We add two subsets: AITrans (where dialectic information is removed from IndEng) and AIGen (where LLMs are prompted to generate conversations). Our evaluation uses pre-trained and fine-tuned versions of two closed-source (GPT-4/3.5) and two open-source LLMs (Mistral and Gemma). LLMs perform significantly better for US English than Indian English for both TWP and TWS, for all settings. While GPT-based models perform the best, the comparatively smaller models work more equitably for short conversations (<8 turns). Our results on AIGen and AITrans (the best and worst-performing subset) respectively show that LLMs may learn a dialect of their own based on the composition of the training data, and that dialect robustness is indeed a challenging task. Our evaluation methodology exhibits a novel way to examine attributes of language models using pre-existing dialogue datasets.","sentences":["With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained.","Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of `taboo'.","We formulate two evaluative tasks: target word prediction (TWP) (i.e.predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words).","Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng subsets.","We add two subsets: AITrans (where dialectic information is removed from IndEng) and AIGen (where LLMs are prompted to generate conversations).","Our evaluation uses pre-trained and fine-tuned versions of two closed-source (GPT-4/3.5) and two open-source LLMs (Mistral and Gemma).","LLMs perform significantly better for US English than Indian English for both TWP and TWS, for all settings.","While GPT-based models perform the best, the comparatively smaller models work more equitably for short conversations (<8 turns).","Our results on AIGen and AITrans (the best and worst-performing subset) respectively show that LLMs may learn a dialect of their own based on the composition of the training data, and that dialect robustness is indeed a challenging task.","Our evaluation methodology exhibits a novel way to examine attributes of language models using pre-existing dialogue datasets."],"url":"http://arxiv.org/abs/2405.05688v1","category":"cs.CL"}
{"created":"2024-05-09 11:32:30","title":"Diffraction casting","abstract":"Optical computing is considered a promising solution for the growing demand for parallel computing in various cutting-edge fields, requiring high integration and high speed computational capacity. In this paper, we propose a novel optical computation architecture called diffraction casting (DC) for flexible and scalable parallel logic operations. In DC, a diffractive neural network (DNN) is designed for single instruction, multiple data (SIMD) operations. This approach allows for the alteration of logic operations simply by changing the illumination patterns. Furthermore, it eliminates the need for encoding and decoding the input and output, respectively, by introducing a buffer around the input area, facilitating end-to-end all-optical computing. We numerically demonstrate DC by performing all 16 logic operations on two arbitrary 256 bits parallel binary inputs. Additionally, we showcase several distinctive attributes inherent in DC, such as the benefit of cohesively designing the diffractive elements for SIMD logic operations, assuring high scalability and integration capability. Our study offers a novel design architecture for optical computers and paves the way for a next-generation optical computing paradigm.","sentences":["Optical computing is considered a promising solution for the growing demand for parallel computing in various cutting-edge fields, requiring high integration and high speed computational capacity.","In this paper, we propose a novel optical computation architecture called diffraction casting (DC) for flexible and scalable parallel logic operations.","In DC, a diffractive neural network (DNN) is designed for single instruction, multiple data (SIMD) operations.","This approach allows for the alteration of logic operations simply by changing the illumination patterns.","Furthermore, it eliminates the need for encoding and decoding the input and output, respectively, by introducing a buffer around the input area, facilitating end-to-end all-optical computing.","We numerically demonstrate DC by performing all 16 logic operations on two arbitrary 256 bits parallel binary inputs.","Additionally, we showcase several distinctive attributes inherent in DC, such as the benefit of cohesively designing the diffractive elements for SIMD logic operations, assuring high scalability and integration capability.","Our study offers a novel design architecture for optical computers and paves the way for a next-generation optical computing paradigm."],"url":"http://arxiv.org/abs/2405.05686v1","category":"physics.optics"}
{"created":"2024-05-09 11:28:43","title":"Asymptotic preserving finite volume method for the compressible Euler equations: analysis via dissipative measure-valued solutions","abstract":"We propose and analyze a new asymptotic preserving (AP) finite volume scheme for the multidimensional compressible barotropic Euler equations to simulate low Mach number flows. The proposed scheme uses a stabilized upwind numerical flux, with the stabilization term being proportional to the stiff pressure gradient, and we prove its conditional energy stability and consistency. Utilizing the concept of dissipative measure-valued (DMV) solutions, we rigorously illustrate the AP properties of the scheme for well-prepared initial data. In particular, we prove that the numerical solutions will converge weakly to a DMV solution of the compressible Euler equations as the mesh parameter vanishes, while the Mach number is fixed. The DMV solutions then converge to a classical solution of the incompressible Euler system as the Mach number goes to zero. Conversely, we show that if the mesh parameter is kept fixed, we obtain an energy stable and consistent finite-volume scheme approximating the incompressible Euler equations as the Mach number goes to zero. The numerical solutions generated by this scheme then converge weakly to a DMV solution of the incompressible Euler system as the mesh parameter vanishes. Invoking the weak-strong uniqueness principle, we conclude that the DMV solution and classical solution of the incompressible Euler system coincide, proving the AP property of the scheme. We also present an extensive numerical case study in order to illustrate the theoretical convergences, wherein we utilize the techniques of K-convergence.","sentences":["We propose and analyze a new asymptotic preserving (AP) finite volume scheme for the multidimensional compressible barotropic Euler equations to simulate low Mach number flows.","The proposed scheme uses a stabilized upwind numerical flux, with the stabilization term being proportional to the stiff pressure gradient, and we prove its conditional energy stability and consistency.","Utilizing the concept of dissipative measure-valued (DMV) solutions, we rigorously illustrate the AP properties of the scheme for well-prepared initial data.","In particular, we prove that the numerical solutions will converge weakly to a DMV solution of the compressible Euler equations as the mesh parameter vanishes, while the Mach number is fixed.","The DMV solutions then converge to a classical solution of the incompressible Euler system as the Mach number goes to zero.","Conversely, we show that if the mesh parameter is kept fixed, we obtain an energy stable and consistent finite-volume scheme approximating the incompressible Euler equations as the Mach number goes to zero.","The numerical solutions generated by this scheme then converge weakly to a DMV solution of the incompressible Euler system as the mesh parameter vanishes.","Invoking the weak-strong uniqueness principle, we conclude that the DMV solution and classical solution of the incompressible Euler system coincide, proving the AP property of the scheme.","We also present an extensive numerical case study in order to illustrate the theoretical convergences, wherein we utilize the techniques of K-convergence."],"url":"http://arxiv.org/abs/2405.05685v1","category":"math.NA"}
{"created":"2024-05-09 11:27:38","title":"Comparison Principles for the Finsler Infinity Laplacian with Applications to Minimal Lipschitz Extensions","abstract":"This paper proves comparison principles for elliptic PDE involving the Finsler infinity Laplacian, a second-order differential operator with discontinuities in the gradient variable arising in $L^{\\infty}$-variational problems and tug-of-war games. The core of the paper consists in proving generalized cone comparison principles. Among other consequences, these results imply that, for any Finsler norm $\\varphi$ in $\\mathbb{R}^{d}$, a function $u$ is a $\\varphi$-absolutely minimizing Lipschitz extension if and only if it is a viscosity solution of the $\\varphi$-infinity Laplace equation, settling a longstanding question in the $L^{\\infty}$-calculus of variations. The proofs combine new geometric constructions with classical notions from convex analysis.","sentences":["This paper proves comparison principles for elliptic PDE involving the Finsler infinity Laplacian, a second-order differential operator with discontinuities in the gradient variable arising in $L^{\\infty}$-variational problems and tug-of-war games.","The core of the paper consists in proving generalized cone comparison principles.","Among other consequences, these results imply that, for any Finsler norm $\\varphi$ in $\\mathbb{R}^{d}$, a function $u$ is a $\\varphi$-absolutely minimizing Lipschitz extension if and only if it is a viscosity solution of the $\\varphi$-infinity Laplace equation, settling a longstanding question in the $L^{\\infty}$-calculus of variations.","The proofs combine new geometric constructions with classical notions from convex analysis."],"url":"http://arxiv.org/abs/2405.05684v1","category":"math.AP"}
{"created":"2024-05-09 11:16:38","title":"About generalized complex structures on $\\mathbb S^6$","abstract":"We study the existence of generalized complex structures on the six-dimensional sphere $\\mathbb S^6$. We work with the generalized tangent bundle $\\mathbb T\\mathbb S^6\\to \\mathbb S^6$ and define the integrability of generalized geometric structures in terms of the Dorfman bracket. Specifically, we prove that there is not a direct way to induce a generalized complex structure on $\\mathbb S^6$ from its usual nearly K\\\"ahler structure inherited from the octonions product.","sentences":["We study the existence of generalized complex structures on the six-dimensional sphere $\\mathbb S^6$. We work with the generalized tangent bundle $\\mathbb T\\mathbb S^6\\to \\mathbb S^6$ and define the integrability of generalized geometric structures in terms of the Dorfman bracket.","Specifically, we prove that there is not a direct way to induce a generalized complex structure on $\\mathbb S^6$ from its usual nearly K\\\"ahler structure inherited from the octonions product."],"url":"http://arxiv.org/abs/2405.05681v1","category":"math.DG"}
{"created":"2024-05-09 11:10:29","title":"Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment","abstract":"AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction. The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems. In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication. By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent. This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems.","sentences":["AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction.","The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems.","In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication.","By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent.","This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems."],"url":"http://arxiv.org/abs/2405.05678v1","category":"cs.HC"}
{"created":"2024-05-09 11:00:06","title":"TransAnaNet: Transformer-based Anatomy Change Prediction Network for Head and Neck Cancer Patient Radiotherapy","abstract":"Early identification of head and neck cancer (HNC) patients who would experience significant anatomical change during radiotherapy (RT) is important to optimize patient clinical benefit and treatment resources. This study aims to assess the feasibility of using a vision-transformer (ViT) based neural network to predict RT-induced anatomic change in HNC patients. We retrospectively included 121 HNC patients treated with definitive RT/CRT. We collected the planning CT (pCT), planned dose, CBCTs acquired at the initial treatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp) and involved nodal volume (GTVn) delineated on both pCT and CBCTs for model construction and evaluation. A UNet-style ViT network was designed to learn spatial correspondence and contextual information from embedded CT, dose, CBCT01, GTVp, and GTVn image patches. The model estimated the deformation vector field between CBCT01 and CBCT21 as the prediction of anatomic change, and deformed CBCT01 was used as the prediction of CBCT21. We also generated binary masks of GTVp, GTVn, and patient body for volumetric change evaluation. The predicted image from the proposed method yielded the best similarity to the real image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison models. The average MSE and SSIM between the normalized predicted CBCT to CBCT21 are 0.009 and 0.933, while the average dice coefficient between body mask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively. The proposed method showed promising performance for predicting radiotherapy-induced anatomic change, which has the potential to assist in the decision-making of HNC Adaptive RT.","sentences":["Early identification of head and neck cancer (HNC) patients who would experience significant anatomical change during radiotherapy (RT) is important to optimize patient clinical benefit and treatment resources.","This study aims to assess the feasibility of using a vision-transformer (ViT) based neural network to predict RT-induced anatomic change in HNC patients.","We retrospectively included 121 HNC patients treated with definitive RT/CRT.","We collected the planning CT (pCT), planned dose, CBCTs acquired at the initial treatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp) and involved nodal volume (GTVn) delineated on both pCT and CBCTs for model construction and evaluation.","A UNet-style ViT network was designed to learn spatial correspondence and contextual information from embedded CT, dose, CBCT01, GTVp, and GTVn image patches.","The model estimated the deformation vector field between CBCT01 and CBCT21 as the prediction of anatomic change, and deformed CBCT01 was used as the prediction of CBCT21.","We also generated binary masks of GTVp, GTVn, and patient body for volumetric change evaluation.","The predicted image from the proposed method yielded the best similarity to the real image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison models.","The average MSE and SSIM between the normalized predicted CBCT to CBCT21 are 0.009 and 0.933, while the average dice coefficient between body mask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively.","The proposed method showed promising performance for predicting radiotherapy-induced anatomic change, which has the potential to assist in the decision-making of HNC Adaptive RT."],"url":"http://arxiv.org/abs/2405.05674v1","category":"cs.CV"}
{"created":"2024-05-09 10:51:07","title":"Between proof construction and SAT-solving","abstract":"The classical satisfiability problem (SAT) is used as a natural and general tool to express and solve combinatorial problems that are in NP. We postulate that provability for implicational intuitionistic propositional logic (IIPC) can serve as a similar natural tool to express problems in Pspace. This approach can be particularly convenient for two reasons. One is that provability in full IPC (with all connectives) can be reduced to provability of implicational formulas of order three. Another advantage is a convenient interpretation in terms of simple alternating automata. Additionally, we distinguish some natural subclasses of IIPC corresponding to the complexity classes NP and co-NP. Our experimental results show that a simple decision procedure requires a significant amount of time only in a small fraction of cases.","sentences":["The classical satisfiability problem (SAT) is used as a natural and general tool to express and solve combinatorial problems that are in NP.","We postulate that provability for implicational intuitionistic propositional logic (IIPC) can serve as a similar natural tool to express problems in Pspace.","This approach can be particularly convenient for two reasons.","One is that provability in full IPC (with all connectives) can be reduced to provability of implicational formulas of order three.","Another advantage is a convenient interpretation in terms of simple alternating automata.","Additionally, we distinguish some natural subclasses of IIPC corresponding to the complexity classes NP and co-NP.","Our experimental results show that a simple decision procedure requires a significant amount of time only in a small fraction of cases."],"url":"http://arxiv.org/abs/2405.05670v1","category":"cs.LO"}
{"created":"2024-05-09 10:41:20","title":"Guess the Drift with LOP-UKF: LiDAR Odometry and Pacejka Model for Real-Time Racecar Sideslip Estimation","abstract":"The sideslip angle, crucial for vehicle safety and stability, is determined using both longitudinal and lateral velocities. However, measuring the lateral component often necessitates costly sensors, leading to its common estimation, a topic thoroughly explored in existing literature. This paper introduces LOP-UKF, a novel method for estimating vehicle lateral velocity by integrating Lidar Odometry with the Pacejka tire model predictions, resulting in a robust estimation via an Unscendent Kalman Filter (UKF). This combination represents a distinct alternative to more traditional methodologies, resulting in a reliable solution also in edge cases. We present experimental results obtained using the Dallara AV-21 across diverse circuits and track conditions, demonstrating the effectiveness of our method.","sentences":["The sideslip angle, crucial for vehicle safety and stability, is determined using both longitudinal and lateral velocities.","However, measuring the lateral component often necessitates costly sensors, leading to its common estimation, a topic thoroughly explored in existing literature.","This paper introduces LOP-UKF, a novel method for estimating vehicle lateral velocity by integrating Lidar Odometry with the Pacejka tire model predictions, resulting in a robust estimation via an Unscendent Kalman Filter (UKF).","This combination represents a distinct alternative to more traditional methodologies, resulting in a reliable solution also in edge cases.","We present experimental results obtained using the Dallara AV-21 across diverse circuits and track conditions, demonstrating the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.05668v1","category":"cs.RO"}
{"created":"2024-05-09 10:41:18","title":"VM-DDPM: Vision Mamba Diffusion for Medical Image Synthesis","abstract":"In the realm of smart healthcare, researchers enhance the scale and diversity of medical datasets through medical image synthesis. However, existing methods are limited by CNN local perception and Transformer quadratic complexity, making it difficult to balance structural texture consistency. To this end, we propose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully combining CNN local perception and SSM global modeling capabilities, while maintaining linear computational complexity. Specifically, we designed a multi-level feature extraction module called Multi-level State Space Block (MSSBlock), and a basic unit of encoder-decoder structure called State Space Layer (SSLayer) for medical pathological images. Besides, we designed a simple, Plug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan Module (CSM), which enabled the S6 module to fully perceive the spatial features of the 2D image and stimulate the generalization potential of the model. To our best knowledge, this is the first medical image synthesis model based on the SSM-CNN hybrid architecture. Our experimental evaluation on three datasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as qualitative evaluation by radiologists, demonstrate that VM-DDPM achieves state-of-the-art performance.","sentences":["In the realm of smart healthcare, researchers enhance the scale and diversity of medical datasets through medical image synthesis.","However, existing methods are limited by CNN local perception and Transformer quadratic complexity, making it difficult to balance structural texture consistency.","To this end, we propose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully combining CNN local perception and SSM global modeling capabilities, while maintaining linear computational complexity.","Specifically, we designed a multi-level feature extraction module called Multi-level State Space Block (MSSBlock), and a basic unit of encoder-decoder structure called State Space Layer (SSLayer) for medical pathological images.","Besides, we designed a simple, Plug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan Module (CSM), which enabled the S6 module to fully perceive the spatial features of the 2D image and stimulate the generalization potential of the model.","To our best knowledge, this is the first medical image synthesis model based on the SSM-CNN hybrid architecture.","Our experimental evaluation on three datasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as qualitative evaluation by radiologists, demonstrate that VM-DDPM achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.05667v1","category":"eess.IV"}
{"created":"2024-05-09 10:35:44","title":"RPBG: Towards Robust Neural Point-based Graphics in the Wild","abstract":"Point-based representations have recently gained popularity in novel view synthesis, for their unique advantages, e.g., intuitive geometric representation, simple manipulation, and faster convergence. However, based on our observation, these point-based neural re-rendering methods are only expected to perform well under ideal conditions and suffer from noisy, patchy points and unbounded scenes, which are challenging to handle but defacto common in real applications. To this end, we revisit one such influential method, known as Neural Point-based Graphics (NPBG), as our baseline, and propose Robust Point-based Graphics (RPBG). We in-depth analyze the factors that prevent NPBG from achieving satisfactory renderings on generic datasets, and accordingly reform the pipeline to make it more robust to varying datasets in-the-wild. Inspired by the practices in image restoration, we greatly enhance the neural renderer to enable the attention-based correction of point visibility and the inpainting of incomplete rasterization, with only acceptable overheads. We also seek for a simple and lightweight alternative for environment modeling and an iterative method to alleviate the problem of poor geometry. By thorough evaluation on a wide range of datasets with different shooting conditions and camera trajectories, RPBG stably outperforms the baseline by a large margin, and exhibits its great robustness over state-of-the-art NeRF-based variants. Code available at https://github.com/QT-Zhu/RPBG.","sentences":["Point-based representations have recently gained popularity in novel view synthesis, for their unique advantages, e.g., intuitive geometric representation, simple manipulation, and faster convergence.","However, based on our observation, these point-based neural re-rendering methods are only expected to perform well under ideal conditions and suffer from noisy, patchy points and unbounded scenes, which are challenging to handle but defacto common in real applications.","To this end, we revisit one such influential method, known as Neural Point-based Graphics (NPBG), as our baseline, and propose Robust Point-based Graphics (RPBG).","We in-depth analyze the factors that prevent NPBG from achieving satisfactory renderings on generic datasets, and accordingly reform the pipeline to make it more robust to varying datasets in-the-wild.","Inspired by the practices in image restoration, we greatly enhance the neural renderer to enable the attention-based correction of point visibility and the inpainting of incomplete rasterization, with only acceptable overheads.","We also seek for a simple and lightweight alternative for environment modeling and an iterative method to alleviate the problem of poor geometry.","By thorough evaluation on a wide range of datasets with different shooting conditions and camera trajectories, RPBG stably outperforms the baseline by a large margin, and exhibits its great robustness over state-of-the-art NeRF-based variants.","Code available at https://github.com/QT-Zhu/RPBG."],"url":"http://arxiv.org/abs/2405.05663v1","category":"cs.CV"}
{"created":"2024-05-09 10:33:07","title":"Approximate Dec-POMDP Solving Using Multi-Agent A*","abstract":"We present an A*-based algorithm to compute policies for finite-horizon Dec-POMDPs. Our goal is to sacrifice optimality in favor of scalability for larger horizons. The main ingredients of our approach are (1) using clustered sliding window memory, (2) pruning the A* search tree, and (3) using novel A* heuristics. Our experiments show competitive performance to the state-of-the-art. Moreover, for multiple benchmarks, we achieve superior performance. In addition, we provide an A* algorithm that finds upper bounds for the optimum, tailored towards problems with long horizons. The main ingredient is a new heuristic that periodically reveals the state, thereby limiting the number of reachable beliefs. Our experiments demonstrate the efficacy and scalability of the approach.","sentences":["We present an A*-based algorithm to compute policies for finite-horizon Dec-POMDPs.","Our goal is to sacrifice optimality in favor of scalability for larger horizons.","The main ingredients of our approach are (1) using clustered sliding window memory, (2) pruning the A* search tree, and (3) using novel A* heuristics.","Our experiments show competitive performance to the state-of-the-art.","Moreover, for multiple benchmarks, we achieve superior performance.","In addition, we provide an A* algorithm that finds upper bounds for the optimum, tailored towards problems with long horizons.","The main ingredient is a new heuristic that periodically reveals the state, thereby limiting the number of reachable beliefs.","Our experiments demonstrate the efficacy and scalability of the approach."],"url":"http://arxiv.org/abs/2405.05662v1","category":"cs.AI"}
{"created":"2024-05-09 10:22:12","title":"Josephson effect in a Fibonacci quasicrystal","abstract":"Quasiperiodicity has recently been proposed to enhance superconductivity and its proximity effect. At the same time, there has been significant experimental progress in the fabrication of quasiperiodic structures, also in reduced dimensions. Motivated by these developments, we use microscopic tight-binding theory to investigate the DC Josephson effect through a ballistic Fibonacci chain attached to two superconducting leads. The Fibonacci chain is one of the most studied examples of quasicrystals, hosting a rich multifractal spectrum, containing topological gaps with different winding numbers. We study how the Andreev bound states (ABS), current-phase relation, and the critical current depend on the quasiperiodic degrees of freedom, from short to long junctions. While the current-phase relation shows a traditional $2\\pi$ sinusoidal or sawtooth profile, we find that the ABS obtain quasiperiodic oscillations and that the Andreev reflection is qualitatively altered, leading to quasiperiodic oscillations in the critical current as a function of junction length. Surprisingly, despite earlier proposals of enhanced superconductivity, we do not in general find an enhanced critical current. However, we find significant enhancement for reduced interface transparency due to the modified Andreev reflection. Furthermore, by varying the chemical potential, e.g.~by an applied gate voltage, we find a fractal oscillation between superconductor-normal metal-superconductor (SNS) and superconductor-insulator-superconductor (SIS) behavior. Finally, we show that the winding of the subgap states leads to an equivalent winding in the critical current, such that the winding numbers, and thus the topological invariant, can be determined.","sentences":["Quasiperiodicity has recently been proposed to enhance superconductivity and its proximity effect.","At the same time, there has been significant experimental progress in the fabrication of quasiperiodic structures, also in reduced dimensions.","Motivated by these developments, we use microscopic tight-binding theory to investigate the DC Josephson effect through a ballistic Fibonacci chain attached to two superconducting leads.","The Fibonacci chain is one of the most studied examples of quasicrystals, hosting a rich multifractal spectrum, containing topological gaps with different winding numbers.","We study how the Andreev bound states (ABS), current-phase relation, and the critical current depend on the quasiperiodic degrees of freedom, from short to long junctions.","While the current-phase relation shows a traditional $2\\pi$ sinusoidal or sawtooth profile, we find that the ABS obtain quasiperiodic oscillations and that the Andreev reflection is qualitatively altered, leading to quasiperiodic oscillations in the critical current as a function of junction length.","Surprisingly, despite earlier proposals of enhanced superconductivity, we do not in general find an enhanced critical current.","However, we find significant enhancement for reduced interface transparency due to the modified Andreev reflection.","Furthermore, by varying the chemical potential, e.g.~by an applied gate voltage, we find a fractal oscillation between superconductor-normal metal-superconductor (SNS) and superconductor-insulator-superconductor (SIS) behavior.","Finally, we show that the winding of the subgap states leads to an equivalent winding in the critical current, such that the winding numbers, and thus the topological invariant, can be determined."],"url":"http://arxiv.org/abs/2405.05660v1","category":"cond-mat.supr-con"}
{"created":"2024-05-09 10:14:49","title":"End-to-End Waveform and Beamforming Optimization for RF Wireless Power Transfer","abstract":"Radio frequency (RF) wireless power transfer (WPT) is a key technology for future low-power wireless systems. However, the inherently low end-to-end power transfer efficiency (PTE) is challenging for practical applications. The main factors contributing to it are the channel losses, transceivers' power consumption, and losses related, e.g., to the digital-to-analog converter (DAC), high-power amplifier, and rectenna. Optimizing PTE requires careful consideration of these factors, motivating the current work. Herein, we consider an analog multi-antenna power transmitter that aims to charge a single energy harvester. We first provide a mathematical framework to calculate the harvested power from multi-tone signal transmissions and the system power consumption. Then, we formulate the joint waveform and analog beamforming design problem to minimize power consumption and meet the charging requirements. Finally, we propose an optimization approach relying on swarm intelligence to solve the specified problem. Simulation results quantify the power consumption reduction as the DAC, phase shifters resolution, and antenna length are increased, while it is seen that increasing system frequency results in higher power consumption.","sentences":["Radio frequency (RF) wireless power transfer (WPT) is a key technology for future low-power wireless systems.","However, the inherently low end-to-end power transfer efficiency (PTE) is challenging for practical applications.","The main factors contributing to it are the channel losses, transceivers' power consumption, and losses related, e.g., to the digital-to-analog converter (DAC), high-power amplifier, and rectenna.","Optimizing PTE requires careful consideration of these factors, motivating the current work.","Herein, we consider an analog multi-antenna power transmitter that aims to charge a single energy harvester.","We first provide a mathematical framework to calculate the harvested power from multi-tone signal transmissions and the system power consumption.","Then, we formulate the joint waveform and analog beamforming design problem to minimize power consumption and meet the charging requirements.","Finally, we propose an optimization approach relying on swarm intelligence to solve the specified problem.","Simulation results quantify the power consumption reduction as the DAC, phase shifters resolution, and antenna length are increased, while it is seen that increasing system frequency results in higher power consumption."],"url":"http://arxiv.org/abs/2405.05659v1","category":"eess.SP"}
{"created":"2024-05-09 10:12:17","title":"Artificial intelligence for abnormality detection in high volume neuroimaging: a systematic review and meta-analysis","abstract":"Purpose: Most studies evaluating artificial intelligence (AI) models that detect abnormalities in neuroimaging are either tested on unrepresentative patient cohorts or are insufficiently well-validated, leading to poor generalisability to real-world tasks. The aim was to determine the diagnostic test accuracy and summarise the evidence supporting the use of AI models performing first-line, high-volume neuroimaging tasks.   Methods: Medline, Embase, Cochrane library and Web of Science were searched until September 2021 for studies that temporally or externally validated AI capable of detecting abnormalities in first-line CT or MR neuroimaging. A bivariate random-effects model was used for meta-analysis where appropriate. PROSPERO: CRD42021269563.   Results: Only 16 studies were eligible for inclusion. Included studies were not compromised by unrepresentative datasets or inadequate validation methodology. Direct comparison with radiologists was available in 4/16 studies. 15/16 had a high risk of bias. Meta-analysis was only suitable for intracranial haemorrhage detection in CT imaging (10/16 studies), where AI systems had a pooled sensitivity and specificity 0.90 (95% CI 0.85 - 0.94) and 0.90 (95% CI 0.83 - 0.95) respectively. Other AI studies using CT and MRI detected target conditions other than haemorrhage (2/16), or multiple target conditions (4/16). Only 3/16 studies implemented AI in clinical pathways, either for pre-read triage or as post-read discrepancy identifiers.   Conclusion: The paucity of eligible studies reflects that most abnormality detection AI studies were not adequately validated in representative clinical cohorts. The few studies describing how abnormality detection AI could impact patients and clinicians did not explore the full ramifications of clinical implementation.","sentences":["Purpose: Most studies evaluating artificial intelligence (AI) models that detect abnormalities in neuroimaging are either tested on unrepresentative patient cohorts or are insufficiently well-validated, leading to poor generalisability to real-world tasks.","The aim was to determine the diagnostic test accuracy and summarise the evidence supporting the use of AI models performing first-line, high-volume neuroimaging tasks.   ","Methods: Medline, Embase, Cochrane library and Web of Science were searched until September 2021 for studies that temporally or externally validated AI capable of detecting abnormalities in first-line CT or MR neuroimaging.","A bivariate random-effects model was used for meta-analysis where appropriate.","PROSPERO:","CRD42021269563.   ","Results: Only 16 studies were eligible for inclusion.","Included studies were not compromised by unrepresentative datasets or inadequate validation methodology.","Direct comparison with radiologists was available in 4/16 studies.","15/16 had a high risk of bias.","Meta-analysis was only suitable for intracranial haemorrhage detection in CT imaging (10/16 studies), where AI systems had a pooled sensitivity and specificity 0.90 (95% CI 0.85 - 0.94) and 0.90 (95% CI 0.83 - 0.95) respectively.","Other AI studies using CT and MRI detected target conditions other than haemorrhage (2/16), or multiple target conditions (4/16).","Only 3/16 studies implemented AI in clinical pathways, either for pre-read triage or as post-read discrepancy identifiers.   ","Conclusion: The paucity of eligible studies reflects that most abnormality detection AI studies were not adequately validated in representative clinical cohorts.","The few studies describing how abnormality detection AI could impact patients and clinicians did not explore the full ramifications of clinical implementation."],"url":"http://arxiv.org/abs/2405.05658v1","category":"eess.IV"}
{"created":"2024-05-09 10:08:17","title":"Dynamics of McMillan mappings II. Axially symmetric map","abstract":"In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens. Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet. The main goal of our work is to close this gap and understand the limitations and potentials of this device. It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates. Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams. In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables. This provides an evaluation of the dynamical aperture, Poincar\\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes. Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time. The second part investigates the particle dynamics as a function of system parameters. We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations. Each regime is considered in great detail, including the limiting cases of large and small amplitudes. In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra.","sentences":["In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens.","Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet.","The main goal of our work is to close this gap and understand the limitations and potentials of this device.","It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates.","Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams.","In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables.","This provides an evaluation of the dynamical aperture, Poincar\\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes.","Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time.","The second part investigates the particle dynamics as a function of system parameters.","We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations.","Each regime is considered in great detail, including the limiting cases of large and small amplitudes.","In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra."],"url":"http://arxiv.org/abs/2405.05657v1","category":"nlin.SI"}
{"created":"2024-05-09 09:58:31","title":"Dynamics of McMillan mappings I. McMillan multipoles","abstract":"In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan. Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter. While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored. We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\\'e rotation numbers, and canonical action-angle variables.   In the second part, we establish connections between these maps and general chaotic maps in standard form. Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization). Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude. This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude. Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios.","sentences":["In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan.","Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter.","While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored.","We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\\'e rotation numbers, and canonical action-angle variables.   ","In the second part, we establish connections between these maps and general chaotic maps in standard form.","Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization).","Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude.","This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude.","Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios."],"url":"http://arxiv.org/abs/2405.05652v1","category":"nlin.SI"}
{"created":"2024-05-09 09:44:51","title":"ASGrasp: Generalizable Transparent Object Reconstruction and Grasping from RGB-D Active Stereo Camera","abstract":"In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.Project page: https://pku-epic.github.io/ASGrasp","sentences":["In this paper, we tackle the problem of grasping transparent and specular objects.","This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras.","For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera.","ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments.","In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction.","We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion.","Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer.","Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.","Project page: https://pku-epic.github.io/ASGrasp"],"url":"http://arxiv.org/abs/2405.05648v1","category":"cs.RO"}
{"created":"2024-05-09 09:41:19","title":"Letter to the Editor: What are the legal and ethical considerations of submitting radiology reports to ChatGPT?","abstract":"This letter critically examines the recent article by Infante et al. assessing the utility of large language models (LLMs) like GPT-4, Perplexity, and Bard in identifying urgent findings in emergency radiology reports. While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR.","sentences":["This letter critically examines the recent article by Infante et al. assessing the utility of large language models (LLMs) like GPT-4, Perplexity, and Bard in identifying urgent findings in emergency radiology reports.","While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR."],"url":"http://arxiv.org/abs/2405.05647v1","category":"cs.CV"}
{"created":"2024-05-09 09:27:18","title":"An Efficient Finite Difference Approximation via a Double Sample-Recycling Approach","abstract":"Estimating stochastic gradients is pivotal in fields like service systems within operations research. The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs. Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE). To tackle this problem, we propose a double sample-recycling approach in this paper. Firstly, pilot samples are recycled to estimate the optimal perturbation. Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator. We analyze its bias, variance and MSE. Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator. Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator. In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes.","sentences":["Estimating stochastic gradients is pivotal in fields like service systems within operations research.","The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs.","Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE).","To tackle this problem, we propose a double sample-recycling approach in this paper.","Firstly, pilot samples are recycled to estimate the optimal perturbation.","Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator.","We analyze its bias, variance and MSE.","Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator.","Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator.","In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes."],"url":"http://arxiv.org/abs/2405.05638v1","category":"stat.ME"}
{"created":"2024-05-09 09:24:54","title":"Thermal junctions controlled with magnetic phases","abstract":"Unlike charge, heat flows are difficult to control. We show that, in mesoscopic conductors, electronic thermal currents can be manipulated with a magnetic field by using the Aharonov-Bohm effect: the magnetic control of the interference pattern enhances the thermoelectric effect, while heat transport can be totally suppressed. In a three-terminal configuration, the flux-induced broken reciprocity generates a non-local thermoelectric response and translates to the circulation of heat. This way, efficient thermoelectric generators, thermal switches and thermal circulators, as well as energy harvesters can be defined for minimally disturbing thermal management at the nanoscale.","sentences":["Unlike charge, heat flows are difficult to control.","We show that, in mesoscopic conductors, electronic thermal currents can be manipulated with a magnetic field by using the Aharonov-Bohm effect: the magnetic control of the interference pattern enhances the thermoelectric effect, while heat transport can be totally suppressed.","In a three-terminal configuration, the flux-induced broken reciprocity generates a non-local thermoelectric response and translates to the circulation of heat.","This way, efficient thermoelectric generators, thermal switches and thermal circulators, as well as energy harvesters can be defined for minimally disturbing thermal management at the nanoscale."],"url":"http://arxiv.org/abs/2405.05637v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 09:22:09","title":"SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space","abstract":"Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the framework's generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at http://swaptalk.cc.","sentences":["Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation.","However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space.","To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space.","Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance.","To enhance the framework's generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module.","Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality.","In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos.","To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios.","Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos.","Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency.","Our demo is available at http://swaptalk.cc."],"url":"http://arxiv.org/abs/2405.05636v1","category":"cs.CV"}
{"created":"2024-05-09 09:21:15","title":"Large Bricks and Join-irreducible torsionfree classes","abstract":"We show that every join-irreducible torsionfree class in the category of finitely generated modules over an artinian ring is cogenerated by a single (not necessarily finitely generated) brick.   This is a partial extension of the characterisation of completely join-irreducible torsionfree classes given by Barnard, Carroll and Zhu.","sentences":["We show that every join-irreducible torsionfree class in the category of finitely generated modules over an artinian ring is cogenerated by a single (not necessarily finitely generated) brick.   ","This is a partial extension of the characterisation of completely join-irreducible torsionfree classes given by Barnard, Carroll and Zhu."],"url":"http://arxiv.org/abs/2405.05635v1","category":"math.RT"}
{"created":"2024-05-09 08:57:22","title":"Calculation of $6j$-symbols for the Lie algebra $\\mathfrak{gl}_n$","abstract":"We give an explicit description of the multiplicity space that describes an occurence of a certain irreducible representation into a splitting of a tensor product of two irreducible representations of $\\mathfrak{gl}_n$. Using this descripition an explicit formula for an arbitrary $6j$-symbol for the algebra $\\mathfrak{gl}_n$ is derived. It is expressed through a value of a generalized hypergeometric function.","sentences":["We give an explicit description of the multiplicity space that describes an occurence of a certain irreducible representation into a splitting of a tensor product of two irreducible representations of $\\mathfrak{gl}_n$. Using this descripition an explicit formula for an arbitrary $6j$-symbol for the algebra $\\mathfrak{gl}_n$ is derived.","It is expressed through a value of a generalized hypergeometric function."],"url":"http://arxiv.org/abs/2405.05628v1","category":"math.RT"}
{"created":"2024-05-09 08:54:30","title":"AI in Your Toolbox: A Plugin for Generating Renderings from 3D Models","abstract":"With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology. This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino's features. It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality. Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools. Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment.","sentences":["With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology.","This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino's features.","It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality.","Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools.","Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment."],"url":"http://arxiv.org/abs/2405.05627v1","category":"cs.HC"}
{"created":"2024-05-09 08:51:29","title":"Does Dynamical Wormhole Evolve From Emergent Scenario?","abstract":"In the present work we analyse a dynamical wormhole solution with two fluids system (one isotropic and homogeneous and the other being inhomogeneous and anisotropic in nature) as the matter at the throat. We choose two different forms of Equation of State(EoS) and investigate two solutions of the wormhole geometry. The properties to ensure existence and traversability has been analysed. Also, the model of the dynamic wormhole has been examined for a possibility of the Emergent Universe(EU) model in cosmological context. Finally, for the dynamical wormholes so obtained, Null Energy Condition(NEC) has been examined near the throat.","sentences":["In the present work we analyse a dynamical wormhole solution with two fluids system (one isotropic and homogeneous and the other being inhomogeneous and anisotropic in nature) as the matter at the throat.","We choose two different forms of Equation of State(EoS) and investigate two solutions of the wormhole geometry.","The properties to ensure existence and traversability has been analysed.","Also, the model of the dynamic wormhole has been examined for a possibility of the Emergent Universe(EU) model in cosmological context.","Finally, for the dynamical wormholes so obtained, Null Energy Condition(NEC) has been examined near the throat."],"url":"http://arxiv.org/abs/2405.05625v1","category":"gr-qc"}
{"created":"2024-05-09 08:46:30","title":"Current progress in corrosion of multi principal element alloys","abstract":"Whilst multi-principal element alloys (MPEAs) remain a promising class of materials owing to several attractive mechanical properties, their corrosion performance is also unique. In this concise review, we present an emerging overview of some of the general features related to MPEA corrosion, following a decade of work in the field. This includes highlighting some of the key aspects related to the electrochemical phenomena in MPEA corrosion, and the relevant future works required for a holistic mechanistic understanding. In addition, a comprehensive database of the reported corrosion performance of MPEAs is presented, based on works reported to date. The database is assembled to also allow users to undertake machine learning or their own data analysis, with a parsed representation of alloy composition, test electrolyte, and corrosion related parameters.","sentences":["Whilst multi-principal element alloys (MPEAs) remain a promising class of materials owing to several attractive mechanical properties, their corrosion performance is also unique.","In this concise review, we present an emerging overview of some of the general features related to MPEA corrosion, following a decade of work in the field.","This includes highlighting some of the key aspects related to the electrochemical phenomena in MPEA corrosion, and the relevant future works required for a holistic mechanistic understanding.","In addition, a comprehensive database of the reported corrosion performance of MPEAs is presented, based on works reported to date.","The database is assembled to also allow users to undertake machine learning or their own data analysis, with a parsed representation of alloy composition, test electrolyte, and corrosion related parameters."],"url":"http://arxiv.org/abs/2405.05623v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 08:35:21","title":"Rectified Gaussian kernel multi-view k-means clustering","abstract":"In this paper, we show two new variants of multi-view k-means (MVKM) algorithms to address multi-view data. The general idea is to outline the distance between $h$-th view data points $x_i^h$ and $h$-th view cluster centers $a_k^h$ in a different manner of centroid-based approach. Unlike other methods, our proposed methods learn the multi-view data by calculating the similarity using Euclidean norm in the space of Gaussian-kernel, namely as multi-view k-means with exponent distance (MVKM-ED). By simultaneously aligning the stabilizer parameter $p$ and kernel coefficients $\\beta^h$, the compression of Gaussian-kernel based weighted distance in Euclidean norm reduce the sensitivity of MVKM-ED. To this end, this paper designated as Gaussian-kernel multi-view k-means (GKMVKM) clustering algorithm. Numerical evaluation of five real-world multi-view data demonstrates the robustness and efficiency of our proposed MVKM-ED and GKMVKM approaches.","sentences":["In this paper, we show two new variants of multi-view k-means (MVKM) algorithms to address multi-view data.","The general idea is to outline the distance between $h$-th view data points $x_i^h$ and $h$-th view cluster centers $a_k^h$ in a different manner of centroid-based approach.","Unlike other methods, our proposed methods learn the multi-view data by calculating the similarity using Euclidean norm in the space of Gaussian-kernel, namely as multi-view k-means with exponent distance (MVKM-ED).","By simultaneously aligning the stabilizer parameter $p$ and kernel coefficients $\\beta^h$, the compression of Gaussian-kernel based weighted distance in Euclidean norm reduce the sensitivity of MVKM-ED.","To this end, this paper designated as Gaussian-kernel multi-view k-means (GKMVKM) clustering algorithm.","Numerical evaluation of five real-world multi-view data demonstrates the robustness and efficiency of our proposed MVKM-ED and GKMVKM approaches."],"url":"http://arxiv.org/abs/2405.05619v1","category":"cs.LG"}
{"created":"2024-05-09 08:32:55","title":"An Automatic Prompt Generation System for Tabular Data Tasks","abstract":"Efficient processing of tabular data is important in various industries, especially when working with datasets containing a large number of columns. Large language models (LLMs) have demonstrated their ability on several tasks through carefully crafted prompts. However, creating effective prompts for tabular datasets is challenging due to the structured nature of the data and the need to manage numerous columns. This paper presents an innovative auto-prompt generation system suitable for multiple LLMs, with minimal training. It proposes two novel methods; 1) A Reinforcement Learning-based algorithm for identifying and sequencing task-relevant columns 2) Cell-level similarity-based approach for enhancing few-shot example selection. Our approach has been extensively tested across 66 datasets, demonstrating improved performance in three downstream tasks: data imputation, error detection, and entity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B.","sentences":["Efficient processing of tabular data is important in various industries, especially when working with datasets containing a large number of columns.","Large language models (LLMs) have demonstrated their ability on several tasks through carefully crafted prompts.","However, creating effective prompts for tabular datasets is challenging due to the structured nature of the data and the need to manage numerous columns.","This paper presents an innovative auto-prompt generation system suitable for multiple LLMs, with minimal training.","It proposes two novel methods; 1) A Reinforcement Learning-based algorithm for identifying and sequencing task-relevant columns 2) Cell-level similarity-based approach for enhancing few-shot example selection.","Our approach has been extensively tested across 66 datasets, demonstrating improved performance in three downstream tasks: data imputation, error detection, and entity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B."],"url":"http://arxiv.org/abs/2405.05618v1","category":"cs.LG"}
{"created":"2024-05-09 08:28:12","title":"G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning","abstract":"Commonsense question answering has demonstrated considerable potential across various applications like assistants and social robots. Although fully fine-tuned pre-trained Language Models(LM) have achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability. Some studies have explored combining LMs with Knowledge Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural Network(GNN)-based reasoning that lacks a profound interaction between heterogeneous modalities. In this paper, we propose a novel Graph-based Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP, aiming to maintain a balance between heterogeneous knowledge and enhance the cross-modal interaction within the LM+GNNs model. In particular, an evidence graph is constructed by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance. Afterward, a structure-aware frozen PLM is employed to fully incorporate the structured and textual information from the evidence graph, where the generation of prompts is driven by graph entities and relations. Finally, a heterogeneous message-passing reasoning module is used to facilitate deep interaction of knowledge between the LM and graph-based networks. Empirical validation, conducted through extensive experiments on three benchmark datasets, demonstrates the notable performance of the proposed model. The results reveal a significant advancement over the existing models, especially, with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.","sentences":["Commonsense question answering has demonstrated considerable potential across various applications like assistants and social robots.","Although fully fine-tuned pre-trained Language Models(LM) have achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability.","Some studies have explored combining LMs with Knowledge Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural Network(GNN)-based reasoning that lacks a profound interaction between heterogeneous modalities.","In this paper, we propose a novel Graph-based Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP, aiming to maintain a balance between heterogeneous knowledge and enhance the cross-modal interaction within the LM+GNNs model.","In particular, an evidence graph is constructed by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.","Afterward, a structure-aware frozen PLM is employed to fully incorporate the structured and textual information from the evidence graph, where the generation of prompts is driven by graph entities and relations.","Finally, a heterogeneous message-passing reasoning module is used to facilitate deep interaction of knowledge between the LM and graph-based networks.","Empirical validation, conducted through extensive experiments on three benchmark datasets, demonstrates the notable performance of the proposed model.","The results reveal a significant advancement over the existing models, especially, with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset."],"url":"http://arxiv.org/abs/2405.05616v1","category":"cs.CL"}
{"created":"2024-05-09 17:07:16","title":"Multilevel Regression and Poststratification Interface: Application to Track Community-level COVID-19 Viral Transmission","abstract":"In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP). MRP adjusts for selection bias and yields stable small area estimates. We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.","sentences":["In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system.","The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP).","MRP adjusts for selection bias and yields stable small area estimates.","We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow.","We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan."],"url":"http://arxiv.org/abs/2405.05909v1","category":"stat.AP"}
{"created":"2024-05-09 15:48:07","title":"The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels","abstract":"Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement--some challenged by perspectivist approaches, and some that remain to be addressed--as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.","sentences":["Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators.","But what should we do when annotators disagree?","Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information.","In this position paper, we examine practices and assumptions surrounding the causes of disagreement--some challenged by perspectivist approaches, and some that remain to be addressed--as well as practical and normative challenges for work operating under these assumptions.","We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement."],"url":"http://arxiv.org/abs/2405.05860v1","category":"cs.LG"}
{"created":"2024-05-09 15:14:58","title":"Upper and Lower Bounds on Phase-Space Rearrangements","abstract":"Broad classes of plasma phenomena can be understood in terms of phase-space rearrangements. For example, the net effect of a wave-particle interaction may consist of moving populations of particles from one region of phase space to another. Different phenomena drive rearrangements that obey different rules. When those rules can be specified, it is possible to calculate bounds that limit the possible effects the rearrangement could have (such as limits on how much energy can be extracted from the particles). This leads to two problems. The first is to understand the mapping between the allowed class of rearrangements and the possible outcomes that these rearrangements can have on the overall distribution. The second is to understand which rules are appropriate for which physical systems. There has been recent progress on both fronts, but a variety of interesting questions remain.","sentences":["Broad classes of plasma phenomena can be understood in terms of phase-space rearrangements.","For example, the net effect of a wave-particle interaction may consist of moving populations of particles from one region of phase space to another.","Different phenomena drive rearrangements that obey different rules.","When those rules can be specified, it is possible to calculate bounds that limit the possible effects the rearrangement could have (such as limits on how much energy can be extracted from the particles).","This leads to two problems.","The first is to understand the mapping between the allowed class of rearrangements and the possible outcomes that these rearrangements can have on the overall distribution.","The second is to understand which rules are appropriate for which physical systems.","There has been recent progress on both fronts, but a variety of interesting questions remain."],"url":"http://arxiv.org/abs/2405.05835v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 14:57:22","title":"Probing CPV mixing in the Higgs sector in VBF at 1 TeV ILC","abstract":"With the current precision of measurements by the ATLAS and CMS experiments, it cannot be excluded that a SM-like Higgs boson is a CP violating mixture of CP-even and CP-odd states. We explore this possibility here, assuming Higgs boson production in ZZ-fusion, at 1 TeV ILC, with unpolarized beams. The full simulation of SM background and fast simulation of the signal is performed, simulating 8 ab$^{-1}$ of data collected with the ILD detector. We demonstrate that the CP mixing angle $\\Psi_{\\mathrm{CP}}$ between scalar and pseudoscalar states can be measured with the statistical uncertainty of 3.8 mrad at 68% CL, corresponding to 1.44 $\\cdot 10^{-5}$ for the CP parameter $f_\\mathrm{CP}$, for the pure scalar state. This is the first result on sensitivity of an $e^{+}e^{-}$ collider to measure $f_\\mathrm{CP}$ in the Higgs production vertex in vector boson fusion.","sentences":["With the current precision of measurements by the ATLAS and CMS experiments, it cannot be excluded that a SM-like Higgs boson is a CP violating mixture of CP-even and CP-odd states.","We explore this possibility here, assuming Higgs boson production in ZZ-fusion, at 1 TeV ILC, with unpolarized beams.","The full simulation of SM background and fast simulation of the signal is performed, simulating 8 ab$^{-1}$ of data collected with the ILD detector.","We demonstrate that the CP mixing angle $\\Psi_{\\mathrm{CP}}$ between scalar and pseudoscalar states can be measured with the statistical uncertainty of 3.8 mrad at 68% CL, corresponding to 1.44 $\\cdot 10^{-5}$ for the CP parameter $f_\\mathrm{CP}$, for the pure scalar state.","This is the first result on sensitivity of an $e^{+}e^{-}$ collider to measure $f_\\mathrm{CP}$ in the Higgs production vertex in vector boson fusion."],"url":"http://arxiv.org/abs/2405.05820v1","category":"hep-ex"}
{"created":"2024-05-09 14:43:09","title":"NeuRSS: Enhancing AUV Localization and Bathymetric Mapping with Neural Rendering for Sidescan SLAM","abstract":"Implicit neural representations and neural rendering have gained increasing attention for bathymetry estimation from sidescan sonar (SSS). These methods incorporate multiple observations of the same place from SSS data to constrain the elevation estimate, converging to a globally-consistent bathymetric model. However, the quality and precision of the bathymetric estimate are limited by the positioning accuracy of the autonomous underwater vehicle (AUV) equipped with the sonar. The global positioning estimate of the AUV relying on dead reckoning (DR) has an unbounded error due to the absence of a geo-reference system like GPS underwater. To address this challenge, we propose in this letter a modern and scalable framework, NeuRSS, for SSS SLAM based on DR and loop closures (LCs) over large timescales, with an elevation prior provided by the bathymetric estimate using neural rendering from SSS. This framework is an iterative procedure that improves localization and bathymetric mapping. Initially, the bathymetry estimated from SSS using the DR estimate, though crude, can provide an important elevation prior in the nonlinear least-squares (NLS) optimization that estimates the relative pose between two loop-closure vertices in a pose graph. Subsequently, the global pose estimate from the SLAM component improves the positioning estimate of the vehicle, thus improving the bathymetry estimation. We validate our localization and mapping approach on two large surveys collected with a surface vessel and an AUV, respectively. We evaluate their localization results against the ground truth and compare the bathymetry estimation against data collected with multibeam echo sounders (MBES).","sentences":["Implicit neural representations and neural rendering have gained increasing attention for bathymetry estimation from sidescan sonar (SSS).","These methods incorporate multiple observations of the same place from SSS data to constrain the elevation estimate, converging to a globally-consistent bathymetric model.","However, the quality and precision of the bathymetric estimate are limited by the positioning accuracy of the autonomous underwater vehicle (AUV) equipped with the sonar.","The global positioning estimate of the AUV relying on dead reckoning (DR) has an unbounded error due to the absence of a geo-reference system like GPS underwater.","To address this challenge, we propose in this letter a modern and scalable framework, NeuRSS, for SSS SLAM based on DR and loop closures (LCs) over large timescales, with an elevation prior provided by the bathymetric estimate using neural rendering from SSS.","This framework is an iterative procedure that improves localization and bathymetric mapping.","Initially, the bathymetry estimated from SSS using the DR estimate, though crude, can provide an important elevation prior in the nonlinear least-squares (NLS) optimization that estimates the relative pose between two loop-closure vertices in a pose graph.","Subsequently, the global pose estimate from the SLAM component improves the positioning estimate of the vehicle, thus improving the bathymetry estimation.","We validate our localization and mapping approach on two large surveys collected with a surface vessel and an AUV, respectively.","We evaluate their localization results against the ground truth and compare the bathymetry estimation against data collected with multibeam echo sounders (MBES)."],"url":"http://arxiv.org/abs/2405.05807v1","category":"cs.RO"}
{"created":"2024-05-09 12:43:43","title":"Change point localisation and inference in fragmented functional data","abstract":"We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment. The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points. To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates. With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges. Such results are the first time seen in the fragmented functional data literature. As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021). Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions. Extensive numerical studies are also provided to support our theoretical results.","sentences":["We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment.","The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points.","To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates.","With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges.","Such results are the first time seen in the fragmented functional data literature.","As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021).","Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions.","Extensive numerical studies are also provided to support our theoretical results."],"url":"http://arxiv.org/abs/2405.05730v1","category":"stat.ME"}
{"created":"2024-05-09 09:08:09","title":"Policy Gradient with Active Importance Sampling","abstract":"Importance sampling (IS) represents a fundamental technique for a large surge of off-policy reinforcement learning approaches. Policy gradient (PG) methods, in particular, significantly benefit from IS, enabling the effective reuse of previously collected samples, thus increasing sample efficiency. However, classically, IS is employed in RL as a passive tool for re-weighting historical samples. However, the statistical community employs IS as an active tool combined with the use of behavioral distributions that allow the reduction of the estimate variance even below the sample mean one. In this paper, we focus on this second setting by addressing the behavioral policy optimization (BPO) problem. We look for the best behavioral policy from which to collect samples to reduce the policy gradient variance as much as possible. We provide an iterative algorithm that alternates between the cross-entropy estimation of the minimum-variance behavioral policy and the actual policy optimization, leveraging on defensive IS. We theoretically analyze such an algorithm, showing that it enjoys a convergence rate of order $O(\\epsilon^{-4})$ to a stationary point, but depending on a more convenient variance term w.r.t. standard PG methods. We then provide a practical version that is numerically validated, showing the advantages in the policy gradient estimation variance and on the learning speed.","sentences":["Importance sampling (IS) represents a fundamental technique for a large surge of off-policy reinforcement learning approaches.","Policy gradient (PG) methods, in particular, significantly benefit from IS, enabling the effective reuse of previously collected samples, thus increasing sample efficiency.","However, classically, IS is employed in RL as a passive tool for re-weighting historical samples.","However, the statistical community employs IS as an active tool combined with the use of behavioral distributions that allow the reduction of the estimate variance even below the sample mean one.","In this paper, we focus on this second setting by addressing the behavioral policy optimization (BPO) problem.","We look for the best behavioral policy from which to collect samples to reduce the policy gradient variance as much as possible.","We provide an iterative algorithm that alternates between the cross-entropy estimation of the minimum-variance behavioral policy and the actual policy optimization, leveraging on defensive IS.","We theoretically analyze such an algorithm, showing that it enjoys a convergence rate of order $O(\\epsilon^{-4})$ to a stationary point, but depending on a more convenient variance term w.r.t. standard PG methods.","We then provide a practical version that is numerically validated, showing the advantages in the policy gradient estimation variance and on the learning speed."],"url":"http://arxiv.org/abs/2405.05630v1","category":"cs.LG"}
{"created":"2024-05-09 08:46:11","title":"The effect of anisotropy on the formation of heavy quarkonium bound states","abstract":"We study the real part of the static potential of a heavy quark-antiquark system in an anisotropic plasma medium. We use a quasi-particle approach where the collective dynamics of the plasma constituents is described using hard-loop perturbation theory. The parton distribution function is characterized by a set of parameters that can accurately describe the anisotropy of the plasma produced in a heavy ion collision. We calculate the potential numerically in strongly anisotropic systems and study the angular dependence of the distortion of the potential relative to the isotropic one. We obtain an analytic expression for the real part of the heavy quark potential in the limit of weak anisotropy using a model that expresses the potential in terms of effective screening masses that depend on the anisotropy parameters and the orientation of the quark-antiquark pair. A 1-dimensional potential is formulated in terms of angle averaged screening masses that incorporate the anisotropy of the medium into a radial coordinate. We solve the corresponding Schr\\\"odinger equation and show that the magnitude of the binding energy typically increases with anisotropy. Anisotropy can play an important role, especially in states with non-zero angular momentum. This means that the number of bound states that are formed could depend on specific characteristics of the anisotropy of the plasma. Our study suggests that plasma anisotropy plays an important role in the dynamics of heavy quarkonium and motivates further study.","sentences":["We study the real part of the static potential of a heavy quark-antiquark system in an anisotropic plasma medium.","We use a quasi-particle approach where the collective dynamics of the plasma constituents is described using hard-loop perturbation theory.","The parton distribution function is characterized by a set of parameters that can accurately describe the anisotropy of the plasma produced in a heavy ion collision.","We calculate the potential numerically in strongly anisotropic systems and study the angular dependence of the distortion of the potential relative to the isotropic one.","We obtain an analytic expression for the real part of the heavy quark potential in the limit of weak anisotropy using a model that expresses the potential in terms of effective screening masses that depend on the anisotropy parameters and the orientation of the quark-antiquark pair.","A 1-dimensional potential is formulated in terms of angle averaged screening masses that incorporate the anisotropy of the medium into a radial coordinate.","We solve the corresponding Schr\\\"odinger equation and show that the magnitude of the binding energy typically increases with anisotropy.","Anisotropy can play an important role, especially in states with non-zero angular momentum.","This means that the number of bound states that are formed could depend on specific characteristics of the anisotropy of the plasma.","Our study suggests that plasma anisotropy plays an important role in the dynamics of heavy quarkonium and motivates further study."],"url":"http://arxiv.org/abs/2405.05622v1","category":"hep-ph"}
{"created":"2024-05-09 17:49:04","title":"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning","abstract":"The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces \"Smurfs\", a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and execution without necessitating extra training. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents. The framework gives access to external tools to efficiently solve complex tasks. Our empirical investigation, featuring the mistral-7b-instruct model as a case study, showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.","sentences":["The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance.","Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly.","This paper introduces \"Smurfs\", a cutting-edge multi-agent framework designed to revolutionize the application of LLMs.","By transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and execution without necessitating extra training.","This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents.","The framework gives access to external tools to efficiently solve complex tasks.","Our empirical investigation, featuring the mistral-7b-instruct model as a case study, showcases Smurfs' superior capability in intricate tool utilization scenarios.","Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded performance of a GPT-4 model at 73.5%.","Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy.","This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems."],"url":"http://arxiv.org/abs/2405.05955v1","category":"cs.CL"}
{"created":"2024-05-09 17:36:23","title":"A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities","abstract":"Politics is the set of activities related to strategic decision-making in groups. Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions. While some decisions might alleviate social problems, others might lead to disasters such as war and conflict. Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts. However, political researchers' perspectives on data visualization are unclear. This paper examines political scientists' perspectives on visualization and how they apply data visualization in their research. We discovered a growing trend in the use of graphs in political science journals. However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers. To reduce this gap, we survey visualization techniques applicable to the political scientists' research and report the visual analytics systems implemented for and evaluated by political scientists. At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics. Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective.","sentences":["Politics is the set of activities related to strategic decision-making in groups.","Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions.","While some decisions might alleviate social problems, others might lead to disasters such as war and conflict.","Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts.","However, political researchers' perspectives on data visualization are unclear.","This paper examines political scientists' perspectives on visualization and how they apply data visualization in their research.","We discovered a growing trend in the use of graphs in political science journals.","However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers.","To reduce this gap, we survey visualization techniques applicable to the political scientists' research and report the visual analytics systems implemented for and evaluated by political scientists.","At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics.","Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective."],"url":"http://arxiv.org/abs/2405.05947v1","category":"cs.HC"}
{"created":"2024-05-09 17:24:06","title":"Dynamics of a Towed Cable with Sensor-Array for Underwater Target Motion Analysis","abstract":"During a war situation, many times an underwater target motion analysis (TMA) is performed using bearing-only measurements, obtained from a sensor array, which is towed by an own-ship with the help of a connected cable. It is well known that the own-ship is required to perform a manoeuvre in order to make the system observable and localise the target successfully. During the maneuver, it is important to know the location of the sensor array with respect to the own-ship. This paper develops a dynamic model of a cable-sensor array system to localise the sensor array, which is towed behind a sea-surface vessel. We adopt a lumped-mass approach to represent the towed cable. The discretized cable elements are modelled as an interconnected rigid body, kinematically related to one another. The governing equations are derived by balancing the moments acting on each node. The derived dynamics are solved simultaneously for all the nodes to determine the orientation of the cable and sensor array. The position of the sensor array obtained from this proposed model will further be used by TMA algorithms to enhance the accuracy of the tracking system.","sentences":["During a war situation, many times an underwater target motion analysis (TMA) is performed using bearing-only measurements, obtained from a sensor array, which is towed by an own-ship with the help of a connected cable.","It is well known that the own-ship is required to perform a manoeuvre in order to make the system observable and localise the target successfully.","During the maneuver, it is important to know the location of the sensor array with respect to the own-ship.","This paper develops a dynamic model of a cable-sensor array system to localise the sensor array, which is towed behind a sea-surface vessel.","We adopt a lumped-mass approach to represent the towed cable.","The discretized cable elements are modelled as an interconnected rigid body, kinematically related to one another.","The governing equations are derived by balancing the moments acting on each node.","The derived dynamics are solved simultaneously for all the nodes to determine the orientation of the cable and sensor array.","The position of the sensor array obtained from this proposed model will further be used by TMA algorithms to enhance the accuracy of the tracking system."],"url":"http://arxiv.org/abs/2405.05937v1","category":"eess.SP"}
{"created":"2024-05-09 17:18:39","title":"Interfacial proximity and interplay between Kondo and short-range magnetic correlations in heterostructures","abstract":"In this work, we investigate the influence of interlayer distance in a heterostructure containing both Kondo effects and short-range magnetic correlations. Our proposed heterostructure comprises three coupled square lattice layers. The first layer is governed by the Kondo-Heisenberg lattice model involving $f$- and $d$-electrons, which interact via Kondo and Heisenberg couplings, $J_{K}$ and $J_{H}$, respectively. The other two layers consist of non-interacting itinerant electrons, where coupling with the first layer is determined by two perpendicular hopping parameters. We find that varying the interlayer couplings induces electronic dynamics at the interface, altering the behavior of mean-field parameters describing the Kondo effect and short-range magnetic correlations. The system's temperature - interlayer hopping parameter phase diagram exhibits a sequence of discontinuous and continuous transitions. In the cases, $|J_{K}|<|J_{H}|$ and $|J_{K}|>|J_{H}|$ rich phase diagrams are found which include Kondo, ferromagnetic and antiferromagnetic correlations. Our work provides insights into hosting Kondo correlations in heterostructures.","sentences":["In this work, we investigate the influence of interlayer distance in a heterostructure containing both Kondo effects and short-range magnetic correlations.","Our proposed heterostructure comprises three coupled square lattice layers.","The first layer is governed by the Kondo-Heisenberg lattice model involving $f$- and $d$-electrons, which interact via Kondo and Heisenberg couplings, $J_{K}$ and $J_{H}$, respectively.","The other two layers consist of non-interacting itinerant electrons, where coupling with the first layer is determined by two perpendicular hopping parameters.","We find that varying the interlayer couplings induces electronic dynamics at the interface, altering the behavior of mean-field parameters describing the Kondo effect and short-range magnetic correlations.","The system's temperature - interlayer hopping parameter phase diagram exhibits a sequence of discontinuous and continuous transitions.","In the cases, $|J_{K}|<|J_{H}|$ and $|J_{K}|>|J_{H}|$ rich phase diagrams are found which include Kondo, ferromagnetic and antiferromagnetic correlations.","Our work provides insights into hosting Kondo correlations in heterostructures."],"url":"http://arxiv.org/abs/2405.05935v1","category":"cond-mat.str-el"}
{"created":"2024-05-09 16:52:43","title":"A Comprehensive Survey of Masked Faces: Recognition, Detection, and Unmasking","abstract":"Masked face recognition (MFR) has emerged as a critical domain in biometric identification, especially by the global COVID-19 pandemic, which introduced widespread face masks. This survey paper presents a comprehensive analysis of the challenges and advancements in recognising and detecting individuals with masked faces, which has seen innovative shifts due to the necessity of adapting to new societal norms. Advanced through deep learning techniques, MFR, along with Face Mask Recognition (FMR) and Face Unmasking (FU), represent significant areas of focus. These methods address unique challenges posed by obscured facial features, from fully to partially covered faces. Our comprehensive review delves into the various deep learning-based methodologies developed for MFR, FMR, and FU, highlighting their distinctive challenges and the solutions proposed to overcome them. Additionally, we explore benchmark datasets and evaluation metrics specifically tailored for assessing performance in MFR research. The survey also discusses the substantial obstacles still facing researchers in this field and proposes future directions for the ongoing development of more robust and effective masked face recognition systems. This paper serves as an invaluable resource for researchers and practitioners, offering insights into the evolving landscape of face recognition technologies in the face of global health crises and beyond.","sentences":["Masked face recognition (MFR) has emerged as a critical domain in biometric identification, especially by the global COVID-19 pandemic, which introduced widespread face masks.","This survey paper presents a comprehensive analysis of the challenges and advancements in recognising and detecting individuals with masked faces, which has seen innovative shifts due to the necessity of adapting to new societal norms.","Advanced through deep learning techniques, MFR, along with Face Mask Recognition (FMR) and Face Unmasking (FU), represent significant areas of focus.","These methods address unique challenges posed by obscured facial features, from fully to partially covered faces.","Our comprehensive review delves into the various deep learning-based methodologies developed for MFR, FMR, and FU, highlighting their distinctive challenges and the solutions proposed to overcome them.","Additionally, we explore benchmark datasets and evaluation metrics specifically tailored for assessing performance in MFR research.","The survey also discusses the substantial obstacles still facing researchers in this field and proposes future directions for the ongoing development of more robust and effective masked face recognition systems.","This paper serves as an invaluable resource for researchers and practitioners, offering insights into the evolving landscape of face recognition technologies in the face of global health crises and beyond."],"url":"http://arxiv.org/abs/2405.05900v1","category":"cs.CV"}
{"created":"2024-05-09 16:51:40","title":"An Infinite Family of Integrable Sigma Models Using Auxiliary Fields","abstract":"We introduce a class of sigma models in two spacetime dimensions which are parameterized by an interaction function of one real variable. In addition to the physical group-valued field $g$, these models include an auxiliary vector field $v_\\alpha$ which mediates interactions in a prescribed way. We prove that every model in this family is weakly integrable, in the sense that the classical equations of motion are equivalent to flatness of a Lax connection for any value of a spectral parameter. We also show that these models are strongly integrable, in the sense that the Poisson bracket of the Lax connection takes the Maillet form, which guarantees the existence of an infinite set of conserved charges in involution. This class of theories includes the principal chiral model (PCM) and all deformations of the PCM by functions of the energy-momentum tensor, such as $T \\overline{T}$ and root-$T \\overline{T}$.","sentences":["We introduce a class of sigma models in two spacetime dimensions which are parameterized by an interaction function of one real variable.","In addition to the physical group-valued field $g$, these models include an auxiliary vector field $v_\\alpha$ which mediates interactions in a prescribed way.","We prove that every model in this family is weakly integrable, in the sense that the classical equations of motion are equivalent to flatness of a Lax connection for any value of a spectral parameter.","We also show that these models are strongly integrable, in the sense that the Poisson bracket of the Lax connection takes the Maillet form, which guarantees the existence of an infinite set of conserved charges in involution.","This class of theories includes the principal chiral model (PCM) and all deformations of the PCM by functions of the energy-momentum tensor, such as $T \\overline{T}$ and root-$T \\overline{T}$."],"url":"http://arxiv.org/abs/2405.05899v1","category":"hep-th"}
{"created":"2024-05-09 16:48:22","title":"Efficient numerical computation of spiral spectra with exponentially-weighted preconditioners","abstract":"The stability of nonlinear waves on spatially extended domains is commonly probed by computing the spectrum of the linearization of the underlying PDE about the wave profile. It is known that convective transport, whether driven by the nonlinear pattern itself or an underlying fluid flow, can cause exponential growth of the resolvent of the linearization as a function of the domain length. In particular, sparse eigenvalue algorithms may result in inaccurate and spurious spectra in the convective regime. In this work, we focus on spiral waves, which arise in many natural processes and which exhibit convective transport. We prove that exponential weights can serve as effective, inexpensive preconditioners that result in resolvents that are uniformly bounded in the domain size and that stabilize numerical spectral computations. We also show that the optimal exponential rates can be computed reliably from a simpler asymptotic problem posed in one space dimension.","sentences":["The stability of nonlinear waves on spatially extended domains is commonly probed by computing the spectrum of the linearization of the underlying PDE about the wave profile.","It is known that convective transport, whether driven by the nonlinear pattern itself or an underlying fluid flow, can cause exponential growth of the resolvent of the linearization as a function of the domain length.","In particular, sparse eigenvalue algorithms may result in inaccurate and spurious spectra in the convective regime.","In this work, we focus on spiral waves, which arise in many natural processes and which exhibit convective transport.","We prove that exponential weights can serve as effective, inexpensive preconditioners that result in resolvents that are uniformly bounded in the domain size and that stabilize numerical spectral computations.","We also show that the optimal exponential rates can be computed reliably from a simpler asymptotic problem posed in one space dimension."],"url":"http://arxiv.org/abs/2405.05897v1","category":"math.NA"}
{"created":"2024-05-09 16:17:41","title":"Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes","abstract":"Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields. However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals. To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model. Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system. In the real-world driving dataset, our system achieved 96.16% success rate in night scenes and 89.7% in gloomy scenes regarding reasonable predictions. Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver.","sentences":["Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields.","However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals.","To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes.","A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model.","Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system.","In the real-world driving dataset, our system achieved 96.16% success rate in night scenes and 89.7% in gloomy scenes regarding reasonable predictions.","Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver."],"url":"http://arxiv.org/abs/2405.05885v1","category":"cs.RO"}
{"created":"2024-05-09 16:10:02","title":"Cohen--Macaulay Complexes, Duality Groups, and the dualizing module of ${\\rm{Out}}(F_N)$","abstract":"We explain how Cohen--Macaulay classifying spaces are ubiquitous among discrete groups that satisfy Bieri--Eckmann duality, and compare Bieri--Eckmann duality to duality results for Cohen--Macaulay complexes. We use this comparison to give a description of the dualizing module of ${\\rm{Out}}(F_N)$ in terms of the local cohomology cosheaf of the spine of Outer space.","sentences":["We explain how Cohen--Macaulay classifying spaces are ubiquitous among discrete groups that satisfy Bieri--Eckmann duality, and compare Bieri--Eckmann duality to duality results for Cohen--Macaulay complexes.","We use this comparison to give a description of the dualizing module of ${\\rm{Out}}(F_N)$ in terms of the local cohomology cosheaf of the spine of Outer space."],"url":"http://arxiv.org/abs/2405.05881v1","category":"math.GR"}
{"created":"2024-05-09 16:05:57","title":"Optical contrast analysis of \u03b1-RuCl$_3$ nanoflakes on oxidized silicon wafers","abstract":"{\\alpha}-RuCl$_3$, a narrow-band Mott insulator with large work function, offers intriguing potential as a quantum material or as a charge acceptor for electrical contacts in van der Waals devices. In this work, we perform a systematic study of the optical reflection contrast of {\\alpha}-RuCl$_3$ nanoflakes on oxidized silicon wafers and estimate the accuracy of this imaging technique to assess the crystal thickness. Via spectroscopic micro-ellipsometry measurements, we characterize the wavelength-dependent complex refractive index of {\\alpha}-RuCl$_3$ nanoflakes of varying thickness in the visible and near-infrared. Building on these results, we simulate the optical contrast of {\\alpha}-RuCl$_3$ nanoflakes with thicknesses below 100 nm on SiO$_2$/Si substrates under different illumination conditions. We compare the simulated optical contrast with experimental values extracted from optical microscopy images and obtain good agreement. Finally, we show that optical contrast imaging allows us to retrieve the thickness of the RuCl$_3$ nanoflakes exfoliated on an oxidized silicon substrate with a mean deviation of -0.2 nm for thicknesses below 100 nm with a standard deviation of only 1 nm. Our results demonstrate that optical contrast can be used as a non-invasive, fast, and reliable technique to estimate the {\\alpha}-RuCl$_3$ thickness.","sentences":["{\\alpha}-RuCl$_3$, a narrow-band Mott insulator with large work function, offers intriguing potential as a quantum material or as a charge acceptor for electrical contacts in van der Waals devices.","In this work, we perform a systematic study of the optical reflection contrast of {\\alpha}-RuCl$_3$ nanoflakes on oxidized silicon wafers and estimate the accuracy of this imaging technique to assess the crystal thickness.","Via spectroscopic micro-ellipsometry measurements, we characterize the wavelength-dependent complex refractive index of {\\alpha}-RuCl$_3$ nanoflakes of varying thickness in the visible and near-infrared.","Building on these results, we simulate the optical contrast of {\\alpha}-RuCl$_3$ nanoflakes with thicknesses below 100 nm on SiO$_2$/Si substrates under different illumination conditions.","We compare the simulated optical contrast with experimental values extracted from optical microscopy images and obtain good agreement.","Finally, we show that optical contrast imaging allows us to retrieve the thickness of the RuCl$_3$ nanoflakes exfoliated on an oxidized silicon substrate with a mean deviation of -0.2 nm for thicknesses below 100 nm with a standard deviation of only 1 nm.","Our results demonstrate that optical contrast can be used as a non-invasive, fast, and reliable technique to estimate the {\\alpha}-RuCl$_3$ thickness."],"url":"http://arxiv.org/abs/2405.05880v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 16:05:39","title":"Uniqueness Problem for the Backward Differential Equation of a Continuous-State Branching Process","abstract":"The distributional properties of a multi-dimensional continuous-state branching process are determined by its cumulant semigroup, which is defined by the backward differential equation. We provide a proof of the assertion of Rhyzhov and Skorokhod (Theory Probab. Appl., 1970) on the uniqueness of the solutions to the equation, which is based on a characterization of the process as the pathwise unique solution to a system of stochastic equations.","sentences":["The distributional properties of a multi-dimensional continuous-state branching process are determined by its cumulant semigroup, which is defined by the backward differential equation.","We provide a proof of the assertion of Rhyzhov and Skorokhod (Theory Probab.","Appl., 1970) on the uniqueness of the solutions to the equation, which is based on a characterization of the process as the pathwise unique solution to a system of stochastic equations."],"url":"http://arxiv.org/abs/2405.05879v1","category":"math.PR"}
{"created":"2024-05-09 16:02:09","title":"Duality for Cohen--Macaulay Complexes through Combinatorial Sheaves","abstract":"We prove a duality theorem for Cohen--Macaulay simplicial complexes. This is a generalisation of Poincar\\'e Duality, framed in the language of combinatorial sheaves. Our treatment is self-contained and accessible for readers with a working knowledge of simplicial complexes and (co)homology. The main motivation is a link with Bieri-Eckmann duality for discrete groups, which is explored in a companion paper.","sentences":["We prove a duality theorem for Cohen--Macaulay simplicial complexes.","This is a generalisation of Poincar\\'e Duality, framed in the language of combinatorial sheaves.","Our treatment is self-contained and accessible for readers with a working knowledge of simplicial complexes and (co)homology.","The main motivation is a link with Bieri-Eckmann duality for discrete groups, which is explored in a companion paper."],"url":"http://arxiv.org/abs/2405.05873v1","category":"math.AT"}
{"created":"2024-05-09 15:54:02","title":"Parameter identification for an uncertain reaction-diffusion equation via setpoint regulation","abstract":"The problem of estimating the reaction coefficient of a system governed by a reaction-diffusion partial differential equation is tackled. An estimator relying on boundary measurements only is proposed. The estimator is based upon a setpoint regulation strategy and leads to an asymptotically converging estimate of the unknown reaction coefficient. The proposed estimator is combined with a state observer and shown to provide an asymptotic estimate of the actual system state. A numerical example supports and illustrates the theoretical results.","sentences":["The problem of estimating the reaction coefficient of a system governed by a reaction-diffusion partial differential equation is tackled.","An estimator relying on boundary measurements only is proposed.","The estimator is based upon a setpoint regulation strategy and leads to an asymptotically converging estimate of the unknown reaction coefficient.","The proposed estimator is combined with a state observer and shown to provide an asymptotic estimate of the actual system state.","A numerical example supports and illustrates the theoretical results."],"url":"http://arxiv.org/abs/2405.05866v1","category":"math.OC"}
{"created":"2024-05-09 15:53:43","title":"Faster Linear Systems and Matrix Norm Approximation via Multi-level Sketched Preconditioning","abstract":"We present a new class of preconditioned iterative methods for solving linear systems of the form $Ax = b$. Our methods are based on constructing a low-rank Nystr\\\"om approximation to $A$ using sparse random sketching. This approximation is used to construct a preconditioner, which itself is inverted quickly using additional levels of random sketching and preconditioning. We prove that the convergence of our methods depends on a natural average condition number of $A$, which improves as the rank of the Nystr\\\"om approximation increases. Concretely, this allows us to obtain faster runtimes for a number of fundamental linear algebraic problems:   1. We show how to solve any $n\\times n$ linear system that is well-conditioned except for $k$ outlying large singular values in $\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of [Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.   2. We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm for solving a regularized linear system $(A + \\lambda I)x = b$, where $A$ is positive semidefinite with effective dimension $d_\\lambda$. This problem arises in applications like Gaussian process regression.   3. We give faster algorithms for approximating Schatten $p$-norms and other matrix norms. For example, for the Schatten 1 (nuclear) norm, we give an algorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an $\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018].   Interestingly, previous state-of-the-art algorithms for most of the problems above relied on stochastic iterative methods, like stochastic coordinate and gradient descent. Our work takes a completely different approach, instead leveraging tools from matrix sketching.","sentences":["We present a new class of preconditioned iterative methods for solving linear systems of the form $Ax = b$. Our methods are based on constructing a low-rank Nystr\\\"om approximation to $A$ using sparse random sketching.","This approximation is used to construct a preconditioner, which itself is inverted quickly using additional levels of random sketching and preconditioning.","We prove that the convergence of our methods depends on a natural average condition number of $A$, which improves as the rank of the Nystr\\\"om approximation increases.","Concretely, this allows us to obtain faster runtimes for a number of fundamental linear algebraic problems:   1.","We show how to solve any $n\\times n$ linear system that is well-conditioned except for $k$ outlying large singular values in $\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of [Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.   2.","We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm for solving a regularized linear system $(A + \\lambda I)x","= b$, where $A$ is positive semidefinite with effective dimension $d_\\lambda$. This problem arises in applications like Gaussian process regression.   ","3.","We give faster algorithms for approximating Schatten $p$-norms and other matrix norms.","For example, for the Schatten 1 (nuclear) norm, we give an algorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an $\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018].   ","Interestingly, previous state-of-the-art algorithms for most of the problems above relied on stochastic iterative methods, like stochastic coordinate and gradient descent.","Our work takes a completely different approach, instead leveraging tools from matrix sketching."],"url":"http://arxiv.org/abs/2405.05865v1","category":"cs.DS"}
{"created":"2024-05-09 15:49:42","title":"Strong electrostatic control of excitonic features in MoS$_2$ by a free-standing ultrahigh-$\u03ba$ ferroelectric perovskite","abstract":"We present the electrostatic control of photoluminescence of monolayer MoS$_2$ at room temperature via integration of free-standing BaTiO$_3$ (BTO), a ferroelectric perovskite oxide, layers. We show that the use of BTO leads to highly tunable exciton emission of MoS$_2$ in a minimal range of gate voltages, effectively controlling the neutral excitons to charged excitons (trions) conversion. Due to BTO's ferroelectric polarization-induced doping we observe large peak emission shifts as well as a large and tunable A trion binding energy in the range of 40-100 meV. To further investigate the efficacy of electrostatic control, we compared our measurements with those carried out when the BTO is replaced by a hexagonal boron nitride (hBN) dielectric layer of comparable thickness, confirming BTO's superior gating properties and thus lower power consumption. Additionally, we take advantage of the ferroelectric switching of BTO by fabricating devices where the BTO layer is decoupled from the gate electrode with a SiO$_2$ layer. Choosing to isolate the BTO allows us to induce large remanent behavior of MoS$_2$'s excitonic features, observing hysteretic behavior in the peak energy ratio between A exciton and its trion, as well as hysteretic behavior in the doping-related trion energy shift. This study illustrates the rich physics involved in combining free-standing complex oxide layers with two-dimensional materials.","sentences":["We present the electrostatic control of photoluminescence of monolayer MoS$_2$ at room temperature via integration of free-standing BaTiO$_3$ (BTO), a ferroelectric perovskite oxide, layers.","We show that the use of BTO leads to highly tunable exciton emission of MoS$_2$ in a minimal range of gate voltages, effectively controlling the neutral excitons to charged excitons (trions) conversion.","Due to BTO's ferroelectric polarization-induced doping we observe large peak emission shifts as well as a large and tunable A trion binding energy in the range of 40-100 meV. To further investigate the efficacy of electrostatic control, we compared our measurements with those carried out when the BTO is replaced by a hexagonal boron nitride (hBN) dielectric layer of comparable thickness, confirming BTO's superior gating properties and thus lower power consumption.","Additionally, we take advantage of the ferroelectric switching of BTO by fabricating devices where the BTO layer is decoupled from the gate electrode with a SiO$_2$ layer.","Choosing to isolate the BTO allows us to induce large remanent behavior of MoS$_2","$'s excitonic features, observing hysteretic behavior in the peak energy ratio between A exciton and its trion, as well as hysteretic behavior in the doping-related trion energy shift.","This study illustrates the rich physics involved in combining free-standing complex oxide layers with two-dimensional materials."],"url":"http://arxiv.org/abs/2405.05862v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:41:59","title":"Infinitely many isolas of modulational instability for Stokes waves","abstract":"We prove the long-standing conjecture regarding the existence of infinitely many high-frequency modulational instability ``isolas\" for a Stokes wave in arbitrary depth $ \\mathtt{h} > 0 $, subject to longitudinal perturbations. We completely describe the spectral bands with non-zero real part away from the origin of the $L^2(\\mathbb{R})$-spectrum of the water waves system linearized at a Stokes waves of small amplitude $ \\epsilon > 0 $. The unstable spectrum is the union of isolas of elliptical shape, parameterized by integers $ \\mathtt{p}\\geq 2 $, with semiaxis of size $ |\\beta_1^{(\\mathtt{p})} (\\mathtt{h})| \\epsilon^\\mathtt{p}+ O(\\epsilon^{\\mathtt{p}+1} )$ where $\\beta_1^{( \\mathtt{p})} (\\mathtt{h})$ is a nonzero analytic function of the depth $ \\mathtt{h} $ that depends on the Taylor coefficients of the Stokes waves up to order $\\mathtt{p}$.","sentences":["We prove the long-standing conjecture regarding the existence of infinitely many high-frequency modulational instability ``isolas\" for a Stokes wave in arbitrary depth $ \\mathtt{h} > 0 $, subject to longitudinal perturbations.","We completely describe the spectral bands with non-zero real part away from the origin of the $L^2(\\mathbb{R})$-spectrum of the water waves system linearized at a Stokes waves of small amplitude $ \\epsilon > 0 $.","The unstable spectrum is the union of isolas of elliptical shape, parameterized by integers $ \\mathtt{p}\\geq 2 $, with semiaxis of size $ |\\beta_1^{(\\mathtt{p})} (\\mathtt{h})| \\epsilon^\\mathtt{p}+ O(\\epsilon^{\\mathtt{p}+1} )$ where $\\beta_1^{( \\mathtt{p})} (\\mathtt{h})$ is a nonzero analytic function of the depth $ \\mathtt{h} $ that depends on the Taylor coefficients of the Stokes waves up to order $\\mathtt{p}$."],"url":"http://arxiv.org/abs/2405.05854v1","category":"math.AP"}
{"created":"2024-05-09 15:34:20","title":"Age of Information and Energy Consumption in IoT: an Experimental Evaluation","abstract":"The Age of Information (AoI) is an end-to-end metric frequently used to understand how \"fresh\" the information about a remote system is. In this paper, we present an experimental study of the relationship between AoI and the energy spent by the device that produces information, e.g. an IoT device or a monitoring sensor. Such a relationship has been almost neglected so far, but it is particularly important whenever the sensing side is battery-operated. The study is carried out in a scenario where access is achieved via the cellular network and information is transferred using MQTT, a popular messaging protocol in the IoT domain. Numerous parameters of operation are considered, and the most efficient solutions in all configurations are provided.","sentences":["The Age of Information (AoI) is an end-to-end metric frequently used to understand how \"fresh\" the information about a remote system is.","In this paper, we present an experimental study of the relationship between AoI and the energy spent by the device that produces information, e.g. an IoT device or a monitoring sensor.","Such a relationship has been almost neglected so far, but it is particularly important whenever the sensing side is battery-operated.","The study is carried out in a scenario where access is achieved via the cellular network and information is transferred using MQTT, a popular messaging protocol in the IoT domain.","Numerous parameters of operation are considered, and the most efficient solutions in all configurations are provided."],"url":"http://arxiv.org/abs/2405.05849v1","category":"cs.NI"}
{"created":"2024-05-09 15:34:18","title":"Distributed Estimation for a 3-D Moving Target in Quaternion Space with Unknown Correlation","abstract":"For distributed estimations in a sensor network, the consistency and accuracy of an estimator are greatly affected by the unknown correlations between individual estimates. An inconsistent or too conservative estimate may degrade the estimation performance and even cause divergence of the estimator. Cooperative estimation methods based on Inverse Covariance Intersection (ICI) can utilize a network of sensors to provide a consistent and tight estimate of a target. In this paper, unlike most existing ICI-based estimators that only consider two-dimensional (2-D) target state estimation in the vector space, we address this problem in a 3-D environment by extending the ICI algorithm to the augmented quaternion space. In addition, the proposed algorithm is fully distributed, as each agent only uses the local information from itself and its communication neighbors, which is also robust to a time-varying communication topology. To evaluate the performance, we test the proposed algorithm in a camera network to track the pose of a target. Extensive Monte Carlo simulations have been performed to show the effectiveness of our approach.","sentences":["For distributed estimations in a sensor network, the consistency and accuracy of an estimator are greatly affected by the unknown correlations between individual estimates.","An inconsistent or too conservative estimate may degrade the estimation performance and even cause divergence of the estimator.","Cooperative estimation methods based on Inverse Covariance Intersection (ICI) can utilize a network of sensors to provide a consistent and tight estimate of a target.","In this paper, unlike most existing ICI-based estimators that only consider two-dimensional (2-D) target state estimation in the vector space, we address this problem in a 3-D environment by extending the ICI algorithm to the augmented quaternion space.","In addition, the proposed algorithm is fully distributed, as each agent only uses the local information from itself and its communication neighbors, which is also robust to a time-varying communication topology.","To evaluate the performance, we test the proposed algorithm in a camera network to track the pose of a target.","Extensive Monte Carlo simulations have been performed to show the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.05848v1","category":"eess.SY"}
{"created":"2024-05-09 15:30:11","title":"Structure-preserving parametric finite element methods for simulating axisymmetric solid-state dewetting problems with anisotropic surface energies","abstract":"Solid-state dewetting (SSD), a widespread phenomenon in solid-solid-vapor system, could be used to describe the accumulation of solid thin films on the substrate. In this work, we consider the sharp interface model for axisymmetric SSD with anisotropic surface energy. By introducing two types of surface energy matrices from the anisotropy functions,we aim to design two structure-preserving algorithms for the axisymmetric SSD. The newly designed schemes are applicable to a broader range of anisotropy functions, and we can theoretically prove their volume conservation and energy stability. In addition, based on a novel weak formulation for the axisymmetric SSD, we further build another two numerical schemes that have good mesh properties. Finally, numerous numerical tests are reported to showcase the accuracy and efficiency of the numerical methods.","sentences":["Solid-state dewetting (SSD), a widespread phenomenon in solid-solid-vapor system, could be used to describe the accumulation of solid thin films on the substrate.","In this work, we consider the sharp interface model for axisymmetric SSD with anisotropic surface energy.","By introducing two types of surface energy matrices from the anisotropy functions,we aim to design two structure-preserving algorithms for the axisymmetric SSD.","The newly designed schemes are applicable to a broader range of anisotropy functions, and we can theoretically prove their volume conservation and energy stability.","In addition, based on a novel weak formulation for the axisymmetric SSD, we further build another two numerical schemes that have good mesh properties.","Finally, numerous numerical tests are reported to showcase the accuracy and efficiency of the numerical methods."],"url":"http://arxiv.org/abs/2405.05844v1","category":"math.NA"}
{"created":"2024-05-09 15:16:22","title":"Altermagnetic Polar Metallic phase in Ultra-Thin Epitaxially-Strained RuO2 Films","abstract":"Altermagnetism refers to a wide class of compensated magnetic orders featuring magnetic sublattices with opposite spins related by rotational symmetry rather than inversion or translational operations, resulting in non-trivial spin splitting and high-order multipolar orders. Here, by combining theoretical analysis, electrical transport, X-ray and optical spectroscopies, and nonlinear optical measurements, we establish a phase diagram in hybrid molecular beam epitaxy-grown RuO2/TiO2 (110) films, mapping the broken symmetries along the altermagnetic/electronic/structural phase transitions as functions of film thickness and temperature. This phase diagram features a novel altermagnetic metallic polar phase in strained 2 nm samples, extending the concept of multiferroics to altermagnetic systems. These results provide a comprehensive understanding of altermagnetism upon epitaxial heterostructure design for emergent novel phases with multifunctionalities.","sentences":["Altermagnetism refers to a wide class of compensated magnetic orders featuring magnetic sublattices with opposite spins related by rotational symmetry rather than inversion or translational operations, resulting in non-trivial spin splitting and high-order multipolar orders.","Here, by combining theoretical analysis, electrical transport, X-ray and optical spectroscopies, and nonlinear optical measurements, we establish a phase diagram in hybrid molecular beam epitaxy-grown RuO2/TiO2 (110) films, mapping the broken symmetries along the altermagnetic/electronic/structural phase transitions as functions of film thickness and temperature.","This phase diagram features a novel altermagnetic metallic polar phase in strained 2 nm samples, extending the concept of multiferroics to altermagnetic systems.","These results provide a comprehensive understanding of altermagnetism upon epitaxial heterostructure design for emergent novel phases with multifunctionalities."],"url":"http://arxiv.org/abs/2405.05838v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:15:34","title":"Informed Decision-Making through Advancements in Open Set Recognition and Unknown Sample Detection","abstract":"Machine learning-based techniques open up many opportunities and improvements to derive deeper and more practical insights from data that can help businesses make informed decisions. However, the majority of these techniques focus on the conventional closed-set scenario, in which the label spaces for the training and test sets are identical. Open set recognition (OSR) aims to bring classification tasks in a situation that is more like reality, which focuses on classifying the known classes as well as handling unknown classes effectively. In such an open-set problem the gathered samples in the training set cannot encompass all the classes and the system needs to identify unknown samples at test time. On the other hand, building an accurate and comprehensive model in a real dynamic environment presents a number of obstacles, because it is prohibitively expensive to train for every possible example of unknown items, and the model may fail when tested in testbeds. This study provides an algorithm exploring a new representation of feature space to improve classification in OSR tasks. The efficacy and efficiency of business processes and decision-making can be improved by integrating OSR, which offers more precise and insightful predictions of outcomes. We demonstrate the performance of the proposed method on three established datasets. The results indicate that the proposed model outperforms the baseline methods in accuracy and F1-score.","sentences":["Machine learning-based techniques open up many opportunities and improvements to derive deeper and more practical insights from data that can help businesses make informed decisions.","However, the majority of these techniques focus on the conventional closed-set scenario, in which the label spaces for the training and test sets are identical.","Open set recognition (OSR) aims to bring classification tasks in a situation that is more like reality, which focuses on classifying the known classes as well as handling unknown classes effectively.","In such an open-set problem the gathered samples in the training set cannot encompass all the classes and the system needs to identify unknown samples at test time.","On the other hand, building an accurate and comprehensive model in a real dynamic environment presents a number of obstacles, because it is prohibitively expensive to train for every possible example of unknown items, and the model may fail when tested in testbeds.","This study provides an algorithm exploring a new representation of feature space to improve classification in OSR tasks.","The efficacy and efficiency of business processes and decision-making can be improved by integrating OSR, which offers more precise and insightful predictions of outcomes.","We demonstrate the performance of the proposed method on three established datasets.","The results indicate that the proposed model outperforms the baseline methods in accuracy and F1-score."],"url":"http://arxiv.org/abs/2405.05836v1","category":"cs.LG"}
{"created":"2024-05-09 15:13:34","title":"The Riemann hypothesis and dynamics of Backtracking New Q-Newton's method","abstract":"A new variant of Newton's method -- named Backtracking New Q-Newton's method (BNQN) -- was recently introduced by the second author. This method has good convergence guarantees, specially concerning finding roots of meromorphic functions. This paper explores using BNQN for the Riemann xi function. We show in particular that the Riemann hypothesis is equivalent to that all attractors of BNQN lie on the critical line. We also explain how an apparent relation between the basins of attraction of BNQN and Voronoi's diagram can be helpful for verifying the Riemann hypothesis or finding a counterexample to it. Some illustrating experimental results are included, which convey some interesting phenomena. The experiments show that BNQN works very stably with highly transcendental functions like the Riemann xi function and its derivatives. Based on insights from the experiments, we discuss some concrete steps on using BNQN towards the Riemann hypothesis. Ideas and results from this paper can be extended to other zeta functions.","sentences":["A new variant of Newton's method -- named Backtracking New Q-Newton's method (BNQN) -- was recently introduced by the second author.","This method has good convergence guarantees, specially concerning finding roots of meromorphic functions.","This paper explores using BNQN for the Riemann xi function.","We show in particular that the Riemann hypothesis is equivalent to that all attractors of BNQN lie on the critical line.","We also explain how an apparent relation between the basins of attraction of BNQN and Voronoi's diagram can be helpful for verifying the Riemann hypothesis or finding a counterexample to it.","Some illustrating experimental results are included, which convey some interesting phenomena.","The experiments show that BNQN works very stably with highly transcendental functions like the Riemann xi function and its derivatives.","Based on insights from the experiments, we discuss some concrete steps on using BNQN towards the Riemann hypothesis.","Ideas and results from this paper can be extended to other zeta functions."],"url":"http://arxiv.org/abs/2405.05834v1","category":"math.DS"}
{"created":"2024-05-09 15:06:46","title":"Common information in well-mixing graphs and applications to information-theoretic cryptography","abstract":"We study the connection between mixing properties for bipartite graphs and materialization of the mutual information in one-shot settings. We show that mixing properties of a graph imply impossibility to extract the mutual information shared by the ends of an edge randomly sampled in the graph. We apply these impossibility results to some questions motivated by information-theoretic cryptography. In particular, we show that communication complexity of a secret key agreement in one-shot setting is inherently uneven: for some inputs, almost all communication complexity inevitably falls on only one party.","sentences":["We study the connection between mixing properties for bipartite graphs and materialization of the mutual information in one-shot settings.","We show that mixing properties of a graph imply impossibility to extract the mutual information shared by the ends of an edge randomly sampled in the graph.","We apply these impossibility results to some questions motivated by information-theoretic cryptography.","In particular, we show that communication complexity of a secret key agreement in one-shot setting is inherently uneven: for some inputs, almost all communication complexity inevitably falls on only one party."],"url":"http://arxiv.org/abs/2405.05831v1","category":"cs.IT"}
{"created":"2024-05-09 15:02:26","title":"MAD-ICP: It Is All About Matching Data -- Robust and Informed LiDAR Odometry","abstract":"LiDAR odometry is the task of estimating the ego-motion of the sensor from sequential laser scans. This problem has been addressed by the community for more than two decades, and many effective solutions are available nowadays. Most of these systems implicitly rely on assumptions about the operating environment, the sensor used, and motion pattern. When these assumptions are violated, several well-known systems tend to perform poorly. This paper presents a LiDAR odometry system that can overcome these limitations and operate well under different operating conditions while achieving performance comparable with domain-specific methods. Our algorithm follows the well-known ICP paradigm that leverages a PCA-based kd-tree implementation that is used to extract structural information about the clouds being registered and to compute the minimization metric for the alignment. The drift is bound by managing the local map based on the estimated uncertainty of the tracked pose. To benefit the community, we release an open-source C++ anytime real-time implementation.","sentences":["LiDAR odometry is the task of estimating the ego-motion of the sensor from sequential laser scans.","This problem has been addressed by the community for more than two decades, and many effective solutions are available nowadays.","Most of these systems implicitly rely on assumptions about the operating environment, the sensor used, and motion pattern.","When these assumptions are violated, several well-known systems tend to perform poorly.","This paper presents a LiDAR odometry system that can overcome these limitations and operate well under different operating conditions while achieving performance comparable with domain-specific methods.","Our algorithm follows the well-known ICP paradigm that leverages a PCA-based kd-tree implementation that is used to extract structural information about the clouds being registered and to compute the minimization metric for the alignment.","The drift is bound by managing the local map based on the estimated uncertainty of the tracked pose.","To benefit the community, we release an open-source C++","anytime real-time implementation."],"url":"http://arxiv.org/abs/2405.05828v1","category":"cs.RO"}
{"created":"2024-05-09 15:00:06","title":"Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning","abstract":"This paper presents the development of a novel ethical reasoning framework for robots. \"Robots Can Feel\" is the first system for robots that utilizes a combination of logic and human-like emotion simulation to make decisions in morally complex situations akin to humans. The key feature of the approach is the management of the Emotion Weight Coefficient - a customizable parameter to assign the role of emotions in robot decision-making. The system aims to serve as a tool that can equip robots of any form and purpose with ethical behavior close to human standards. Besides the platform, the system is independent of the choice of the base model. During the evaluation, the system was tested on 8 top up-to-date LLMs (Large Language Models). This list included both commercial and open-source models developed by various companies and countries. The research demonstrated that regardless of the model choice, the Emotions Weight Coefficient influences the robot's decision similarly. According to ANOVA analysis, the use of different Emotion Weight Coefficients influenced the final decision in a range of situations, such as in a request for a dietary violation F(4, 35) = 11.2, p = 0.0001 and in an animal compassion situation F(4, 35) = 8.5441, p = 0.0001. A demonstration code repository is provided at: https://github.com/TemaLykov/robots_can_feel","sentences":["This paper presents the development of a novel ethical reasoning framework for robots.","\"Robots Can Feel\" is the first system for robots that utilizes a combination of logic and human-like emotion simulation to make decisions in morally complex situations akin to humans.","The key feature of the approach is the management of the Emotion Weight Coefficient - a customizable parameter to assign the role of emotions in robot decision-making.","The system aims to serve as a tool that can equip robots of any form and purpose with ethical behavior close to human standards.","Besides the platform, the system is independent of the choice of the base model.","During the evaluation, the system was tested on 8 top up-to-date LLMs (Large Language Models).","This list included both commercial and open-source models developed by various companies and countries.","The research demonstrated that regardless of the model choice, the Emotions Weight Coefficient influences the robot's decision similarly.","According to ANOVA analysis, the use of different Emotion Weight Coefficients influenced the final decision in a range of situations, such as in a request for a dietary violation F(4, 35) = 11.2, p = 0.0001 and in an animal compassion situation F(4, 35) = 8.5441, p = 0.0001.","A demonstration code repository is provided at: https://github.com/TemaLykov/robots_can_feel"],"url":"http://arxiv.org/abs/2405.05824v1","category":"cs.RO"}
{"created":"2024-05-09 14:56:49","title":"Fine-grained Analysis and Faster Algorithms for Iteratively Solving Linear Systems","abstract":"While effective in practice, iterative methods for solving large systems of linear equations can be significantly affected by problem-dependent condition number quantities. This makes characterizing their time complexity challenging, particularly when we wish to make comparisons between deterministic and stochastic methods, that may or may not rely on preconditioning and/or fast matrix multiplication. In this work, we consider a fine-grained notion of complexity for iterative linear solvers which we call the spectral tail condition number, $\\kappa_\\ell$, defined as the ratio between the $\\ell$th largest and the smallest singular value of the matrix representing the system.   Concretely, we prove the following main algorithmic result: Given an $n\\times n$ matrix $A$ and a vector $b$, we can find $\\tilde{x}$ such that $\\|A\\tilde{x}-b\\|\\leq\\epsilon\\|b\\|$ in time $\\tilde{O}(\\kappa_\\ell\\cdot n^2\\log 1/\\epsilon)$ for any $\\ell = O(n^{\\frac1{\\omega-1}})=O(n^{0.729})$, where $\\omega \\approx 2.372$ is the current fast matrix multiplication exponent. This guarantee is achieved by Sketch-and-Project with Nesterov's acceleration. Some of the implications of our result, and of the use of $\\kappa_\\ell$, include direct improvement over a fine-grained analysis of the Conjugate Gradient method, suggesting a stronger separation between deterministic and stochastic iterative solvers; and relating the complexity of iterative solvers to the ongoing algorithmic advances in fast matrix multiplication, since the bound on $\\ell$ improves with $\\omega$.   Our main technical contributions are new sharp characterizations for the first and second moments of the random projection matrix that commonly arises in sketching algorithms, building on a combination of techniques from combinatorial sampling via determinantal point processes and Gaussian universality results from random matrix theory.","sentences":["While effective in practice, iterative methods for solving large systems of linear equations can be significantly affected by problem-dependent condition number quantities.","This makes characterizing their time complexity challenging, particularly when we wish to make comparisons between deterministic and stochastic methods, that may or may not rely on preconditioning and/or fast matrix multiplication.","In this work, we consider a fine-grained notion of complexity for iterative linear solvers which we call the spectral tail condition number, $\\kappa_\\ell$, defined as the ratio between the $\\ell$th largest and the smallest singular value of the matrix representing the system.   ","Concretely, we prove the following main algorithmic result: Given an $n\\times n$ matrix $A$ and a vector $b$, we can find $\\tilde{x}$ such that $\\|A\\tilde{x}-b\\|\\leq\\epsilon\\|b\\|$ in time $\\tilde{O}(\\kappa_\\ell\\cdot n^2\\log 1/\\epsilon)$ for any $\\ell = O(n^{\\frac1{\\omega-1}})=O(n^{0.729})$, where $\\omega \\approx 2.372$ is the current fast matrix multiplication exponent.","This guarantee is achieved by Sketch-and-Project with Nesterov's acceleration.","Some of the implications of our result, and of the use of $\\kappa_\\ell$, include direct improvement over a fine-grained analysis of the Conjugate Gradient method, suggesting a stronger separation between deterministic and stochastic iterative solvers; and relating the complexity of iterative solvers to the ongoing algorithmic advances in fast matrix multiplication, since the bound on $\\ell$ improves with $\\omega$.   Our main technical contributions are new sharp characterizations for the first and second moments of the random projection matrix that commonly arises in sketching algorithms, building on a combination of techniques from combinatorial sampling via determinantal point processes and Gaussian universality results from random matrix theory."],"url":"http://arxiv.org/abs/2405.05818v1","category":"cs.DS"}
{"created":"2024-05-09 14:55:30","title":"Time-dependent long-term hydrodynamic simulations of the inner protoplanetary disk III: The influence of photoevaporation","abstract":"The final stages of a protoplanetary disk are essential for our understanding of the formation and evolution of planets. Photoevaporation is an important mechanism that contributes to the dispersal of an accretion disk and has significant consequences for the disk's lifetime. However, the combined effects of photoevaporation and star-disk interaction have not been investigated in previous studies. We combined an implicit disk evolution model with a photoevaporative mass-loss profile. By including the innermost disk regions down to 0.01 AU, we could calculate the star-disk interaction, the stellar spin evolution, and the transition from an accreting disk to the propeller regime self-consistently. Starting from an early Class II star-disk system, we calculated the long-term evolution of the system until the disk becomes almost completely dissolved. Photoevaporation has a significant effect on disk structure and evolution. The radial extent of the dead zone decreases, and the number of episodic accretion events (outbursts) is reduced by high stellar X-ray luminosities. Reasonable accretion rates in combination with photoevaporative gaps are possible for a dead zone that is still massive enough to develop episodic accretion events. Furthermore, the stellar spin evolution during the Class II evolution is less affected by the star-disk interaction in the case of high X-ray luminosities. Our results suggest that the formation of planets, especially habitable planets, in the dead zone is strongly impaired in the case of strong X-ray luminosities. Additionally, the importance of the star-disk interaction during the Class II phase with respect to the stellar spin evolution is reduced.","sentences":["The final stages of a protoplanetary disk are essential for our understanding of the formation and evolution of planets.","Photoevaporation is an important mechanism that contributes to the dispersal of an accretion disk and has significant consequences for the disk's lifetime.","However, the combined effects of photoevaporation and star-disk interaction have not been investigated in previous studies.","We combined an implicit disk evolution model with a photoevaporative mass-loss profile.","By including the innermost disk regions down to 0.01 AU, we could calculate the star-disk interaction, the stellar spin evolution, and the transition from an accreting disk to the propeller regime self-consistently.","Starting from an early Class II star-disk system, we calculated the long-term evolution of the system until the disk becomes almost completely dissolved.","Photoevaporation has a significant effect on disk structure and evolution.","The radial extent of the dead zone decreases, and the number of episodic accretion events (outbursts) is reduced by high stellar X-ray luminosities.","Reasonable accretion rates in combination with photoevaporative gaps are possible for a dead zone that is still massive enough to develop episodic accretion events.","Furthermore, the stellar spin evolution during the Class II evolution is less affected by the star-disk interaction in the case of high X-ray luminosities.","Our results suggest that the formation of planets, especially habitable planets, in the dead zone is strongly impaired in the case of strong X-ray luminosities.","Additionally, the importance of the star-disk interaction during the Class II phase with respect to the stellar spin evolution is reduced."],"url":"http://arxiv.org/abs/2405.05816v1","category":"astro-ph.EP"}
{"created":"2024-05-09 14:55:19","title":"Non-myopic GOSPA-driven Gaussian Bernoulli Sensor Management","abstract":"In this paper, we propose an algorithm for non-myopic sensor management for Bernoulli filtering, i.e., when there may be at most one target present in the scene. The algorithm is based on selecting the action that solves a Bellman-type minimisation problem, whose cost function is the mean square generalised optimal sub-pattern assignment (GOSPA) error, over a future time window. We also propose an implementation of the sensor management algorithm based on an upper bound of the mean square GOSPA error and a Gaussian single-target posterior. Finally, we develop a Monte Carlo tree search algorithm to find an approximate optimal action within a given computational budget. The benefits of the proposed approach are demonstrated via simulations.","sentences":["In this paper, we propose an algorithm for non-myopic sensor management for Bernoulli filtering, i.e., when there may be at most one target present in the scene.","The algorithm is based on selecting the action that solves a Bellman-type minimisation problem, whose cost function is the mean square generalised optimal sub-pattern assignment (GOSPA) error, over a future time window.","We also propose an implementation of the sensor management algorithm based on an upper bound of the mean square GOSPA error and a Gaussian single-target posterior.","Finally, we develop a Monte Carlo tree search algorithm to find an approximate optimal action within a given computational budget.","The benefits of the proposed approach are demonstrated via simulations."],"url":"http://arxiv.org/abs/2405.05815v1","category":"eess.SY"}
{"created":"2024-05-09 14:50:07","title":"Parallel Cross Strip Attention Network for Single Image Dehazing","abstract":"The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals. Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size. While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal dehazing performance. In this study, we introduce a novel dehazing network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy. PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain. To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features.","sentences":["The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals.","Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size.","While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks.","Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal dehazing performance.","In this study, we introduce a novel dehazing network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy.","PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain.","To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.","Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features."],"url":"http://arxiv.org/abs/2405.05811v1","category":"cs.CV"}
{"created":"2024-05-09 14:36:53","title":"3D Positioning using a New Diffraction Path Model","abstract":"Enhancing 3D and Z-axis positioning accuracy is crucial for effective rescue in indoor emergencies, ensuring safety for emergency responders and at-risk individuals. Additionally, reducing the dependence of a positioning system on fixed infrastructure is crucial, given its vulnerability to power failures and damage during emergencies. Further challenges from a signal propagation perspective include poor indoor signal coverage, multipath effects and the problem of Non-Line-OfSight (NLOS) measurement bias. In this study, we utilize the mobility provided by a rapidly deployable Uncrewed Aerial Vehicle (UAV) based wireless network to address these challenges. We recognize diffraction from window edges as a crucial signal propagation mechanism and employ the Geometrical Theory of Diffraction (GTD) to introduce a novel NLOS path length model. Using this path length model, we propose two different techniques to improve the indoor positioning performance for emergency scenarios.","sentences":["Enhancing 3D and Z-axis positioning accuracy is crucial for effective rescue in indoor emergencies, ensuring safety for emergency responders and at-risk individuals.","Additionally, reducing the dependence of a positioning system on fixed infrastructure is crucial, given its vulnerability to power failures and damage during emergencies.","Further challenges from a signal propagation perspective include poor indoor signal coverage, multipath effects and the problem of Non-Line-OfSight (NLOS) measurement bias.","In this study, we utilize the mobility provided by a rapidly deployable Uncrewed Aerial Vehicle (UAV) based wireless network to address these challenges.","We recognize diffraction from window edges as a crucial signal propagation mechanism and employ the Geometrical Theory of Diffraction (GTD) to introduce a novel NLOS path length model.","Using this path length model, we propose two different techniques to improve the indoor positioning performance for emergency scenarios."],"url":"http://arxiv.org/abs/2405.05801v1","category":"eess.SP"}
{"created":"2024-05-09 14:29:23","title":"Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata","abstract":"In this paper we study the emergence of homeostasis in a two-layer system of the Game of Life, in which the Game of Life in the first layer couples with another system of cellular automata in the second layer. Homeostasis is defined here as a space-time dynamic that regulates the number of cells in state-1 in the Game of Life layer. A genetic algorithm is used to evolve the rules of the second layer to control the pattern of the Game of Life. We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer. The homeostasis sustained by these attractors are compared with the homeostatic dynamics observed in Daisy World.","sentences":["In this paper we study the emergence of homeostasis in a two-layer system of the Game of Life, in which the Game of Life in the first layer couples with another system of cellular automata in the second layer.","Homeostasis is defined here as a space-time dynamic that regulates the number of cells in state-1 in the Game of Life layer.","A genetic algorithm is used to evolve the rules of the second layer to control the pattern of the Game of Life.","We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer.","The homeostasis sustained by these attractors are compared with the homeostatic dynamics observed in Daisy World."],"url":"http://arxiv.org/abs/2405.05797v1","category":"cs.NE"}
{"created":"2024-05-09 14:17:26","title":"Sequential Amodal Segmentation via Cumulative Occlusion Learning","abstract":"To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines.","sentences":["To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order.","Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications.","Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories.","This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects.","It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes.","Experimental results across three amodal datasets show that our method outperforms established baselines."],"url":"http://arxiv.org/abs/2405.05791v1","category":"cs.CV"}
{"created":"2024-05-09 14:06:40","title":"Quantum Resource Theories beyond Convexity","abstract":"A class of quantum resource theories, based on non-convex star-shape sets, presented in this work captures the key quantum properties that cannot be studied by standard convex theories. We provide operational interpretations for a resource of this class and demonstrate its advantage to improve performance of correlated quantum discrimination tasks and testing of quantum combs. Proposed techniques provide useful tools to describe quantum discord, total correlations in composite quantum systems and to estimate the degree of non-Markovianity of an analyzed quantum dynamics. Other applications include the problem of unistochasticity of a given bistochastic matrix, with relevance for quantization of classical dynamics and studies of violation of CP-symmetry in high energy physics. In all these cases, the non-linear witnesses introduced here outperform the standard linear witnesses. Importance of our findings for quantum information theory is also emphasized.","sentences":["A class of quantum resource theories, based on non-convex star-shape sets, presented in this work captures the key quantum properties that cannot be studied by standard convex theories.","We provide operational interpretations for a resource of this class and demonstrate its advantage to improve performance of correlated quantum discrimination tasks and testing of quantum combs.","Proposed techniques provide useful tools to describe quantum discord, total correlations in composite quantum systems and to estimate the degree of non-Markovianity of an analyzed quantum dynamics.","Other applications include the problem of unistochasticity of a given bistochastic matrix, with relevance for quantization of classical dynamics and studies of violation of CP-symmetry in high energy physics.","In all these cases, the non-linear witnesses introduced here outperform the standard linear witnesses.","Importance of our findings for quantum information theory is also emphasized."],"url":"http://arxiv.org/abs/2405.05785v1","category":"quant-ph"}
{"created":"2024-05-09 14:03:31","title":"Intermediate spectral statistics of rational triangular quantum billiards","abstract":"Triangular billiards whose angles are rational multiples of $\\pi$ are one of the simplest examples of pseudo-integrable models with intriguing classical and quantum properties. We perform an extensive numerical study of spectral statistics of eight quantized rational triangles, six belonging to the family of right-angled Veech triangles and two obtuse rational triangles. Large spectral samples of up to one million energy levels were calculated for each triangle which permits to determine their spectral statistics with great accuracy. It is demonstrated that they are of the intermediate type, sharing some features with chaotic systems, like level repulsion and some with integrable systems, like exponential tails of the level spacing distributions. Another distinctive feature of intermediate spectral statistics is a finite value of the level compressibility.   The short range statistics such as the level spacing distributions, and long-range statistics such as the number variance and spectral form factors were analyzed in detail. An excellent agreement between the numerical data and the model of gamma distributions is revealed.","sentences":["Triangular billiards whose angles are rational multiples of $\\pi$ are one of the simplest examples of pseudo-integrable models with intriguing classical and quantum properties.","We perform an extensive numerical study of spectral statistics of eight quantized rational triangles, six belonging to the family of right-angled Veech triangles and two obtuse rational triangles.","Large spectral samples of up to one million energy levels were calculated for each triangle which permits to determine their spectral statistics with great accuracy.","It is demonstrated that they are of the intermediate type, sharing some features with chaotic systems, like level repulsion and some with integrable systems, like exponential tails of the level spacing distributions.","Another distinctive feature of intermediate spectral statistics is a finite value of the level compressibility.   ","The short range statistics such as the level spacing distributions, and long-range statistics such as the number variance and spectral form factors were analyzed in detail.","An excellent agreement between the numerical data and the model of gamma distributions is revealed."],"url":"http://arxiv.org/abs/2405.05783v1","category":"nlin.CD"}
{"created":"2024-05-09 13:59:46","title":"Minimax problems for ensembles of affine-control systems","abstract":"In this paper, we consider ensembles of affine-control systems in $\\mathbb{R}^n$, and we study simultaneous optimal control problems related to the worst-case minimization. After proving that such problems admit solutions, denoting with $(\\Theta^N)_N$ a sequence of compact sets that parametrize the ensembles of systems, we first show that the corresponding minimax optimal control problems are $\\Gamma$-convergent whenever $(\\Theta^N)_N$ has a limit with respect to the Hausdorff distance. Besides its independent interest, the previous result plays a crucial role for establishing the Pontryagin Maximum Principle (PMP) when the ensemble is parametrized by a set $\\Theta$ consisting of infinitely many points. Namely, we first approximate $\\Theta$ by finite and increasing-in-size sets $(\\Theta^N)_N$ for which the PMP is known, and then we derive the PMP for the $\\Gamma$-limiting problem. The same strategy can be pursued in applications where we can reduce infinite ensembles to finite ones to compute the minimizers numerically.","sentences":["In this paper, we consider ensembles of affine-control systems in $\\mathbb{R}^n$, and we study simultaneous optimal control problems related to the worst-case minimization.","After proving that such problems admit solutions, denoting with $(\\Theta^N)_N$ a sequence of compact sets that parametrize the ensembles of systems, we first show that the corresponding minimax optimal control problems are $\\Gamma$-convergent whenever $(\\Theta^N)_N$ has a limit with respect to the Hausdorff distance.","Besides its independent interest, the previous result plays a crucial role for establishing the Pontryagin Maximum Principle (PMP) when the ensemble is parametrized by a set $\\Theta$ consisting of infinitely many points.","Namely, we first approximate $\\Theta$ by finite and increasing-in-size sets $(\\Theta^N)_N$ for which the PMP is known, and then we derive the PMP for the $\\Gamma$-limiting problem.","The same strategy can be pursued in applications where we can reduce infinite ensembles to finite ones to compute the minimizers numerically."],"url":"http://arxiv.org/abs/2405.05782v1","category":"math.OC"}
{"created":"2024-05-09 13:58:11","title":"Nonparametric estimation of a future entry time distribution given the knowledge of a past state occupation in a progressive multistate model with current status data","abstract":"Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies. In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation. This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data. We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities. Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm. The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data. We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer.","sentences":["Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies.","In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation.","This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data.","We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities.","Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm.","The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data.","We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer."],"url":"http://arxiv.org/abs/2405.05781v1","category":"stat.ME"}
{"created":"2024-05-09 13:39:10","title":"The mechanisms behind extreme susceptibility of photon avalanche emission to quenching","abstract":"The photon avalanche (PA) process that emerges in lanthanide-doped crystals yields a threshold and highly nonlinear (of the power law order > 5) optical response to photoexcitation. PA emission is the outcome of excited-state absorption combined with a cross-relaxation process, which creates positive and efficient energy looping. In consequence, this combination of processes should be highly susceptible to small perturbations in energy distribution and thus can be hindered by other competitive 'parasite' processes such as energy transfer (ET) to quenching sites. Although luminescence quenching is a well-known phenomenon, the exact mechanisms of susceptibility of PA to resonant energy transfer (RET) remain poorly understood limiting practical applications. A deeper understanding of these mechanisms may pave the way to new areas of PA exploitation. This study focuses on the investigation of the LiYF$_{4}$:3%Tm$^{3+}$ PA system co-doped with $Nd^{3+}$ acceptor ions, which was found to impact both the looping and emitting levels and thus to effectively disrupt PA emission, causing an increase in the PA threshold ($I_{th}$) and a decrease in PA nonlinearity ($S_{max}$). Our complementary modelling results revealed that the ET from the looping level increased $I_{th}$ and $S_{max}$, whereas the ET from the emitting level diminished $S_{max}$ and the final emission intensity. Ultimately, significant PA emission quenching demonstrates a high relative sensitivity ($S_{R}$) to infinitesimal amounts of $Nd^{3+}$ acceptors, highlighting the potential for PA to be utilized as an ultra-sensitive, fluorescence-based reporting mechanism that is suitable for the detection and quantification of physical and biological phenomena or reactions.","sentences":["The photon avalanche (PA) process that emerges in lanthanide-doped crystals yields a threshold and highly nonlinear (of the power law order > 5) optical response to photoexcitation.","PA emission is the outcome of excited-state absorption combined with a cross-relaxation process, which creates positive and efficient energy looping.","In consequence, this combination of processes should be highly susceptible to small perturbations in energy distribution and thus can be hindered by other competitive 'parasite' processes such as energy transfer (ET) to quenching sites.","Although luminescence quenching is a well-known phenomenon, the exact mechanisms of susceptibility of PA to resonant energy transfer (RET) remain poorly understood limiting practical applications.","A deeper understanding of these mechanisms may pave the way to new areas of PA exploitation.","This study focuses on the investigation of the LiYF$_{4}$:3%Tm$^{3+}$ PA system co-doped with $Nd^{3+}$ acceptor ions, which was found to impact both the looping and emitting levels and thus to effectively disrupt PA emission, causing an increase in the PA threshold ($I_{th}$) and a decrease in PA nonlinearity ($S_{max}$).","Our complementary modelling results revealed that the ET from the looping level increased $I_{th}$ and $S_{max}$, whereas the ET from the emitting level diminished $S_{max}$ and the final emission intensity.","Ultimately, significant PA emission quenching demonstrates a high relative sensitivity ($S_{R}$) to infinitesimal amounts of $Nd^{3+}$ acceptors, highlighting the potential for PA to be utilized as an ultra-sensitive, fluorescence-based reporting mechanism that is suitable for the detection and quantification of physical and biological phenomena or reactions."],"url":"http://arxiv.org/abs/2405.05764v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 13:18:41","title":"Achieving Precisely-Assigned Performance Requirements for Spacecraft Attitude Control","abstract":"This paper investigates the attitude control problem of spacecraft, with the objective of achieving precise performance criteria including precise settling time, steady-state error, and overshoot elimination. To tackle this challenge, we propose the Precisely-Assigned Performance (PAP) control scheme. Firstly, we utilize a parameterized function to explicitly characterize a reference for the transient responses, termed the Reference Performance Function (RPF). Subsequently, leveraging the concept of the RPF, we define a performance-satisfied tube region and introduce the concept of control barrier functions to derive a sufficient condition for the state trajectory to converge and remain confined within this tube region. By introducing the concept of Sontag's universal formula for stabilization, a PAP controller, constructed based on the backstepping method, is then designed to guide the system to satisfy these affine constraint conditions, and a disturbance observer is further integrated to handle perturbations. Theoretical proofs are presented to demonstrate the controller's capability to establish the boundedness of the overall system and ensure that each state trajectory will converge into the performance-satisfied region within a finite time duration under any conditions. Finally, numerical simulation results are presented to validate the effectiveness of the proposed method.","sentences":["This paper investigates the attitude control problem of spacecraft, with the objective of achieving precise performance criteria including precise settling time, steady-state error, and overshoot elimination.","To tackle this challenge, we propose the Precisely-Assigned Performance (PAP) control scheme.","Firstly, we utilize a parameterized function to explicitly characterize a reference for the transient responses, termed the Reference Performance Function (RPF).","Subsequently, leveraging the concept of the RPF, we define a performance-satisfied tube region and introduce the concept of control barrier functions to derive a sufficient condition for the state trajectory to converge and remain confined within this tube region.","By introducing the concept of Sontag's universal formula for stabilization, a PAP controller, constructed based on the backstepping method, is then designed to guide the system to satisfy these affine constraint conditions, and a disturbance observer is further integrated to handle perturbations.","Theoretical proofs are presented to demonstrate the controller's capability to establish the boundedness of the overall system and ensure that each state trajectory will converge into the performance-satisfied region within a finite time duration under any conditions.","Finally, numerical simulation results are presented to validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.05754v1","category":"eess.SY"}
{"created":"2024-05-09 13:13:10","title":"Cluster statistics of critical Ising and Ashkin-Teller models","abstract":"Motivated by recent progress on the scaling behavior of entanglement entropy, we study the scaling behavior of the number of clusters crossing the boundary between two subsystems for several classical statistical models in two dimension. This number exhibits a subleading logarithmic dependence of the linear dimension of the boundary when the model is at critical, in analogy to the entanglement entropy of a quantum system. It is shown that the logarithmic scaling of the cluster number originates from the conformal invariance of the critical system. We check this numerically for Ising and Ashkin-Teller models by using Monte Carlo simulations, and show that whether a universal coefficient of the logarithmic term can be observed numerically may strongly depend on the geometry and boundary condictions of the system.","sentences":["Motivated by recent progress on the scaling behavior of entanglement entropy, we study the scaling behavior of the number of clusters crossing the boundary between two subsystems for several classical statistical models in two dimension.","This number exhibits a subleading logarithmic dependence of the linear dimension of the boundary when the model is at critical, in analogy to the entanglement entropy of a quantum system.","It is shown that the logarithmic scaling of the cluster number originates from the conformal invariance of the critical system.","We check this numerically for Ising and Ashkin-Teller models by using Monte Carlo simulations, and show that whether a universal coefficient of the logarithmic term can be observed numerically may strongly depend on the geometry and boundary condictions of the system."],"url":"http://arxiv.org/abs/2405.05747v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-09 12:59:11","title":"How Quality Affects Deep Neural Networks in Fine-Grained Image Classification","abstract":"In this paper, we propose a No-Reference Image Quality Assessment (NRIQA) guided cut-off point selection (CPS) strategy to enhance the performance of a fine-grained classification system. Scores given by existing NRIQA methods on the same image may vary and not be as independent of natural image augmentations as expected, which weakens their connection and explainability to fine-grained image classification. Taking the three most commonly adopted image augmentation configurations -- cropping, rotating, and blurring -- as the entry point, we formulate a two-step mechanism for selecting the most discriminative subset from a given image dataset by considering both the confidence of model predictions and the density distribution of image qualities over several NRIQA methods. Concretely, the cut-off points yielded by those methods are aggregated via majority voting to inform the process of image subset selection. The efficacy and efficiency of such a mechanism have been confirmed by comparing the models being trained on high-quality images against a combination of high- and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial product dataset in terms of mean accuracy through four deep neural classifiers. The robustness of the mechanism has been proven by the observations that all the selected high-quality images can work jointly with 70% low-quality images with 1.3% of classification precision sacrificed when using ResNet34 in an ablation study.","sentences":["In this paper, we propose a No-Reference Image Quality Assessment (NRIQA) guided cut-off point selection (CPS) strategy to enhance the performance of a fine-grained classification system.","Scores given by existing NRIQA methods on the same image may vary and not be as independent of natural image augmentations as expected, which weakens their connection and explainability to fine-grained image classification.","Taking the three most commonly adopted image augmentation configurations -- cropping, rotating, and blurring -- as the entry point, we formulate a two-step mechanism for selecting the most discriminative subset from a given image dataset by considering both the confidence of model predictions and the density distribution of image qualities over several NRIQA methods.","Concretely, the cut-off points yielded by those methods are aggregated via majority voting to inform the process of image subset selection.","The efficacy and efficiency of such a mechanism have been confirmed by comparing the models being trained on high-quality images against a combination of high- and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial product dataset in terms of mean accuracy through four deep neural classifiers.","The robustness of the mechanism has been proven by the observations that all the selected high-quality images can work jointly with 70% low-quality images with 1.3% of classification precision sacrificed when using ResNet34 in an ablation study."],"url":"http://arxiv.org/abs/2405.05742v1","category":"cs.CV"}
{"created":"2024-05-09 12:22:13","title":"Bimodal Plasmonic Refractive Index Sensors Based on SU-8 Waveguides","abstract":"Plasmonic refractive index sensors are essential for detecting subtle variations in the ambient environment through surface plasmon interactions. Current efforts utilizing CMOS-compatible, plasmo-photonic Mach-Zehnder interferometers with active power balancing exhibit high sensitivities at the cost of fabrication and measurement complexity. Alternatively, passive bimodal plasmonic interferometers based on SU-8 waveguides present a cost-effective solution with a smaller device footprint, though they currently lack opto-mechanical isolation due to exposed photonic waveguides. In this work, we introduce innovative polymer-core and polymer-cladded bimodal plasmonic refractive index sensors with high refractive index contrast. Our sensors feature an aluminum stripe, a bilayer SU-8 photonic waveguide core, and the experimental optical cladding polymer SX AR LWL 2.0. They achieve a sensitivity of (6300 $\\pm$ 460) nm/RIU (refractive index unit), surpassing both traditional and polymer-based plasmo-photonic sensors. This approach enables integrated, wafer-scale, CMOS-compatible, and low-cost sensors and facilitates plasmonic refractive index sensing platforms for various applications.","sentences":["Plasmonic refractive index sensors are essential for detecting subtle variations in the ambient environment through surface plasmon interactions.","Current efforts utilizing CMOS-compatible, plasmo-photonic Mach-Zehnder interferometers with active power balancing exhibit high sensitivities at the cost of fabrication and measurement complexity.","Alternatively, passive bimodal plasmonic interferometers based on SU-8 waveguides present a cost-effective solution with a smaller device footprint, though they currently lack opto-mechanical isolation due to exposed photonic waveguides.","In this work, we introduce innovative polymer-core and polymer-cladded bimodal plasmonic refractive index sensors with high refractive index contrast.","Our sensors feature an aluminum stripe, a bilayer SU-8 photonic waveguide core, and the experimental optical cladding polymer SX AR LWL 2.0.","They achieve a sensitivity of (6300 $\\pm$ 460) nm/RIU (refractive index unit), surpassing both traditional and polymer-based plasmo-photonic sensors.","This approach enables integrated, wafer-scale, CMOS-compatible, and low-cost sensors and facilitates plasmonic refractive index sensing platforms for various applications."],"url":"http://arxiv.org/abs/2405.05716v1","category":"physics.optics"}
{"created":"2024-05-09 12:21:17","title":"Shifting the ISAC Trade-Off with Fluid Antenna Systems","abstract":"As an emerging antenna technology, a fluid antenna system (FAS) enhances spatial diversity to improve both sensing and communication performance by shifting the active antennas among available ports. In this letter, we study the potential of shifting the integrated sensing and communication (ISAC) trade-off with FAS. We propose the model for FAS-enabled ISAC and jointly optimize the transmit beamforming and port selection of FAS. In particular, we aim to minimize the transmit power, while satisfying both communication and sensing requirements. An efficient iterative algorithm based on sparse optimization, convex approximation, and a penalty approach is developed. The simulation results show that the proposed scheme can attain 33% reductions in transmit power with guaranteed sensing and communication performance, showing the great potential of the fluid antenna for striking a flexible tradeoff between sensing and communication in ISAC systems.","sentences":["As an emerging antenna technology, a fluid antenna system (FAS) enhances spatial diversity to improve both sensing and communication performance by shifting the active antennas among available ports.","In this letter, we study the potential of shifting the integrated sensing and communication (ISAC) trade-off with FAS.","We propose the model for FAS-enabled ISAC and jointly optimize the transmit beamforming and port selection of FAS.","In particular, we aim to minimize the transmit power, while satisfying both communication and sensing requirements.","An efficient iterative algorithm based on sparse optimization, convex approximation, and a penalty approach is developed.","The simulation results show that the proposed scheme can attain 33% reductions in transmit power with guaranteed sensing and communication performance, showing the great potential of the fluid antenna for striking a flexible tradeoff between sensing and communication in ISAC systems."],"url":"http://arxiv.org/abs/2405.05715v1","category":"eess.SP"}
{"created":"2024-05-09 12:15:45","title":"Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity","abstract":"Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning. While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds. The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of \\cite{li2023stochastic}. However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice. To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness. Regarding efficiency, our accelerated algorithm has the function query complexity of $\\mathcal{O}(\\epsilon^{-7/4}d)$ for finding an $\\epsilon$-approximate first-order stationary point. By introducing a small perturbation, it exhibits a function query complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space. Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem. Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\\tilde{\\mathcal{O}}(\\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\\tilde{\\mathcal{O}}(\\epsilon^{3/4})$.","sentences":["Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning.","While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds.","The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of \\cite{li2023stochastic}.","However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice.","To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness.","Regarding efficiency, our accelerated algorithm has the function query complexity of $\\mathcal{O}(\\epsilon^{-7/4}d)$ for finding an $\\epsilon$-approximate first-order stationary point.","By introducing a small perturbation, it exhibits a function query complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space.","Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem.","Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\\tilde{\\mathcal{O}}(\\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\\tilde{\\mathcal{O}}(\\epsilon^{3/4})$."],"url":"http://arxiv.org/abs/2405.05713v1","category":"math.OC"}
{"created":"2024-05-09 12:10:18","title":"On the Capacity of Correlated MIMO Phase-Noise Channels: An Electro-Optic Frequency Comb Example","abstract":"The capacity of a discrete-time multiple-input-multiple-output channel with correlated phase noises is investigated. In particular, the electro-optic frequency comb system is considered, where the phase noise of each channel is a combination of two independent Wiener phase-noise sources. Capacity upper and lower bounds are derived for this channel and are compared with lower bounds obtained by numerically evaluating the achievable information rates using quadrature amplitude modulation constellations. Capacity upper and lower bounds are provided for the high signal-to-noise ratio (SNR) regime. The multiplexing gain (pre-log) is shown to be $M-1$, where $M$ represents the number of channels. A constant gap between the asymptotic upper and lower bounds is observed, which depends on the number of channels $M$. For the specific case of $M=2$, capacity is characterized up to a term that vanishes as the SNR grows large.","sentences":["The capacity of a discrete-time multiple-input-multiple-output channel with correlated phase noises is investigated.","In particular, the electro-optic frequency comb system is considered, where the phase noise of each channel is a combination of two independent Wiener phase-noise sources.","Capacity upper and lower bounds are derived for this channel and are compared with lower bounds obtained by numerically evaluating the achievable information rates using quadrature amplitude modulation constellations.","Capacity upper and lower bounds are provided for the high signal-to-noise ratio (SNR) regime.","The multiplexing gain (pre-log) is shown to be $M-1$, where $M$ represents the number of channels.","A constant gap between the asymptotic upper and lower bounds is observed, which depends on the number of channels $M$. For the specific case of $M=2$, capacity is characterized up to a term that vanishes as the SNR grows large."],"url":"http://arxiv.org/abs/2405.05709v1","category":"cs.IT"}
{"created":"2024-05-09 12:02:58","title":"Unbounded visibility domains: metric estimates and an application","abstract":"We give an explicit lower bound, in terms of the distance from the boundary, for the Kobayashi metric of a certain class of bounded pseudoconvex domains in $\\mathbb{C}^n$ with $\\mathcal{C}^2$-smooth boundary using the regularity theory for the complex Monge--Ampere equation. Using such an estimate, we construct a family of unbounded Kobayashi hyperbolic domains in $\\mathbb{C}^n$ having a certain negative-curvature-type property with respect to the Kobayashi distance. As an application, we prove a Picard-type extension theorem for the latter domains.","sentences":["We give an explicit lower bound, in terms of the distance from the boundary, for the Kobayashi metric of a certain class of bounded pseudoconvex domains in $\\mathbb{C}^n$ with $\\mathcal{C}^2$-smooth boundary using the regularity theory for the complex Monge--Ampere equation.","Using such an estimate, we construct a family of unbounded Kobayashi hyperbolic domains in $\\mathbb{C}^n$ having a certain negative-curvature-type property with respect to the Kobayashi distance.","As an application, we prove a Picard-type extension theorem for the latter domains."],"url":"http://arxiv.org/abs/2405.05704v1","category":"math.CV"}
{"created":"2024-05-09 11:51:00","title":"Investigating entropic dynamics of complicated cavity QED system","abstract":"Various aspects of entropy of a complicated cavity QED system are explored. Atoms are held in optical cavities through optical tweezers and can jump between different cavities through the tunneling effect. The interaction of atom with the cavity results in electronic transitions and the creation and annihilation of photon. Covalent bond and phonon are introduced into the model. The effect of all kinds of interactions on entropy is studied. At the same time, the von Neumann entropy of different subsystems is compared. The results show that by selectively choosing system parameters, the entropic dynamics can be controlled.","sentences":["Various aspects of entropy of a complicated cavity QED system are explored.","Atoms are held in optical cavities through optical tweezers and can jump between different cavities through the tunneling effect.","The interaction of atom with the cavity results in electronic transitions and the creation and annihilation of photon.","Covalent bond and phonon are introduced into the model.","The effect of all kinds of interactions on entropy is studied.","At the same time, the von Neumann entropy of different subsystems is compared.","The results show that by selectively choosing system parameters, the entropic dynamics can be controlled."],"url":"http://arxiv.org/abs/2405.05696v1","category":"quant-ph"}
{"created":"2024-05-09 11:38:35","title":"Magnetic evolution of Cr$_2$Te$_3$ epitaxially grown on graphene with post-growth annealing","abstract":"Two-dimensional and van der Waals ferromagnets are ideal platform to study low dimensional magnetism and proximity effects in van der Waals heterostructures. Their ultimate two dimensional character offers also the opportunity to easily adjust their magnetic properties using strain or electric fields. Among 2D ferromagnets, the Cr$_{1+x}$Te$_2$ compounds with $x$=0-1 are very promising because their magnetic properties depend on the amount of self-intercalated Cr atoms between pure CrTe$_2$ layers and the Curie temperature (T$_C$) can reach room temperature for certain compositions. Here, we investigate the evolution of the composition, structural and magnetic properties of thin Cr$_{1.33}$Te$_2$ (Cr$_2$Te$_3$) films epitaxially grown on graphene upon annealing. We observe a transition above 450{\\deg}C from the Cr$_{1.33}$Te$_2$ phase with perpendicular magnetic anisotropy and a T$_C$ of 180 K to a composition close to Cr$_{1.39}$Te$_2$ with in-plane magnetic anisotropy and a T$_C$ of 240-250 K. This phase remains stable up to 650{\\deg}C above which a pure Cr film starts to form. This work demonstrates the complex interplay between intercalated Cr, lattice parameters and magnetic properties in Cr$_{1+x}$Te$_2$ compounds.","sentences":["Two-dimensional and van der Waals ferromagnets are ideal platform to study low dimensional magnetism and proximity effects in van der Waals heterostructures.","Their ultimate two dimensional character offers also the opportunity to easily adjust their magnetic properties using strain or electric fields.","Among 2D ferromagnets, the Cr$_{1+x}$Te$_2$ compounds with $x$=0-1 are very promising because their magnetic properties depend on the amount of self-intercalated Cr atoms between pure CrTe$_2$ layers and the Curie temperature (T$_C$) can reach room temperature for certain compositions.","Here, we investigate the evolution of the composition, structural and magnetic properties of thin Cr$_{1.33}$Te$_2$ (Cr$_2$Te$_3$) films epitaxially grown on graphene upon annealing.","We observe a transition above 450{\\deg}C from the Cr$_{1.33}$Te$_2$ phase with perpendicular magnetic anisotropy and a T$_C$ of 180 K to a composition close to Cr$_{1.39}$Te$_2$ with in-plane magnetic anisotropy and a T$_C$ of 240-250 K. This phase remains stable up to 650{\\deg}C above which a pure Cr film starts to form.","This work demonstrates the complex interplay between intercalated Cr, lattice parameters and magnetic properties in Cr$_{1+x}$Te$_2$ compounds."],"url":"http://arxiv.org/abs/2405.05689v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 11:38:08","title":"Massive interacting binaries as an enrichment source for multiple populations in star clusters","abstract":"We present a suite of binary evolution models with massive primaries (10 $\\leq$ M$_1$ $\\leq$ 40 M$_\\odot$) and periods and mass ratios chosen such that the systems undergo non-conservative mass transfer while the primaries have helium cores. We track the total mass and chemical composition of the ejecta from these systems. This material shows the abundance signatures of hot hydrogen burning which are needed to explain the abundance patterns seen in multiple populations in massive star clusters. We then calculate the total yield of a population of binary stars with masses, mass ratios, and periods consistent with their distribution in a field population. We show that the overall abundance of this material is enriched in helium, nitrogen, sodium, and aluminum, and depleted in carbon, oxygen, and magnesium, by amounts that are consistent with observations. We also show that such a population of binaries will return approximately 25% of its mass in this ejecta (compared to 4% if all the stars were single), over a characteristic timescale of about 12 Myr. We argue that massive binaries must be seriously considered as a contributor to the source of enriched material needed to explain the multiple populations in massive clusters, since essentially all massive stars are formed in binaries or higher order multiples, massive binaries are primarily formed in clusters, and massive binaries naturally produce material of the right composition.","sentences":["We present a suite of binary evolution models with massive primaries (10 $\\leq$ M$_1$ $\\leq$ 40 M$_\\odot$) and periods and mass ratios chosen such that the systems undergo non-conservative mass transfer while the primaries have helium cores.","We track the total mass and chemical composition of the ejecta from these systems.","This material shows the abundance signatures of hot hydrogen burning which are needed to explain the abundance patterns seen in multiple populations in massive star clusters.","We then calculate the total yield of a population of binary stars with masses, mass ratios, and periods consistent with their distribution in a field population.","We show that the overall abundance of this material is enriched in helium, nitrogen, sodium, and aluminum, and depleted in carbon, oxygen, and magnesium, by amounts that are consistent with observations.","We also show that such a population of binaries will return approximately 25% of its mass in this ejecta (compared to 4% if all the stars were single), over a characteristic timescale of about 12 Myr.","We argue that massive binaries must be seriously considered as a contributor to the source of enriched material needed to explain the multiple populations in massive clusters, since essentially all massive stars are formed in binaries or higher order multiples, massive binaries are primarily formed in clusters, and massive binaries naturally produce material of the right composition."],"url":"http://arxiv.org/abs/2405.05687v1","category":"astro-ph.SR"}
{"created":"2024-05-09 11:18:19","title":"Mapping dissolved carbon in space and time: An experimental technique for the measurement of pH and total carbon concentration in density driven convection of CO$_2$ dissolved in water","abstract":"We present an experimental technique for determining the pH and the total carbon concentration when \\ch{CO2} diffuses and flows in water. The technique employs three different pH indicators, which, when combined with an image analysis technique, provides a dynamic range in pH from 4.0 to 9.5. In contrast to usual techniques in which a single pH indicator is used, the methodology presented allows not only to produce a binary classification (pH larger or smaller than a given threshold) but to access a much more complete continuous spatial distribution of pH and concentration levels in the system. We calibrate the method against benchmark solutions and further demonstrate its potential by measuring the pH and total carbon concentration in a density driven convection (DDC) of carbon-enriched water. The motivation for testing the method in this particular experiment comes from the fact that DDC plays a pivotal role in the efficiency of engineered carbon storage processes. The application of the technique presented here provided a direct window for the analysis of the spatial distribution of captured carbon in the DDC flow.","sentences":["We present an experimental technique for determining the pH and the total carbon concentration when \\ch{CO2} diffuses and flows in water.","The technique employs three different pH indicators, which, when combined with an image analysis technique, provides a dynamic range in pH from 4.0 to 9.5.","In contrast to usual techniques in which a single pH indicator is used, the methodology presented allows not only to produce a binary classification (pH larger or smaller than a given threshold) but to access a much more complete continuous spatial distribution of pH and concentration levels in the system.","We calibrate the method against benchmark solutions and further demonstrate its potential by measuring the pH and total carbon concentration in a density driven convection (DDC) of carbon-enriched water.","The motivation for testing the method in this particular experiment comes from the fact that DDC plays a pivotal role in the efficiency of engineered carbon storage processes.","The application of the technique presented here provided a direct window for the analysis of the spatial distribution of captured carbon in the DDC flow."],"url":"http://arxiv.org/abs/2405.05682v1","category":"physics.flu-dyn"}
{"created":"2024-05-09 11:03:15","title":"Maximum Correntropy Polynomial Chaos Kalman Filter for Underwater Navigation","abstract":"This paper develops an underwater navigation solution that utilizes a strapdown inertial navigation system (SINS) and fuses a set of auxiliary sensors such as an acoustic positioning system, Doppler velocity log, depth meter, attitude meter, and magnetometer to accurately estimate an underwater vessel's position and orientation. The conventional integrated navigation system assumes Gaussian measurement noise, while in reality, the noises are non-Gaussian, particularly contaminated by heavy-tailed impulsive noises. To address this issue, and to fuse the system model with the acquired sensor measurements efficiently, we develop a square root polynomial chaos Kalman filter based on maximum correntropy criteria. The filter is initialized using acoustic beaconing to accurately locate the initial position of the vehicle. The computational complexity of the proposed filter is calculated in terms of flops count. The proposed method is compared with the existing maximum correntropy sigma point filters in terms of estimation accuracy and computational complexity. The simulation results demonstrate an improved accuracy compared to the conventional deterministic sample point filters.","sentences":["This paper develops an underwater navigation solution that utilizes a strapdown inertial navigation system (SINS) and fuses a set of auxiliary sensors such as an acoustic positioning system, Doppler velocity log, depth meter, attitude meter, and magnetometer to accurately estimate an underwater vessel's position and orientation.","The conventional integrated navigation system assumes Gaussian measurement noise, while in reality, the noises are non-Gaussian, particularly contaminated by heavy-tailed impulsive noises.","To address this issue, and to fuse the system model with the acquired sensor measurements efficiently, we develop a square root polynomial chaos Kalman filter based on maximum correntropy criteria.","The filter is initialized using acoustic beaconing to accurately locate the initial position of the vehicle.","The computational complexity of the proposed filter is calculated in terms of flops count.","The proposed method is compared with the existing maximum correntropy sigma point filters in terms of estimation accuracy and computational complexity.","The simulation results demonstrate an improved accuracy compared to the conventional deterministic sample point filters."],"url":"http://arxiv.org/abs/2405.05676v1","category":"eess.SP"}
{"created":"2024-05-09 11:00:18","title":"Dynamical properties of a small heterogeneous chain network of neurons in discrete time","abstract":"We propose a novel nonlinear bidirectionally coupled heterogeneous chain network whose dynamics evolve in discrete time. The backbone of the model is a pair of popular map-based neuron models, the Chialvo and the Rulkov maps. This model is assumed to proximate the intricate dynamical properties of neurons in the widely complex nervous system. The model is first realized via various nonlinear analysis techniques: fixed point analysis, phase portraits, Jacobian matrix, and bifurcation diagrams. We observe the coexistence of chaotic and period-4 attractors. Various codimension-1 and -2 patterns for example saddle-node, period-doubling, Neimark-Sacker, double Neimark-Sacker, flip- and fold-Neimark Sacker, and 1:1 and 1:2 resonance are also explored. Furthermore, the study employs two synchronization measures to quantify how the oscillators in the network behave in tandem with each other over a long number of iterations. Finally, a time series analysis of the model is performed to investigate its complexity in terms of sample entropy.","sentences":["We propose a novel nonlinear bidirectionally coupled heterogeneous chain network whose dynamics evolve in discrete time.","The backbone of the model is a pair of popular map-based neuron models, the Chialvo and the Rulkov maps.","This model is assumed to proximate the intricate dynamical properties of neurons in the widely complex nervous system.","The model is first realized via various nonlinear analysis techniques: fixed point analysis, phase portraits, Jacobian matrix, and bifurcation diagrams.","We observe the coexistence of chaotic and period-4 attractors.","Various codimension-1 and -2 patterns for example saddle-node, period-doubling, Neimark-Sacker, double Neimark-Sacker, flip- and fold-Neimark Sacker, and 1:1 and 1:2 resonance are also explored.","Furthermore, the study employs two synchronization measures to quantify how the oscillators in the network behave in tandem with each other over a long number of iterations.","Finally, a time series analysis of the model is performed to investigate its complexity in terms of sample entropy."],"url":"http://arxiv.org/abs/2405.05675v1","category":"nlin.AO"}
{"created":"2024-05-09 10:49:12","title":"Passive Obstacle Aware Control to Follow Desired Velocities","abstract":"Evaluating and updating the obstacle avoidance velocity for an autonomous robot in real-time ensures robustness against noise and disturbances. A passive damping controller can obtain the desired motion with a torque-controlled robot, which remains compliant and ensures a safe response to external perturbations. Here, we propose a novel approach for designing the passive control policy. Our algorithm complies with obstacle-free zones while transitioning to increased damping near obstacles to ensure collision avoidance. This approach ensures stability across diverse scenarios, effectively mitigating disturbances. Validation on a 7DoF robot arm demonstrates superior collision rejection capabilities compared to the baseline, underlining its practicality for real-world applications. Our obstacle-aware damping controller represents a substantial advancement in secure robot control within complex and uncertain environments.","sentences":["Evaluating and updating the obstacle avoidance velocity for an autonomous robot in real-time ensures robustness against noise and disturbances.","A passive damping controller can obtain the desired motion with a torque-controlled robot, which remains compliant and ensures a safe response to external perturbations.","Here, we propose a novel approach for designing the passive control policy.","Our algorithm complies with obstacle-free zones while transitioning to increased damping near obstacles to ensure collision avoidance.","This approach ensures stability across diverse scenarios, effectively mitigating disturbances.","Validation on a 7DoF robot arm demonstrates superior collision rejection capabilities compared to the baseline, underlining its practicality for real-world applications.","Our obstacle-aware damping controller represents a substantial advancement in secure robot control within complex and uncertain environments."],"url":"http://arxiv.org/abs/2405.05669v1","category":"cs.RO"}
{"created":"2024-05-09 10:24:51","title":"Dynamics of a multilink wheeled vehicle: partial solutions and unbounded speedup","abstract":"A mathematical model featuring the motion of a multilink wheeled vehicle is developed using a nonholonomic model. A detailed analysis of the inertial motion is made. Fixed points of the reduced system are identified, their stability is analyzed, and invariant manifolds are found. For the case of three platforms (links), a phase portrait for motion on an invariant manifold is shown and trajectories of the attachment points of the wheel pairs of the three-link vehicle are presented. In addition, an analysis is made of motion in the case where the leading platform has a rotor whose angular velocity is a periodic function of time. The existence of trajectories for which one of the velocity components increases without bound is established, and the asymptotics for it is found.","sentences":["A mathematical model featuring the motion of a multilink wheeled vehicle is developed using a nonholonomic model.","A detailed analysis of the inertial motion is made.","Fixed points of the reduced system are identified, their stability is analyzed, and invariant manifolds are found.","For the case of three platforms (links), a phase portrait for motion on an invariant manifold is shown and trajectories of the attachment points of the wheel pairs of the three-link vehicle are presented.","In addition, an analysis is made of motion in the case where the leading platform has a rotor whose angular velocity is a periodic function of time.","The existence of trajectories for which one of the velocity components increases without bound is established, and the asymptotics for it is found."],"url":"http://arxiv.org/abs/2405.05661v1","category":"math.DS"}
{"created":"2024-05-09 09:40:56","title":"Outlier-robust Kalman Filtering through Generalised Bayes","abstract":"We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models. Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter. We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models. Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost. We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks.","sentences":["We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models.","Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter.","We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models.","Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost.","We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks."],"url":"http://arxiv.org/abs/2405.05646v1","category":"stat.ML"}
{"created":"2024-05-09 09:40:15","title":"Fractional Payment Transactions: Executing Payment Transactions in Parallel with Less than f+1 Validations","abstract":"We consider the problem of supporting payment transactions in an asynchronous system in which up to $f$ validators are subject to Byzantine failures under the control of an adaptive adversary. It was shown that, in the case of a single owner, this problem can be solved without consensus by using byzantine quorum systems (requiring a quorum of $2f+1$ validations per transaction). Nonetheless, the process of validating transactions remains sequential. For example, if one has a balance of ten coins and intends to make separate payments of two coins each to two distinct recipients, both transactions must undergo processing by a common correct validator. On the other hand, these two transactions are non-conflicting as they do not lead to double spending, allowing in principle for parallel validation. In this paper, we show that it is possible to validate payment transactions in parallel with less than $f$ validations per transaction in an asynchronous system, provided that each transaction spends only a small fraction of a balance. Our solution relies on a novel class of probabilistic quorum systems that we introduce in this paper, termed \\textit{$(k_1,k_2)$-quorum systems}. In the absence of an adaptive adversary, \\textit{$(k_1,k_2)$-quorum systems} can be used to enable concurrent and asynchronous validation of up to $k_1$ transactions while preventing validation of more than $k_2$ transactions. Employing a $(k_1, k_2)$-quorum system, we introduce protocols enabling a payer to validate multiple \\textit{fractional spending} transactions in parallel with less than $f+1$ validations per transaction. Subsequently, the payer reclaims any remaining funds through a fully validated transaction, referred to as a \\textit{settlement} transaction.","sentences":["We consider the problem of supporting payment transactions in an asynchronous system in which up to $f$ validators are subject to Byzantine failures under the control of an adaptive adversary.","It was shown that, in the case of a single owner, this problem can be solved without consensus by using byzantine quorum systems (requiring a quorum of $2f+1$ validations per transaction).","Nonetheless, the process of validating transactions remains sequential.","For example, if one has a balance of ten coins and intends to make separate payments of two coins each to two distinct recipients, both transactions must undergo processing by a common correct validator.","On the other hand, these two transactions are non-conflicting as they do not lead to double spending, allowing in principle for parallel validation.","In this paper, we show that it is possible to validate payment transactions in parallel with less than $f$ validations per transaction in an asynchronous system, provided that each transaction spends only a small fraction of a balance.","Our solution relies on a novel class of probabilistic quorum systems that we introduce in this paper, termed \\textit{$(k_1,k_2)$-quorum systems}.","In the absence of an adaptive adversary, \\textit{$(k_1,k_2)$-quorum systems} can be used to enable concurrent and asynchronous validation of up to $k_1$ transactions while preventing validation of more than $k_2$ transactions.","Employing a $(k_1, k_2)$-quorum system, we introduce protocols enabling a payer to validate multiple \\textit{fractional spending} transactions in parallel with less than $f+1$ validations per transaction.","Subsequently, the payer reclaims any remaining funds through a fully validated transaction, referred to as a \\textit{settlement} transaction."],"url":"http://arxiv.org/abs/2405.05645v1","category":"cs.DC"}
{"created":"2024-05-09 09:29:41","title":"Complex network analysis of cryptocurrency market during crashes","abstract":"This paper identifies the cryptocurrency market crashes and analyses its dynamics using the complex network. We identify three distinct crashes during 2017-20, and the analysis is carried out by dividing the time series into pre-crash, crash, and post-crash periods. Partial correlation based complex network analysis is carried out to study the crashes. Degree density ($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient ($\\overline{cc}$) are estimated from these networks. We find that both $\\rho_D$ and $\\overline{cc}$ are smallest during the pre-crash period, and spike during the crash suggesting the network is dense during a crash. Although $\\rho_D$ and $\\overline{cc}$ decrease in the post-crash period, they remain higher than pre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market attempt to return to normalcy. We get $\\bar{l}$ is minimal during the crash period, suggesting a rapid flow of information. A dense network and rapid information flow suggest that during a crash uninformed synchronized panic sell-off happens. However, during the 2019-20 crash, the values of $\\rho_D$, $\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal change in dynamics compared to other crashes. The findings of this study may guide investors in making decisions during market crashes.","sentences":["This paper identifies the cryptocurrency market crashes and analyses its dynamics using the complex network.","We identify three distinct crashes during 2017-20, and the analysis is carried out by dividing the time series into pre-crash, crash, and post-crash periods.","Partial correlation based complex network analysis is carried out to study the crashes.","Degree density ($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient ($\\overline{cc}$) are estimated from these networks.","We find that both $\\rho_D$ and $\\overline{cc}$ are smallest during the pre-crash period, and spike during the crash suggesting the network is dense during a crash.","Although $\\rho_D$ and $\\overline{cc}$ decrease in the post-crash period, they remain higher than pre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market attempt to return to normalcy.","We get $\\bar{l}$ is minimal during the crash period, suggesting a rapid flow of information.","A dense network and rapid information flow suggest that during a crash uninformed synchronized panic sell-off happens.","However, during the 2019-20 crash, the values of $\\rho_D$, $\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal change in dynamics compared to other crashes.","The findings of this study may guide investors in making decisions during market crashes."],"url":"http://arxiv.org/abs/2405.05642v1","category":"q-fin.ST"}
{"created":"2024-05-09 09:29:34","title":"Channel Estimation for Holographic MIMO: Wavenumber-Domain Sparsity Inspired Approaches","abstract":"This paper investigates the sparse channel estimation for holographic multiple-input multiple-output (HMIMO) systems. Given that the wavenumber-domain representation is based on a series of Fourier harmonics that are in essence a series of orthogonal basis functions, a novel wavenumber-domain sparsifying basis is designed to expose the sparsity inherent in HMIMO channels. Furthermore, by harnessing the beneficial sparsity in the wavenumber domain, the sparse estimation of HMIMO channels is structured as a compressed sensing problem, which can be efficiently solved by our proposed wavenumber-domain orthogonal matching pursuit (WD-OMP) algorithm. Finally, numerical results demonstrate that the proposed wavenumber-domain sparsifying basis maintains its detection accuracy regardless of the number of antenna elements and antenna spacing. Additionally, in the case of antenna spacing being much less than half a wavelength, the wavenumber-domain approach remains highly accurate in identifying the significant angular power of HMIMO channels.","sentences":["This paper investigates the sparse channel estimation for holographic multiple-input multiple-output (HMIMO) systems.","Given that the wavenumber-domain representation is based on a series of Fourier harmonics that are in essence a series of orthogonal basis functions, a novel wavenumber-domain sparsifying basis is designed to expose the sparsity inherent in HMIMO channels.","Furthermore, by harnessing the beneficial sparsity in the wavenumber domain, the sparse estimation of HMIMO channels is structured as a compressed sensing problem, which can be efficiently solved by our proposed wavenumber-domain orthogonal matching pursuit (WD-OMP) algorithm.","Finally, numerical results demonstrate that the proposed wavenumber-domain sparsifying basis maintains its detection accuracy regardless of the number of antenna elements and antenna spacing.","Additionally, in the case of antenna spacing being much less than half a wavelength, the wavenumber-domain approach remains highly accurate in identifying the significant angular power of HMIMO channels."],"url":"http://arxiv.org/abs/2405.05641v1","category":"eess.SP"}
{"created":"2024-05-09 09:28:43","title":"Experience and Analysis of Scalable High-Fidelity Computational Fluid Dynamics on Modular Supercomputing Architectures","abstract":"The never-ending computational demand from simulations of turbulence makes computational fluid dynamics (CFD) a prime application use case for current and future exascale systems. High-order finite element methods, such as the spectral element method, have been gaining traction as they offer high performance on both multicore CPUs and modern GPU-based accelerators. In this work, we assess how high-fidelity CFD using the spectral element method can exploit the modular supercomputing architecture at scale through domain partitioning, where the computational domain is split between a Booster module powered by GPUs and a Cluster module with conventional CPU nodes. We investigate several different flow cases and computer systems based on the modular supercomputing architecture (MSA). We observe that for our simulations, the communication overhead and load balancing issues incurred by incorporating different computing architectures are seldom worthwhile, especially when I/O is also considered, but when the simulation at hand requires more than the combined global memory on the GPUs, utilizing additional CPUs to increase the available memory can be fruitful. We support our results with a simple performance model to assess when running across modules might be beneficial. As MSA is becoming more widespread and efforts to increase system utilization are growing more important our results give insight into when and how a monolithic application can utilize and spread out to more than one module and obtain a faster time to solution.","sentences":["The never-ending computational demand from simulations of turbulence makes computational fluid dynamics (CFD) a prime application use case for current and future exascale systems.","High-order finite element methods, such as the spectral element method, have been gaining traction as they offer high performance on both multicore CPUs and modern GPU-based accelerators.","In this work, we assess how high-fidelity CFD using the spectral element method can exploit the modular supercomputing architecture at scale through domain partitioning, where the computational domain is split between a Booster module powered by GPUs and a Cluster module with conventional CPU nodes.","We investigate several different flow cases and computer systems based on the modular supercomputing architecture (MSA).","We observe that for our simulations, the communication overhead and load balancing issues incurred by incorporating different computing architectures are seldom worthwhile, especially when I/O is also considered, but when the simulation at hand requires more than the combined global memory on the GPUs, utilizing additional CPUs to increase the available memory can be fruitful.","We support our results with a simple performance model to assess when running across modules might be beneficial.","As MSA is becoming more widespread and efforts to increase system utilization are growing more important our results give insight into when and how a monolithic application can utilize and spread out to more than one module and obtain a faster time to solution."],"url":"http://arxiv.org/abs/2405.05640v1","category":"cs.DC"}
{"created":"2024-05-09 09:27:28","title":"Supercomputers as a Continous Medium","abstract":"As supercomputers' complexity has grown, the traditional boundaries between processor, memory, network, and accelerators have blurred, making a homogeneous computer model, in which the overall computer system is modeled as a continuous medium with homogeneously distributed computational power, memory, and data movement transfer capabilities, an intriguing and powerful abstraction. By applying a homogeneous computer model to algorithms with a given I/O complexity, we recover from first principles, other discrete computer models, such as the roofline model, parallel computing laws, such as Amdahl's and Gustafson's laws, and phenomenological observations, such as super-linear speedup. One of the homogeneous computer model's distinctive advantages is the capability of directly linking the performance limits of an application to the physical properties of a classical computer system. Applying the homogeneous computer model to supercomputers, such as Frontier, Fugaku, and the Nvidia DGX GH200, shows that applications, such as Conjugate Gradient (CG) and Fast Fourier Transforms (FFT), are rapidly approaching the fundamental classical computational limits, where the performance of even denser systems in terms of compute and memory are fundamentally limited by the speed of light.","sentences":["As supercomputers' complexity has grown, the traditional boundaries between processor, memory, network, and accelerators have blurred, making a homogeneous computer model, in which the overall computer system is modeled as a continuous medium with homogeneously distributed computational power, memory, and data movement transfer capabilities, an intriguing and powerful abstraction.","By applying a homogeneous computer model to algorithms with a given I/O complexity, we recover from first principles, other discrete computer models, such as the roofline model, parallel computing laws, such as Amdahl's and Gustafson's laws, and phenomenological observations, such as super-linear speedup.","One of the homogeneous computer model's distinctive advantages is the capability of directly linking the performance limits of an application to the physical properties of a classical computer system.","Applying the homogeneous computer model to supercomputers, such as Frontier, Fugaku, and the Nvidia DGX GH200, shows that applications, such as Conjugate Gradient (CG) and Fast Fourier Transforms (FFT), are rapidly approaching the fundamental classical computational limits, where the performance of even denser systems in terms of compute and memory are fundamentally limited by the speed of light."],"url":"http://arxiv.org/abs/2405.05639v1","category":"cs.DC"}
{"created":"2024-05-09 09:06:04","title":"Short proofs of Tverberg-type theorems for cell complexes","abstract":"We present short proofs of two Tverberg-type theorems for cell complexes by S. Hasui, D. Kishimoto, M. Takeda, and M. Tsutaya. One of them states that for any prime power $r$, any simplicial sphere $X$ of dimension $(d+1)(r-1)-1$, and any continuous map $f:X\\to\\mathbb R^d$ there are pairwise disjoint faces $\\sigma_1,\\ldots,\\sigma_r$ of $X$ such that $f(\\sigma_1)\\cap\\ldots f(\\sigma_r)\\ne\\emptyset$.","sentences":["We present short proofs of two Tverberg-type theorems for cell complexes by S. Hasui, D. Kishimoto, M. Takeda, and M. Tsutaya.","One of them states that for any prime power $r$, any simplicial sphere $X$ of dimension $(d+1)(r-1)-1$, and any continuous map $f:X\\to\\mathbb R^d$ there are pairwise disjoint faces $\\sigma_1,\\ldots,\\sigma_r$ of $X$ such that $f(\\sigma_1)\\cap\\ldots f(\\sigma_r)\\ne\\emptyset$."],"url":"http://arxiv.org/abs/2405.05629v1","category":"math.GT"}
{"created":"2024-05-09 08:50:17","title":"Onset of Quantum Thermalization in Jahn-Teller model","abstract":"We investigate the onset of quantum thermalization in a system governed by the Jahn-Teller Hamiltonian which describes the interaction between a single spin and two bosonic modes. We find that the Jahn-Teller model exhibits a finite-size quantum phase transition between the normal phase and two types of super-radiant phase when the ratios of spin-level splitting to each of the two bosonic frequencies grow to infinity. We test the prediction of the Eigenstate Thermalization Hypothesis in the Jahn-Teller model. We show that the expectation value of the spin observable quickly approaches its long-time average value. We find that the distance between the diagonal ensemble average and the microcanonical ensemble average of the spin observable decreases with the effective thermodynamic parameter. Furthermore, we show that the mean-time fluctuations of the spin observable are small and are inversely proportional to the effective system dimension.","sentences":["We investigate the onset of quantum thermalization in a system governed by the Jahn-Teller Hamiltonian which describes the interaction between a single spin and two bosonic modes.","We find that the Jahn-Teller model exhibits a finite-size quantum phase transition between the normal phase and two types of super-radiant phase when the ratios of spin-level splitting to each of the two bosonic frequencies grow to infinity.","We test the prediction of the Eigenstate Thermalization Hypothesis in the Jahn-Teller model.","We show that the expectation value of the spin observable quickly approaches its long-time average value.","We find that the distance between the diagonal ensemble average and the microcanonical ensemble average of the spin observable decreases with the effective thermodynamic parameter.","Furthermore, we show that the mean-time fluctuations of the spin observable are small and are inversely proportional to the effective system dimension."],"url":"http://arxiv.org/abs/2405.05624v1","category":"quant-ph"}
{"created":"2024-05-09 08:37:34","title":"Enhancing (quasi-)long-range order in a two-dimensional driven crystal","abstract":"It has been recently shown that 2D systems can exhibit crystalline phases with long-range translational order showcasing a striking violation of the Hohenberg-Mermin-Wagner (HMW) theorem which is valid at equilibrium. This is made possible by athermal driving mechanisms that inject energy into the system without exciting long wavelength modes of the density field. However, as thermal fluctuations are superimposed on the non-equilibrium driving, long-range translational order is inevitably lost. In this paper, we discuss the possibility of exploiting non-equilibrium effects to suppress arbitrarily large density fluctuations even when a global thermal bath is coupled to the system. We introduce a model of a harmonic crystal driven both by a global thermal bath and by a momentum conserving noise, where the typical observables related to density fluctuations and long-range translational order can be analytically derived and put in relation. This model allows us to rationalize the violation of the HMW theorem observed in previous studies through the prediction of large-wavelength phonons which thermalize at a vanishing effective temperature when the global bath is switched off. The conceptual framework introduced through this theory is then applied to numerical simulations of a hard-disk solid in contact with a thermal bath and driven out-of-equilibrium by active collisions. Our numerical analysis demonstrates how varying driving and dissipative parameters can lead to an arbitrary enhancement of the quasi-long-range order in the system regardless of the applied global noise amplitude. Finally, we outline a possible experimental procedure to apply our results to a realistic granular system.","sentences":["It has been recently shown that 2D systems can exhibit crystalline phases with long-range translational order showcasing a striking violation of the Hohenberg-Mermin-Wagner (HMW) theorem which is valid at equilibrium.","This is made possible by athermal driving mechanisms that inject energy into the system without exciting long wavelength modes of the density field.","However, as thermal fluctuations are superimposed on the non-equilibrium driving, long-range translational order is inevitably lost.","In this paper, we discuss the possibility of exploiting non-equilibrium effects to suppress arbitrarily large density fluctuations even when a global thermal bath is coupled to the system.","We introduce a model of a harmonic crystal driven both by a global thermal bath and by a momentum conserving noise, where the typical observables related to density fluctuations and long-range translational order can be analytically derived and put in relation.","This model allows us to rationalize the violation of the HMW theorem observed in previous studies through the prediction of large-wavelength phonons which thermalize at a vanishing effective temperature when the global bath is switched off.","The conceptual framework introduced through this theory is then applied to numerical simulations of a hard-disk solid in contact with a thermal bath and driven out-of-equilibrium by active collisions.","Our numerical analysis demonstrates how varying driving and dissipative parameters can lead to an arbitrary enhancement of the quasi-long-range order in the system regardless of the applied global noise amplitude.","Finally, we outline a possible experimental procedure to apply our results to a realistic granular system."],"url":"http://arxiv.org/abs/2405.05621v1","category":"cond-mat.soft"}
{"created":"2024-05-09 17:32:39","title":"Improved Evolutionary Algorithms for Submodular Maximization with Cost Constraints","abstract":"We present an evolutionary algorithm evo-SMC for the problem of Submodular Maximization under Cost constraints (SMC). Our algorithm achieves $1/2$-approximation with a high probability $1-1/n$ within $\\mathcal{O}(n^2K_{\\beta})$ iterations, where $K_{\\beta}$ denotes the maximum size of a feasible solution set with cost constraint $\\beta$. To the best of our knowledge, this is the best approximation guarantee offered by evolutionary algorithms for this problem. We further refine evo-SMC, and develop {\\sc st-evo-SMC}. This stochastic version yields a significantly faster algorithm while maintaining the approximation ratio of $1/2$, with probability $1-\\epsilon$. The required number of iterations reduces to $\\mathcal{O}(nK_{\\beta}\\log{(1/\\epsilon)}/p)$, where the user defined parameters $p \\in (0,1]$ represents the stochasticity probability, and $\\epsilon \\in (0,1]$ denotes the error threshold. Finally, the empirical evaluations carried out through extensive experimentation substantiate the efficiency and effectiveness of our proposed algorithms. Our algorithms consistently outperform existing methods, producing higher-quality solutions.","sentences":["We present an evolutionary algorithm evo-SMC for the problem of Submodular Maximization under Cost constraints (SMC).","Our algorithm achieves $1/2$-approximation with a high probability $1-1/n$ within $\\mathcal{O}(n^2K_{\\beta})$ iterations, where $K_{\\beta}$ denotes the maximum size of a feasible solution set with cost constraint $\\beta$. To the best of our knowledge, this is the best approximation guarantee offered by evolutionary algorithms for this problem.","We further refine evo-SMC, and develop {\\sc st-evo-SMC}.","This stochastic version yields a significantly faster algorithm while maintaining the approximation ratio of $1/2$, with probability $1-\\epsilon$. The required number of iterations reduces to $\\mathcal{O}(nK_{\\beta}\\log{(1/\\epsilon)}/p)$, where the user defined parameters $p \\in (0,1]$ represents the stochasticity probability, and $\\epsilon \\in (0,1]$ denotes the error threshold.","Finally, the empirical evaluations carried out through extensive experimentation substantiate the efficiency and effectiveness of our proposed algorithms.","Our algorithms consistently outperform existing methods, producing higher-quality solutions."],"url":"http://arxiv.org/abs/2405.05942v1","category":"cs.DS"}
{"created":"2024-05-09 16:13:32","title":"Non-monotonic conductivity of aqueous electrolytes: beyond the first Wien effect","abstract":"The conductivity of strong electrolytes increases under high electric fields, a nonlinear response known as the first Wien effect. Here, using molecular dynamics simulations we show that this nonlinear response is non-monotonic for moderately concentrated aqueous electrolytes. We attribute this unanticipated behavior to the fact that, under high electric fields, the permittivity of water decreases and becomes anisotropic. The permittivity tensor measured in the simulations can be reproduced by a model of water molecules as dipoles. We incorporate the resulting anisotropic interactions between the ions into a generalised Stochastic Density Field Theory and calculate ionic correlations as well as corrections to the Nernst-Einstein conductivity which are in good agreement with the numerical simulations.","sentences":["The conductivity of strong electrolytes increases under high electric fields, a nonlinear response known as the first Wien effect.","Here, using molecular dynamics simulations we show that this nonlinear response is non-monotonic for moderately concentrated aqueous electrolytes.","We attribute this unanticipated behavior to the fact that, under high electric fields, the permittivity of water decreases and becomes anisotropic.","The permittivity tensor measured in the simulations can be reproduced by a model of water molecules as dipoles.","We incorporate the resulting anisotropic interactions between the ions into a generalised Stochastic Density Field Theory and calculate ionic correlations as well as corrections to the Nernst-Einstein conductivity which are in good agreement with the numerical simulations."],"url":"http://arxiv.org/abs/2405.05882v1","category":"cond-mat.soft"}
{"created":"2024-05-09 16:05:34","title":"Fourier decay of product measures","abstract":"Can one characterise the Fourier decay of a product measure in terms of the Fourier decay of its marginals? We make inroads on this question by describing the Fourier spectrum of a product measure in terms of the Fourier spectrum of its marginals. The Fourier spectrum is a continuously parametrised family of dimensions living between the Fourier and Hausdorff dimensions and captures more Fourier analytic information than either dimension considered in isolation. We provide several examples and applications, including to Kakeya and Furstenberg sets. In the process we derive a novel Fourier analytic characterisation of the upper box dimension.","sentences":["Can one characterise the Fourier decay of a product measure in terms of the Fourier decay of its marginals?","We make inroads on this question by describing the Fourier spectrum of a product measure in terms of the Fourier spectrum of its marginals.","The Fourier spectrum is a continuously parametrised family of dimensions living between the Fourier and Hausdorff dimensions and captures more Fourier analytic information than either dimension considered in isolation.","We provide several examples and applications, including to Kakeya and Furstenberg sets.","In the process we derive a novel Fourier analytic characterisation of the upper box dimension."],"url":"http://arxiv.org/abs/2405.05878v1","category":"math.CA"}
{"created":"2024-05-09 15:59:11","title":"Comment on: Testing the speed of the spooky action at a distance in a tabletop experiment. [Sci Rep 13, 8201 (2023)]","abstract":"In 1989, Eberhard proposed a v-causal model where quantum correlations between entangled particles are established by communications moving at a superluminal speed v_t > c in a preferred frame. In successive years, several experiments established lower bounds for the possible tachyons velocities. In a recent paper, Luigi Santamaria Amato et al. performed an interesting east-west aligned tabletop experiment under the assumption that the preferred frame is the Cosmic Microwave Background (CMB). In that paper, they criticize long-distance experiments but here we show that most of their criticisms are not applicable to long-distance tunnel experiments where the highest lower bound was obtained.","sentences":["In 1989, Eberhard proposed a v-causal model where quantum correlations between entangled particles are established by communications moving at a superluminal speed v_t","> c in a preferred frame.","In successive years, several experiments established lower bounds for the possible tachyons velocities.","In a recent paper, Luigi Santamaria Amato et al. performed an interesting east-west aligned tabletop experiment under the assumption that the preferred frame is the Cosmic Microwave Background (CMB).","In that paper, they criticize long-distance experiments but here we show that most of their criticisms are not applicable to long-distance tunnel experiments where the highest lower bound was obtained."],"url":"http://arxiv.org/abs/2405.05869v1","category":"quant-ph"}
{"created":"2024-05-09 15:59:09","title":"Trustworthy Dimensionality Reduction","abstract":"Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon's mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground. Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility). On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand. Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability. To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability. Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent. The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well. We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP.","sentences":["Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon's mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground.","Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility).","On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand.","Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability.","To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability.","Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent.","The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well.","We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP."],"url":"http://arxiv.org/abs/2405.05868v1","category":"stat.ME"}
{"created":"2024-05-09 15:41:10","title":"Robust and Explainable Fine-Grained Visual Classification with Transfer Learning: A Dual-Carriageway Framework","abstract":"In the realm of practical fine-grained visual classification applications rooted in deep learning, a common scenario involves training a model using a pre-existing dataset. Subsequently, a new dataset becomes available, prompting the desire to make a pivotal decision for achieving enhanced and leveraged inference performance on both sides: Should one opt to train datasets from scratch or fine-tune the model trained on the initial dataset using the newly released dataset? The existing literature reveals a lack of methods to systematically determine the optimal training strategy, necessitating explainability. To this end, we present an automatic best-suit training solution searching framework, the Dual-Carriageway Framework (DCF), to fill this gap. DCF benefits from the design of a dual-direction search (starting from the pre-existing or the newly released dataset) where five different training settings are enforced. In addition, DCF is not only capable of figuring out the optimal training strategy with the capability of avoiding overfitting but also yields built-in quantitative and visual explanations derived from the actual input and weights of the trained model. We validated DCF's effectiveness through experiments with three convolutional neural networks (ResNet18, ResNet34 and Inception-v3) on two temporally continued commercial product datasets. Results showed fine-tuning pathways outperformed training-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and new datasets, respectively, in terms of mean accuracy. Furthermore, DCF identified reflection padding as the superior padding method, enhancing testing accuracy by 3.72% on average. This framework stands out for its potential to guide the development of robust and explainable AI solutions in fine-grained visual classification tasks.","sentences":["In the realm of practical fine-grained visual classification applications rooted in deep learning, a common scenario involves training a model using a pre-existing dataset.","Subsequently, a new dataset becomes available, prompting the desire to make a pivotal decision for achieving enhanced and leveraged inference performance on both sides: Should one opt to train datasets from scratch or fine-tune the model trained on the initial dataset using the newly released dataset?","The existing literature reveals a lack of methods to systematically determine the optimal training strategy, necessitating explainability.","To this end, we present an automatic best-suit training solution searching framework, the Dual-Carriageway Framework (DCF), to fill this gap.","DCF benefits from the design of a dual-direction search (starting from the pre-existing or the newly released dataset) where five different training settings are enforced.","In addition, DCF is not only capable of figuring out the optimal training strategy with the capability of avoiding overfitting but also yields built-in quantitative and visual explanations derived from the actual input and weights of the trained model.","We validated DCF's effectiveness through experiments with three convolutional neural networks (ResNet18, ResNet34 and Inception-v3) on two temporally continued commercial product datasets.","Results showed fine-tuning pathways outperformed training-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and new datasets, respectively, in terms of mean accuracy.","Furthermore, DCF identified reflection padding as the superior padding method, enhancing testing accuracy by 3.72% on average.","This framework stands out for its potential to guide the development of robust and explainable AI solutions in fine-grained visual classification tasks."],"url":"http://arxiv.org/abs/2405.05853v1","category":"cs.CV"}
{"created":"2024-05-09 15:12:44","title":"Multiplicity of solutions for mixed local-nonlocal elliptic equations with singular nonlinearity","abstract":"We will prove multiplicity results for the mixed local-nonlocal elliptic equation of the form \\begin{eqnarray} \\begin{split} -\\Delta_pu+(-\\Delta)_p^s u&=\\frac{\\lambda}{u^{\\gamma}}+u^r \\text { in } \\Omega, \\\\u&>0 \\text{ in } \\Omega,\\\\u&=0 \\text { in }\\mathbb{R}^n \\backslash \\Omega; \\end{split} \\end{eqnarray} where \\begin{equation*} (-\\Delta )_p^s u(x)= c_{n,s}\\operatorname{P.V.}\\int_{\\mathbb{R}^n}\\frac{|u(x)-u(y)|^{p-2}(u(x)-u(y))}{|x-y|^{n+sp}} d y, \\end{equation*} and $-\\Delta_p$ is the usual $p$-Laplace operator. Under the assumptions that $\\Omega$ is a bounded domain in $\\mathbb{R}^{n}$ with regular enough boundary, $p>1$, $n> p$, $s\\in(0,1)$, $\\lambda>0$ and $r\\in(p-1,p^*-1)$ where $p^*$ is the critical Sobolev exponent, we will show there exist at least two weak solutions to our problem for $0<\\gamma<1$ and some certain values of $\\lambda$. Further, for every $\\gamma>0$, assuming strict convexity of $\\Omega$, for $p=2$ and $s\\in(0,1/2)$, we will show the existence of at least two positive weak solutions to the problem, for small values of $\\lambda$, extending the result of \\cite{garaingeometric}. Here $c_{n,s}$ is a suitable normalization constant, and $\\operatorname{P.V.}$ stands for Cauchy Principal Value.","sentences":["We will prove multiplicity results for the mixed local-nonlocal elliptic equation of the form \\begin{eqnarray} \\begin{split} -\\Delta_pu+(-\\Delta)_p^s u&=\\frac{\\lambda}{u^{\\gamma}}+u^r \\text { in } \\Omega, \\\\u&>0 \\text{ in } \\Omega,\\\\u&=0 \\text { in }\\mathbb{R}^n \\backslash \\Omega; \\end{split} \\end{eqnarray} where \\begin{equation*} (-\\Delta )_","p^s u(x)= c_{n,s}\\operatorname{P.V.}\\int_{\\mathbb{R}^n}\\frac{|u(x)-u(y)|^{p-2}(u(x)-u(y))}{|x-y|^{n+sp}} d y, \\end{equation*} and $-\\Delta_p$ is the usual $p$-Laplace operator.","Under the assumptions that $\\Omega$ is a bounded domain in $\\mathbb{R}^{n}$ with regular enough boundary, $p>1$, $n> p$, $s\\in(0,1)$, $\\lambda>0$ and $r\\in(p-1,p^*-1)$ where $p^*$ is the critical Sobolev exponent, we will show there exist at least two weak solutions to our problem for $0<\\gamma<1$ and some certain values of $\\lambda$. Further, for every $\\gamma>0$, assuming strict convexity of $\\Omega$, for $p=2$ and $s\\in(0,1/2)$, we will show the existence of at least two positive weak solutions to the problem, for small values of $\\lambda$, extending the result of \\cite{garaingeometric}.","Here $c_{n,s}$ is a suitable normalization constant, and $\\operatorname{P.V.}$ stands for Cauchy Principal Value."],"url":"http://arxiv.org/abs/2405.05832v1","category":"math.AP"}
{"created":"2024-05-09 15:04:07","title":"Mask-TS Net: Mask Temperature Scaling Uncertainty Calibration for Polyp Segmentation","abstract":"Lots of popular calibration methods in medical images focus on classification, but there are few comparable studies on semantic segmentation. In polyp segmentation of medical images, we find most diseased area occupies only a small portion of the entire image, resulting in previous models being not well-calibrated for lesion regions but well-calibrated for background, despite their seemingly better Expected Calibration Error (ECE) scores overall. Therefore, we proposed four-branches calibration network with Mask-Loss and Mask-TS strategies to more focus on the scaling of logits within potential lesion regions, which serves to mitigate the influence of background interference. In the experiments, we compare the existing calibration methods with the proposed Mask Temperature Scaling (Mask-TS). The results indicate that the proposed calibration network outperforms other methods both qualitatively and quantitatively.","sentences":["Lots of popular calibration methods in medical images focus on classification, but there are few comparable studies on semantic segmentation.","In polyp segmentation of medical images, we find most diseased area occupies only a small portion of the entire image, resulting in previous models being not well-calibrated for lesion regions but well-calibrated for background, despite their seemingly better Expected Calibration Error (ECE) scores overall.","Therefore, we proposed four-branches calibration network with Mask-Loss and Mask-TS strategies to more focus on the scaling of logits within potential lesion regions, which serves to mitigate the influence of background interference.","In the experiments, we compare the existing calibration methods with the proposed Mask Temperature Scaling (Mask-TS).","The results indicate that the proposed calibration network outperforms other methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.05830v1","category":"cs.CV"}
{"created":"2024-05-09 15:02:31","title":"Drift-Energy Replacement Effect in Multi-Ion Magnetized Plasmas","abstract":"Fast rotation can improve the stability and confinement of fusion plasmas. However, to maintain a rapidly rotating fusion plasma in steady state, significant energy must be invested in spinning up each incoming fuel ion. We show here that, under the right circumstances, collisional cross-field radial fueling can directly transfer drift energy between outgoing and incoming ions without the need for external power recirculation, thereby reducing the energy costs of maintaining the rotation.","sentences":["Fast rotation can improve the stability and confinement of fusion plasmas.","However, to maintain a rapidly rotating fusion plasma in steady state, significant energy must be invested in spinning up each incoming fuel ion.","We show here that, under the right circumstances, collisional cross-field radial fueling can directly transfer drift energy between outgoing and incoming ions without the need for external power recirculation, thereby reducing the energy costs of maintaining the rotation."],"url":"http://arxiv.org/abs/2405.05829v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 14:47:15","title":"Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes","abstract":"Neural network sparsity has attracted many research interests due to its similarity to biological schemes and high energy efficiency. However, existing methods depend on long-time training or fine-tuning, which prevents large-scale applications. Recently, some works focusing on post-training sparsity (PTS) have emerged. They get rid of the high training cost but usually suffer from distinct accuracy degradation due to neglect of the reasonable sparsity rate at each layer. Previous methods for finding sparsity rates mainly focus on the training-aware scenario, which usually fails to converge stably under the PTS setting with limited data and much less training cost. In this paper, we propose a fast and controllable post-training sparsity (FCPTS) framework. By incorporating a differentiable bridge function and a controllable optimization objective, our method allows for rapid and accurate sparsity allocation learning in minutes, with the added assurance of convergence to a predetermined global sparsity rate. Equipped with these techniques, we can surpass the state-of-the-art methods by a large margin, e.g., over 30\\% improvement for ResNet-50 on ImageNet under the sparsity rate of 80\\%. Our plug-and-play code and supplementary materials are open-sourced at https://github.com/ModelTC/FCPTS.","sentences":["Neural network sparsity has attracted many research interests due to its similarity to biological schemes and high energy efficiency.","However, existing methods depend on long-time training or fine-tuning, which prevents large-scale applications.","Recently, some works focusing on post-training sparsity (PTS) have emerged.","They get rid of the high training cost but usually suffer from distinct accuracy degradation due to neglect of the reasonable sparsity rate at each layer.","Previous methods for finding sparsity rates mainly focus on the training-aware scenario, which usually fails to converge stably under the PTS setting with limited data and much less training cost.","In this paper, we propose a fast and controllable post-training sparsity (FCPTS) framework.","By incorporating a differentiable bridge function and a controllable optimization objective, our method allows for rapid and accurate sparsity allocation learning in minutes, with the added assurance of convergence to a predetermined global sparsity rate.","Equipped with these techniques, we can surpass the state-of-the-art methods by a large margin, e.g., over 30\\% improvement for ResNet-50 on ImageNet under the sparsity rate of 80\\%.","Our plug-and-play code and supplementary materials are open-sourced at https://github.com/ModelTC/FCPTS."],"url":"http://arxiv.org/abs/2405.05808v1","category":"cs.CV"}
{"created":"2024-05-09 13:57:28","title":"Neural Network Learning of Black-Scholes Equation for Option Pricing","abstract":"One of the most discussed problems in the financial world is stock option pricing. The Black-Scholes Equation is a Parabolic Partial Differential Equation which provides an option pricing model. The present work proposes an approach based on Neural Networks to solve the Black-Scholes Equations. Real-world data from the stock options market were used as the initial boundary to solve the Black-Scholes Equation. In particular, times series of call options prices of Brazilian companies Petrobras and Vale were employed. The results indicate that the network can learn to solve the Black-Sholes Equation for a specific real-world stock options time series. The experimental results showed that the Neural network option pricing based on the Black-Sholes Equation solution can reach an option pricing forecasting more accurate than the traditional Black-Sholes analytical solutions. The experimental results making it possible to use this methodology to make short-term call option price forecasts in options markets.","sentences":["One of the most discussed problems in the financial world is stock option pricing.","The Black-Scholes Equation is a Parabolic Partial Differential Equation which provides an option pricing model.","The present work proposes an approach based on Neural Networks to solve the Black-Scholes Equations.","Real-world data from the stock options market were used as the initial boundary to solve the Black-Scholes Equation.","In particular, times series of call options prices of Brazilian companies Petrobras and Vale were employed.","The results indicate that the network can learn to solve the Black-Sholes Equation for a specific real-world stock options time series.","The experimental results showed that the Neural network option pricing based on the Black-Sholes Equation solution can reach an option pricing forecasting more accurate than the traditional Black-Sholes analytical solutions.","The experimental results making it possible to use this methodology to make short-term call option price forecasts in options markets."],"url":"http://arxiv.org/abs/2405.05780v1","category":"cs.LG"}
{"created":"2024-05-09 13:55:00","title":"Weak coupling limit of a Brownian particle in the curl of the 2D GFF","abstract":"In this article, we study the weak coupling limit of the following equation in $\\mathbb{R}^2$: $$dX_t^\\varepsilon=\\frac{\\hat{\\lambda}}{\\sqrt{\\log\\frac1\\varepsilon}}\\omega^\\varepsilon(X_t^\\varepsilon)dt+\\nu dB_t,\\quad X_0^\\varepsilon=0. $$ Here $\\omega^\\varepsilon=\\nabla^{\\perp}\\rho_\\varepsilon*\\xi$ with $\\xi$ representing the $2d$ Gaussian Free Field (GFF) and $\\rho_\\varepsilon$ denoting an appropriate identity. $B_t$ denotes a two-dimensional standard Brownian motion, and $\\hat{\\lambda},\\nu>0$ are two given constants. We use the approach from \\cite{Cannizzaro.2023} to show that the second moment of $X_t^\\varepsilon$ under the annealed law converges to $(c(\\nu)^2+2\\nu^2)t$ with a precisely determined constant $c(\\nu)>0$, which implies a non-trivial limit of the drift terms as $\\varepsilon$ vanishes. We also prove that in this weak coupling regime, the sequence of solutions converges in distribution to $\\left(\\sqrt{\\frac{c(\\nu)^2}{2}+\\nu^2}\\right)\\widetilde{B}_t$ as $\\varepsilon$ vanishes, where $\\widetilde{B}_t$ is a two-dimensional standard Brownian motion.","sentences":["In this article, we study the weak coupling limit of the following equation in $\\mathbb{R}^2$: $$dX_t^\\varepsilon=\\frac{\\hat{\\lambda}}{\\sqrt{\\log\\frac1\\varepsilon}}\\omega^\\varepsilon(X_t^\\varepsilon)dt+\\nu dB_t,\\quad X_0^\\varepsilon=0.","$$ Here $\\omega^\\varepsilon=\\nabla^{\\perp}\\rho_\\varepsilon*\\xi$ with $\\xi$ representing the $2d$ Gaussian Free Field (GFF) and $\\rho_\\varepsilon$ denoting an appropriate identity.","$B_t$ denotes a two-dimensional standard Brownian motion, and $\\hat{\\lambda},\\nu>0$ are two given constants.","We use the approach from \\cite{Cannizzaro.2023} to show that the second moment of $X_t^\\varepsilon$ under the annealed law converges to $(c(\\nu)^2+2\\nu^2)t$ with a precisely determined constant $c(\\nu)>0$, which implies a non-trivial limit of the drift terms as $\\varepsilon$ vanishes.","We also prove that in this weak coupling regime, the sequence of solutions converges in distribution to $\\left(\\sqrt{\\frac{c(\\nu)^2}{2}+\\nu^2}\\right)\\widetilde{B}_t$ as $\\varepsilon$ vanishes, where $\\widetilde{B}_t$ is a two-dimensional standard Brownian motion."],"url":"http://arxiv.org/abs/2405.05778v1","category":"math.PR"}
{"created":"2024-05-09 13:32:26","title":"Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media","abstract":"The purpose of semantic location prediction is to extract relevant semantic location information from multimodal social media posts, offering a more contextual understanding of daily activities compared to GPS coordinates. However, this task becomes challenging due to the presence of noise and irrelevant information in \"text-image\" pairs. Existing methods suffer from insufficient feature representations and fail to consider the comprehensive integration of similarity at different granularities, making it difficult to filter out noise and irrelevant information. To address these challenges, we propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting social users' semantic locations. First, we utilize a pre-trained large-scale vision-language model to extract high-quality feature representations from social media posts. Then, we introduce a Similarity-Guided Interaction Module (SIM) to alleviate modality heterogeneity and noise interference by incorporating coarse-grained and fine-grained similarity guidance for modality interactions. Specifically, we propose a novel similarity-aware feature interpolation attention mechanism at the coarse level, leveraging modality-wise similarity to mitigate heterogeneity and reduce noise within each modality. Meanwhile, we employ a similarity-aware feed-forward block at the fine level, utilizing element-wise similarity to further mitigate the impact of modality heterogeneity. Building upon pre-processed features with minimal noise and modal interference, we propose a Similarity-aware Feature Fusion Module (SFM) to fuse two modalities with cross-attention mechanism. Comprehensive experimental results demonstrate the superior performance of our proposed method in handling modality imbalance while maintaining efficient fusion effectiveness.","sentences":["The purpose of semantic location prediction is to extract relevant semantic location information from multimodal social media posts, offering a more contextual understanding of daily activities compared to GPS coordinates.","However, this task becomes challenging due to the presence of noise and irrelevant information in \"text-image\" pairs.","Existing methods suffer from insufficient feature representations and fail to consider the comprehensive integration of similarity at different granularities, making it difficult to filter out noise and irrelevant information.","To address these challenges, we propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting social users' semantic locations.","First, we utilize a pre-trained large-scale vision-language model to extract high-quality feature representations from social media posts.","Then, we introduce a Similarity-Guided Interaction Module (SIM) to alleviate modality heterogeneity and noise interference by incorporating coarse-grained and fine-grained similarity guidance for modality interactions.","Specifically, we propose a novel similarity-aware feature interpolation attention mechanism at the coarse level, leveraging modality-wise similarity to mitigate heterogeneity and reduce noise within each modality.","Meanwhile, we employ a similarity-aware feed-forward block at the fine level, utilizing element-wise similarity to further mitigate the impact of modality heterogeneity.","Building upon pre-processed features with minimal noise and modal interference, we propose a Similarity-aware Feature Fusion Module (SFM) to fuse two modalities with cross-attention mechanism.","Comprehensive experimental results demonstrate the superior performance of our proposed method in handling modality imbalance while maintaining efficient fusion effectiveness."],"url":"http://arxiv.org/abs/2405.05760v1","category":"cs.CV"}
{"created":"2024-05-09 12:35:57","title":"Private Online Community Detection for Censored Block Models","abstract":"We study the private online change detection problem for dynamic communities, using a censored block model (CBM). Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed method.","sentences":["We study the private online change detection problem for dynamic communities, using a censored block model (CBM).","Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels.","We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy.","Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP.","Simulation and real data examples are provided to validate the proposed method."],"url":"http://arxiv.org/abs/2405.05724v1","category":"cs.SI"}
{"created":"2024-05-09 12:10:02","title":"Characteristic-Mode Based Conformal Design of Ultra-Wideband Antenna Array","abstract":"An innovative design method of conformal array antennas is presented by utilizing characteristic mode analysis (CMA) in this work. A single-layer continuous perfect electric conductor under bending conditions is conducted by CMA to evaluate the variations in operating performance. By using this method, the design process of a conformal array is simplified. The results indicate that the operating performance of the antenna with single-layer metal radiation structure remains stable within a certain range of curvature. Subsequently, an infinite array element using single-layer metal radiation structure is designed, operating in ultra-wideband and dual polarization. Following, an 8 * 8 ultra-wideband dual-polarized cylindrical-conformal array (UDCA) is developed by wrapping the planar arrays to a cylindric surface, which has a stable operating performance even at a curvature radius as small as 100 mm. Finally, a physical prototype is cost-effectively fabricated by novel manufacturing solutions that stack three-layer conformal substrate. The experimental result demonstrates that the proposed UDCA with a 1.2{\\lambda} curvature radius operates at 3.6~9.6 GHz (90.9%) and achieves 60{\\deg} wide-angle scanning in two principal planes, which provides a practical and promising solution for conformal array applications. The insights derived from the CMA offer a direction for further advancement in conformal antenna research.","sentences":["An innovative design method of conformal array antennas is presented by utilizing characteristic mode analysis (CMA) in this work.","A single-layer continuous perfect electric conductor under bending conditions is conducted by CMA to evaluate the variations in operating performance.","By using this method, the design process of a conformal array is simplified.","The results indicate that the operating performance of the antenna with single-layer metal radiation structure remains stable within a certain range of curvature.","Subsequently, an infinite array element using single-layer metal radiation structure is designed, operating in ultra-wideband and dual polarization.","Following, an 8 * 8 ultra-wideband dual-polarized cylindrical-conformal array (UDCA) is developed by wrapping the planar arrays to a cylindric surface, which has a stable operating performance even at a curvature radius as small as 100 mm.","Finally, a physical prototype is cost-effectively fabricated by novel manufacturing solutions that stack three-layer conformal substrate.","The experimental result demonstrates that the proposed UDCA with a 1.2{\\lambda} curvature radius operates at 3.6~9.6 GHz (90.9%) and achieves 60{\\deg} wide-angle scanning in two principal planes, which provides a practical and promising solution for conformal array applications.","The insights derived from the CMA offer a direction for further advancement in conformal antenna research."],"url":"http://arxiv.org/abs/2405.05708v1","category":"cs.IT"}
{"created":"2024-05-09 11:12:03","title":"Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms","abstract":"In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials. We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H\\\"{o}lder condition with exponent $q\\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution. Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition. Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms. Numerical experiments are conducted to sample from several distributions and the results support our main findings.","sentences":["In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials.","We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H\\\"{o}lder condition with exponent $q\\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution.","Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition.","Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms.","Numerical experiments are conducted to sample from several distributions and the results support our main findings."],"url":"http://arxiv.org/abs/2405.05679v1","category":"math.ST"}
{"created":"2024-05-09 10:51:48","title":"Self-correcting GKP qubit and gates in a driven-dissipative circuit","abstract":"We propose a circuit architecture for a dissipatively error-corrected GKP qubit. The device consists of a high-impedance LC circuit coupled to a Josephson junction and a resistor via a controllable switch. When the switch is activated via a particular family of stepwise protocols, the resistor absorbs all noise-induced entropy, resulting in dissipative error correction of both phase and amplitude errors. This leads to an exponential increase of qubit lifetime, reaching beyond 10ms in simulations with near-feasible parameters. We show that the lifetime remains exponentially long in the presence of extrinsic noise and device/control imperfections (e.g., due to parasitics and finite control bandwidth) under specific thresholds. In this regime, lifetime is likely only limited by phase slips and quasiparticle tunneling. We show that the qubit can be read out and initialized via measurement of the supercurrent in the Josephson junction. We finally show that the qubit supports native self-correcting single-qubit Clifford gates, where dissipative error-correction of control noise leads to exponential suppression of gate infidelity.","sentences":["We propose a circuit architecture for a dissipatively error-corrected GKP qubit.","The device consists of a high-impedance LC circuit coupled to a Josephson junction and a resistor via a controllable switch.","When the switch is activated via a particular family of stepwise protocols, the resistor absorbs all noise-induced entropy, resulting in dissipative error correction of both phase and amplitude errors.","This leads to an exponential increase of qubit lifetime, reaching beyond 10ms in simulations with near-feasible parameters.","We show that the lifetime remains exponentially long in the presence of extrinsic noise and device/control imperfections (e.g., due to parasitics and finite control bandwidth) under specific thresholds.","In this regime, lifetime is likely only limited by phase slips and quasiparticle tunneling.","We show that the qubit can be read out and initialized via measurement of the supercurrent in the Josephson junction.","We finally show that the qubit supports native self-correcting single-qubit Clifford gates, where dissipative error-correction of control noise leads to exponential suppression of gate infidelity."],"url":"http://arxiv.org/abs/2405.05671v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 09:57:01","title":"Development of realistic simulations for the polarization of the cosmic microwave background","abstract":"Polarization of the cosmic microwave background (CMB) can help probe cosmic inflation (via primordial $B$ modes) and test parity-violating physics (via cosmic birefringence), but realizing the potential of these opportunities requires precise control and mitigation of systematic effects. To this end, some experiments (including LiteBIRD) will use rotating half-wave plates (HWPs) as polarization modulators. Ideally, this choice should remove the $1/f$ noise component in the observed polarization and reduce intensity-to-polarization leakage, but any real HWP is characterized by non-idealities that, if not properly treated in the analysis, can lead to new systematics. In this thesis, after briefly introducing the science case, we discuss the macro steps that make up any CMB experiment, introduce the HWP, and present a new time-ordered data (TOD) simulation pipeline tailored to a LiteBIRD-like experiment that returns TOD and binned maps for realistic beams and HWPs. We show that the simulation framework can be used to study how the HWP non-idealities affect the measured cosmic birefringence angle, resulting in a few degrees bias for a realistic choice of HWP. We also derive analytical formulae to model the observed temperature and polarization maps and test them against the simulations. Finally, we present a simple, semi-analytical end-to-end model to propagate the non-idealities through the macro-steps that make up any CMB experiment (observation of multi-frequency maps, foreground cleaning, and power spectra estimation) and compute the HWP-induced bias on the estimated tensor-to-scalar ratio, $r$, finding that the HWP leads to underestimating $r$. We also show how gain calibration of the CMB temperature can be used to partially mitigate the non-idealities' effect and present a set of recommendations for the HWP design that can help maximize the benefits of gain calibration. [abridged]","sentences":["Polarization of the cosmic microwave background (CMB) can help probe cosmic inflation (via primordial $B$ modes) and test parity-violating physics (via cosmic birefringence), but realizing the potential of these opportunities requires precise control and mitigation of systematic effects.","To this end, some experiments (including LiteBIRD) will use rotating half-wave plates (HWPs) as polarization modulators.","Ideally, this choice should remove the $1/f$ noise component in the observed polarization and reduce intensity-to-polarization leakage, but any real HWP is characterized by non-idealities that, if not properly treated in the analysis, can lead to new systematics.","In this thesis, after briefly introducing the science case, we discuss the macro steps that make up any CMB experiment, introduce the HWP, and present a new time-ordered data (TOD) simulation pipeline tailored to a LiteBIRD-like experiment that returns TOD and binned maps for realistic beams and HWPs.","We show that the simulation framework can be used to study how the HWP non-idealities affect the measured cosmic birefringence angle, resulting in a few degrees bias for a realistic choice of HWP.","We also derive analytical formulae to model the observed temperature and polarization maps and test them against the simulations.","Finally, we present a simple, semi-analytical end-to-end model to propagate the non-idealities through the macro-steps that make up any CMB experiment (observation of multi-frequency maps, foreground cleaning, and power spectra estimation) and compute the HWP-induced bias on the estimated tensor-to-scalar ratio, $r$, finding that the HWP leads to underestimating $r$. We also show how gain calibration of the CMB temperature can be used to partially mitigate the non-idealities' effect and present a set of recommendations for the HWP design that can help maximize the benefits of gain calibration.","[abridged]"],"url":"http://arxiv.org/abs/2405.05651v1","category":"astro-ph.CO"}
{"created":"2024-05-09 09:34:54","title":"Estimation of ill-conditioned models using penalized sums of squares of the residuals","abstract":"This paper analyzes the estimation of econometric models by penalizing the sum of squares of the residuals with a factor that makes the model estimates approximate those that would be obtained when considering the possible simple regressions between the dependent variable of the econometric model and each of its independent variables. It is shown that the ridge estimator is a particular case of the penalized estimator obtained, which, upon analysis of its main characteristics, presents better properties than the ridge especially in reference to the individual boostrap inference of the coefficients of the model and the numerical stability of the estimates obtained. This improvement is due to the fact that instead of shrinking the estimator towards zero, the estimator shrinks towards the estimates of the coefficients of the simple regressions discussed above.","sentences":["This paper analyzes the estimation of econometric models by penalizing the sum of squares of the residuals with a factor that makes the model estimates approximate those that would be obtained when considering the possible simple regressions between the dependent variable of the econometric model and each of its independent variables.","It is shown that the ridge estimator is a particular case of the penalized estimator obtained, which, upon analysis of its main characteristics, presents better properties than the ridge especially in reference to the individual boostrap inference of the coefficients of the model and the numerical stability of the estimates obtained.","This improvement is due to the fact that instead of shrinking the estimator towards zero, the estimator shrinks towards the estimates of the coefficients of the simple regressions discussed above."],"url":"http://arxiv.org/abs/2405.05644v1","category":"math.ST"}
{"created":"2024-05-09 09:10:11","title":"Controlled Fabrication of Native Ultra-Thin Amorphous Gallium Oxide from 2D Gallium Sulfide for Emerging Electronic Applications","abstract":"Oxidation of two-dimensional (2D) layered materials has proven advantageous in creating oxide/2D material heterostructures, opening the door for a new paradigm of low-power electronic devices. Gallium (II) sulfide ($\\beta$-GaS), a hexagonal phase group III monochalcogenide, is a wide bandgap semiconductor with a bandgap exceeding 3 eV in single and few layer form. Its oxide, gallium oxide (Ga$_2$O$_3$), combines large bandgap (4.4-5.3 eV) with high dielectric constant (~10). Despite the technological potential of both materials, controlled oxidation of atomically-thin $\\beta$-GaS remains under-explored. This study focuses into the controlled oxidation of $\\beta$-GaS using oxygen plasma treatment, achieving ultrathin native oxide (GaS$_x$O$_y$, ~4 nm) and GaS$_x$O$_y$/GaS heterostructures where the GaS layer beneath remains intact. By integrating such structures between metal electrodes and applying electric stresses as voltage ramps or pulses, we investigate their use for resistive random-access memory (ReRAM). The ultrathin nature of the produced oxide enables low operation power with energy use as low as 0.22 nJ per operation while maintaining endurance and retention of 350 cycles and 10$^4$ s, respectively. These results show the significant potential of the oxidation-based GaS$_x$O$_y$/GaS heterostructure for electronic applications and, in particular, low-power memory devices.","sentences":["Oxidation of two-dimensional (2D) layered materials has proven advantageous in creating oxide/2D material heterostructures, opening the door for a new paradigm of low-power electronic devices.","Gallium (II) sulfide ($\\beta$-GaS), a hexagonal phase group III monochalcogenide, is a wide bandgap semiconductor with a bandgap exceeding 3 eV in single and few layer form.","Its oxide, gallium oxide (Ga$_2$O$_3$), combines large bandgap (4.4-5.3 eV) with high dielectric constant (~10).","Despite the technological potential of both materials, controlled oxidation of atomically-thin $\\beta$-GaS remains under-explored.","This study focuses into the controlled oxidation of $\\beta$-GaS using oxygen plasma treatment, achieving ultrathin native oxide (GaS$_x$O$_y$, ~4 nm) and GaS$_x$O$_y$/GaS heterostructures where the GaS layer beneath remains intact.","By integrating such structures between metal electrodes and applying electric stresses as voltage ramps or pulses, we investigate their use for resistive random-access memory (ReRAM).","The ultrathin nature of the produced oxide enables low operation power with energy use as low as 0.22 nJ per operation while maintaining endurance and retention of 350 cycles and 10$^4$ s, respectively.","These results show the significant potential of the oxidation-based GaS$_x$O$_y$/GaS heterostructure for electronic applications and, in particular, low-power memory devices."],"url":"http://arxiv.org/abs/2405.05632v1","category":"physics.app-ph"}
{"created":"2024-05-09 08:54:12","title":"An Uncertainty-aware, Mesh-free Numerical Method for Kolmogorov PDEs","abstract":"This study introduces an uncertainty-aware, mesh-free numerical method for solving Kolmogorov PDEs. In the proposed method, we use Gaussian process regression (GPR) to smoothly interpolate pointwise solutions that are obtained by Monte Carlo methods based on the Feynman-Kac formula. The proposed method has two main advantages: 1. uncertainty assessment, which is facilitated by the probabilistic nature of GPR, and 2. mesh-free computation, which allows efficient handling of high-dimensional PDEs. The quality of the solution is improved by adjusting the kernel function and incorporating noise information from the Monte Carlo samples into the GPR noise model. The performance of the method is rigorously analyzed based on a theoretical lower bound on the posterior variance, which serves as a measure of the error between the numerical and true solutions. Extensive tests on three representative PDEs demonstrate the high accuracy and robustness of the method compared to existing methods.","sentences":["This study introduces an uncertainty-aware, mesh-free numerical method for solving Kolmogorov PDEs.","In the proposed method, we use Gaussian process regression (GPR) to smoothly interpolate pointwise solutions that are obtained by Monte Carlo methods based on the Feynman-Kac formula.","The proposed method has two main advantages: 1. uncertainty assessment, which is facilitated by the probabilistic nature of GPR, and 2. mesh-free computation, which allows efficient handling of high-dimensional PDEs.","The quality of the solution is improved by adjusting the kernel function and incorporating noise information from the Monte Carlo samples into the GPR noise model.","The performance of the method is rigorously analyzed based on a theoretical lower bound on the posterior variance, which serves as a measure of the error between the numerical and true solutions.","Extensive tests on three representative PDEs demonstrate the high accuracy and robustness of the method compared to existing methods."],"url":"http://arxiv.org/abs/2405.05626v1","category":"math.NA"}
{"created":"2024-05-09 08:35:38","title":"Emerging Optimization Problems for Distribution in Same-day Delivery","abstract":"Same-day deliveries (SDD) have become a new standard to satisfy the \"instant gratification\" of online customers. Despite the existing powerful technologies deployed in last-mile delivery, SDD services face new decision-making challenges related to the trade-off between delivery cost and time. In addition, new challenges related to environmental issues, customer satisfaction, or fairness arise. Researchers have explored various approaches to face these challenges in the context of SDD, where stochastic and dynamic data uncertainty plays a fundamental role. In this paper, we carefully review the emerging routing problems and solutions proposed in the existing literature for SDD services. We address the questions related to how to deal with dynamic arrival times of orders, how to allocate time slots to deliveries, how to select the right delivery options, how to design pickup and delivery routes, or how to partition the delivery areas and decide the composition of the fleet. We also formulate and compare models for representative problems elaborating on the pros and cons that might guide practitioners in choosing the most appropriate objectives and constraints. Finally, we sketch challenges and identify future research directions.","sentences":["Same-day deliveries (SDD) have become a new standard to satisfy the \"instant gratification\" of online customers.","Despite the existing powerful technologies deployed in last-mile delivery, SDD services face new decision-making challenges related to the trade-off between delivery cost and time.","In addition, new challenges related to environmental issues, customer satisfaction, or fairness arise.","Researchers have explored various approaches to face these challenges in the context of SDD, where stochastic and dynamic data uncertainty plays a fundamental role.","In this paper, we carefully review the emerging routing problems and solutions proposed in the existing literature for SDD services.","We address the questions related to how to deal with dynamic arrival times of orders, how to allocate time slots to deliveries, how to select the right delivery options, how to design pickup and delivery routes, or how to partition the delivery areas and decide the composition of the fleet.","We also formulate and compare models for representative problems elaborating on the pros and cons that might guide practitioners in choosing the most appropriate objectives and constraints.","Finally, we sketch challenges and identify future research directions."],"url":"http://arxiv.org/abs/2405.05620v1","category":"math.OC"}
{"created":"2024-05-09 15:01:04","title":"Efficient designs for threshold group testing without gap","abstract":"Given $d$ defective items in a population of $n$ items with $d \\ll n$, in threshold group testing without gap, the outcome of a test on a subset of items is positive if the subset has at least $u$ defective items and negative otherwise, where $1 \\leq u \\leq d$. The basic goal of threshold group testing is to quickly identify the defective items via a small number of tests. In non-adaptive design, all tests are designed independently and can be performed in parallel. The decoding time in the non-adaptive state-of-the-art work is a polynomial of $(d/u)^u (d/(d-u))^{d - u}, d$, and $\\log{n}$. In this work, we present a novel design that significantly reduces the number of tests and the decoding time to polynomials of $\\min\\{u^u, (d - u)^{d - u}\\}, d$, and $\\log{n}$. In particular, when $u$ is a constant, the number of tests and the decoding time are $O(d^3 (\\log^2{n}) \\log{(n/d)} )$ and $O\\big(d^3 (\\log^2{n}) \\log{(n/d)} + d^2 (\\log{n}) \\log^3{(n/d)} \\big)$, respectively. For a special case when $u = 2$, with non-adaptive design, the number of tests and the decoding time are $O(d^3 (\\log{n}) \\log{(n/d)} )$ and $O(d^2 (\\log{n} + \\log^2{(n/d)}) )$, respectively. Moreover, with 2-stage design, the number of tests and the decoding time are $O(d^2 \\log^2{(n/d)} )$.","sentences":["Given $d$ defective items in a population of $n$ items with $d \\ll n$, in threshold group testing without gap, the outcome of a test on a subset of items is positive if the subset has at least $u$ defective items and negative otherwise, where $1 \\leq u \\leq d$.","The basic goal of threshold group testing is to quickly identify the defective items via a small number of tests.","In non-adaptive design, all tests are designed independently and can be performed in parallel.","The decoding time in the non-adaptive state-of-the-art work is a polynomial of $(d/u)^u (d/(d-u))^{d - u}, d$, and $\\log{n}$. In this work, we present a novel design that significantly reduces the number of tests and the decoding time to polynomials of $\\min\\{u^u, (d - u)^{d - u}\\}, d$, and $\\log{n}$. In particular, when $u$ is a constant, the number of tests and the decoding time are $O(d^3 (\\log^2{n}) \\log{(n/d)} )$ and $O\\big(d^3 (\\log^2{n}) \\log{(n/d)}","+ d^2 (\\log{n})","\\log^3{(n/d)} \\big)$, respectively.","For a special case when $u = 2$, with non-adaptive design, the number of tests and the decoding time are $O(d^3 (\\log{n}) \\log{(n/d)} )$ and $O(d^2 (\\log{n} + \\log^2{(n/d)}) )$, respectively.","Moreover, with 2-stage design, the number of tests and the decoding time are $O(d^2 \\log^2{(n/d)} )$."],"url":"http://arxiv.org/abs/2405.05827v1","category":"cs.IT"}
{"created":"2024-05-09 17:58:25","title":"Age Aware Scheduling for Differentially-Private Federated Learning","abstract":"This paper explores differentially-private federated learning (FL) across time-varying databases, delving into a nuanced three-way tradeoff involving age, accuracy, and differential privacy (DP). Emphasizing the potential advantages of scheduling, we propose an optimization problem aimed at meeting DP requirements while minimizing the loss difference between the aggregated model and the model obtained without DP constraints. To harness the benefits of scheduling, we introduce an age-dependent upper bound on the loss, leading to the development of an age-aware scheduling design. Simulation results underscore the superior performance of our proposed scheme compared to FL with classic DP, which does not consider scheduling as a design factor. This research contributes insights into the interplay of age, accuracy, and DP in federated learning, with practical implications for scheduling strategies.","sentences":["This paper explores differentially-private federated learning (FL) across time-varying databases, delving into a nuanced three-way tradeoff involving age, accuracy, and differential privacy (DP).","Emphasizing the potential advantages of scheduling, we propose an optimization problem aimed at meeting DP requirements while minimizing the loss difference between the aggregated model and the model obtained without DP constraints.","To harness the benefits of scheduling, we introduce an age-dependent upper bound on the loss, leading to the development of an age-aware scheduling design.","Simulation results underscore the superior performance of our proposed scheme compared to FL with classic DP, which does not consider scheduling as a design factor.","This research contributes insights into the interplay of age, accuracy, and DP in federated learning, with practical implications for scheduling strategies."],"url":"http://arxiv.org/abs/2405.05962v1","category":"cs.LG"}
{"created":"2024-05-09 17:53:28","title":"OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning","abstract":"Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.However, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications. Training smaller models is an effective way to address this problem. Therefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3\\% with minimal performance loss. OpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios.","sentences":["Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.","However, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications.","Training smaller models is an effective way to address this problem.","Therefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model.","OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3\\% with minimal performance loss.","OpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER).","OpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios."],"url":"http://arxiv.org/abs/2405.05957v1","category":"cs.CL"}
{"created":"2024-05-09 16:47:47","title":"A note on the volume entropy on a harmonic manifold of hypergeometric type","abstract":"Harmonic manifolds of hypergeometric type form a class of non-compact harmonic manifolds that includes rank one symmetric spaces of non-compact type and Damek-Ricci spaces. When normalizing the metric of a harmonic manifold of hypergeometric type to satisfy the Ricci curvature $\\mathrm{Ric} = -(n-1)$, we show that the volume entropy of this manifold satisfies a certain inequality. Additionally, we show that manifolds yielding the upper bound of volume entropy are only real hyperbolic spaces with sectional curvature $-1$, while examples of Damek-Ricci spaces yielding the lower bound exist in only four cases.","sentences":["Harmonic manifolds of hypergeometric type form a class of non-compact harmonic manifolds that includes rank one symmetric spaces of non-compact type and Damek-Ricci spaces.","When normalizing the metric of a harmonic manifold of hypergeometric type to satisfy the Ricci curvature $\\mathrm{Ric} = -(n-1)$, we show that the volume entropy of this manifold satisfies a certain inequality.","Additionally, we show that manifolds yielding the upper bound of volume entropy are only real hyperbolic spaces with sectional curvature $-1$, while examples of Damek-Ricci spaces yielding the lower bound exist in only four cases."],"url":"http://arxiv.org/abs/2405.05896v1","category":"math.DG"}
{"created":"2024-05-09 16:00:49","title":"The Iwasawa $\u03bc$-invariant of certain elliptic curves of analytic rank zero","abstract":"This paper is about the Iwasawa theory of elliptic curves over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}^{\\text{cyc}}$ of $\\mathbb{Q}$. We discuss a deep conjecture of Greenberg that if $E/\\mathbb{Q}$ is an elliptic curve with good ordinary reduction at $p$, and $E[p]$ is irreducible as a Galois module, then the Selmer group of $E$ over $\\mathbb{Q}^{\\text{cyc}}$ has $\\mu$-invariant zero. We prove new cases of Greenberg's conjecture for some elliptic curves of analytic rank $0$. The proof involves studying the $p$-adic $L$-function of $E$. The crucial input is a new technique using the Rankin-Selberg method.","sentences":["This paper is about the Iwasawa theory of elliptic curves over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}^{\\text{cyc}}$ of $\\mathbb{Q}$. We discuss a deep conjecture of Greenberg that if $E/\\mathbb{Q}$ is an elliptic curve with good ordinary reduction at $p$, and $E[p]$ is irreducible as a Galois module, then the Selmer group of $E$ over $\\mathbb{Q}^{\\text{cyc}}$ has $\\mu$-invariant zero.","We prove new cases of Greenberg's conjecture for some elliptic curves of analytic rank $0$. The proof involves studying the $p$-adic $L$-function of $E$. The crucial input is a new technique using the Rankin-Selberg method."],"url":"http://arxiv.org/abs/2405.05871v1","category":"math.NT"}
{"created":"2024-05-09 15:13:24","title":"K-stable valuations and Calabi-Yau metrics on affine spherical varieties","abstract":"After providing an explicit K-stability condition for a $\\mathbb{Q}$-Gorenstein log spherical cone, we prove the existence and uniqueness of an equivariant K-stable degeneration of the cone, and deduce uniqueness of the asymptotic cone of a given complete $K$-invariant Calabi-Yau metric in the trivial class of an affine $G$-spherical manifold, $K$ being the maximal compact subgroup of $G$.   Next, we prove that the valuation induced by $K$-invariant Calabi-Yau metrics on affine $G$-spherical manifolds is in fact $G$-invariant. As an application, we point out an affine smoothing of a Calabi-Yau cone that does not admit any $K$-invariant Calabi-Yau metrics asymptotic to the cone. Another corollary is that on $\\mathbb{C}^3$, there are no other complete Calabi-Yau metrics with maximal volume growth and spherical symmetry other than the standard flat metric and the Li-Conlon-Rochon-Sz\\'ekelyhidi metrics with horospherical asymptotic cone. This answers the question whether there is a nontrivial asymptotic cone with smooth cross section on $\\mathbb{C}^{3}$ raised by Conlon-Rochon when the symmetry is spherical.","sentences":["After providing an explicit K-stability condition for a $\\mathbb{Q}$-Gorenstein log spherical cone, we prove the existence and uniqueness of an equivariant K-stable degeneration of the cone, and deduce uniqueness of the asymptotic cone of a given complete $K$-invariant Calabi-Yau metric in the trivial class of an affine $G$-spherical manifold, $K$ being the maximal compact subgroup of $G$.   Next, we prove that the valuation induced by $K$-invariant Calabi-Yau metrics on affine $G$-spherical manifolds is in fact $G$-invariant.","As an application, we point out an affine smoothing of a Calabi-Yau cone that does not admit any $K$-invariant Calabi-Yau metrics asymptotic to the cone.","Another corollary is that on $\\mathbb{C}^3$, there are no other complete Calabi-Yau metrics with maximal volume growth and spherical symmetry other than the standard flat metric and the Li-Conlon-Rochon-Sz\\'ekelyhidi metrics with horospherical asymptotic cone.","This answers the question whether there is a nontrivial asymptotic cone with smooth cross section on $\\mathbb{C}^{3}$ raised by Conlon-Rochon when the symmetry is spherical."],"url":"http://arxiv.org/abs/2405.05833v1","category":"math.AG"}
{"created":"2024-05-09 14:11:26","title":"Determining the population of Large Meteoroids in Major Meteor Showers","abstract":"We have estimated the largest meteoroids present in major meteor showers from observations conducted between 2019-2022 by the Geostationary Lightning Mapper (GLM) instrument on the GOES-R satellites. Our integrated time area products for the Leonids, Perseids and eta Aquariids are of order 5 x 10^10 km2 hours. We compute photometric masses for shower fireballs using the approach of Vojacek et al. 2022 to correct from narrow-band GLM luminosity to bolometric luminosity and apply the luminous efficiency relation of Ceplecha and McCrosky 1976 at high speeds. Between 2019 and 2022, the showers definitely observed by GLM were the Leonids, Perseids, and eta Aquariids, with probable detections of the Orionids and Taurids. We find the largest meteoroids to be of order 7 kg for the Leonids, 3 kg for the Perseids, and 3 kg for the eta Aquariids, corresponding to meteoroids of ~ 0.2m diameter. The Orionids and Taurids had maximum meteoroid masses of 4 kg and 150 kg respectively. The Leonids and eta Aquariids are well fit by a single power-law with differential mass exponent, s, of 2.08 +/- 0.08 and 2.00 +/- 0.09 over the mass range 10^-7 < m < 1 kg. All showers had maximum meteoroid masses compatible with Whipple gas-drag ejection, with the exception of the Perseids which have much larger meteoroids than expected which is also consistent with observations from ground based instruments. This may reflect preferential ejection in narrow jets or possibly some form of mantle erosion/release in the past for the parent comet, 109P/Swift-Tuttle.","sentences":["We have estimated the largest meteoroids present in major meteor showers from observations conducted between 2019-2022 by the Geostationary Lightning Mapper (GLM) instrument on the GOES-R satellites.","Our integrated time area products for the Leonids, Perseids and eta Aquariids are of order 5 x 10^10 km2 hours.","We compute photometric masses for shower fireballs using the approach of Vojacek et al. 2022 to correct from narrow-band GLM luminosity to bolometric luminosity and apply the luminous efficiency relation of Ceplecha and McCrosky 1976 at high speeds.","Between 2019 and 2022, the showers definitely observed by GLM were the Leonids, Perseids, and eta Aquariids, with probable detections of the Orionids and Taurids.","We find the largest meteoroids to be of order 7 kg for the Leonids, 3 kg for the Perseids, and 3 kg for the eta Aquariids, corresponding to meteoroids of ~ 0.2m diameter.","The Orionids and Taurids had maximum meteoroid masses of 4 kg and 150 kg respectively.","The Leonids and eta Aquariids are well fit by a single power-law with differential mass exponent, s, of 2.08 +/- 0.08 and 2.00 +/- 0.09 over the mass range 10^-7 < m < 1 kg.","All showers had maximum meteoroid masses compatible with Whipple gas-drag ejection, with the exception of the Perseids which have much larger meteoroids than expected which is also consistent with observations from ground based instruments.","This may reflect preferential ejection in narrow jets or possibly some form of mantle erosion/release in the past for the parent comet, 109P/Swift-Tuttle."],"url":"http://arxiv.org/abs/2405.05788v1","category":"astro-ph.EP"}
{"created":"2024-05-09 13:53:39","title":"On the dimension of the singular set of perimeter minimizers in spaces with a two-sided Ricci curvature bound","abstract":"We show that the Hausdorff dimension of the singular set of perimeter minimizers in non-collapsed Ricci limit spaces with a two-sided Ricci curvature bound is at most $N-5$, where $N$ is the dimension of the ambient space. The estimate is sharp.","sentences":["We show that the Hausdorff dimension of the singular set of perimeter minimizers in non-collapsed Ricci limit spaces with a two-sided Ricci curvature bound is at most $N-5$, where $N$ is the dimension of the ambient space.","The estimate is sharp."],"url":"http://arxiv.org/abs/2405.05775v1","category":"math.DG"}
{"created":"2024-05-09 13:49:45","title":"Monoidal bicategories, differential linear logic, and analytic functors","abstract":"We develop further the theory of monoidal bicategories by introducing and studying bicategorical counterparts of the notions of a linear explonential comonad, as considered in the study of linear logic, and of a codereliction transformation, introduced to study differential linear logic via differential categories. As an application, we extend the differential calculus of Joyal's analytic functors to analytic functors between presheaf categories, just as ordinary calculus extends from a single variable to many variables.","sentences":["We develop further the theory of monoidal bicategories by introducing and studying bicategorical counterparts of the notions of a linear explonential comonad, as considered in the study of linear logic, and of a codereliction transformation, introduced to study differential linear logic via differential categories.","As an application, we extend the differential calculus of Joyal's analytic functors to analytic functors between presheaf categories, just as ordinary calculus extends from a single variable to many variables."],"url":"http://arxiv.org/abs/2405.05774v1","category":"math.CT"}
{"created":"2024-05-09 11:45:47","title":"Matter Asymmetries in the $Z_N$ Dark matter -companion Models","abstract":"A class of $Z_{N\\geq 3}$-symmetric WIMP dark matter models that are characterized by the semi-annihilation into the companion of dark matter has been proposed in ref.~\\cite{Guo:2021rre}, providing a mechanism to evade the stringent direct detection constraint. In this work, we point out that such models naturally provide the three Sakharov elements necessary for dark matter asymmetry, and moreover this asymmetry can be transferred to the visible sector with a proper link to the leptonic or quark sector. In our minimal $Z_3$ example, the migration to the leptonic sector is via the asymmetric companion decay into neutrinos, and the lepton asymmetry can be further transferred to the quark sector. The CP violation parameter in the model is suppressed in the limit of static annihilation of dark matter, and the lift from thermal motion has been studied for the first time. A preliminary numerical analysis based on the Boltzmann equations shows that both correct relic density of dark matter and baryon asymmetry can be accommodated.","sentences":["A class of $Z_{N\\geq 3}$-symmetric WIMP dark matter models that are characterized by the semi-annihilation into the companion of dark matter has been proposed in ref.~\\cite{Guo:2021rre}, providing a mechanism to evade the stringent direct detection constraint.","In this work, we point out that such models naturally provide the three Sakharov elements necessary for dark matter asymmetry, and moreover this asymmetry can be transferred to the visible sector with a proper link to the leptonic or quark sector.","In our minimal $Z_3$ example, the migration to the leptonic sector is via the asymmetric companion decay into neutrinos, and the lepton asymmetry can be further transferred to the quark sector.","The CP violation parameter in the model is suppressed in the limit of static annihilation of dark matter, and the lift from thermal motion has been studied for the first time.","A preliminary numerical analysis based on the Boltzmann equations shows that both correct relic density of dark matter and baryon asymmetry can be accommodated."],"url":"http://arxiv.org/abs/2405.05694v1","category":"hep-ph"}
{"created":"2024-05-09 11:18:55","title":"NLO corrections to $J/\u03c8+c+\\bar{c}$ photoproduction","abstract":"Based on the factorization framework of nonrelativistic quantum chromodynamics, we study the associated $J/\\psi+c+\\bar{c}$ photoproduction process at next-to-leading order in $\\alpha_s$ and leading order in the velocity expansion. The total cross section and differential cross section in $p_T^2$, $W$ and $z$ are presented. The results indicate that the next-to-leading order corrections are substantial, and testable in experiment.","sentences":["Based on the factorization framework of nonrelativistic quantum chromodynamics, we study the associated $J/\\psi+c+\\bar{c}$ photoproduction process at next-to-leading order in $\\alpha_s$ and leading order in the velocity expansion.","The total cross section and differential cross section in $p_T^2$, $W$ and $z$ are presented.","The results indicate that the next-to-leading order corrections are substantial, and testable in experiment."],"url":"http://arxiv.org/abs/2405.05683v1","category":"hep-ph"}
{"created":"2024-05-09 10:35:47","title":"Uniqueness, non-degeneracy, and exact multiplicity of positive solutions for superlinear elliptic problems","abstract":"In this paper, we focus our attention on the positive solutions to second-order nonlinear ordinary differential equations of the form $u''+q(t)g(u)=0$, where $q$ is a sign-changing weight and $g$ is a superlinear function. We exploit the classical shooting approach and the comparison theorem to present non-degeneracy and exact multiplicity results for positive solutions. This completes the multiplicity results obtained by Feltrin and Zanolin. Numerical examples and some related open problems are also discussed.","sentences":["In this paper, we focus our attention on the positive solutions to second-order nonlinear ordinary differential equations of the form $u''+q(t)g(u)=0$, where $q$ is a sign-changing weight and $g$ is a superlinear function.","We exploit the classical shooting approach and the comparison theorem to present non-degeneracy and exact multiplicity results for positive solutions.","This completes the multiplicity results obtained by Feltrin and Zanolin.","Numerical examples and some related open problems are also discussed."],"url":"http://arxiv.org/abs/2405.05664v1","category":"math.AP"}
{"created":"2024-05-09 10:01:05","title":"On supposed oscillations of differential cross sections in pp-scattering at sqrt{s} = 13 TeV","abstract":"The question of possible existence of oscillations in the region of the diffraction peak in pp-scattering is considered in detail at sqrt{s}=13 TeV. It is shown that within the framework of the available experimental data published by the TOTEM and ALFA/ATLAS collaborations, raising the question of searching for such a subtle effect looks premature.","sentences":["The question of possible existence of oscillations in the region of the diffraction peak in pp-scattering is considered in detail at sqrt{s}=13 TeV. It is shown that within the framework of the available experimental data published by the TOTEM and ALFA/ATLAS collaborations, raising the question of searching for such a subtle effect looks premature."],"url":"http://arxiv.org/abs/2405.05653v1","category":"hep-ph"}
{"created":"2024-05-09 09:48:39","title":"Evaluation of the X-ray SOI pixel detector with the on-chip ADC","abstract":"XRPIX is the monolithic X-ray SOI (silicon-on-insulator) pixel detector, which has a time resolution better than 10 $\\rm{\\mu}$s as well as a high detection efficiency for X-rays above 10 keV. XRPIX is planned to be installed on future X-ray satellites. To mount on satellites, it is essential that the ADC (analog-to-digital converter) be implemented on the detector because such peripheral circuits must be as compact as possible to achieve a large imaging area in the limited space in satellites. Thus, we developed a new XRPIX device with the on-chip ADC, and evaluated its performances. As the results, the integral non-linearity was evaluated to be 6 LSB (least significant bit), equivalent to 36~eV. The differential non-linearity was less than 0.7 LSB, and input noise from the on-chip ADC was 5~$\\rm{e^{-}}$. Also, we evaluated end-to-end performance including the sensor part as well as the on-chip ADC. As the results, energy resolution at 5.9~keV was 294 $\\rm{\\pm}$ 4~eV in full-width at half maximum for the best pixel.","sentences":["XRPIX is the monolithic X-ray SOI (silicon-on-insulator) pixel detector, which has a time resolution better than 10 $\\rm{\\mu}$s as well as a high detection efficiency for X-rays above 10 keV. XRPIX is planned to be installed on future X-ray satellites.","To mount on satellites, it is essential that the ADC (analog-to-digital converter) be implemented on the detector because such peripheral circuits must be as compact as possible to achieve a large imaging area in the limited space in satellites.","Thus, we developed a new XRPIX device with the on-chip ADC, and evaluated its performances.","As the results, the integral non-linearity was evaluated to be 6 LSB (least significant bit), equivalent to 36~eV. The differential non-linearity was less than 0.7 LSB, and input noise from the on-chip ADC was 5~$\\rm{e^{-}}$. Also, we evaluated end-to-end performance including the sensor part as well as the on-chip ADC.","As the results, energy resolution at 5.9~keV was 294 $\\rm{\\pm}$ 4~eV in full-width at half maximum for the best pixel."],"url":"http://arxiv.org/abs/2405.05649v1","category":"astro-ph.IM"}
{"created":"2024-05-09 09:13:32","title":"HarmonyBatch: Batching multi-SLO DNN Inference with Heterogeneous Serverless Functions","abstract":"Deep Neural Network (DNN) inference on serverless functions is gaining prominence due to its potential for substantial budget savings. Existing works on serverless DNN inference solely optimize batching requests from one application with a single Service Level Objective (SLO) on CPU functions. However, production serverless DNN inference traces indicate that the request arrival rate of applications is surprisingly low, which inevitably causes a long batching time and SLO violations. Hence, there is an urgent need for batching multiple DNN inference requests with diverse SLOs (i.e., multi-SLO DNN inference) in serverless platforms. Moreover, the potential performance and cost benefits of deploying heterogeneous (i.e., CPU and GPU) functions for DNN inference have received scant attention.   In this paper, we present HarmonyBatch, a cost-efficient resource provisioning framework designed to achieve predictable performance for multi-SLO DNN inference with heterogeneous serverless functions. Specifically, we construct an analytical performance and cost model of DNN inference on both CPU and GPU functions, by explicitly considering the GPU time-slicing scheduling mechanism and request arrival rate distribution. Based on such a model, we devise a two-stage merging strategy in HarmonyBatch to judiciously batch the multi-SLO DNN inference requests into application groups. It aims to minimize the budget of function provisioning for each application group while guaranteeing diverse performance SLOs of inference applications. We have implemented a prototype of HarmonyBatch on Alibaba Cloud Function Compute. Extensive prototype experiments with representative DNN inference workloads demonstrate that HarmonyBatch can provide predictable performance to serverless DNN inference workloads while reducing the monetary cost by up to 82.9% compared to the state-of-the-art methods.","sentences":["Deep Neural Network (DNN) inference on serverless functions is gaining prominence due to its potential for substantial budget savings.","Existing works on serverless DNN inference solely optimize batching requests from one application with a single Service Level Objective (SLO) on CPU functions.","However, production serverless DNN inference traces indicate that the request arrival rate of applications is surprisingly low, which inevitably causes a long batching time and SLO violations.","Hence, there is an urgent need for batching multiple DNN inference requests with diverse SLOs (i.e., multi-SLO DNN inference) in serverless platforms.","Moreover, the potential performance and cost benefits of deploying heterogeneous (i.e., CPU and GPU) functions for DNN inference have received scant attention.   ","In this paper, we present HarmonyBatch, a cost-efficient resource provisioning framework designed to achieve predictable performance for multi-SLO DNN inference with heterogeneous serverless functions.","Specifically, we construct an analytical performance and cost model of DNN inference on both CPU and GPU functions, by explicitly considering the GPU time-slicing scheduling mechanism and request arrival rate distribution.","Based on such a model, we devise a two-stage merging strategy in HarmonyBatch to judiciously batch the multi-SLO DNN inference requests into application groups.","It aims to minimize the budget of function provisioning for each application group while guaranteeing diverse performance SLOs of inference applications.","We have implemented a prototype of HarmonyBatch on Alibaba Cloud Function Compute.","Extensive prototype experiments with representative DNN inference workloads demonstrate that HarmonyBatch can provide predictable performance to serverless DNN inference workloads while reducing the monetary cost by up to 82.9% compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.05633v1","category":"cs.DC"}
{"created":"2024-05-09 17:59:56","title":"Learned harmonic mean estimation of the Bayesian evidence with normalizing flows","abstract":"We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.","sentences":["We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison.","Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach.","The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator.","In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator.","Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously.","We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions.","We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates.","The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available."],"url":"http://arxiv.org/abs/2405.05969v1","category":"astro-ph.IM"}
{"created":"2024-05-09 17:16:54","title":"Theoretical Guarantees of Data Augmented Last Layer Retraining Methods","abstract":"Ensuring fair predictions across many distinct subpopulations in the training data can be prohibitive for large models. Recently, simple linear last layer retraining strategies, in combination with data augmentation methods such as upweighting, downsampling and mixup, have been shown to achieve state-of-the-art performance for worst-group accuracy, which quantifies accuracy for the least prevalent subpopulation. For linear last layer retraining and the abovementioned augmentations, we present the optimal worst-group accuracy when modeling the distribution of the latent representations (input to the last layer) as Gaussian for each subpopulation. We evaluate and verify our results for both synthetic and large publicly available datasets.","sentences":["Ensuring fair predictions across many distinct subpopulations in the training data can be prohibitive for large models.","Recently, simple linear last layer retraining strategies, in combination with data augmentation methods such as upweighting, downsampling and mixup, have been shown to achieve state-of-the-art performance for worst-group accuracy, which quantifies accuracy for the least prevalent subpopulation.","For linear last layer retraining and the abovementioned augmentations, we present the optimal worst-group accuracy when modeling the distribution of the latent representations (input to the last layer) as Gaussian for each subpopulation.","We evaluate and verify our results for both synthetic and large publicly available datasets."],"url":"http://arxiv.org/abs/2405.05934v1","category":"cs.LG"}
{"created":"2024-05-09 16:44:35","title":"An RNN-policy gradient approach for quantum architecture search","abstract":"Variational quantum circuits are one of the promising ways to exploit the advantages of quantum computing in the noisy intermediate-scale quantum technology era. The design of the quantum circuit architecture might greatly affect the performance capability of the quantum algorithms. The quantum architecture search is the process of automatically designing quantum circuit architecture, aiming at finding the optimal quantum circuit composition architecture by the algorithm for a given task, so that the algorithm can learn to design the circuit architecture. Compared to manual design, quantum architecture search algorithms are more effective in finding quantum circuits with better performance capabilities. In this paper, based on the deep reinforcement learning, we propose an approach for quantum circuit architecture search. The sampling of the circuit architecture is learnt through reinforcement learning based controller. Layer-based search is also used to accelerate the computational efficiency of the search algorithm. Applying to data classification tasks we show that the method can search for quantum circuit architectures with better accuracies. Moreover, the circuit has a smaller number of quantum gates and parameters.","sentences":["Variational quantum circuits are one of the promising ways to exploit the advantages of quantum computing in the noisy intermediate-scale quantum technology era.","The design of the quantum circuit architecture might greatly affect the performance capability of the quantum algorithms.","The quantum architecture search is the process of automatically designing quantum circuit architecture, aiming at finding the optimal quantum circuit composition architecture by the algorithm for a given task, so that the algorithm can learn to design the circuit architecture.","Compared to manual design, quantum architecture search algorithms are more effective in finding quantum circuits with better performance capabilities.","In this paper, based on the deep reinforcement learning, we propose an approach for quantum circuit architecture search.","The sampling of the circuit architecture is learnt through reinforcement learning based controller.","Layer-based search is also used to accelerate the computational efficiency of the search algorithm.","Applying to data classification tasks we show that the method can search for quantum circuit architectures with better accuracies.","Moreover, the circuit has a smaller number of quantum gates and parameters."],"url":"http://arxiv.org/abs/2405.05892v1","category":"quant-ph"}
{"created":"2024-05-09 13:09:15","title":"Designing Social Learning","abstract":"This paper studies strategic communication in the context of social learning. Product reviews are used by consumers to learn product quality, but in order to write a review, a consumer must be convinced to purchase the item first. When reviewers care about welfare of future consumers, this leads to a conflict: a reviewer today wants the future consumers to purchase the item even when this comes at a loss to them, so that more information is revealed for the consumers that come after. We show that due to this conflict, communication via reviews is inevitably noisy, regardless of whether reviewers can commit to a communication strategy or have to resort to cheap talk. The optimal communication mechanism involves truthful communication of extreme experiences and pools the moderate experiences together.","sentences":["This paper studies strategic communication in the context of social learning.","Product reviews are used by consumers to learn product quality, but in order to write a review, a consumer must be convinced to purchase the item first.","When reviewers care about welfare of future consumers, this leads to a conflict: a reviewer today wants the future consumers to purchase the item even when this comes at a loss to them, so that more information is revealed for the consumers that come after.","We show that due to this conflict, communication via reviews is inevitably noisy, regardless of whether reviewers can commit to a communication strategy or have to resort to cheap talk.","The optimal communication mechanism involves truthful communication of extreme experiences and pools the moderate experiences together."],"url":"http://arxiv.org/abs/2405.05744v1","category":"econ.TH"}
{"created":"2024-05-09 12:50:16","title":"Batched Stochastic Bandit for Nondegenerate Functions","abstract":"This paper studies batched bandit learning problems for nondegenerate functions. We introduce an algorithm that solves the batched bandit problem for nondegenerate functions near-optimally. More specifically, we introduce an algorithm, called Geometric Narrowing (GN), whose regret bound is of order $\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$. In addition, GN only needs $\\mathcal{O} (\\log \\log T)$ batches to achieve this regret. We also provide lower bound analysis for this problem. More specifically, we prove that over some (compact) doubling metric space of doubling dimension $d$: 1. For any policy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of order ${\\Omega} ( A_-^d \\sqrt{T})$; 2. No policy can achieve a regret of order $ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log \\log T ) $ rounds of communications. Our lower bound analysis shows that the GN algorithm achieves near optimal regret with minimal number of batches.","sentences":["This paper studies batched bandit learning problems for nondegenerate functions.","We introduce an algorithm that solves the batched bandit problem for nondegenerate functions near-optimally.","More specifically, we introduce an algorithm, called Geometric Narrowing (GN), whose regret bound is of order $\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$.","In addition, GN only needs $\\mathcal{O} (\\log \\log T)$ batches to achieve this regret.","We also provide lower bound analysis for this problem.","More specifically, we prove that over some (compact) doubling metric space of doubling dimension $d$: 1.","For any policy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of order ${\\Omega} ( A_-^d \\sqrt{T})$; 2.","No policy can achieve a regret of order $ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log \\log T )","$ rounds of communications.","Our lower bound analysis shows that the GN algorithm achieves near optimal regret with minimal number of batches."],"url":"http://arxiv.org/abs/2405.05733v1","category":"stat.ML"}
{"created":"2024-05-09 12:03:38","title":"Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution","abstract":"Many tasks related to Computational Social Science and Web Content Analysis involve classifying pieces of text based on the claims they contain. State-of-the-art approaches usually involve fine-tuning models on large annotated datasets, which are costly to produce. In light of this, we propose and release a qualitative and versatile few-shot learning methodology as a common paradigm for any claim-based textual classification task. This methodology involves defining the classes as arbitrarily sophisticated taxonomies of claims, and using Natural Language Inference models to obtain the textual entailment between these and a corpus of interest. The performance of these models is then boosted by annotating a minimal sample of data points, dynamically sampled using the well-established statistical heuristic of Probabilistic Bisection. We illustrate this methodology in the context of three tasks: climate change contrarianism detection, topic/stance classification and depression-relates symptoms detection. This approach rivals traditional pre-train/fine-tune approaches while drastically reducing the need for data annotation.","sentences":["Many tasks related to Computational Social Science and Web Content Analysis involve classifying pieces of text based on the claims they contain.","State-of-the-art approaches usually involve fine-tuning models on large annotated datasets, which are costly to produce.","In light of this, we propose and release a qualitative and versatile few-shot learning methodology as a common paradigm for any claim-based textual classification task.","This methodology involves defining the classes as arbitrarily sophisticated taxonomies of claims, and using Natural Language Inference models to obtain the textual entailment between these and a corpus of interest.","The performance of these models is then boosted by annotating a minimal sample of data points, dynamically sampled using the well-established statistical heuristic of Probabilistic Bisection.","We illustrate this methodology in the context of three tasks: climate change contrarianism detection, topic/stance classification and depression-relates symptoms detection.","This approach rivals traditional pre-train/fine-tune approaches while drastically reducing the need for data annotation."],"url":"http://arxiv.org/abs/2405.05705v1","category":"cs.CL"}
{"created":"2024-05-09 10:58:40","title":"Imprecise Multi-Armed Bandits","abstract":"We introduce a novel multi-armed bandit framework, where each arm is associated with a fixed unknown credal set over the space of outcomes (which can be richer than just the reward). The arm-to-credal-set correspondence comes from a known class of hypotheses. We then define a notion of regret corresponding to the lower prevision defined by these credal sets. Equivalently, the setting can be regarded as a two-player zero-sum game, where, on each round, the agent chooses an arm and the adversary chooses the distribution over outcomes from a set of options associated with this arm. The regret is defined with respect to the value of game. For certain natural hypothesis classes, loosely analgous to stochastic linear bandits (which are a special case of the resulting setting), we propose an algorithm and prove a corresponding upper bound on regret. We also prove lower bounds on regret for particular special cases.","sentences":["We introduce a novel multi-armed bandit framework, where each arm is associated with a fixed unknown credal set over the space of outcomes (which can be richer than just the reward).","The arm-to-credal-set correspondence comes from a known class of hypotheses.","We then define a notion of regret corresponding to the lower prevision defined by these credal sets.","Equivalently, the setting can be regarded as a two-player zero-sum game, where, on each round, the agent chooses an arm and the adversary chooses the distribution over outcomes from a set of options associated with this arm.","The regret is defined with respect to the value of game.","For certain natural hypothesis classes, loosely analgous to stochastic linear bandits (which are a special case of the resulting setting), we propose an algorithm and prove a corresponding upper bound on regret.","We also prove lower bounds on regret for particular special cases."],"url":"http://arxiv.org/abs/2405.05673v1","category":"cs.LG"}
{"created":"2024-05-09 10:37:33","title":"SubGDiff: A Subgraph Diffusion Model to Improve Molecular Representation Learning","abstract":"Molecular representation learning has shown great success in advancing AI-based drug discovery. The core of many recent works is based on the fact that the 3D geometric structure of molecules provides essential information about their physical and chemical characteristics. Recently, denoising diffusion probabilistic models have achieved impressive performance in 3D molecular representation learning. However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the molecular substructures. This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information within the diffusion process. We propose a novel diffusion model termed SubGDiff for involving the molecular subgraph information in diffusion. Specifically, SubGDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) k-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network. Experimentally, extensive downstream tasks demonstrate the superior performance of our approach. The code is available at https://github.com/youjibiying/SubGDiff.","sentences":["Molecular representation learning has shown great success in advancing AI-based drug discovery.","The core of many recent works is based on the fact that the 3D geometric structure of molecules provides essential information about their physical and chemical characteristics.","Recently, denoising diffusion probabilistic models have achieved impressive performance in 3D molecular representation learning.","However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the molecular substructures.","This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information within the diffusion process.","We propose a novel diffusion model termed SubGDiff for involving the molecular subgraph information in diffusion.","Specifically, SubGDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) k-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network.","Experimentally, extensive downstream tasks demonstrate the superior performance of our approach.","The code is available at https://github.com/youjibiying/SubGDiff."],"url":"http://arxiv.org/abs/2405.05665v1","category":"cs.LG"}
{"created":"2024-05-09 08:23:20","title":"Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning","abstract":"Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models. In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information. Motivated by the finding that Feed-Forward Network (FFN) of language models acts as \"key-value memory\", we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection. Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the finetuned VL models and surpasses the performance of previous PEFT methods. Code: https://github.com/JieShibo/MemVP","sentences":["Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT).","However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models.","In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information.","Motivated by the finding that Feed-Forward Network (FFN) of language models acts as \"key-value memory\", we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection.","Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the finetuned VL models and surpasses the performance of previous PEFT methods.","Code: https://github.com/JieShibo/MemVP"],"url":"http://arxiv.org/abs/2405.05615v1","category":"cs.CV"}
{"created":"2024-05-09 16:31:54","title":"Quantum entanglement enables single-shot trajectory sensing for weakly interacting particles","abstract":"Sensors for mapping the trajectory of an incoming particle find important utility in experimental high energy physics and searches for dark matter. For a quantum sensing protocol that uses projective measurements on a multi-qubit sensor array to infer the trajectory of an incident particle, we show that entanglement can dramatically reduce the particle-sensor interaction strength $\\theta$ required for perfect trajectory discrimination. Within an interval of $\\theta$ above this reduced threshold, any unentangled sensor requires $\\Theta(\\log(1/\\epsilon))$ repetitions of the protocol to estimate the particle trajectory with $\\epsilon$ error probability, whereas an entangled sensor can succeed with zero error in a single shot.","sentences":["Sensors for mapping the trajectory of an incoming particle find important utility in experimental high energy physics and searches for dark matter.","For a quantum sensing protocol that uses projective measurements on a multi-qubit sensor array to infer the trajectory of an incident particle, we show that entanglement can dramatically reduce the particle-sensor interaction strength $\\theta$ required for perfect trajectory discrimination.","Within an interval of $\\theta$ above this reduced threshold, any unentangled sensor requires $\\Theta(\\log(1/\\epsilon))$ repetitions of the protocol to estimate the particle trajectory with $\\epsilon$ error probability, whereas an entangled sensor can succeed with zero error in a single shot."],"url":"http://arxiv.org/abs/2405.05888v1","category":"quant-ph"}
{"created":"2024-05-09 14:12:41","title":"High-Performance Privacy-Preserving Matrix Completion for Trajectory Recovery","abstract":"Matrix completion has important applications in trajectory recovery and mobile social networks. However, sending raw data containing personal, sensitive information to cloud computing nodes may lead to privacy exposure issue.The privacy-preserving matrix completion is a useful approach to perform matrix completion while preserving privacy. In this paper, we propose a high-performance method for privacy-preserving matrix completion. First,we use a lightweight encryption scheme to encrypt the raw data and then perform matrix completion using alternating direction method of multipliers (ADMM). Then,the complemented matrix is decrypted and compared with the original matrix to calculate the error. This method has faster speed with higher accuracy. The results of numerical experiments reveal that the proposed method is faster than other algorithms.","sentences":["Matrix completion has important applications in trajectory recovery and mobile social networks.","However, sending raw data containing personal, sensitive information to cloud computing nodes may lead to privacy exposure issue.","The privacy-preserving matrix completion is a useful approach to perform matrix completion while preserving privacy.","In this paper, we propose a high-performance method for privacy-preserving matrix completion.","First,we use a lightweight encryption scheme to encrypt the raw data and then perform matrix completion using alternating direction method of multipliers (ADMM).","Then,the complemented matrix is decrypted and compared with the original matrix to calculate the error.","This method has faster speed with higher accuracy.","The results of numerical experiments reveal that the proposed method is faster than other algorithms."],"url":"http://arxiv.org/abs/2405.05789v1","category":"cs.CR"}
{"created":"2024-05-09 13:32:42","title":"Spin parameter optimization for spin-polarized extended tight-binding methods","abstract":"We present an optimization strategy for atom-specific spin-polarization constants within the spin-polarized GFN2-xTB framework, aiming to enhance the accuracy of molecular simulations. We compare a sequential and global optimization of spin parameters for hydrogen, carbon, nitrogen, oxygen, and fluorine. Sensitivity analysis using Sobol indices guides the identification of the most influential parameters for a given reference dataset, allowing for a nuanced understanding of their impact on diverse molecular properties. In the case of the W4-11 dataset, substantial error reduction was achieved, demonstrating the potential of the optimization. Transferability of the optimized spin-polarization constants over different properties, however, is limited, as we demonstrate by applying the optimized parameters on a set of singlet-triplet gaps in carbenes. Further studies on ionization potentials and electron affinities highlight some inherent limitations of current extended tight-binding methods that can not be resolved by simple parameter optimization. We conclude that the significantly improved accuracy strongly encourages the present re-optimization of the spin-polarization constants, whereas the limited transferability motivates a property-specific optimization strategy.","sentences":["We present an optimization strategy for atom-specific spin-polarization constants within the spin-polarized GFN2-xTB framework, aiming to enhance the accuracy of molecular simulations.","We compare a sequential and global optimization of spin parameters for hydrogen, carbon, nitrogen, oxygen, and fluorine.","Sensitivity analysis using Sobol indices guides the identification of the most influential parameters for a given reference dataset, allowing for a nuanced understanding of their impact on diverse molecular properties.","In the case of the W4-11 dataset, substantial error reduction was achieved, demonstrating the potential of the optimization.","Transferability of the optimized spin-polarization constants over different properties, however, is limited, as we demonstrate by applying the optimized parameters on a set of singlet-triplet gaps in carbenes.","Further studies on ionization potentials and electron affinities highlight some inherent limitations of current extended tight-binding methods that can not be resolved by simple parameter optimization.","We conclude that the significantly improved accuracy strongly encourages the present re-optimization of the spin-polarization constants, whereas the limited transferability motivates a property-specific optimization strategy."],"url":"http://arxiv.org/abs/2405.05761v1","category":"physics.chem-ph"}
{"created":"2024-05-09 12:37:22","title":"Minimum Time Escape from a Circular Region of a Dubins Car","abstract":"A turn-constrained evader strives to escape a circular region in minimum time.","sentences":["A turn-constrained evader strives to escape a circular region in minimum time."],"url":"http://arxiv.org/abs/2405.05725v1","category":"math.OC"}
{"created":"2024-05-09 11:45:33","title":"Development and optimization of large-scale integration of 2D material in memristors","abstract":"Two-dimensional (2D) materials like transition metal dichalcogenides (TMD) have proved to be serious candidates to replace silicon in several technologies with enhanced performances. In this respect, the two remaining challenges are the wafer scale growth of TMDs and their integration into operational devices using clean room compatible processes. In this work, two different CMOS-compatible protocols are developed for the fabrication of MoS$_2$-based memristors, and the resulting performances are compared. The quality of MoS$_2$ at each stage of the process is characterized by Raman spectroscopy and x-ray photoemission spectroscopy. In the first protocol, the structure of MoS$_2$ is preserved during transfer and patterning processes. However, a polymer layer with a minimum thickness of 3 nm remains at the surface of MoS$_2$ limiting the electrical switching performances. In the second protocol, the contamination layer is completely removed resulting in improved electrical switching performances and reproducibility. Based on physico-chemical and electrical results, the switching mechanism is discussed in terms of conduction through grain boundaries.","sentences":["Two-dimensional (2D) materials like transition metal dichalcogenides (TMD) have proved to be serious candidates to replace silicon in several technologies with enhanced performances.","In this respect, the two remaining challenges are the wafer scale growth of TMDs and their integration into operational devices using clean room compatible processes.","In this work, two different CMOS-compatible protocols are developed for the fabrication of MoS$_2$-based memristors, and the resulting performances are compared.","The quality of MoS$_2$ at each stage of the process is characterized by Raman spectroscopy and x-ray photoemission spectroscopy.","In the first protocol, the structure of MoS$_2$ is preserved during transfer and patterning processes.","However, a polymer layer with a minimum thickness of 3 nm remains at the surface of MoS$_2$ limiting the electrical switching performances.","In the second protocol, the contamination layer is completely removed resulting in improved electrical switching performances and reproducibility.","Based on physico-chemical and electrical results, the switching mechanism is discussed in terms of conduction through grain boundaries."],"url":"http://arxiv.org/abs/2405.05693v1","category":"physics.app-ph"}
{"created":"2024-05-09 10:06:30","title":"Consistent Empirical Bayes estimation of the mean of a mixing distribution without identifiability assumption. With applications to treatment of non-response","abstract":"{\\bf Abstract}   Consider a Non-Parametric Empirical Bayes (NPEB) setup. We observe $Y_i, \\sim f(y|\\theta_i)$, $\\theta_i \\in \\Theta$ independent, where $\\theta_i \\sim G$ are independent $i=1,...,n$. The mixing distribution $G$ is unknown $G \\in \\{G\\}$ with no parametric assumptions about the class $\\{G \\}$. The common NPEB task is to estimate $\\theta_i, \\; i=1,...,n$. Conditions that imply 'optimality' of such NPEB estimators typically require identifiability of $G$ based on $Y_1,...,Y_n$. We consider the task of estimating $E_G \\theta$. We show that `often' consistent estimation of $E_G \\theta$ is implied without identifiability.   We motivate the later task, especially in setups with non-response and missing data. We demonstrate consistency in simulations.","sentences":["{\\bf Abstract}   Consider a Non-Parametric Empirical Bayes (NPEB) setup.","We observe $Y_i, \\sim f(y|\\theta_i)$, $\\theta_i \\in \\Theta$ independent, where $\\theta_i \\sim G$ are independent $i=1,...,n$.","The mixing distribution $G$ is unknown $G \\in \\{G\\}$ with no parametric assumptions about the class $\\{G \\}$. The common NPEB task is to estimate $\\theta_i, \\; i=1,...,n$. Conditions that imply 'optimality' of such NPEB estimators typically require identifiability of $G$ based on $Y_1,...,Y_n$.","We consider the task of estimating $E_G \\theta$. We show that `often' consistent estimation of $E_G \\theta$ is implied without identifiability.   ","We motivate the later task, especially in setups with non-response and missing data.","We demonstrate consistency in simulations."],"url":"http://arxiv.org/abs/2405.05656v1","category":"math.ST"}
{"created":"2024-05-09 17:37:20","title":"CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts","abstract":"Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side. Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference. CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage. Auxiliary losses are used to ensure a balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets. The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo.","sentences":["Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks.","However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side.","Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo.","CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference.","CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage.","Auxiliary losses are used to ensure a balanced loading of experts.","CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets.","The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo."],"url":"http://arxiv.org/abs/2405.05949v1","category":"cs.CV"}
{"created":"2024-05-09 17:13:41","title":"In-medium changes of nucleon cross sections tested in neutrino-induced reactions","abstract":"Historically studied in the context of heavy-ion collisions, the extent to which free nucleon-nucleon forces are modified in-medium remains undetermined by these data sets. Therefore, we investigate the impact of NN in-medium modifications on neutrino-nucleus cross section predictions using the GiBUU transport model. We find that including an in-medium lowering of the NN cross section and density dependence on $\\Delta$ excitation improves agreement with MicroBooNE neutrino-argon scattering data. This is observed for both proton and neutral pion spectra in charged-current muon neutrino and neutral-current single pion production datasets. The impact of collision broadening of the $\\Delta$ resonance is also investigated.","sentences":["Historically studied in the context of heavy-ion collisions, the extent to which free nucleon-nucleon forces are modified in-medium remains undetermined by these data sets.","Therefore, we investigate the impact of NN in-medium modifications on neutrino-nucleus cross section predictions using the GiBUU transport model.","We find that including an in-medium lowering of the NN cross section and density dependence on $\\Delta$ excitation improves agreement with MicroBooNE neutrino-argon scattering data.","This is observed for both proton and neutral pion spectra in charged-current muon neutrino and neutral-current single pion production datasets.","The impact of collision broadening of the $\\Delta$ resonance is also investigated."],"url":"http://arxiv.org/abs/2405.05921v1","category":"hep-ex"}
{"created":"2024-05-09 15:45:06","title":"Non-perturbative determination of the $N_f=2+1$ QCD sphaleron rate","abstract":"The strong sphaleron rate, i.e., the rate of real time QCD topological transitions, is a key phenomenological quantity, playing a fundamental role in several physical contexts. In heavy-ion collisions, a non-vanishing rate can lead to the so-called Chiral Magnetic Effect. In early-Universe cosmology, instead, it can be related to the rate of thermal production of QCD axions. In this talk, we present the first reliable fully non-perturbative computation of the strong sphaleron rate in $N_f=2+1$ QCD at the physical point by means of lattice simulations, in a range of temperatures going from 200 MeV to 600 MeV. Our strategy is based on the inversion of lattice correlators via a recently-proposed modified version of the Backus-Gilbert method.","sentences":["The strong sphaleron rate, i.e., the rate of real time QCD topological transitions, is a key phenomenological quantity, playing a fundamental role in several physical contexts.","In heavy-ion collisions, a non-vanishing rate can lead to the so-called Chiral Magnetic Effect.","In early-Universe cosmology, instead, it can be related to the rate of thermal production of QCD axions.","In this talk, we present the first reliable fully non-perturbative computation of the strong sphaleron rate in $N_f=2+1$ QCD at the physical point by means of lattice simulations, in a range of temperatures going from 200 MeV to 600 MeV.","Our strategy is based on the inversion of lattice correlators via a recently-proposed modified version of the Backus-Gilbert method."],"url":"http://arxiv.org/abs/2405.05857v1","category":"hep-lat"}
{"created":"2024-05-09 15:34:56","title":"Extreme high lattice-misfit superalloys with regular cubic L12 particles and excellent creep resistance","abstract":"In novel Co and CoNi based superalloys, the creep resistance is limited at high temperatures due to low lattice misfit and solvus temperature. In this study, we combined the advantages of Co-Ti (high lattice misfit and solvus temperature) and Co-Al based superalloys (cuboidal precipitates) by using Ti to substitute Al in alloys of Co-30Ni-(12.5-x)Al-xTi-2.5Mo-2.5W (at.%) composition. With high Ti content, the alloys obtained extreme high lattice misfit (bigger than 1.3 %) and solvus temperature (bigger than 1150 degree). During aging at 900 degree, alloys with high Ti/Al ratio exhibited a lower gamma prime precipitate coarsening rate resulting from their lower gamma prime and gamma interfacial energy and higher lattice misfit. In addition, high Ti/Al ratio brought higher gamma prime volume fraction and excellent mechanical properties, such as higher yield stress and better creep resistance. However, at high temperature of 1100 degree, the cubic gamma prime phase was decomposed into deleterious Eta phase with D024 structure if the Ti/Al ratio exceeded 1. Based on this, we outreached new alloys design with a high content of Cr and Ta and appropriate Ti/Al ratio, i.e., Ti/Al ratio is smaller than 1. The newly designed alloys still have high solvus temperature (bigger than 1200 degree) and exhibit high lattice misfit (bigger than 1.2 %) as Co-12Ti (at.%) superalloys but more regular cubic gamma prime precipitates and significantly better creep resistance than superalloys Co-9Al-9W and Co-9Al-9W-2Ti at 850 and 950 degree. Nevertheless, compared with creep resistance of Ni based superalloys, our newly designed alloys still need to be further improved, especially in the 1000 and 1050 degree range.","sentences":["In novel Co and CoNi based superalloys, the creep resistance is limited at high temperatures due to low lattice misfit and solvus temperature.","In this study, we combined the advantages of Co-Ti (high lattice misfit and solvus temperature) and Co-Al based superalloys (cuboidal precipitates) by using Ti to substitute Al in alloys of Co-30Ni-(12.5-x)Al-xTi-2.5Mo-2.5W (at.%) composition.","With high Ti content, the alloys obtained extreme high lattice misfit (bigger than 1.3 %) and solvus temperature (bigger than 1150 degree).","During aging at 900 degree, alloys with high Ti/Al ratio exhibited a lower gamma prime precipitate coarsening rate resulting from their lower gamma prime and gamma interfacial energy and higher lattice misfit.","In addition, high Ti/Al ratio brought higher gamma prime volume fraction and excellent mechanical properties, such as higher yield stress and better creep resistance.","However, at high temperature of 1100 degree, the cubic gamma prime phase was decomposed into deleterious Eta phase with D024 structure if the Ti/Al ratio exceeded 1.","Based on this, we outreached new alloys design with a high content of Cr and Ta and appropriate Ti/Al ratio, i.e., Ti/Al ratio is smaller than 1.","The newly designed alloys still have high solvus temperature (bigger than 1200 degree) and exhibit high lattice misfit (bigger than 1.2 %) as Co-12Ti (at.%) superalloys but more regular cubic gamma prime precipitates and significantly better creep resistance than superalloys Co-9Al-9W and Co-9Al-9W-2Ti at 850 and 950 degree.","Nevertheless, compared with creep resistance of Ni based superalloys, our newly designed alloys still need to be further improved, especially in the 1000 and 1050 degree range."],"url":"http://arxiv.org/abs/2405.05851v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:34:23","title":"Dissociative recombination and vibrational excitation of BF$^{+}$ in low energy electron collisions","abstract":"The latest molecular data - potential energy curves and Rydberg-valence interactions - characterising the super-excited electronic states of BF are reviewed in order to provide the input for the study of their fragmentation dynamics. Starting from this input, the main paths and mechanisms of BF$^+$ dissociative recombination and vibrational excitation are analysed. Their cross sections are computed for the first time using a method based on the multichannel quantum defect theory (MQDT), and Maxwellian rate-coefficients are calculated and displayed in ready-to-be-used format for low temperature plasma kinetics simulations.","sentences":["The latest molecular data - potential energy curves and Rydberg-valence interactions - characterising the super-excited electronic states of BF are reviewed in order to provide the input for the study of their fragmentation dynamics.","Starting from this input, the main paths and mechanisms of BF$^+$ dissociative recombination and vibrational excitation are analysed.","Their cross sections are computed for the first time using a method based on the multichannel quantum defect theory (MQDT), and Maxwellian rate-coefficients are calculated and displayed in ready-to-be-used format for low temperature plasma kinetics simulations."],"url":"http://arxiv.org/abs/2405.05850v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 15:27:05","title":"Alleviating $H_0$ and $S_8$ Tensions Simultaneously in K-essence Cosmology","abstract":"The present work begins by examining the early-Universe inflationary epoch of a special K-essence model, which incorporates a linear coupling term between the scalar field potential and the canonical Lagrangian. For the power law potential, we both numerically and analytically prove that the inflationary parameters such as the spectral index and tensor-to-scalar ratio are compatible with the recent BICEP/Keck observations. Continuing this work, our analysis based on comparing early-Universe observations with late-Universe measurements indicates that the tension on the Hubble parameter $H_0$ and the growth of structure parameter $S_8$ can be alleviated simultaneously. More precisely, compared to the standard $\\Lambda$CDM model, our model can reduce $H_0$ tension to roughly $2.2 \\sigma$ and the $S_8$ discrepancy diminishes to $0.82\\sigma$.","sentences":["The present work begins by examining the early-Universe inflationary epoch of a special K-essence model, which incorporates a linear coupling term between the scalar field potential and the canonical Lagrangian.","For the power law potential, we both numerically and analytically prove that the inflationary parameters such as the spectral index and tensor-to-scalar ratio are compatible with the recent BICEP/Keck observations.","Continuing this work, our analysis based on comparing early-Universe observations with late-Universe measurements indicates that the tension on the Hubble parameter $H_0$ and the growth of structure parameter $S_8$ can be alleviated simultaneously.","More precisely, compared to the standard $\\Lambda$CDM model, our model can reduce $H_0$ tension to roughly $2.2 \\sigma$ and the $S_8$ discrepancy diminishes to $0.82\\sigma$."],"url":"http://arxiv.org/abs/2405.05843v1","category":"astro-ph.CO"}
{"created":"2024-05-09 14:56:54","title":"X-ray signatures of galactic outflows into the circumgalactic medium","abstract":"We present a set of controlled hydrodynamical simulations to study the effects of strong galactic outflows on the density and temperature structures, and associated X-ray signatures, of extra-planar and circumgalactic gas. We consider three initial state models, isothermal, isentropic, and rotating cooling-flow, for the hot circumgalactic medium (CGM) into which the outflows are driven. The energy sources are either stellar winds and supernovae, or active galactic nuclei. We consider energy injection rates in the range $10^{40} < \\dot{E}_{\\rm inj} <10^{44.5}$ erg s$^{-1}$, and compute the time-dependent soft X-ray (0.5-2 keV) surface brightness. For $\\dot{E}_{\\rm inj} \\gtrsim 10^{41} - 10^{42}$ erg s$^{-1}$, with the exact threshold depending on the initial CGM state, the X-ray response is dominated by dense hot gas in the forward shock that eventually fades into the CGM as a sound wave. The shock surrounds an inner hot bubble leading to a radial flattening of the X-ray surface brightness. For lower energy injection rates, the X-ray surface brightness of the initial CGM state is almost unaffected. We present analytic approximations for the outflow shock propagation and the associated X-ray emissions.","sentences":["We present a set of controlled hydrodynamical simulations to study the effects of strong galactic outflows on the density and temperature structures, and associated X-ray signatures, of extra-planar and circumgalactic gas.","We consider three initial state models, isothermal, isentropic, and rotating cooling-flow, for the hot circumgalactic medium (CGM) into which the outflows are driven.","The energy sources are either stellar winds and supernovae, or active galactic nuclei.","We consider energy injection rates in the range $10^{40} < \\dot{E}_{\\rm inj} <10^{44.5}$ erg s$^{-1}$, and compute the time-dependent soft X-ray (0.5-2 keV) surface brightness.","For $\\dot{E}_{\\rm inj} \\gtrsim 10^{41} - 10^{42}$ erg s$^{-1}$, with the exact threshold depending on the initial CGM state, the X-ray response is dominated by dense hot gas in the forward shock that eventually fades into the CGM as a sound wave.","The shock surrounds an inner hot bubble leading to a radial flattening of the X-ray surface brightness.","For lower energy injection rates, the X-ray surface brightness of the initial CGM state is almost unaffected.","We present analytic approximations for the outflow shock propagation and the associated X-ray emissions."],"url":"http://arxiv.org/abs/2405.05819v1","category":"astro-ph.GA"}
{"created":"2024-05-09 14:31:28","title":"On the fine structure of the solutions to nonlinear thin two-membrane problems in 2D","abstract":"We prove a structure theorem for the solutions of nonlinear thin two-membrane problems in dimension two. Using the theory of quasi-conformal maps, we show that the difference of the sheets is topologically equivalent to a solution of the linear thin obstacle problem, thus inheriting its free boundary structure. More precisely, we show that even in the nonlinear case the branching points can only occur in finite number. We apply our methods to one-phase free boundaries approaching a fixed analytic boundary and to the solutions of a one-sided two-phase Bernoulli problem.","sentences":["We prove a structure theorem for the solutions of nonlinear thin two-membrane problems in dimension two.","Using the theory of quasi-conformal maps, we show that the difference of the sheets is topologically equivalent to a solution of the linear thin obstacle problem, thus inheriting its free boundary structure.","More precisely, we show that even in the nonlinear case the branching points can only occur in finite number.","We apply our methods to one-phase free boundaries approaching a fixed analytic boundary and to the solutions of a one-sided two-phase Bernoulli problem."],"url":"http://arxiv.org/abs/2405.05799v1","category":"math.AP"}
{"created":"2024-05-09 14:30:49","title":"A PAge-like Unified Dark Fluid Model","abstract":"The unified dark fluid model unifies dark matter and dark energy into a single component, providing an alternative and more concise framework for interpreting cosmological observations. We introduce a PAge-like Unified Dark Fluid (PUDF) model based on the PAge approximation (Huang 2020), which is parameterized by the age of the universe and an $\\eta$ parameter indicating the deviation from Einstein-De Sitter Universe. The PUDF model shares many similar features of the standard Lambda cold dark matter ($\\Lambda$CDM) model and can effectively describe the large-scale structure formation and late-time cosmic acceleration. We constrain the PUDF model with the Planck 2018 cosmic microwave background anisotropies, baryon acoustic oscillation measurements including those from the most recent DESI 2024, the Pantheon+ sample of Type Ia supernovae, and the Cosmic Chronometers compilation. Although the PUDF performs well in fitting all the cosmological datasets, the joint analysis of the data still favors the $\\Lambda$CDM model over the PUDF model, according to the Bayesian evidence of model comparison.","sentences":["The unified dark fluid model unifies dark matter and dark energy into a single component, providing an alternative and more concise framework for interpreting cosmological observations.","We introduce a PAge-like Unified Dark Fluid (PUDF) model based on the PAge approximation (Huang 2020), which is parameterized by the age of the universe and an $\\eta$ parameter indicating the deviation from Einstein-De Sitter Universe.","The PUDF model shares many similar features of the standard Lambda cold dark matter ($\\Lambda$CDM) model and can effectively describe the large-scale structure formation and late-time cosmic acceleration.","We constrain the PUDF model with the Planck 2018 cosmic microwave background anisotropies, baryon acoustic oscillation measurements including those from the most recent DESI 2024, the Pantheon+ sample of Type Ia supernovae, and the Cosmic Chronometers compilation.","Although the PUDF performs well in fitting all the cosmological datasets, the joint analysis of the data still favors the $\\Lambda$CDM model over the PUDF model, according to the Bayesian evidence of model comparison."],"url":"http://arxiv.org/abs/2405.05798v1","category":"astro-ph.CO"}
{"created":"2024-05-09 13:54:15","title":"Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions","abstract":"Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said. While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial. In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures. We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans. The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences.","sentences":["Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said.","While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial.","In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures.","We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans.","The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences."],"url":"http://arxiv.org/abs/2405.05776v1","category":"cs.CL"}
{"created":"2024-05-09 13:12:47","title":"3D bulk field theories for 2D non-unitary N=1 supersymmetric minimal models","abstract":"We propose bulk 3D N=4 rank-0 superconformal field theories, which are related to 2D N=1 supersymmetric minimal models, SM(2, ...) and SM(3, ...), via recently discovered non-unitary bulk-boundary correspondence. The correspondence relates a 3D N=4 rank-0 superconformal field theory to 2D chiral rational conformal field theories. A topologically twisted theory of the rank-0 SCFT supports the rational chiral algebra at the boundary upon a proper choice of boundary condition. We test the proposal by checking several non-trivial dictionaries of the correspondence.","sentences":["We propose bulk 3D N=4 rank-0 superconformal field theories, which are related to 2D N=1 supersymmetric minimal models, SM(2, ...) and SM(3, ...), via recently discovered non-unitary bulk-boundary correspondence.","The correspondence relates a 3D N=4 rank-0 superconformal field theory to 2D chiral rational conformal field theories.","A topologically twisted theory of the rank-0 SCFT supports the rational chiral algebra at the boundary upon a proper choice of boundary condition.","We test the proposal by checking several non-trivial dictionaries of the correspondence."],"url":"http://arxiv.org/abs/2405.05746v1","category":"hep-th"}
{"created":"2024-05-09 13:06:23","title":"Development, Characterization and Production of a novel Water-based Liquid Scintillator based on the Surfactant TRITON X-100","abstract":"Water-based Liquid Scintillator (WbLS) is a novel detector medium for particle physics experiments. Applications range from the use as hybrid Cherenkov/scintillation target in low-energy and accelerator neutrino experiments to large-volume neutron vetoes for dark matter detectors. Here, we present a novel WbLS featuring new components (the surfactant Triton-X and vitamin C for long-term stability), a new production recipe, and a thorough characterization of its properties. Moreover, based on neutron scattering data we are able to demonstrate that the pulse shape discrimination capabilities of this particular LS are comparable to fully-organic LAB based scintillators.","sentences":["Water-based Liquid Scintillator (WbLS) is a novel detector medium for particle physics experiments.","Applications range from the use as hybrid Cherenkov/scintillation target in low-energy and accelerator neutrino experiments to large-volume neutron vetoes for dark matter detectors.","Here, we present a novel WbLS featuring new components (the surfactant Triton-X and vitamin C for long-term stability), a new production recipe, and a thorough characterization of its properties.","Moreover, based on neutron scattering data we are able to demonstrate that the pulse shape discrimination capabilities of this particular LS are comparable to fully-organic LAB based scintillators."],"url":"http://arxiv.org/abs/2405.05743v1","category":"hep-ex"}
{"created":"2024-05-09 12:42:17","title":"Incoherent Fermionic Dark Matter Absorption with Nucleon Fermi Motion","abstract":"We investigate the incoherent regime of the fermionic dark matter absorption by nuclei using the relativistic Fermi gas model and nuclear form factors. With the momentum transfer being roughly equal to the dark matter mass $m_\\chi$, the incoherent regime contributes significantly to the absorption process for $m_\\chi \\gtrsim 100$ MeV with a spin-independent operator and for even smaller mass with a spin-dependent one. We also compare the situations for various target nuclei ($^{131}$Xe, $^{72}$Ge, $^{40}$Ar, $^{20}$Ne and $^4$He) that are typically used in the dark matter direct detection. A heavier nucleus actually has the advantage of probing the incoherent scattering of the fermionic absorption dark matter. Observing both the coherent and incoherent contributions would be an important justification of the fermionic dark matter absorption.","sentences":["We investigate the incoherent regime of the fermionic dark matter absorption by nuclei using the relativistic Fermi gas model and nuclear form factors.","With the momentum transfer being roughly equal to the dark matter mass $m_\\chi$, the incoherent regime contributes significantly to the absorption process for $m_\\chi \\gtrsim 100$ MeV with a spin-independent operator and for even smaller mass with a spin-dependent one.","We also compare the situations for various target nuclei ($^{131}$Xe, $^{72}$Ge, $^{40}$Ar, $^{20}$Ne and $^4$He) that are typically used in the dark matter direct detection.","A heavier nucleus actually has the advantage of probing the incoherent scattering of the fermionic absorption dark matter.","Observing both the coherent and incoherent contributions would be an important justification of the fermionic dark matter absorption."],"url":"http://arxiv.org/abs/2405.05728v1","category":"hep-ph"}
{"created":"2024-05-09 12:33:40","title":"Prolegomena to the Bestiary","abstract":"``Calabi-Yau Manifolds: a Bestiary for Physicists'' by Tristan Hubsch in 1992 was a classic that served to introduce algebraic geometry to physicists when the first string theory revolution of 1984 - 94 brought, inter alia, the subject of Calabi-Yau manifolds to the staple of high-energy theorists. We are fortunate that a substantially expanded and updated new edition of the Bestiary will shortly appear. This brief note will serve as an afterword to the much anticipated volume.","sentences":["``Calabi-Yau Manifolds: a Bestiary for Physicists'' by Tristan Hubsch in 1992 was a classic that served to introduce algebraic geometry to physicists when the first string theory revolution of 1984 - 94 brought, inter alia, the subject of Calabi-Yau manifolds to the staple of high-energy theorists.","We are fortunate that a substantially expanded and updated new edition of the Bestiary will shortly appear.","This brief note will serve as an afterword to the much anticipated volume."],"url":"http://arxiv.org/abs/2405.05720v1","category":"math.HO"}
{"created":"2024-05-09 11:55:01","title":"Parallelizing Air Shower Simulation for Background Characterization in IceCube","abstract":"The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the Geographic South Pole. For every observed neutrino event, there are over $10^6$ background events caused by cosmic ray air shower muons. In order to properly separate signal from background, it is necessary to produce Monte Carlo simulations of these air showers. Although to-date, IceCube has produced large quantities of background simulation, these studies still remain statistics limited. The first stage of simulation requires heavy CPU usage while the second stage requires heavy GPU usage. Processing both of these stages on the same node will result in an underutilized GPU but using different nodes will encounter bandwidth bottlenecks. Furthermore, due to the power-law energy spectrum of cosmic rays, the memory footprint of the detector response often exceeded the limit in unpredictable ways. This proceeding presents new client-server code which parallelizes the first stage onto multiple CPUs on the same node and then passes it on to the GPU for photon propagation. This results in GPU utilization of greater than 90% as well as more predictable memory usage and an overall factor of 20 improvement in speed over previous techniques.","sentences":["The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the Geographic South Pole.","For every observed neutrino event, there are over $10^6$ background events caused by cosmic ray air shower muons.","In order to properly separate signal from background, it is necessary to produce Monte Carlo simulations of these air showers.","Although to-date, IceCube has produced large quantities of background simulation, these studies still remain statistics limited.","The first stage of simulation requires heavy CPU usage while the second stage requires heavy GPU usage.","Processing both of these stages on the same node will result in an underutilized GPU but using different nodes will encounter bandwidth bottlenecks.","Furthermore, due to the power-law energy spectrum of cosmic rays, the memory footprint of the detector response often exceeded the limit in unpredictable ways.","This proceeding presents new client-server code which parallelizes the first stage onto multiple CPUs on the same node and then passes it on to the GPU for photon propagation.","This results in GPU utilization of greater than 90% as well as more predictable memory usage and an overall factor of 20 improvement in speed over previous techniques."],"url":"http://arxiv.org/abs/2405.05700v1","category":"astro-ph.HE"}
{"created":"2024-05-09 11:54:35","title":"MCMC inversions of the internal rotation of Kepler subgiants","abstract":"The measurement of the internal rotation of post-main sequence stars using data from space-based photometry missions has demonstrated the need for an efficient angular momentum transport in stellar interiors. So far, no clear solution has emerged and explaining the observed trends remain a challenge for stellar modellers. We aim at constraining both the shape of the internal rotation profile of six Kepler subgiants studied in details in 2014 and the properties of the missing angular momentum transport process acting in stellar interiors from MCMC inversions of the internal rotation. We apply a new MCMC inversion technique to existing Kepler subgiant targets and test various shapes of the internal rotation profile of all six original subgiants observed in 2014. We also constrain the limitations on the number of free parameters that can be used in the MCMC inversion, showing the limitations in the amount of information in the seismic data. First, we show that large-scale fossil magnetic fields are not able to explain the internal rotation of subgiants, similarly to what was determined from detailed studies of Kepler red giants. We are also able to constrain the location of the transition in the internal rotation profile for the most evolved stars in the available set of subgiants. We find that some of them exhibit a transition located close to the border of the helium core while one clearly does not. We conclude that it might be possible that various processes might be at play to explain our observations, but that revealing the physical nature of the angular momentum process will require a consistent detailed modelling of all subgiants available, particularly the least evolved. In addition, increasing the number of stars for which such inferences are possible (e.g. with the future PLATO mission) is paramount given the key role they play in validating transport process candidates.","sentences":["The measurement of the internal rotation of post-main sequence stars using data from space-based photometry missions has demonstrated the need for an efficient angular momentum transport in stellar interiors.","So far, no clear solution has emerged and explaining the observed trends remain a challenge for stellar modellers.","We aim at constraining both the shape of the internal rotation profile of six Kepler subgiants studied in details in 2014 and the properties of the missing angular momentum transport process acting in stellar interiors from MCMC inversions of the internal rotation.","We apply a new MCMC inversion technique to existing Kepler subgiant targets and test various shapes of the internal rotation profile of all six original subgiants observed in 2014.","We also constrain the limitations on the number of free parameters that can be used in the MCMC inversion, showing the limitations in the amount of information in the seismic data.","First, we show that large-scale fossil magnetic fields are not able to explain the internal rotation of subgiants, similarly to what was determined from detailed studies of Kepler red giants.","We are also able to constrain the location of the transition in the internal rotation profile for the most evolved stars in the available set of subgiants.","We find that some of them exhibit a transition located close to the border of the helium core while one clearly does not.","We conclude that it might be possible that various processes might be at play to explain our observations, but that revealing the physical nature of the angular momentum process will require a consistent detailed modelling of all subgiants available, particularly the least evolved.","In addition, increasing the number of stars for which such inferences are possible (e.g. with the future PLATO mission) is paramount given the key role they play in validating transport process candidates."],"url":"http://arxiv.org/abs/2405.05699v1","category":"astro-ph.SR"}
{"created":"2024-05-09 09:16:42","title":"High-Frequency Stock Market Order Transitions during the US-China Trade War 2018: A Discrete-Time Markov Chain Analysis","abstract":"Statistical analysis of high-frequency stock market order transaction data is conducted to understand order transition dynamics. We employ a first-order time-homogeneous discrete-time Markov chain model to the sequence of orders of stocks belonging to six different sectors during the USA-China trade war of 2018. The Markov property of the order sequence is validated by the Chi-square test. We estimate the transition probability matrix of the sequence using maximum likelihood estimation. From the heat-map of these matrices, we found the presence of active participation by different types of traders during high volatility days. On such days, these traders place limit orders primarily with the intention of deleting the majority of them to influence the market. These findings are supported by high stationary distribution and low mean recurrence values of add and delete orders. Further, we found similar spectral gap and entropy rate values, which indicates that similar trading strategies are employed on both high and low volatility days during the trade war. Among all the sectors considered in this study, we observe that there is a recurring pattern of full execution orders in Finance & Banking sector. This shows that the banking stocks are resilient during the trade war. Hence, this study may be useful in understanding stock market order dynamics and devise trading strategies accordingly on high and low volatility days during extreme macroeconomic events.","sentences":["Statistical analysis of high-frequency stock market order transaction data is conducted to understand order transition dynamics.","We employ a first-order time-homogeneous discrete-time Markov chain model to the sequence of orders of stocks belonging to six different sectors during the USA-China trade war of 2018.","The Markov property of the order sequence is validated by the Chi-square test.","We estimate the transition probability matrix of the sequence using maximum likelihood estimation.","From the heat-map of these matrices, we found the presence of active participation by different types of traders during high volatility days.","On such days, these traders place limit orders primarily with the intention of deleting the majority of them to influence the market.","These findings are supported by high stationary distribution and low mean recurrence values of add and delete orders.","Further, we found similar spectral gap and entropy rate values, which indicates that similar trading strategies are employed on both high and low volatility days during the trade war.","Among all the sectors considered in this study, we observe that there is a recurring pattern of full execution orders in Finance & Banking sector.","This shows that the banking stocks are resilient during the trade war.","Hence, this study may be useful in understanding stock market order dynamics and devise trading strategies accordingly on high and low volatility days during extreme macroeconomic events."],"url":"http://arxiv.org/abs/2405.05634v1","category":"q-fin.ST"}
{"created":"2024-05-09 08:31:00","title":"Semiclassical solution of black hole information paradox","abstract":"We resolve black hole information paradox within semiclassical gravity, in a manner that does not depend on details of unknown quantum gravity. Our crucial insight is that outgoing Hawking particles are physical only at distances much larger than the Schwarzschild radius, so they are created far from the horizon and entangled with degrees of freedom at smaller distances. The later degrees of freedom can be understood as quasi-classical coherent states, implying that Hawking radiation is accompanied by additional radiation similar to classical radiation by which the black hole loses hair during the classical gravitational collapse. The two kinds of radiation are entangled, which resolves black hole information paradox.","sentences":["We resolve black hole information paradox within semiclassical gravity, in a manner that does not depend on details of unknown quantum gravity.","Our crucial insight is that outgoing Hawking particles are physical only at distances much larger than the Schwarzschild radius, so they are created far from the horizon and entangled with degrees of freedom at smaller distances.","The later degrees of freedom can be understood as quasi-classical coherent states, implying that Hawking radiation is accompanied by additional radiation similar to classical radiation by which the black hole loses hair during the classical gravitational collapse.","The two kinds of radiation are entangled, which resolves black hole information paradox."],"url":"http://arxiv.org/abs/2405.05617v1","category":"hep-th"}
