{"created":"2024-04-22 17:59:57","title":"AutoAD III: The Prequel -- Back to the Pixels","abstract":"Generating Audio Description (AD) for movies is a challenging task that requires fine-grained visual understanding and an awareness of the characters and their names. Currently, visual language models for AD generation are limited by a lack of suitable training data, and also their evaluation is hampered by using performance measures not specialized to the AD domain. In this paper, we make three contributions: (i) We propose two approaches for constructing AD datasets with aligned video data, and build training and evaluation datasets using these. These datasets will be publicly released; (ii) We develop a Q-former-based architecture which ingests raw video and generates AD, using frozen pre-trained visual encoders and large language models; and (iii) We provide new evaluation metrics to benchmark AD quality that are well-matched to human performance. Taken together, we improve the state of the art on AD generation.","sentences":["Generating Audio Description (AD) for movies is a challenging task that requires fine-grained visual understanding and an awareness of the characters and their names.","Currently, visual language models for AD generation are limited by a lack of suitable training data, and also their evaluation is hampered by using performance measures not specialized to the AD domain.","In this paper, we make three contributions: (i) We propose two approaches for constructing AD datasets with aligned video data, and build training and evaluation datasets using these.","These datasets will be publicly released; (ii) We develop a Q-former-based architecture which ingests raw video and generates AD, using frozen pre-trained visual encoders and large language models; and (iii) We provide new evaluation metrics to benchmark AD quality that are well-matched to human performance.","Taken together, we improve the state of the art on AD generation."],"url":"http://arxiv.org/abs/2404.14412v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:36","title":"CrossScore: Towards Multi-View Image Evaluation and Scoring","abstract":"We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references. By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable. Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references.","sentences":["We introduce a novel cross-reference image quality assessment method that effectively fills the gap in the image assessment landscape, complementing the array of established evaluation schemes -- ranging from full-reference metrics like SSIM, no-reference metrics such as NIQE, to general-reference metrics including FID, and Multi-modal-reference metrics, e.g., CLIPScore.","Utilising a neural network with the cross-attention mechanism and a unique data collection pipeline from NVS optimisation, our method enables accurate image quality assessment without requiring ground truth references.","By comparing a query image against multiple views of the same scene, our method addresses the limitations of existing metrics in novel view synthesis (NVS) and similar tasks where direct reference images are unavailable.","Experimental results show that our method is closely correlated to the full-reference metric SSIM, while not requiring ground truth references."],"url":"http://arxiv.org/abs/2404.14409v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:29","title":"SpaceByte: Towards Deleting Tokenization from Large Language Modeling","abstract":"Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.","sentences":["Tokenization is widely used in large language models because it significantly improves performance.","However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity.","To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling.","SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers.","We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries.","Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures."],"url":"http://arxiv.org/abs/2404.14408v1","category":"cs.CL"}
{"created":"2024-04-22 17:59:07","title":"Learning H-Infinity Locomotion Control","abstract":"Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances. However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities. In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint. In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration. To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces. Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances. We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot. The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice. On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains. All code, checkpoints, and real-world deployment guidance will be made public.","sentences":["Stable locomotion in precipitous environments is an essential capability of quadruped robots, demanding the ability to resist various external disturbances.","However, recent learning-based policies only use basic domain randomization to improve the robustness of learned policies, which cannot guarantee that the robot has adequate disturbance resistance capabilities.","In this paper, we propose to model the learning process as an adversarial interaction between the actor and a newly introduced disturber and ensure their optimization with $H_{\\infty}$ constraint.","In contrast to the actor that maximizes the discounted overall reward, the disturber is responsible for generating effective external forces and is optimized by maximizing the error between the task reward and its oracle, i.e., \"cost\" in each iteration.","To keep joint optimization between the actor and the disturber stable, our $H_{\\infty}$ constraint mandates the bound of ratio between the cost to the intensity of the external forces.","Through reciprocal interaction throughout the training phase, the actor can acquire the capability to navigate increasingly complex physical disturbances.","We verify the robustness of our approach on quadrupedal locomotion tasks with Unitree Aliengo robot, and also a more challenging task with Unitree A1 robot, where the quadruped is expected to perform locomotion merely on its hind legs as if it is a bipedal robot.","The simulated quantitative results show improvement against baselines, demonstrating the effectiveness of the method and each design choice.","On the other hand, real-robot experiments qualitatively exhibit how robust the policy is when interfering with various disturbances on various terrains, including stairs, high platforms, slopes, and slippery terrains.","All code, checkpoints, and real-world deployment guidance will be made public."],"url":"http://arxiv.org/abs/2404.14405v1","category":"cs.RO"}
{"created":"2024-04-22 17:58:36","title":"GeoDiffuser: Geometry-Based Image Editing with Diffusion Models","abstract":"The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information.","sentences":["The success of image generative models has enabled us to build methods that can edit images based on text or other user input.","However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits.","We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method.","Our key insight is to view image editing operations as geometric transformations.","We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations.","Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows.","It also inpaints disoccluded parts of the image where the object was originally located.","Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing.","GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal.","We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods.","Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information."],"url":"http://arxiv.org/abs/2404.14403v1","category":"cs.CV"}
{"created":"2024-04-22 17:58:34","title":"A Python GPU-accelerated solver for the Gross-Pitaevskii equation and applications to many-body cavity QED","abstract":"TorchGPE is a general-purpose Python package developed for solving the Gross-Pitaevskii equation (GPE). This solver is designed to integrate wave functions across a spectrum of linear and non-linear potentials. A distinctive aspect of TorchGPE is its modular approach, which allows the incorporation of arbitrary self-consistent and time-dependent potentials, e.g., those relevant in many-body cavity QED models. The package employs a symmetric split-step Fourier propagation method, effective in both real and imaginary time. In our work, we demonstrate a significant improvement in computational efficiency by leveraging GPU computing capabilities. With the integration of the latter technology, TorchGPE achieves a substantial speed-up with respect to conventional CPU-based methods, greatly expanding the scope and potential of research in this field.","sentences":["TorchGPE is a general-purpose Python package developed for solving the Gross-Pitaevskii equation (GPE).","This solver is designed to integrate wave functions across a spectrum of linear and non-linear potentials.","A distinctive aspect of TorchGPE is its modular approach, which allows the incorporation of arbitrary self-consistent and time-dependent potentials, e.g., those relevant in many-body cavity QED models.","The package employs a symmetric split-step Fourier propagation method, effective in both real and imaginary time.","In our work, we demonstrate a significant improvement in computational efficiency by leveraging GPU computing capabilities.","With the integration of the latter technology, TorchGPE achieves a substantial speed-up with respect to conventional CPU-based methods, greatly expanding the scope and potential of research in this field."],"url":"http://arxiv.org/abs/2404.14401v1","category":"physics.comp-ph"}
{"created":"2024-04-22 17:56:09","title":"SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation","abstract":"The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X.","sentences":["The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA.","However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data.","In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation.","We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks.","Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning.","We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications.","The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X."],"url":"http://arxiv.org/abs/2404.14396v1","category":"cs.CV"}
{"created":"2024-04-22 17:55:56","title":"PARAMANU-GANITA: Language Model with Mathematical Capabilities","abstract":"In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics. The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus. We evaluate our model on both perplexity metric and GSM8k mathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively. Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively. The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters. Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation. Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end. In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model.","sentences":["In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto Regressive (AR) decoder based language model on mathematics.","The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus.","We evaluate our model on both perplexity metric and GSM8k mathematical benchmark.","Paramanu-Ganita despite being 35 times smaller than 7B LLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2 7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and math specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0% points in GSM8k test accuracy metric respectively.","Paramanu-Ganita also outperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8% points, LLaMa-1","33B by 3.8% points and Vicuna 13B by 11.8% points respectively.","The large significant margin improvement in performance of our math model over the existing LLMs signifies that reasoning capabilities of language model are just not restricted to LLMs with humongous number of parameters.","Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent.","Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation.","Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end.","In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model."],"url":"http://arxiv.org/abs/2404.14395v1","category":"cs.CL"}
{"created":"2024-04-22 17:55:11","title":"A Multimodal Automated Interpretability Agent","abstract":"This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.","sentences":["This paper describes MAIA, a Multimodal Automated Interpretability Agent.","MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery.","It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior.","These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results.","Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior.","We evaluate applications of MAIA to computer vision models.","We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images.","Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters.","We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified."],"url":"http://arxiv.org/abs/2404.14394v1","category":"cs.AI"}
{"created":"2024-04-22 17:55:02","title":"Towards testing the general bounce cosmology with the CMB B-mode auto-bispectrum","abstract":"It has been shown that a three-point correlation function of tensor perturbations from a bounce model in general relativity with a minimally-coupled scalar field is highly suppressed, and the resultant three-point function of cosmic microwave background (CMB) B-mode polarizations is too small to be detected by CMB experiments. On the other hand, bounce models in a more general class with a non-minimal derivative coupling between a scalar field and gravity can predict the three-point correlation function of the tensor perturbations without any suppression, the amplitude of which is allowed to be much larger than that in general relativity. In this paper, we evaluate the three-point function of the B-mode polarizations from the general bounce cosmology with the non-minimal coupling and show that a signal-to-noise ratio of the B-mode auto-bispectrum in the general class can reach unity for $\\ell_{\\rm max}\\geq9$ and increase up to $5.39$ for $\\ell_{\\rm max}=100$ in the full-sky case. We also discuss the possibility to test the general class of bounce models by upcoming CMB experiments.","sentences":["It has been shown that a three-point correlation function of tensor perturbations from a bounce model in general relativity with a minimally-coupled scalar field is highly suppressed, and the resultant three-point function of cosmic microwave background (CMB) B-mode polarizations is too small to be detected by CMB experiments.","On the other hand, bounce models in a more general class with a non-minimal derivative coupling between a scalar field and gravity can predict the three-point correlation function of the tensor perturbations without any suppression, the amplitude of which is allowed to be much larger than that in general relativity.","In this paper, we evaluate the three-point function of the B-mode polarizations from the general bounce cosmology with the non-minimal coupling and show that a signal-to-noise ratio of the B-mode auto-bispectrum in the general class can reach unity for $\\ell_{\\rm max}\\geq9$ and increase up to $5.39$ for $\\ell_{\\rm max}=100$ in the full-sky case.","We also discuss the possibility to test the general class of bounce models by upcoming CMB experiments."],"url":"http://arxiv.org/abs/2404.14393v1","category":"astro-ph.CO"}
{"created":"2024-04-22 17:43:23","title":"A Survey on Self-Evolution of Large Language Models","abstract":"Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs.","sentences":["Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications.","However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase.","To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing.","This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence.","In this work, we present a comprehensive survey of self-evolution approaches in LLMs.","We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation.","Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module.","Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs."],"url":"http://arxiv.org/abs/2404.14387v1","category":"cs.CL"}
{"created":"2024-04-22 17:36:03","title":"TAVGBench: Benchmarking Text to Audible-Video Generation","abstract":"The Text to Audible-Video Generation (TAVG) task involves generating videos with accompanying audio based on text descriptions. Achieving this requires skillful alignment of both audio and video elements. To support research in this field, we have developed a comprehensive Text to Audible-Video Generation Benchmark (TAVGBench), which contains over 1.7 million clips with a total duration of 11.8 thousand hours. We propose an automatic annotation pipeline to ensure each audible video has detailed descriptions for both its audio and video contents. We also introduce the Audio-Visual Harmoni score (AVHScore) to provide a quantitative measure of the alignment between the generated audio and video modalities. Additionally, we present a baseline model for TAVG called TAVDiffusion, which uses a two-stream latent diffusion model to provide a fundamental starting point for further research in this area. We achieve the alignment of audio and video by employing cross-attention and contrastive learning. Through extensive experiments and evaluations on TAVGBench, we demonstrate the effectiveness of our proposed model under both conventional metrics and our proposed metrics.","sentences":["The Text to Audible-Video Generation (TAVG) task involves generating videos with accompanying audio based on text descriptions.","Achieving this requires skillful alignment of both audio and video elements.","To support research in this field, we have developed a comprehensive Text to Audible-Video Generation Benchmark (TAVGBench), which contains over 1.7 million clips with a total duration of 11.8 thousand hours.","We propose an automatic annotation pipeline to ensure each audible video has detailed descriptions for both its audio and video contents.","We also introduce the Audio-Visual Harmoni score (AVHScore) to provide a quantitative measure of the alignment between the generated audio and video modalities.","Additionally, we present a baseline model for TAVG called TAVDiffusion, which uses a two-stream latent diffusion model to provide a fundamental starting point for further research in this area.","We achieve the alignment of audio and video by employing cross-attention and contrastive learning.","Through extensive experiments and evaluations on TAVGBench, we demonstrate the effectiveness of our proposed model under both conventional metrics and our proposed metrics."],"url":"http://arxiv.org/abs/2404.14381v1","category":"cs.CV"}
{"created":"2024-04-22 17:32:52","title":"Penn & Slavery Project's Augmented Reality Tour: Augmenting a Campus to Reveal a Hidden History","abstract":"In 2006 and 2016, the University of Pennsylvania denied any ties to slavery. In 2017, a group of undergraduate researchers, led by Professor Kathleen Brown, investigated this claim. Initial research, focused on 18th century faculty and trustees who owned slaves, revealed deep connections between the university's history and the institution of slavery. These findings, and discussions amongst the researchers shaped the Penn and Slavery Project's goal of redefining complicity beyond ownership. Breanna Moore's contributions in PSP's second semester expanded the project's focus to include generational wealth gaps. In 2018, VanJessica Gladney served as the PSP's Public History Fellow and spread the project outreach in the greater Philadelphia area. That year, the PSP team began to design an augmented reality app as a Digital Interruption and an attempt to display the truth about Penn's history on its campus. Unfortunately, PSP faced delays due to COVID 19. Despite setbacks, the project persisted, engaging with activists and the wider community to confront historical injustices and modern inequalities.","sentences":["In 2006 and 2016, the University of Pennsylvania denied any ties to slavery.","In 2017, a group of undergraduate researchers, led by Professor Kathleen Brown, investigated this claim.","Initial research, focused on 18th century faculty and trustees who owned slaves, revealed deep connections between the university's history and the institution of slavery.","These findings, and discussions amongst the researchers shaped the Penn and Slavery Project's goal of redefining complicity beyond ownership.","Breanna Moore's contributions in PSP's second semester expanded the project's focus to include generational wealth gaps.","In 2018, VanJessica Gladney served as the PSP's Public History Fellow and spread the project outreach in the greater Philadelphia area.","That year, the PSP team began to design an augmented reality app as a Digital Interruption and an attempt to display the truth about Penn's history on its campus.","Unfortunately, PSP faced delays due to COVID 19.","Despite setbacks, the project persisted, engaging with activists and the wider community to confront historical injustices and modern inequalities."],"url":"http://arxiv.org/abs/2404.14379v1","category":"cs.HC"}
{"created":"2024-04-22 17:29:52","title":"Pipeline Provenance for Analysis, Evaluation, Trust or Reproducibility","abstract":"Data volumes and rates of research infrastructures will continue to increase in the upcoming years and impact how we interact with their final data products. Little of the processed data can be directly investigated and most of it will be automatically processed with as little user interaction as possible. Capturing all necessary information of such processing ensures reproducibility of the final results and generates trust in the entire process. We present PRAETOR, a software suite that enables automated generation, modelling, and analysis of provenance information of Python pipelines. Furthermore, the evaluation of the pipeline performance, based upon a user defined quality matrix in the provenance, enables the first step of machine learning processes, where such information can be fed into dedicated optimisation procedures.","sentences":["Data volumes and rates of research infrastructures will continue to increase in the upcoming years and impact how we interact with their final data products.","Little of the processed data can be directly investigated and most of it will be automatically processed with as little user interaction as possible.","Capturing all necessary information of such processing ensures reproducibility of the final results and generates trust in the entire process.","We present PRAETOR, a software suite that enables automated generation, modelling, and analysis of provenance information of Python pipelines.","Furthermore, the evaluation of the pipeline performance, based upon a user defined quality matrix in the provenance, enables the first step of machine learning processes, where such information can be fed into dedicated optimisation procedures."],"url":"http://arxiv.org/abs/2404.14378v1","category":"astro-ph.IM"}
{"created":"2024-04-22 17:22:31","title":"Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph","abstract":"Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs. Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph.","sentences":["Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs).","However, it can fall short in specific scenarios where simple customized methods excel.","In this paper, we delve into the patent approval pre-diction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data.","Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up.","Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text.","As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction.","Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly.","We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.","Our source code and dataset can be obtained from http://github.com/ShangDataLab/FLAN-Graph."],"url":"http://arxiv.org/abs/2404.14372v1","category":"cs.CL"}
{"created":"2024-04-22 17:21:39","title":"Analysing the interaction of expansion decisions by end customers and grid development in the context of a municipal energy system","abstract":"In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector. Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators. These measures thus change both the load and the generation of the future energy supply concept. Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary. The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions. The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation. To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion. As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated.","sentences":["In order to achieve greenhouse gas neutrality by 2045, the Climate Protection Act sets emission reduction targets for the years 2030 and 2040, as well as decreasing annual emission volumes for some sectors, including the building sector.","Measures to decarbonize the building sector include energy retrofits and the expansion of renewable, decentralized power generators and low-CO2 heat generators.","These measures thus change both the load and the generation of the future energy supply concept.","Considering the interactions of the changed installed technologies on the building level and their influence on the electrical grid infrastructure is necessary.","The grid operator will remedy the future congested grid states by grid expansion measures and pass on the costs to the connected grid users, which in turn could influence their behaviour and decisions.","The aim of this work is a holistic analysis of the staggered interactions of generation expansion and grid expansion for a future decentralized energy supply concept conditioned by the expansion in the field of self-generation.","To enable the analysis of the interactions, a multi-criteria optimization procedure for expansion and operation decisions at the building level is combined with an approach to determine grid expansion.","As part of this work, the effect of an expansion of hosting capacity on the grid charges and thus the decision-making behaviour was investigated."],"url":"http://arxiv.org/abs/2404.14371v1","category":"eess.SY"}
{"created":"2024-04-22 17:21:24","title":"Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation","abstract":"The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely.","sentences":["The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes.","This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files.","In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams.","We used 3 different prompts for each input, and we manually evaluated the results.","We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code.","On average, the model was able to generate source code for 88% of the elements shown in the diagrams.","Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files.","However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams.","In summary, further investigations are necessary to exploit the model's potential completely."],"url":"http://arxiv.org/abs/2404.14370v1","category":"cs.SE"}
{"created":"2024-04-22 17:20:38","title":"Graphic Design with Large Multimodal Model","abstract":"In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design. One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements. It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload. In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements. To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models. Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element. We develop new evaluation metrics for HLG. Graphist outperforms prior arts and establishes a strong baseline for this field. Project homepage: https://github.com/graphic-design-ai/graphist","sentences":["In the field of graphic design, automating the integration of design elements into a cohesive multi-layered artwork not only boosts productivity but also paves the way for the democratization of graphic design.","One existing practice is Graphic Layout Generation (GLG), which aims to layout sequential design elements.","It has been constrained by the necessity for a predefined correct sequence of layers, thus limiting creative potential and increasing user workload.","In this paper, we present Hierarchical Layout Generation (HLG) as a more flexible and pragmatic setup, which creates graphic composition from unordered sets of design elements.","To tackle the HLG task, we introduce Graphist, the first layout generation model based on large multimodal models.","Graphist efficiently reframes the HLG as a sequence generation problem, utilizing RGB-A images as input, outputs a JSON draft protocol, indicating the coordinates, size, and order of each element.","We develop new evaluation metrics for HLG.","Graphist outperforms prior arts and establishes a strong baseline for this field.","Project homepage: https://github.com/graphic-design-ai/graphist"],"url":"http://arxiv.org/abs/2404.14368v1","category":"cs.CV"}
{"created":"2024-04-22 17:20:18","title":"Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data","abstract":"Learning from preference labels plays a crucial role in fine-tuning large language models. There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning. Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient. This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why? In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.","sentences":["Learning from preference labels plays a crucial role in fine-tuning large language models.","There are several distinct approaches for preference fine-tuning, including supervised learning, on-policy reinforcement learning (RL), and contrastive learning.","Different methods come with different implementation tradeoffs and performance differences, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find (offline) contrastive or even purely supervised methods sufficient.","This raises a natural question: what kind of approaches are important for fine-tuning with preference data and why?","In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems.","Our main finding is that, in general, approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ a \"negative gradient\") outperform offline and maximum likelihood objectives.","We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions.","Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively.","Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement."],"url":"http://arxiv.org/abs/2404.14367v2","category":"cs.LG"}
{"created":"2024-04-22 17:18:36","title":"An inverse problem in Polya-Schur theory. I. Non-genegerate and degenerate operators","abstract":"Given a linear ordinary differential operator T with polynomial coefficients, we study the class of closed subsets of the complex plane such that T sends any polynomial (resp. any polynomial of degree exceeding a given positive integer) with all roots in a given subset to a polynomial with all roots in the same subset or to 0. Below we discuss some general properties of such invariant subsets as well as the problem of existence of the minimal under inclusion invariant subset.","sentences":["Given a linear ordinary differential operator T with polynomial coefficients, we study the class of closed subsets of the complex plane such that T sends any polynomial (resp.","any polynomial of degree exceeding a given positive integer) with all roots in a given subset to a polynomial with all roots in the same subset or to 0.","Below we discuss some general properties of such invariant subsets as well as the problem of existence of the minimal under inclusion invariant subset."],"url":"http://arxiv.org/abs/2404.14365v1","category":"math.CA"}
{"created":"2024-04-22 17:15:32","title":"Better Synthetic Data by Retrieving and Transforming Existing Datasets","abstract":"Despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, we introduce a method, \\textit{DataTune}, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, we find that finetuning language models via DataTune improves over a few-shot prompting baseline by 49\\% and improves over existing methods that use synthetic or retrieved training data by 34\\%. We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. We integrate DataTune into an open-source repository to make this method accessible to the community: https://github.com/neulab/prompt2model.","sentences":["Despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data.","However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive.","Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity.","To address these limitations, we introduce a method, \\textit{DataTune}, to make better use of existing, publicly available datasets to improve automatic dataset generation.","DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks.","On a diverse set of language-based tasks from the BIG-Bench benchmark, we find that finetuning language models via DataTune improves over a few-shot prompting baseline by 49\\% and improves over existing methods that use synthetic or retrieved training data by 34\\%.","We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks.","We integrate DataTune into an open-source repository to make this method accessible to the community: https://github.com/neulab/prompt2model."],"url":"http://arxiv.org/abs/2404.14361v1","category":"cs.CL"}
{"created":"2024-04-22 17:14:17","title":"The Alaric parton shower for hadron colliders","abstract":"We introduce the Alaric parton shower for simulating QCD radiation at hadron colliders and present numerical results from an implementation in the event generator Sherpa. Alaric provides a consistent framework to quantify certain systematic uncertainties which cannot be eliminated by comparing the parton shower with analytic resummation. In particular, it allows to study recoil effects away from the soft and collinear limits without the need to change the evolution variable or the splitting functions. We assess the performance of Alaric in Drell-Yan lepton pair and QCD jet production, and present the first multi-jet merging for the new algorithm.","sentences":["We introduce the Alaric parton shower for simulating QCD radiation at hadron colliders and present numerical results from an implementation in the event generator Sherpa.","Alaric provides a consistent framework to quantify certain systematic uncertainties which cannot be eliminated by comparing the parton shower with analytic resummation.","In particular, it allows to study recoil effects away from the soft and collinear limits without the need to change the evolution variable or the splitting functions.","We assess the performance of Alaric in Drell-Yan lepton pair and QCD jet production, and present the first multi-jet merging for the new algorithm."],"url":"http://arxiv.org/abs/2404.14360v1","category":"hep-ph"}
{"created":"2024-04-22 17:12:58","title":"A General Continuous-Time Formulation of Stochastic ADMM and Its Variants","abstract":"Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems. In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis. The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM. Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise. Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2.","sentences":["Stochastic versions of the alternating direction method of multiplier (ADMM) and its variants play a key role in many modern large-scale machine learning problems.","In this work, we introduce a unified algorithmic framework called generalized stochastic ADMM and investigate their continuous-time analysis.","The generalized framework widely includes many stochastic ADMM variants such as standard, linearized and gradient-based ADMM.","Our continuous-time analysis provides us with new insights into stochastic ADMM and variants, and we rigorously prove that under some proper scaling, the trajectory of stochastic ADMM weakly converges to the solution of a stochastic differential equation with small noise.","Our analysis also provides a theoretical explanation of why the relaxation parameter should be chosen between 0 and 2."],"url":"http://arxiv.org/abs/2404.14358v1","category":"math.OC"}
{"created":"2024-04-22 17:10:27","title":"Rethinking Legal Compliance Automation: Opportunities with Large Language Models","abstract":"As software-intensive systems face growing pressure to comply with laws and regulations, providing automated support for compliance analysis has become paramount. Despite advances in the Requirements Engineering (RE) community on legal compliance analysis, important obstacles remain in developing accurate and generalizable compliance automation solutions. This paper highlights some observed limitations of current approaches and examines how adopting new automation strategies that leverage Large Language Models (LLMs) can help address these shortcomings and open up fresh opportunities. Specifically, we argue that the examination of (textual) legal artifacts should, first, employ a broader context than sentences, which have widely been used as the units of analysis in past research. Second, the mode of analysis with legal artifacts needs to shift from classification and information extraction to more end-to-end strategies that are not only accurate but also capable of providing explanation and justification. We present a compliance analysis approach designed to address these limitations. We further outline our evaluation plan for the approach and provide preliminary evaluation results based on data processing agreements (DPAs) that must comply with the General Data Protection Regulation (GDPR). Our initial findings suggest that our approach yields substantial accuracy improvements and, at the same time, provides justification for compliance decisions.","sentences":["As software-intensive systems face growing pressure to comply with laws and regulations, providing automated support for compliance analysis has become paramount.","Despite advances in the Requirements Engineering (RE) community on legal compliance analysis, important obstacles remain in developing accurate and generalizable compliance automation solutions.","This paper highlights some observed limitations of current approaches and examines how adopting new automation strategies that leverage Large Language Models (LLMs) can help address these shortcomings and open up fresh opportunities.","Specifically, we argue that the examination of (textual) legal artifacts should, first, employ a broader context than sentences, which have widely been used as the units of analysis in past research.","Second, the mode of analysis with legal artifacts needs to shift from classification and information extraction to more end-to-end strategies that are not only accurate but also capable of providing explanation and justification.","We present a compliance analysis approach designed to address these limitations.","We further outline our evaluation plan for the approach and provide preliminary evaluation results based on data processing agreements (DPAs) that must comply with the General Data Protection Regulation (GDPR).","Our initial findings suggest that our approach yields substantial accuracy improvements and, at the same time, provides justification for compliance decisions."],"url":"http://arxiv.org/abs/2404.14356v1","category":"cs.SE"}
{"created":"2024-04-22 17:07:25","title":"Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the Calculator Improves Numeracy in Language Models","abstract":"Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Our code and data are available at https://github.com/calc-cmu/pre-calc.","sentences":["Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models.","While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders.","In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively.","We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding.","Our code and data are available at https://github.com/calc-cmu/pre-calc."],"url":"http://arxiv.org/abs/2404.14355v1","category":"cs.CL"}
{"created":"2024-04-22 17:05:29","title":"A Genetic Algorithm For Convex Hull Optimisation","abstract":"Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures. A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species. This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures.","sentences":["Computationally efficient and automated generation of convex hulls is desirable for high throughput materials discovery of thermodynamically stable multi-species crystal structures.","A convex hull genetic algorithm is proposed that uses methodology adapted from multi-objective optimisation techniques to optimise the convex hull itself as an object, enabling efficient discovery of convex hulls for N >= 2 species.","This method, when tested on a LiSi system utilising pre-trained machine learned potentials, was found to be able to efficiently discover reported structures as well as new potential LiSi candidate structures."],"url":"http://arxiv.org/abs/2404.14354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 17:05:06","title":"Electroporation-mediated Metformin for effective anticancer treatment of triple-negative breast cancer cells","abstract":"In this research, we investigated the efficacy of Metformin, the most commonly administered type-2 diabetes drug for triple negative breast cancer (TNBC) treatment, due to its various anticancer properties. It is a plant-based bio-compound, synthesized as a novel biguanide, called dimethyl biguanide or metformin. One of the ways it operates is by hindering electron transport chain-complex I, in mitochondria, which causes a drop-in energy (ATP) generation. This eventually builds energetic stress and a decline in energy. Therefore, the natural cellular processes and proliferating tumor cells are obstructed. Here, we used electroporation, where, the MDA-MB-231, human TNBC cells were subjected to high intensity, short-duration electrical pulses (EP) in the presence of Metformin. The cell viability results indicate lower cell viability of 43.45% as compared to 85.20% with drug alone at 5mM concentration. This indicates that Metformin, the most common diabetes drug could also be explored for cancer treatment.","sentences":["In this research, we investigated the efficacy of Metformin, the most commonly administered type-2 diabetes drug for triple negative breast cancer (TNBC) treatment, due to its various anticancer properties.","It is a plant-based bio-compound, synthesized as a novel biguanide, called dimethyl biguanide or metformin.","One of the ways it operates is by hindering electron transport chain-complex I, in mitochondria, which causes a drop-in energy (ATP) generation.","This eventually builds energetic stress and a decline in energy.","Therefore, the natural cellular processes and proliferating tumor cells are obstructed.","Here, we used electroporation, where, the MDA-MB-231, human TNBC cells were subjected to high intensity, short-duration electrical pulses (EP) in the presence of Metformin.","The cell viability results indicate lower cell viability of 43.45% as compared to 85.20% with drug alone at 5mM concentration.","This indicates that Metformin, the most common diabetes drug could also be explored for cancer treatment."],"url":"http://arxiv.org/abs/2404.14353v1","category":"q-bio.BM"}
{"created":"2024-04-22 17:03:16","title":"Integration of first-order ODEs by Jacobi fields","abstract":"A new class of vector fields enabling the integration of first-order ordinary differential equations (ODEs) is introduced. These vector fields are not, in general, Lie point symmetries. The results are based on a relation between 2-dimensional Riemannian manifolds and the integrability of first-order ODEs, which was established in a previous work of the authors. An integration procedure is provided, together with several examples to illustrate it. A connection between integrating factors of first-order ODEs and Schr\\\"odinger-type equations is highlighted.","sentences":["A new class of vector fields enabling the integration of first-order ordinary differential equations (ODEs) is introduced.","These vector fields are not, in general, Lie point symmetries.","The results are based on a relation between 2-dimensional Riemannian manifolds and the integrability of first-order ODEs, which was established in a previous work of the authors.","An integration procedure is provided, together with several examples to illustrate it.","A connection between integrating factors of first-order ODEs and Schr\\\"odinger-type equations is highlighted."],"url":"http://arxiv.org/abs/2404.14352v1","category":"math.CA"}
{"created":"2024-04-22 17:00:57","title":"Automatic Discovery of Visual Circuits","abstract":"To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor. We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept. We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity. We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks.","sentences":["To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor.","We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept.","We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity.","We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks."],"url":"http://arxiv.org/abs/2404.14349v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:48","title":"Operando Analysis of Adsorption-Limited Hydrogen Oxidation Reaction at Palladium Surfaces","abstract":"Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions. This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates. Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive. This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst. Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy. The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution. Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced. Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors. These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies. The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions.","sentences":["Palladium (Pd) catalysts have been extensively studied for the direct synthesis of H2O through the hydrogen oxidation reaction at ambient conditions.","This heterogeneous catalytic reaction not only holds considerable practical significance but also serves as a classical model for investigating fundamental mechanisms, including adsorption and reactions between adsorbates.","Nonetheless, the governing mechanisms and kinetics of its intermediate reaction stages under varying gas conditions remains elusive.","This is attributed to the intricate interplay between adsorption, atomic diffusion, and concurrent phase transformation of catalyst.","Herein, the Pd-catalyzed, water-forming hydrogen oxidation is studied, in situ, to investigate intermediate reaction stages via fluid cell transmission electron microscopy.","The dynamic behaviors of water generation, associated with reversible palladium hydride formation, are captured in real time with a nanoscale spatial resolution.","Our findings suggest that the hydrogen oxidation rate catalyzed by Pd is significantly affected by the sequence in which gases are introduced.","Through direct evidence of electron diffraction and density functional theory calculation, we demonstrate that the hydrogen oxidation rate is limited by adsorption processes of gas precursors.","These nanoscale insights help identify the optimal reaction conditions for Pd-catalyzed hydrogen oxidation, which has substantial implications for water production technologies.","The developed understanding also advocates a broader exploration of analogous mechanisms in other metal-catalyzed reactions."],"url":"http://arxiv.org/abs/2404.14348v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 17:00:10","title":"Direct Comparison of SiPM and PMT Sensor Performances in a large-size imaging air Cherenkov telescope","abstract":"The peak photon detection efficiency (PDE) of silicon photomultipliers (SiPMs) can be as good or better than the PDE of photomultiplier tubes (PMTs). There are experiments where the signal is measured in the presence of a strong, steady background light emission. In these, one needs to accurately evaluate the signal-to-noise ratio. Imaging Atmospheric Cherenkov Telescopes (IACTs) observe in the presence of strong noise induced by the light of the night sky. It is certainly interesting to investigate the SiPM performance under operational conditions of IACTs and to compare it with that of the PMTs. For that purpose, we built a SiPM-based detector module, which was installed in one of the imaging cameras of the two Major Atmospheric Gamma-ray Imaging Cherenkov (MAGIC) telescopes in 2015. The experience gained from that module was used to design the second generation of modules of improved performance. Two such modules were installed in 2017. MAGIC is a system of two IACTs located on the Canary Island of La Palma. The mechanical structure of the MAGIC imaging cameras offers the possibility to install up to 6 additional detector modules of 7 pixels each into the open vertices of the hexagonal-shaped camera. This allows us to directly, without making any assumption, compare the performance of the PMT-based modules with that of the SiPM-based prototype modules, where SiPMs from three different manufacturers are used.","sentences":["The peak photon detection efficiency (PDE) of silicon photomultipliers (SiPMs) can be as good or better than the PDE of photomultiplier tubes (PMTs).","There are experiments where the signal is measured in the presence of a strong, steady background light emission.","In these, one needs to accurately evaluate the signal-to-noise ratio.","Imaging Atmospheric Cherenkov Telescopes (IACTs) observe in the presence of strong noise induced by the light of the night sky.","It is certainly interesting to investigate the SiPM performance under operational conditions of IACTs and to compare it with that of the PMTs.","For that purpose, we built a SiPM-based detector module, which was installed in one of the imaging cameras of the two Major Atmospheric Gamma-ray Imaging Cherenkov (MAGIC) telescopes in 2015.","The experience gained from that module was used to design the second generation of modules of improved performance.","Two such modules were installed in 2017.","MAGIC is a system of two IACTs located on the Canary Island of La Palma.","The mechanical structure of the MAGIC imaging cameras offers the possibility to install up to 6 additional detector modules of 7 pixels each into the open vertices of the hexagonal-shaped camera.","This allows us to directly, without making any assumption, compare the performance of the PMT-based modules with that of the SiPM-based prototype modules, where SiPMs from three different manufacturers are used."],"url":"http://arxiv.org/abs/2404.14346v1","category":"astro-ph.IM"}
{"created":"2024-04-22 16:59:43","title":"On-the-Fly Point Annotation for Fast Medical Video Labeling","abstract":"Purpose: In medical research, deep learning models rely on high-quality annotated data, a process often laborious and timeconsuming. This is particularly true for detection tasks where bounding box annotations are required. The need to adjust two corners makes the process inherently frame-by-frame. Given the scarcity of experts' time, efficient annotation methods suitable for clinicians are needed. Methods: We propose an on-the-fly method for live video annotation to enhance the annotation efficiency. In this approach, a continuous single-point annotation is maintained by keeping the cursor on the object in a live video, mitigating the need for tedious pausing and repetitive navigation inherent in traditional annotation methods. This novel annotation paradigm inherits the point annotation's ability to generate pseudo-labels using a point-to-box teacher model. We empirically evaluate this approach by developing a dataset and comparing on-the-fly annotation time against traditional annotation method. Results: Using our method, annotation speed was 3.2x faster than the traditional annotation technique. We achieved a mean improvement of 6.51 +- 0.98 AP@50 over conventional method at equivalent annotation budgets on the developed dataset. Conclusion: Without bells and whistles, our approach offers a significant speed-up in annotation tasks. It can be easily implemented on any annotation platform to accelerate the integration of deep learning in video-based medical research.","sentences":["Purpose: In medical research, deep learning models rely on high-quality annotated data, a process often laborious and timeconsuming.","This is particularly true for detection tasks where bounding box annotations are required.","The need to adjust two corners makes the process inherently frame-by-frame.","Given the scarcity of experts' time, efficient annotation methods suitable for clinicians are needed.","Methods: We propose an on-the-fly method for live video annotation to enhance the annotation efficiency.","In this approach, a continuous single-point annotation is maintained by keeping the cursor on the object in a live video, mitigating the need for tedious pausing and repetitive navigation inherent in traditional annotation methods.","This novel annotation paradigm inherits the point annotation's ability to generate pseudo-labels using a point-to-box teacher model.","We empirically evaluate this approach by developing a dataset and comparing on-the-fly annotation time against traditional annotation method.","Results: Using our method, annotation speed was 3.2x faster than the traditional annotation technique.","We achieved a mean improvement of 6.51 +- 0.98 AP@50 over conventional method at equivalent annotation budgets on the developed dataset.","Conclusion: Without bells and whistles, our approach offers a significant speed-up in annotation tasks.","It can be easily implemented on any annotation platform to accelerate the integration of deep learning in video-based medical research."],"url":"http://arxiv.org/abs/2404.14344v1","category":"cs.CV"}
{"created":"2024-04-22 16:58:20","title":"Recommendations for Early Definition Science with the Nancy Grace Roman Space Telescope","abstract":"The Nancy Grace Roman Space Telescope (Roman), NASA's next flagship observatory, has significant mission time to be spent on surveys for general astrophysics in addition to its three core community surveys. We considered what types of observations outside the core surveys would most benefit from early definition, given 700 hours of mission time in the first two years of Roman's operation. We recommend that a survey of the Galactic plane be defined early, based on the broad range of stakeholders for such a survey, the added scientific value of a first pass to obtain a baseline for proper motions complementary to Gaia's, and the significant potential synergies with ground-based surveys, notably the Legacy Survey of Space and Time (LSST) on Rubin. We also found strong motivation to follow a community definition process for ultra-deep observations with Roman.","sentences":["The Nancy Grace Roman Space Telescope (Roman), NASA's next flagship observatory, has significant mission time to be spent on surveys for general astrophysics in addition to its three core community surveys.","We considered what types of observations outside the core surveys would most benefit from early definition, given 700 hours of mission time in the first two years of Roman's operation.","We recommend that a survey of the Galactic plane be defined early, based on the broad range of stakeholders for such a survey, the added scientific value of a first pass to obtain a baseline for proper motions complementary to Gaia's, and the significant potential synergies with ground-based surveys, notably the Legacy Survey of Space and Time (LSST) on Rubin.","We also found strong motivation to follow a community definition process for ultra-deep observations with Roman."],"url":"http://arxiv.org/abs/2404.14342v1","category":"astro-ph.GA"}
{"created":"2024-04-22 16:57:13","title":"Hybrid Intersection Types for PCF (Extended Version)","abstract":"Intersection type systems have been independently applied to different evaluation strategies, such as call-by-name (CBN) and call-by-value (CBV). These type systems have been then generalized to different subsuming paradigms being able, in particular, to encode CBN and CBV in a unique unifying framework. However, there are no intersection type systems that explicitly enable CBN and CBV to cohabit together without making use of an encoding into a common target framework. This work proposes an intersection type system for PCF with a specific notion of evaluation, called PCFH. Evaluation in PCFH actually has a hybrid nature, in the sense that CBN and CBV operational behaviors cohabit together. Indeed, PCFH combines a CBV-like operational behavior for function application with a CBN-like behavior for recursion. This hybrid nature is reflected in the type system, which turns out to be sound and complete with respect to PCFH: not only typability implies normalization, but also the converse holds. Moreover, the type system is quantitative, in the sense that the size of typing derivations provides upper bounds for the length of the reduction sequences to normal form. This type system is then refined to a tight one, offering exact information regarding the length of normalization sequences. This is the first time that a sound and complete quantitative type system has been designed for a hybrid computational model.","sentences":["Intersection type systems have been independently applied to different evaluation strategies, such as call-by-name (CBN) and call-by-value (CBV).","These type systems have been then generalized to different subsuming paradigms being able, in particular, to encode CBN and CBV in a unique unifying framework.","However, there are no intersection type systems that explicitly enable CBN and CBV to cohabit together without making use of an encoding into a common target framework.","This work proposes an intersection type system for PCF with a specific notion of evaluation, called PCFH.","Evaluation in PCFH actually has a hybrid nature, in the sense that CBN and CBV operational behaviors cohabit together.","Indeed, PCFH combines a CBV-like operational behavior for function application with a CBN-like behavior for recursion.","This hybrid nature is reflected in the type system, which turns out to be sound and complete with respect to PCFH: not only typability implies normalization, but also the converse holds.","Moreover, the type system is quantitative, in the sense that the size of typing derivations provides upper bounds for the length of the reduction sequences to normal form.","This type system is then refined to a tight one, offering exact information regarding the length of normalization sequences.","This is the first time that a sound and complete quantitative type system has been designed for a hybrid computational model."],"url":"http://arxiv.org/abs/2404.14340v1","category":"cs.LO"}
{"created":"2024-04-22 16:52:34","title":"Regularized Conformal Electrodynamics: Novel C-metric in (2+1) Dimensions","abstract":"Conformal electrodynamics is a particularly interesting example of power Maxwell non-linear electrodynamics, designed to possess conformal symmetry in all dimensions. In this paper, we propose a regularized version of Conformal electrodynamics, minimally regularizing the field of a point charge at the origin by breaking the conformal invariance of the theory with a dimensionfull \"Born-Infeld-like\" parameter. In four dimensions the new theory reduces to the recently studied Regularized Maxwell electrodynamics, distinguished by its \"Maxwell-like\" solutions for accelerated and slowly rotating black hole spacetimes. Focusing on three dimensions, we show that the new theory shares many of the properties of its four-dimensional cousin, including the existence of the charged C-metric solution (currently unknown in the Maxwell theory).","sentences":["Conformal electrodynamics is a particularly interesting example of power Maxwell non-linear electrodynamics, designed to possess conformal symmetry in all dimensions.","In this paper, we propose a regularized version of Conformal electrodynamics, minimally regularizing the field of a point charge at the origin by breaking the conformal invariance of the theory with a dimensionfull \"Born-Infeld-like\" parameter.","In four dimensions the new theory reduces to the recently studied Regularized Maxwell electrodynamics, distinguished by its \"Maxwell-like\" solutions for accelerated and slowly rotating black hole spacetimes.","Focusing on three dimensions, we show that the new theory shares many of the properties of its four-dimensional cousin, including the existence of the charged C-metric solution (currently unknown in the Maxwell theory)."],"url":"http://arxiv.org/abs/2404.14335v1","category":"gr-qc"}
{"created":"2024-04-22 16:47:10","title":"Full Event Particle-Level Unfolding with Variable-Length Latent Variational Diffusion","abstract":"The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions. One approach, unfolding, statistically adjusts the experimental data for detector effects. Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions. However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data. A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces. The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider.","sentences":["The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions.","One approach, unfolding, statistically adjusts the experimental data for detector effects.","Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions.","However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data.","A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces.","The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider."],"url":"http://arxiv.org/abs/2404.14332v1","category":"hep-ex"}
{"created":"2024-04-22 16:40:11","title":"X-Ray: A Sequential 3D Representation for Generation","abstract":"In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects. Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces. This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos. Such a structure ensures the 3D representation is composed solely of critical surface information. Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models. The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field.","sentences":["In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects.","Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces.","This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos.","Such a structure ensures the 3D representation is composed solely of critical surface information.","Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models.","The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field."],"url":"http://arxiv.org/abs/2404.14329v1","category":"cs.CV"}
{"created":"2024-04-22 16:39:32","title":"Preserving linear invariants in ensemble filtering methods","abstract":"Formulating dynamical models for physical phenomena is essential for understanding the interplay between the different mechanisms and predicting the evolution of physical states. However, a dynamical model alone is often insufficient to address these fundamental tasks, as it suffers from model errors and uncertainties. One common remedy is to rely on data assimilation, where the state estimate is updated with observations of the true system. Ensemble filters sequentially assimilate observations by updating a set of samples over time. They operate in two steps: a forecast step that propagates each sample through the dynamical model and an analysis step that updates the samples with incoming observations. For accurate and robust predictions of dynamical systems, discrete solutions must preserve their critical invariants. While modern numerical solvers satisfy these invariants, existing invariant-preserving analysis steps are limited to Gaussian settings and are often not compatible with classical regularization techniques of ensemble filters, e.g., inflation and covariance tapering. The present work focuses on preserving linear invariants, such as mass, stoichiometric balance of chemical species, and electrical charges. Using tools from measure transport theory (Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear ensemble filters that automatically preserve desired linear invariants in non-Gaussian filtering problems. By specializing this framework to the Gaussian setting, we recover a constrained formulation of the Kalman filter. Then, we show how to combine existing regularization techniques for the ensemble Kalman filter (Evensen, 1994, J. Geophys. Res.) with the preservation of the linear invariants. Finally, we assess the benefits of preserving linear invariants for the ensemble Kalman filter and nonlinear ensemble filters.","sentences":["Formulating dynamical models for physical phenomena is essential for understanding the interplay between the different mechanisms and predicting the evolution of physical states.","However, a dynamical model alone is often insufficient to address these fundamental tasks, as it suffers from model errors and uncertainties.","One common remedy is to rely on data assimilation, where the state estimate is updated with observations of the true system.","Ensemble filters sequentially assimilate observations by updating a set of samples over time.","They operate in two steps: a forecast step that propagates each sample through the dynamical model and an analysis step that updates the samples with incoming observations.","For accurate and robust predictions of dynamical systems, discrete solutions must preserve their critical invariants.","While modern numerical solvers satisfy these invariants, existing invariant-preserving analysis steps are limited to Gaussian settings and are often not compatible with classical regularization techniques of ensemble filters, e.g., inflation and covariance tapering.","The present work focuses on preserving linear invariants, such as mass, stoichiometric balance of chemical species, and electrical charges.","Using tools from measure transport theory (Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear ensemble filters that automatically preserve desired linear invariants in non-Gaussian filtering problems.","By specializing this framework to the Gaussian setting, we recover a constrained formulation of the Kalman filter.","Then, we show how to combine existing regularization techniques for the ensemble Kalman filter (Evensen, 1994, J. Geophys.","Res.)","with the preservation of the linear invariants.","Finally, we assess the benefits of preserving linear invariants for the ensemble Kalman filter and nonlinear ensemble filters."],"url":"http://arxiv.org/abs/2404.14328v1","category":"stat.CO"}
{"created":"2024-04-22 16:38:41","title":"Machine Learning Techniques for MRI Data Processing at Expanding Scale","abstract":"Imaging sites around the world generate growing amounts of medical scan data with ever more versatile and affordable technology. Large-scale studies acquire MRI for tens of thousands of participants, together with metadata ranging from lifestyle questionnaires to biochemical assays, genetic analyses and more. These large datasets encode substantial information about human health and hold considerable potential for machine learning training and analysis. This chapter examines ongoing large-scale studies and the challenge of distribution shifts between them. Transfer learning for overcoming such shifts is discussed, together with federated learning for safe access to distributed training data securely held at multiple institutions. Finally, representation learning is reviewed as a methodology for encoding embeddings that express abstract relationships in multi-modal input formats.","sentences":["Imaging sites around the world generate growing amounts of medical scan data with ever more versatile and affordable technology.","Large-scale studies acquire MRI for tens of thousands of participants, together with metadata ranging from lifestyle questionnaires to biochemical assays, genetic analyses and more.","These large datasets encode substantial information about human health and hold considerable potential for machine learning training and analysis.","This chapter examines ongoing large-scale studies and the challenge of distribution shifts between them.","Transfer learning for overcoming such shifts is discussed, together with federated learning for safe access to distributed training data securely held at multiple institutions.","Finally, representation learning is reviewed as a methodology for encoding embeddings that express abstract relationships in multi-modal input formats."],"url":"http://arxiv.org/abs/2404.14326v1","category":"cs.LG"}
{"created":"2024-04-22 16:38:38","title":"Adapting to time: why nature evolved a diverse set of neurons","abstract":"Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information. In addition, it is known empirically that spike timing is a significant factor in neural computations. However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases. In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns. In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure. Indeed, when adaptation was restricted to weights, networks were unable to solve most problems. We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design.","sentences":["Evolution has yielded a diverse set of neurons with varying morphologies and physiological properties that impact their processing of temporal information.","In addition, it is known empirically that spike timing is a significant factor in neural computations.","However, despite these two observations, most neural network models deal with spatially structured inputs with synchronous time steps, while restricting variation to parameters like weights and biases.","In this study, we investigate the relevance of adapting temporal parameters, like time constants and delays, in feedforward networks that map spatio-temporal spike patterns.","In this context, we show that networks with richer potential dynamics are able to more easily and robustly learn tasks with temporal structure.","Indeed, when adaptation was restricted to weights, networks were unable to solve most problems.","We also show strong interactions between the various parameters and the advantages of adapting temporal parameters when dealing with noise in inputs and weights, which might prove useful in neuromorphic hardware design."],"url":"http://arxiv.org/abs/2404.14325v1","category":"cs.NE"}
{"created":"2024-04-22 16:32:40","title":"Mechanisms for producing Primordial Black Holes from Inflationary Models Beyond Fine-Tuning","abstract":"In this study, we present an analysis of the fine-tuning required in various inflationary models in order to explain the production of Primordial Black Holes (PBHs). We specifically examine the degree of fine-tuning necessary in two prominent single field inflationary models: those with an inflection point and those with step-like features in the potential. Our findings indicate that models with step-like features generally require less fine-tuning compared to those with an inflection point, making them more viable for consistent PBH production. An interesting outcome of these models is that, in addition to improved fine-tuning, they may also predict low-frequency signals that can be detected by pulsar timing array (PTA) collaborations. Additionally, we extend our analysis to multifield inflationary models to assess whether the integration of additional fields can further alleviate the fine-tuning demands. The study also explores the role of a spectator field and its impact on the fine-tuning process. Through a comparative analysis across these models, we evaluate their parametric sensitivities and alignment with the constraints from Cosmic Microwave Background (CMB) observations. Our results indicate that although mechanisms involving a spectator field can circumvent the issue of fine-tuning parameters for PBH production, both multifield models and models with step-like features present promising alternatives. While fine-tuning involves multiple considerations, our primary objective is to evaluate various inflationary models to identify the one that most naturally explains the formation of PBHs. Hence, this study aims to the development of less constrained inflationary scenarios that align with observational data and support the theoretical possibility of PBH production.","sentences":["In this study, we present an analysis of the fine-tuning required in various inflationary models in order to explain the production of Primordial Black Holes (PBHs).","We specifically examine the degree of fine-tuning necessary in two prominent single field inflationary models: those with an inflection point and those with step-like features in the potential.","Our findings indicate that models with step-like features generally require less fine-tuning compared to those with an inflection point, making them more viable for consistent PBH production.","An interesting outcome of these models is that, in addition to improved fine-tuning, they may also predict low-frequency signals that can be detected by pulsar timing array (PTA) collaborations.","Additionally, we extend our analysis to multifield inflationary models to assess whether the integration of additional fields can further alleviate the fine-tuning demands.","The study also explores the role of a spectator field and its impact on the fine-tuning process.","Through a comparative analysis across these models, we evaluate their parametric sensitivities and alignment with the constraints from Cosmic Microwave Background (CMB) observations.","Our results indicate that although mechanisms involving a spectator field can circumvent the issue of fine-tuning parameters for PBH production, both multifield models and models with step-like features present promising alternatives.","While fine-tuning involves multiple considerations, our primary objective is to evaluate various inflationary models to identify the one that most naturally explains the formation of PBHs.","Hence, this study aims to the development of less constrained inflationary scenarios that align with observational data and support the theoretical possibility of PBH production."],"url":"http://arxiv.org/abs/2404.14321v1","category":"astro-ph.CO"}
{"created":"2024-04-22 16:31:32","title":"Bisecting masses with families of parallel hyperplanes","abstract":"We prove a common generalization to several mass partition results using hyperplane arrangements to split $\\mathbb{R}^d$ into two sets. Our main result implies the ham-sandwich theorem, the necklace splitting theorem for two thieves, a theorem about chessboard splittings with hyperplanes with fixed directions, and all known cases of Langerman's conjecture about equipartitions with $n$ hyperplanes.   Our main result also confirms an infinite number of previously unknown cases of the following conjecture of Takahashi and Sober\\'on:   For any $d+k-1$ measures in $\\mathbb{R}^d$, there exist an arrangement of $k$ parallel hyperplanes that bisects each of the measures.   The general result follows from the case of measures that are supported on a finite set with an odd number of points. The proof for this case is inspired by ideas of differential and algebraic topology, but it is a completely elementary parity argument.","sentences":["We prove a common generalization to several mass partition results using hyperplane arrangements to split $\\mathbb{R}^d$ into two sets.","Our main result implies the ham-sandwich theorem, the necklace splitting theorem for two thieves, a theorem about chessboard splittings with hyperplanes with fixed directions, and all known cases of Langerman's conjecture about equipartitions with $n$ hyperplanes.   ","Our main result also confirms an infinite number of previously unknown cases of the following conjecture of Takahashi and Sober\\'on:   For any $d+k-1$ measures in $\\mathbb{R}^d$, there exist an arrangement of $k$ parallel hyperplanes that bisects each of the measures.   ","The general result follows from the case of measures that are supported on a finite set with an odd number of points.","The proof for this case is inspired by ideas of differential and algebraic topology, but it is a completely elementary parity argument."],"url":"http://arxiv.org/abs/2404.14320v1","category":"math.CO"}
{"created":"2024-04-22 16:29:41","title":"Twisted holography on AdS$_3 \\times S^3 \\times K3$ & the planar chiral algebra","abstract":"In this work, we revisit and elaborate on twisted holography for AdS$_3 \\times S^3 \\times X$ with $X= T^4$, K3, with a particular focus on K3. We describe the twist of supergravity, identify the corresponding (generalization of) BCOV theory, and enumerate twisted supergravity states. We use this knowledge, and the technique of Koszul duality, to obtain the $N \\rightarrow \\infty$, or planar, limit of the chiral algebra of the dual CFT. The resulting symmetries are strong enough to fix planar 2 and 3-point functions in the twisted theory or, equivalently, in a 1/4-BPS subsector of the original duality. This technique can in principle be used to compute corrections to the chiral algebra perturbatively in $1/N$.","sentences":["In this work, we revisit and elaborate on twisted holography for AdS$_3 \\times S^3 \\times X$ with $X= T^4$, K3, with a particular focus on K3.","We describe the twist of supergravity, identify the corresponding (generalization of) BCOV theory, and enumerate twisted supergravity states.","We use this knowledge, and the technique of Koszul duality, to obtain the $N \\rightarrow \\infty$, or planar, limit of the chiral algebra of the dual CFT.","The resulting symmetries are strong enough to fix planar 2 and 3-point functions in the twisted theory or, equivalently, in a 1/4-BPS subsector of the original duality.","This technique can in principle be used to compute corrections to the chiral algebra perturbatively in $1/N$."],"url":"http://arxiv.org/abs/2404.14318v1","category":"hep-th"}
{"created":"2024-04-22 16:20:36","title":"Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels","abstract":"When prompting a language model (LM), users frequently expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles into a model can be resource-intensive and technically challenging, generally requiring human preference labels or examples. We introduce SAMI, a method for teaching a pretrained LM to follow behavioral principles that does not require any preference labels or demonstrations. SAMI is an iterative algorithm that finetunes a pretrained LM to increase the conditional mutual information between constitutions and self-generated responses given queries from a datasest. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a \"principle writer\" model; to avoid dependence on stronger models, we further evaluate aligning a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct). The SAMI-trained mixtral-8x7b outperforms both the initial model and the instruction-finetuned model, achieving a 65% win rate on summarization. Our results indicate that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.","sentences":["When prompting a language model (LM), users frequently expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language.","Instilling such principles into a model can be resource-intensive and technically challenging, generally requiring human preference labels or examples.","We introduce SAMI, a method for teaching a pretrained LM to follow behavioral principles that does not require any preference labels or demonstrations.","SAMI is an iterative algorithm that finetunes a pretrained LM to increase the conditional mutual information between constitutions and self-generated responses given queries from a datasest.","On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%.","Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue.","SAMI requires a \"principle writer\" model; to avoid dependence on stronger models, we further evaluate aligning a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct).","The SAMI-trained mixtral-8x7b outperforms both the initial model and the instruction-finetuned model, achieving a 65% win rate on summarization.","Our results indicate that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight."],"url":"http://arxiv.org/abs/2404.14313v1","category":"cs.CL"}
{"created":"2024-04-22 16:10:38","title":"Towards Better Adversarial Purification via Adversarial Denoising Diffusion Training","abstract":"Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks. However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support. We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness. To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness. Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations. To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution. Empirical results show that ADDT improves the robustness of DBP models. Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations.","sentences":["Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks.","However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support.","We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness.","To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness.","Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations.","To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT).","This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution.","Empirical results show that ADDT improves the robustness of DBP models.","Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations."],"url":"http://arxiv.org/abs/2404.14309v1","category":"cs.CV"}
{"created":"2024-04-22 16:10:12","title":"Extensions of discrete Helly theorems for boxes","abstract":"We prove extensions of Halman's discrete Helly theorem for axis-parallel boxes in $\\mathbb{R}^d$. Halman's theorem says that, given a set $S$ in $\\mathbb{R}^d$, if $F$ is a finite family of axis-parallel boxes such that the intersection of any $2d$ contains a point of $S$, then the intersection of $F$ contains a point of $S$. We prove colorful, fractional, and quantitative versions of Halman's theorem. For the fractional versions, it is enough to check that many $(d+1)$-tuples of the family contain points of $S$. Among the colorful versions we include variants where the coloring condition is replaced by an arbitrary matroid. Our results generalize beyond axis-parallel boxes to $H$-convex sets.","sentences":["We prove extensions of Halman's discrete Helly theorem for axis-parallel boxes in $\\mathbb{R}^d$. Halman's theorem says that, given a set $S$ in $\\mathbb{R}^d$, if $F$ is a finite family of axis-parallel boxes such that the intersection of any $2d$ contains a point of $S$, then the intersection of $F$ contains a point of $S$. We prove colorful, fractional, and quantitative versions of Halman's theorem.","For the fractional versions, it is enough to check that many $(d+1)$-tuples of the family contain points of $S$. Among the colorful versions we include variants where the coloring condition is replaced by an arbitrary matroid.","Our results generalize beyond axis-parallel boxes to $H$-convex sets."],"url":"http://arxiv.org/abs/2404.14308v1","category":"math.CO"}
{"created":"2024-04-22 16:08:52","title":"One Trillion True Random Bits Generated with a Field Programmable Gate Array Actuated Magnetic Tunnel Junction","abstract":"Large quantities of random numbers are crucial in a wide range of applications. We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses. However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates. Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs (SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation. In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ. The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography.","sentences":["Large quantities of random numbers are crucial in a wide range of applications.","We have recently demonstrated that perpendicular nanopillar magnetic tunnel junctions (pMTJs) can produce true random bits when actuated with short pulses.","However, our implementation used high-end and expensive electronics, such as a high bandwidth arbitrary waveform generator and analog-to-digital converter, and was limited to relatively low data rates.","Here, we significantly increase the speed of true random number generation (TRNG) of our stochastic actuated pMTJs","(SMART-pMTJs) using Field Programmable Gate Arrays (FPGAs), demonstrating the generation of over $10^{12}$ bits at rates exceeding 10Mb/s. The resulting bitstreams pass the NIST Statistical Test Suite for randomness with only one XOR operation.","In addition to a hundred-fold reduction in the setup cost and a thousand-fold increase in bitrate, the advancement includes simplifying and optimizing random bit generation with a custom-designed analog daughter board to interface an FPGA and SMART-pMTJ.","The resulting setup further enables FPGA at-speed processing of MTJ data for stochastic modeling and cryptography."],"url":"http://arxiv.org/abs/2404.14307v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 16:04:51","title":"Vortex nonlinear optics in monolayer van der Waals crystals","abstract":"In addition to wavelength and polarization, coherent light possesses a degree of freedom associated with its spatial topology that, when exploited through nonlinear optics, can unlock a plethora of new photonic phenomena. A prime example involves the use of vortex beams, which allow for the tuning of light's orbital angular momentum (OAM) on demand. Such processes can not only reveal emergent physics but also enable high-density classical and quantum communication paradigms by allowing access to an infinitely large set of orthogonal optical states. Nevertheless, structured nonlinear optics have failed to keep pace with the ever-present need to shrink the length-scale of optoelectronic and photonic technologies to the nanoscale regime. Here, we push the boundaries of vortex nonlinear optics to the ultimate limits of material dimensionality. By exploiting second and third-order nonlinear frequency-mixing processes in van der Waals semiconductor monolayers, we show the free manipulation of the wavelength, topological charge, and radial index of vortex light-fields. We demonstrate that such control can be supported over a broad spectral bandwidth, unconstrained by traditional limits associated with bulk nonlinear optical (NLO) materials, due to the atomically-thin nature of the host crystal. Our work breaks through traditional constraints in optics and promises to herald a new avenue for next-generation optoelectronic and photonics technologies empowered by twisted nanoscale nonlinear light-matter interactions.","sentences":["In addition to wavelength and polarization, coherent light possesses a degree of freedom associated with its spatial topology that, when exploited through nonlinear optics, can unlock a plethora of new photonic phenomena.","A prime example involves the use of vortex beams, which allow for the tuning of light's orbital angular momentum (OAM) on demand.","Such processes can not only reveal emergent physics but also enable high-density classical and quantum communication paradigms by allowing access to an infinitely large set of orthogonal optical states.","Nevertheless, structured nonlinear optics have failed to keep pace with the ever-present need to shrink the length-scale of optoelectronic and photonic technologies to the nanoscale regime.","Here, we push the boundaries of vortex nonlinear optics to the ultimate limits of material dimensionality.","By exploiting second and third-order nonlinear frequency-mixing processes in van der Waals semiconductor monolayers, we show the free manipulation of the wavelength, topological charge, and radial index of vortex light-fields.","We demonstrate that such control can be supported over a broad spectral bandwidth, unconstrained by traditional limits associated with bulk nonlinear optical (NLO) materials, due to the atomically-thin nature of the host crystal.","Our work breaks through traditional constraints in optics and promises to herald a new avenue for next-generation optoelectronic and photonics technologies empowered by twisted nanoscale nonlinear light-matter interactions."],"url":"http://arxiv.org/abs/2404.14306v1","category":"physics.optics"}
{"created":"2024-04-22 16:02:48","title":"Explaining Arguments' Strength: Unveiling the Role of Attacks and Supports (Technical Report)","abstract":"Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention. Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments. These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength. In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength. We show that RAEs satisfy several desirable properties. We also propose a probabilistic algorithm to approximate RAEs efficiently. Finally, we show the application value of RAEs in fraud detection and large language models case studies.","sentences":["Quantitatively explaining the strength of arguments under gradual semantics has recently received increasing attention.","Specifically, several works in the literature provide quantitative explanations by computing the attribution scores of arguments.","These works disregard the importance of attacks and supports, even though they play an essential role when explaining arguments' strength.","In this paper, we propose a novel theory of Relation Attribution Explanations (RAEs), adapting Shapley values from game theory to offer fine-grained insights into the role of attacks and supports in quantitative bipolar argumentation towards obtaining the arguments' strength.","We show that RAEs satisfy several desirable properties.","We also propose a probabilistic algorithm to approximate RAEs efficiently.","Finally, we show the application value of RAEs in fraud detection and large language models case studies."],"url":"http://arxiv.org/abs/2404.14304v1","category":"cs.AI"}
{"created":"2024-04-22 16:02:21","title":"A Global Minimum Tax for Large Firms Only: Implications for Tax Competition","abstract":"The Global Minimum Tax (GMT) is applied only to firms above a certain size threshold. We set up a simple model of tax competition and profit shifting by heterogeneous multinational firms to evaluate the effects of this partial coverage of the GMT. A non-haven and a haven country are bound by the GMT rate for large multinationals, but can set tax rates for firms below the threshold non-cooperatively. We show that the introduction of the GMT with a moderate tax rate increases tax revenues in both the non-haven and the haven countries. Gradual increases in the GMT rate, however, trigger a sudden change in the tax competition equilibrium from a uniform to a split corporate tax rate, at which tax revenues in the non-haven country decline. In contrast, gradual increases in the coverage of the GMT never harm the non-haven country. We also discuss the quantitative effects of introducing a $15\\%$ GMT rate in a calibrated version of our model.","sentences":["The Global Minimum Tax (GMT) is applied only to firms above a certain size threshold.","We set up a simple model of tax competition and profit shifting by heterogeneous multinational firms to evaluate the effects of this partial coverage of the GMT.","A non-haven and a haven country are bound by the GMT rate for large multinationals, but can set tax rates for firms below the threshold non-cooperatively.","We show that the introduction of the GMT with a moderate tax rate increases tax revenues in both the non-haven and the haven countries.","Gradual increases in the GMT rate, however, trigger a sudden change in the tax competition equilibrium from a uniform to a split corporate tax rate, at which tax revenues in the non-haven country decline.","In contrast, gradual increases in the coverage of the GMT never harm the non-haven country.","We also discuss the quantitative effects of introducing a $15\\%$ GMT rate in a calibrated version of our model."],"url":"http://arxiv.org/abs/2404.14302v1","category":"econ.GN"}
{"created":"2024-04-22 16:00:46","title":"Marking: Visual Grading with Highlighting Errors and Annotating Missing Bits","abstract":"In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights. Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers. We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task. We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing. The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively. We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers. Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset. We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study. Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future. The code and dataset can be found at https://github.com/luffycodes/marking.","sentences":["In this paper, we introduce \"Marking\", a novel grading task that enhances automated grading systems by performing an in-depth analysis of student responses and providing students with visual highlights.","Unlike traditional systems that provide binary scores, \"marking\" identifies and categorizes segments of the student response as correct, incorrect, or irrelevant and detects omissions from gold answers.","We introduce a new dataset meticulously curated by Subject Matter Experts specifically for this task.","We frame \"Marking\" as an extension of the Natural Language Inference (NLI) task, which is extensively explored in the field of Natural Language Processing.","The gold answer and the student response play the roles of premise and hypothesis in NLI, respectively.","We subsequently train language models to identify entailment, contradiction, and neutrality from student response, akin to NLI, and with the added dimension of identifying omissions from gold answers.","Our experimental setup involves the use of transformer models, specifically BERT and RoBERTa, and an intelligent training step using the e-SNLI dataset.","We present extensive baseline results highlighting the complexity of the \"Marking\" task, which sets a clear trajectory for the upcoming study.","Our work not only opens up new avenues for research in AI-powered educational assessment tools, but also provides a valuable benchmark for the AI in education community to engage with and improve upon in the future.","The code and dataset can be found at https://github.com/luffycodes/marking."],"url":"http://arxiv.org/abs/2404.14301v1","category":"cs.CL"}
{"created":"2024-04-22 15:54:53","title":"Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach","abstract":"Recent years have witnessed significant progress in developing deep learning-based models for automated code completion. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.","sentences":["Recent years have witnessed significant progress in developing deep learning-based models for automated code completion.","Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement.","In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model?","To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion.","In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior.","The acquired posteriors from these shadow models are subsequently employed to train a membership classifier.","Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model.","We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder).","Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively.","Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving amper space for further improvement.","Finally, we also try to explain the findings from the perspective of model memorization."],"url":"http://arxiv.org/abs/2404.14296v1","category":"cs.SE"}
{"created":"2024-04-22 15:54:26","title":"$\u039b$ polarization in very high energy heavy ion collisions as a probe of the Quark-Gluon Plasma formation and properties","abstract":"We have studied the spin polarization of $\\Lambda$ hyperons in heavy ion collisions at center-of-mass energies $\\sqrt{s_{NN}} = 200$ GeV and $\\sqrt{s_{NN}} = 5.02$ TeV carried out at RHIC and LHC colliders. We have calculated the mean spin vector at local thermodynamic equilibrium, including all known first-order terms in the gradients of the thermo-hydrodynamic fields, assuming that the hadronization hypersurface has a uniform temperature. We have also included the feed-down contributions to the polarization of $\\Lambda$ stemming from the decays of polarized $\\Sigma^*$ and $\\Sigma^0$ hyperons. The obtained results are in good agreement with the data. In general, the component of the spin vector along the global angular momentum, orthogonal to the reaction plane, shows strong sensitivity to the initial longitudinal flow velocity. Furthermore, the longitudinal component of the spin vector turns out to be very sensitive to the bulk viscosity of the plasma at the highest LHC energy. Therefore, the azimuthal dependence of spin polarization can effectively constrain the initial hydrodynamic conditions and the transport coefficients of the Quark Gluon Plasma.","sentences":["We have studied the spin polarization of $\\Lambda$ hyperons in heavy ion collisions at center-of-mass energies $\\sqrt{s_{NN}} = 200$ GeV and $\\sqrt{s_{NN}} = 5.02$ TeV carried out at RHIC and LHC colliders.","We have calculated the mean spin vector at local thermodynamic equilibrium, including all known first-order terms in the gradients of the thermo-hydrodynamic fields, assuming that the hadronization hypersurface has a uniform temperature.","We have also included the feed-down contributions to the polarization of $\\Lambda$ stemming from the decays of polarized $\\Sigma^*$ and $\\Sigma^0$ hyperons.","The obtained results are in good agreement with the data.","In general, the component of the spin vector along the global angular momentum, orthogonal to the reaction plane, shows strong sensitivity to the initial longitudinal flow velocity.","Furthermore, the longitudinal component of the spin vector turns out to be very sensitive to the bulk viscosity of the plasma at the highest LHC energy.","Therefore, the azimuthal dependence of spin polarization can effectively constrain the initial hydrodynamic conditions and the transport coefficients of the Quark Gluon Plasma."],"url":"http://arxiv.org/abs/2404.14295v1","category":"nucl-th"}
{"created":"2024-04-22 15:53:08","title":"A Survey on Efficient Inference for Large Language Models","abstract":"Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.","sentences":["Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks.","However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios.","Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference.","This paper presents a comprehensive survey of the existing literature on efficient LLM inference.","We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach.","Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization.","Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights.","Last but not least, we provide some knowledge summary and discuss future research directions."],"url":"http://arxiv.org/abs/2404.14294v1","category":"cs.CL"}
{"created":"2024-04-22 15:47:34","title":"PM2D: A parallel GPU-based code for the kinetic simulation of laser plasma instabilities in large scale plasmas","abstract":"Laser plasma instabilities (LPIs) have significant influences on the laser energy deposition efficiency, hot electron generation, and uniformity of irradiation in inertial confined fusion (ICF). In contrast to theoretical analysis of linear development of LPIs, numerical simulations play a more and more important role in revealing the complex physics of LPIs. Since LPIs are typically a three-wave coupling process, the precise kinetic simulation of LPIs requires to resolve the laser period (around one femtosecond) and laser wavelength (less than one micron). In this paper, a full wave fluid model of LPIs is constructed and numerically solved by the particle-mesh method, where the plasma is described by macro particles that can move across the mesh grids freely. Based upon this model, a two-dimensional (2D) GPU code named PM2D is developed. It can simulate the kinetic effects of LPIs self-consistently as normal particle-in-cell (PIC) codes. Moreover, as the physical model adopted in the PM2D code is specifically constructed for LPIs, the required macro particles per grid in the simulations can be largely reduced and thus overall simulation cost is considerably reduced comparing with typical PIC codes. Moreover, the numerical noise in our PM2D code is much lower, which makes it more robust than PIC codes in the simulation of LPIs for the long-time scale above 10 picoseconds. After the distributed computing is realized, our PM2D code is able to run on GPU clusters with a total mesh grids up to several billions, which meets the typical requirements for the simulations of LPIs at ICF experimental scale with reasonable cost.","sentences":["Laser plasma instabilities (LPIs) have significant influences on the laser energy deposition efficiency, hot electron generation, and uniformity of irradiation in inertial confined fusion (ICF).","In contrast to theoretical analysis of linear development of LPIs, numerical simulations play a more and more important role in revealing the complex physics of LPIs.","Since LPIs are typically a three-wave coupling process, the precise kinetic simulation of LPIs requires to resolve the laser period (around one femtosecond) and laser wavelength (less than one micron).","In this paper, a full wave fluid model of LPIs is constructed and numerically solved by the particle-mesh method, where the plasma is described by macro particles that can move across the mesh grids freely.","Based upon this model, a two-dimensional (2D) GPU code named PM2D is developed.","It can simulate the kinetic effects of LPIs self-consistently as normal particle-in-cell (PIC) codes.","Moreover, as the physical model adopted in the PM2D code is specifically constructed for LPIs, the required macro particles per grid in the simulations can be largely reduced and thus overall simulation cost is considerably reduced comparing with typical PIC codes.","Moreover, the numerical noise in our PM2D code is much lower, which makes it more robust than PIC codes in the simulation of LPIs for the long-time scale above 10 picoseconds.","After the distributed computing is realized, our PM2D code is able to run on GPU clusters with a total mesh grids up to several billions, which meets the typical requirements for the simulations of LPIs at ICF experimental scale with reasonable cost."],"url":"http://arxiv.org/abs/2404.14293v1","category":"physics.plasm-ph"}
{"created":"2024-04-22 15:42:50","title":"Methodological Reconstruction of Historical Landslide Tsunamis Using Bayesian Inference","abstract":"Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards. Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region. Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis. This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing. After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts. However, the most viable landslides may push the boundaries of geological plausibility. Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event.","sentences":["Indonesia is one of the world's most densely populated regions and lies among the epicenters of Earth's greatest natural hazards.","Effectively reducing the disaster potential of these hazards through resource allocation and preparedness first requires an analysis of the risk factors of the region.","Since destructive tsunamis present one of the most eminent dangers to coastal communities, understanding their sources and geological history is necessary to determine the potential future risk.   ","Inspired by results from Cummins et al. 2020, and previous efforts that identified source parameters for earthquake-generated tsunamis, we consider landslide-generated tsunamis.","This is done by constructing a probability distribution of potential landslide sources based on anecdotal observations of the 1852 Banda Sea tsunami, using Bayesian inference and scientific computing.","After collecting over 100,000 samples (simulating 100,000 landslide induced tsunamis), we conclude that a landslide event provides a reasonable match to the tsunami reported in the anecdotal accounts.","However, the most viable landslides may push the boundaries of geological plausibility.","Future work creating a joint landslide-earthquake model may compensate for the weaknesses associated with an individual landslide or earthquake source event."],"url":"http://arxiv.org/abs/2404.14288v1","category":"physics.geo-ph"}
{"created":"2024-04-22 15:37:08","title":"Evidence for eccentricity in the population of binary black holes observed by LIGO-Virgo-KAGRA","abstract":"Binary black holes (BBHs) in eccentric orbits produce distinct modulations the emitted gravitational waves (GWs). The measurement of orbital eccentricity can provide robust evidence for dynamical binary formation channels. We analyze 57 GW events from the first, second and third observing runs of the LIGO-Virgo-KAGRA (LVK) Collaboration using a multipolar aligned-spin inspiral-merger-ringdown waveform model with two eccentric parameters: eccentricity and relativistic anomaly. This is made computationally feasible with the machine-learning code DINGO which accelerates inference by 2-3 orders of magnitude compared to traditional inference. First, we find eccentric aligned-spin versus quasi-circular aligned-spin $\\log_{10}$ Bayes factors of 1.84 to 4.75 (depending on the glitch mitigation) for GW200129, 3.0 for GW190701 and 1.77 for GW200208_22. We measure $e_{\\text{gw}, 10Hz}$ to be $0.27_{-0.12}^{+0.10}$ to $0.17_{-0.13}^{+0.14}$ for GW200129, $0.35_{-0.11}^{+0.32}$ for GW190701 and $0.35_{-0.21}^{+0.18}$ for GW200208_22. Second, we find $\\log_{10}$ Bayes factors between the eccentric aligned-spin versus quasi-circular precessing-spin hypothesis between 1.43 and 4.92 for GW200129, 2.61 for GW190701 and 1.23 for GW200208_22. Third, our analysis does not show evidence for eccentricity in GW190521, which has an eccentric aligned-spin against quasi-circular aligned-spin $\\log_{10}$ Bayes factor of 0.04. Fourth, we estimate that if we neglect the spin-precession and use an astrophysical prior, the probability of one out of the 57 events being eccentric is greater than 99.5% or $(100 - 8.4 \\times 10^{-4})$% (depending on the glitch mitigation). Fifth, we study the impact on parameter estimation when neglecting either eccentricity or higher modes in eccentric models. These results underscore the importance of including eccentric parameters in the characterization of BBHs for GW detectors.","sentences":["Binary black holes (BBHs) in eccentric orbits produce distinct modulations the emitted gravitational waves (GWs).","The measurement of orbital eccentricity can provide robust evidence for dynamical binary formation channels.","We analyze 57 GW events from the first, second and third observing runs of the LIGO-Virgo-KAGRA (LVK) Collaboration using a multipolar aligned-spin inspiral-merger-ringdown waveform model with two eccentric parameters: eccentricity and relativistic anomaly.","This is made computationally feasible with the machine-learning code DINGO which accelerates inference by 2-3 orders of magnitude compared to traditional inference.","First, we find eccentric aligned-spin versus quasi-circular aligned-spin $\\log_{10}$ Bayes factors of 1.84 to 4.75 (depending on the glitch mitigation) for GW200129, 3.0 for GW190701 and 1.77 for GW200208_22.","We measure $e_{\\text{gw}, 10Hz}$ to be $0.27_{-0.12}^{+0.10}$ to $0.17_{-0.13}^{+0.14}$ for GW200129, $0.35_{-0.11}^{+0.32}$ for GW190701 and $0.35_{-0.21}^{+0.18}$ for GW200208_22.","Second, we find $\\log_{10}$ Bayes factors between the eccentric aligned-spin versus quasi-circular precessing-spin hypothesis between 1.43 and 4.92 for GW200129, 2.61 for GW190701 and 1.23 for GW200208_22.","Third, our analysis does not show evidence for eccentricity in GW190521, which has an eccentric aligned-spin against quasi-circular aligned-spin $\\log_{10}$ Bayes factor of 0.04.","Fourth, we estimate that if we neglect the spin-precession and use an astrophysical prior, the probability of one out of the 57 events being eccentric is greater than 99.5% or $(100 - 8.4 \\times 10^{-4})$% (depending on the glitch mitigation).","Fifth, we study the impact on parameter estimation when neglecting either eccentricity or higher modes in eccentric models.","These results underscore the importance of including eccentric parameters in the characterization of BBHs for GW detectors."],"url":"http://arxiv.org/abs/2404.14286v1","category":"gr-qc"}
{"created":"2024-04-22 15:35:33","title":"LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots","abstract":"Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities. However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences. We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics. Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations. The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller. Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner. In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences. We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences. Project page: https://donggehan.github.io/projectllmpersonalize/.","sentences":["Large language models (LLMs) have shown significant potential for robotics applications, particularly task planning, by harnessing their language comprehension and text generation capabilities.","However, in applications such as household robotics, a critical gap remains in the personalization of these models to individual user preferences.","We introduce LLM-Personalize, a novel framework with an optimization pipeline designed to personalize LLM planners for household robotics.","Our LLM-Personalize framework features an LLM planner that performs iterative planning in multi-room, partially-observable household scenarios, making use of a scene graph constructed with local observations.","The generated plan consists of a sequence of high-level actions which are subsequently executed by a controller.","Central to our approach is the optimization pipeline, which combines imitation learning and iterative self-training to personalize the LLM planner.","In particular, the imitation learning phase performs initial LLM alignment from demonstrations, and bootstraps the model to facilitate effective iterative self-training, which further explores and aligns the model to user preferences.","We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, and show that LLM-Personalize achieves more than a 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences.","Project page: https://donggehan.github.io/projectllmpersonalize/."],"url":"http://arxiv.org/abs/2404.14285v1","category":"cs.RO"}
{"created":"2024-04-22 15:28:42","title":"Co-designing a Sub-millisecond Latency Event-based Eye Tracking System with Submanifold Sparse CNN","abstract":"Eye-tracking technology is integral to numerous consumer electronics applications, particularly in the realm of virtual and augmented reality (VR/AR). These applications demand solutions that excel in three crucial aspects: low-latency, low-power consumption, and precision. Yet, achieving optimal performance across all these fronts presents a formidable challenge, necessitating a balance between sophisticated algorithms and efficient backend hardware implementations. In this study, we tackle this challenge through a synergistic software/hardware co-design of the system with an event camera. Leveraging the inherent sparsity of event-based input data, we integrate a novel sparse FPGA dataflow accelerator customized for submanifold sparse convolution neural networks (SCNN). The SCNN implemented on the accelerator can efficiently extract the embedding feature vector from each representation of event slices by only processing the non-zero activations. Subsequently, these vectors undergo further processing by a gated recurrent unit (GRU) and a fully connected layer on the host CPU to generate the eye centers. Deployment and evaluation of our system reveal outstanding performance metrics. On the Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy, 99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while only consuming 2.29 mJ per inference. Notably, our solution opens up opportunities for future eye-tracking systems. Code is available at https://github.com/CASR-HKU/ESDA/tree/eye_tracking.","sentences":["Eye-tracking technology is integral to numerous consumer electronics applications, particularly in the realm of virtual and augmented reality (VR/AR).","These applications demand solutions that excel in three crucial aspects: low-latency, low-power consumption, and precision.","Yet, achieving optimal performance across all these fronts presents a formidable challenge, necessitating a balance between sophisticated algorithms and efficient backend hardware implementations.","In this study, we tackle this challenge through a synergistic software/hardware co-design of the system with an event camera.","Leveraging the inherent sparsity of event-based input data, we integrate a novel sparse FPGA dataflow accelerator customized for submanifold sparse convolution neural networks (SCNN).","The SCNN implemented on the accelerator can efficiently extract the embedding feature vector from each representation of event slices by only processing the non-zero activations.","Subsequently, these vectors undergo further processing by a gated recurrent unit (GRU) and a fully connected layer on the host CPU to generate the eye centers.","Deployment and evaluation of our system reveal outstanding performance metrics.","On the Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy, 99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while only consuming 2.29 mJ per inference.","Notably, our solution opens up opportunities for future eye-tracking systems.","Code is available at https://github.com/CASR-HKU/ESDA/tree/eye_tracking."],"url":"http://arxiv.org/abs/2404.14279v1","category":"cs.CV"}
{"created":"2024-04-22 15:26:59","title":"Mass-radius relationships and contraction of condensed planets by cooling or despinning","abstract":"Condensed planets contract or expand as their temperature changes. With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components. However, changes in density affect pressure and gravity and, consequently, the planet's compressibility. A planet's radius is also linked to its rate of rotation. Here again, changes in pressure, gravity and compressibility are coupled. In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model. We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies. Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate. We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility). This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter. While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius. Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets.","sentences":["Condensed planets contract or expand as their temperature changes.","With the exception of the effect of phase changes, this phenomenon is generally interpreted as being solely related to the thermal expansivity of the planet's components.","However, changes in density affect pressure and gravity and, consequently, the planet's compressibility.","A planet's radius is also linked to its rate of rotation.","Here again, changes in pressure, gravity and compressibility are coupled.","In this article we clarify how the radius of a condensed planet changes with temperature and rotation, using a simple and rigorous thermodynamic model.","We consider condensed materials to obey a simple equation of state which generalizes a polytopic EoS as temperature varies.","Using this equation, we build simple models of condensed planet's interiors including exoplanets, derive their mass-radius relationships, and study the dependence of their radius with temperature and rotation rate.","We show that it depends crucially on the value of $\\rho_s g R/K_s$ ($\\rho_s$ being surface density, $g$ gravity, $R$ radius, $K_s$ surface incompressibility).","This non-dimensional number is also the ratio of the dissipation number which appears in compressible convection and the Grune\\\"isen mineralogic parameter.","While the radius of small planets depends on temperature, this is not the case for large planets with large dissipation numbers; Earth and a super-Earth like CoRoT-7b are in something of an intermediate state, with a moderately temperature-dependent radius.","Similarly, while the radius of these two planets are functions of their rotation rates, this is not the case for smaller or larger planets."],"url":"http://arxiv.org/abs/2404.14278v1","category":"astro-ph.EP"}
{"created":"2024-04-22 15:26:24","title":"A Bayesian Approach for Prioritising Driving Behaviour Investigations in Telematic Auto Insurance Policies","abstract":"Automotive insurers increasingly have access to telematic information via black-box recorders installed in the insured vehicle, and wish to identify undesirable behaviour which may signify increased risk or uninsured activities. However, identification of such behaviour with machine learning is non-trivial, and results are far from perfect, requiring human investigation to verify suspected cases. An appropriately formed priority score, generated by automated analysis of GPS data, allows underwriters to make more efficient use of their time, improving detection of the behaviour under investigation.   An example of such behaviour is the use of a privately insured vehicle for commercial purposes, such as delivering meals and parcels. We first make use of trip GPS and accelerometer data, augmented by geospatial information, to train an imperfect classifier for delivery driving on a per-trip basis. We make use of a mixture of Beta-Binomial distributions to model the propensity of a policyholder to undertake trips which result in a positive classification as being drawn from either a rare high-scoring or common low-scoring group, and learn the parameters of this model using MCMC. This model provides us with a posterior probability that any policyholder will be a regular generator of automated alerts given any number of trips and alerts. This posterior probability is converted to a priority score, which was used to select the most valuable candidates for manual investigation.   Testing over a 1-year period ranked policyholders by likelihood of commercial driving activity on a weekly basis. The top 0.9% have been reviewed at least once by the underwriters at the time of writing, and of those 99.4% have been confirmed as correctly identified, showing the approach has achieved a significant improvement in efficiency of human resource allocation compared to manual searching.","sentences":["Automotive insurers increasingly have access to telematic information via black-box recorders installed in the insured vehicle, and wish to identify undesirable behaviour which may signify increased risk or uninsured activities.","However, identification of such behaviour with machine learning is non-trivial, and results are far from perfect, requiring human investigation to verify suspected cases.","An appropriately formed priority score, generated by automated analysis of GPS data, allows underwriters to make more efficient use of their time, improving detection of the behaviour under investigation.   ","An example of such behaviour is the use of a privately insured vehicle for commercial purposes, such as delivering meals and parcels.","We first make use of trip GPS and accelerometer data, augmented by geospatial information, to train an imperfect classifier for delivery driving on a per-trip basis.","We make use of a mixture of Beta-Binomial distributions to model the propensity of a policyholder to undertake trips which result in a positive classification as being drawn from either a rare high-scoring or common low-scoring group, and learn the parameters of this model using MCMC.","This model provides us with a posterior probability that any policyholder will be a regular generator of automated alerts given any number of trips and alerts.","This posterior probability is converted to a priority score, which was used to select the most valuable candidates for manual investigation.   ","Testing over a 1-year period ranked policyholders by likelihood of commercial driving activity on a weekly basis.","The top 0.9% have been reviewed at least once by the underwriters at the time of writing, and of those 99.4% have been confirmed as correctly identified, showing the approach has achieved a significant improvement in efficiency of human resource allocation compared to manual searching."],"url":"http://arxiv.org/abs/2404.14276v1","category":"stat.ML"}
{"created":"2024-04-22 15:23:30","title":"Maximally informative feature selection using Information Imbalance: Application to COVID-19 severity prediction","abstract":"Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others. We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes. We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients. We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021. Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease. The optimal number of features, which is determined automatically, turns out to be between 10 and 15. These features can be measured at admission. The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation. Clinical insights deriving from this study are also discussed.","sentences":["Clinical databases typically include, for each patient, many heterogeneous features, for example blood exams, the clinical history before the onset of the disease, the evolution of the symptoms, the results of imaging exams, and many others.","We here propose to exploit a recently developed statistical approach, the Information Imbalance, to compare different subsets of patient features, and automatically select the set of features which is maximally informative for a given clinical purpose, especially in minority classes.","We adapt the Information Imbalance approach to work in a clinical framework, where patient features are often categorical and are generally available only for a fraction of the patients.","We apply this algorithm to a data set of ~ 1,300 patients treated for COVID-19 in Udine hospital before October 2021.","Using this approach, we find combinations of features which, if used in combination, are maximally informative of the clinical fate and of the severity of the disease.","The optimal number of features, which is determined automatically, turns out to be between 10 and 15.","These features can be measured at admission.","The approach can be used also if the features are available only for a fraction of the patients, does not require imputation and, importantly, is able to automatically select features with small inter-feature correlation.","Clinical insights deriving from this study are also discussed."],"url":"http://arxiv.org/abs/2404.14275v1","category":"stat.ME"}
{"created":"2024-04-22 15:23:28","title":"A Locally Divergence-Free Oscillation-Eliminating Discontinuous Galerkin Method for Ideal Magnetohydrodynamic Equations","abstract":"Numerical simulations of ideal compressible magnetohydrodynamic (MHD) equations are challenging, as the solutions are required to be magnetic divergence-free for general cases as well as oscillation-free for cases involving discontinuities. To overcome these difficulties, we develop a locally divergence-free oscillation-eliminating discontinuous Galerkin (LDF-OEDG) method for ideal compressible MHD equations. In the LDF-OEDG method, the numerical solution is advanced in time by using a strong stability preserving Runge-Kutta scheme. Following the solution update in each Runge-Kutta stage, an oscillation-eliminating (OE) procedure is performed to suppress spurious oscillations near discontinuities by damping the modal coefficients of the numerical solution. Subsequently, on each element, the magnetic filed of the oscillation-free DG solution is projected onto a local divergence-free space, to satisfy the divergence-free condition. The OE procedure and the LDF projection are fully decoupled from the Runge-Kutta stage update, and can be non-intrusively integrated into existing DG codes as independent modules. The damping equation of the OE procedure can be solved exactly, making the LDF-OEDG method remain stable under normal CFL conditions. These features enable a straightforward implementation of a high-order LDF-OEDG solver, which can be used to efficiently simulate the ideal compressible MHD equations. Numerical results for benchmark cases demonstrate the high-order accuracy, strong shock capturing capability and robustness of the LDF-OEDG method.","sentences":["Numerical simulations of ideal compressible magnetohydrodynamic (MHD) equations are challenging, as the solutions are required to be magnetic divergence-free for general cases as well as oscillation-free for cases involving discontinuities.","To overcome these difficulties, we develop a locally divergence-free oscillation-eliminating discontinuous Galerkin (LDF-OEDG) method for ideal compressible MHD equations.","In the LDF-OEDG method, the numerical solution is advanced in time by using a strong stability preserving Runge-Kutta scheme.","Following the solution update in each Runge-Kutta stage, an oscillation-eliminating (OE) procedure is performed to suppress spurious oscillations near discontinuities by damping the modal coefficients of the numerical solution.","Subsequently, on each element, the magnetic filed of the oscillation-free DG solution is projected onto a local divergence-free space, to satisfy the divergence-free condition.","The OE procedure and the LDF projection are fully decoupled from the Runge-Kutta stage update, and can be non-intrusively integrated into existing DG codes as independent modules.","The damping equation of the OE procedure can be solved exactly, making the LDF-OEDG method remain stable under normal CFL conditions.","These features enable a straightforward implementation of a high-order LDF-OEDG solver, which can be used to efficiently simulate the ideal compressible MHD equations.","Numerical results for benchmark cases demonstrate the high-order accuracy, strong shock capturing capability and robustness of the LDF-OEDG method."],"url":"http://arxiv.org/abs/2404.14274v1","category":"math.NA"}
{"created":"2024-04-22 15:12:55","title":"Superoscillations in High Energy Physics and Gravity","abstract":"We explore superoscillations within the context of classical and quantum field theories, presenting novel solutions to Klein-Gordon's, Dirac's, Maxwell's and Einstein's equations. In particular, we illustrate a procedure of second quantization of fields and how to construct a Fock space which encompasses Superoscillating states. Furthermore, we extend the application of superoscillations to quantum tunnelings, scatterings and mixings of particles, squeezed states and potential advancements in laser interferometry, which could open new avenues for experimental tests of Quantum Gravity effects. By delving into the relationship among superoscillations and phenomena such as Hawking radiation, the Black Hole (BH) information and the Firewall paradox, we propose an alternative mechanism for information transfer across the BH event horizon.","sentences":["We explore superoscillations within the context of classical and quantum field theories, presenting novel solutions to Klein-Gordon's, Dirac's, Maxwell's and Einstein's equations.","In particular, we illustrate a procedure of second quantization of fields and how to construct a Fock space which encompasses Superoscillating states.","Furthermore, we extend the application of superoscillations to quantum tunnelings, scatterings and mixings of particles, squeezed states and potential advancements in laser interferometry, which could open new avenues for experimental tests of Quantum Gravity effects.","By delving into the relationship among superoscillations and phenomena such as Hawking radiation, the Black Hole (BH) information and the Firewall paradox, we propose an alternative mechanism for information transfer across the BH event horizon."],"url":"http://arxiv.org/abs/2404.14266v1","category":"hep-th"}
{"created":"2024-04-22 15:12:47","title":"Deep Learning as Ricci Flow","abstract":"Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data. It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications. While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required. Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology. To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems. By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set. Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning.","sentences":["Deep neural networks (DNNs) are powerful tools for approximating the distribution of complex data.","It is known that data passing through a trained DNN classifier undergoes a series of geometric and topological simplifications.","While some progress has been made toward understanding these transformations in neural networks with smooth activation functions, an understanding in the more general setting of non-smooth activation functions, such as the rectified linear unit (ReLU), which tend to perform better, is required.","Here we propose that the geometric transformations performed by DNNs during classification tasks have parallels to those expected under Hamilton's Ricci flow - a tool from differential geometry that evolves a manifold by smoothing its curvature, in order to identify its topology.","To illustrate this idea, we present a computational framework to quantify the geometric changes that occur as data passes through successive layers of a DNN, and use this framework to motivate a notion of `global Ricci network flow' that can be used to assess a DNN's ability to disentangle complex data geometries to solve classification problems.","By training more than $1,500$ DNN classifiers of different widths and depths on synthetic and real-world data, we show that the strength of global Ricci network flow-like behaviour correlates with accuracy for well-trained DNNs, independently of depth, width and data set.","Our findings motivate the use of tools from differential and discrete geometry to the problem of explainability in deep learning."],"url":"http://arxiv.org/abs/2404.14265v1","category":"cs.LG"}
{"created":"2024-04-22 15:11:25","title":"Photon distillation schemes with reduced resource costs based on multiphoton Fourier interference","abstract":"Improving the indistinguishability of single photons is a crucial prerequisite for achieving large-scale photonic quantum computation. Photon distillation uses quantum interference to enhance the quality of single photons, sacrificing multiple photons to generate one photon with enhanced indistinguishability. By studying multiphoton interference in Fourier matrices, we find photon distillation schemes that require fewer photons to achieve the same improvement in indistinguishability, compared to the state of the art. These results may find application as a component in large-scale photonic quantum computers.","sentences":["Improving the indistinguishability of single photons is a crucial prerequisite for achieving large-scale photonic quantum computation.","Photon distillation uses quantum interference to enhance the quality of single photons, sacrificing multiple photons to generate one photon with enhanced indistinguishability.","By studying multiphoton interference in Fourier matrices, we find photon distillation schemes that require fewer photons to achieve the same improvement in indistinguishability, compared to the state of the art.","These results may find application as a component in large-scale photonic quantum computers."],"url":"http://arxiv.org/abs/2404.14262v1","category":"quant-ph"}
{"created":"2024-04-22 15:07:57","title":"Quantum-Enhanced Neural Exchange-Correlation Functionals","abstract":"Kohn-Sham Density Functional Theory (KS-DFT) provides the exact ground state energy and electron density of a molecule, contingent on the as-yet-unknown universal exchange-correlation (XC) functional. Recent research has demonstrated that neural networks can efficiently learn to represent approximations to that functional, offering accurate generalizations to molecules not present during the training process. With the latest advancements in quantum-enhanced machine learning (ML), evidence is growing that Quantum Neural Network (QNN) models may offer advantages in ML applications. In this work, we explore the use of QNNs for representing XC functionals, enhancing and comparing them to classical ML techniques. We present QNNs based on differentiable quantum circuits (DQCs) as quantum (hybrid) models for XC in KS-DFT, implemented across various architectures. We assess their performance on 1D and 3D systems. To that end, we expand existing differentiable KS-DFT frameworks and propose strategies for efficient training of such functionals, highlighting the importance of fractional orbital occupation for accurate results. Our best QNN-based XC functional yields energy profiles of the H$_2$ and planar H$_4$ molecules that deviate by no more than 1 mHa from the reference DMRG and FCI/6-31G results, respectively. Moreover, they reach chemical precision on a system, H$_2$H$_2$, not present in the training dataset, using only a few variational parameters. This work lays the foundation for the integration of quantum models in KS-DFT, thereby opening new avenues for expressing XC functionals in a differentiable way and facilitating computations of various properties.","sentences":["Kohn-Sham Density Functional Theory (KS-DFT) provides the exact ground state energy and electron density of a molecule, contingent on the as-yet-unknown universal exchange-correlation (XC) functional.","Recent research has demonstrated that neural networks can efficiently learn to represent approximations to that functional, offering accurate generalizations to molecules not present during the training process.","With the latest advancements in quantum-enhanced machine learning (ML), evidence is growing that Quantum Neural Network (QNN) models may offer advantages in ML applications.","In this work, we explore the use of QNNs for representing XC functionals, enhancing and comparing them to classical ML techniques.","We present QNNs based on differentiable quantum circuits (DQCs) as quantum (hybrid) models for XC in KS-DFT, implemented across various architectures.","We assess their performance on 1D and 3D systems.","To that end, we expand existing differentiable KS-DFT frameworks and propose strategies for efficient training of such functionals, highlighting the importance of fractional orbital occupation for accurate results.","Our best QNN-based XC functional yields energy profiles of the H$_2$ and planar H$_4$ molecules that deviate by no more than 1 mHa from the reference DMRG and FCI/6-31G results, respectively.","Moreover, they reach chemical precision on a system, H$_2$H$_2$, not present in the training dataset, using only a few variational parameters.","This work lays the foundation for the integration of quantum models in KS-DFT, thereby opening new avenues for expressing XC functionals in a differentiable way and facilitating computations of various properties."],"url":"http://arxiv.org/abs/2404.14258v1","category":"quant-ph"}
{"created":"2024-04-22 15:03:45","title":"On a fundamental statistical edge principle","abstract":"This paper establishes that conditioning the probability of execution of new orders on the self-generated historical trading information (HTI) of a trading strategy is a necessary condition for a statistical trading edge. It is shown, in particular, that, given any trading strategy S that does not use its own HTI, it is always possible to construct a new strategy S* that yields a systematically increasing improvement over S in terms of profit and loss (PnL) by using the self-generated HTI. This holds true under rather general conditions that are frequently met in practice, and it is proven through a decision mechanism specifically designed to formally prove this idea. Simulations and real-world trading evidence are included for validation and illustration, respectively.","sentences":["This paper establishes that conditioning the probability of execution of new orders on the self-generated historical trading information (HTI) of a trading strategy is a necessary condition for a statistical trading edge.","It is shown, in particular, that, given any trading strategy S that does not use its own HTI, it is always possible to construct a new strategy S* that yields a systematically increasing improvement over S in terms of profit and loss (PnL) by using the self-generated HTI.","This holds true under rather general conditions that are frequently met in practice, and it is proven through a decision mechanism specifically designed to formally prove this idea.","Simulations and real-world trading evidence are included for validation and illustration, respectively."],"url":"http://arxiv.org/abs/2404.14252v1","category":"q-fin.PM"}
{"created":"2024-04-22 15:01:12","title":"NTIRE 2024 Challenge on Low Light Image Enhancement: Methods and Results","abstract":"This paper reviews the NTIRE 2024 low light image enhancement challenge, highlighting the proposed solutions and results. The aim of this challenge is to discover an effective network design or solution capable of generating brighter, clearer, and visually appealing results when dealing with a variety of conditions, including ultra-high resolution (4K and beyond), non-uniform illumination, backlighting, extreme darkness, and night scenes. A notable total of 428 participants registered for the challenge, with 22 teams ultimately making valid submissions. This paper meticulously evaluates the state-of-the-art advancements in enhancing low-light images, reflecting the significant progress and creativity in this field.","sentences":["This paper reviews the NTIRE 2024 low light image enhancement challenge, highlighting the proposed solutions and results.","The aim of this challenge is to discover an effective network design or solution capable of generating brighter, clearer, and visually appealing results when dealing with a variety of conditions, including ultra-high resolution (4K and beyond), non-uniform illumination, backlighting, extreme darkness, and night scenes.","A notable total of 428 participants registered for the challenge, with 22 teams ultimately making valid submissions.","This paper meticulously evaluates the state-of-the-art advancements in enhancing low-light images, reflecting the significant progress and creativity in this field."],"url":"http://arxiv.org/abs/2404.14248v1","category":"cs.CV"}
{"created":"2024-04-22 14:57:17","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images","abstract":"Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future.","sentences":["Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media.","One notable consequence is the use of AI-generated images for fake profiles on social media.","While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking.","In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter.","We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline.","Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform.","We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior.","The results also reveal several motives, including spamming and political amplification campaigns.","Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future."],"url":"http://arxiv.org/abs/2404.14244v1","category":"cs.CR"}
{"created":"2024-04-22 14:56:36","title":"Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast Recommendation","abstract":"A series of graph filtering (GF)-based collaborative filtering (CF) showcases state-of-the-art performance on the recommendation accuracy by using a low-pass filter (LPF) without a training process. However, conventional GF-based CF approaches mostly perform matrix decomposition on the item-item similarity graph to realize the ideal LPF, which results in a non-trivial computational cost and thus makes them less practical in scenarios where rapid recommendations are essential. In this paper, we propose Turbo-CF, a GF-based CF method that is both training-free and matrix decomposition-free. Turbo-CF employs a polynomial graph filter to circumvent the issue of expensive matrix decompositions, enabling us to make full use of modern computer hardware components (i.e., GPU). Specifically, Turbo-CF first constructs an item-item similarity graph whose edge weights are effectively regulated. Then, our own polynomial LPFs are designed to retain only low-frequency signals without explicit matrix decompositions. We demonstrate that Turbo-CF is extremely fast yet accurate, achieving a runtime of less than 1 second on real-world benchmark datasets while achieving recommendation accuracies comparable to best competitors.","sentences":["A series of graph filtering (GF)-based collaborative filtering (CF) showcases state-of-the-art performance on the recommendation accuracy by using a low-pass filter (LPF) without a training process.","However, conventional GF-based CF approaches mostly perform matrix decomposition on the item-item similarity graph to realize the ideal LPF, which results in a non-trivial computational cost and thus makes them less practical in scenarios where rapid recommendations are essential.","In this paper, we propose Turbo-CF, a GF-based CF method that is both training-free and matrix decomposition-free.","Turbo-CF employs a polynomial graph filter to circumvent the issue of expensive matrix decompositions, enabling us to make full use of modern computer hardware components (i.e., GPU).","Specifically, Turbo-CF first constructs an item-item similarity graph whose edge weights are effectively regulated.","Then, our own polynomial LPFs are designed to retain only low-frequency signals without explicit matrix decompositions.","We demonstrate that Turbo-CF is extremely fast yet accurate, achieving a runtime of less than 1 second on real-world benchmark datasets while achieving recommendation accuracies comparable to best competitors."],"url":"http://arxiv.org/abs/2404.14243v1","category":"cs.IR"}
{"created":"2024-04-22 14:56:01","title":"Large-scale photonic chip based pulse interleaver for low-noise microwave generation","abstract":"Microwaves generated by optical techniques have demonstrated unprecedentedly low noise and hold significance in various applications such as communication, radar, instrumentation, and metrology. To date, the purest microwave signals are generated using optical frequency division with femtosecond mode-locked lasers. However, many femtosecond laser combs have a radio frequency (RF) repetition rate in the hundreds of megahertz range, necessitating methods to translate the generated low-noise RF signal to the microwave domain. Benchtop pulse interleavers can multiply the pulse repetition rate, avoid saturation of photodetectors, and facilitate the generation of high-power low-noise microwave signals, which have to date only been demonstrated using optical fibers or free space optics. Here, we introduce a large-scale photonic integrated circuit-based interleaver, offering size reduction and enhanced stability. The all-on-chip interleaver attains a 64-fold multiplication of the repetition rate, directly translated from 216 MHz to 14 GHz in microwave Ku-Band. By overcoming photodetector saturation, the generated microwave power was improved by 36 dB, with a phase noise floor reduced by more than 10 folds to -160 dBc/Hz on the 14 GHz carrier. The device is based on a low-loss and high-density photonic integrated circuit fabricated by the photonic Damascene process. Six cascaded stages of Mach-Zehnder interferometers with optical delay lines up to 33 centimeters long are fully integrated into a compact footprint of 8.5 mmx1.7 mm. The lithographically defined precision of the optical waveguide path length enables the scaling up of the interleaved frequency to millimeter-wave bands, which is challenging the fiber-based counterparts. This interleaver has the potential to reduce the cost and footprint of mode-locked-laser-based microwave generation, allowing for field deployment.","sentences":["Microwaves generated by optical techniques have demonstrated unprecedentedly low noise and hold significance in various applications such as communication, radar, instrumentation, and metrology.","To date, the purest microwave signals are generated using optical frequency division with femtosecond mode-locked lasers.","However, many femtosecond laser combs have a radio frequency (RF) repetition rate in the hundreds of megahertz range, necessitating methods to translate the generated low-noise RF signal to the microwave domain.","Benchtop pulse interleavers can multiply the pulse repetition rate, avoid saturation of photodetectors, and facilitate the generation of high-power low-noise microwave signals, which have to date only been demonstrated using optical fibers or free space optics.","Here, we introduce a large-scale photonic integrated circuit-based interleaver, offering size reduction and enhanced stability.","The all-on-chip interleaver attains a 64-fold multiplication of the repetition rate, directly translated from 216 MHz to 14 GHz in microwave Ku-Band.","By overcoming photodetector saturation, the generated microwave power was improved by 36 dB, with a phase noise floor reduced by more than 10 folds to -160 dBc/Hz on the 14 GHz carrier.","The device is based on a low-loss and high-density photonic integrated circuit fabricated by the photonic Damascene process.","Six cascaded stages of Mach-Zehnder interferometers with optical delay lines up to 33 centimeters long are fully integrated into a compact footprint of 8.5 mmx1.7 mm.","The lithographically defined precision of the optical waveguide path length enables the scaling up of the interleaved frequency to millimeter-wave bands, which is challenging the fiber-based counterparts.","This interleaver has the potential to reduce the cost and footprint of mode-locked-laser-based microwave generation, allowing for field deployment."],"url":"http://arxiv.org/abs/2404.14242v1","category":"physics.optics"}
{"created":"2024-04-22 14:53:27","title":"UrbanCross: Enhancing Satellite Image-Text Retrieval with Cross-Domain Adaptation","abstract":"Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications. However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains. To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval. UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity. It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance. Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains. Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap.","sentences":["Urbanization challenges underscore the necessity for effective satellite image-text retrieval methods to swiftly access specific information enriched with geographic semantics for urban applications.","However, existing methods often overlook significant domain gaps across diverse urban landscapes, primarily focusing on enhancing retrieval performance within single domains.","To tackle this issue, we present UrbanCross, a new framework for cross-domain satellite image-text retrieval.","UrbanCross leverages a high-quality, cross-domain dataset enriched with extensive geo-tags from three countries to highlight domain diversity.","It employs the Large Multimodal Model (LMM) for textual refinement and the Segment Anything Model (SAM) for visual augmentation, achieving a fine-grained alignment of images, segments and texts, yielding a 10% improvement in retrieval performance.","Additionally, UrbanCross incorporates an adaptive curriculum-based source sampler and a weighted adversarial cross-domain fine-tuning module, progressively enhancing adaptability across various domains.","Extensive experiments confirm UrbanCross's superior efficiency in retrieval and adaptation to new urban environments, demonstrating an average performance increase of 15% over its version without domain adaptation mechanisms, effectively bridging the domain gap."],"url":"http://arxiv.org/abs/2404.14241v1","category":"cs.CV"}
{"created":"2024-04-22 14:49:46","title":"Collaborative Filtering Based on Diffusion Models: Unveiling the Potential of High-Order Connectivity","abstract":"A recent study has shown that diffusion models are well-suited for modeling the generative process of user-item interactions in recommender systems due to their denoising nature. However, existing diffusion model-based recommender systems do not explicitly leverage high-order connectivities that contain crucial collaborative signals for accurate recommendations. Addressing this gap, we propose CF-Diff, a new diffusion model-based collaborative filtering (CF) method, which is capable of making full use of collaborative signals along with multi-hop neighbors. Specifically, the forward-diffusion process adds random noise to user-item interactions, while the reverse-denoising process accommodates our own learning model, named cross-attention-guided multi-hop autoencoder (CAM-AE), to gradually recover the original user-item interactions. CAM-AE consists of two core modules: 1) the attention-aided AE module, responsible for precisely learning latent representations of user-item interactions while preserving the model's complexity at manageable levels, and 2) the multi-hop cross-attention module, which judiciously harnesses high-order connectivity information to capture enhanced collaborative signals. Through comprehensive experiments on three real-world datasets, we demonstrate that CF-Diff is (a) Superior: outperforming benchmark recommendation methods, achieving remarkable gains up to 7.29% compared to the best competitor, (b) Theoretically-validated: reducing computations while ensuring that the embeddings generated by our model closely approximate those from the original cross-attention, and (c) Scalable: proving the computational efficiency that scales linearly with the number of users or items.","sentences":["A recent study has shown that diffusion models are well-suited for modeling the generative process of user-item interactions in recommender systems due to their denoising nature.","However, existing diffusion model-based recommender systems do not explicitly leverage high-order connectivities that contain crucial collaborative signals for accurate recommendations.","Addressing this gap, we propose CF-Diff, a new diffusion model-based collaborative filtering (CF) method, which is capable of making full use of collaborative signals along with multi-hop neighbors.","Specifically, the forward-diffusion process adds random noise to user-item interactions, while the reverse-denoising process accommodates our own learning model, named cross-attention-guided multi-hop autoencoder (CAM-AE), to gradually recover the original user-item interactions.","CAM-AE consists of two core modules: 1) the attention-aided AE module, responsible for precisely learning latent representations of user-item interactions while preserving the model's complexity at manageable levels, and 2) the multi-hop cross-attention module, which judiciously harnesses high-order connectivity information to capture enhanced collaborative signals.","Through comprehensive experiments on three real-world datasets, we demonstrate that CF-Diff is (a) Superior: outperforming benchmark recommendation methods, achieving remarkable gains up to 7.29% compared to the best competitor, (b) Theoretically-validated: reducing computations while ensuring that the embeddings generated by our model closely approximate those from the original cross-attention, and (c) Scalable: proving the computational efficiency that scales linearly with the number of users or items."],"url":"http://arxiv.org/abs/2404.14240v1","category":"cs.IR"}
{"created":"2024-04-22 14:47:54","title":"MultiBooth: Towards Generating All Your Concepts in an Image from Text","abstract":"This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/","sentences":["This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text.","Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost.","MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase.","During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept.","In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map.","This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images.","This strategy not only improves concept fidelity but also reduces additional inference cost.","MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency.","Project Page: https://multibooth.github.io/"],"url":"http://arxiv.org/abs/2404.14239v1","category":"cs.CV"}
{"created":"2024-04-22 14:47:42","title":"Beyond the Edge: An Advanced Exploration of Reinforcement Learning for Mobile Edge Computing, its Applications, and Future Research Trajectories","abstract":"Mobile Edge Computing (MEC) broadens the scope of computation and storage beyond the central network, incorporating edge nodes close to end devices. This expansion facilitates the implementation of large-scale \"connected things\" within edge networks. The advent of applications necessitating real-time, high-quality service presents several challenges, such as low latency, high data rate, reliability, efficiency, and security, all of which demand resolution. The incorporation of reinforcement learning (RL) methodologies within MEC networks promotes a deeper understanding of mobile user behaviors and network dynamics, thereby optimizing resource use in computing and communication processes. This paper offers an exhaustive survey of RL applications in MEC networks, initially presenting an overview of RL from its fundamental principles to the latest advanced frameworks. Furthermore, it outlines various RL strategies employed in offloading, caching, and communication within MEC networks. Finally, it explores open issues linked with software and hardware platforms, representation, RL robustness, safe RL, large-scale scheduling, generalization, security, and privacy. The paper proposes specific RL techniques to mitigate these issues and provides insights into their practical applications.","sentences":["Mobile Edge Computing (MEC) broadens the scope of computation and storage beyond the central network, incorporating edge nodes close to end devices.","This expansion facilitates the implementation of large-scale \"connected things\" within edge networks.","The advent of applications necessitating real-time, high-quality service presents several challenges, such as low latency, high data rate, reliability, efficiency, and security, all of which demand resolution.","The incorporation of reinforcement learning (RL) methodologies within MEC networks promotes a deeper understanding of mobile user behaviors and network dynamics, thereby optimizing resource use in computing and communication processes.","This paper offers an exhaustive survey of RL applications in MEC networks, initially presenting an overview of RL from its fundamental principles to the latest advanced frameworks.","Furthermore, it outlines various RL strategies employed in offloading, caching, and communication within MEC networks.","Finally, it explores open issues linked with software and hardware platforms, representation, RL robustness, safe RL, large-scale scheduling, generalization, security, and privacy.","The paper proposes specific RL techniques to mitigate these issues and provides insights into their practical applications."],"url":"http://arxiv.org/abs/2404.14238v1","category":"cs.NI"}
{"created":"2024-04-22 14:47:35","title":"Constraining the emergent dark energy models with cosmology-independent observational data","abstract":"In this work, we investigate the phenomenologically emergent dark energy (PEDE) model and its generalized form, namely the generalized emergent dark energy (GEDE) model, which introduces a free parameter \\unboldmath {\\( \\Delta \\)} that can discriminate between the \\unboldmath{$\\mathrm{\\Lambda}$}CDM model and the PEDE model. Fitting the emergent dark energy (EDE) models with the observational datasets including the cosmology-independent gamma-ray bursts (GRBs) at high-redshift and the observational Hubble data (OHD), we find a large value of $H_0$ which is close to the results of local measurement of $H_0$ from the SH0ES Collaboration in both EDE models. These results suggest that PEDE and GEDE models can %serve as an important supplement and be possible alternative to the standard cosmological model, pending further theoretical explorations and observational verifications.","sentences":["In this work, we investigate the phenomenologically emergent dark energy (PEDE) model and its generalized form, namely the generalized emergent dark energy (GEDE) model, which introduces a free parameter \\unboldmath {\\( \\Delta \\)} that can discriminate between the \\unboldmath{$\\mathrm{\\Lambda}$}CDM model and the PEDE model.","Fitting the emergent dark energy (EDE) models with the observational datasets including the cosmology-independent gamma-ray bursts (GRBs) at high-redshift and the observational Hubble data (OHD), we find a large value of $H_0$ which is close to the results of local measurement of $H_0$ from the SH0ES Collaboration in both EDE models.","These results suggest that PEDE and GEDE models can %serve as an important supplement and be possible alternative to the standard cosmological model, pending further theoretical explorations and observational verifications."],"url":"http://arxiv.org/abs/2404.14237v1","category":"astro-ph.CO"}
{"created":"2024-04-22 14:46:30","title":"Computing the LCP Array of a Labeled Graph","abstract":"The LCP array is an important tool in stringology, allowing to speed up pattern matching algorithms and enabling compact representations of the suffix tree. Recently, Conte et al. [DCC 2023] and Cotumaccio et al. [SPIRE 2023] extended the definition of this array to Wheeler DFAs and, ultimately, to arbitrary labeled graphs, proving that it can be used to efficiently solve matching statistics queries on the graph's paths. In this paper, we provide the first efficient algorithm building the LCP array of a directed labeled graph with $n$ nodes and $m$ edges labeled over an alphabet of size $\\sigma$. After arguing that the natural generalization of a compact-space LCP-construction algorithm by Beller et al. [J. Discrete Algorithms 2013] runs in time $\\Omega(n\\sigma)$, we present a new algorithm based on dynamic range stabbing building the LCP array in $O(n\\log \\sigma)$ time and $O(n\\log\\sigma)$ bits of working space.","sentences":["The LCP array is an important tool in stringology, allowing to speed up pattern matching algorithms and enabling compact representations of the suffix tree.","Recently, Conte et al.","[DCC 2023] and Cotumaccio et al.","[SPIRE 2023] extended the definition of this array to Wheeler DFAs and, ultimately, to arbitrary labeled graphs, proving that it can be used to efficiently solve matching statistics queries on the graph's paths.","In this paper, we provide the first efficient algorithm building the LCP array of a directed labeled graph with $n$ nodes and $m$ edges labeled over an alphabet of size $\\sigma$. After arguing that the natural generalization of a compact-space LCP-construction algorithm by Beller et al.","[J. Discrete Algorithms 2013] runs in time $\\Omega(n\\sigma)$, we present a new algorithm based on dynamic range stabbing building the LCP array in $O(n\\log \\sigma)$ time and $O(n\\log\\sigma)$ bits of working space."],"url":"http://arxiv.org/abs/2404.14235v1","category":"cs.DS"}
{"created":"2024-04-22 14:46:10","title":"Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback","abstract":"The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.","sentences":["The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs.","Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts).","To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback.","The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship).","Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model.","Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning.","Extensive experiments demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.14233v1","category":"cs.CV"}
{"created":"2024-04-22 14:45:30","title":"Shifting Focus with HCEye: Exploring the Dynamics of Visual Highlighting and Cognitive Load on User Attention and Saliency Prediction","abstract":"Visual highlighting can guide user attention in complex interfaces. However, its effectiveness under limited attentional capacities is underexplored. This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour. Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing. The presence of these factors significantly alters what people attend to and thus what is salient. Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads. Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking.","sentences":["Visual highlighting can guide user attention in complex interfaces.","However, its effectiveness under limited attentional capacities is underexplored.","This paper examines the joint impact of visual highlighting (permanent and dynamic) and dual-task-induced cognitive load on gaze behaviour.","Our analysis, using eye-movement data from 27 participants viewing 150 unique webpages reveals that while participants' ability to attend to UI elements decreases with increasing cognitive load, dynamic adaptations (i.e., highlighting) remain attention-grabbing.","The presence of these factors significantly alters what people attend to and thus what is salient.","Accordingly, we show that state-of-the-art saliency models increase their performance when accounting for different cognitive loads.","Our empirical insights, along with our openly available dataset, enhance our understanding of attentional processes in UIs under varying cognitive (and perceptual) loads and open the door for new models that can predict user attention while multitasking."],"url":"http://arxiv.org/abs/2404.14232v1","category":"cs.HC"}
{"created":"2024-04-22 14:41:39","title":"Resistance Against Manipulative AI: key factors and possible actions","abstract":"If AI is the new electricity, what should we do to keep ourselves from getting electrocuted? In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions. We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential. We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements. Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary. Based on these experiments, we discuss two actions that can protect us from LLM manipulation. In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences. We also propose an ad hoc solution, a classifier that detects manipulation of LLMs - a Manipulation Fuse.","sentences":["If AI is the new electricity, what should we do to keep ourselves from getting electrocuted?","In this work, we explore factors related to the potential of large language models (LLMs) to manipulate human decisions.","We describe the results of two experiments designed to determine what characteristics of humans are associated with their susceptibility to LLM manipulation, and what characteristics of LLMs are associated with their manipulativeness potential.","We explore human factors by conducting user studies in which participants answer general knowledge questions using LLM-generated hints, whereas LLM factors by provoking language models to create manipulative statements.","Then, we analyze their obedience, the persuasion strategies used, and the choice of vocabulary.","Based on these experiments, we discuss two actions that can protect us from LLM manipulation.","In the long term, we put AI literacy at the forefront, arguing that educating society would minimize the risk of manipulation and its consequences.","We also propose an ad hoc solution, a classifier that detects manipulation of LLMs - a Manipulation Fuse."],"url":"http://arxiv.org/abs/2404.14230v1","category":"cs.HC"}
{"created":"2024-04-22 14:40:30","title":"Colored Stochastic Multiplicative Processes with Additive Noise Unveil a Third-Order PDE, Defying Conventional FPE and Fick-Law Paradigms","abstract":"Research on stochastic differential equations (SDE) involving both additive and multiplicative noise has been extensive. In situations where the primary process is driven by a multiplicative stochastic process, additive white noise typically represents an intrinsic and unavoidable fast factor, including phenomena like thermal fluctuations, inherent uncertainties in measurement processes, or rapid wind forcing in ocean dynamics. This work focuses on a significant class of such systems, particularly those characterized by linear drift and multiplicative noise, extensively explored in the literature. Conventionally, multiplicative stochastic processes are also treated as white noise in existing studies. However, when considering colored multiplicative noise, the emphasis has been on characterizing the far tails of the probability density function (PDF), regardless of the spectral properties of the noise. In the absence of additive noise and with a general colored multiplicative SDE, standard perturbation approaches lead to a second-order PDE known as the Fokker-Planck Equation (FPE), consistent with Fick's law. This investigation unveils a notable departure from this standard behavior when introducing additive white noise. At the leading order of the stochastic process strength, perturbation approaches yield a \\textit{third-order PDE}, irrespective of the white noise intensity. The breakdown of the FPE further signifies the breakdown of Fick's law. Additionally, we derive the explicit solution for the equilibrium PDF corresponding to this third-order PDE Master Equation. Through numerical simulations, we demonstrate significant deviations from outcomes derived using the FPE obtained through the application of Fick's law.","sentences":["Research on stochastic differential equations (SDE) involving both additive and multiplicative noise has been extensive.","In situations where the primary process is driven by a multiplicative stochastic process, additive white noise typically represents an intrinsic and unavoidable fast factor, including phenomena like thermal fluctuations, inherent uncertainties in measurement processes, or rapid wind forcing in ocean dynamics.","This work focuses on a significant class of such systems, particularly those characterized by linear drift and multiplicative noise, extensively explored in the literature.","Conventionally, multiplicative stochastic processes are also treated as white noise in existing studies.","However, when considering colored multiplicative noise, the emphasis has been on characterizing the far tails of the probability density function (PDF), regardless of the spectral properties of the noise.","In the absence of additive noise and with a general colored multiplicative SDE, standard perturbation approaches lead to a second-order PDE known as the Fokker-Planck Equation (FPE), consistent with Fick's law.","This investigation unveils a notable departure from this standard behavior when introducing additive white noise.","At the leading order of the stochastic process strength, perturbation approaches yield a \\textit{third-order PDE}, irrespective of the white noise intensity.","The breakdown of the FPE further signifies the breakdown of Fick's law.","Additionally, we derive the explicit solution for the equilibrium PDF corresponding to this third-order PDE Master Equation.","Through numerical simulations, we demonstrate significant deviations from outcomes derived using the FPE obtained through the application of Fick's law."],"url":"http://arxiv.org/abs/2404.14229v1","category":"math.ST"}
{"created":"2024-04-22 14:37:33","title":"Estimation for SLS models: finite sample guarantees","abstract":"This note continues and extends the study from Spokoiny (2023a) about estimation for parametric models with possibly large or even infinite parameter dimension. We consider a special class of stochastically linear smooth (SLS) models satisfying three major conditions: the stochastic component of the log-likelihood is linear in the model parameter, while the expected log-likelihood is a smooth and concave function. For the penalized maximum likelihood estimators (pMLE), we establish several finite sample bounds about its concentration and large deviations as well as the Fisher and Wilks expansions and risk bounds. In all results, the remainder is given explicitly and can be evaluated in terms of the effective sample size $ n $ and effective parameter dimension $ \\mathbb{p} $ which allows us to identify the so-called \\emph{critical parameter dimension}. The results are also dimension and coordinate-free. Despite generality, all the presented bounds are nearly sharp and the classical asymptotic results can be obtained as simple corollaries. Our results indicate that the use of advanced fourth-order expansions allows to relax the critical dimension condition $ \\mathbb{p}^{3} \\ll n $ from Spokoiny (2023a) to $ \\mathbb{p}^{3/2} \\ll n $. Examples for classical models like logistic regression, log-density and precision matrix estimation illustrate the applicability of general results.","sentences":["This note continues and extends the study from Spokoiny (2023a) about estimation for parametric models with possibly large or even infinite parameter dimension.","We consider a special class of stochastically linear smooth (SLS) models satisfying three major conditions: the stochastic component of the log-likelihood is linear in the model parameter, while the expected log-likelihood is a smooth and concave function.","For the penalized maximum likelihood estimators (pMLE), we establish several finite sample bounds about its concentration and large deviations as well as the Fisher and Wilks expansions and risk bounds.","In all results, the remainder is given explicitly and can be evaluated in terms of the effective sample size $ n $ and effective parameter dimension $ \\mathbb{p} $ which allows us to identify the so-called \\emph{critical parameter dimension}.","The results are also dimension and coordinate-free.","Despite generality, all the presented bounds are nearly sharp and the classical asymptotic results can be obtained as simple corollaries.","Our results indicate that the use of advanced fourth-order expansions allows to relax the critical dimension condition $ \\mathbb{p}^{3} \\ll n $ from Spokoiny (2023a) to $ \\mathbb{p}^{3/2} \\ll n $.","Examples for classical models like logistic regression, log-density and precision matrix estimation illustrate the applicability of general results."],"url":"http://arxiv.org/abs/2404.14227v1","category":"math.ST"}
{"created":"2024-04-22 14:36:53","title":"General degree divergence-free finite element methods for the Stokes problem on smooth domains","abstract":"In this paper, we construct and analyze divergence-free finite element methods for the Stokes problem on smooth domains. The discrete spaces are based on the Scott-Vogelius finite element pair of arbitrary polynomial degree greater than two. By combining the Piola transform with the classical isoparametric framework, and with a judicious choice of degrees of freedom, we prove that the method converges with optimal order in the energy norm. We also show that the discrete velocity error converges with optimal order in the $L^2$-norm. Numerical experiments are presented, which support the theoretical results.","sentences":["In this paper, we construct and analyze divergence-free finite element methods for the Stokes problem on smooth domains.","The discrete spaces are based on the Scott-Vogelius finite element pair of arbitrary polynomial degree greater than two.","By combining the Piola transform with the classical isoparametric framework, and with a judicious choice of degrees of freedom, we prove that the method converges with optimal order in the energy norm.","We also show that the discrete velocity error converges with optimal order in the $L^2$-norm.","Numerical experiments are presented, which support the theoretical results."],"url":"http://arxiv.org/abs/2404.14226v1","category":"math.NA"}
{"created":"2024-04-22 14:35:24","title":"GravitoMagneto-Hydrodynamics and Spacetime Turbulence in Early Universe","abstract":"Based on the gravitoelectromagnetic formalism and inspired by the rich analogies between electrodynamics and general relativity, we try one step further along this line and suggest a new counterpart in the gravitoelectromagnetic world analogue to the electromagnetic physics. A counterpart model of the MagnetoHydroDynamics that could help us to understand the possible new physics in tightly bounded spacetime-matter systems such as the case of extremely relativistic fluids in the early Universe. This new viewpoint also suggests a possible new form of spacetime-matter turbulence which may be tested through gravitational wave observations.","sentences":["Based on the gravitoelectromagnetic formalism and inspired by the rich analogies between electrodynamics and general relativity, we try one step further along this line and suggest a new counterpart in the gravitoelectromagnetic world analogue to the electromagnetic physics.","A counterpart model of the MagnetoHydroDynamics that could help us to understand the possible new physics in tightly bounded spacetime-matter systems such as the case of extremely relativistic fluids in the early Universe.","This new viewpoint also suggests a possible new form of spacetime-matter turbulence which may be tested through gravitational wave observations."],"url":"http://arxiv.org/abs/2404.14225v1","category":"gr-qc"}
{"created":"2024-04-22 14:35:05","title":"Modeling principles for a physiology-based whole-body model of human metabolism","abstract":"Physiological whole-body models are valuable tools for the development of novel drugs where understanding the system aspects is important. This paper presents a generalized model that encapsulates the structure and flow of whole-body human physiology. The model contains vascular, interstitial, and cellular subcompartments for each organ. Scaling of volumes and blood flows is described to allow for investigation across populations or specific patient groups. The model equations and the corresponding parameters are presented along with a catalog of functions that can be used to define the organ transport model and the biochemical reaction model. A simple example illustrates the procedure.","sentences":["Physiological whole-body models are valuable tools for the development of novel drugs where understanding the system aspects is important.","This paper presents a generalized model that encapsulates the structure and flow of whole-body human physiology.","The model contains vascular, interstitial, and cellular subcompartments for each organ.","Scaling of volumes and blood flows is described to allow for investigation across populations or specific patient groups.","The model equations and the corresponding parameters are presented along with a catalog of functions that can be used to define the organ transport model and the biochemical reaction model.","A simple example illustrates the procedure."],"url":"http://arxiv.org/abs/2404.14224v1","category":"q-bio.QM"}
{"created":"2024-04-22 14:33:16","title":"An Artificial Neuron for Enhanced Problem Solving in Large Language Models","abstract":"Recent advancements in artificial intelligence have propelled the capabilities of Large Language Models, yet their ability to mimic nuanced human reasoning remains limited. This paper introduces a novel conceptual enhancement to LLMs, termed the Artificial Neuron, designed to significantly bolster cognitive processing by integrating external memory systems. This enhancement mimics neurobiological processes, facilitating advanced reasoning and learning through a dynamic feedback loop mechanism. We propose a unique framework wherein each LLM interaction specifically in solving complex math word problems and common sense reasoning tasks is recorded and analyzed. Incorrect responses are refined using a higher capacity LLM or human in the loop corrections, and both the query and the enhanced response are stored in a vector database, structured much like neuronal synaptic connections. This Artificial Neuron thus serves as an external memory aid, allowing the LLM to reference past interactions and apply learned reasoning strategies to new problems. Our experimental setup involves training with the GSM8K dataset for initial model response generation, followed by systematic refinements through feedback loops. Subsequent testing demonstrated a significant improvement in accuracy and efficiency, underscoring the potential of external memory systems to advance LLMs beyond current limitations. This approach not only enhances the LLM's problem solving precision but also reduces computational redundancy, paving the way for more sophisticated applications of artificial intelligence in cognitive tasks. This paper details the methodology, implementation, and implications of the Artificial Neuron model, offering a transformative perspective on enhancing machine intelligence.","sentences":["Recent advancements in artificial intelligence have propelled the capabilities of Large Language Models, yet their ability to mimic nuanced human reasoning remains limited.","This paper introduces a novel conceptual enhancement to LLMs, termed the Artificial Neuron, designed to significantly bolster cognitive processing by integrating external memory systems.","This enhancement mimics neurobiological processes, facilitating advanced reasoning and learning through a dynamic feedback loop mechanism.","We propose a unique framework wherein each LLM interaction specifically in solving complex math word problems and common sense reasoning tasks is recorded and analyzed.","Incorrect responses are refined using a higher capacity LLM or human in the loop corrections, and both the query and the enhanced response are stored in a vector database, structured much like neuronal synaptic connections.","This Artificial Neuron thus serves as an external memory aid, allowing the LLM to reference past interactions and apply learned reasoning strategies to new problems.","Our experimental setup involves training with the GSM8K dataset for initial model response generation, followed by systematic refinements through feedback loops.","Subsequent testing demonstrated a significant improvement in accuracy and efficiency, underscoring the potential of external memory systems to advance LLMs beyond current limitations.","This approach not only enhances the LLM's problem solving precision but also reduces computational redundancy, paving the way for more sophisticated applications of artificial intelligence in cognitive tasks.","This paper details the methodology, implementation, and implications of the Artificial Neuron model, offering a transformative perspective on enhancing machine intelligence."],"url":"http://arxiv.org/abs/2404.14222v1","category":"cs.HC"}
{"created":"2024-04-22 14:33:02","title":"Sequential Outlier Hypothesis Testing under Universality Constraints","abstract":"We revisit sequential outlier hypothesis testing and derive bounds on the achievable exponents. Specifically, the task of outlier hypothesis testing is to identify the set of outliers that are generated from an anomalous distribution among all observed sequences where most are generated from a nominal distribution. In the sequential setting, one obtains a sample from each sequence per unit time until a reliable decision could be made. We assume that the number of outliers is known while both the nominal and anomalous distributions are unknown. For the case of exactly one outlier, our bounds on the achievable exponents are tight, providing exact large deviations characterization of sequential tests and strengthening a previous result of Li, Nitinawarat and Veeravalli (2017). In particular, we propose a sequential test that has bounded average sample size and better theoretical performance than the fixed-length test, which could not be guaranteed by the corresponding sequential test of Li, Nitinawarat and Veeravalli (2017). Our results are also generalized to the case of multiple outliers.","sentences":["We revisit sequential outlier hypothesis testing and derive bounds on the achievable exponents.","Specifically, the task of outlier hypothesis testing is to identify the set of outliers that are generated from an anomalous distribution among all observed sequences where most are generated from a nominal distribution.","In the sequential setting, one obtains a sample from each sequence per unit time until a reliable decision could be made.","We assume that the number of outliers is known while both the nominal and anomalous distributions are unknown.","For the case of exactly one outlier, our bounds on the achievable exponents are tight, providing exact large deviations characterization of sequential tests and strengthening a previous result of Li, Nitinawarat and Veeravalli (2017).","In particular, we propose a sequential test that has bounded average sample size and better theoretical performance than the fixed-length test, which could not be guaranteed by the corresponding sequential test of Li, Nitinawarat and Veeravalli (2017).","Our results are also generalized to the case of multiple outliers."],"url":"http://arxiv.org/abs/2404.14221v1","category":"cs.IT"}
{"created":"2024-04-22 14:32:33","title":"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone","abstract":"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).","sentences":["We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone.","The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data.","The model is also further aligned for robustness, safety, and chat format.","We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench)."],"url":"http://arxiv.org/abs/2404.14219v1","category":"cs.CL"}
{"created":"2024-04-22 14:31:55","title":"General protocols for the efficient distillation of indistinguishable photons","abstract":"Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing. We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance. We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols. We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry.","sentences":["Highly pure and indistinguishable photons are a prerequisite for use in quantum information processing.","We introduce protocols for the distillation of indistinguishable photons that offer a significant improvement over previous work, reducing distinguishability error rates by a factor of $n$, with resource requirements scaling linearly in $n$. We present the protocols, based on the discrete Fourier transform and Hadamard (Sylvester) matrices, then give both analytical and numerical results regarding their performance.","We observe that the same symmetry properties governing suppression laws are instrumental in understanding the behavior of these distillation protocols.","We also prove, adapting a result from the Hadamard case, that for the $n$-photon discrete Fourier transform with $n$ a prime power, the suppression laws are exactly characterized by the well-known Zero Transmission Law based on permutation symmetry."],"url":"http://arxiv.org/abs/2404.14217v1","category":"quant-ph"}
{"created":"2024-04-22 14:31:28","title":"Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction","abstract":"The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called $T^3$(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our code and data can be found at https://github.com/HKUST-KnowComp/LiveSum-TTT.","sentences":["The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining.","Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration.","However, there is a lack of both datasets and methodologies towards this task.","In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts.","We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called $T^3$(Text-Tuple-Table) to improve their performances.","Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training.","Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets.","Our code and data can be found at https://github.com/HKUST-KnowComp/LiveSum-TTT."],"url":"http://arxiv.org/abs/2404.14215v1","category":"cs.CL"}
{"created":"2024-04-22 14:21:37","title":"Toward Routing River Water in Land Surface Models with Recurrent Neural Networks","abstract":"Machine learning is playing an increasing role in hydrology, supplementing or replacing physics-based models. One notable example is the use of recurrent neural networks (RNNs) for forecasting streamflow given observed precipitation and geographic characteristics. Training of such a model over the continental United States has demonstrated that a single set of model parameters can be used across independent catchments, and that RNNs can outperform physics-based models. In this work, we take a next step and study the performance of RNNs for river routing in land surface models (LSMs). Instead of observed precipitation, the LSM-RNN uses instantaneous runoff calculated from physics-based models as an input. We train the model with data from river basins spanning the globe and test it in streamflow hindcasts. The model demonstrates skill at generalization across basins (predicting streamflow in unseen catchments) and across time (predicting streamflow during years not used in training). We compare the predictions from the LSM-RNN to an existing physics-based model calibrated with a similar dataset and find that the LSM-RNN outperforms the physics-based model. Our results give further evidence that RNNs are effective for global streamflow prediction from runoff inputs and motivate the development of complete routing models that can capture nested sub-basis connections.","sentences":["Machine learning is playing an increasing role in hydrology, supplementing or replacing physics-based models.","One notable example is the use of recurrent neural networks (RNNs) for forecasting streamflow given observed precipitation and geographic characteristics.","Training of such a model over the continental United States has demonstrated that a single set of model parameters can be used across independent catchments, and that RNNs can outperform physics-based models.","In this work, we take a next step and study the performance of RNNs for river routing in land surface models (LSMs).","Instead of observed precipitation, the LSM-RNN uses instantaneous runoff calculated from physics-based models as an input.","We train the model with data from river basins spanning the globe and test it in streamflow hindcasts.","The model demonstrates skill at generalization across basins (predicting streamflow in unseen catchments) and across time (predicting streamflow during years not used in training).","We compare the predictions from the LSM-RNN to an existing physics-based model calibrated with a similar dataset and find that the LSM-RNN outperforms the physics-based model.","Our results give further evidence that RNNs are effective for global streamflow prediction from runoff inputs and motivate the development of complete routing models that can capture nested sub-basis connections."],"url":"http://arxiv.org/abs/2404.14212v1","category":"physics.comp-ph"}
{"created":"2024-04-22 14:21:13","title":"Fidelitous Augmentation of Human Accelerometric Data for Deep Learning","abstract":"Time series (TS) data have consistently been in short supply, yet their demand remains high for training systems in prediction, modeling, classification, and various other applications. Synthesis can serve to expand the sample population, yet it is crucial to maintain the statistical characteristics between the synthesized and the original TS : this ensures consistent sampling of data for both training and testing purposes. However the time domain features of the data may not be maintained. This motivates for our work, the objective which is to preserve the following features in a synthesized TS: its fundamental statistical characteristics and important time domain features like its general shape and prominent transients. In a novel way, we first isolate important TS features into various components using a spectrogram and singular spectrum analysis. The residual signal is then randomized in a way that preserves its statistical properties. These components are then recombined for the synthetic time series. Using accelerometer data in a clinical setting, we use statistical and shape measures to compare our method to others. We show it has higher fidelity to the original signal features, has good diversity and performs better data classification in a deep learning application.","sentences":["Time series (TS) data have consistently been in short supply, yet their demand remains high for training systems in prediction, modeling, classification, and various other applications.","Synthesis can serve to expand the sample population, yet it is crucial to maintain the statistical characteristics between the synthesized and the original TS : this ensures consistent sampling of data for both training and testing purposes.","However the time domain features of the data may not be maintained.","This motivates for our work, the objective which is to preserve the following features in a synthesized TS: its fundamental statistical characteristics and important time domain features like its general shape and prominent transients.","In a novel way, we first isolate important TS features into various components using a spectrogram and singular spectrum analysis.","The residual signal is then randomized in a way that preserves its statistical properties.","These components are then recombined for the synthetic time series.","Using accelerometer data in a clinical setting, we use statistical and shape measures to compare our method to others.","We show it has higher fidelity to the original signal features, has good diversity and performs better data classification in a deep learning application."],"url":"http://arxiv.org/abs/2404.14211v1","category":"eess.SP"}
{"created":"2024-04-22 14:16:27","title":"Localization Based on MIMO Backscattering from Retro-Directive Antenna Arrays","abstract":"In next-generation vehicular environments, precise localization is crucial for facilitating advanced applications such as autonomous driving. As automation levels escalate, the demand rises for enhanced accuracy, reliability, energy efficiency, update rate, and reduced latency in position information delivery. In this paper, we propose the exploitation of backscattering from retro-directive antenna arrays (RAAs) to address these imperatives. We introduce and discuss two RAA-based architectures designed for various applications, including network localization and navigation. These architectures enable swift and simple angle-of-arrival estimation by using signals backscattered from RAAs. They also leverage multiple antennas to capitalize on multiple-input-multiple-output (MIMO) gains, thereby addressing the challenges posed by the inherent path loss in backscatter communication, especially when operating at high frequencies. Consequently, angle-based localization becomes achievable with remarkably low latency, ideal for mobile and vehicular applications. This paper introduces ad-hoc signalling and processing schemes for this purpose, and their performance is analytically investigated. Numerical results underscore the potential of these schemes, offering precise and ultra-low-latency localization with low complexity and ultra-low energy consumption devices.","sentences":["In next-generation vehicular environments, precise localization is crucial for facilitating advanced applications such as autonomous driving.","As automation levels escalate, the demand rises for enhanced accuracy, reliability, energy efficiency, update rate, and reduced latency in position information delivery.","In this paper, we propose the exploitation of backscattering from retro-directive antenna arrays (RAAs) to address these imperatives.","We introduce and discuss two RAA-based architectures designed for various applications, including network localization and navigation.","These architectures enable swift and simple angle-of-arrival estimation by using signals backscattered from RAAs.","They also leverage multiple antennas to capitalize on multiple-input-multiple-output (MIMO) gains, thereby addressing the challenges posed by the inherent path loss in backscatter communication, especially when operating at high frequencies.","Consequently, angle-based localization becomes achievable with remarkably low latency, ideal for mobile and vehicular applications.","This paper introduces ad-hoc signalling and processing schemes for this purpose, and their performance is analytically investigated.","Numerical results underscore the potential of these schemes, offering precise and ultra-low-latency localization with low complexity and ultra-low energy consumption devices."],"url":"http://arxiv.org/abs/2404.14206v1","category":"eess.SP"}
{"created":"2024-04-22 14:14:46","title":"Properties of the `friend of a friend' model for network generation","abstract":"The way in which a social network is generated, in terms of how individuals attach to each other, determines the properties of the resulting network. Here we study an intuitively appealing `friend of a friend' model, where a network is formed by each newly added individual attaching first to a randomly chosen target and then to $n_q\\geq 1$ randomly chosen friends of the target, each with probability $0<q\\leq1$. We revisit the master equation of the expected degree distribution for this model, providing an exact solution for the case when $n_q$ allows for attachment to all of the chosen target's friends (a case previously studied by \\cite{lambiotte2016}), and demonstrating why such a solution is hard to obtain when $n_q$ is fixed (a case previously studied by \\cite{Levens2022}.) In the case where attachment to all friends is allowed, we also show that when $q<q^*\\approx0.5671$, the expected degree distribution of the model is stationary as the network size tends to infinity. We go on to look at the clustering behaviour and the triangle count, focusing on the cases where $n_q$ is fixed.","sentences":["The way in which a social network is generated, in terms of how individuals attach to each other, determines the properties of the resulting network.","Here we study an intuitively appealing `friend of a friend' model, where a network is formed by each newly added individual attaching first to a randomly chosen target and then to $n_q\\geq 1$ randomly chosen friends of the target, each with probability $0<q\\leq1$.","We revisit the master equation of the expected degree distribution for this model, providing an exact solution for the case when $n_q$ allows for attachment to all of the chosen target's friends (a case previously studied by \\cite{lambiotte2016}), and demonstrating why such a solution is hard to obtain when $n_q$ is fixed (a case previously studied by \\cite{Levens2022}.)","In the case where attachment to all friends is allowed, we also show that when $q<q^*\\approx0.5671$, the expected degree distribution of the model is stationary as the network size tends to infinity.","We go on to look at the clustering behaviour and the triangle count, focusing on the cases where $n_q$ is fixed."],"url":"http://arxiv.org/abs/2404.14205v1","category":"physics.soc-ph"}
{"created":"2024-04-22 14:13:36","title":"TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading","abstract":"Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching. In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.","sentences":["Next-generation mobile networks are expected to facilitate fast AI model downloading to end users.","By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching.","In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching).","TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency.","To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency.","We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists.","To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice.","In such a case, a polynomial-time algorithm with $\\left(1-\\epsilon\\right)/2$-approximation guarantee is developed.","Subsequently, we address the original problem for the general case by developing a greedy algorithm.","Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models."],"url":"http://arxiv.org/abs/2404.14204v1","category":"cs.NI"}
{"created":"2024-04-22 14:07:42","title":"BCFPL: Binary classification ConvNet based Fast Parking space recognition with Low resolution image","abstract":"The automobile plays an important role in the economic activities of mankind, especially in the metropolis. Under the circumstances, the demand of quick search for available parking spaces has become a major concern for the automobile drivers. Meanwhile, the public sense of privacy is also awaking, the image-based parking space recognition methods lack the attention of privacy protection. In this paper, we proposed a binary convolutional neural network with lightweight design structure named BCFPL, which can be used to train with low-resolution parking space images and offer a reasonable recognition result. The images of parking space were collected from various complex environments, including different weather, occlusion conditions, and various camera angles. We conducted the training and testing progresses among different datasets and partial subsets. The experimental results show that the accuracy of BCFPL does not decrease compared with the original resolution image directly, and can reach the average level of the existing mainstream method. BCFPL also has low hardware requirements and fast recognition speed while meeting the privacy requirements, so it has application potential in intelligent city construction and automatic driving field.","sentences":["The automobile plays an important role in the economic activities of mankind, especially in the metropolis.","Under the circumstances, the demand of quick search for available parking spaces has become a major concern for the automobile drivers.","Meanwhile, the public sense of privacy is also awaking, the image-based parking space recognition methods lack the attention of privacy protection.","In this paper, we proposed a binary convolutional neural network with lightweight design structure named BCFPL, which can be used to train with low-resolution parking space images and offer a reasonable recognition result.","The images of parking space were collected from various complex environments, including different weather, occlusion conditions, and various camera angles.","We conducted the training and testing progresses among different datasets and partial subsets.","The experimental results show that the accuracy of BCFPL does not decrease compared with the original resolution image directly, and can reach the average level of the existing mainstream method.","BCFPL also has low hardware requirements and fast recognition speed while meeting the privacy requirements, so it has application potential in intelligent city construction and automatic driving field."],"url":"http://arxiv.org/abs/2404.14198v1","category":"cs.CV"}
{"created":"2024-04-22 14:06:27","title":"Universal formal asymptotics for localized oscillation of a discrete mass-spring-damper system of time-varying properties, embedded into a one-dimensional medium described by the telegraph equation with variable coefficients","abstract":"We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system. This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation. In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed. Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible. In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position. This general statement can describe a number of mechanical systems of various nature. We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system. In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters. In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters. Finally, we have justified the universal asymptotics by numerical calculations for some particular cases.","sentences":["We consider a quite general problem concerning a linear free oscillation of a discrete mass-spring-damper system.","This discrete sub-system is embedded into a one-dimensional continuum medium described by the linear telegraph equation.","In a particular case, the discrete sub-system can move along the continuum one at a sub-critical speed.","Provided that the dissipation in both discrete and continuum sub-systems is absent, if parameters of the sub-systems are constants, under certain conditions (the localization conditions), a non-vanishing oscillation localized near the discrete sub-system can be possible.","In the paper we assume that the dissipation in the damper and the medium is small, and all discrete-continuum system parameters are slowly varying functions in time and in space (when applicable), such that the localization condition is fulfilled for the instantaneous values of the parameters in a certain neighbourhood of the discrete sub-system position.","This general statement can describe a number of mechanical systems of various nature.","We derive the expression for the leading-order term of a universal asymptotics, which describes a localized oscillation of the discrete sub-system.","In the non-dissipative case, the leading-order term of the expansion for the amplitude is found in the form of an algebraic expression, which involves the instantaneous values of the system parameters.","In the dissipative case, the leading-order term for the amplitude, generally, is found in quadratures in the form of a functional, which depends on the history of the system parameters, though in some exceptional cases the result can be obtained as a function of time and the instantaneous limiting values of the system parameters.","Finally, we have justified the universal asymptotics by numerical calculations for some particular cases."],"url":"http://arxiv.org/abs/2404.14196v1","category":"physics.class-ph"}
{"created":"2024-04-22 14:03:55","title":"On h-refined meshless solution to Navier-Stokes problem in porous media: comparing meshless Lattice Boltzman Method with ACM RBF-FD approach","abstract":"In this paper, two mesh-free CFD solvers for pore-scale fluid flow through porous media are considered, namely the Lattice Boltzmann Method with the two relaxation time collision term and the direct Navier-Stokes solver under the artificial compressibility limit. The porous media is built with a regular arrangement of spherical grains with variable radii, which allows control of the porosity. Both solvers use the same h-refined meshless spatial discretization to adequately capture the underlying geometry and the same Radial Basis Function (RBF) method to approximate the involved fields and partial differential operators. First, the results are compared with the data from the literature in terms of drag coefficient and permeability at different porosities achieving excellent agreement with the reported results. Next, the simulations are extended beyond the porosity range reported in the literature using proposed h-refined CFD solvers. The results are supported by convergence and timing analyses and discussions on meshless parameters such as stencil size and refinement settings.","sentences":["In this paper, two mesh-free CFD solvers for pore-scale fluid flow through porous media are considered, namely the Lattice Boltzmann Method with the two relaxation time collision term and the direct Navier-Stokes solver under the artificial compressibility limit.","The porous media is built with a regular arrangement of spherical grains with variable radii, which allows control of the porosity.","Both solvers use the same h-refined meshless spatial discretization to adequately capture the underlying geometry and the same Radial Basis Function (RBF) method to approximate the involved fields and partial differential operators.","First, the results are compared with the data from the literature in terms of drag coefficient and permeability at different porosities achieving excellent agreement with the reported results.","Next, the simulations are extended beyond the porosity range reported in the literature using proposed h-refined CFD solvers.","The results are supported by convergence and timing analyses and discussions on meshless parameters such as stencil size and refinement settings."],"url":"http://arxiv.org/abs/2404.14195v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 14:01:24","title":"LLAMP: Assessing Network Latency Tolerance of HPC Applications with Linear Programming","abstract":"The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications. As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation. Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming. In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming. LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts. Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%. Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies.","sentences":["The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications.","As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation.","Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming.","In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming.","LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts.","Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%.","Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies."],"url":"http://arxiv.org/abs/2404.14193v1","category":"cs.DC"}
{"created":"2024-04-22 14:00:52","title":"A generators and relations derivation of Khovanov homology of 2 strand braid links","abstract":"In the first part of the Thesis, we reformulate the Murakami-Ohtsuki-Yamada state-sum description of the level n Jones polynomial of an oriented link in terms of a suitable braided monoidal category whose morphisms are Q[q, q-1] s-linear combinations of oriented trivalent planar graphs, and give a corresponding description for the HOMFLY-PT polynomial. In the second part, we extend this construction and express the Khovanov Rozansky homology of an oriented link in terms of a combinatorially defined category whose morphisms are equivalence classes of formal complexes of (formal direct sums of shifted) oriented plane graphs. By working combinatorially, one avoids many of the computational difficulties involved in the matrix factorization computations of the original Khovanov-Rozansky formulation: one systematically uses combinatorial relations satisfied by these matrix factorizations to simplify the computation at a level that is easily handled. By using this technique, we can provide a computation of the level n Khovanov-Rozansky invariant of the 2-strand braid link with k crossings, for arbitrary n and k, confirming and extending previous results and conjectural predictions by Anokhina-Morozov, Carqueville-Murfet, Dolotin-Morozov, Gukov-Iqbal-Kozcaz-Vafa, and Rasmussen","sentences":["In the first part of the Thesis, we reformulate the Murakami-Ohtsuki-Yamada state-sum description of the level n Jones polynomial of an oriented link in terms of a suitable braided monoidal category whose morphisms are Q[q, q-1] s-linear combinations of oriented trivalent planar graphs, and give a corresponding description for the HOMFLY-PT polynomial.","In the second part, we extend this construction and express the Khovanov Rozansky homology of an oriented link in terms of a combinatorially defined category whose morphisms are equivalence classes of formal complexes of (formal direct sums of shifted) oriented plane graphs.","By working combinatorially, one avoids many of the computational difficulties involved in the matrix factorization computations of the original Khovanov-Rozansky formulation: one systematically uses combinatorial relations satisfied by these matrix factorizations to simplify the computation at a level that is easily handled.","By using this technique, we can provide a computation of the level n Khovanov-Rozansky invariant of the 2-strand braid link with k crossings, for arbitrary n and k, confirming and extending previous results and conjectural predictions by Anokhina-Morozov, Carqueville-Murfet, Dolotin-Morozov, Gukov-Iqbal-Kozcaz-Vafa, and Rasmussen"],"url":"http://arxiv.org/abs/2404.14191v1","category":"math.GT"}
{"created":"2024-04-22 13:57:30","title":"Bayesian Windkessel calibration using optimized 0D surrogate models","abstract":"Boundary condition (BC) calibration to assimilate clinical measurements is an essential step in any subject-specific simulation of cardiovascular fluid dynamics. Bayesian calibration approaches have successfully quantified the uncertainties inherent in identified parameters. Yet, routinely estimating the posterior distribution for all BC parameters in 3D simulations has been unattainable due to the infeasible computational demand. We propose an efficient method to identify Windkessel parameter posteriors using results from a single high-fidelity three-dimensional (3D) model evaluation. We only evaluate the 3D model once for an initial choice of BCs and use the result to create a highly accurate zero-dimensional (0D) surrogate. We then perform Sequential Monte Carlo (SMC) using the optimized 0D model to derive the high-dimensional Windkessel BC posterior distribution. We validate this approach in a publicly available dataset of N=72 subject-specific vascular models. We found that optimizing 0D models to match 3D data a priori lowered their median approximation error by nearly one order of magnitude. In a subset of models, we confirm that the optimized 0D models still generalize to a wide range of BCs. Finally, we present the high-dimensional Windkessel parameter posterior for different measured signal-to-noise ratios in a vascular model using SMC. We further validate that the 0D-derived posterior is a good approximation of the 3D posterior. The minimal computational demand of our method using a single 3D simulation, combined with the open-source nature of all software and data used in this work, will increase access and efficiency of Bayesian Windkessel calibration in cardiovascular fluid dynamics simulations.","sentences":["Boundary condition (BC) calibration to assimilate clinical measurements is an essential step in any subject-specific simulation of cardiovascular fluid dynamics.","Bayesian calibration approaches have successfully quantified the uncertainties inherent in identified parameters.","Yet, routinely estimating the posterior distribution for all BC parameters in 3D simulations has been unattainable due to the infeasible computational demand.","We propose an efficient method to identify Windkessel parameter posteriors using results from a single high-fidelity three-dimensional (3D) model evaluation.","We only evaluate the 3D model once for an initial choice of BCs and use the result to create a highly accurate zero-dimensional (0D) surrogate.","We then perform Sequential Monte Carlo (SMC) using the optimized 0D model to derive the high-dimensional Windkessel BC posterior distribution.","We validate this approach in a publicly available dataset of N=72 subject-specific vascular models.","We found that optimizing 0D models to match 3D data a priori lowered their median approximation error by nearly one order of magnitude.","In a subset of models, we confirm that the optimized 0D models still generalize to a wide range of BCs.","Finally, we present the high-dimensional Windkessel parameter posterior for different measured signal-to-noise ratios in a vascular model using SMC.","We further validate that the 0D-derived posterior is a good approximation of the 3D posterior.","The minimal computational demand of our method using a single 3D simulation, combined with the open-source nature of all software and data used in this work, will increase access and efficiency of Bayesian Windkessel calibration in cardiovascular fluid dynamics simulations."],"url":"http://arxiv.org/abs/2404.14187v1","category":"cs.CE"}
{"created":"2024-04-22 13:57:01","title":"Magnon Landau-Zener tunnelling and spin current generation by electric field","abstract":"To control the magnon transport in magnetic systems is of great interest in magnonics. Due to the feasibility of electric field, how to generate and manipulate magnon with pure electrical method is one of the most desired goals. Here we propose that the magnon spin current is generated by applying time-dependent electric field, where the coupling between the magnon and electric field is invoked via Aharonov-Casher effect. In particular, the magnon spin current is dominated by electric field component which perpendicular to the magnetization direction. We apply our theory to 1D ferromagnetic SSH model and show that the generated magnon spin current is closely related to the band geometry. Our findings expands the horizons of magnonics and electric-control-magnon mechanisms.","sentences":["To control the magnon transport in magnetic systems is of great interest in magnonics.","Due to the feasibility of electric field, how to generate and manipulate magnon with pure electrical method is one of the most desired goals.","Here we propose that the magnon spin current is generated by applying time-dependent electric field, where the coupling between the magnon and electric field is invoked via Aharonov-Casher effect.","In particular, the magnon spin current is dominated by electric field component which perpendicular to the magnetization direction.","We apply our theory to 1D ferromagnetic SSH model and show that the generated magnon spin current is closely related to the band geometry.","Our findings expands the horizons of magnonics and electric-control-magnon mechanisms."],"url":"http://arxiv.org/abs/2404.14186v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 13:56:07","title":"SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection","abstract":"We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. The task featured three subtasks. Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine. This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track. Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM. Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine. The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30). In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For all subtasks, the best systems used LLMs.","sentences":["We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection.","The task featured three subtasks.","Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine.","This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track.","Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM.","Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine.","The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30).","In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used.","For all subtasks, the best systems used LLMs."],"url":"http://arxiv.org/abs/2404.14183v1","category":"cs.CL"}
{"created":"2024-04-22 13:54:26","title":"Mathematical Crystal Chemistry","abstract":"Efficient heuristics have predicted many functional materials such as high-temperature superconducting hydrides, while inorganic structural chemistry explains why and how the crystal structures are stabilized. Here we develop the paired mathematical programming formalism for searching and systematizing the structural prototypes of crystals. The first is the minimization of the volume of the unit cell under the constraints of only the minimum and maximum distances between pairs of atoms. We show the capabilities of linear relaxations of inequality constraints to optimize structures by the steepest-descent method, which is computationally very efficient. The second is the discrete optimization to assign five kinds of geometrical constraints including chemical bonds for pairs of atoms. Under the constraints, the two object functions, formulated as mathematical programming, are alternately optimized to realize the given coordination numbers of atoms. This approach successfully generates a wide variety of crystal structures of oxides such as spinel, pyrochlore-$\\alpha$, and $\\mathrm{K}_2 \\mathrm{NiF}_4$ structures.","sentences":["Efficient heuristics have predicted many functional materials such as high-temperature superconducting hydrides, while inorganic structural chemistry explains why and how the crystal structures are stabilized.","Here we develop the paired mathematical programming formalism for searching and systematizing the structural prototypes of crystals.","The first is the minimization of the volume of the unit cell under the constraints of only the minimum and maximum distances between pairs of atoms.","We show the capabilities of linear relaxations of inequality constraints to optimize structures by the steepest-descent method, which is computationally very efficient.","The second is the discrete optimization to assign five kinds of geometrical constraints including chemical bonds for pairs of atoms.","Under the constraints, the two object functions, formulated as mathematical programming, are alternately optimized to realize the given coordination numbers of atoms.","This approach successfully generates a wide variety of crystal structures of oxides such as spinel, pyrochlore-$\\alpha$, and $\\mathrm{K}_2 \\mathrm{NiF}_4$ structures."],"url":"http://arxiv.org/abs/2404.14181v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 13:49:43","title":"Non-trivial $r$-wise agreeing families","abstract":"A family of sets is $r$-wise agreeing if for any $r$ sets from the family there is an element $x$ that is either contained in all or contained in none of the $r$ sets. The study of such families is motivated by questions in discrete optimization. In this paper, we determine the size of the largest non-trivial $r$-wise agreeing family. This can be seen as a generalization of the classical Brace-Daykin theorem.","sentences":["A family of sets is $r$-wise agreeing if for any $r$ sets from the family there is an element $x$ that is either contained in all or contained in none of the $r$ sets.","The study of such families is motivated by questions in discrete optimization.","In this paper, we determine the size of the largest non-trivial $r$-wise agreeing family.","This can be seen as a generalization of the classical Brace-Daykin theorem."],"url":"http://arxiv.org/abs/2404.14178v1","category":"math.CO"}
{"created":"2024-04-22 13:36:25","title":"Optimal frequency for undulatory motion in granular media","abstract":"Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour. This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand. Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed. Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate. We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium. Potential implications in the design of artificial swimmers are discussed.","sentences":["Sand is a highly dissipative system, where the local spatial arrangements and densities depend strongly on the applied forces, resulting in fluid-like or solid-like behaviour.","This makes sand swimming challenging and intriguing, raising questions about the nature of the motion and how to optimize the design of artificial swimmers able to swim in sand.","Recent experiments suggest that lateral undulatory motion enables efficient locomotion, with a non-monotonic dependence of the swimming speed on the undulatory frequency and the height of the sediment bed.","Here, we propose a quasi-2D granular model, where the effect of the bed height is modeled by a coarse-grained frictional force with the substrate.","We show that the optimal frequency coincides with the second vibrational mode of the swimmer and explain the underlying mechanism through a characterization of the rheology of the medium.","Potential implications in the design of artificial swimmers are discussed."],"url":"http://arxiv.org/abs/2404.14168v1","category":"cond-mat.soft"}
{"created":"2024-04-22 13:34:50","title":"A multi-robot system for the detection of explosive devices","abstract":"In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role. However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system. In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms. This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines. This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation. The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles. This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator. Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection.","sentences":["In order to clear the world of the threat posed by landmines and other explosive devices, robotic systems can play an important role.","However, the development of such field robots that need to operate in hazardous conditions requires the careful consideration of multiple aspects related to the perception, mobility, and collaboration capabilities of the system.","In the framework of a European challenge, the Artificial Intelligence for Detection of Explosive Devices - eXtended (AIDEDeX) project proposes to design a heterogeneous multi-robot system with advanced sensor fusion algorithms.","This system is specifically designed to detect and classify improvised explosive devices, explosive ordnances, and landmines.","This project integrates specialised sensors, including electromagnetic induction, ground penetrating radar, X-Ray backscatter imaging, Raman spectrometers, and multimodal cameras, to achieve comprehensive threat identification and localisation.","The proposed system comprises a fleet of unmanned ground vehicles and unmanned aerial vehicles.","This article details the operational phases of the AIDEDeX system, from rapid terrain exploration using unmanned aerial vehicles to specialised detection and classification by unmanned ground vehicles equipped with a robotic manipulator.","Initially focusing on a centralised approach, the project will also explore the potential of a decentralised control architecture, taking inspiration from swarm robotics to provide a robust, adaptable, and scalable solution for explosive detection."],"url":"http://arxiv.org/abs/2404.14167v1","category":"cs.RO"}
{"created":"2024-04-22 13:26:42","title":"New Solutions Based on the Generalized Eigenvalue Problem for the Data Collaboration Analysis","abstract":"In recent years, the accumulation of data across various institutions has garnered attention for the technology of confidential data analysis, which improves analytical accuracy by sharing data between multiple institutions while protecting sensitive information. Among these methods, Data Collaboration Analysis (DCA) is noted for its efficiency in terms of computational cost and communication load, facilitating data sharing and analysis across different institutions while safeguarding confidential information. However, existing optimization problems for determining the necessary collaborative functions have faced challenges, such as the optimal solution for the collaborative representation often being a zero matrix and the difficulty in understanding the process of deriving solutions. This research addresses these issues by formulating the optimization problem through the segmentation of matrices into column vectors and proposing a solution method based on the generalized eigenvalue problem. Additionally, we demonstrate methods for constructing collaborative functions more effectively through weighting and the selection of efficient algorithms suited to specific situations. Experiments using real-world datasets have shown that our proposed formulation and solution for the collaborative function optimization problem achieve superior predictive accuracy compared to existing methods.","sentences":["In recent years, the accumulation of data across various institutions has garnered attention for the technology of confidential data analysis, which improves analytical accuracy by sharing data between multiple institutions while protecting sensitive information.","Among these methods, Data Collaboration Analysis (DCA) is noted for its efficiency in terms of computational cost and communication load, facilitating data sharing and analysis across different institutions while safeguarding confidential information.","However, existing optimization problems for determining the necessary collaborative functions have faced challenges, such as the optimal solution for the collaborative representation often being a zero matrix and the difficulty in understanding the process of deriving solutions.","This research addresses these issues by formulating the optimization problem through the segmentation of matrices into column vectors and proposing a solution method based on the generalized eigenvalue problem.","Additionally, we demonstrate methods for constructing collaborative functions more effectively through weighting and the selection of efficient algorithms suited to specific situations.","Experiments using real-world datasets have shown that our proposed formulation and solution for the collaborative function optimization problem achieve superior predictive accuracy compared to existing methods."],"url":"http://arxiv.org/abs/2404.14164v1","category":"cs.LG"}
{"created":"2024-04-22 13:21:09","title":"FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on","abstract":"Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details.","sentences":["Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text.","To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON.","FLDM-VTON improves the conventional latent diffusion process in three major aspects.","First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors.","Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision.","Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling.","Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details."],"url":"http://arxiv.org/abs/2404.14162v1","category":"cs.CV"}
{"created":"2024-04-22 13:20:01","title":"Multidimensional Interpolants","abstract":"In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases. In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework. Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations. Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path. Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization. When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations. The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier.","sentences":["In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases.","In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework.","Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations.","Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path.","Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization.","When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations.","The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier."],"url":"http://arxiv.org/abs/2404.14161v1","category":"cs.LG"}
{"created":"2024-04-22 13:15:56","title":"Temporal characterization of laser pulses using an air-based knife-edge technique","abstract":"We present the characterization of ultrashort laser pulses by using the plasma-induced frequency resolved optical switching (PI-FROSt) technique, implemented in ambient air. This recently developed method allows for a temporal reconstruction of a pulse at its focal spot by utilizing a moderately intense pump laser pulse for generating an ionization-induced ultrafast defocusing lens. When propagating through the produced plasma lens, the probe beam to characterize experiences an increase of its size in the far field. The spectrum of the defocused probe field, measured as a function of the pump-probe delay, allows for a comprehensive characterization of the temporal and spectral attributes of the pulse. We report herein the ability of this technique, initially designed for use in rare gases, to operate in ambient air conditions with similar performance. The method is remarkably straightforward to implement and requires no additional optical component other than a focusing mirror, while delivering laser pulse reconstructions of high reliability.","sentences":["We present the characterization of ultrashort laser pulses by using the plasma-induced frequency resolved optical switching (PI-FROSt) technique, implemented in ambient air.","This recently developed method allows for a temporal reconstruction of a pulse at its focal spot by utilizing a moderately intense pump laser pulse for generating an ionization-induced ultrafast defocusing lens.","When propagating through the produced plasma lens, the probe beam to characterize experiences an increase of its size in the far field.","The spectrum of the defocused probe field, measured as a function of the pump-probe delay, allows for a comprehensive characterization of the temporal and spectral attributes of the pulse.","We report herein the ability of this technique, initially designed for use in rare gases, to operate in ambient air conditions with similar performance.","The method is remarkably straightforward to implement and requires no additional optical component other than a focusing mirror, while delivering laser pulse reconstructions of high reliability."],"url":"http://arxiv.org/abs/2404.14158v1","category":"physics.optics"}
{"created":"2024-04-22 13:07:56","title":"Invariant measures as obstructions to attractors in dynamical systems and their role in nonholonomic mechanics","abstract":"We present some rigorous results on the absence of a wide class of invariant measures for dynamical systems possessing attractors. We then consider a generalization of the classical nonholonomic Suslov problem which shows how previous investigations of existence of invariant measures for nonholonomic systems should necessarily be extended beyond the class of measures with strictly positive $C^1$ densities if one wishes to determine dynamical obstructions to the presence of attractors.","sentences":["We present some rigorous results on the absence of a wide class of invariant measures for dynamical systems possessing attractors.","We then consider a generalization of the classical nonholonomic Suslov problem which shows how previous investigations of existence of invariant measures for nonholonomic systems should necessarily be extended beyond the class of measures with strictly positive $C^1$ densities if one wishes to determine dynamical obstructions to the presence of attractors."],"url":"http://arxiv.org/abs/2404.14156v1","category":"math.DS"}
{"created":"2024-04-22 13:05:45","title":"Ab initio description of monopole resonances in light- and medium-mass nuclei: III. Moments evaluation in ab initio PGCM calculations","abstract":"The paper is the third of a series dedicated to the ab initio description of monopole giant resonances in mid-mass closed- and open-shell nuclei via the so-called projected generator coordinate method. The present focus is on the computation of the moments $m_k$ of the monopole strength distribution, which are used to quantify its centroid energy and dispersion. First, the capacity to compute low-order moments via two different methods is developed and benchmarked for the $m_1$ moment. Second, the impact of the angular momentum projection on the centroid energy and dispersion of the monopole strength is analysed before comparing the results to those obtained from consistent quasi-particle random phase approximation calculations. Next, the so-called energy weighted sum rule (EWSR) is investigated. First, the appropriate ESWR in the center-of-mass frame is derived analytically. Second, the exhaustion of the intrinsic EWSR is tested in order to quantify the (unwanted) local-gauge symmetry breaking of the presently employed chiral effective field theory ($\\chi$EFT) interactions. Finally, the infinite nuclear matter incompressibility associated with the employed $\\chi$EFT interactions is extracted by extrapolating the finite-nucleus incompressibility computed from the monopole centroid energy.","sentences":["The paper is the third of a series dedicated to the ab initio description of monopole giant resonances in mid-mass closed- and open-shell nuclei via the so-called projected generator coordinate method.","The present focus is on the computation of the moments $m_k$ of the monopole strength distribution, which are used to quantify its centroid energy and dispersion.","First, the capacity to compute low-order moments via two different methods is developed and benchmarked for the $m_1$ moment.","Second, the impact of the angular momentum projection on the centroid energy and dispersion of the monopole strength is analysed before comparing the results to those obtained from consistent quasi-particle random phase approximation calculations.","Next, the so-called energy weighted sum rule (EWSR) is investigated.","First, the appropriate ESWR in the center-of-mass frame is derived analytically.","Second, the exhaustion of the intrinsic EWSR is tested in order to quantify the (unwanted) local-gauge symmetry breaking of the presently employed chiral effective field theory ($\\chi$EFT) interactions.","Finally, the infinite nuclear matter incompressibility associated with the employed $\\chi$EFT interactions is extracted by extrapolating the finite-nucleus incompressibility computed from the monopole centroid energy."],"url":"http://arxiv.org/abs/2404.14154v1","category":"nucl-th"}
{"created":"2024-04-22 13:02:38","title":"185 mW, 1 MHz, 15 fs carrier-envelope phase-stable pulse generation via polarization-optimized down-conversion from gas-filled hollow-core fiber","abstract":"Gas-filled hollow core fibers allow the generation of single-cycle pulses at megahertz repetition rates. When coupled with difference frequency generation, they can be an ideal driver for the generation of carrier-envelope phase stable, octave-spanning pulses in the short-wavelength infrared. In this work, we investigate the dependence of the polarization state in gas-filled hollow-core fibers on the subsequent difference frequency generation stage. We show that by adjusting the input polarization state of light in geometrically symmetric systems, such as hollow-core fibers, one can achieve precise control over the polarization state of the output pulses. Importantly, this manipulation preserves the temporal characteristics of the ultrashort pulses generated, especially when operating near the single-cycle regime. We leverage this property to boost the down-conversion efficiency of these pulses in a type I difference frequency generation stage. Our technique overcomes the bandwidth and dispersion constraints of the previous methods that rely on broadband waveplates or adjustment of crystal axes relative to the laboratory frame. This advancement is crucial for experiments demanding pure polarization states in the eigenmodes of the laboratory frame.","sentences":["Gas-filled hollow core fibers allow the generation of single-cycle pulses at megahertz repetition rates.","When coupled with difference frequency generation, they can be an ideal driver for the generation of carrier-envelope phase stable, octave-spanning pulses in the short-wavelength infrared.","In this work, we investigate the dependence of the polarization state in gas-filled hollow-core fibers on the subsequent difference frequency generation stage.","We show that by adjusting the input polarization state of light in geometrically symmetric systems, such as hollow-core fibers, one can achieve precise control over the polarization state of the output pulses.","Importantly, this manipulation preserves the temporal characteristics of the ultrashort pulses generated, especially when operating near the single-cycle regime.","We leverage this property to boost the down-conversion efficiency of these pulses in a type I difference frequency generation stage.","Our technique overcomes the bandwidth and dispersion constraints of the previous methods that rely on broadband waveplates or adjustment of crystal axes relative to the laboratory frame.","This advancement is crucial for experiments demanding pure polarization states in the eigenmodes of the laboratory frame."],"url":"http://arxiv.org/abs/2404.14153v1","category":"physics.optics"}
{"created":"2024-04-22 12:55:55","title":"A Linear Relationship between Correlation and Cohen's Kappa for Binary Data and Simulating Multivariate Nominal and Ordinal Data with Specified Kappa Matrix","abstract":"Cohen's kappa is a useful measure for agreement between the judges, inter-rater reliability, and also goodness of fit in classification problems. For binary nominal and ordinal data, kappa and correlation are equally applicable. We have found a linear relationship between correlation and kappa for binary data. Exact bounds of kappa are much more important as kappa can be only .5 even if there is very strong agreement. The exact upper bound was developed by Cohen (1960) but the exact lower bound is also important if the range of kappa is small for some marginals. We have developed an algorithm to find the exact lower bound given marginal proportions. Our final contribution is a method to generate multivariate nominal and ordinal data with a specified kappa matrix based on the rearrangement of independently generated marginal data to a multidimensional contingency table, where cell counts are found by solving system of linear equations for positive roots.","sentences":["Cohen's kappa is a useful measure for agreement between the judges, inter-rater reliability, and also goodness of fit in classification problems.","For binary nominal and ordinal data, kappa and correlation are equally applicable.","We have found a linear relationship between correlation and kappa for binary data.","Exact bounds of kappa are much more important as kappa can be only .5","even if there is very strong agreement.","The exact upper bound was developed by Cohen (1960) but the exact lower bound is also important if the range of kappa is small for some marginals.","We have developed an algorithm to find the exact lower bound given marginal proportions.","Our final contribution is a method to generate multivariate nominal and ordinal data with a specified kappa matrix based on the rearrangement of independently generated marginal data to a multidimensional contingency table, where cell counts are found by solving system of linear equations for positive roots."],"url":"http://arxiv.org/abs/2404.14149v1","category":"stat.ME"}
{"created":"2024-04-22 12:50:24","title":"Universality of the Wigner-Gurau limit for random tensors","abstract":"In this article, we develop a combinatorial approach for studying moments of the resolvent trace for random tensors proposed by Razvan Gurau. Our work is based on the study of hypergraphs and extends the combinatorial proof of moments convergence for Wigner's theorem. This also opens up paths for research akin to free probability for random tensors.   Specifically, trace invariants form a complete basis of tensor invariants and constitute the moments of the resolvent trace. For a random tensor with entries independent, centered, with the right variance and bounded moments, we prove the convergence of the expectation and bound the variance of the balanced single trace invariant. This implies the universality of the convergence of the associated measure towards the law obtained by Gurau in the Gaussian case, whose limiting moments are given by the Fuss-Catalan numbers. This generalizes Wigner's theorem for random tensors.   Additionally, in the Gaussian case, we show that the limiting distribution of the $k$-times contracted $p$-order random tensor by a deterministic vector is always the Wigner-Gurau law at order $p-k$, dilated by $\\sqrt{\\binom{p-1}{k}}$. This establishes a connection with the approach of random tensors through the matrix study of the $p-2$ contractions of the tensor.","sentences":["In this article, we develop a combinatorial approach for studying moments of the resolvent trace for random tensors proposed by Razvan Gurau.","Our work is based on the study of hypergraphs and extends the combinatorial proof of moments convergence for Wigner's theorem.","This also opens up paths for research akin to free probability for random tensors.   ","Specifically, trace invariants form a complete basis of tensor invariants and constitute the moments of the resolvent trace.","For a random tensor with entries independent, centered, with the right variance and bounded moments, we prove the convergence of the expectation and bound the variance of the balanced single trace invariant.","This implies the universality of the convergence of the associated measure towards the law obtained by Gurau in the Gaussian case, whose limiting moments are given by the Fuss-Catalan numbers.","This generalizes Wigner's theorem for random tensors.   ","Additionally, in the Gaussian case, we show that the limiting distribution of the $k$-times contracted $p$-order random tensor by a deterministic vector is always the Wigner-Gurau law at order $p-k$, dilated by $\\sqrt{\\binom{p-1}{k}}$. This establishes a connection with the approach of random tensors through the matrix study of the $p-2$ contractions of the tensor."],"url":"http://arxiv.org/abs/2404.14144v1","category":"math.PR"}
{"created":"2024-04-22 12:50:15","title":"Access-Point to Access-Point Connectivity for PON-based OWC Spine and Leaf Data Centre Architecture","abstract":"In this paper, we propose incorporating Optical Wireless Communication (OWC) and Passive Optical Network (PON) technologies into next generation spine-and-leaf Data Centre Networks (DCNs). In this work, OWC systems are used to connect the Data Centre (DC) racks through Wavelength Division Multiplexing (WDM) Infrared (IR) transceivers. The transceivers are placed on top of the racks and at distributed Access Points (APs) in the ceiling. Each transceiver on a rack is connected to a leaf switch that connects the servers within the rack. We replace the spine switches by Optical Line Terminal (OLT) and Network Interface Cards (NIC) in the APs to achieve the desired connectivity. We benchmark the power consumption of the proposed OWC-PON-based spine-and-leaf DC against traditional spine-and-leaf DC and report 46% reduction in the power consumption when considering eight racks.","sentences":["In this paper, we propose incorporating Optical Wireless Communication (OWC) and Passive Optical Network (PON) technologies into next generation spine-and-leaf Data Centre Networks (DCNs).","In this work, OWC systems are used to connect the Data Centre (DC) racks through Wavelength Division Multiplexing (WDM) Infrared (IR) transceivers.","The transceivers are placed on top of the racks and at distributed Access Points (APs) in the ceiling.","Each transceiver on a rack is connected to a leaf switch that connects the servers within the rack.","We replace the spine switches by Optical Line Terminal (OLT) and Network Interface Cards (NIC) in the APs to achieve the desired connectivity.","We benchmark the power consumption of the proposed OWC-PON-based spine-and-leaf DC against traditional spine-and-leaf DC and report 46% reduction in the power consumption when considering eight racks."],"url":"http://arxiv.org/abs/2404.14143v1","category":"cs.NI"}
{"created":"2024-04-22 12:46:35","title":"Competition and Collaboration in Crowdsourcing Communities: What happens when peers evaluate each other?","abstract":"Crowdsourcing has evolved as an organizational approach to distributed problem solving and innovation. As contests are embedded in online communities and evaluation rights are assigned to the crowd, community members face a tension: they find themselves exposed to both competitive motives to win the contest prize and collaborative participation motives in the community. The competitive motive suggests they may evaluate rivals strategically according to their self-interest, the collaborative motive suggests they may evaluate their peers truthfully according to mutual interest. Using field data from Threadless on 38 million peer evaluations of more than 150,000 submissions across 75,000 individuals over 10 years and two natural experiments to rule out alternative explanations, we answer the question of how community members resolve this tension. We show that as their skill level increases, they become increasingly competitive and shift from using self-promotion to sabotaging their closest competitors. However, we also find signs of collaborative behavior when high-skilled members show leniency toward those community members who do not directly threaten their chance of winning. We explain how the individual-level use of strategic evaluations translates into important organizational-level outcomes by affecting the community structure through individuals' long-term participation. While low-skill targets of sabotage are less likely to participate in future contests, high-skill targets are more likely. This suggests a feedback loop between competitive evaluation behavior and future participation. These findings have important implications for the literature on crowdsourcing design, and the evolution and sustainability of crowdsourcing communities.","sentences":["Crowdsourcing has evolved as an organizational approach to distributed problem solving and innovation.","As contests are embedded in online communities and evaluation rights are assigned to the crowd, community members face a tension: they find themselves exposed to both competitive motives to win the contest prize and collaborative participation motives in the community.","The competitive motive suggests they may evaluate rivals strategically according to their self-interest, the collaborative motive suggests they may evaluate their peers truthfully according to mutual interest.","Using field data from Threadless on 38 million peer evaluations of more than 150,000 submissions across 75,000 individuals over 10 years and two natural experiments to rule out alternative explanations, we answer the question of how community members resolve this tension.","We show that as their skill level increases, they become increasingly competitive and shift from using self-promotion to sabotaging their closest competitors.","However, we also find signs of collaborative behavior when high-skilled members show leniency toward those community members who do not directly threaten their chance of winning.","We explain how the individual-level use of strategic evaluations translates into important organizational-level outcomes by affecting the community structure through individuals' long-term participation.","While low-skill targets of sabotage are less likely to participate in future contests, high-skill targets are more likely.","This suggests a feedback loop between competitive evaluation behavior and future participation.","These findings have important implications for the literature on crowdsourcing design, and the evolution and sustainability of crowdsourcing communities."],"url":"http://arxiv.org/abs/2404.14141v1","category":"econ.GN"}
{"created":"2024-04-22 12:45:40","title":"Generative Artificial Intelligence Assisted Wireless Sensing: Human Flow Detection in Practical Communication Environments","abstract":"Groundbreaking applications such as ChatGPT have heightened research interest in generative artificial intelligence (GAI). Essentially, GAI excels not only in content generation but also in signal processing, offering support for wireless sensing. Hence, we introduce a novel GAI-assisted human flow detection system (G-HFD). Rigorously, G-HFD first uses channel state information (CSI) to estimate the velocity and acceleration of propagation path length change of the human-induced reflection (HIR). Then, given the strong inference ability of the diffusion model, we propose a unified weighted conditional diffusion model (UW-CDM) to denoise the estimation results, enabling the detection of the number of targets. Next, we use the CSI obtained by a uniform linear array with wavelength spacing to estimate the HIR's time of flight and direction of arrival (DoA). In this process, UW-CDM solves the problem of ambiguous DoA spectrum, ensuring accurate DoA estimation. Finally, through clustering, G-HFD determines the number of subflows and the number of targets in each subflow, i.e., the subflow size. The evaluation based on practical downlink communication signals shows G-HFD's accuracy of subflow size detection can reach 91%. This validates its effectiveness and underscores the significant potential of GAI in the context of wireless sensing.","sentences":["Groundbreaking applications such as ChatGPT have heightened research interest in generative artificial intelligence (GAI).","Essentially, GAI excels not only in content generation but also in signal processing, offering support for wireless sensing.","Hence, we introduce a novel GAI-assisted human flow detection system (G-HFD).","Rigorously, G-HFD first uses channel state information (CSI) to estimate the velocity and acceleration of propagation path length change of the human-induced reflection (HIR).","Then, given the strong inference ability of the diffusion model, we propose a unified weighted conditional diffusion model (UW-CDM) to denoise the estimation results, enabling the detection of the number of targets.","Next, we use the CSI obtained by a uniform linear array with wavelength spacing to estimate the HIR's time of flight and direction of arrival (DoA).","In this process, UW-CDM solves the problem of ambiguous DoA spectrum, ensuring accurate DoA estimation.","Finally, through clustering, G-HFD determines the number of subflows and the number of targets in each subflow, i.e., the subflow size.","The evaluation based on practical downlink communication signals shows G-HFD's accuracy of subflow size detection can reach 91%.","This validates its effectiveness and underscores the significant potential of GAI in the context of wireless sensing."],"url":"http://arxiv.org/abs/2404.14140v1","category":"eess.SP"}
{"created":"2024-04-22 12:40:18","title":"Elicitability and identifiability of tail risk measures","abstract":"Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk and Expected Shortfall being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.","sentences":["Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk and Expected Shortfall being prime examples.","They are induced by law-based risk measures, called their generators, evaluated on the tail distribution.","This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively.","As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile.","The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile.","For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures."],"url":"http://arxiv.org/abs/2404.14136v1","category":"q-fin.ST"}
{"created":"2024-04-22 12:34:53","title":"Quantum Convolutional Neural Networks for the detection of Gamma-Ray Bursts in the AGILE space mission data","abstract":"Quantum computing represents a cutting-edge frontier in artificial intelligence. It makes use of hybrid quantum-classical computation which tries to leverage quantum mechanic principles that allow us to use a different approach to deep learning classification problems. The work presented here falls within the context of the AGILE space mission, launched in 2007 by the Italian Space Agency. We implement different Quantum Convolutional Neural Networks (QCNN) that analyze data acquired by the instruments onboard AGILE to detect Gamma-Ray Bursts from sky maps or light curves. We use several frameworks such as TensorFlow-Quantum, Qiskit and PennyLane to simulate a quantum computer. We achieved an accuracy of 95.1% on sky maps with QCNNs, while the classical counterpart achieved 98.8% on the same data, using however hundreds of thousands more parameters.","sentences":["Quantum computing represents a cutting-edge frontier in artificial intelligence.","It makes use of hybrid quantum-classical computation which tries to leverage quantum mechanic principles that allow us to use a different approach to deep learning classification problems.","The work presented here falls within the context of the AGILE space mission, launched in 2007 by the Italian Space Agency.","We implement different Quantum Convolutional Neural Networks (QCNN) that analyze data acquired by the instruments onboard AGILE to detect Gamma-Ray Bursts from sky maps or light curves.","We use several frameworks such as TensorFlow-Quantum, Qiskit and PennyLane to simulate a quantum computer.","We achieved an accuracy of 95.1% on sky maps with QCNNs, while the classical counterpart achieved 98.8% on the same data, using however hundreds of thousands more parameters."],"url":"http://arxiv.org/abs/2404.14133v1","category":"astro-ph.HE"}
{"created":"2024-04-22 12:33:07","title":"Possible signatures of higher dimension in thin accretion disk around brane world black hole","abstract":"We probe deeply into the characteristics of thin accretion disk surrounding black hole within the brane world paradigm. We investigate how model parameters affect the physical properties of the disk. Our findings indicate that as the tidal charge parameter inherited from the higher dimension increases, the energy flux, the radiation temperature, the spectral cutoff frequency, the spectral luminosity, and the conversion efficiency of the disk all increase, but the radius of the innermost stable circular orbit decreases. Compared to cases of the Kerr and Schwarzschild black holes, the disk is hotter and more luminous for positive tidal charge parameter, while it is cooler and less luminous for negative tidal charge parameter, which suggests the potential for probing possible signatures of higher dimension.","sentences":["We probe deeply into the characteristics of thin accretion disk surrounding black hole within the brane world paradigm.","We investigate how model parameters affect the physical properties of the disk.","Our findings indicate that as the tidal charge parameter inherited from the higher dimension increases, the energy flux, the radiation temperature, the spectral cutoff frequency, the spectral luminosity, and the conversion efficiency of the disk all increase, but the radius of the innermost stable circular orbit decreases.","Compared to cases of the Kerr and Schwarzschild black holes, the disk is hotter and more luminous for positive tidal charge parameter, while it is cooler and less luminous for negative tidal charge parameter, which suggests the potential for probing possible signatures of higher dimension."],"url":"http://arxiv.org/abs/2404.14131v1","category":"gr-qc"}
{"created":"2024-04-22 12:25:00","title":"Brane mechanics and gapped Lie n-algebroids","abstract":"We draw a parallel between the BV/BRST formalism for higher-dimensional ($\\ge 2$) Hamiltonian mechanics and higher notions of torsion and basic curvature tensors for generalized connections in specific Lie $n$-algebroids based on homotopy Poisson structures. The gauge systems we consider include Poisson sigma models in any dimension and ``generalised R-flux'' deformations thereof, such as models with an $(n+2)$-form-twisted R-Poisson target space. Their BV/BRST action includes interaction terms among the fields, ghosts and antifields whose coefficients acquire a geometric meaning by considering twisted Koszul multibrackets that endow the target space with a structure that we call a gapped almost Lie $n$-algebroid. Studying covariant derivatives along $n$-forms, we define suitable polytorsion and basic polycurvature tensors and identify them with the interaction coefficients in the gauge theory, thus relating models for topological $n$-branes to differential geometry on Lie $n$-algebroids.","sentences":["We draw a parallel between the BV/BRST formalism for higher-dimensional ($\\ge 2$) Hamiltonian mechanics and higher notions of torsion and basic curvature tensors for generalized connections in specific Lie $n$-algebroids based on homotopy Poisson structures.","The gauge systems we consider include Poisson sigma models in any dimension and ``generalised R-flux'' deformations thereof, such as models with an $(n+2)$-form-twisted R-Poisson target space.","Their BV/BRST action includes interaction terms among the fields, ghosts and antifields whose coefficients acquire a geometric meaning by considering twisted Koszul multibrackets that endow the target space with a structure that we call a gapped almost Lie $n$-algebroid.","Studying covariant derivatives along $n$-forms, we define suitable polytorsion and basic polycurvature tensors and identify them with the interaction coefficients in the gauge theory, thus relating models for topological $n$-branes to differential geometry on Lie $n$-algebroids."],"url":"http://arxiv.org/abs/2404.14126v1","category":"hep-th"}
{"created":"2024-04-22 12:24:49","title":"Gaussian distributional structural equation models: A framework for modeling latent heteroscedasticity","abstract":"Accounting for the complexity of psychological theories requires methods that can predict not only changes in the means of latent variables -- such as personality factors, creativity, or intelligence -- but also changes in their variances. Structural equation modeling (SEM) is the framework of choice for analyzing complex relationships among latent variables, but current methods do not allow modeling latent variances as a function of other latent variables. In this paper, we develop a Bayesian framework for Gaussian distributional SEM which overcomes this limitation. We validate our framework using extensive simulations, which demonstrate that the new models produce reliable statistical inference and can be computed with sufficient efficiency for practical everyday use. We illustrate our framework's applicability in a real-world case study that addresses a substantive hypothesis from personality psychology.","sentences":["Accounting for the complexity of psychological theories requires methods that can predict not only changes in the means of latent variables -- such as personality factors, creativity, or intelligence -- but also changes in their variances.","Structural equation modeling (SEM) is the framework of choice for analyzing complex relationships among latent variables, but current methods do not allow modeling latent variances as a function of other latent variables.","In this paper, we develop a Bayesian framework for Gaussian distributional SEM which overcomes this limitation.","We validate our framework using extensive simulations, which demonstrate that the new models produce reliable statistical inference and can be computed with sufficient efficiency for practical everyday use.","We illustrate our framework's applicability in a real-world case study that addresses a substantive hypothesis from personality psychology."],"url":"http://arxiv.org/abs/2404.14124v1","category":"stat.ME"}
{"created":"2024-04-22 12:23:09","title":"Cooperativity, information gain, and energy cost during early LTP in dendritic spines","abstract":"We investigate a mutual relationship between information and energy during early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics. In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower dimensional manageable system of closed equations. It is found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e. during stimulation), and after that they recover to their baseline low values, as opposed to a memory trace that lasts much longer. This suggests that learning phase is much more energy demanding than the memory phase. We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration. In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity. We also find that dendritic spines can use sparse representations for encoding of long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP. In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding and its energy cost during learning and memory in stochastic systems of interacting synapses.","sentences":["We investigate a mutual relationship between information and energy during early phase of LTP induction and maintenance in a large-scale system of mutually coupled dendritic spines, with discrete internal states and probabilistic dynamics, within the framework of nonequilibrium stochastic thermodynamics.","In order to analyze this computationally intractable stochastic multidimensional system, we introduce a pair approximation, which allows us to reduce the spine dynamics into a lower dimensional manageable system of closed equations.","It is found that the rates of information gain and energy attain their maximal values during an initial period of LTP (i.e. during stimulation), and after that they recover to their baseline low values, as opposed to a memory trace that lasts much longer.","This suggests that learning phase is much more energy demanding than the memory phase.","We show that positive correlations between neighboring spines increase both a duration of memory trace and energy cost during LTP, but the memory time per invested energy increases dramatically for very strong positive synaptic cooperativity, suggesting a beneficial role of synaptic clustering on memory duration.","In contrast, information gain after LTP is the largest for negative correlations, and energy efficiency of that information generally declines with increasing synaptic cooperativity.","We also find that dendritic spines can use sparse representations for encoding of long-term information, as both energetic and structural efficiencies of retained information and its lifetime exhibit maxima for low fractions of stimulated synapses during LTP.","In general, our stochastic thermodynamics approach provides a unifying framework for studying, from first principles, information encoding and its energy cost during learning and memory in stochastic systems of interacting synapses."],"url":"http://arxiv.org/abs/2404.14123v1","category":"q-bio.NC"}
{"created":"2024-04-22 12:07:10","title":"Hierarchical localization with panoramic views and triplet loss functions","abstract":"The main objective of this paper is to address the mobile robot localization problem with Triplet Convolutional Neural Networks and test their robustness against changes of the lighting conditions. We have used omnidirectional images from real indoor environments captured in dynamic conditions that have been converted to panoramic format. Two approaches are proposed to address localization by means of triplet neural networks. First, hierarchical localization, which consists in estimating the robot position in two stages: a coarse localization, which involves a room retrieval task, and a fine localization is addressed by means of image retrieval in the previously selected room. Second, global localization, which consists in estimating the position of the robot inside the entire map in a unique step. Besides, an exhaustive study of the loss function influence on the network learning process has been made. The experimental section proves that triplet neural networks are an efficient and robust tool to address the localization of mobile robots in indoor environments, considering real operation conditions.","sentences":["The main objective of this paper is to address the mobile robot localization problem with Triplet Convolutional Neural Networks and test their robustness against changes of the lighting conditions.","We have used omnidirectional images from real indoor environments captured in dynamic conditions that have been converted to panoramic format.","Two approaches are proposed to address localization by means of triplet neural networks.","First, hierarchical localization, which consists in estimating the robot position in two stages: a coarse localization, which involves a room retrieval task, and a fine localization is addressed by means of image retrieval in the previously selected room.","Second, global localization, which consists in estimating the position of the robot inside the entire map in a unique step.","Besides, an exhaustive study of the loss function influence on the network learning process has been made.","The experimental section proves that triplet neural networks are an efficient and robust tool to address the localization of mobile robots in indoor environments, considering real operation conditions."],"url":"http://arxiv.org/abs/2404.14117v1","category":"cs.RO"}
{"created":"2024-04-22 11:54:03","title":"HomeLabGym: A real-world testbed for home energy management systems","abstract":"Amid growing environmental concerns and resulting energy costs, there is a rising need for efficient Home Energy Management Systems (HEMS). Evaluating such innovative HEMS solutions typically relies on simulations that may not model the full complexity of a real-world scenario. On the other hand, real-world testing, while more accurate, is labor-intensive, particularly when dealing with diverse assets, each using a distinct communication protocol or API. Centralizing and synchronizing the control of such a heterogeneous pool of assets thus poses a significant challenge. In this paper, we introduce HomeLabGym, a real-world testbed to ease such real-world evaluations of HEMS and flexible assets control in general, by adhering to the well-known OpenAI Gym paradigm. HomeLabGym allows researchers to prototype, deploy, and analyze HEMS controllers within the controlled test environment of a real-world house (the IDLab HomeLab), providing access to all its available sensors and smart appliances. The easy-to-use Python interface eliminates concerns about intricate communication protocols associated with sensors and appliances, streamlining the evaluation of various control strategies. We present an overview of HomeLabGym, and demonstrate its usefulness to researchers in a comparison between real-world and simulated environments in controlling a residential battery in response to real-time prices.","sentences":["Amid growing environmental concerns and resulting energy costs, there is a rising need for efficient Home Energy Management Systems (HEMS).","Evaluating such innovative HEMS solutions typically relies on simulations that may not model the full complexity of a real-world scenario.","On the other hand, real-world testing, while more accurate, is labor-intensive, particularly when dealing with diverse assets, each using a distinct communication protocol or API.","Centralizing and synchronizing the control of such a heterogeneous pool of assets thus poses a significant challenge.","In this paper, we introduce HomeLabGym, a real-world testbed to ease such real-world evaluations of HEMS and flexible assets control in general, by adhering to the well-known OpenAI Gym paradigm.","HomeLabGym allows researchers to prototype, deploy, and analyze HEMS controllers within the controlled test environment of a real-world house (the IDLab HomeLab), providing access to all its available sensors and smart appliances.","The easy-to-use Python interface eliminates concerns about intricate communication protocols associated with sensors and appliances, streamlining the evaluation of various control strategies.","We present an overview of HomeLabGym, and demonstrate its usefulness to researchers in a comparison between real-world and simulated environments in controlling a residential battery in response to real-time prices."],"url":"http://arxiv.org/abs/2404.14110v1","category":"eess.SY"}
{"created":"2024-04-22 11:52:23","title":"PGNAA Spectral Classification of Aluminium and Copper Alloys with Machine Learning","abstract":"In this paper, we explore the optimization of metal recycling with a focus on real-time differentiation between alloys of copper and aluminium. Spectral data, obtained through Prompt Gamma Neutron Activation Analysis (PGNAA), is utilized for classification. The study compares data from two detectors, cerium bromide (CeBr$_{3}$) and high purity germanium (HPGe), considering their energy resolution and sensitivity. We test various data generation, preprocessing, and classification methods, with Maximum Likelihood Classifier (MLC) and Conditional Variational Autoencoder (CVAE) yielding the best results. The study also highlights the impact of different detector types on classification accuracy, with CeBr$_{3}$ excelling in short measurement times and HPGe performing better in longer durations. The findings suggest the importance of selecting the appropriate detector and methodology based on specific application requirements.","sentences":["In this paper, we explore the optimization of metal recycling with a focus on real-time differentiation between alloys of copper and aluminium.","Spectral data, obtained through Prompt Gamma Neutron Activation Analysis (PGNAA), is utilized for classification.","The study compares data from two detectors, cerium bromide (CeBr$_{3}$) and high purity germanium (HPGe), considering their energy resolution and sensitivity.","We test various data generation, preprocessing, and classification methods, with Maximum Likelihood Classifier (MLC) and Conditional Variational Autoencoder (CVAE) yielding the best results.","The study also highlights the impact of different detector types on classification accuracy, with CeBr$_{3}$ excelling in short measurement times and HPGe performing better in longer durations.","The findings suggest the importance of selecting the appropriate detector and methodology based on specific application requirements."],"url":"http://arxiv.org/abs/2404.14107v1","category":"cs.LG"}
{"created":"2024-04-22 11:52:11","title":"DPTraj-PM: Differentially Private Trajectory Synthesis Using Prefix Tree and Markov Process","abstract":"The increasing use of GPS-enabled devices has generated a large amount of trajectory data. These data offer us vital insights to understand the movements of individuals and populations, benefiting a broad range of applications from transportation planning to epidemic modeling. However, improper release of trajectory data is increasing concerns on individual privacy. Previous attempts either lack strong privacy guarantees, or fail to preserve sufficient basic characteristics of the original data. In this paper, we propose DPTraj-PM, a method to synthesize trajectory dataset under the differential privacy (DP) framework while ensures high data utility. Based on the assumption that an individual's trajectory could be mainly determined by the initial trajectory segment (which depicts the starting point and the initial direction) and the next location point, DPTraj-PM discretizes the raw trajectories into neighboring cells, and models them by combining a prefix tree structure and an m-order Markov process. After adding noise to the model under differential privacy, DPTraj-PM generates a synthetic dataset from the noisy model to enable a wider spectrum of data mining and modeling tasks. The output traces crafted by DPTraj-PM not only preserves the patterns and variability in individuals' mobility behaviors, but also protects individual privacy. Experiments on two real-world datasets demonstrate that DPTraj-PM substantially outperforms the state-of-the-art techniques in terms of data utility. Our code is available at https://github.com/wnn5/DP-PrefixTreeMarkov.","sentences":["The increasing use of GPS-enabled devices has generated a large amount of trajectory data.","These data offer us vital insights to understand the movements of individuals and populations, benefiting a broad range of applications from transportation planning to epidemic modeling.","However, improper release of trajectory data is increasing concerns on individual privacy.","Previous attempts either lack strong privacy guarantees, or fail to preserve sufficient basic characteristics of the original data.","In this paper, we propose DPTraj-PM, a method to synthesize trajectory dataset under the differential privacy (DP) framework while ensures high data utility.","Based on the assumption that an individual's trajectory could be mainly determined by the initial trajectory segment (which depicts the starting point and the initial direction) and the next location point, DPTraj-PM discretizes the raw trajectories into neighboring cells, and models them by combining a prefix tree structure and an m-order Markov process.","After adding noise to the model under differential privacy, DPTraj-PM generates a synthetic dataset from the noisy model to enable a wider spectrum of data mining and modeling tasks.","The output traces crafted by DPTraj-PM not only preserves the patterns and variability in individuals' mobility behaviors, but also protects individual privacy.","Experiments on two real-world datasets demonstrate that DPTraj-PM substantially outperforms the state-of-the-art techniques in terms of data utility.","Our code is available at https://github.com/wnn5/DP-PrefixTreeMarkov."],"url":"http://arxiv.org/abs/2404.14106v1","category":"cs.CR"}
{"created":"2024-04-22 11:51:00","title":"The effect of frames on engagement with quantum technology","abstract":"Quantum technology is predicted to have a significant impact on society once it matures. This study (n = 637 adults representative of the Dutch population) examined the effect of different frames on engagement - specifically, information seeking, internal efficacy, general interest and perceived knowledge - with quantum technology. The different frames were: enigmatic, explaining quantum physics, benefit, risk and balanced. Results indicated that framing quantum as enigmatic does not affect engagement, while explaining quantum physics positively influences general interest. Furthermore, emphasising a benefit of quantum technology increases participants' internal efficacy, whereas highlighting both a benefit and a risk of quantum technology decreases perceived knowledge. Based on these findings, we offer practical advice for science communicators in the field and suggest further research.","sentences":["Quantum technology is predicted to have a significant impact on society once it matures.","This study (n = 637 adults representative of the Dutch population) examined the effect of different frames on engagement - specifically, information seeking, internal efficacy, general interest and perceived knowledge - with quantum technology.","The different frames were: enigmatic, explaining quantum physics, benefit, risk and balanced.","Results indicated that framing quantum as enigmatic does not affect engagement, while explaining quantum physics positively influences general interest.","Furthermore, emphasising a benefit of quantum technology increases participants' internal efficacy, whereas highlighting both a benefit and a risk of quantum technology decreases perceived knowledge.","Based on these findings, we offer practical advice for science communicators in the field and suggest further research."],"url":"http://arxiv.org/abs/2404.14104v1","category":"physics.soc-ph"}
{"created":"2024-04-22 11:40:08","title":"Efficient molecular conformation generation with quantum-inspired algorithm","abstract":"Conformation generation, also known as molecular unfolding (MU), is a crucial step in structure-based drug design, remaining a challenging combinatorial optimization problem. Quantum annealing (QA) has shown great potential for solving certain combinatorial optimization problems over traditional classical methods such as simulated annealing (SA). However, a recent study showed that a 2000-qubit QA hardware was still unable to outperform SA for the MU problem. Here, we propose the use of quantum-inspired algorithm to solve the MU problem, in order to go beyond traditional SA. We introduce a highly-compact phase encoding method which can exponentially reduce the representation space, compared with the previous one-hot encoding method. For benchmarking, we tested this new approach on the public QM9 dataset generated by density functional theory (DFT). The root-mean-square deviation between the conformation determined by our approach and DFT is negligible (less than about 0.5 Angstrom), which underpins the validity of our approach. Furthermore, the median time-to-target metric can be reduced by a factor of five compared to SA. Additionally, we demonstrate a simulation experiment by MindQuantum using quantum approximate optimization algorithm (QAOA) to reach optimal results. These results indicate that quantum-inspired algorithms can be applied to solve practical problems even before quantum hardware become mature.","sentences":["Conformation generation, also known as molecular unfolding (MU), is a crucial step in structure-based drug design, remaining a challenging combinatorial optimization problem.","Quantum annealing (QA) has shown great potential for solving certain combinatorial optimization problems over traditional classical methods such as simulated annealing (SA).","However, a recent study showed that a 2000-qubit QA hardware was still unable to outperform SA for the MU problem.","Here, we propose the use of quantum-inspired algorithm to solve the MU problem, in order to go beyond traditional SA.","We introduce a highly-compact phase encoding method which can exponentially reduce the representation space, compared with the previous one-hot encoding method.","For benchmarking, we tested this new approach on the public QM9 dataset generated by density functional theory (DFT).","The root-mean-square deviation between the conformation determined by our approach and DFT is negligible (less than about 0.5 Angstrom), which underpins the validity of our approach.","Furthermore, the median time-to-target metric can be reduced by a factor of five compared to SA.","Additionally, we demonstrate a simulation experiment by MindQuantum using quantum approximate optimization algorithm (QAOA) to reach optimal results.","These results indicate that quantum-inspired algorithms can be applied to solve practical problems even before quantum hardware become mature."],"url":"http://arxiv.org/abs/2404.14101v1","category":"quant-ph"}
{"created":"2024-04-22 11:28:34","title":"Immersive Rover Control and Obstacle Detection based on Extended Reality and Artificial Intelligence","abstract":"Lunar exploration has become a key focus, driving scientific and technological advances. Ongoing missions are deploying rovers to the surface of the Moon, targeting the far side and south pole. However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks. This work proposes a novel system that integrates eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar rovers. It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the location of the robot. This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches","sentences":["Lunar exploration has become a key focus, driving scientific and technological advances.","Ongoing missions are deploying rovers to the surface of the Moon, targeting the far side and south pole.","However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks.","This work proposes a novel system that integrates eXtended Reality (XR) and Artificial Intelligence (AI) to teleoperate lunar rovers.","It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the location of the robot.","This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches"],"url":"http://arxiv.org/abs/2404.14095v1","category":"cs.RO"}
{"created":"2024-04-22 11:25:37","title":"Multi-agent Reinforcement Learning-based Joint Precoding and Phase Shift Optimization for RIS-aided Cell-Free Massive MIMO Systems","abstract":"Cell-free (CF) massive multiple-input multiple-output (mMIMO) is a promising technique for achieving high spectral efficiency (SE) using multiple distributed access points (APs). However, harsh propagation environments often lead to significant communication performance degradation due to high penetration loss. To overcome this issue, we introduce the reconfigurable intelligent surface (RIS) into the CF mMIMO system as a low-cost and power-efficient solution. In this paper, we focus on optimizing the joint precoding design of the RIS-aided CF mMIMO system to maximize the sum SE. This involves optimizing the precoding matrix at the APs and the reflection coefficients at the RIS. To tackle this problem, we propose a fully distributed multi-agent reinforcement learning (MARL) algorithm that incorporates fuzzy logic (FL). Unlike conventional approaches that rely on alternating optimization techniques, our FL-based MARL algorithm only requires local channel state information, which reduces the need for high backhaul capacity. Simulation results demonstrate that our proposed FL-MARL algorithm effectively reduces computational complexity while achieving similar performance as conventional MARL methods.","sentences":["Cell-free (CF) massive multiple-input multiple-output (mMIMO) is a promising technique for achieving high spectral efficiency (SE) using multiple distributed access points (APs).","However, harsh propagation environments often lead to significant communication performance degradation due to high penetration loss.","To overcome this issue, we introduce the reconfigurable intelligent surface (RIS) into the CF mMIMO system as a low-cost and power-efficient solution.","In this paper, we focus on optimizing the joint precoding design of the RIS-aided CF mMIMO system to maximize the sum SE.","This involves optimizing the precoding matrix at the APs and the reflection coefficients at the RIS.","To tackle this problem, we propose a fully distributed multi-agent reinforcement learning (MARL) algorithm that incorporates fuzzy logic (FL).","Unlike conventional approaches that rely on alternating optimization techniques, our FL-based MARL algorithm only requires local channel state information, which reduces the need for high backhaul capacity.","Simulation results demonstrate that our proposed FL-MARL algorithm effectively reduces computational complexity while achieving similar performance as conventional MARL methods."],"url":"http://arxiv.org/abs/2404.14092v1","category":"cs.IT"}
{"created":"2024-04-22 11:22:02","title":"Local univalence versus stability and causality in hydrodynamic models","abstract":"Our main objective is to compare the analytic properties of hydrodynamic series with the stability and causality conditions applied to hydrodynamic modes. Analyticity, in this context, implies that the hydrodynamic series behaves as a univalent or single-valued function. Stability and causality adhere to physical constraints where hydrodynamic modes neither exhibit exponential growth nor travel faster than the speed of light. Through an examination of various hydrodynamic models, such as the Muller-Israel-Stewart (MIS) and the first-order hydro models like the BDNK (Bemfica-Disconzi-Noronha-Kovtun) model, we observe no new restrictions stemming from the analyticity limits in the shear channel of these models. However, local univalence is maintained in the sound channel of these models despite the global divergence of the hydrodynamic series. Notably, differences in the sound equations between the MIS and BDNK models lead to distinct analyticity limits. The MIS model's sound mode remains univalent at high momenta within a specific transport range. Conversely, in the BDNK model, the univalence of the sound mode extends to intermediate momenta across all stable and causal regions. Generally, the convergence radius is independent of univalence and the given dispersion relation predominantly influences their correlation. For second-order frequency dispersions, the relationship is precise, i.e. within the convergence radius, the hydro series demonstrates univalence. However, with higher-order dispersions, the hydro series is locally univalent within certain transport regions, which may fall within or outside the stable and causal zones.","sentences":["Our main objective is to compare the analytic properties of hydrodynamic series with the stability and causality conditions applied to hydrodynamic modes.","Analyticity, in this context, implies that the hydrodynamic series behaves as a univalent or single-valued function.","Stability and causality adhere to physical constraints where hydrodynamic modes neither exhibit exponential growth nor travel faster than the speed of light.","Through an examination of various hydrodynamic models, such as the Muller-Israel-Stewart (MIS) and the first-order hydro models like the BDNK (Bemfica-Disconzi-Noronha-Kovtun) model, we observe no new restrictions stemming from the analyticity limits in the shear channel of these models.","However, local univalence is maintained in the sound channel of these models despite the global divergence of the hydrodynamic series.","Notably, differences in the sound equations between the MIS and BDNK models lead to distinct analyticity limits.","The MIS model's sound mode remains univalent at high momenta within a specific transport range.","Conversely, in the BDNK model, the univalence of the sound mode extends to intermediate momenta across all stable and causal regions.","Generally, the convergence radius is independent of univalence and the given dispersion relation predominantly influences their correlation.","For second-order frequency dispersions, the relationship is precise, i.e. within the convergence radius, the hydro series demonstrates univalence.","However, with higher-order dispersions, the hydro series is locally univalent within certain transport regions, which may fall within or outside the stable and causal zones."],"url":"http://arxiv.org/abs/2404.14091v1","category":"hep-ph"}
{"created":"2024-04-22 11:20:10","title":"Microscale Fiber-Integrated Vector Magnetometer with On-Tip Field Biasing using NV Ensembles in Diamond Microcystals","abstract":"In quantum sensing of magnetic fields, ensembles of NV centers in diamond offer high sensitivity, high bandwidth and outstanding spatial resolution while operating in harsh environments. Moreover, the orientation of defect centers along four crystal axes forms an intrinsic coordinate system, enabling vector magnetometry within a single diamond crystal. While most vector magnetometers rely on a known bias magnetic field for full recovery of three-dimensional field information, employing external 3D Helmholtz coils or permanent magnets results in bulky, laboratory-bound setups, impeding miniaturization of the device. Here, a novel approach is presented that utilizes a fiber-integrated microscale coil at the fiber tip to generate a localized uniaxial magnetic field. The same fiber-tip coil is used in parallel for spin control by combining DC and microwave signals in a bias tee. To implement vector magnetometry using a uniaxial bias field, we preselect the orientation of the diamond crystal and then fully characterize it by rotating a static magnetic field in three planes of rotation. We demonstrate the measurement of vector magnetic fields in the full solid angle with a shot-noise limited sensitivity of $19.4\\:\\textrm{nT/Hz}^{1/2}$ and microscale spatial resolution while achieving a cross section of the fiber sensor head below $1\\:\\textrm{mm}^2.$","sentences":["In quantum sensing of magnetic fields, ensembles of NV centers in diamond offer high sensitivity, high bandwidth and outstanding spatial resolution while operating in harsh environments.","Moreover, the orientation of defect centers along four crystal axes forms an intrinsic coordinate system, enabling vector magnetometry within a single diamond crystal.","While most vector magnetometers rely on a known bias magnetic field for full recovery of three-dimensional field information, employing external 3D Helmholtz coils or permanent magnets results in bulky, laboratory-bound setups, impeding miniaturization of the device.","Here, a novel approach is presented that utilizes a fiber-integrated microscale coil at the fiber tip to generate a localized uniaxial magnetic field.","The same fiber-tip coil is used in parallel for spin control by combining DC and microwave signals in a bias tee.","To implement vector magnetometry using a uniaxial bias field, we preselect the orientation of the diamond crystal and then fully characterize it by rotating a static magnetic field in three planes of rotation.","We demonstrate the measurement of vector magnetic fields in the full solid angle with a shot-noise limited sensitivity of $19.4\\:\\textrm{nT/Hz}^{1/2}$ and microscale spatial resolution while achieving a cross section of the fiber sensor head below $1\\:\\textrm{mm}^2.$"],"url":"http://arxiv.org/abs/2404.14089v1","category":"physics.app-ph"}
{"created":"2024-04-22 11:03:05","title":"Pointed quandles of linkoinds","abstract":"In this paper we define the fundamental quandle of knotoids and linkoids and prove that it is invariant under the under forbidden-move and hence encodes only the information of the underclosure of the knotoid. We then introduce $n$-pointed quandles, which generalize quandles by specifying $n$ elements as ordered basepoints. This leads to the notion of fundamental pointed quandles of linkoids, which enhances the fundamental quandle. Using $2$-pointed quandle allows us to distinguish 1-linkoids with equivalent under-closures and leads to a couple of 1-linkoid invariants. We also generalize the notion of homogeneity of quandles to $n$-homogeneity of quandles. We classify all $\\infty$-homogeneous, finite quandles.","sentences":["In this paper we define the fundamental quandle of knotoids and linkoids and prove that it is invariant under the under forbidden-move and hence encodes only the information of the underclosure of the knotoid.","We then introduce $n$-pointed quandles, which generalize quandles by specifying $n$ elements as ordered basepoints.","This leads to the notion of fundamental pointed quandles of linkoids, which enhances the fundamental quandle.","Using $2$-pointed quandle allows us to distinguish 1-linkoids with equivalent under-closures and leads to a couple of 1-linkoid invariants.","We also generalize the notion of homogeneity of quandles to $n$-homogeneity of quandles.","We classify all $\\infty$-homogeneous, finite quandles."],"url":"http://arxiv.org/abs/2404.14083v1","category":"math.GT"}
{"created":"2024-04-22 11:01:51","title":"Mechanistic Interpretability for AI Safety -- A Review","abstract":"Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.","sentences":["Understanding AI systems' inner workings is critical for ensuring value alignment and safety.","This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding.","We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation.","We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety.","We investigate challenges surrounding scalability, automation, and comprehensive interpretation.","We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning.","Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable."],"url":"http://arxiv.org/abs/2404.14082v1","category":"cs.AI"}
{"created":"2024-04-22 10:55:43","title":"Nonadiabatic evolution and thermodynamics for a boundary-driven system with a weak intrasubsystem interaction","abstract":"We derive a time-dependent master equation for an externally driven system whose subsystems weakly interact with each other and locally connect to the thermal reservoirs. The nonadiabatic equation obtained here can be viewed as a generalization of the local master equation, which has already been extensively used in describing the dynamics of a boundary-driven system. In addition, we investigate the fundamental reason underlying the thermodynamic inconsistency generated by the local and nonadiabatic master equations. We fnd that these two equations are consistent with the second law of thermodynamics when the system is far away from the steady state, while they give rise to the contradiction at the steady state. Finally, we numerically confrm our results by considering a toy model consisting of two qubits and two local heat baths.","sentences":["We derive a time-dependent master equation for an externally driven system whose subsystems weakly interact with each other and locally connect to the thermal reservoirs.","The nonadiabatic equation obtained here can be viewed as a generalization of the local master equation, which has already been extensively used in describing the dynamics of a boundary-driven system.","In addition, we investigate the fundamental reason underlying the thermodynamic inconsistency generated by the local and nonadiabatic master equations.","We fnd that these two equations are consistent with the second law of thermodynamics when the system is far away from the steady state, while they give rise to the contradiction at the steady state.","Finally, we numerically confrm our results by considering a toy model consisting of two qubits and two local heat baths."],"url":"http://arxiv.org/abs/2404.14081v1","category":"quant-ph"}
{"created":"2024-04-22 10:52:38","title":"Rota-Baxter operators on dihedral and alternating groups","abstract":"Rota-Baxter operators on algebras, which appeared in 1960, have connections with different versions of the Yang-Baxter equation, pre- and postalgebras, double Poisson algebras, etc. In 2020, the notion of Rota-Baxter operator on a group was defined by L. Guo, H. Lang, Yu. Sheng.   In 2023, V. Bardakov and the second author showed that all Rota-Baxter operators on simple sporadic groups are splitting, i. e. they are defined via exact factorizations. In the current work, we clarify for which $n$, there exist non-splitting Rota-Baxter operators on the alternating group $\\mathrm{A}_n$. For the corresponding $n$, we describe all non-splitting Rota-Baxter operators on $\\mathrm{A}_n$. Moreover, we describe Rota-Baxter operators on dihedral groups $D_{2n}$ providing the general construction which lies behind all non-splitting Rota-Baxter operators on $\\mathrm{A}_n$ and $D_{2n}$.","sentences":["Rota-Baxter operators on algebras, which appeared in 1960, have connections with different versions of the Yang-Baxter equation, pre- and postalgebras, double Poisson algebras, etc.","In 2020, the notion of Rota-Baxter operator on a group was defined by L. Guo, H. Lang, Yu.","Sheng.   ","In 2023, V. Bardakov and the second author showed that all Rota-Baxter operators on simple sporadic groups are splitting, i. e. they are defined via exact factorizations.","In the current work, we clarify for which $n$, there exist non-splitting Rota-Baxter operators on the alternating group $\\mathrm{A}_n$. For the corresponding $n$, we describe all non-splitting Rota-Baxter operators on $\\mathrm{A}_n$. Moreover, we describe Rota-Baxter operators on dihedral groups $D_{2n}$ providing the general construction which lies behind all non-splitting Rota-Baxter operators on $\\mathrm{A}_n$ and $D_{2n}$."],"url":"http://arxiv.org/abs/2404.14078v1","category":"math.GR"}
{"created":"2024-04-22 10:45:59","title":"Noise contrastive estimation with soft targets for conditional models","abstract":"Soft targets combined with the cross-entropy loss have shown to improve generalization performance of deep neural networks on supervised classification tasks. The standard cross-entropy loss however assumes data to be categorically distributed, which may often not be the case in practice. In contrast, InfoNCE does not rely on such an explicit assumption but instead implicitly estimates the true conditional through negative sampling. Unfortunately, it cannot be combined with soft targets in its standard formulation, hindering its use in combination with sophisticated training strategies. In this paper, we address this limitation by proposing a principled loss function that is compatible with probabilistic targets. Our new soft target InfoNCE loss is conceptually simple, efficient to compute, and can be derived within the framework of noise contrastive estimation. Using a toy example, we demonstrate shortcomings of the categorical distribution assumption of cross-entropy, and discuss implications of sampling from soft distributions. We observe that soft target InfoNCE performs on par with strong soft target cross-entropy baselines and outperforms hard target NLL and InfoNCE losses on popular benchmarks, including ImageNet. Finally, we provide a simple implementation of our loss, geared towards supervised classification and fully compatible with deep classification model trained with cross-entropy.","sentences":["Soft targets combined with the cross-entropy loss have shown to improve generalization performance of deep neural networks on supervised classification tasks.","The standard cross-entropy loss however assumes data to be categorically distributed, which may often not be the case in practice.","In contrast, InfoNCE does not rely on such an explicit assumption but instead implicitly estimates the true conditional through negative sampling.","Unfortunately, it cannot be combined with soft targets in its standard formulation, hindering its use in combination with sophisticated training strategies.","In this paper, we address this limitation by proposing a principled loss function that is compatible with probabilistic targets.","Our new soft target InfoNCE loss is conceptually simple, efficient to compute, and can be derived within the framework of noise contrastive estimation.","Using a toy example, we demonstrate shortcomings of the categorical distribution assumption of cross-entropy, and discuss implications of sampling from soft distributions.","We observe that soft target InfoNCE performs on par with strong soft target cross-entropy baselines and outperforms hard target NLL and InfoNCE losses on popular benchmarks, including ImageNet.","Finally, we provide a simple implementation of our loss, geared towards supervised classification and fully compatible with deep classification model trained with cross-entropy."],"url":"http://arxiv.org/abs/2404.14076v1","category":"cs.LG"}
{"created":"2024-04-22 10:34:58","title":"Towards Robust Trajectory Representations: Isolating Environmental Confounders with Causal Learning","abstract":"Trajectory modeling refers to characterizing human movement behavior, serving as a pivotal step in understanding mobility patterns. Nevertheless, existing studies typically ignore the confounding effects of geospatial context, leading to the acquisition of spurious correlations and limited generalization capabilities. To bridge this gap, we initially formulate a Structural Causal Model (SCM) to decipher the trajectory representation learning process from a causal perspective. Building upon the SCM, we further present a Trajectory modeling framework (TrajCL) based on Causal Learning, which leverages the backdoor adjustment theory as an intervention tool to eliminate the spurious correlations between geospatial context and trajectories. Extensive experiments on two real-world datasets verify that TrajCL markedly enhances performance in trajectory classification tasks while showcasing superior generalization and interpretability.","sentences":["Trajectory modeling refers to characterizing human movement behavior, serving as a pivotal step in understanding mobility patterns.","Nevertheless, existing studies typically ignore the confounding effects of geospatial context, leading to the acquisition of spurious correlations and limited generalization capabilities.","To bridge this gap, we initially formulate a Structural Causal Model (SCM) to decipher the trajectory representation learning process from a causal perspective.","Building upon the SCM, we further present a Trajectory modeling framework (TrajCL) based on Causal Learning, which leverages the backdoor adjustment theory as an intervention tool to eliminate the spurious correlations between geospatial context and trajectories.","Extensive experiments on two real-world datasets verify that TrajCL markedly enhances performance in trajectory classification tasks while showcasing superior generalization and interpretability."],"url":"http://arxiv.org/abs/2404.14073v1","category":"cs.LG"}
{"created":"2024-04-22 10:29:04","title":"No General Code of Ethics for All: Ethical Considerations in Human-bot Psycho-counseling","abstract":"The pervasive use of AI applications is increasingly influencing our everyday decisions. However, the ethical challenges associated with AI transcend conventional ethics and single-discipline approaches. In this paper, we propose aspirational ethical principles specifically tailored for human-bot psycho-counseling during an era when AI-powered mental health services are continually emerging. We examined the responses generated by EVA2.0, GPT-3.5, and GPT-4.0 in the context of psycho-counseling and mental health inquiries. Our analysis focused on standard psycho-counseling ethical codes (respect for autonomy, non-maleficence, beneficence, justice, and responsibility) as well as crisis intervention strategies (risk assessment, involvement of emergency services, and referral to human professionals). The results indicate that although there has been progress in adhering to regular ethical codes as large language models (LLMs) evolve, the models' capabilities in handling crisis situations need further improvement. Additionally, we assessed the linguistic quality of the generated responses and found that misleading responses are still produced by the models. Furthermore, the ability of LLMs to encourage individuals to introspect in the psycho-counseling setting remains underdeveloped.","sentences":["The pervasive use of AI applications is increasingly influencing our everyday decisions.","However, the ethical challenges associated with AI transcend conventional ethics and single-discipline approaches.","In this paper, we propose aspirational ethical principles specifically tailored for human-bot psycho-counseling during an era when AI-powered mental health services are continually emerging.","We examined the responses generated by EVA2.0, GPT-3.5, and GPT-4.0 in the context of psycho-counseling and mental health inquiries.","Our analysis focused on standard psycho-counseling ethical codes (respect for autonomy, non-maleficence, beneficence, justice, and responsibility) as well as crisis intervention strategies (risk assessment, involvement of emergency services, and referral to human professionals).","The results indicate that although there has been progress in adhering to regular ethical codes as large language models (LLMs) evolve, the models' capabilities in handling crisis situations need further improvement.","Additionally, we assessed the linguistic quality of the generated responses and found that misleading responses are still produced by the models.","Furthermore, the ability of LLMs to encourage individuals to introspect in the psycho-counseling setting remains underdeveloped."],"url":"http://arxiv.org/abs/2404.14070v1","category":"cs.HC"}
{"created":"2024-04-22 10:26:49","title":"Holistic Safety and Responsibility Evaluations of Advanced AI Models","abstract":"Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice. In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation. In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience. Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals. Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines. Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms. For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos. The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem.","sentences":["Safety and responsibility evaluations of advanced AI models are a critical but developing field of research and practice.","In the development of Google DeepMind's advanced AI models, we innovated on and applied a broad set of approaches to safety evaluation.","In this report, we summarise and share elements of our evolving approach as well as lessons learned for a broad audience.","Key lessons learned include: First, theoretical underpinnings and frameworks are invaluable to organise the breadth of risk domains, modalities, forms, metrics, and goals.","Second, theory and practice of safety evaluation development each benefit from collaboration to clarify goals, methods and challenges, and facilitate the transfer of insights between different stakeholders and disciplines.","Third, similar key methods, lessons, and institutions apply across the range of concerns in responsibility and safety - including established and emerging harms.","For this reason it is important that a wide range of actors working on safety evaluation and safety research communities work together to develop, refine and implement novel evaluation approaches and best practices, rather than operating in silos.","The report concludes with outlining the clear need to rapidly advance the science of evaluations, to integrate new evaluations into the development and governance of AI, to establish scientifically-grounded norms and standards, and to promote a robust evaluation ecosystem."],"url":"http://arxiv.org/abs/2404.14068v1","category":"cs.AI"}
{"created":"2024-04-22 10:20:41","title":"LVNS-RAVE: Diversified audio generation with RAVE and Latent Vector Novelty Search","abstract":"Evolutionary Algorithms and Generative Deep Learning have been two of the most powerful tools for sound generation tasks. However, they have limitations: Evolutionary Algorithms require complicated designs, posing challenges in control and achieving realistic sound generation. Generative Deep Learning models often copy from the dataset and lack creativity. In this paper, we propose LVNS-RAVE, a method to combine Evolutionary Algorithms and Generative Deep Learning to produce realistic and novel sounds. We use the RAVE model as the sound generator and the VGGish model as a novelty evaluator in the Latent Vector Novelty Search (LVNS) algorithm. The reported experiments show that the method can successfully generate diversified, novel audio samples under different mutation setups using different pre-trained RAVE models. The characteristics of the generation process can be easily controlled with the mutation parameters. The proposed algorithm can be a creative tool for sound artists and musicians.","sentences":["Evolutionary Algorithms and Generative Deep Learning have been two of the most powerful tools for sound generation tasks.","However, they have limitations: Evolutionary Algorithms require complicated designs, posing challenges in control and achieving realistic sound generation.","Generative Deep Learning models often copy from the dataset and lack creativity.","In this paper, we propose LVNS-RAVE, a method to combine Evolutionary Algorithms and Generative Deep Learning to produce realistic and novel sounds.","We use the RAVE model as the sound generator and the VGGish model as a novelty evaluator in the Latent Vector Novelty Search (LVNS) algorithm.","The reported experiments show that the method can successfully generate diversified, novel audio samples under different mutation setups using different pre-trained RAVE models.","The characteristics of the generation process can be easily controlled with the mutation parameters.","The proposed algorithm can be a creative tool for sound artists and musicians."],"url":"http://arxiv.org/abs/2404.14063v1","category":"cs.SD"}
{"created":"2024-04-22 10:19:02","title":"FedTAD: Topology-aware Data-free Knowledge Distillation for Subgraph Federated Learning","abstract":"Subgraph federated learning (subgraph-FL) is a new distributed paradigm that facilitates the collaborative training of graph neural networks (GNNs) by multi-client subgraphs. Unfortunately, a significant challenge of subgraph-FL arises from subgraph heterogeneity, which stems from node and topology variation, causing the impaired performance of the global GNN. Despite various studies, they have not yet thoroughly investigated the impact mechanism of subgraph heterogeneity. To this end, we decouple node and topology variation, revealing that they correspond to differences in label distribution and structure homophily. Remarkably, these variations lead to significant differences in the class-wise knowledge reliability of multiple local GNNs, misguiding the model aggregation with varying degrees. Building on this insight, we propose topology-aware data-free knowledge distillation technology (FedTAD), enhancing reliable knowledge transfer from the local model to the global model. Extensive experiments on six public datasets consistently demonstrate the superiority of FedTAD over state-of-the-art baselines.","sentences":["Subgraph federated learning (subgraph-FL) is a new distributed paradigm that facilitates the collaborative training of graph neural networks (GNNs) by multi-client subgraphs.","Unfortunately, a significant challenge of subgraph-FL arises from subgraph heterogeneity, which stems from node and topology variation, causing the impaired performance of the global GNN.","Despite various studies, they have not yet thoroughly investigated the impact mechanism of subgraph heterogeneity.","To this end, we decouple node and topology variation, revealing that they correspond to differences in label distribution and structure homophily.","Remarkably, these variations lead to significant differences in the class-wise knowledge reliability of multiple local GNNs, misguiding the model aggregation with varying degrees.","Building on this insight, we propose topology-aware data-free knowledge distillation technology (FedTAD), enhancing reliable knowledge transfer from the local model to the global model.","Extensive experiments on six public datasets consistently demonstrate the superiority of FedTAD over state-of-the-art baselines."],"url":"http://arxiv.org/abs/2404.14061v1","category":"cs.LG"}
{"created":"2024-04-22 10:17:20","title":"Dual Representation of Unbounded Dynamic Concave Utilities","abstract":"In several linear spaces of possibly unbounded endowments, we represent the dynamic concave utilities (hence the dynamic convex risk measures) as the solutions of backward stochastic differential equations (BSDEs) with unbounded terminal values, with the help of our recent existence and uniqueness results on unbounded solutions of scalar BSDEs whose generators have a linear, super-linear, sub-quadratic or quadratic growth. The Legendre-Fenchel transform (dual representation) of convex functions, the de la vall\\'{e}e-Poussin theorem, and Young's and Gronwall's inequalities constitute the main ingredients of these representation results.","sentences":["In several linear spaces of possibly unbounded endowments, we represent the dynamic concave utilities (hence the dynamic convex risk measures) as the solutions of backward stochastic differential equations (BSDEs) with unbounded terminal values, with the help of our recent existence and uniqueness results on unbounded solutions of scalar BSDEs whose generators have a linear, super-linear, sub-quadratic or quadratic growth.","The Legendre-Fenchel transform (dual representation) of convex functions, the de la vall\\'{e}e-Poussin theorem, and Young's and Gronwall's inequalities constitute the main ingredients of these representation results."],"url":"http://arxiv.org/abs/2404.14059v1","category":"math.PR"}
{"created":"2024-04-22 10:16:02","title":"Bored to Death: Artificial Intelligence Research Reveals the Role of Boredom in Suicide Behavior","abstract":"Background: Recent advancements in Artificial Intelligence (AI) contributed significantly to suicide assessment, however, our theoretical understanding of this complex behavior is still limited. Objective: This study aimed to harness AI methodologies to uncover hidden risk factors that trigger or aggravate suicide behaviors. Method: The primary dataset included 228,052 Facebook postings by 1,006 users who completed the gold-standard Columbia Suicide Severity Rating Scale. This dataset was analyzed using a bottom-up research pipeline without a-priory hypotheses and its findings were validated using a top-down analysis of a new dataset. This secondary dataset included responses by 1,062 participants to the same suicide scale as well as to well-validated scales measuring depression and boredom. Results: An almost fully automated, AI-guided research pipeline resulted in four Facebook topics that predicted the risk of suicide, of which the strongest predictor was boredom. A comprehensive literature review using APA PsycInfo revealed that boredom is rarely perceived as a unique risk factor of suicide. A complementing top-down path analysis of the secondary dataset uncovered an indirect relationship between boredom and suicide, which was mediated by depression. An equivalent mediated relationship was observed in the primary Facebook dataset as well. However, here, a direct relationship between boredom and suicide risk was also observed. Conclusions: Integrating AI methods allowed the discovery of an under-researched risk factor of suicide. The study signals boredom as a maladaptive 'ingredient' that might trigger suicide behaviors, regardless of depression. Further studies are recommended to direct clinicians' attention to this burdening, and sometimes existential experience.","sentences":["Background: Recent advancements in Artificial Intelligence (AI) contributed significantly to suicide assessment, however, our theoretical understanding of this complex behavior is still limited.","Objective:","This study aimed to harness AI methodologies to uncover hidden risk factors that trigger or aggravate suicide behaviors.","Method: The primary dataset included 228,052 Facebook postings by 1,006 users who completed the gold-standard Columbia Suicide Severity Rating Scale.","This dataset was analyzed using a bottom-up research pipeline without a-priory hypotheses and its findings were validated using a top-down analysis of a new dataset.","This secondary dataset included responses by 1,062 participants to the same suicide scale as well as to well-validated scales measuring depression and boredom.","Results:","An almost fully automated, AI-guided research pipeline resulted in four Facebook topics that predicted the risk of suicide, of which the strongest predictor was boredom.","A comprehensive literature review using APA PsycInfo revealed that boredom is rarely perceived as a unique risk factor of suicide.","A complementing top-down path analysis of the secondary dataset uncovered an indirect relationship between boredom and suicide, which was mediated by depression.","An equivalent mediated relationship was observed in the primary Facebook dataset as well.","However, here, a direct relationship between boredom and suicide risk was also observed.","Conclusions: Integrating AI methods allowed the discovery of an under-researched risk factor of suicide.","The study signals boredom as a maladaptive 'ingredient' that might trigger suicide behaviors, regardless of depression.","Further studies are recommended to direct clinicians' attention to this burdening, and sometimes existential experience."],"url":"http://arxiv.org/abs/2404.14057v1","category":"cs.CL"}
{"created":"2024-04-22 10:06:17","title":"Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms","abstract":"Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts. This has important implications: unlike indirect discrimination, there is generally no 'objective justification' stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se. In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic. We draw on computer science literature to suggest that, in the algorithmic context, 'treatment on the grounds of' needs to be understood in terms of two steps: proxy capacity and proxy use. Only where both elements can be made out can direct discrimination be said to be `on grounds of' a protected characteristic. We analyse the legal conditions of our proposed proxy capacity and proxy use tests. Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making.","sentences":["Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts.","This has important implications: unlike indirect discrimination, there is generally no 'objective justification' stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se.","In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic.","We draw on computer science literature to suggest that, in the algorithmic context, 'treatment on the grounds of' needs to be understood in terms of two steps: proxy capacity and proxy use.","Only where both elements can be made out can direct discrimination be said to be `on grounds of' a protected characteristic.","We analyse the legal conditions of our proposed proxy capacity and proxy use tests.","Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making."],"url":"http://arxiv.org/abs/2404.14050v1","category":"cs.AI"}
{"created":"2024-04-22 09:59:12","title":"Defining the type IIB matrix model without breaking Lorentz symmetry","abstract":"The type IIB matrix model is a promising nonperturbative formulation of superstring theory, which may elucidate the emergence of (3+1)-dimensional space-time. However, the partition function is divergent due to the Lorentz symmetry, which is represented by a noncompact group. This divergence has been regularized conventionally by introducing some infrared cutoff, which breaks the Lorentz symmetry. Here we point out that Lorentz invariant observables become classical as one removes the infrared cutoff and that this \"classicalization\" is actually an artifact of the Lorentz symmetry breaking cutoff. In order to overcome this problem, we propose a natural way to \"gauge-fix\" the Lorentz symmetry in a fully nonperturbative manner. This also enables us to perform numerical simulations in such a way that the time-evolution can be extracted directly from the matrix configurations.","sentences":["The type IIB matrix model is a promising nonperturbative formulation of superstring theory, which may elucidate the emergence of (3+1)-dimensional space-time.","However, the partition function is divergent due to the Lorentz symmetry, which is represented by a noncompact group.","This divergence has been regularized conventionally by introducing some infrared cutoff, which breaks the Lorentz symmetry.","Here we point out that Lorentz invariant observables become classical as one removes the infrared cutoff and that this \"classicalization\" is actually an artifact of the Lorentz symmetry breaking cutoff.","In order to overcome this problem, we propose a natural way to \"gauge-fix\" the Lorentz symmetry in a fully nonperturbative manner.","This also enables us to perform numerical simulations in such a way that the time-evolution can be extracted directly from the matrix configurations."],"url":"http://arxiv.org/abs/2404.14045v1","category":"hep-th"}
{"created":"2024-04-22 09:56:59","title":"LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules.","sentences":["Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge.","However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step.","Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step.","In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing.","Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval.","Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG.","Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules."],"url":"http://arxiv.org/abs/2404.14043v1","category":"cs.CL"}
{"created":"2024-04-22 09:54:06","title":"Natural Capital as a Stock Option","abstract":"The unfolding climate crisis is a physical manifestation of the damage that market economy, driven by the high intensity consumption of fossil fuels, has inflicted on the Earth System and on the stability conditions that were established by a complex conjugation of natural factors during the Holoecene. The magnitude of the human activities and its predatory nature is such that it is no longer possible to consider the Earth System and the services it provides for the habitability of the planet, the so-called natural capital, as an economical externality. Thus one is left with two main choices in what concerns the sustaintability of the planet's habitability: radical economic degrowth or highly efficient solutions to internalise the maintenance and the restoration of ecosystems and the services of the Earth System. It is proposed that an interesting strategy for the latter is to consider the natural capital as a stock option.","sentences":["The unfolding climate crisis is a physical manifestation of the damage that market economy, driven by the high intensity consumption of fossil fuels, has inflicted on the Earth System and on the stability conditions that were established by a complex conjugation of natural factors during the Holoecene.","The magnitude of the human activities and its predatory nature is such that it is no longer possible to consider the Earth System and the services it provides for the habitability of the planet, the so-called natural capital, as an economical externality.","Thus one is left with two main choices in what concerns the sustaintability of the planet's habitability: radical economic degrowth or highly efficient solutions to internalise the maintenance and the restoration of ecosystems and the services of the Earth System.","It is proposed that an interesting strategy for the latter is to consider the natural capital as a stock option."],"url":"http://arxiv.org/abs/2404.14041v1","category":"econ.GN"}
{"created":"2024-04-22 09:53:55","title":"Surgical-DeSAM: Decoupling SAM for Instrument Segmentation in Robotic Surgery","abstract":"Purpose: The recent Segment Anything Model (SAM) has demonstrated impressive performance with point, text or bounding box prompts, in various applications. However, in safety-critical surgical tasks, prompting is not possible due to (i) the lack of per-frame prompts for supervised learning, (ii) it is unrealistic to prompt frame-by-frame in a real-time tracking application, and (iii) it is expensive to annotate prompts for offline applications.   Methods: We develop Surgical-DeSAM to generate automatic bounding box prompts for decoupling SAM to obtain instrument segmentation in real-time robotic surgery. We utilise a commonly used detection architecture, DETR, and fine-tuned it to obtain bounding box prompt for the instruments. We then empolyed decoupling SAM (DeSAM) by replacing the image encoder with DETR encoder and fine-tune prompt encoder and mask decoder to obtain instance segmentation for the surgical instruments. To improve detection performance, we adopted the Swin-transformer to better feature representation.   Results: The proposed method has been validated on two publicly available datasets from the MICCAI surgical instruments segmentation challenge EndoVis 2017 and 2018. The performance of our method is also compared with SOTA instrument segmentation methods and demonstrated significant improvements with dice metrics of 89.62 and 90.70 for the EndoVis 2017 and 2018.   Conclusion: Our extensive experiments and validations demonstrate that Surgical-DeSAM enables real-time instrument segmentation without any additional prompting and outperforms other SOTA segmentation methods.","sentences":["Purpose: The recent Segment Anything Model (SAM) has demonstrated impressive performance with point, text or bounding box prompts, in various applications.","However, in safety-critical surgical tasks, prompting is not possible due to (i) the lack of per-frame prompts for supervised learning, (ii) it is unrealistic to prompt frame-by-frame in a real-time tracking application, and (iii) it is expensive to annotate prompts for offline applications.   ","Methods: We develop Surgical-DeSAM to generate automatic bounding box prompts for decoupling SAM to obtain instrument segmentation in real-time robotic surgery.","We utilise a commonly used detection architecture, DETR, and fine-tuned it to obtain bounding box prompt for the instruments.","We then empolyed decoupling SAM (DeSAM) by replacing the image encoder with DETR encoder and fine-tune prompt encoder and mask decoder to obtain instance segmentation for the surgical instruments.","To improve detection performance, we adopted the Swin-transformer to better feature representation.   ","Results:","The proposed method has been validated on two publicly available datasets from the MICCAI surgical instruments segmentation challenge EndoVis 2017 and 2018.","The performance of our method is also compared with SOTA instrument segmentation methods and demonstrated significant improvements with dice metrics of 89.62 and 90.70 for the EndoVis 2017 and 2018.   ","Conclusion: Our extensive experiments and validations demonstrate that Surgical-DeSAM enables real-time instrument segmentation without any additional prompting and outperforms other SOTA segmentation methods."],"url":"http://arxiv.org/abs/2404.14040v1","category":"cs.CV"}
{"created":"2024-04-22 09:51:43","title":"GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian Splatting","abstract":"Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms.","sentences":["Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results.","However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts.","In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting.","With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models.","GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer.","Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation.","Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos.","Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality.","Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms."],"url":"http://arxiv.org/abs/2404.14037v1","category":"cs.CV"}
{"created":"2024-04-22 09:50:11","title":"Apodotiko: Enabling Efficient Serverless Federated Learning in Heterogeneous Environments","abstract":"Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process. While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients. To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL. Our strategy incorporates a scoring mechanism that evaluates each client's hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance. We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies. Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x. Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments.","sentences":["Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized.","Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders.","However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process.","While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients.","To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL.","Our strategy incorporates a scoring mechanism that evaluates each client's hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance.","We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies.","Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x.","Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments."],"url":"http://arxiv.org/abs/2404.14033v1","category":"cs.DC"}
{"created":"2024-04-22 09:41:19","title":"Weak Lipschitz structures and their connections with the topological structures","abstract":"Two approaches to Lipschitz structures for any set are presented, studied and compared. The first approach is similar to the one proposed in Fraser, Jr. R. B., Axiom systems for Lipschitz structures, Fundamenta Mathematicae, (1970), where Lipschitz structures are defined as families of pseudo-metrics satisfying suitable conditions. The other one, here introduced, is expressed by using weak pseudo-metrics, which (unlike the pseudo-metrics) do not necessarily vanish on the whole of the diagonal of the cartesian product of the considered set. In this case we will talk about weak Lipschitz structures. Since all topological structures are defined by a a family of weak pseudo-metrics (as we will show in Section 4) we can find some connections between topological structures and weak Lipschitz structures, and a link between continuous maps and weak Lipschitz maps.   A central part of this paper is devoted to the weak Lipschitz uniformity defined by a weak Lipschitz structure, which is introduced in Section 8. A notion of uniform continuity with respect to weak Lipschitz uniformities is proposed and studied. In particular, we prove that the weak Lipschitz maps acting between two weak Lipschitz spaces are uniformly continuous with respect to the weak Lipschitz uniformities defined by the respective weak Lipschitz structures.","sentences":["Two approaches to Lipschitz structures for any set are presented, studied and compared.","The first approach is similar to the one proposed in Fraser, Jr. R. B., Axiom systems for Lipschitz structures, Fundamenta Mathematicae, (1970), where Lipschitz structures are defined as families of pseudo-metrics satisfying suitable conditions.","The other one, here introduced, is expressed by using weak pseudo-metrics, which (unlike the pseudo-metrics) do not necessarily vanish on the whole of the diagonal of the cartesian product of the considered set.","In this case we will talk about weak Lipschitz structures.","Since all topological structures are defined by a a family of weak pseudo-metrics (as we will show in Section 4) we can find some connections between topological structures and weak Lipschitz structures, and a link between continuous maps and weak Lipschitz maps.   ","A central part of this paper is devoted to the weak Lipschitz uniformity defined by a weak Lipschitz structure, which is introduced in Section 8.","A notion of uniform continuity with respect to weak Lipschitz uniformities is proposed and studied.","In particular, we prove that the weak Lipschitz maps acting between two weak Lipschitz spaces are uniformly continuous with respect to the weak Lipschitz uniformities defined by the respective weak Lipschitz structures."],"url":"http://arxiv.org/abs/2404.14026v1","category":"math.GN"}
{"created":"2024-04-22 09:35:25","title":"Perfect Matching in Product Graphs and in their Random Subgraphs","abstract":"For $t \\in \\mathbb{N}$ and every $i\\in[t]$, let $H_i$ be a $d_i$-regular connected graph, with $1<|V(H_i)|\\le C$ for some integer $C\\ge 2$. Let $G=\\square_{i=1}^tH_i$ be the Cartesian product of $H_1, \\ldots, H_t$. We show that if $t\\ge 5C\\log_2C$ then $G$ contains a (nearly-)perfect matching.   Then, considering the random graph process on $G$, we generalise the result of Bollob\\'as on the binary hypercube $Q^t$, showing that with high probability, the hitting times for minimum degree one, connectivity, and the existence of a (nearly-)perfect matching in the $G$-random-process are the same. We develop several tools which may be of independent interest in a more general setting of the typical existence of a perfect matching under percolation.","sentences":["For $t \\in \\mathbb{N}$ and every $i\\in[t]$, let $H_i$ be a $d_i$-regular connected graph, with $1<|V(H_i)|\\le C$ for some integer $C\\ge 2$. Let $G=\\square_{i=1}^tH_i$ be the Cartesian product of $H_1, \\ldots, H_t$. We show that if $t\\ge 5C\\log_2C$ then $G$ contains a (nearly-)perfect matching.   ","Then, considering the random graph process on $G$, we generalise the result of Bollob\\'as on the binary hypercube $Q^t$, showing that with high probability, the hitting times for minimum degree one, connectivity, and the existence of a (nearly-)perfect matching in the $G$-random-process are the same.","We develop several tools which may be of independent interest in a more general setting of the typical existence of a perfect matching under percolation."],"url":"http://arxiv.org/abs/2404.14020v1","category":"math.CO"}
{"created":"2024-04-22 09:33:36","title":"On Pro-zero homomorphisms and sequences in local (co-)homology","abstract":"Let $\\xx= x_1,\\ldots,x_r$ denote a system of elements of a commutative ring $R$. For an $R$-module $M$ we investigate when $\\xx$ is $M$-pro-regular resp. $M$-weakly pro-regular as generalizations of $M$-regular sequences. This is done in terms of \\v{C}ech co-homology resp. homology, defined by $H^i(\\check{C}_{\\xx} \\otimes_R \\cdot)$ resp. by $H_i({\\textrm{R}} \\Hom_R(\\check{C}_{\\xx},\\cdot)) \\cong H_i(\\Hom_R(\\mathcal{L}_{\\xx},\\cdot))$, where $\\check{C}_{\\xx}$ denotes the \\v{C}ech complex and $\\mathcal{L}_{\\xx}$ is a bounded free resolution of it as constructed in [17] resp. [16]. The property of $\\xx$ being $M$-pro-regular resp. $M$-weakly pro-regular follows by the vanishing of certain \\v{C}ech co-homology resp. homology modules, which is related to completions. This extends previously work by Greenlees and May (see) [5] and Lipman et al. (see [1]}). This contributes to a further understanding of \\v{C}ech (co-)homology in the non-Noetherian case. As a technical tool we use one of Emmanouil's results (see [4]) about the inverse limits and its derived functor. As an application we prove a global variant of the results with an application to prisms in the sense of Bhatt and Scholze (see[3]).","sentences":["Let $\\xx= x_1,\\ldots,x_r$ denote a system of elements of a commutative ring $R$. For an $R$-module $M$ we investigate when $\\xx$ is $M$-pro-regular resp.","$M$-weakly pro-regular as generalizations of $M$-regular sequences.","This is done in terms of \\v{C}ech co-homology resp.","homology, defined by $H^i(\\check{C}_{\\xx} \\otimes_R \\cdot)$ resp.","by $H_i({\\textrm{R}} \\Hom_R(\\check{C}_{\\xx},\\cdot))","\\cong H_i(\\Hom_R(\\mathcal{L}_{\\xx},\\cdot))$, where $\\check{C}_{\\xx}$ denotes the \\v{C}ech complex and $\\mathcal{L}_{\\xx}$ is a bounded free resolution of it as constructed in [17] resp.","[16].","The property of $\\xx$ being $M$-pro-regular resp.","$M$-weakly pro-regular follows by the vanishing of certain \\v{C}ech co-homology resp.","homology modules, which is related to completions.","This extends previously work by Greenlees and May (see)","[5] and Lipman et al.","(see [1]}).","This contributes to a further understanding of \\v{C}ech (co-)homology in the non-Noetherian case.","As a technical tool we use one of Emmanouil's results (see [4]) about the inverse limits and its derived functor.","As an application we prove a global variant of the results with an application to prisms in the sense of Bhatt and Scholze (see[3])."],"url":"http://arxiv.org/abs/2404.14018v1","category":"math.AC"}
{"created":"2024-04-22 09:32:38","title":"Hybrid Ensemble-Based Travel Mode Prediction","abstract":"Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips. This is also a major step towards fostering sustainable transportation. As behaviour may evolve over time, we also face the question of detecting concept drift in the data. This necessitates using appropriate methods to address potential concept drift. In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models. To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data. It relies on the combination of drift detectors with batch learning and stream mining models. We compare it against batch and incremental learners, including methods relying on active drift detection. Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data. The method has a higher rank than batch and stream learners.","sentences":["Travel mode choice (TMC) prediction, which can be formulated as a classification task, helps in understanding what makes citizens choose different modes of transport for individual trips.","This is also a major step towards fostering sustainable transportation.","As behaviour may evolve over time, we also face the question of detecting concept drift in the data.","This necessitates using appropriate methods to address potential concept drift.","In particular, it is necessary to decide whether batch or stream mining methods should be used to develop periodically updated TMC models.","To address the challenge of the development of TMC models, we propose the novel Incremental Ensemble of Batch and Stream Models (IEBSM) method aimed at adapting travel mode choice classifiers to concept drift possibly occurring in the data.","It relies on the combination of drift detectors with batch learning and stream mining models.","We compare it against batch and incremental learners, including methods relying on active drift detection.","Experiments with varied travel mode data sets representing both city and country levels show that the IEBSM method both detects drift in travel mode data and successfully adapts the models to evolving travel mode choice data.","The method has a higher rank than batch and stream learners."],"url":"http://arxiv.org/abs/2404.14017v1","category":"cs.LG"}
{"created":"2024-04-22 09:23:55","title":"Coordinated Planning for Stability Enhancement in High IBR-Penetrated Systems","abstract":"Security and stability challenges in future power systems with high penetration Inverter-Based Resources (IBR) have been anticipated as the main barrier to decolonization. Grid-following IBRs may become unstable under small disturbances in weak grids, while, during transient processes, system stability and protection may be jeopardized due to the lack of sufficient Short-Circuit Current (SCC). To solve these challenges and achieve decarbonization, the future system has to be carefully planned. However, it remains unclear how both small-signal and transient processes can be considered during the system planning stage. In this context, this paper proposes a coordinated planning model of different resources to enhance system-level stability. The system strength and SCC constraints are analytically derived by considering the different characteristics of synchronous units and IBRs, which are further effectively linearized through a novel data-driven approach, where an active sampling method is proposed to generate a representative data set. The significant economic value of the proposed coordinated planning framework in both system asset investment and system operation is demonstrated through detailed case studies.","sentences":["Security and stability challenges in future power systems with high penetration Inverter-Based Resources (IBR) have been anticipated as the main barrier to decolonization.","Grid-following IBRs may become unstable under small disturbances in weak grids, while, during transient processes, system stability and protection may be jeopardized due to the lack of sufficient Short-Circuit Current (SCC).","To solve these challenges and achieve decarbonization, the future system has to be carefully planned.","However, it remains unclear how both small-signal and transient processes can be considered during the system planning stage.","In this context, this paper proposes a coordinated planning model of different resources to enhance system-level stability.","The system strength and SCC constraints are analytically derived by considering the different characteristics of synchronous units and IBRs, which are further effectively linearized through a novel data-driven approach, where an active sampling method is proposed to generate a representative data set.","The significant economic value of the proposed coordinated planning framework in both system asset investment and system operation is demonstrated through detailed case studies."],"url":"http://arxiv.org/abs/2404.14012v1","category":"cs.SY"}
{"created":"2024-04-22 09:16:25","title":"Infusion: Preventing Customized Text-to-Image Diffusion from Overfitting","abstract":"Text-to-image (T2I) customization aims to create images that embody specific visual concepts delineated in textual descriptions. However, existing works still face a main challenge, concept overfitting. To tackle this challenge, we first analyze overfitting, categorizing it into concept-agnostic overfitting, which undermines non-customized concept knowledge, and concept-specific overfitting, which is confined to customize on limited modalities, i.e, backgrounds, layouts, styles. To evaluate the overfitting degree, we further introduce two metrics, i.e, Latent Fisher divergence and Wasserstein metric to measure the distribution changes of non-customized and customized concept respectively. Drawing from the analysis, we propose Infusion, a T2I customization method that enables the learning of target concepts to avoid being constrained by limited training modalities, while preserving non-customized knowledge. Remarkably, Infusion achieves this feat with remarkable efficiency, requiring a mere 11KB of trained parameters. Extensive experiments also demonstrate that our approach outperforms state-of-the-art methods in both single and multi-concept customized generation.","sentences":["Text-to-image (T2I) customization aims to create images that embody specific visual concepts delineated in textual descriptions.","However, existing works still face a main challenge, concept overfitting.","To tackle this challenge, we first analyze overfitting, categorizing it into concept-agnostic overfitting, which undermines non-customized concept knowledge, and concept-specific overfitting, which is confined to customize on limited modalities, i.e, backgrounds, layouts, styles.","To evaluate the overfitting degree, we further introduce two metrics, i.e, Latent Fisher divergence and Wasserstein metric to measure the distribution changes of non-customized and customized concept respectively.","Drawing from the analysis, we propose Infusion, a T2I customization method that enables the learning of target concepts to avoid being constrained by limited training modalities, while preserving non-customized knowledge.","Remarkably, Infusion achieves this feat with remarkable efficiency, requiring a mere 11KB of trained parameters.","Extensive experiments also demonstrate that our approach outperforms state-of-the-art methods in both single and multi-concept customized generation."],"url":"http://arxiv.org/abs/2404.14007v1","category":"cs.CV"}
{"created":"2024-04-22 09:10:17","title":"A regressor-based hysteresis formulation for the magnetic characterisation of low carbon steels","abstract":"In this work, two different parametric hysteresis models, the Jiles-Atherton model and the Mel'gui relation, have been combined to form a more general hysteresis operator, suitable for the description of families of experimental B(H) curves obtained for low carbon (LC) steel specimens after isothermal annealing at different temperatures and times. As it has been demonstrated in a number of previous studies, characteristic values of steel hysteresis curves can be used as very efficient identifiers for the monitoring of the different metallurgical transformations that take place during the annealing, such as recovery and recrystallisation processes. It is thus important from a practical point to be able to reproduce the experimental curves obtained under different conditions, as precisely as possible, in order to proceed to the samples characterisation. Hybridisation of the two aforementioned models demonstrated satisfactory results for the reproduction of all considered curves obtained under the different considered annealing conditions.","sentences":["In this work, two different parametric hysteresis models, the Jiles-Atherton model and the Mel'gui relation, have been combined to form a more general hysteresis operator, suitable for the description of families of experimental B(H) curves obtained for low carbon (LC) steel specimens after isothermal annealing at different temperatures and times.","As it has been demonstrated in a number of previous studies, characteristic values of steel hysteresis curves can be used as very efficient identifiers for the monitoring of the different metallurgical transformations that take place during the annealing, such as recovery and recrystallisation processes.","It is thus important from a practical point to be able to reproduce the experimental curves obtained under different conditions, as precisely as possible, in order to proceed to the samples characterisation.","Hybridisation of the two aforementioned models demonstrated satisfactory results for the reproduction of all considered curves obtained under the different considered annealing conditions."],"url":"http://arxiv.org/abs/2404.14004v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 09:09:09","title":"Temporal genomics help in deciphering neutral and adaptive patterns in the contemporary evolution of kelp populations","abstract":"The impact of climate change on populations will be contingent upon their contemporary adaptive evolution. In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing. These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations. Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients. This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered. Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time. This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time. In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity. The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments. Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change.","sentences":["The impact of climate change on populations will be contingent upon their contemporary adaptive evolution.","In this study, we investigated the contemporary evolution of four populations of the cold-water kelp Laminaria digitata by analysing their spatial and temporal genomic variation using ddRAD-sequencing.","These populations were sampled from the center to the southern margin of its north-eastern Atlantic distribution at two-time points, spanning at least two generations.","Through genome scans for local adaptation at a single time point, we identified candidate loci that showed clinal variation correlated with changes in sea surface temperature (SST) along latitudinal gradients.","This finding suggests that SST may drive the adaptive response of these kelp populations, although factors such as species' demographic history should also be considered.","Additionally, we performed a simulation approach to distinguish the effect of selection from genetic drift in allele frequency changes over time.","This enabled the detection of loci in the southernmost population that exhibited temporal differentiation beyond what would be expected from genetic drift alone: these are candidate loci which could have evolved under selection over time.","In contrast, we did not detect any outlier locus based on temporal differentiation in the population from the North Sea, which also displayed low and decreasing levels of genetic diversity.","The diverse evolutionary scenarios observed among populations can be attributed to variations in the prevalence of selection relative to genetic drift across different environments.","Therefore, our study highlights the potential of temporal genomics to offer valuable insights into the contemporary evolution of marine foundation species facing climate change."],"url":"http://arxiv.org/abs/2404.14003v1","category":"q-bio.PE"}
{"created":"2024-04-22 08:59:35","title":"Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion","abstract":"Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.","sentences":["Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation.","However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible.","This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images.","In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration.","Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem.","To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks.","Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks.","Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series."],"url":"http://arxiv.org/abs/2404.13993v1","category":"cs.MM"}
{"created":"2024-04-22 08:58:57","title":"Dynamic Proxy Domain Generalizes the Crowd Localization by Better Binary Segmentation","abstract":"Crowd localization targets on predicting each instance precise location within an image. Current advanced methods propose the pixel-wise binary classification to tackle the congested prediction, in which the pixel-level thresholds binarize the prediction confidence of being the pedestrian head. Since the crowd scenes suffer from extremely varying contents, counts and scales, the confidence-threshold learner is fragile and under-generalized encountering domain knowledge shift. Moreover, at the most time, the target domain is agnostic in training. Hence, it is imperative to exploit how to enhance the generalization of confidence-threshold locator to the latent target domain. In this paper, we propose a Dynamic Proxy Domain (DPD) method to generalize the learner under domain shift. Concretely, based on the theoretical analysis to the generalization error risk upper bound on the latent target domain to a binary classifier, we propose to introduce a generated proxy domain to facilitate generalization. Then, based on the theory, we design a DPD algorithm which is composed by a training paradigm and proxy domain generator to enhance the domain generalization of the confidence-threshold learner. Besides, we conduct our method on five kinds of domain shift scenarios, demonstrating the effectiveness on generalizing the crowd localization. Our code will be available at https://github.com/zhangda1018/DPD.","sentences":["Crowd localization targets on predicting each instance precise location within an image.","Current advanced methods propose the pixel-wise binary classification to tackle the congested prediction, in which the pixel-level thresholds binarize the prediction confidence of being the pedestrian head.","Since the crowd scenes suffer from extremely varying contents, counts and scales, the confidence-threshold learner is fragile and under-generalized encountering domain knowledge shift.","Moreover, at the most time, the target domain is agnostic in training.","Hence, it is imperative to exploit how to enhance the generalization of confidence-threshold locator to the latent target domain.","In this paper, we propose a Dynamic Proxy Domain (DPD) method to generalize the learner under domain shift.","Concretely, based on the theoretical analysis to the generalization error risk upper bound on the latent target domain to a binary classifier, we propose to introduce a generated proxy domain to facilitate generalization.","Then, based on the theory, we design a DPD algorithm which is composed by a training paradigm and proxy domain generator to enhance the domain generalization of the confidence-threshold learner.","Besides, we conduct our method on five kinds of domain shift scenarios, demonstrating the effectiveness on generalizing the crowd localization.","Our code will be available at https://github.com/zhangda1018/DPD."],"url":"http://arxiv.org/abs/2404.13992v1","category":"cs.CV"}
{"created":"2024-04-22 08:58:30","title":"5GC$^2$ache: Improving 5G UPF Performance via Cache Optimization","abstract":"Last Level Cache (LLC) is a precious and critical resource that impacts the performance of applications running on top of CPUs. In this paper, we reveal the significant impact of LLC on the performance of the 5G user plane function (UPF) when running a cloudified 5G core on general-purposed servers. With extensive measurements showing that the throughput can degrade by over 50\\% when the precious LLC resource of UPF is not properly allocated, we identify three categories of performance degradation caused by incorrect LLC usage: DMA leakage problem, hot/cold mbuf problem and cache contention. To address these problems, we introduce the design and implementation of 5GC$^2$ache that monitors the LLC status as well as the throughput performance and dynamically adjusts key parameters of the LLC resource allocation. Our experiments show that 5GC$^2$ache enables a commercial 5G core to increase its throughput to 76.41Gbps, 39.41\\% higher than the original performance and 29.55\\% higher than the state-of-the-art.","sentences":["Last Level Cache (LLC) is a precious and critical resource that impacts the performance of applications running on top of CPUs.","In this paper, we reveal the significant impact of LLC on the performance of the 5G user plane function (UPF) when running a cloudified 5G core on general-purposed servers.","With extensive measurements showing that the throughput can degrade by over 50\\% when the precious LLC resource of UPF is not properly allocated, we identify three categories of performance degradation caused by incorrect LLC usage: DMA leakage problem, hot/cold mbuf problem and cache contention.","To address these problems, we introduce the design and implementation of 5GC$^2$ache that monitors the LLC status as well as the throughput performance and dynamically adjusts key parameters of the LLC resource allocation.","Our experiments show that 5GC$^2$ache enables a commercial 5G core to increase its throughput to 76.41Gbps, 39.41\\% higher than the original performance and 29.55\\% higher than the state-of-the-art."],"url":"http://arxiv.org/abs/2404.13991v1","category":"cs.NI"}
{"created":"2024-04-22 08:56:38","title":"Two Simple Reduction Formulas for the Denumerant Functions","abstract":"Let $A$ be a nonempty set of positive integers. The restricted partition function $p_A(n)$ denotes the number of partitions of $n$ with parts in $A$. When the elements in $A$ are pairwise relatively prime positive integers, Ehrhart and Sert\\\"oz-\\\"Ozl\\\"uk derived two reduction formulas for $p_A(n)$ for $A$ with three parameters. We extend their findings for general $A$ using the Bernoulli-Barnes polynomials.","sentences":["Let $A$ be a nonempty set of positive integers.","The restricted partition function $p_A(n)$ denotes the number of partitions of $n$ with parts in $A$.","When the elements in $A$ are pairwise relatively prime positive integers, Ehrhart and Sert\\\"oz-\\\"Ozl\\\"uk derived two reduction formulas for $p_A(n)$ for $A$ with three parameters.","We extend their findings for general $A$ using the Bernoulli-Barnes polynomials."],"url":"http://arxiv.org/abs/2404.13989v1","category":"math.CO"}
{"created":"2024-04-22 08:52:28","title":"Supergravity in the Geometric Approach and its Hidden Graded Lie Algebra","abstract":"In this contribution, we present the geometric approach to supergravity. In the first part, we discuss in some detail the peculiarities of the approach and apply the formalism to the case of pure supergravity in four space-time dimensions. In the second part, we extend the discussion to theories in higher dimensions, which include antisymmetric tensors of degree higher than one, focussing on the case of eleven dimensional space-time. Here, we report the formulation first introduced in 1981 by R. D'Auria and P. Fr\\`e, corresponding to a generalization of a Chevalley-Eilenberg Lie algebra, together with some more recent results, pointing out the relation of the formalism with the mathematical framework of $L_\\infty$ algebras.","sentences":["In this contribution, we present the geometric approach to supergravity.","In the first part, we discuss in some detail the peculiarities of the approach and apply the formalism to the case of pure supergravity in four space-time dimensions.","In the second part, we extend the discussion to theories in higher dimensions, which include antisymmetric tensors of degree higher than one, focussing on the case of eleven dimensional space-time.","Here, we report the formulation first introduced in 1981 by R. D'Auria and P. Fr\\`e, corresponding to a generalization of a Chevalley-Eilenberg Lie algebra, together with some more recent results, pointing out the relation of the formalism with the mathematical framework of $L_\\infty$ algebras."],"url":"http://arxiv.org/abs/2404.13987v1","category":"hep-th"}
{"created":"2024-04-22 08:49:14","title":"Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler","abstract":"In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models. Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al. (1998) and Omori et al. (2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions. Under this mixture representation, we sample the parameters and latent volatilities in one block. We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps. The proposed method is extended to the SVM model with leverage. The methodology and models are applied to excess holding yields in empirical studies, and the SVM model with leverage is shown to outperform competing volatility models based on marginal likelihoods.","sentences":["In this paper we consider the simulation-based Bayesian analysis of stochastic volatility in mean (SVM) models.","Extending the highly efficient Markov chain Monte Carlo mixture sampler for the SV model proposed in Kim et al.","(1998) and Omori et al.","(2007), we develop an accurate approximation of the non-central chi-squared distribution as a mixture of thirty normal distributions.","Under this mixture representation, we sample the parameters and latent volatilities in one block.","We also detail a correction of the small approximation error by using additional Metropolis-Hastings steps.","The proposed method is extended to the SVM model with leverage.","The methodology and models are applied to excess holding yields in empirical studies, and the SVM model with leverage is shown to outperform competing volatility models based on marginal likelihoods."],"url":"http://arxiv.org/abs/2404.13986v1","category":"econ.EM"}
{"created":"2024-04-22 08:44:34","title":"RHanDS: Refining Malformed Hands for Generated Images with Decoupled Structure and Style Guidance","abstract":"Although diffusion models can generate high-quality human images, their applications are limited by the instability in generating hands with correct structures. Some previous works mitigate the problem by considering hand structure yet struggle to maintain style consistency between refined malformed hands and other image regions. In this paper, we aim to solve the problem of inconsistency regarding hand structure and style. We propose a conditional diffusion-based framework RHanDS to refine the hand region with the help of decoupled structure and style guidance. Specifically, the structure guidance is the hand mesh reconstructed from the malformed hand, serving to correct the hand structure. The style guidance is a hand image, e.g., the malformed hand itself, and is employed to furnish the style reference for hand refining. In order to suppress the structure leakage when referencing hand style and effectively utilize hand data to improve the capability of the model, we build a multi-style hand dataset and introduce a twostage training strategy. In the first stage, we use paired hand images for training to generate hands with the same style as the reference. In the second stage, various hand images generated based on the human mesh are used for training to enable the model to gain control over the hand structure. We evaluate our method and counterparts on the test dataset of the proposed multi-style hand dataset. The experimental results show that RHanDS can effectively refine hands structure- and style- correctly compared with previous methods. The codes and datasets will be available soon.","sentences":["Although diffusion models can generate high-quality human images, their applications are limited by the instability in generating hands with correct structures.","Some previous works mitigate the problem by considering hand structure yet struggle to maintain style consistency between refined malformed hands and other image regions.","In this paper, we aim to solve the problem of inconsistency regarding hand structure and style.","We propose a conditional diffusion-based framework RHanDS to refine the hand region with the help of decoupled structure and style guidance.","Specifically, the structure guidance is the hand mesh reconstructed from the malformed hand, serving to correct the hand structure.","The style guidance is a hand image, e.g., the malformed hand itself, and is employed to furnish the style reference for hand refining.","In order to suppress the structure leakage when referencing hand style and effectively utilize hand data to improve the capability of the model, we build a multi-style hand dataset and introduce a twostage training strategy.","In the first stage, we use paired hand images for training to generate hands with the same style as the reference.","In the second stage, various hand images generated based on the human mesh are used for training to enable the model to gain control over the hand structure.","We evaluate our method and counterparts on the test dataset of the proposed multi-style hand dataset.","The experimental results show that RHanDS can effectively refine hands structure- and style- correctly compared with previous methods.","The codes and datasets will be available soon."],"url":"http://arxiv.org/abs/2404.13984v1","category":"cs.CV"}
{"created":"2024-04-22 08:44:10","title":"Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network","abstract":"Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape. As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape. However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body. In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow. Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph. AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map. The design could better improve the consistency between body parts. (2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain. Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD. With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting. Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics.","sentences":["Given a source portrait, the automatic human body reshaping task aims at editing it to an aesthetic body shape.","As the technology has been widely used in media, several methods have been proposed mainly focusing on generating optical flow to warp the body shape.","However, those previous works only consider the local transformation of different body parts (arms, torso, and legs), ignoring the global affinity, and limiting the capacity to ensure consistency and quality across the entire body.","In this paper, we propose a novel Adaptive Affinity-Graph Network (AAGN), which extracts the global affinity between different body parts to enhance the quality of the generated optical flow.","Specifically, our AAGN primarily introduces the following designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages the characteristic of a fully connected graph.","AAG represents different body parts as nodes in an adaptive fully connected graph and captures all the affinities between nodes to obtain a global affinity map.","The design could better improve the consistency between body parts.","(2) Besides, for high-frequency details are crucial for photo aesthetics, a Body Shape Discriminator (BSD) is designed to extract information from both high-frequency and spatial domain.","Particularly, an SRM filter is utilized to extract high-frequency details, which are combined with spatial features as input to the BSD.","With this design, BSD guides the Flow Generator (FG) to pay attention to various fine details rather than rigid pixel-level fitting.","Extensive experiments conducted on the BR-5K dataset demonstrate that our framework significantly enhances the aesthetic appeal of reshaped photos, marginally surpassing all previous work to achieve state-of-the-art in all evaluation metrics."],"url":"http://arxiv.org/abs/2404.13983v1","category":"cs.CV"}
{"created":"2024-04-22 08:43:39","title":"Special Issue on Modified Gravity Approaches to the Tensions of $\u039b$CDM: Goals and Highlights","abstract":"The Special Issue on \"Modified Gravity Approaches to the Tensions of $\\Lambda$CDM\"} in the Universe journal tackles significant challenges faced by the $\\Lambda$CDM model, including discrepancies in the Hubble constant, growth rate of structures, and cosmological anisotropies. These issues suggest foundational cracks in the model, raising questions about the validity of General Relativity, dark energy, and cosmological principles at large scales. This collection brings together leading researchers to delve into Modified Gravity theories as potential solutions. Covering approaches from Scalar-Tensor theories to $f(R,T)$ gravity and beyond, each contribution presents innovative research aimed at addressing the limitations of the $\\Lambda$CDM model. This Special Issue not only highlights the theoretical and empirical strengths of Modified Gravity models but also opens avenues for future investigations, emphasizing the synergy between theoretical advancements and observational evidence to deepen our cosmological understanding.","sentences":["The Special Issue on \"Modified Gravity Approaches to the Tensions of $\\Lambda$CDM\"} in the Universe journal tackles significant challenges faced by the $\\Lambda$CDM model, including discrepancies in the Hubble constant, growth rate of structures, and cosmological anisotropies.","These issues suggest foundational cracks in the model, raising questions about the validity of General Relativity, dark energy, and cosmological principles at large scales.","This collection brings together leading researchers to delve into Modified Gravity theories as potential solutions.","Covering approaches from Scalar-Tensor theories to $f(R,T)$ gravity and beyond, each contribution presents innovative research aimed at addressing the limitations of the $\\Lambda$CDM model.","This Special Issue not only highlights the theoretical and empirical strengths of Modified Gravity models but also opens avenues for future investigations, emphasizing the synergy between theoretical advancements and observational evidence to deepen our cosmological understanding."],"url":"http://arxiv.org/abs/2404.13981v1","category":"gr-qc"}
{"created":"2024-04-22 08:41:43","title":"Modelling Technique for GDPR-compliance: Toward a Comprehensive Solution","abstract":"Data-driven applications and services have been increasingly deployed in all aspects of life including healthcare and medical services in which a huge amount of personal data is collected, aggregated, and processed in a centralised server from various sources. As a consequence, preserving the data privacy and security of these applications is of paramount importance. Since May 2018, the new data protection legislation in the EU/UK, namely the General Data Protection Regulation (GDPR), has come into force and this has called for a critical need for modelling compliance with the GDPR's sophisticated requirements. Existing threat modelling techniques are not designed to model GDPR compliance, particularly in a complex system where personal data is collected, processed, manipulated, and shared with third parties. In this paper, we present a novel comprehensive solution for developing a threat modelling technique to address threats of non-compliance and mitigate them by taking GDPR requirements as the baseline and combining them with the existing security and privacy modelling techniques (i.e., \\textit{STRIDE} and \\textit{LINDDUN}, respectively). For this purpose, we propose a new data flow diagram integrated with the GDPR principles, develop a knowledge base for the non-compliance threats, and leverage an inference engine for reasoning the GDPR non-compliance threats over the knowledge base. Finally, we demonstrate our solution for threats of non-compliance with legal basis and accountability in a telehealth system to show the feasibility and effectiveness of the proposed solution.","sentences":["Data-driven applications and services have been increasingly deployed in all aspects of life including healthcare and medical services in which a huge amount of personal data is collected, aggregated, and processed in a centralised server from various sources.","As a consequence, preserving the data privacy and security of these applications is of paramount importance.","Since May 2018, the new data protection legislation in the EU/UK, namely the General Data Protection Regulation (GDPR), has come into force and this has called for a critical need for modelling compliance with the GDPR's sophisticated requirements.","Existing threat modelling techniques are not designed to model GDPR compliance, particularly in a complex system where personal data is collected, processed, manipulated, and shared with third parties.","In this paper, we present a novel comprehensive solution for developing a threat modelling technique to address threats of non-compliance and mitigate them by taking GDPR requirements as the baseline and combining them with the existing security and privacy modelling techniques (i.e., \\textit{STRIDE} and \\textit{LINDDUN}, respectively).","For this purpose, we propose a new data flow diagram integrated with the GDPR principles, develop a knowledge base for the non-compliance threats, and leverage an inference engine for reasoning the GDPR non-compliance threats over the knowledge base.","Finally, we demonstrate our solution for threats of non-compliance with legal basis and accountability in a telehealth system to show the feasibility and effectiveness of the proposed solution."],"url":"http://arxiv.org/abs/2404.13979v1","category":"cs.CR"}
{"created":"2024-04-22 08:38:57","title":"Josephson effect in a junction coupled to an electron reservoir","abstract":"We extend the scattering theory of the Josephson effect to include a coupling of the Josephson junction to a gapless electron reservoir in the normal state. By opening up the system with a quasiparticle escape rate $1/\\tau$, the supercurrent carried at zero temperature by an Andreev level at energy $\\varepsilon_{\\rm A}$ is reduced by a factor $(2/\\pi)\\arctan(2\\varepsilon_{\\rm A}\\tau/\\hbar)$ . We make contact with recent work on \"non-Hermitian Josephson junctions\", by comparing this result to different proposed generalizations of the Josephson effect to non-Hermitian Hamiltonians.","sentences":["We extend the scattering theory of the Josephson effect to include a coupling of the Josephson junction to a gapless electron reservoir in the normal state.","By opening up the system with a quasiparticle escape rate $1/\\tau$, the supercurrent carried at zero temperature by an Andreev level at energy $\\varepsilon_{\\rm A}$ is reduced by a factor $(2/\\pi)\\arctan(2\\varepsilon_{\\rm A}\\tau/\\hbar)$ .","We make contact with recent work on \"non-Hermitian Josephson junctions\", by comparing this result to different proposed generalizations of the Josephson effect to non-Hermitian Hamiltonians."],"url":"http://arxiv.org/abs/2404.13976v1","category":"cond-mat.supr-con"}
{"created":"2024-04-22 08:34:25","title":"A single-sided all-at-once preconditioning for linear system from a non-local evolutionary equation with weakly singular kernels","abstract":"{In [X. L. Lin, M. K. Ng, and Y. Zhi. {\\it J. Comput. Phys.}, 434 (2021), pp. 110221] and [Y. L. Zhao, J. Wu, X. M. Gu, and H. Li. {\\it Comput. Math. Appl.}, 148(2023), pp. 200--210]}, two-sided preconditioning techniques are proposed for non-local evolutionary equations, which possesses (i) mesh-size independent theoretical bound of condition number of the two-sided preconditioned matrix; (ii) small and stable iteration numbers in numerical tests. In this paper, we modify the two-sided preconditioning by multiplying the left-sided and the right-sided preconditioners together as a single-sided preconditioner. Such a single-sided preconditioner essentially derives from approximating the spatial matrix with a fast diagonalizable matrix and keeping the temporal matrix unchanged. Clearly, the matrix-vector multiplication of the single-sided preconditioning is faster to compute than that of the two-sided one, since the single-sided preconditioned matrix has a simpler structure. More importantly, we show theoretically that the single-sided preconditioned generalized minimal residual (GMRES) method has a convergence rate no worse than the two-sided preconditioned one. As a result, the one-sided preconditioned GMRES solver requires less computational time than the two-sided preconditioned GMRES solver in total. Numerical results are reported to show the efficiency of the proposed single-sided preconditioning technique.","sentences":["{In [X. L. Lin, M. K. Ng, and Y. Zhi. {\\it J. Comput.","Phys.}, 434 (2021), pp. 110221] and [Y. L. Zhao, J. Wu, X. M. Gu, and H. Li.","{\\it Comput.","Math.","Appl.}, 148(2023), pp.","200--210]}, two-sided preconditioning techniques are proposed for non-local evolutionary equations, which possesses (i) mesh-size independent theoretical bound of condition number of the two-sided preconditioned matrix; (ii) small and stable iteration numbers in numerical tests.","In this paper, we modify the two-sided preconditioning by multiplying the left-sided and the right-sided preconditioners together as a single-sided preconditioner.","Such a single-sided preconditioner essentially derives from approximating the spatial matrix with a fast diagonalizable matrix and keeping the temporal matrix unchanged.","Clearly, the matrix-vector multiplication of the single-sided preconditioning is faster to compute than that of the two-sided one, since the single-sided preconditioned matrix has a simpler structure.","More importantly, we show theoretically that the single-sided preconditioned generalized minimal residual (GMRES) method has a convergence rate no worse than the two-sided preconditioned one.","As a result, the one-sided preconditioned GMRES solver requires less computational time than the two-sided preconditioned GMRES solver in total.","Numerical results are reported to show the efficiency of the proposed single-sided preconditioning technique."],"url":"http://arxiv.org/abs/2404.13974v1","category":"math.NA"}
{"created":"2024-04-22 08:29:00","title":"DEQ-MCL: Discrete-Event Queue-based Monte-Carlo Localization","abstract":"Spatial cognition in hippocampal formation is posited to play a crucial role in the development of self-localization techniques for robots. In this paper, we propose a self-localization approach, DEQ-MCL, based on the discrete event queue hypothesis associated with phase precession within the hippocampal formation. Our method effectively estimates the posterior distribution of states, encompassing both past, present, and future states that are organized as a queue. This approach enables the smoothing of the posterior distribution of past states using current observations and the weighting of the joint distribution by considering the feasibility of future states. Our findings indicate that the proposed method holds promise for augmenting self-localization performance in indoor environments.","sentences":["Spatial cognition in hippocampal formation is posited to play a crucial role in the development of self-localization techniques for robots.","In this paper, we propose a self-localization approach, DEQ-MCL, based on the discrete event queue hypothesis associated with phase precession within the hippocampal formation.","Our method effectively estimates the posterior distribution of states, encompassing both past, present, and future states that are organized as a queue.","This approach enables the smoothing of the posterior distribution of past states using current observations and the weighting of the joint distribution by considering the feasibility of future states.","Our findings indicate that the proposed method holds promise for augmenting self-localization performance in indoor environments."],"url":"http://arxiv.org/abs/2404.13973v1","category":"cs.AI"}
{"created":"2024-04-22 08:28:41","title":"Non-Uniform Exposure Imaging via Neuromorphic Shutter Control","abstract":"By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments. However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control. To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure. Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes. To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns. Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches.","sentences":["By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments.","However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control.","To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure.","Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes.","To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns.","Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.13972v1","category":"cs.CV"}
{"created":"2024-04-22 08:26:04","title":"DALI sensitivity to streaming axion dark matter","abstract":"Dark matter substructures emerge naturally in a scenario in which the axion angular field acquires propagating degrees of freedom in a post-inflationary Universe. The DALI experiment is a new generation wavy dark matters interferometer currently in prototyping. Although DALI's main objective is to explore the virialized DM in our Galaxy, to a large degree in this article we explore the prospect for detection of fine grained streams made of axions that would enter the Solar System, as suggested by previous work. We find that DALI will have a sensitivity to encounters with these coherent objects that fully spans the window defined by the coupling strength to photons of representative axion models over a stacked bandwidth of two decades in mass.","sentences":["Dark matter substructures emerge naturally in a scenario in which the axion angular field acquires propagating degrees of freedom in a post-inflationary Universe.","The DALI experiment is a new generation wavy dark matters interferometer currently in prototyping.","Although DALI's main objective is to explore the virialized DM in our Galaxy, to a large degree in this article we explore the prospect for detection of fine grained streams made of axions that would enter the Solar System, as suggested by previous work.","We find that DALI will have a sensitivity to encounters with these coherent objects that fully spans the window defined by the coupling strength to photons of representative axion models over a stacked bandwidth of two decades in mass."],"url":"http://arxiv.org/abs/2404.13970v1","category":"hep-ph"}
{"created":"2024-04-22 08:16:07","title":"Protecting Your LLMs with Information Bottleneck","abstract":"The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.","sentences":["The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.","Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.","To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.","The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.","Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.","Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed.","Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models."],"url":"http://arxiv.org/abs/2404.13968v1","category":"cs.CL"}
{"created":"2024-04-22 08:10:38","title":"An Economic Solution to Copyright Challenges of Generative AI","abstract":"Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.","sentences":["Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media.","There is growing concern that such systems may infringe on the copyright interests of training data contributors.","To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content.","The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics.","This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance.","Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training.","Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners."],"url":"http://arxiv.org/abs/2404.13964v1","category":"cs.LG"}
{"created":"2024-04-22 08:08:45","title":"Chiral-odd generalized parton distributions of sea quarks at $\u03be=0$ in the light-cone quark model","abstract":"We study the chiral-odd generalized parton distributions (GPDs) of the $\\bar{u}$ and $\\bar{d}$ quarks inside the proton at zero skewness using the overlap representation within the light cone formalism. Using the light cone wave functions (LCWFs) of the proton obtained from the baryon-meson fluctuation model in terms of the $|q\\bar{q}B\\rangle$ Fock states, we provide the expressions of the GPDs $\\widetilde{H}^{\\bar{q}/P}_T(x,0,t)$, $H_T^{\\bar{q}/P}(x,0,t)$ and $E_T^{\\bar{q}/P}(x,0,t)$ for $\\bar{q}=\\bar{u}$ and $\\bar{d}$. Numerical results for these GPDs in momentum space as well as in impact parameter space are presented. We further investigate certain combinations of the chiral-odd GPDs in impact parameter space to the spin-orbit correlation effect of the sea quarks.","sentences":["We study the chiral-odd generalized parton distributions (GPDs) of the $\\bar{u}$ and $\\bar{d}$ quarks inside the proton at zero skewness using the overlap representation within the light cone formalism.","Using the light cone wave functions (LCWFs) of the proton obtained from the baryon-meson fluctuation model in terms of the $|q\\bar{q}B\\rangle$ Fock states, we provide the expressions of the GPDs $\\widetilde{H}^{\\bar{q}/P}_T(x,0,t)$, $H_T^{\\bar{q}/P}(x,0,t)$ and $E_T^{\\bar{q}/P}(x,0,t)$ for $\\bar{q}=\\bar{u}$ and $\\bar{d}$. Numerical results for these GPDs in momentum space as well as in impact parameter space are presented.","We further investigate certain combinations of the chiral-odd GPDs in impact parameter space to the spin-orbit correlation effect of the sea quarks."],"url":"http://arxiv.org/abs/2404.13962v1","category":"hep-ph"}
{"created":"2024-04-22 08:08:07","title":"Sharp quantitative stability of the Yamabe problem","abstract":"Given a smooth closed Riemannian manifold $(M,g)$ of dimension $N \\ge 3$, we derive sharp quantitative stability estimates for nonnegative functions near the solution set of the Yamabe problem on $(M,g)$. The seminal work of Struwe (1984) \\cite{S} states that if $\\Gamma(u) := \\|\\Delta_g u - \\frac{N-2}{4(N-1)} R_g u + u^{\\frac{N+2}{N-2}}\\|_{H^{-1}(M)} \\to 0$, then $\\|u-(u_0+\\sum_{i=1}^{\\nu} \\mathcal{V}_i)\\|_{H^1(M)} \\to 0$ where $u_0$ is a solution to the Yamabe problem on $(M,g)$, $\\nu \\in \\mathbb{N} \\cup \\{0\\}$, and $\\mathcal{V}_i$ is a bubble-like function. If $M$ is the round sphere $\\mathbb{S}^N$, then $u_0 \\equiv 0$ and a natural candidate of $\\mathcal{V}_i$ is a bubble itself. If $M$ is not conformally equivalent to $\\mathbb{S}^N$, then either $u_0 > 0$ or $u_0 \\equiv 0$, there is no canonical choice of $\\mathcal{V}_i$, and so a careful selection of $\\mathcal{V}_i$ must be made to attain optimal estimates.   For $3 \\le N \\le 5$, we construct suitable $\\mathcal{V}_i$'s and then establish the inequality $\\|u-(u_0+\\sum_{i=1}^{\\nu} \\mathcal{V}_i)\\|_{H^1(M)}$ $ \\le C\\zeta(\\Gamma(u))$ where $C > 0$ and $\\zeta(t) = t$, consistent with the result of Figalli and Glaudo (2020) \\cite{FG} on $\\mathbb{S}^N$. In the case of $N \\ge 6$, we investigate the single-bubbling phenomenon $(\\nu = 1)$ on generic Riemannian manifolds $(M,g)$, proving that $\\zeta(t)$ is determined by $N$, $u_0$, and $g$, and can be much larger than $t$. This exhibits a striking difference from the result of Ciraolo, Figalli, and Maggi (2018) \\cite{CFM} on $\\mathbb{S}^N$. All of the estimates presented herein are optimal.","sentences":["Given a smooth closed Riemannian manifold $(M,g)$ of dimension $N \\ge 3$, we derive sharp quantitative stability estimates for nonnegative functions near the solution set of the Yamabe problem on $(M,g)$.","The seminal work of Struwe (1984) \\cite{S} states that if $\\Gamma(u) := \\|\\Delta_g u - \\frac{N-2}{4(N-1)} R_g u + u^{\\frac{N+2}{N-2}}\\|_{H^{-1}(M)} \\to 0$, then $\\|u-(u_0+\\sum_{i=1}^{\\nu} \\mathcal{V}_i)\\|_{H^1(M)} \\to 0$ where $u_0$ is a solution to the Yamabe problem on $(M,g)$, $\\nu \\in \\mathbb{N} \\cup \\{0\\}$, and $\\mathcal{V}_i$ is a bubble-like function.","If $M$ is the round sphere $\\mathbb{S}^N$, then $u_0 \\equiv 0$ and a natural candidate of $\\mathcal{V}_i$ is a bubble itself.","If $M$ is not conformally equivalent to $\\mathbb{S}^N$, then either $u_0 > 0$ or $u_0 \\equiv 0$, there is no canonical choice of $\\mathcal{V}_i$, and so a careful selection of $\\mathcal{V}_i$ must be made to attain optimal estimates.   ","For $3 \\le N \\le 5$, we construct suitable $\\mathcal{V}_i$'s and then establish the inequality $\\|u-(u_0+\\sum_{i=1}^{\\nu} \\mathcal{V}_i)\\|_{H^1(M)}$ $ \\le C\\zeta(\\Gamma(u))$ where $C > 0$ and $\\zeta(t) = t$, consistent with the result of Figalli and Glaudo (2020) \\cite{FG} on $\\mathbb{S}^N$. In the case of $N \\ge 6$, we investigate the single-bubbling phenomenon $(\\nu = 1)$ on generic Riemannian manifolds $(M,g)$, proving that $\\zeta(t)$ is determined by $N$, $u_0$, and $g$, and can be much larger than $t$. This exhibits a striking difference from the result of Ciraolo, Figalli, and Maggi (2018) \\cite{CFM} on $\\mathbb{S}^N$. All of the estimates presented herein are optimal."],"url":"http://arxiv.org/abs/2404.13961v1","category":"math.AP"}
{"created":"2024-04-22 08:00:51","title":"How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability with ECHO","abstract":"The role-play ability of Large Language Models (LLMs) has emerged as a popular research direction. However, existing studies focus on imitating well-known public figures or fictional characters, overlooking the potential for simulating ordinary individuals. Such an oversight limits the potential for advancements in digital human clones and non-player characters in video games. To bridge this gap, we introduce ECHO, an evaluative framework inspired by the Turing test. This framework engages the acquaintances of the target individuals to distinguish between human and machine-generated responses. Notably, our framework focuses on emulating average individuals rather than historical or fictional figures, presenting a unique advantage to apply the Turing Test. We evaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as foundational models, alongside the online application GPTs from OpenAI. Our results demonstrate that GPT-4 more effectively deceives human evaluators, and GPTs achieves a leading success rate of 48.3%. Furthermore, we investigated whether LLMs could discern between human-generated and machine-generated texts. While GPT-4 can identify differences, it could not determine which texts were human-produced. Our code and results of reproducing the role-playing LLMs are made publicly available via https://github.com/CUHK-ARISE/ECHO.","sentences":["The role-play ability of Large Language Models (LLMs) has emerged as a popular research direction.","However, existing studies focus on imitating well-known public figures or fictional characters, overlooking the potential for simulating ordinary individuals.","Such an oversight limits the potential for advancements in digital human clones and non-player characters in video games.","To bridge this gap, we introduce ECHO, an evaluative framework inspired by the Turing test.","This framework engages the acquaintances of the target individuals to distinguish between human and machine-generated responses.","Notably, our framework focuses on emulating average individuals rather than historical or fictional figures, presenting a unique advantage to apply the Turing Test.","We evaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as foundational models, alongside the online application GPTs from OpenAI.","Our results demonstrate that GPT-4 more effectively deceives human evaluators, and GPTs achieves a leading success rate of 48.3%.","Furthermore, we investigated whether LLMs could discern between human-generated and machine-generated texts.","While GPT-4 can identify differences, it could not determine which texts were human-produced.","Our code and results of reproducing the role-playing LLMs are made publicly available via https://github.com/CUHK-ARISE/ECHO."],"url":"http://arxiv.org/abs/2404.13957v1","category":"cs.CL"}
{"created":"2024-04-22 07:55:11","title":"Unraveling the Kinematics of IZw18: A Detailed Study of Ionized Gas with MEGARA/GTC","abstract":"This study delves into the intricate kinematic behavior of ionized gas within IZw18, a galaxy known for its remarkably low metallicity and proximity. Leveraging data from MEGARA/GTC, we meticulously analyzed the galaxy's structure and dynamics using H{\\alpha} line profiles. Employing single and double Gaussian component fittings, we generated detailed maps of luminosity, velocity, and velocity dispersion across both the main body (MB) and Halo regions. By extracting integrated spectra from various galactic zones, we enhanced the signal-to-noise (S/N) ratio. In the MB, a clear rotational pattern emerged, supplemented by a more intricate kinematic structure from the double-component fitting, notably featuring a broad component with a FWHM nearing 2000 km/s. This broad component, exhibiting wide spatial extension, hints at a high-energy outflow and suggests non-localized sources of significant kinetic energy. The considerable velocity disparities between narrow and broad components imply potential spatial segregation, possibly due to dense gas near the kinematic origin acting as a momentum-reflection 'wall'. Examining the halos, the NE Halo appears tranquil with low velocity dispersions, while the SW Halo displays higher velocities and complex kinematics, indicative of varied dynamic interactions. The presence of the broad component throughout the MB and the intricate kinematics across all regions suggests widespread and subtle turbulent motion. This nuanced understanding of IZw18's kinematic behavior, elucidating the interplay of gas components and internal structures, enriches our comprehension of dynamics in blue compact dwarf galaxies. It holds promise for shedding light on early galaxy formation and the complex kinematics inherent to such environments.","sentences":["This study delves into the intricate kinematic behavior of ionized gas within IZw18, a galaxy known for its remarkably low metallicity and proximity.","Leveraging data from MEGARA/GTC, we meticulously analyzed the galaxy's structure and dynamics using H{\\alpha} line profiles.","Employing single and double Gaussian component fittings, we generated detailed maps of luminosity, velocity, and velocity dispersion across both the main body (MB) and Halo regions.","By extracting integrated spectra from various galactic zones, we enhanced the signal-to-noise (S/N) ratio.","In the MB, a clear rotational pattern emerged, supplemented by a more intricate kinematic structure from the double-component fitting, notably featuring a broad component with a FWHM nearing 2000 km/s. This broad component, exhibiting wide spatial extension, hints at a high-energy outflow and suggests non-localized sources of significant kinetic energy.","The considerable velocity disparities between narrow and broad components imply potential spatial segregation, possibly due to dense gas near the kinematic origin acting as a momentum-reflection 'wall'.","Examining the halos, the NE Halo appears tranquil with low velocity dispersions, while the SW Halo displays higher velocities and complex kinematics, indicative of varied dynamic interactions.","The presence of the broad component throughout the MB and the intricate kinematics across all regions suggests widespread and subtle turbulent motion.","This nuanced understanding of IZw18's kinematic behavior, elucidating the interplay of gas components and internal structures, enriches our comprehension of dynamics in blue compact dwarf galaxies.","It holds promise for shedding light on early galaxy formation and the complex kinematics inherent to such environments."],"url":"http://arxiv.org/abs/2404.13956v1","category":"astro-ph.GA"}
{"created":"2024-04-22 07:54:56","title":"A survey of air combat behavior modeling using machine learning","abstract":"With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest. This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training. Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps. Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods. Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge. The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents. Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization. Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration. These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications.","sentences":["With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest.","This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training.","Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps.","Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods.","Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge.","The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents.","Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization.","Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration.","These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications."],"url":"http://arxiv.org/abs/2404.13954v1","category":"cs.LG"}
{"created":"2024-04-22 07:54:53","title":"360VOTS: Visual Object Tracking and Segmentation in Omnidirectional Videos","abstract":"Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360{\\deg} images. To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks. Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS). The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories. To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences. Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment. Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset. Homepage: https://360vots.hkustvgd.com/","sentences":["Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360{\\deg} images.","To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks.","Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS).","The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories.","To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences.","Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment.","Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset.","Homepage: https://360vots.hkustvgd.com/"],"url":"http://arxiv.org/abs/2404.13953v1","category":"cs.CV"}
{"created":"2024-04-22 07:51:13","title":"SPLATE: Sparse Late Interaction Retrieval","abstract":"The late interaction paradigm introduced with ColBERT stands out in the neural Information Retrieval space, offering a compelling effectiveness-efficiency trade-off across many benchmarks. Efficient late interaction retrieval is based on an optimized multi-step strategy, where an approximate search first identifies a set of candidate documents to re-rank exactly. In this work, we introduce SPLATE, a simple and lightweight adaptation of the ColBERTv2 model which learns an ``MLM adapter'', mapping its frozen token embeddings to a sparse vocabulary space with a partially learned SPLADE module. This allows us to perform the candidate generation step in late interaction pipelines with traditional sparse retrieval techniques, making it particularly appealing for running ColBERT in CPU environments. Our SPLATE ColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2 engine by re-ranking 50 documents that can be retrieved under 10ms.","sentences":["The late interaction paradigm introduced with ColBERT stands out in the neural Information Retrieval space, offering a compelling effectiveness-efficiency trade-off across many benchmarks.","Efficient late interaction retrieval is based on an optimized multi-step strategy, where an approximate search first identifies a set of candidate documents to re-rank exactly.","In this work, we introduce SPLATE, a simple and lightweight adaptation of the ColBERTv2 model which learns an ``MLM adapter'', mapping its frozen token embeddings to a sparse vocabulary space with a partially learned SPLADE module.","This allows us to perform the candidate generation step in late interaction pipelines with traditional sparse retrieval techniques, making it particularly appealing for running ColBERT in CPU environments.","Our SPLATE ColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2 engine by re-ranking 50 documents that can be retrieved under 10ms."],"url":"http://arxiv.org/abs/2404.13950v1","category":"cs.IR"}
{"created":"2024-04-22 07:49:36","title":"Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations","abstract":"The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.","sentences":["The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications.","Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors.","In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness.","Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects.","Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents.","We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs.","The experimental results show that GARAG consistently achieves high attack success rates.","Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world."],"url":"http://arxiv.org/abs/2404.13948v1","category":"cs.CL"}
{"created":"2024-04-22 07:44:20","title":"Boter: Bootstrapping Knowledge Selection and Question Answering for Knowledge-based VQA","abstract":"Knowledge-based Visual Question Answering (VQA) requires models to incorporate external knowledge to respond to questions about visual content. Previous methods mostly follow the \"retrieve and generate\" paradigm. Initially, they utilize a pre-trained retriever to fetch relevant knowledge documents, subsequently employing them to generate answers. While these methods have demonstrated commendable performance in the task, they possess limitations: (1) they employ an independent retriever to acquire knowledge solely based on the similarity between the query and knowledge embeddings, without assessing whether the knowledge document is truly conducive to helping answer the question; (2) they convert the image into text and then conduct retrieval and answering in natural language space, which may not ensure comprehensive acquisition of all image information. To address these limitations, we propose Boter, a novel framework designed to bootstrap knowledge selection and question answering by leveraging the robust multimodal perception capabilities of the Multimodal Large Language Model (MLLM). The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned in a simple cycle: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat. Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%.","sentences":["Knowledge-based Visual Question Answering (VQA) requires models to incorporate external knowledge to respond to questions about visual content.","Previous methods mostly follow the \"retrieve and generate\" paradigm.","Initially, they utilize a pre-trained retriever to fetch relevant knowledge documents, subsequently employing them to generate answers.","While these methods have demonstrated commendable performance in the task, they possess limitations: (1) they employ an independent retriever to acquire knowledge solely based on the similarity between the query and knowledge embeddings, without assessing whether the knowledge document is truly conducive to helping answer the question; (2) they convert the image into text and then conduct retrieval and answering in natural language space, which may not ensure comprehensive acquisition of all image information.","To address these limitations, we propose Boter, a novel framework designed to bootstrap knowledge selection and question answering by leveraging the robust multimodal perception capabilities of the Multimodal Large Language Model (MLLM).","The framework consists of two modules: Selector and Answerer, where both are initialized by the MLLM and parameter-efficiently finetuned in a simple cycle: find key knowledge in the retrieved knowledge documents using the Selector, and then use them to finetune the Answerer to predict answers; obtain the pseudo-labels of key knowledge documents based on the predictions of the Answerer and weak supervision labels, and then finetune the Selector to select key knowledge; repeat.","Our framework significantly enhances the performance of the baseline on the challenging open-domain Knowledge-based VQA benchmark, OK-VQA, achieving a state-of-the-art accuracy of 62.83%."],"url":"http://arxiv.org/abs/2404.13947v1","category":"cs.CV"}
{"created":"2024-04-22 07:44:02","title":"Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning","abstract":"In recent years, the neural network backdoor hidden in the parameters of the federated learning model has been proved to have great security risks. Considering the characteristics of trigger generation, data poisoning and model training in backdoor attack, this paper designs a backdoor attack method based on federated learning. Firstly, aiming at the concealment of the backdoor trigger, a TrojanGan steganography model with encoder-decoder structure is designed. The model can encode specific attack information as invisible noise and attach it to the image as a backdoor trigger, which improves the concealment and data transformations of the backdoor trigger.Secondly, aiming at the problem of single backdoor trigger mode, an image poisoning attack method called combination trigger attack is proposed. This method realizes multi-backdoor triggering by multiplexing combined triggers and improves the robustness of backdoor attacks. Finally, aiming at the problem that the local training mechanism leads to the decrease of the success rate of backdoor attack, a dual model replacement backdoor attack algorithm based on federated learning is designed. This method can improve the success rate of backdoor attack while maintaining the performance of the federated learning aggregation model. Experiments show that the attack strategy in this paper can not only achieve high backdoor concealment and diversification of trigger forms under federated learning, but also achieve good attack success rate in multi-target attacks.door concealment and diversification of trigger forms but also achieve good results in multi-target attacks.","sentences":["In recent years, the neural network backdoor hidden in the parameters of the federated learning model has been proved to have great security risks.","Considering the characteristics of trigger generation, data poisoning and model training in backdoor attack, this paper designs a backdoor attack method based on federated learning.","Firstly, aiming at the concealment of the backdoor trigger, a TrojanGan steganography model with encoder-decoder structure is designed.","The model can encode specific attack information as invisible noise and attach it to the image as a backdoor trigger, which improves the concealment and data transformations of the backdoor trigger.","Secondly, aiming at the problem of single backdoor trigger mode, an image poisoning attack method called combination trigger attack is proposed.","This method realizes multi-backdoor triggering by multiplexing combined triggers and improves the robustness of backdoor attacks.","Finally, aiming at the problem that the local training mechanism leads to the decrease of the success rate of backdoor attack, a dual model replacement backdoor attack algorithm based on federated learning is designed.","This method can improve the success rate of backdoor attack while maintaining the performance of the federated learning aggregation model.","Experiments show that the attack strategy in this paper can not only achieve high backdoor concealment and diversification of trigger forms under federated learning, but also achieve good attack success rate in multi-target attacks.door concealment and diversification of trigger forms but also achieve good results in multi-target attacks."],"url":"http://arxiv.org/abs/2404.13946v1","category":"cs.LG"}
{"created":"2024-04-22 07:41:41","title":"Benchmarking Multi-Modal LLMs for Testing Visual Deep Learning Systems Through the Lens of Image Mutation","abstract":"Visual deep learning (VDL) systems have shown significant success in real-world applications like image recognition, object detection, and autonomous driving. To evaluate the reliability of VDL, a mainstream approach is software testing, which requires diverse and controllable mutations over image semantics. The rapid development of multi-modal large language models (MLLMs) has introduced revolutionary image mutation potentials through instruction-driven methods. Users can now freely describe desired mutations and let MLLMs generate the mutated images.   However, the quality of MLLM-produced test inputs in VDL testing remains largely unexplored. We present the first study, aiming to assess MLLMs' adequacy from 1) the semantic validity of MLLM mutated images, 2) the alignment of MLLM mutated images with their text instructions (prompts), 3) the faithfulness of how different mutations preserve semantics that are ought to remain unchanged, and 4) the effectiveness of detecting VDL faults. With large-scale human studies and quantitative evaluations, we identify MLLM's promising potentials in expanding the covered semantics of image mutations. Notably, while SoTA MLLMs (e.g., GPT-4V) fail to support or perform worse in editing existing semantics in images (as in traditional mutations like rotation), they generate high-quality test inputs using \"semantic-additive\" mutations (e.g., \"dress a dog with clothes\"), which bring extra semantics to images; these were infeasible for past approaches. Hence, we view MLLM-based mutations as a vital complement to traditional mutations, and advocate future VDL testing tasks to combine MLLM-based methods and traditional image mutations for comprehensive and reliable testing.","sentences":["Visual deep learning (VDL) systems have shown significant success in real-world applications like image recognition, object detection, and autonomous driving.","To evaluate the reliability of VDL, a mainstream approach is software testing, which requires diverse and controllable mutations over image semantics.","The rapid development of multi-modal large language models (MLLMs) has introduced revolutionary image mutation potentials through instruction-driven methods.","Users can now freely describe desired mutations and let MLLMs generate the mutated images.   ","However, the quality of MLLM-produced test inputs in VDL testing remains largely unexplored.","We present the first study, aiming to assess MLLMs' adequacy from 1) the semantic validity of MLLM mutated images, 2) the alignment of MLLM mutated images with their text instructions (prompts), 3) the faithfulness of how different mutations preserve semantics that are ought to remain unchanged, and 4) the effectiveness of detecting VDL faults.","With large-scale human studies and quantitative evaluations, we identify MLLM's promising potentials in expanding the covered semantics of image mutations.","Notably, while SoTA MLLMs (e.g., GPT-4V) fail to support or perform worse in editing existing semantics in images (as in traditional mutations like rotation), they generate high-quality test inputs using \"semantic-additive\" mutations (e.g., \"dress a dog with clothes\"), which bring extra semantics to images; these were infeasible for past approaches.","Hence, we view MLLM-based mutations as a vital complement to traditional mutations, and advocate future VDL testing tasks to combine MLLM-based methods and traditional image mutations for comprehensive and reliable testing."],"url":"http://arxiv.org/abs/2404.13945v1","category":"cs.SE"}
{"created":"2024-04-22 07:40:53","title":"Gorgeous: Create Your Desired Character Facial Makeup from Any Ideas","abstract":"Contemporary makeup transfer methods primarily focus on replicating makeup from one face to another, considerably limiting their use in creating diverse and creative character makeup essential for visual storytelling. Such methods typically fail to address the need for uniqueness and contextual relevance, specifically aligning with character and story settings as they depend heavily on existing facial makeup in reference images. This approach also presents a significant challenge when attempting to source a perfectly matched facial makeup style, further complicating the creation of makeup designs inspired by various story elements, such as theme, background, and props that do not necessarily feature faces. To address these limitations, we introduce $Gorgeous$, a novel diffusion-based makeup application method that goes beyond simple transfer by innovatively crafting unique and thematic facial makeup. Unlike traditional methods, $Gorgeous$ does not require the presence of a face in the reference images. Instead, it draws artistic inspiration from a minimal set of three to five images, which can be of any type, and transforms these elements into practical makeup applications directly on the face. Our comprehensive experiments demonstrate that $Gorgeous$ can effectively generate distinctive character facial makeup inspired by the chosen thematic reference images. This approach opens up new possibilities for integrating broader story elements into character makeup, thereby enhancing the narrative depth and visual impact in storytelling.","sentences":["Contemporary makeup transfer methods primarily focus on replicating makeup from one face to another, considerably limiting their use in creating diverse and creative character makeup essential for visual storytelling.","Such methods typically fail to address the need for uniqueness and contextual relevance, specifically aligning with character and story settings as they depend heavily on existing facial makeup in reference images.","This approach also presents a significant challenge when attempting to source a perfectly matched facial makeup style, further complicating the creation of makeup designs inspired by various story elements, such as theme, background, and props that do not necessarily feature faces.","To address these limitations, we introduce $Gorgeous$, a novel diffusion-based makeup application method that goes beyond simple transfer by innovatively crafting unique and thematic facial makeup.","Unlike traditional methods, $Gorgeous$ does not require the presence of a face in the reference images.","Instead, it draws artistic inspiration from a minimal set of three to five images, which can be of any type, and transforms these elements into practical makeup applications directly on the face.","Our comprehensive experiments demonstrate that $Gorgeous$ can effectively generate distinctive character facial makeup inspired by the chosen thematic reference images.","This approach opens up new possibilities for integrating broader story elements into character makeup, thereby enhancing the narrative depth and visual impact in storytelling."],"url":"http://arxiv.org/abs/2404.13944v1","category":"cs.CV"}
{"created":"2024-04-22 07:34:28","title":"Autoencoder-assisted Feature Ensemble Net for Incipient Faults","abstract":"Deep learning has shown the great power in the field of fault detection. However, for incipient faults with tiny amplitude, the detection performance of the current deep learning networks (DLNs) is not satisfactory. Even if prior information about the faults is utilized, DLNs can't successfully detect faults 3, 9 and 15 in Tennessee Eastman process (TEP). These faults are notoriously difficult to detect, lacking effective detection technologies in the field of fault detection. In this work, we propose Autoencoder-assisted Feature Ensemble Net (AE-FENet): a deep feature ensemble framework that uses the unsupervised autoencoder to conduct the feature transformation. Compared with the principle component analysis (PCA) technique adopted in the original Feature Ensemble Net (FENet), autoencoder can mine more exact features on incipient faults, which results in the better detection performance of AE-FENet. With same kinds of basic detectors, AE-FENet achieves a state-of-the-art average accuracy over 96% on faults 3, 9 and 15 in TEP, which represents a significant enhancement in performance compared to other methods. Plenty of experiments have been done to extend our framework, proving that DLNs can be utilized efficiently within this architecture.","sentences":["Deep learning has shown the great power in the field of fault detection.","However, for incipient faults with tiny amplitude, the detection performance of the current deep learning networks (DLNs) is not satisfactory.","Even if prior information about the faults is utilized, DLNs can't successfully detect faults 3, 9 and 15 in Tennessee Eastman process (TEP).","These faults are notoriously difficult to detect, lacking effective detection technologies in the field of fault detection.","In this work, we propose Autoencoder-assisted Feature Ensemble Net (AE-FENet): a deep feature ensemble framework that uses the unsupervised autoencoder to conduct the feature transformation.","Compared with the principle component analysis (PCA) technique adopted in the original Feature Ensemble Net (FENet), autoencoder can mine more exact features on incipient faults, which results in the better detection performance of AE-FENet.","With same kinds of basic detectors, AE-FENet achieves a state-of-the-art average accuracy over 96% on faults 3, 9 and 15 in TEP, which represents a significant enhancement in performance compared to other methods.","Plenty of experiments have been done to extend our framework, proving that DLNs can be utilized efficiently within this architecture."],"url":"http://arxiv.org/abs/2404.13941v1","category":"eess.SY"}
{"created":"2024-04-22 07:31:06","title":"Unlocking Insights: Enhanced Analysis of Covariance in General Factorial Designs through Multiple Contrast Tests under Variance Heteroscedasticity","abstract":"A common goal in clinical trials is to conduct tests on estimated treatment effects adjusted for covariates such as age or sex. Analysis of Covariance (ANCOVA) is often used in these scenarios to test the global null hypothesis of no treatment effect using an $F$-test. However, in several samples, the $F$-test does not provide any information about individual null hypotheses and has strict assumptions such as variance homoscedasticity. We extend the method proposed by Konietschke et al. (2021) to a multiple contrast test procedure (MCTP), which allows us to test arbitrary linear hypotheses and provides information about the global as well as the individual null hypotheses. Further, we can calculate compatible simultaneous confidence intervals for the individual effects. We derive a small sample size approximation of the distribution of the test statistic via a multivariate t-distribution. As an alternative, we introduce a Wild-bootstrap method. Extensive simulations show that our methods are applicable even when sample sizes are small. Their application is further illustrated within a real data example.","sentences":["A common goal in clinical trials is to conduct tests on estimated treatment effects adjusted for covariates such as age or sex.","Analysis of Covariance (ANCOVA) is often used in these scenarios to test the global null hypothesis of no treatment effect using an $F$-test.","However, in several samples, the $F$-test does not provide any information about individual null hypotheses and has strict assumptions such as variance homoscedasticity.","We extend the method proposed by Konietschke et al. (2021) to a multiple contrast test procedure (MCTP), which allows us to test arbitrary linear hypotheses and provides information about the global as well as the individual null hypotheses.","Further, we can calculate compatible simultaneous confidence intervals for the individual effects.","We derive a small sample size approximation of the distribution of the test statistic via a multivariate t-distribution.","As an alternative, we introduce a Wild-bootstrap method.","Extensive simulations show that our methods are applicable even when sample sizes are small.","Their application is further illustrated within a real data example."],"url":"http://arxiv.org/abs/2404.13939v1","category":"stat.ME"}
{"created":"2024-04-22 07:04:51","title":"Bell Correlations via Constrained Colliders -- a Path from W to V","abstract":"In previous work with Ken Wharton, I have proposed that Bell correlations are a special sort of selection artefact, explained by a combination of (i) collider bias and (ii) a boundary constraint on the collider variable. This hypothesis requires no direct causal influence outside lightcones, and may hence offer a new way to reconcile Bell nonlocality and relativity. This piece outlines a new argument for the proposal. It explains how it is valid for a special class of (W-shaped) Bell experiments involving delayed-choice entanglement swapping, and argues that it can be extended to the general (V-shaped) case.","sentences":["In previous work with Ken Wharton, I have proposed that Bell correlations are a special sort of selection artefact, explained by a combination of (i) collider bias and (ii) a boundary constraint on the collider variable.","This hypothesis requires no direct causal influence outside lightcones, and may hence offer a new way to reconcile Bell nonlocality and relativity.","This piece outlines a new argument for the proposal.","It explains how it is valid for a special class of (W-shaped)","Bell experiments involving delayed-choice entanglement swapping, and argues that it can be extended to the general (V-shaped) case."],"url":"http://arxiv.org/abs/2404.13928v1","category":"quant-ph"}
{"created":"2024-04-22 07:01:19","title":"ActSonic: Everyday Activity Recognition on Smart Glasses using Active Acoustic Sensing","abstract":"In this paper, we introduce ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses. ActSonic is designed to recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing). It only needs a pair of miniature speakers and microphones mounted on each hinge of eyeglasses to emit ultrasonic waves to create an acoustic aura around the body. Based on the position and motion of various body parts, the acoustic signals are reflected with unique patterns captured by the microphone and analyzed by a customized self-supervised deep learning framework to infer the performed activities. ActSonic was deployed in a user study with 19 participants across 19 households to evaluate its efficacy. Without requiring any training data from a new user (leave-one-participant-out evaluation), ActSonic was able to detect 27 activities with an inference resolution of 1 second, achieving an average F1-score of 86.6% in an unconstrained setting and 93.4% in a prompted setting.","sentences":["In this paper, we introduce ActSonic, an intelligent, low-power active acoustic sensing system integrated into eyeglasses.","ActSonic is designed to recognize 27 different everyday activities (e.g., eating, drinking, toothbrushing).","It only needs a pair of miniature speakers and microphones mounted on each hinge of eyeglasses to emit ultrasonic waves to create an acoustic aura around the body.","Based on the position and motion of various body parts, the acoustic signals are reflected with unique patterns captured by the microphone and analyzed by a customized self-supervised deep learning framework to infer the performed activities.","ActSonic was deployed in a user study with 19 participants across 19 households to evaluate its efficacy.","Without requiring any training data from a new user (leave-one-participant-out evaluation), ActSonic was able to detect 27 activities with an inference resolution of 1 second, achieving an average F1-score of 86.6% in an unconstrained setting and 93.4% in a prompted setting."],"url":"http://arxiv.org/abs/2404.13924v1","category":"cs.HC"}
{"created":"2024-04-22 07:00:17","title":"MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets","abstract":"Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance. By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space. However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture. As a result, material maps optimized by SDS inevitably involve spurious correlated components. The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios. In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics. Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior. Based on such a prior model, we devise a mechanism to parse material in 3D space. We maintain a UV stack, each map of which is unprojected from a specific viewpoint. After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts. To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method.","sentences":["Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance.","By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space.","However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture.","As a result, material maps optimized by SDS inevitably involve spurious correlated components.","The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios.","In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics.","Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior.","Based on such a prior model, we devise a mechanism to parse material in 3D space.","We maintain a UV stack, each map of which is unprojected from a specific viewpoint.","After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts.","To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations.","Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.13923v1","category":"cs.CV"}
{"created":"2024-04-22 06:59:53","title":"A Platform for All-optical Thomson/ Compton Scattering with Versatile Parameters","abstract":"A dual-beam platform for all-optical electron-photon scattering, or Thomson/Compton scattering, with adjustable collision-angle and parameter tuning ability has been developed, which, in principle, can be used for the verification of strong-field quantum electrodynamics effects. Combining this platform with a 200 TW Ti:Sapphire laser system, we demonstrated the generation of inverse Compton scattering X/gamma-rays with tunable energies from tens of keV to MeV. The polarization of X/gamma radiation was manipulated by controlling the polarization of scattering laser. In the near future, by combining this experimental platform with multi-PW laser facilities, it is proposed to experimentally generate X/gamma radiation with orbital angular momentum for the nuclear isomer excitation, and more importantly, to explore the regime transition from nonlinear Thomson scattering to nonlinear Compton scattering, eventually to demonstrate the verification of theories on extremely strong field quantum electrodynamics effects.","sentences":["A dual-beam platform for all-optical electron-photon scattering, or Thomson/Compton scattering, with adjustable collision-angle and parameter tuning ability has been developed, which, in principle, can be used for the verification of strong-field quantum electrodynamics effects.","Combining this platform with a 200 TW Ti:Sapphire laser system, we demonstrated the generation of inverse Compton scattering X/gamma-rays with tunable energies from tens of keV to MeV.","The polarization of X/gamma radiation was manipulated by controlling the polarization of scattering laser.","In the near future, by combining this experimental platform with multi-PW laser facilities, it is proposed to experimentally generate X/gamma radiation with orbital angular momentum for the nuclear isomer excitation, and more importantly, to explore the regime transition from nonlinear Thomson scattering to nonlinear Compton scattering, eventually to demonstrate the verification of theories on extremely strong field quantum electrodynamics effects."],"url":"http://arxiv.org/abs/2404.13922v1","category":"hep-ex"}
{"created":"2024-04-22 06:58:22","title":"Open Datasets for Satellite Radio Resource Control","abstract":"In Non-Terrestrial Networks (NTN), achieving effective radio resource allocation across multi-satellite system, encompassing efficient channel and bandwidth allocation, effective beam management, power control and interference mitigation, poses significant challenges due to the varying satellite links and highly dynamic nature of user traffic. This calls for the development of an intelligent decision-making controller using Artificial Intelligence (AI) to efficiently manage resources in this complex environment. In this context, open datasets can play a crucial role in driving new advancement and facilitating research. Recognizing the significance, this paper aims to contribute the satellite communication research community by providing various open datasets that incorporate realistic traffic flow enabling a variety of uses cases. The primary objective of sharing these datasets is to facilitate the development and benchmarking of advanced resource management solutions, thereby improving the overall satellite communication systems. Furthermore, an application example focused on beam placement optimization via terminal clustering is provided. This assists in optimizing beam allocation task, enabling adaptive beamforming to effectively meet spatiotemporally varying user traffic demands and optimize resource utilization.","sentences":["In Non-Terrestrial Networks (NTN), achieving effective radio resource allocation across multi-satellite system, encompassing efficient channel and bandwidth allocation, effective beam management, power control and interference mitigation, poses significant challenges due to the varying satellite links and highly dynamic nature of user traffic.","This calls for the development of an intelligent decision-making controller using Artificial Intelligence (AI) to efficiently manage resources in this complex environment.","In this context, open datasets can play a crucial role in driving new advancement and facilitating research.","Recognizing the significance, this paper aims to contribute the satellite communication research community by providing various open datasets that incorporate realistic traffic flow enabling a variety of uses cases.","The primary objective of sharing these datasets is to facilitate the development and benchmarking of advanced resource management solutions, thereby improving the overall satellite communication systems.","Furthermore, an application example focused on beam placement optimization via terminal clustering is provided.","This assists in optimizing beam allocation task, enabling adaptive beamforming to effectively meet spatiotemporally varying user traffic demands and optimize resource utilization."],"url":"http://arxiv.org/abs/2404.13920v1","category":"eess.SP"}
{"created":"2024-04-22 06:57:43","title":"Navigating the Path of Writing: Outline-guided Text Generation with Large Language Models","abstract":"Large Language Models (LLMs) have significantly impacted the writing process, enabling collaborative content creation and enhancing productivity. However, generating high-quality, user-aligned text remains challenging. In this paper, we propose Writing Path, a framework that uses explicit outlines to guide LLMs in generating goal-oriented, high-quality pieces of writing. Our approach draws inspiration from structured writing planning and reasoning paths, focusing on capturing and reflecting user intentions throughout the writing process. We construct a diverse dataset from unstructured blog posts to benchmark writing performance and introduce a comprehensive evaluation framework assessing the quality of outlines and generated texts. Our evaluations with GPT-3.5-turbo, GPT-4, and HyperCLOVA X demonstrate that the Writing Path approach significantly enhances text quality according to both LLMs and human evaluations. This study highlights the potential of integrating writing-specific techniques into LLMs to enhance their ability to meet the diverse writing needs of users.","sentences":["Large Language Models (LLMs) have significantly impacted the writing process, enabling collaborative content creation and enhancing productivity.","However, generating high-quality, user-aligned text remains challenging.","In this paper, we propose Writing Path, a framework that uses explicit outlines to guide LLMs in generating goal-oriented, high-quality pieces of writing.","Our approach draws inspiration from structured writing planning and reasoning paths, focusing on capturing and reflecting user intentions throughout the writing process.","We construct a diverse dataset from unstructured blog posts to benchmark writing performance and introduce a comprehensive evaluation framework assessing the quality of outlines and generated texts.","Our evaluations with GPT-3.5-turbo, GPT-4, and HyperCLOVA X demonstrate that the Writing Path approach significantly enhances text quality according to both LLMs and human evaluations.","This study highlights the potential of integrating writing-specific techniques into LLMs to enhance their ability to meet the diverse writing needs of users."],"url":"http://arxiv.org/abs/2404.13919v1","category":"cs.CL"}
{"created":"2024-04-22 17:28:52","title":"The Life and Legacy of Bui Tuong Phong","abstract":"We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year. We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts. Crucially, we present evidence that his name has been cited incorrectly over the last five decades. His family name appears to be B\\`ui, not Phong. By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future.","sentences":["We examine the life and legacy of pioneering Vietnamese American computer scientist B\\`ui Tuong Phong, whose shading and lighting models turned 50 last year.","We trace the trajectory of his life through Vietnam, France, and the United States, and its intersections with global conflicts.","Crucially, we present evidence that his name has been cited incorrectly over the last five decades.","His family name appears to be B\\`ui, not Phong.","By presenting these facts at SIGGRAPH, we hope to collect more information about his life, and ensure that his name is remembered correctly in the future."],"url":"http://arxiv.org/abs/2404.14376v1","category":"cs.GR"}
{"created":"2024-04-22 17:24:08","title":"On the incidence rate of RR Lyrae stars with non-radial modes","abstract":"Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars. These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61. The incidence rate of this phenomenon changes from population to population. It is also strongly dependent on the quality of the analyzed data. Current models explaining these additional signals involve non-radial modes of degrees 8 and 9. Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities. We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models. We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population. To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS. The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity. We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations. Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes.","sentences":["Over the recent years, additional low-amplitude non-radial modes were detected in many of the first-overtone RR Lyrae stars.","These non-radial modes form a characteristic period ratio with the dominant first-overtone mode of around 0.61.","The incidence rate of this phenomenon changes from population to population.","It is also strongly dependent on the quality of the analyzed data.","Current models explaining these additional signals involve non-radial modes of degrees 8 and 9.","Using synthetic horizontal branch populations, we investigate the incidence rate of first-overtone RR Lyrae stars with non-radial modes depending on the population properties, i.e., ages and metallicities.","We compare our results with the observed results for globular clusters and the numerous collection of field first-overtone RR Lyrae stars to test the predictions of the models.","We used synthetic horizontal branches combined with pulsation models to predict how the incidence rate would depend on the age and metallicity of the population.","To test whether the results based on synthetic horizontal branches are realistic, we compared them to incidence rates observed by TESS in first-overtone field RR Lyrae stars, using photometric metallicity values from a newly established calibration for TESS.","The analysis of synthetic horizontal branches showed that the incidence rate decreases with decreasing metallicity.","We inferred photometric metallicity for RR Lyrae stars observed by TESS and showed that the theoretical predictions are in agreement with the observations.","Using the same method, we also conclude that the metallicity distribution of RR Lyrae stars showing an additional mode with a period-ratio around $0.68$ appears to be different from that of both all first-overtone stars and those showing additional non-radial modes."],"url":"http://arxiv.org/abs/2404.14373v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:02:33","title":"Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer","abstract":"We address the task of estimating camera parameters from a set of images depicting a scene. Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud. We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction. This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching. We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images. Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images. Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis. Project page: https://nianticlabs.github.io/acezero/","sentences":["We address the task of estimating camera parameters from a set of images depicting a scene.","Popular feature-based structure-from-motion (SfM) tools solve this task by incremental reconstruction: they repeat triangulation of sparse 3D points and registration of more camera views to the sparse point cloud.","We re-interpret incremental structure-from-motion as an iterated application and refinement of a visual relocalizer, that is, of a method that registers new views to the current state of the reconstruction.","This perspective allows us to investigate alternative visual relocalizers that are not rooted in local feature matching.","We show that scene coordinate regression, a learning-based relocalization approach, allows us to build implicit, neural scene representations from unposed images.","Different from other learning-based reconstruction methods, we do not require pose priors nor sequential inputs, and we optimize efficiently over thousands of images.","Our method, ACE0 (ACE Zero), estimates camera poses to an accuracy comparable to feature-based SfM, as demonstrated by novel view synthesis.","Project page: https://nianticlabs.github.io/acezero/"],"url":"http://arxiv.org/abs/2404.14351v1","category":"cs.CV"}
{"created":"2024-04-22 17:00:31","title":"Machine Learning in Viscoelastic Fluids via Energy-Based Kernel Embedding","abstract":"The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles. In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models. To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy. Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning. Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations. Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers. Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions. Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems.","sentences":["The ability to measure differences in collected data is of fundamental importance for quantitative science and machine learning, motivating the establishment of metrics grounded in physical principles.","In this study, we focus on the development of such metrics for viscoelastic fluid flows governed by a large class of linear and nonlinear stress models.","To do this, we introduce a kernel function corresponding to a given viscoelastic stress model that implicitly embeds flowfield snapshots into a Reproducing Kernel Hilbert Space (RKHS) whose squared norm equals the total mechanical energy.","Working implicitly with lifted representations in the RKHS via the kernel function provides natural and unambiguous metrics for distances and angles between flowfields without the need for hyperparameter tuning.","Additionally, we present a solution to the preimage problem for our kernels, enabling accurate reconstruction of flowfields from their RKHS representations.","Through numerical experiments on an unsteady viscoelastic lid-driven cavity flow, we demonstrate the utility of our kernels for extracting energetically-dominant coherent structures in viscoelastic flows across a range of Reynolds and Weissenberg numbers.","Specifically, the features extracted by Kernel Principal Component Analysis (KPCA) of flowfield snapshots using our kernel functions yield reconstructions with superior accuracy in terms of mechanical energy compared to conventional methods such as ordinary Principal Component Analysis (PCA) with na\\\"ively-defined state vectors or KPCA with ad-hoc choices of kernel functions.","Our findings underscore the importance of principled choices of metrics in both scientific and machine learning investigations of complex fluid systems."],"url":"http://arxiv.org/abs/2404.14347v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 15:29:19","title":"RESFM: Robust Equivariant Multiview Structure from Motion","abstract":"Multiview Structure from Motion is a fundamental and challenging computer vision problem. A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections. This work however made the unrealistic assumption that the point tracks given as input are clean of outliers. Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step. Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers.","sentences":["Multiview Structure from Motion is a fundamental and challenging computer vision problem.","A recent deep-based approach was proposed utilizing matrix equivariant architectures for the simultaneous recovery of camera pose and 3D scene structure from large image collections.","This work however made the unrealistic assumption that the point tracks given as input are clean of outliers.","Here we propose an architecture suited to dealing with outliers by adding an inlier/outlier classifying module that respects the model equivariance and by adding a robust bundle adjustment step.","Experiments demonstrate that our method can be successfully applied in realistic settings that include large image collections and point tracks extracted with common heuristics and include many outliers."],"url":"http://arxiv.org/abs/2404.14280v1","category":"cs.CV"}
{"created":"2024-04-22 15:26:31","title":"Dipolar order controls dielectric response of glass-forming liquids","abstract":"The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations. A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum. Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids. While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory.","sentences":["The dielectric response of liquids reflects both, reorientation of single molecular dipoles and collective modes, i.e., dipolar cross-correlations.","A recent theory predicts the latter to produce an additional slow peak in the dielectric loss spectrum.","Following this idea we argue that in supercooled liquids the high-frequency power law exponent of the dielectric loss $\\beta$ should be correlated with the degree of dipolar order, i.e., the Kirkwood correlation factor $g_K$. This notion is confirmed for 25 supercooled liquids.","While our findings support recent theoretical work the results are shown to violate the earlier Kivelson-Madden theory."],"url":"http://arxiv.org/abs/2404.14277v1","category":"cond-mat.soft"}
{"created":"2024-04-22 13:58:36","title":"Experimental Validation of Ultrasound Beamforming with End-to-End Deep Learning for Single Plane Wave Imaging","abstract":"Ultrafast ultrasound imaging insonifies a medium with one or a combination of a few plane waves at different beam-steered angles instead of many focused waves. It can achieve much higher frame rates, but often at the cost of reduced image quality. Deep learning approaches have been proposed to mitigate this disadvantage, in particular for single plane wave imaging. Predominantly, image-to-image post-processing networks or fully learned data-to-image neural networks are used. Both construct their mapping purely data-driven and require expressive networks and large amounts of training data to perform well. In contrast, we consider data-to-image networks which incorporate a conventional image formation techniques as differentiable layers in the network architecture. This allows for end-to-end training with small amounts of training data. In this work, using f-k migration as an image formation layer is evaluated in-depth with experimental data. We acquired a data collection designed for benchmarking data-driven plane wave imaging approaches using a realistic breast mimicking phantom and an ultrasound calibration phantom. The evaluation considers global and local image similarity measures and contrast, resolution and lesion detectability analysis. The results show that the proposed network architecture is capable of improving the image quality of single plane wave images on all evaluation metrics. Furthermore, these image quality improvements can be achieved with surprisingly little amounts of training data.","sentences":["Ultrafast ultrasound imaging insonifies a medium with one or a combination of a few plane waves at different beam-steered angles instead of many focused waves.","It can achieve much higher frame rates, but often at the cost of reduced image quality.","Deep learning approaches have been proposed to mitigate this disadvantage, in particular for single plane wave imaging.","Predominantly, image-to-image post-processing networks or fully learned data-to-image neural networks are used.","Both construct their mapping purely data-driven and require expressive networks and large amounts of training data to perform well.","In contrast, we consider data-to-image networks which incorporate a conventional image formation techniques as differentiable layers in the network architecture.","This allows for end-to-end training with small amounts of training data.","In this work, using f-k migration as an image formation layer is evaluated in-depth with experimental data.","We acquired a data collection designed for benchmarking data-driven plane wave imaging approaches using a realistic breast mimicking phantom and an ultrasound calibration phantom.","The evaluation considers global and local image similarity measures and contrast, resolution and lesion detectability analysis.","The results show that the proposed network architecture is capable of improving the image quality of single plane wave images on all evaluation metrics.","Furthermore, these image quality improvements can be achieved with surprisingly little amounts of training data."],"url":"http://arxiv.org/abs/2404.14188v1","category":"eess.IV"}
{"created":"2024-04-22 12:25:17","title":"Gaia DR3 detectability of unresolved binary systems","abstract":"Gaia can not individually resolve very close binary systems, however, the collected data can still be used to identify them. A powerful indicator of stellar multiplicity is the sources reported Renormalized Unit Weight Error (ruwe), which effectively captures the astrometric deviations from single-source solutions. We aim to characterise the imprints left on ruwe caused by binarity. By flagging potential binary systems based on ruwe, we aim to characterise which of their properties will contribute the most to their detectability. We develop a model to estimate ruwe values for observations of Gaia sources, based on the biases to the single-source astrometric track arising from the presence of an unseen companion. Then, using the recipes from previous GaiaUnlimited selection functions, we estimate the selection probability of sources with high ruwe, and discuss what binary properties contribute to increasing the sources ruwe. We compute the maximum ruwe value which is compatible with single-source solutions as a function of their location on-sky. We see that binary systems selected as sources with a ruwe higher than this sky-varying threshold have a strong detectability window in their orbital period distribution, which peaks at periods equal to the Gaia observation time baseline. We demonstrate how our sky-varying ruwe threshold provides a more complete sample of binary systems when compared to single sky-averaged values by studying the unresolved binary population in the Gaia Catalogue of Nearby Stars. We provide the code and tools used in this study, as well as the sky-varying ruwe threshold through the GaiaUnlimited Python package","sentences":["Gaia can not individually resolve very close binary systems, however, the collected data can still be used to identify them.","A powerful indicator of stellar multiplicity is the sources reported Renormalized Unit Weight Error (ruwe), which effectively captures the astrometric deviations from single-source solutions.","We aim to characterise the imprints left on ruwe caused by binarity.","By flagging potential binary systems based on ruwe, we aim to characterise which of their properties will contribute the most to their detectability.","We develop a model to estimate ruwe values for observations of Gaia sources, based on the biases to the single-source astrometric track arising from the presence of an unseen companion.","Then, using the recipes from previous GaiaUnlimited selection functions, we estimate the selection probability of sources with high ruwe, and discuss what binary properties contribute to increasing the sources ruwe.","We compute the maximum ruwe value which is compatible with single-source solutions as a function of their location on-sky.","We see that binary systems selected as sources with a ruwe higher than this sky-varying threshold have a strong detectability window in their orbital period distribution, which peaks at periods equal to the Gaia observation time baseline.","We demonstrate how our sky-varying ruwe threshold provides a more complete sample of binary systems when compared to single sky-averaged values by studying the unresolved binary population in the Gaia Catalogue of Nearby Stars.","We provide the code and tools used in this study, as well as the sky-varying ruwe threshold through the GaiaUnlimited Python package"],"url":"http://arxiv.org/abs/2404.14127v1","category":"astro-ph.GA"}
{"created":"2024-04-22 08:12:21","title":"Banded totally positive matrices and normality for mixed multiple orthogonal polynomials","abstract":"This paper serves as an introduction to banded totally positive matrices, exploring various characterizations and associated properties. A significant result within is the demonstration that the collection of such matrices forms a semigroup, notably including a subset permitting positive bidiagonal factorization. Moreover, the paper applies this concept to investigate step line normality concerning the degrees of associated recursion polynomials. It presents a spectral Favard theorem, ensuring the existence of measures, thereby guaranteeing that these recursion polynomials represent mixed multiple orthogonal polynomials that maintain normality on the step line indices.","sentences":["This paper serves as an introduction to banded totally positive matrices, exploring various characterizations and associated properties.","A significant result within is the demonstration that the collection of such matrices forms a semigroup, notably including a subset permitting positive bidiagonal factorization.","Moreover, the paper applies this concept to investigate step line normality concerning the degrees of associated recursion polynomials.","It presents a spectral Favard theorem, ensuring the existence of measures, thereby guaranteeing that these recursion polynomials represent mixed multiple orthogonal polynomials that maintain normality on the step line indices."],"url":"http://arxiv.org/abs/2404.13965v1","category":"math.CA"}
{"created":"2024-04-22 07:32:03","title":"A User-Centric Benchmark for Evaluating Large Language Models","abstract":"Large Language Models (LLMs) are essential tools to collaborate with users on different tasks. Evaluating their performance to serve users' needs in real-world scenarios is important. While many benchmarks have been created, they mainly focus on specific predefined model abilities. Few have covered the intended utilization of LLMs by real users. To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs. We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries. These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents. Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs. Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios. In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs. The benchmark dataset and code are available at https://github.com/Alice1998/URS.","sentences":["Large Language Models (LLMs) are essential tools to collaborate with users on different tasks.","Evaluating their performance to serve users' needs in real-world scenarios is important.","While many benchmarks have been created, they mainly focus on specific predefined model abilities.","Few have covered the intended utilization of LLMs by real users.","To address this oversight, we propose benchmarking LLMs from a user perspective in both dataset construction and evaluation designs.","We first collect 1846 real-world use cases with 15 LLMs from a user study with 712 participants from 23 countries.","These self-reported cases form the User Reported Scenarios(URS) dataset with a categorization of 7 user intents.","Secondly, on this authentic multi-cultural dataset, we benchmark 10 LLM services on their efficacy in satisfying user needs.","Thirdly, we show that our benchmark scores align well with user-reported experience in LLM interactions across diverse intents, both of which emphasize the overlook of subjective scenarios.","In conclusion, our study proposes to benchmark LLMs from a user-centric perspective, aiming to facilitate evaluations that better reflect real user needs.","The benchmark dataset and code are available at https://github.com/Alice1998/URS."],"url":"http://arxiv.org/abs/2404.13940v2","category":"cs.CL"}
{"created":"2024-04-22 07:27:07","title":"Data-Based System Representation and Synchronization for Multiagent Systems","abstract":"This paper presents novel solutions of the data-based synchronization problem for continuous-time multiagent systems. We consider the cases of homogeneous and heterogeneous systems. First, a data-based representation of the synchronization error dynamics is obtained for homogeneous systems, using input-state data collected from the agents. Then, we show how to extend existing data-based stabilization results to the multiagent case to stabilize the obtained synchronization errors. The proposed method relies on the solution of a set of linear matrix inequalities that are shown to be feasible. Then, we solve the synchronization problem for heterogeneous systems by means of dynamic controllers. Different from existing results, we do not require model knowledge for the followers and the leader. The theoretical results are finally validated using numerical simulations.","sentences":["This paper presents novel solutions of the data-based synchronization problem for continuous-time multiagent systems.","We consider the cases of homogeneous and heterogeneous systems.","First, a data-based representation of the synchronization error dynamics is obtained for homogeneous systems, using input-state data collected from the agents.","Then, we show how to extend existing data-based stabilization results to the multiagent case to stabilize the obtained synchronization errors.","The proposed method relies on the solution of a set of linear matrix inequalities that are shown to be feasible.","Then, we solve the synchronization problem for heterogeneous systems by means of dynamic controllers.","Different from existing results, we do not require model knowledge for the followers and the leader.","The theoretical results are finally validated using numerical simulations."],"url":"http://arxiv.org/abs/2404.13937v1","category":"eess.SY"}
{"created":"2024-04-22 06:42:21","title":"Integrated Gradient Correlation: a Dataset-wise Attribution Method","abstract":"Attribution methods are primarily designed to study the distribution of input component contributions to individual model predictions. However, some research applications require a summary of attribution patterns across the entire dataset to facilitate the interpretability of the scrutinized models. In this paper, we present a new method called Integrated Gradient Correlation (IGC) that relates dataset-wise attributions to a model prediction score and enables region-specific analysis by a direct summation over associated components. We demonstrate our method on scalar predictions with the study of image feature representation in the brain from fMRI neural signals and the estimation of neural population receptive fields (NSD dataset), as well as on categorical predictions with the investigation of handwritten digit recognition (MNIST dataset). The resulting IGC attributions show selective patterns, revealing underlying model strategies coherent with their respective objectives.","sentences":["Attribution methods are primarily designed to study the distribution of input component contributions to individual model predictions.","However, some research applications require a summary of attribution patterns across the entire dataset to facilitate the interpretability of the scrutinized models.","In this paper, we present a new method called Integrated Gradient Correlation (IGC) that relates dataset-wise attributions to a model prediction score and enables region-specific analysis by a direct summation over associated components.","We demonstrate our method on scalar predictions with the study of image feature representation in the brain from fMRI neural signals and the estimation of neural population receptive fields (NSD dataset), as well as on categorical predictions with the investigation of handwritten digit recognition (MNIST dataset).","The resulting IGC attributions show selective patterns, revealing underlying model strategies coherent with their respective objectives."],"url":"http://arxiv.org/abs/2404.13910v1","category":"cs.LG"}
{"created":"2024-04-22 06:33:28","title":"Generating Attractive and Authentic Copywriting from Customer Reviews","abstract":"The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it's becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation.","sentences":["The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions.","As e-commerce platforms offer a wide range of services, it's becoming essential to dynamically adjust the styles of these auto-generated descriptions.","Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content.","To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes.","We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information.","Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness.","Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment.","Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction.","The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation."],"url":"http://arxiv.org/abs/2404.13906v1","category":"cs.CL"}
{"created":"2024-04-22 06:18:37","title":"Towards Better Text-to-Image Generation Alignment via Attention Modulation","abstract":"In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results. However, these models encounter challenges when processing text prompts containing multiple entities and attributes. The uneven distribution of attention results in the issues of entity leakage and attribute misalignment. Training from scratch to address this issue requires numerous labeled data and is resource-consuming. Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model. One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps. To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues. An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively. The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost.","sentences":["In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results.","However, these models encounter challenges when processing text prompts containing multiple entities and attributes.","The uneven distribution of attention results in the issues of entity leakage and attribute misalignment.","Training from scratch to address this issue requires numerous labeled data and is resource-consuming.","Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model.","One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps.","To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues.","An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively.","The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost."],"url":"http://arxiv.org/abs/2404.13899v1","category":"cs.CL"}
{"created":"2024-04-22 06:05:35","title":"Optimal Design for Human Feedback","abstract":"Learning of preference models from human feedback has been central to recent advances in artificial intelligence. Motivated by this progress, and the cost of obtaining high-quality human annotations, we study the problem of data collection for learning preference models. The key idea in our work is to generalize optimal designs, a tool for computing efficient data logging policies, to ranked lists. To show the generality of our ideas, we study both absolute and relative feedback on items in the list. We design efficient algorithms for both settings and analyze them. We prove that our preference model estimators improve with more data and so does the ranking error under the estimators. Finally, we experiment with several synthetic and real-world datasets to show the statistical efficiency of our algorithms.","sentences":["Learning of preference models from human feedback has been central to recent advances in artificial intelligence.","Motivated by this progress, and the cost of obtaining high-quality human annotations, we study the problem of data collection for learning preference models.","The key idea in our work is to generalize optimal designs, a tool for computing efficient data logging policies, to ranked lists.","To show the generality of our ideas, we study both absolute and relative feedback on items in the list.","We design efficient algorithms for both settings and analyze them.","We prove that our preference model estimators improve with more data and so does the ranking error under the estimators.","Finally, we experiment with several synthetic and real-world datasets to show the statistical efficiency of our algorithms."],"url":"http://arxiv.org/abs/2404.13895v1","category":"cs.LG"}
{"created":"2024-04-22 05:46:40","title":"Retrieval-Augmented Audio Deepfake Detection","abstract":"With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.","sentences":["With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse.","However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues.","Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection.","We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework.","Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets.","Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance."],"url":"http://arxiv.org/abs/2404.13892v2","category":"cs.SD"}
{"created":"2024-04-22 05:37:22","title":"Minimizing Weighted Counterfactual Regret with Optimistic Online Mirror Descent","abstract":"Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. It decomposes the total regret into counterfactual regrets, utilizing local regret minimization algorithms, such as Regret Matching (RM) or RM+, to minimize them. Recent research establishes a connection between Online Mirror Descent (OMD) and RM+, paving the way for an optimistic variant PRM+ and its extension PCFR+. However, PCFR+ assigns uniform weights for each iteration when determining regrets, leading to substantial regrets when facing dominated actions. This work explores minimizing weighted counterfactual regret with optimistic OMD, resulting in a novel CFR variant PDCFR+. It integrates PCFR+ and Discounted CFR (DCFR) in a principled manner, swiftly mitigating negative effects of dominated actions and consistently leveraging predictions to accelerate convergence. Theoretical analyses prove that PDCFR+ converges to a Nash equilibrium, particularly under distinct weighting schemes for regrets and average strategies. Experimental results demonstrate PDCFR+'s fast convergence in common imperfect-information games. The code is available at https://github.com/rpSebastian/PDCFRPlus.","sentences":["Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games.","It decomposes the total regret into counterfactual regrets, utilizing local regret minimization algorithms, such as Regret Matching (RM) or RM+, to minimize them.","Recent research establishes a connection between Online Mirror Descent (OMD) and RM+, paving the way for an optimistic variant PRM+ and its extension PCFR+.","However, PCFR+ assigns uniform weights for each iteration when determining regrets, leading to substantial regrets when facing dominated actions.","This work explores minimizing weighted counterfactual regret with optimistic OMD, resulting in a novel CFR variant PDCFR+.","It integrates PCFR+ and Discounted CFR (DCFR) in a principled manner, swiftly mitigating negative effects of dominated actions and consistently leveraging predictions to accelerate convergence.","Theoretical analyses prove that PDCFR+ converges to a Nash equilibrium, particularly under distinct weighting schemes for regrets and average strategies.","Experimental results demonstrate PDCFR+'s fast convergence in common imperfect-information games.","The code is available at https://github.com/rpSebastian/PDCFRPlus."],"url":"http://arxiv.org/abs/2404.13891v1","category":"cs.LG"}
{"created":"2024-04-22 05:12:52","title":"Surveying Attitudinal Alignment Between Large Language Models Vs. Humans Towards 17 Sustainable Development Goals","abstract":"Large Language Models (LLMs) have emerged as potent tools for advancing the United Nations' Sustainable Development Goals (SDGs). However, the attitudinal disparities between LLMs and humans towards these goals can pose significant challenges. This study conducts a comprehensive review and analysis of the existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing the comparison between their attitudes and support for each goal and those of humans. We examine the potential disparities, primarily focusing on aspects such as understanding and emotions, cultural and regional differences, task objective variations, and factors considered in the decision-making process. These disparities arise from the underrepresentation and imbalance in LLM training data, historical biases, quality issues, lack of contextual understanding, and skewed ethical values reflected. The study also investigates the risks and harms that may arise from neglecting the attitudes of LLMs towards the SDGs, including the exacerbation of social inequalities, racial discrimination, environmental destruction, and resource wastage. To address these challenges, we propose strategies and recommendations to guide and regulate the application of LLMs, ensuring their alignment with the principles and goals of the SDGs, and therefore creating a more just, inclusive, and sustainable future.","sentences":["Large Language Models (LLMs) have emerged as potent tools for advancing the United Nations' Sustainable Development Goals (SDGs).","However, the attitudinal disparities between LLMs and humans towards these goals can pose significant challenges.","This study conducts a comprehensive review and analysis of the existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing the comparison between their attitudes and support for each goal and those of humans.","We examine the potential disparities, primarily focusing on aspects such as understanding and emotions, cultural and regional differences, task objective variations, and factors considered in the decision-making process.","These disparities arise from the underrepresentation and imbalance in LLM training data, historical biases, quality issues, lack of contextual understanding, and skewed ethical values reflected.","The study also investigates the risks and harms that may arise from neglecting the attitudes of LLMs towards the SDGs, including the exacerbation of social inequalities, racial discrimination, environmental destruction, and resource wastage.","To address these challenges, we propose strategies and recommendations to guide and regulate the application of LLMs, ensuring their alignment with the principles and goals of the SDGs, and therefore creating a more just, inclusive, and sustainable future."],"url":"http://arxiv.org/abs/2404.13885v1","category":"cs.CY"}
{"created":"2024-04-22 04:50:22","title":"Active RIS-Aided Massive MIMO Uplink Systems with Low-Resolution ADCs","abstract":"This letter considers an active reconfigurable intelligent surface (RIS)-aided multi-user uplink massive multipleinput multiple-output (MIMO) system with low-resolution analog-to-digital converters (ADCs). The letter derives the closedform approximate expression for the sum achievable rate (AR), where the maximum ratio combination (MRC) processing and low-resolution ADCs are applied at the base station. The system performance is analyzed, and a genetic algorithm (GA)-based method is proposed to optimize the RIS's phase shifts for enhancing the system performance. Numerical results verify the accuracy of the derivations, and demonstrate that the active RIS has an evident performance gain over the passive RIS.","sentences":["This letter considers an active reconfigurable intelligent surface (RIS)-aided multi-user uplink massive multipleinput multiple-output (MIMO) system with low-resolution analog-to-digital converters (ADCs).","The letter derives the closedform approximate expression for the sum achievable rate (AR), where the maximum ratio combination (MRC) processing and low-resolution ADCs are applied at the base station.","The system performance is analyzed, and a genetic algorithm (GA)-based method is proposed to optimize the RIS's phase shifts for enhancing the system performance.","Numerical results verify the accuracy of the derivations, and demonstrate that the active RIS has an evident performance gain over the passive RIS."],"url":"http://arxiv.org/abs/2404.13875v1","category":"cs.IT"}
{"created":"2024-04-22 04:33:40","title":"TeamTrack: A Dataset for Multi-Sport Multi-Object Tracking in Full-pitch Videos","abstract":"Multi-object tracking (MOT) is a critical and challenging task in computer vision, particularly in situations involving objects with similar appearances but diverse movements, as seen in team sports. Current methods, largely reliant on object detection and appearance, often fail to track targets in such complex scenarios accurately. This limitation is further exacerbated by the lack of comprehensive and diverse datasets covering the full view of sports pitches. Addressing these issues, we introduce TeamTrack, a pioneering benchmark dataset specifically designed for MOT in sports. TeamTrack is an extensive collection of full-pitch video data from various sports, including soccer, basketball, and handball. Furthermore, we perform a comprehensive analysis and benchmarking effort to underscore TeamTrack's utility and potential impact. Our work signifies a crucial step forward, promising to elevate the precision and effectiveness of MOT in complex, dynamic settings such as team sports. The dataset, project code and competition is released at: https://atomscott.github.io/TeamTrack/.","sentences":["Multi-object tracking (MOT) is a critical and challenging task in computer vision, particularly in situations involving objects with similar appearances but diverse movements, as seen in team sports.","Current methods, largely reliant on object detection and appearance, often fail to track targets in such complex scenarios accurately.","This limitation is further exacerbated by the lack of comprehensive and diverse datasets covering the full view of sports pitches.","Addressing these issues, we introduce TeamTrack, a pioneering benchmark dataset specifically designed for MOT in sports.","TeamTrack is an extensive collection of full-pitch video data from various sports, including soccer, basketball, and handball.","Furthermore, we perform a comprehensive analysis and benchmarking effort to underscore TeamTrack's utility and potential impact.","Our work signifies a crucial step forward, promising to elevate the precision and effectiveness of MOT in complex, dynamic settings such as team sports.","The dataset, project code and competition is released at: https://atomscott.github.io/TeamTrack/."],"url":"http://arxiv.org/abs/2404.13868v1","category":"cs.CV"}
{"created":"2024-04-22 04:30:36","title":"Context-Enhanced Language Models for Generating Multi-Paper Citations","abstract":"Citation text plays a pivotal role in elucidating the connection between scientific documents, demanding an in-depth comprehension of the cited paper. Constructing citations is often time-consuming, requiring researchers to delve into extensive literature and grapple with articulating relevant content. To address this challenge, the field of citation text generation (CTG) has emerged. However, while earlier methods have primarily centered on creating single-sentence citations, practical scenarios frequently necessitate citing multiple papers within a single paragraph. To bridge this gap, we propose a method that leverages Large Language Models (LLMs) to generate multi-citation sentences. Our approach involves a single source paper and a collection of target papers, culminating in a coherent paragraph containing multi-sentence citation text. Furthermore, we introduce a curated dataset named MCG-S2ORC, composed of English-language academic research papers in Computer Science, showcasing multiple citation instances. In our experiments, we evaluate three LLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this endeavor. Additionally, we exhibit enhanced performance by integrating knowledge graphs from target papers into the prompts for generating citation text. This research underscores the potential of harnessing LLMs for citation generation, opening a compelling avenue for exploring the intricate connections between scientific documents.","sentences":["Citation text plays a pivotal role in elucidating the connection between scientific documents, demanding an in-depth comprehension of the cited paper.","Constructing citations is often time-consuming, requiring researchers to delve into extensive literature and grapple with articulating relevant content.","To address this challenge, the field of citation text generation (CTG) has emerged.","However, while earlier methods have primarily centered on creating single-sentence citations, practical scenarios frequently necessitate citing multiple papers within a single paragraph.","To bridge this gap, we propose a method that leverages Large Language Models (LLMs) to generate multi-citation sentences.","Our approach involves a single source paper and a collection of target papers, culminating in a coherent paragraph containing multi-sentence citation text.","Furthermore, we introduce a curated dataset named MCG-S2ORC, composed of English-language academic research papers in Computer Science, showcasing multiple citation instances.","In our experiments, we evaluate three LLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this endeavor.","Additionally, we exhibit enhanced performance by integrating knowledge graphs from target papers into the prompts for generating citation text.","This research underscores the potential of harnessing LLMs for citation generation, opening a compelling avenue for exploring the intricate connections between scientific documents."],"url":"http://arxiv.org/abs/2404.13865v1","category":"cs.CL"}
{"created":"2024-04-22 04:16:40","title":"Unveiling and Mitigating Generalized Biases of DNNs through the Intrinsic Dimensions of Perceptual Manifolds","abstract":"Building fair deep neural networks (DNNs) is a crucial step towards achieving trustworthy artificial intelligence. Delving into deeper factors that affect the fairness of DNNs is paramount and serves as the foundation for mitigating model biases. However, current methods are limited in accurately predicting DNN biases, relying solely on the number of training samples and lacking more precise measurement tools. Here, we establish a geometric perspective for analyzing the fairness of DNNs, comprehensively exploring how DNNs internally shape the intrinsic geometric characteristics of datasets-the intrinsic dimensions (IDs) of perceptual manifolds, and the impact of IDs on the fairness of DNNs. Based on multiple findings, we propose Intrinsic Dimension Regularization (IDR), which enhances the fairness and performance of models by promoting the learning of concise and ID-balanced class perceptual manifolds. In various image recognition benchmark tests, IDR significantly mitigates model bias while improving its performance.","sentences":["Building fair deep neural networks (DNNs) is a crucial step towards achieving trustworthy artificial intelligence.","Delving into deeper factors that affect the fairness of DNNs is paramount and serves as the foundation for mitigating model biases.","However, current methods are limited in accurately predicting DNN biases, relying solely on the number of training samples and lacking more precise measurement tools.","Here, we establish a geometric perspective for analyzing the fairness of DNNs, comprehensively exploring how DNNs internally shape the intrinsic geometric characteristics of datasets-the intrinsic dimensions (IDs) of perceptual manifolds, and the impact of IDs on the fairness of DNNs.","Based on multiple findings, we propose Intrinsic Dimension Regularization (IDR), which enhances the fairness and performance of models by promoting the learning of concise and ID-balanced class perceptual manifolds.","In various image recognition benchmark tests, IDR significantly mitigates model bias while improving its performance."],"url":"http://arxiv.org/abs/2404.13859v1","category":"cs.CV"}
{"created":"2024-04-22 03:35:19","title":"ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for Traffic Speed Prediction","abstract":"Traffic speed prediction is significant for intelligent navigation and congestion alleviation. However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush. Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET). Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules. First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair. The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives. Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions. For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs. Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations. Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios.","sentences":["Traffic speed prediction is significant for intelligent navigation and congestion alleviation.","However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush.","Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET).","Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules.","First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair.","The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives.","Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions.","For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs.","Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations.","Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios."],"url":"http://arxiv.org/abs/2404.13853v1","category":"cs.LG"}
{"created":"2024-04-22 03:05:19","title":"Filtered Direct Preference Optimization","abstract":"Reinforcement learning from human feedback (RLHF) plays a crucial role in aligning language models with human preferences. While the significance of dataset quality is generally recognized, explicit investigations into its impact within the RLHF framework, to our knowledge, have been limited. This paper addresses the issue of text quality within the preference dataset by focusing on Direct Preference Optimization (DPO), an increasingly adopted reward-model-free RLHF method. We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF. Building on this new insight, we propose an extension of DPO, termed filtered direct preference optimization (fDPO). fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training. Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized, resulting in a more accurate dataset. Experimental results demonstrate that fDPO enhances the final model performance. Our code is available at https://github.com/CyberAgentAILab/filtered-dpo.","sentences":["Reinforcement learning from human feedback (RLHF) plays a crucial role in aligning language models with human preferences.","While the significance of dataset quality is generally recognized, explicit investigations into its impact within the RLHF framework, to our knowledge, have been limited.","This paper addresses the issue of text quality within the preference dataset by focusing on Direct Preference Optimization (DPO), an increasingly adopted reward-model-free RLHF method.","We confirm that text quality significantly influences the performance of models optimized with DPO more than those optimized with reward-model-based RLHF.","Building on this new insight, we propose an extension of DPO, termed filtered direct preference optimization (fDPO).","fDPO uses a trained reward model to monitor the quality of texts within the preference dataset during DPO training.","Samples of lower quality are discarded based on comparisons with texts generated by the model being optimized, resulting in a more accurate dataset.","Experimental results demonstrate that fDPO enhances the final model performance.","Our code is available at https://github.com/CyberAgentAILab/filtered-dpo."],"url":"http://arxiv.org/abs/2404.13846v1","category":"cs.LG"}
{"created":"2024-04-22 02:52:54","title":"ColA: Collaborative Adaptation with Gradient Learning","abstract":"A primary function of back-propagation is to compute both the gradient of hidden representations and parameters for optimization with gradient descent. Training large models requires high computational costs due to their vast parameter sizes. While Parameter-Efficient Fine-Tuning (PEFT) methods aim to train smaller auxiliary models to save computational space, they still present computational overheads, especially in Fine-Tuning as a Service (FTaaS) for numerous users. We introduce Collaborative Adaptation (ColA) with Gradient Learning (GL), a parameter-free, model-agnostic fine-tuning approach that decouples the computation of the gradient of hidden representations and parameters. In comparison to PEFT methods, ColA facilitates more cost-effective FTaaS by offloading the computation of the gradient to low-cost devices. We also provide a theoretical analysis of ColA and experimentally demonstrate that ColA can perform on par or better than existing PEFT methods on various benchmarks.","sentences":["A primary function of back-propagation is to compute both the gradient of hidden representations and parameters for optimization with gradient descent.","Training large models requires high computational costs due to their vast parameter sizes.","While Parameter-Efficient Fine-Tuning (PEFT) methods aim to train smaller auxiliary models to save computational space, they still present computational overheads, especially in Fine-Tuning as a Service (FTaaS) for numerous users.","We introduce Collaborative Adaptation (ColA) with Gradient Learning (GL), a parameter-free, model-agnostic fine-tuning approach that decouples the computation of the gradient of hidden representations and parameters.","In comparison to PEFT methods, ColA facilitates more cost-effective FTaaS by offloading the computation of the gradient to low-cost devices.","We also provide a theoretical analysis of ColA and experimentally demonstrate that ColA can perform on par or better than existing PEFT methods on various benchmarks."],"url":"http://arxiv.org/abs/2404.13844v1","category":"cs.LG"}
{"created":"2024-04-22 02:41:10","title":"Fair Concurrent Training of Multiple Models in Federated Learning","abstract":"Federated learning (FL) enables collaborative learning across multiple clients. In most FL work, all clients train a single learning task. However, the recent proliferation of FL applications may increasingly require multiple FL tasks to be trained simultaneously, sharing clients' computing and communication resources, which we call Multiple-Model Federated Learning (MMFL). Current MMFL algorithms use naive average-based client-task allocation schemes that can lead to unfair performance when FL tasks have heterogeneous difficulty levels, e.g., tasks with larger models may need more rounds and data to train. Just as naively allocating resources to generic computing jobs with heterogeneous resource needs can lead to unfair outcomes, naive allocation of clients to FL tasks can lead to unfairness, with some tasks having excessively long training times, or lower converged accuracies. Furthermore, in the FL setting, since clients are typically not paid for their training effort, we face a further challenge that some clients may not even be willing to train some tasks, e.g., due to high computational costs, which may exacerbate unfairness in training outcomes across tasks. We address both challenges by firstly designing FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks in each training round. We provide guarantees on airness and FedFairMMFL's convergence rate. We then propose a novel auction design that incentivizes clients to train multiple tasks, so as to fairly distribute clients' training efforts across the tasks. We show how our fairness-based learning and incentive mechanisms impact training convergence and finally evaluate our algorithm with multiple sets of learning tasks on real world datasets.","sentences":["Federated learning (FL) enables collaborative learning across multiple clients.","In most FL work, all clients train a single learning task.","However, the recent proliferation of FL applications may increasingly require multiple FL tasks to be trained simultaneously, sharing clients' computing and communication resources, which we call Multiple-Model Federated Learning (MMFL).","Current MMFL algorithms use naive average-based client-task allocation schemes that can lead to unfair performance when FL tasks have heterogeneous difficulty levels, e.g., tasks with larger models may need more rounds and data to train.","Just as naively allocating resources to generic computing jobs with heterogeneous resource needs can lead to unfair outcomes, naive allocation of clients to FL tasks can lead to unfairness, with some tasks having excessively long training times, or lower converged accuracies.","Furthermore, in the FL setting, since clients are typically not paid for their training effort, we face a further challenge that some clients may not even be willing to train some tasks, e.g., due to high computational costs, which may exacerbate unfairness in training outcomes across tasks.","We address both challenges by firstly designing FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks in each training round.","We provide guarantees on airness and FedFairMMFL's convergence rate.","We then propose a novel auction design that incentivizes clients to train multiple tasks, so as to fairly distribute clients' training efforts across the tasks.","We show how our fairness-based learning and incentive mechanisms impact training convergence and finally evaluate our algorithm with multiple sets of learning tasks on real world datasets."],"url":"http://arxiv.org/abs/2404.13841v1","category":"cs.LG"}
{"created":"2024-04-22 02:40:20","title":"Study of $e^+e^-\\to\u03c9X(3872)$ and $\u03b3X(3872)$ from 4.66 to 4.95 GeV","abstract":"Using data samples with an integrated luminosity of $4.5~\\text{fb}^{-1}$ collected by the BESIII detector at center-of-mass energies ranging from 4.66 to 4.95 GeV, we study the processes of $e^+e^-\\to\\omega X(3872)$ and $e^+e^-\\to\\gamma X(3872)$. With the $e^+e^-\\to\\omega X(3872)$ process, the branching fraction ratio $R\\equiv\\frac{\\mathcal{B}(X(3872)\\to\\gamma J/\\psi)}{\\mathcal{B}(X(3872)\\to\\pi^+\\pi^- J/\\psi)}$ is measured to be $0.38\\pm0.20_\\text{stat.}\\pm0.01_\\text{syst.}$ ($R< 0.83$ at 90\\% confidence level). In addition, we measure the ratio of the average cross section of $e^+e^-\\to\\omega X(3872)$ to $e^+e^-\\to\\omega \\chi_{c1}(\\omega\\chi_{c2})$ to be $\\sigma_{\\omega X(3872)}/\\sigma_{\\omega\\chi_{c1}}~(\\sigma_{\\omega X(3872)}/\\sigma_{\\omega\\chi_{c2}})=5.2\\pm1.0_\\text{stat.}\\pm1.9_\\text{syst.}~ (5.5\\pm1.1_\\text{stat.}\\pm2.4_\\text{syst.})$. Finally, we search for the process of $e^+e^-\\to\\gamma X(3872)$, and no obvious signal is observed. The upper limit on the ratio of the average cross section of $e^+e^-\\to\\gamma X(3872)$ to $e^+e^-\\to\\omega X(3872)$ is set as $\\sigma_{\\gamma X(3872)}/\\sigma_{\\omega X(3872)}<0.23$ at 90\\% confidence level.","sentences":["Using data samples with an integrated luminosity of $4.5~\\text{fb}^{-1}$ collected by the BESIII detector at center-of-mass energies ranging from 4.66 to 4.95 GeV, we study the processes of $e^+e^-\\to\\omega X(3872)$ and $e^+e^-\\to\\gamma X(3872)$. With the $e^+e^-\\to\\omega X(3872)$ process, the branching fraction ratio $R\\equiv\\frac{\\mathcal{B}(X(3872)\\to\\gamma J/\\psi)}{\\mathcal{B}(X(3872)\\to\\pi^+\\pi^- J/\\psi)}$ is measured to be $0.38\\pm0.20_\\text{stat.}\\pm0.01_\\text{syst.}$ ($R< 0.83$ at 90\\% confidence level).","In addition, we measure the ratio of the average cross section of $e^+e^-\\to\\omega X(3872)$ to $e^+e^-\\to\\omega \\chi_{c1}(\\omega\\chi_{c2})$ to be $\\sigma_{\\omega X(3872)}/\\sigma_{\\omega\\chi_{c1}}~(\\sigma_{\\omega X(3872)}/\\sigma_{\\omega\\chi_{c2}})=5.2\\pm1.0_\\text{stat.}\\pm1.9_\\text{syst.}~ (5.5\\pm1.1_\\text{stat.}\\pm2.4_\\text{syst.})$. Finally, we search for the process of $e^+e^-\\to\\gamma X(3872)$, and no obvious signal is observed.","The upper limit on the ratio of the average cross section of $e^+e^-\\to\\gamma X(3872)$ to $e^+e^-\\to\\omega X(3872)$ is set as $\\sigma_{\\gamma X(3872)}/\\sigma_{\\omega X(3872)}<0.23$ at 90\\% confidence level."],"url":"http://arxiv.org/abs/2404.13840v1","category":"hep-ex"}
{"created":"2024-04-22 02:05:15","title":"A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep Learning","abstract":"Point cloud registration (PCR) involves determining a rigid transformation that aligns one point cloud to another. Despite the plethora of outstanding deep learning (DL)-based registration methods proposed, comprehensive and systematic studies on DL-based PCR techniques are still lacking. In this paper, we present a comprehensive survey and taxonomy of recently proposed PCR methods. Firstly, we conduct a taxonomy of commonly utilized datasets and evaluation metrics. Secondly, we classify the existing research into two main categories: supervised and unsupervised registration, providing insights into the core concepts of various influential PCR models. Finally, we highlight open challenges and potential directions for future research. A curated collection of valuable resources is made available at https://github.com/yxzhang15/PCR.","sentences":["Point cloud registration (PCR) involves determining a rigid transformation that aligns one point cloud to another.","Despite the plethora of outstanding deep learning (DL)-based registration methods proposed, comprehensive and systematic studies on DL-based PCR techniques are still lacking.","In this paper, we present a comprehensive survey and taxonomy of recently proposed PCR methods.","Firstly, we conduct a taxonomy of commonly utilized datasets and evaluation metrics.","Secondly, we classify the existing research into two main categories: supervised and unsupervised registration, providing insights into the core concepts of various influential PCR models.","Finally, we highlight open challenges and potential directions for future research.","A curated collection of valuable resources is made available at https://github.com/yxzhang15/PCR."],"url":"http://arxiv.org/abs/2404.13830v1","category":"cs.CV"}
{"created":"2024-04-22 01:53:25","title":"Higher-Order Graphon Theory: Fluctuations, Degeneracies, and Inference","abstract":"Exchangeable random graphs, which include some of the most widely studied network models, have emerged as the mainstay of statistical network analysis in recent years. Graphons, which are the central objects in graph limit theory, provide a natural way to sample exchangeable random graphs. It is well known that network moments (motif/subgraph counts) identify a graphon (up to an isomorphism), hence, understanding the sampling distribution of subgraph counts in random graphs sampled from a graphon is pivotal for nonparametric network inference. In this paper, we derive the joint asymptotic distribution of any finite collection of network moments in random graphs sampled from a graphon, that includes both the non-degenerate case (where the distribution is Gaussian) as well as the degenerate case (where the distribution has both Gaussian or non-Gaussian components). This provides the higher-order fluctuation theory for subgraph counts in the graphon model. We also develop a novel multiplier bootstrap for graphons that consistently approximates the limiting distribution of the network moments (both in the Gaussian and non-Gaussian regimes). Using this and a procedure for testing degeneracy, we construct joint confidence sets for any finite collection of motif densities. This provides a general framework for statistical inference based on network moments in the graphon model. To illustrate the broad scope of our results we also consider the problem of detecting global structure (that is, testing whether the graphon is a constant function) based on small subgraphs. We propose a consistent test for this problem, invoking celebrated results on quasi-random graphs, and derive its limiting distribution both under the null and the alternative.","sentences":["Exchangeable random graphs, which include some of the most widely studied network models, have emerged as the mainstay of statistical network analysis in recent years.","Graphons, which are the central objects in graph limit theory, provide a natural way to sample exchangeable random graphs.","It is well known that network moments (motif/subgraph counts) identify a graphon (up to an isomorphism), hence, understanding the sampling distribution of subgraph counts in random graphs sampled from a graphon is pivotal for nonparametric network inference.","In this paper, we derive the joint asymptotic distribution of any finite collection of network moments in random graphs sampled from a graphon, that includes both the non-degenerate case (where the distribution is Gaussian) as well as the degenerate case (where the distribution has both Gaussian or non-Gaussian components).","This provides the higher-order fluctuation theory for subgraph counts in the graphon model.","We also develop a novel multiplier bootstrap for graphons that consistently approximates the limiting distribution of the network moments (both in the Gaussian and non-Gaussian regimes).","Using this and a procedure for testing degeneracy, we construct joint confidence sets for any finite collection of motif densities.","This provides a general framework for statistical inference based on network moments in the graphon model.","To illustrate the broad scope of our results we also consider the problem of detecting global structure (that is, testing whether the graphon is a constant function) based on small subgraphs.","We propose a consistent test for this problem, invoking celebrated results on quasi-random graphs, and derive its limiting distribution both under the null and the alternative."],"url":"http://arxiv.org/abs/2404.13822v1","category":"math.ST"}
{"created":"2024-04-22 01:22:49","title":"Discovering Quirks through Timing at FASER and Future Forward Experiments at the LHC","abstract":"Quirks are generic predictions of strongly-coupled dark sectors. For weak-scale masses and a broad range of confining scales in the dark sector, quirks can be discovered only at the energy frontier, but quirk--anti-quirk pairs are produced with unusual signatures at low $p_T$, making them difficult to detect at the large LHC detectors. We determine the prospects for discovering quirks using timing information at FASER, FASER2, and an \"ultimate detector'' in the far-forward region at the LHC. NLO QCD corrections are incorporated in the simulation of quirk production, which can significantly increase the production rate. To accurately propagate quirk pairs from the ATLAS interaction point to the forward detectors, the ionization energy loss of charged quirks traveling through matter, the radiation of infracolor glueballs and QCD hadrons during quirk pair oscillations, and the annihilation of quirkonium are properly considered. The quirk signal is separated from the large muon background using timing information from scintillator detectors by requiring either two coincident delayed tracks, based on arrival times at the detector, or two coincident slow tracks, based on time differences between hits in the front and back scintillators. We find that simple cuts preserve much of the signal, but reduce the muon background to negligible levels. With the data already collected, FASER can discover quirks in currently unconstrained parameter space. FASER2, running at the Forward Physics Facility during the HL-LHC era, will greatly extend this reach, probing the TeV-scale quirk masses motivated by the gauge hierarchy problem for the broad range of dark-sector confining scales between 100 eV and 100 keV.","sentences":["Quirks are generic predictions of strongly-coupled dark sectors.","For weak-scale masses and a broad range of confining scales in the dark sector, quirks can be discovered only at the energy frontier, but quirk--anti-quirk pairs are produced with unusual signatures at low $p_T$, making them difficult to detect at the large LHC detectors.","We determine the prospects for discovering quirks using timing information at FASER, FASER2, and an \"ultimate detector'' in the far-forward region at the LHC.","NLO QCD corrections are incorporated in the simulation of quirk production, which can significantly increase the production rate.","To accurately propagate quirk pairs from the ATLAS interaction point to the forward detectors, the ionization energy loss of charged quirks traveling through matter, the radiation of infracolor glueballs and QCD hadrons during quirk pair oscillations, and the annihilation of quirkonium are properly considered.","The quirk signal is separated from the large muon background using timing information from scintillator detectors by requiring either two coincident delayed tracks, based on arrival times at the detector, or two coincident slow tracks, based on time differences between hits in the front and back scintillators.","We find that simple cuts preserve much of the signal, but reduce the muon background to negligible levels.","With the data already collected, FASER can discover quirks in currently unconstrained parameter space.","FASER2, running at the Forward Physics Facility during the HL-LHC era, will greatly extend this reach, probing the TeV-scale quirk masses motivated by the gauge hierarchy problem for the broad range of dark-sector confining scales between 100 eV and 100 keV."],"url":"http://arxiv.org/abs/2404.13814v1","category":"hep-ph"}
{"created":"2024-04-22 01:22:23","title":"From LLM to NMT: Advancing Low-Resource Machine Translation with Claude","abstract":"We show that Claude 3 Opus, a large language model (LLM) released by Anthropic in March 2024, exhibits stronger machine translation competence than other LLMs. Though we find evidence of data contamination with Claude on FLORES-200, we curate new benchmarks that corroborate the effectiveness of Claude for low-resource machine translation into English. We find that Claude has remarkable \\textit{resource efficiency} -- the degree to which the quality of the translation model depends on a language pair's resource level. Finally, we show that advancements in LLM translation can be compressed into traditional neural machine translation (NMT) models. Using Claude to generate synthetic data, we demonstrate that knowledge distillation advances the state-of-the-art in Yoruba-English translation, meeting or surpassing strong baselines like NLLB-54B and Google Translate.","sentences":["We show that Claude 3 Opus, a large language model (LLM) released by Anthropic in March 2024, exhibits stronger machine translation competence than other LLMs.","Though we find evidence of data contamination with Claude on FLORES-200, we curate new benchmarks that corroborate the effectiveness of Claude for low-resource machine translation into English.","We find that Claude has remarkable \\textit{resource efficiency} -- the degree to which the quality of the translation model depends on a language pair's resource level.","Finally, we show that advancements in LLM translation can be compressed into traditional neural machine translation (NMT) models.","Using Claude to generate synthetic data, we demonstrate that knowledge distillation advances the state-of-the-art in Yoruba-English translation, meeting or surpassing strong baselines like NLLB-54B and Google Translate."],"url":"http://arxiv.org/abs/2404.13813v1","category":"cs.CL"}
{"created":"2024-04-22 01:16:11","title":"A Comparative Study on Enhancing Prediction in Social Network Advertisement through Data Augmentation","abstract":"In the ever-evolving landscape of social network advertising, the volume and accuracy of data play a critical role in the performance of predictive models. However, the development of robust predictive algorithms is often hampered by the limited size and potential bias present in real-world datasets. This study presents and explores a generative augmentation framework of social network advertising data. Our framework explores three generative models for data augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and diversity in the context of social network advertising analytics effectiveness. By performing synthetic extensions of the feature space, we find that through data augmentation, the performance of various classifiers has been quantitatively improved. Furthermore, we compare the relative performance gains brought by each data augmentation technique, providing insights for practitioners to select appropriate techniques to enhance model performance. This paper contributes to the literature by showing that synthetic data augmentation alleviates the limitations imposed by small or imbalanced datasets in the field of social network advertising. At the same time, this article also provides a comparative perspective on the practicality of different data augmentation methods, thereby guiding practitioners to choose appropriate techniques to enhance model performance.","sentences":["In the ever-evolving landscape of social network advertising, the volume and accuracy of data play a critical role in the performance of predictive models.","However, the development of robust predictive algorithms is often hampered by the limited size and potential bias present in real-world datasets.","This study presents and explores a generative augmentation framework of social network advertising data.","Our framework explores three generative models for data augmentation - Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability and diversity in the context of social network advertising analytics effectiveness.","By performing synthetic extensions of the feature space, we find that through data augmentation, the performance of various classifiers has been quantitatively improved.","Furthermore, we compare the relative performance gains brought by each data augmentation technique, providing insights for practitioners to select appropriate techniques to enhance model performance.","This paper contributes to the literature by showing that synthetic data augmentation alleviates the limitations imposed by small or imbalanced datasets in the field of social network advertising.","At the same time, this article also provides a comparative perspective on the practicality of different data augmentation methods, thereby guiding practitioners to choose appropriate techniques to enhance model performance."],"url":"http://arxiv.org/abs/2404.13812v1","category":"cs.SI"}
{"created":"2024-04-22 00:48:56","title":"General Item Representation Learning for Cold-start Content Recommendations","abstract":"Cold-start item recommendation is a long-standing challenge in recommendation systems. A common remedy is to use a content-based approach, but rich information from raw contents in various forms has not been fully utilized. In this paper, we propose a domain/data-agnostic item representation learning framework for cold-start recommendations, naturally equipped with multimodal alignment among various features by adopting a Transformer-based architecture. Our proposed model is end-to-end trainable completely free from classification labels, not just costly to collect but suboptimal for recommendation-purpose representation learning. From extensive experiments on real-world movie and news recommendation benchmarks, we verify that our approach better preserves fine-grained user taste than state-of-the-art baselines, universally applicable to multiple domains at large scale.","sentences":["Cold-start item recommendation is a long-standing challenge in recommendation systems.","A common remedy is to use a content-based approach, but rich information from raw contents in various forms has not been fully utilized.","In this paper, we propose a domain/data-agnostic item representation learning framework for cold-start recommendations, naturally equipped with multimodal alignment among various features by adopting a Transformer-based architecture.","Our proposed model is end-to-end trainable completely free from classification labels, not just costly to collect but suboptimal for recommendation-purpose representation learning.","From extensive experiments on real-world movie and news recommendation benchmarks, we verify that our approach better preserves fine-grained user taste than state-of-the-art baselines, universally applicable to multiple domains at large scale."],"url":"http://arxiv.org/abs/2404.13808v1","category":"cs.IR"}
{"created":"2024-04-21 23:03:47","title":"Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome","abstract":"Customizing persuasive conversations related to the outcome of interest for specific users achieves better persuasion results. However, existing persuasive conversation systems rely on persuasive strategies and encounter challenges in dynamically adjusting dialogues to suit the evolving states of individual users during interactions. This limitation restricts the system's ability to deliver flexible or dynamic conversations and achieve suboptimal persuasion outcomes. In this paper, we present a novel approach that tracks a user's latent personality dimensions (LPDs) during ongoing persuasion conversation and generates tailored counterfactual utterances based on these LPDs to optimize the overall persuasion outcome. In particular, our proposed method leverages a Bi-directional Generative Adversarial Network (BiCoGAN) in tandem with a Dialogue-based Personality Prediction Regression (DPPR) model to generate counterfactual data. This enables the system to formulate alternative persuasive utterances that are more suited to the user. Subsequently, we utilize the D3QN model to learn policies for optimized selection of system utterances on counterfactual data. Experimental results we obtained from using the PersuasionForGood dataset demonstrate the superiority of our approach over the existing method, BiCoGAN. The cumulative rewards and Q-values produced by our method surpass ground truth benchmarks, showcasing the efficacy of employing counterfactual reasoning and LPDs to optimize reinforcement learning policy in online interactions.","sentences":["Customizing persuasive conversations related to the outcome of interest for specific users achieves better persuasion results.","However, existing persuasive conversation systems rely on persuasive strategies and encounter challenges in dynamically adjusting dialogues to suit the evolving states of individual users during interactions.","This limitation restricts the system's ability to deliver flexible or dynamic conversations and achieve suboptimal persuasion outcomes.","In this paper, we present a novel approach that tracks a user's latent personality dimensions (LPDs) during ongoing persuasion conversation and generates tailored counterfactual utterances based on these LPDs to optimize the overall persuasion outcome.","In particular, our proposed method leverages a Bi-directional Generative Adversarial Network (BiCoGAN) in tandem with a Dialogue-based Personality Prediction Regression (DPPR) model to generate counterfactual data.","This enables the system to formulate alternative persuasive utterances that are more suited to the user.","Subsequently, we utilize the D3QN model to learn policies for optimized selection of system utterances on counterfactual data.","Experimental results we obtained from using the PersuasionForGood dataset demonstrate the superiority of our approach over the existing method, BiCoGAN.","The cumulative rewards and Q-values produced by our method surpass ground truth benchmarks, showcasing the efficacy of employing counterfactual reasoning and LPDs to optimize reinforcement learning policy in online interactions."],"url":"http://arxiv.org/abs/2404.13792v1","category":"cs.MM"}
{"created":"2024-04-21 23:01:08","title":"Universal Fingerprint Generation: Controllable Diffusion Model with Multimodal Conditions","abstract":"The utilization of synthetic data for fingerprint recognition has garnered increased attention due to its potential to alleviate privacy concerns surrounding sensitive biometric data. However, current methods for generating fingerprints have limitations in creating impressions of the same finger with useful intra-class variations. To tackle this challenge, we present GenPrint, a framework to produce fingerprint images of various types while maintaining identity and offering humanly understandable control over different appearance factors such as fingerprint class, acquisition type, sensor device, and quality level. Unlike previous fingerprint generation approaches, GenPrint is not confined to replicating style characteristics from the training dataset alone: it enables the generation of novel styles from unseen devices without requiring additional fine-tuning. To accomplish these objectives, we developed GenPrint using latent diffusion models with multimodal conditions (text and image) for consistent generation of style and identity. Our experiments leverage a variety of publicly available datasets for training and evaluation. Results demonstrate the benefits of GenPrint in terms of identity preservation, explainable control, and universality of generated images. Importantly, the GenPrint-generated images yield comparable or even superior accuracy to models trained solely on real data and further enhances performance when augmenting the diversity of existing real fingerprint datasets.","sentences":["The utilization of synthetic data for fingerprint recognition has garnered increased attention due to its potential to alleviate privacy concerns surrounding sensitive biometric data.","However, current methods for generating fingerprints have limitations in creating impressions of the same finger with useful intra-class variations.","To tackle this challenge, we present GenPrint, a framework to produce fingerprint images of various types while maintaining identity and offering humanly understandable control over different appearance factors such as fingerprint class, acquisition type, sensor device, and quality level.","Unlike previous fingerprint generation approaches, GenPrint is not confined to replicating style characteristics from the training dataset alone: it enables the generation of novel styles from unseen devices without requiring additional fine-tuning.","To accomplish these objectives, we developed GenPrint using latent diffusion models with multimodal conditions (text and image) for consistent generation of style and identity.","Our experiments leverage a variety of publicly available datasets for training and evaluation.","Results demonstrate the benefits of GenPrint in terms of identity preservation, explainable control, and universality of generated images.","Importantly, the GenPrint-generated images yield comparable or even superior accuracy to models trained solely on real data and further enhances performance when augmenting the diversity of existing real fingerprint datasets."],"url":"http://arxiv.org/abs/2404.13791v1","category":"cs.CV"}
{"created":"2024-04-21 22:44:44","title":"Anchor-aware Deep Metric Learning for Audio-visual Retrieval","abstract":"Metric learning minimizes the gap between similar (positive) pairs of data points and increases the separation of dissimilar (negative) pairs, aiming at capturing the underlying data structure and enhancing the performance of tasks like audio-visual cross-modal retrieval (AV-CMR). Recent works employ sampling methods to select impactful data points from the embedding space during training. However, the model training fails to fully explore the space due to the scarcity of training data points, resulting in an incomplete representation of the overall positive and negative distributions. In this paper, we propose an innovative Anchor-aware Deep Metric Learning (AADML) method to address this challenge by uncovering the underlying correlations among existing data points, which enhances the quality of the shared embedding space. Specifically, our method establishes a correlation graph-based manifold structure by considering the dependencies between each sample as the anchor and its semantically similar samples. Through dynamic weighting of the correlations within this underlying manifold structure using an attention-driven mechanism, Anchor Awareness (AA) scores are obtained for each anchor. These AA scores serve as data proxies to compute relative distances in metric learning approaches. Extensive experiments conducted on two audio-visual benchmark datasets demonstrate the effectiveness of our proposed AADML method, significantly surpassing state-of-the-art models. Furthermore, we investigate the integration of AA proxies with various metric learning methods, further highlighting the efficacy of our approach.","sentences":["Metric learning minimizes the gap between similar (positive) pairs of data points and increases the separation of dissimilar (negative) pairs, aiming at capturing the underlying data structure and enhancing the performance of tasks like audio-visual cross-modal retrieval (AV-CMR).","Recent works employ sampling methods to select impactful data points from the embedding space during training.","However, the model training fails to fully explore the space due to the scarcity of training data points, resulting in an incomplete representation of the overall positive and negative distributions.","In this paper, we propose an innovative Anchor-aware Deep Metric Learning (AADML) method to address this challenge by uncovering the underlying correlations among existing data points, which enhances the quality of the shared embedding space.","Specifically, our method establishes a correlation graph-based manifold structure by considering the dependencies between each sample as the anchor and its semantically similar samples.","Through dynamic weighting of the correlations within this underlying manifold structure using an attention-driven mechanism, Anchor Awareness (AA) scores are obtained for each anchor.","These AA scores serve as data proxies to compute relative distances in metric learning approaches.","Extensive experiments conducted on two audio-visual benchmark datasets demonstrate the effectiveness of our proposed AADML method, significantly surpassing state-of-the-art models.","Furthermore, we investigate the integration of AA proxies with various metric learning methods, further highlighting the efficacy of our approach."],"url":"http://arxiv.org/abs/2404.13789v1","category":"cs.SD"}
{"created":"2024-04-21 22:33:57","title":"AnyPattern: Towards In-context Image Copy Detection","abstract":"This paper explores in-context learning for image copy detection (ICD), i.e., prompting an ICD model to identify replicated images with new tampering patterns without the need for additional training. The prompts (or the contexts) are from a small set of image-replica pairs that reflect the new patterns and are used at inference time. Such in-context ICD has good realistic value, because it requires no fine-tuning and thus facilitates fast reaction against the emergence of unseen patterns. To accommodate the \"seen $\\rightarrow$ unseen\" generalization scenario, we construct the first large-scale pattern dataset named AnyPattern, which has the largest number of tamper patterns ($90$ for training and $10$ for testing) among all the existing ones. We benchmark AnyPattern with popular ICD methods and reveal that existing methods barely generalize to novel tamper patterns. We further propose a simple in-context ICD method named ImageStacker. ImageStacker learns to select the most representative image-replica pairs and employs them as the pattern prompts in a stacking manner (rather than the popular concatenation manner). Experimental results show (1) training with our large-scale dataset substantially benefits pattern generalization ($+26.66 \\%$ $\\mu AP$), (2) the proposed ImageStacker facilitates effective in-context ICD (another round of $+16.75 \\%$ $\\mu AP$), and (3) AnyPattern enables in-context ICD, i.e. without such a large-scale dataset, in-context learning does not emerge even with our ImageStacker. The project (including the proposed dataset AnyPattern and the code for ImageStacker) is publicly available at https://anypattern.github.io under the MIT Licence.","sentences":["This paper explores in-context learning for image copy detection (ICD), i.e., prompting an ICD model to identify replicated images with new tampering patterns without the need for additional training.","The prompts (or the contexts) are from a small set of image-replica pairs that reflect the new patterns and are used at inference time.","Such in-context ICD has good realistic value, because it requires no fine-tuning and thus facilitates fast reaction against the emergence of unseen patterns.","To accommodate the \"seen $\\rightarrow$ unseen\" generalization scenario, we construct the first large-scale pattern dataset named AnyPattern, which has the largest number of tamper patterns ($90$ for training and $10$ for testing) among all the existing ones.","We benchmark AnyPattern with popular ICD methods and reveal that existing methods barely generalize to novel tamper patterns.","We further propose a simple in-context ICD method named ImageStacker.","ImageStacker learns to select the most representative image-replica pairs and employs them as the pattern prompts in a stacking manner (rather than the popular concatenation manner).","Experimental results show (1) training with our large-scale dataset substantially benefits pattern generalization ($+26.66 \\%$ $\\mu AP$), (2) the proposed ImageStacker facilitates effective in-context ICD (another round of $+16.75 \\%$ $\\mu AP$), and (3) AnyPattern enables in-context ICD, i.e. without such a large-scale dataset, in-context learning does not emerge even with our ImageStacker.","The project (including the proposed dataset AnyPattern and the code for ImageStacker) is publicly available at https://anypattern.github.io under the MIT Licence."],"url":"http://arxiv.org/abs/2404.13788v1","category":"cs.CV"}
{"created":"2024-04-21 21:19:31","title":"Multi-channel Emotion Analysis for Consensus Reaching in Group Movie Recommendation Systems","abstract":"Watching movies is one of the social activities typically done in groups. Emotion is the most vital factor that affects movie viewers' preferences. So, the emotional aspect of the movie needs to be determined and analyzed for further recommendations. It can be challenging to choose a movie that appeals to the emotions of a diverse group. Reaching an agreement for a group can be difficult due to the various genres and choices. This paper proposes a novel approach to group movie suggestions by examining emotions from three different channels: movie descriptions (text), soundtracks (audio), and posters (image). We employ the Jaccard similarity index to match each participant's emotional preferences to prospective movie choices, followed by a fuzzy inference technique to determine group consensus. We use a weighted integration process for the fusion of emotion scores from diverse data types. Then, group movie recommendation is based on prevailing emotions and viewers' best-loved movies. After determining the recommendations, the group's consensus level is calculated using a fuzzy inference system, taking participants' feedback as input. Participants (n=130) in the survey were provided with different emotion categories and asked to select the emotions best suited for particular movies (n=12). Comparison results between predicted and actual scores demonstrate the efficiency of using emotion detection for this problem (Jaccard similarity index = 0.76). We explored the relationship between induced emotions and movie popularity as an additional experiment, analyzing emotion distribution in 100 popular movies from the TMDB database. Such systems can potentially improve the accuracy of movie recommendation systems and achieve a high level of consensus among participants with diverse preferences.","sentences":["Watching movies is one of the social activities typically done in groups.","Emotion is the most vital factor that affects movie viewers' preferences.","So, the emotional aspect of the movie needs to be determined and analyzed for further recommendations.","It can be challenging to choose a movie that appeals to the emotions of a diverse group.","Reaching an agreement for a group can be difficult due to the various genres and choices.","This paper proposes a novel approach to group movie suggestions by examining emotions from three different channels: movie descriptions (text), soundtracks (audio), and posters (image).","We employ the Jaccard similarity index to match each participant's emotional preferences to prospective movie choices, followed by a fuzzy inference technique to determine group consensus.","We use a weighted integration process for the fusion of emotion scores from diverse data types.","Then, group movie recommendation is based on prevailing emotions and viewers' best-loved movies.","After determining the recommendations, the group's consensus level is calculated using a fuzzy inference system, taking participants' feedback as input.","Participants (n=130) in the survey were provided with different emotion categories and asked to select the emotions best suited for particular movies (n=12).","Comparison results between predicted and actual scores demonstrate the efficiency of using emotion detection for this problem (Jaccard similarity index = 0.76).","We explored the relationship between induced emotions and movie popularity as an additional experiment, analyzing emotion distribution in 100 popular movies from the TMDB database.","Such systems can potentially improve the accuracy of movie recommendation systems and achieve a high level of consensus among participants with diverse preferences."],"url":"http://arxiv.org/abs/2404.13778v1","category":"cs.AI"}
{"created":"2024-04-21 20:45:18","title":"EncodeNet: A Framework for Boosting DNN Accuracy with Entropy-driven Generalized Converting Autoencoder","abstract":"Image classification is a fundamental task in computer vision, and the quest to enhance DNN accuracy without inflating model size or latency remains a pressing concern. We make a couple of advances in this regard, leading to a novel EncodeNet design and training framework. The first advancement involves Converting Autoencoders, a novel approach that transforms images into an easy-to-classify image of its class. Our prior work that applied the Converting Autoencoder and a simple classifier in tandem achieved moderate accuracy over simple datasets, such as MNIST and FMNIST. However, on more complex datasets like CIFAR-10, the Converting Autoencoder has a large reconstruction loss, making it unsuitable for enhancing DNN accuracy. To address these limitations, we generalize the design of Converting Autoencoders by leveraging a larger class of DNNs, those with architectures comprising feature extraction layers followed by classification layers. We incorporate a generalized algorithmic design of the Converting Autoencoder and intraclass clustering to identify representative images, leading to optimized image feature learning. Next, we demonstrate the effectiveness of our EncodeNet design and training framework, improving the accuracy of well-trained baseline DNNs while maintaining the overall model size. EncodeNet's building blocks comprise the trained encoder from our generalized Converting Autoencoders transferring knowledge to a lightweight classifier network - also extracted from the baseline DNN. Our experimental results demonstrate that EncodeNet improves the accuracy of VGG16 from 92.64% to 94.05% on CIFAR-10 and RestNet20 from 74.56% to 76.04% on CIFAR-100. It outperforms state-of-the-art techniques that rely on knowledge distillation and attention mechanisms, delivering higher accuracy for models of comparable size.","sentences":["Image classification is a fundamental task in computer vision, and the quest to enhance DNN accuracy without inflating model size or latency remains a pressing concern.","We make a couple of advances in this regard, leading to a novel EncodeNet design and training framework.","The first advancement involves Converting Autoencoders, a novel approach that transforms images into an easy-to-classify image of its class.","Our prior work that applied the Converting Autoencoder and a simple classifier in tandem achieved moderate accuracy over simple datasets, such as MNIST and FMNIST.","However, on more complex datasets like CIFAR-10, the Converting Autoencoder has a large reconstruction loss, making it unsuitable for enhancing DNN accuracy.","To address these limitations, we generalize the design of Converting Autoencoders by leveraging a larger class of DNNs, those with architectures comprising feature extraction layers followed by classification layers.","We incorporate a generalized algorithmic design of the Converting Autoencoder and intraclass clustering to identify representative images, leading to optimized image feature learning.","Next, we demonstrate the effectiveness of our EncodeNet design and training framework, improving the accuracy of well-trained baseline DNNs while maintaining the overall model size.","EncodeNet's building blocks comprise the trained encoder from our generalized Converting Autoencoders transferring knowledge to a lightweight classifier network - also extracted from the baseline DNN.","Our experimental results demonstrate that EncodeNet improves the accuracy of VGG16 from 92.64% to 94.05% on CIFAR-10 and RestNet20 from 74.56% to 76.04% on CIFAR-100.","It outperforms state-of-the-art techniques that rely on knowledge distillation and attention mechanisms, delivering higher accuracy for models of comparable size."],"url":"http://arxiv.org/abs/2404.13770v1","category":"cs.CV"}
{"created":"2024-04-21 20:36:12","title":"The Economics of Blockchain Governance: Evaluate Liquid Democracy on the Internet Computer","abstract":"Decentralized Autonomous Organizations (DAOs), utilizing blockchain technology to enable collective governance, are a promising innovation. This research addresses the ongoing query in blockchain governance: How can DAOs optimize human cooperation? Focusing on the Network Nervous System (NNS), a comprehensive on-chain governance framework underpinned by the Internet Computer Protocol (ICP) and liquid democracy principles, we employ theoretical abstraction and simulations to evaluate its potential impact on cooperation and economic growth within DAOs. Our findings emphasize the significance of the NNS's staking mechanism, particularly the reward multiplier, in aligning individual short-term interests with the DAO's long-term prosperity. This study contributes to the understanding and effective design of blockchain-based governance systems.","sentences":["Decentralized Autonomous Organizations (DAOs), utilizing blockchain technology to enable collective governance, are a promising innovation.","This research addresses the ongoing query in blockchain governance: How can DAOs optimize human cooperation?","Focusing on the Network Nervous System (NNS), a comprehensive on-chain governance framework underpinned by the Internet Computer Protocol (ICP) and liquid democracy principles, we employ theoretical abstraction and simulations to evaluate its potential impact on cooperation and economic growth within DAOs.","Our findings emphasize the significance of the NNS's staking mechanism, particularly the reward multiplier, in aligning individual short-term interests with the DAO's long-term prosperity.","This study contributes to the understanding and effective design of blockchain-based governance systems."],"url":"http://arxiv.org/abs/2404.13768v1","category":"econ.GN"}
{"created":"2024-04-21 20:32:02","title":"Autonomous Robot for Disaster Mapping and Victim Localization","abstract":"In response to the critical need for effective reconnaissance in disaster scenarios, this research article presents the design and implementation of a complete autonomous robot system using the Turtlebot3 with Robotic Operating System (ROS) Noetic. Upon deployment in closed, initially unknown environments, the system aims to generate a comprehensive map and identify any present 'victims' using AprilTags as stand-ins. We discuss our solution for search and rescue missions, while additionally exploring more advanced algorithms to improve search and rescue functionalities. We introduce a Cubature Kalman Filter to help reduce the mean squared error [m] for AprilTag localization and an information-theoretic exploration algorithm to expedite exploration in unknown environments. Just like turtles, our system takes it slow and steady, but when it's time to save the day, it moves at ninja-like speed! Despite Donatello's shell, he's no slowpoke - he zips through obstacles with the agility of a teenage mutant ninja turtle. So, hang on tight to your shells and get ready for a whirlwind of reconnaissance!   Full pipeline code https://github.com/rzhao5659/MRProject/tree/main   Exploration code https://github.com/rzhao5659/MRProject/tree/main","sentences":["In response to the critical need for effective reconnaissance in disaster scenarios, this research article presents the design and implementation of a complete autonomous robot system using the Turtlebot3 with Robotic Operating System (ROS) Noetic.","Upon deployment in closed, initially unknown environments, the system aims to generate a comprehensive map and identify any present 'victims' using AprilTags as stand-ins.","We discuss our solution for search and rescue missions, while additionally exploring more advanced algorithms to improve search and rescue functionalities.","We introduce a Cubature Kalman Filter to help reduce the mean squared error [m] for AprilTag localization and an information-theoretic exploration algorithm to expedite exploration in unknown environments.","Just like turtles, our system takes it slow and steady, but when it's time to save the day, it moves at ninja-like speed!","Despite Donatello's shell, he's no slowpoke - he zips through obstacles with the agility of a teenage mutant ninja turtle.","So, hang on tight to your shells and get ready for a whirlwind of reconnaissance!   ","Full pipeline code https://github.com/rzhao5659/MRProject/tree/main   Exploration code https://github.com/rzhao5659/MRProject/tree/main"],"url":"http://arxiv.org/abs/2404.13767v1","category":"cs.RO"}
{"created":"2024-04-21 19:24:15","title":"Towards General Conceptual Model Editing via Adversarial Representation Engineering","abstract":"Recent research has introduced Representation Engineering (RepE) as a promising approach for understanding complex inner workings of large-scale models like Large Language Models (LLMs). However, finding practical and efficient methods to apply these representations for general and flexible model editing remains an open problem. Inspired by the Generative Adversarial Network (GAN) framework, we introduce a novel approach called Adversarial Representation Engineering (ARE). This method leverages RepE by using a representation sensor to guide the editing of LLMs, offering a unified and interpretable framework for conceptual model editing without degrading baseline performance. Our experiments on multiple conceptual editing confirm ARE's effectiveness. Code and data are available at https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.","sentences":["Recent research has introduced Representation Engineering (RepE) as a promising approach for understanding complex inner workings of large-scale models like Large Language Models (LLMs).","However, finding practical and efficient methods to apply these representations for general and flexible model editing remains an open problem.","Inspired by the Generative Adversarial Network (GAN) framework, we introduce a novel approach called Adversarial Representation Engineering (ARE).","This method leverages RepE by using a representation sensor to guide the editing of LLMs, offering a unified and interpretable framework for conceptual model editing without degrading baseline performance.","Our experiments on multiple conceptual editing confirm ARE's effectiveness.","Code and data are available at https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering."],"url":"http://arxiv.org/abs/2404.13752v1","category":"cs.LG"}
{"created":"2024-04-21 19:02:38","title":"A Nasal Cytology Dataset for Object Detection and Deep Learning","abstract":"Nasal Cytology is a new and efficient clinical technique to diagnose rhinitis and allergies that is not much widespread due to the time-consuming nature of cell counting; that is why AI-aided counting could be a turning point for the diffusion of this technique. In this article we present the first dataset of rhino-cytological field images: the NCD (Nasal Cytology Dataset), aimed to train and deploy Object Detection models to support physicians and biologists during clinical practice. The real distribution of the cytotypes, populating the nasal mucosa has been replicated, sampling images from slides of clinical patients, and manually annotating each cell found on them. The correspondent object detection task presents non'trivial issues associated with the strong class imbalancement, involving the rarest cell types. This work contributes to some of open challenges by presenting a novel machine learning-based approach to aid the automated detection and classification of nasal mucosa cells: the DETR and YOLO models shown good performance in detecting cells and classifying them correctly, revealing great potential to accelerate the work of rhinology experts.","sentences":["Nasal Cytology is a new and efficient clinical technique to diagnose rhinitis and allergies that is not much widespread due to the time-consuming nature of cell counting; that is why AI-aided counting could be a turning point for the diffusion of this technique.","In this article we present the first dataset of rhino-cytological field images: the NCD (Nasal Cytology Dataset), aimed to train and deploy Object Detection models to support physicians and biologists during clinical practice.","The real distribution of the cytotypes, populating the nasal mucosa has been replicated, sampling images from slides of clinical patients, and manually annotating each cell found on them.","The correspondent object detection task presents non'trivial issues associated with the strong class imbalancement, involving the rarest cell types.","This work contributes to some of open challenges by presenting a novel machine learning-based approach to aid the automated detection and classification of nasal mucosa cells: the DETR and YOLO models shown good performance in detecting cells and classifying them correctly, revealing great potential to accelerate the work of rhinology experts."],"url":"http://arxiv.org/abs/2404.13745v1","category":"cs.CV"}
{"created":"2024-04-21 18:56:54","title":"Seamless Underwater Navigation with Limited Doppler Velocity Log Measurements","abstract":"Autonomous Underwater Vehicles (AUVs) commonly utilize an inertial navigation system (INS) and a Doppler velocity log (DVL) for underwater navigation. To that end, their measurements are integrated through a nonlinear filter such as the extended Kalman filter (EKF). The DVL velocity vector estimate depends on retrieving reflections from the seabed, ensuring that at least three out of its four transmitted acoustic beams return successfully. When fewer than three beams are obtained, the DVL cannot provide a velocity update to bind the navigation solution drift. To cope with this challenge, in this paper, we propose a hybrid neural coupled (HNC) approach for seamless AUV navigation in situations of limited DVL measurements. First, we drive an approach to regress two or three missing DVL beams. Then, those beams, together with the measured beams, are incorporated into the EKF. We examined INS/DVL fusion both in loosely and tightly coupled approaches. Our method was trained and evaluated on recorded data from AUV experiments conducted in the Mediterranean Sea on two different occasions. The results illustrate that our proposed method outperforms the baseline loosely and tightly coupled model-based approaches by an average of 96.15%. It also demonstrates superior performance compared to a model-based beam estimator by an average of 12.41% in terms of velocity accuracy for scenarios involving two or three missing beams. Therefore, we demonstrate that our approach offers seamless AUV navigation in situations of limited beam measurements.","sentences":["Autonomous Underwater Vehicles (AUVs) commonly utilize an inertial navigation system (INS) and a Doppler velocity log (DVL) for underwater navigation.","To that end, their measurements are integrated through a nonlinear filter such as the extended Kalman filter (EKF).","The DVL velocity vector estimate depends on retrieving reflections from the seabed, ensuring that at least three out of its four transmitted acoustic beams return successfully.","When fewer than three beams are obtained, the DVL cannot provide a velocity update to bind the navigation solution drift.","To cope with this challenge, in this paper, we propose a hybrid neural coupled (HNC) approach for seamless AUV navigation in situations of limited DVL measurements.","First, we drive an approach to regress two or three missing DVL beams.","Then, those beams, together with the measured beams, are incorporated into the EKF.","We examined INS/DVL fusion both in loosely and tightly coupled approaches.","Our method was trained and evaluated on recorded data from AUV experiments conducted in the Mediterranean Sea on two different occasions.","The results illustrate that our proposed method outperforms the baseline loosely and tightly coupled model-based approaches by an average of 96.15%.","It also demonstrates superior performance compared to a model-based beam estimator by an average of 12.41% in terms of velocity accuracy for scenarios involving two or three missing beams.","Therefore, we demonstrate that our approach offers seamless AUV navigation in situations of limited beam measurements."],"url":"http://arxiv.org/abs/2404.13742v1","category":"cs.RO"}
{"created":"2024-04-21 18:34:01","title":"P\u00e9clet dependent Interactions of Self-Propelled Droplets","abstract":"Interactions among biologically active agents is facilitated by their self-generated chemical and hydrodynamic fields. In order to elucidate the pair-wise interactions between such micro-organisms, we employ active droplets as a model system, capable of self-generating chemical and hydrodynamic fields. We demonstrate that the solute P\\'eclet number ($Pe$), characterizing the relative strength of its convective to diffusive transport, plays a crucial role in determining how the chemical and hydrodynamic fields impact their interactions. Our findings reveal that at low $Pe$, the interaction is predominantly governed by chemo-repulsive effects, leading to droplets avoiding physical contact. Conversely, at elevated $Pe$, hydrodynamic interactions become more influential, leading to physical engagement. However, irrespective of $Pe$, the interaction of a droplet with the chemical trail of another droplet is always governed by chemo-repulsive effects. Furthermore, our results establish that the chemo-repulsive deflection/rebounding of droplets is influenced by the droplets' inherent chemical polarity, as determined by its $Pe$, independent of their approach orientation. Our findings offer a methodology for tuning the outcomes of binary interactions among chemically active droplets, laying the groundwork for potential studies on their collective dynamics.","sentences":["Interactions among biologically active agents is facilitated by their self-generated chemical and hydrodynamic fields.","In order to elucidate the pair-wise interactions between such micro-organisms, we employ active droplets as a model system, capable of self-generating chemical and hydrodynamic fields.","We demonstrate that the solute P\\'eclet number ($Pe$), characterizing the relative strength of its convective to diffusive transport, plays a crucial role in determining how the chemical and hydrodynamic fields impact their interactions.","Our findings reveal that at low $Pe$, the interaction is predominantly governed by chemo-repulsive effects, leading to droplets avoiding physical contact.","Conversely, at elevated $Pe$, hydrodynamic interactions become more influential, leading to physical engagement.","However, irrespective of $Pe$, the interaction of a droplet with the chemical trail of another droplet is always governed by chemo-repulsive effects.","Furthermore, our results establish that the chemo-repulsive deflection/rebounding of droplets is influenced by the droplets' inherent chemical polarity, as determined by its $Pe$, independent of their approach orientation.","Our findings offer a methodology for tuning the outcomes of binary interactions among chemically active droplets, laying the groundwork for potential studies on their collective dynamics."],"url":"http://arxiv.org/abs/2404.13740v1","category":"cond-mat.soft"}
{"created":"2024-04-21 18:24:43","title":"Stochastic Multi-round Submodular Optimization with Budget","abstract":"In this work we study the problem of Stochastic Budgeted Multi-round Submodular Maximization (SBMSm), in which we would like to maximize the sum over multiple rounds of the value of a monotone and submodular objective function, subject to the fact that the values of this function depend on the realization of stochastic events and the number of observations that we can make over all rounds is limited by a given budget. This problem extends, and generalizes to multiple round settings, well-studied problems such as (adaptive) influence maximization and stochastic probing.   We first show that whenever a certain single-round optimization problem can be optimally solved in polynomial time, then there is a polynomial time dynamic programming algorithm that returns the same solution as the optimal algorithm, that can adaptively choose both which observations to make and in which round to have them. Unfortunately, this dynamic programming approach cannot be extended to work when the single-round optimization problem cannot be efficiently solved (even if we allow it would be approximated within an arbitrary small constant). Anyway, in this case we are able to provide a simple greedy algorithm for the problem. It guarantees a $(1/2-\\epsilon)$-approximation to the optimal value, even if it non-adaptively allocates the budget to rounds.","sentences":["In this work we study the problem of Stochastic Budgeted Multi-round Submodular Maximization (SBMSm), in which we would like to maximize the sum over multiple rounds of the value of a monotone and submodular objective function, subject to the fact that the values of this function depend on the realization of stochastic events and the number of observations that we can make over all rounds is limited by a given budget.","This problem extends, and generalizes to multiple round settings, well-studied problems such as (adaptive) influence maximization and stochastic probing.   ","We first show that whenever a certain single-round optimization problem can be optimally solved in polynomial time, then there is a polynomial time dynamic programming algorithm that returns the same solution as the optimal algorithm, that can adaptively choose both which observations to make and in which round to have them.","Unfortunately, this dynamic programming approach cannot be extended to work when the single-round optimization problem cannot be efficiently solved (even if we allow it would be approximated within an arbitrary small constant).","Anyway, in this case we are able to provide a simple greedy algorithm for the problem.","It guarantees a $(1/2-\\epsilon)$-approximation to the optimal value, even if it non-adaptively allocates the budget to rounds."],"url":"http://arxiv.org/abs/2404.13737v1","category":"cs.DS"}
{"created":"2024-04-21 18:24:34","title":"Interval Abstractions for Robust Counterfactual Explanations","abstract":"Counterfactual Explanations (CEs) have emerged as a major paradigm in explainable AI research, providing recourse recommendations for users affected by the decisions of machine learning models. However, when slight changes occur in the parameters of the underlying model, CEs found by existing methods often become invalid for the updated models. The literature lacks a way to certify deterministic robustness guarantees for CEs under model changes, in that existing methods to improve CEs' robustness are heuristic, and the robustness performances are evaluated empirically using only a limited number of retrained models. To bridge this gap, we propose a novel interval abstraction technique for parametric machine learning models, which allows us to obtain provable robustness guarantees of CEs under the possibly infinite set of plausible model changes $\\Delta$. We formalise our robustness notion as the $\\Delta$-robustness for CEs, in both binary and multi-class classification settings. We formulate procedures to verify $\\Delta$-robustness based on Mixed Integer Linear Programming, using which we further propose two algorithms to generate CEs that are $\\Delta$-robust. In an extensive empirical study, we demonstrate how our approach can be used in practice by discussing two strategies for determining the appropriate hyperparameter in our method, and we quantitatively benchmark the CEs generated by eleven methods, highlighting the effectiveness of our algorithms in finding robust CEs.","sentences":["Counterfactual Explanations (CEs) have emerged as a major paradigm in explainable AI research, providing recourse recommendations for users affected by the decisions of machine learning models.","However, when slight changes occur in the parameters of the underlying model, CEs found by existing methods often become invalid for the updated models.","The literature lacks a way to certify deterministic robustness guarantees for CEs under model changes, in that existing methods to improve CEs' robustness are heuristic, and the robustness performances are evaluated empirically using only a limited number of retrained models.","To bridge this gap, we propose a novel interval abstraction technique for parametric machine learning models, which allows us to obtain provable robustness guarantees of CEs under the possibly infinite set of plausible model changes $\\Delta$. We formalise our robustness notion as the $\\Delta$-robustness for CEs, in both binary and multi-class classification settings.","We formulate procedures to verify $\\Delta$-robustness based on Mixed Integer Linear Programming, using which we further propose two algorithms to generate CEs that are $\\Delta$-robust.","In an extensive empirical study, we demonstrate how our approach can be used in practice by discussing two strategies for determining the appropriate hyperparameter in our method, and we quantitatively benchmark the CEs generated by eleven methods, highlighting the effectiveness of our algorithms in finding robust CEs."],"url":"http://arxiv.org/abs/2404.13736v1","category":"cs.LG"}
{"created":"2024-04-21 18:19:27","title":"Elucidating the Design Space of Dataset Condensation","abstract":"Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism. This approach significantly improves model training efficiency and is adaptable across multiple application areas. Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule. These strategies are grounded in empirical evidence and theoretical backing. Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and large-scale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.","sentences":["Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism.","This approach significantly improves model training efficiency and is adaptable across multiple application areas.","Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., SRe2L, G-VBSM, and RDED).","To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule.","These strategies are grounded in empirical evidence and theoretical backing.","Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and large-scale dataset condensation.","In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%.","This performance exceeds those of SRe2L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively."],"url":"http://arxiv.org/abs/2404.13733v1","category":"cs.LG"}
{"created":"2024-04-21 17:20:19","title":"The Framework of a Design Process Language","abstract":"The thesis develops a view of design in a concept formation framework and outlines a language to describe both the object of the design and the process of designing. The unknown object at the outset of the design work may be seen as an unknown concept that the designer is to define. Throughout the process, she develops a description of this object by relating it to known concepts. The search stops when the designer is satisfied that the design specification is complete enough to satisfy the requirements from it once built. It is then a collection of propositions that all contribute towards defining the design object - a collection of sentences describing relationships between the object and known concepts. Also, the design process itself may be described by relating known concepts - by organizing known abilities into particular patterns of activation, or mobilization. In view of the demands posed to a language to use in this concept formation process, the framework of a Design Process Language (DPL) is developed. The basis for the language are linguistic categories that act as classes of relations used to combine concepts, containing relations used for describing process and object within the same general system, with some relations being process specific, others being object specific, and with the bulk being used both for process and object description. Another outcome is the distinction of modal relations, or relations describing futurity, possibility, willingness, hypothetical events, and the like. The design process almost always includes aspects such as these, and it is thus necessary for a language facilitating design process description to support such relationships to be constructed. The DPL is argued to be a foundation whereupon to build a language that can be used for enabling computers to be more useful - act more intelligently - in the design process.","sentences":["The thesis develops a view of design in a concept formation framework and outlines a language to describe both the object of the design and the process of designing.","The unknown object at the outset of the design work may be seen as an unknown concept that the designer is to define.","Throughout the process, she develops a description of this object by relating it to known concepts.","The search stops when the designer is satisfied that the design specification is complete enough to satisfy the requirements from it once built.","It is then a collection of propositions that all contribute towards defining the design object - a collection of sentences describing relationships between the object and known concepts.","Also, the design process itself may be described by relating known concepts - by organizing known abilities into particular patterns of activation, or mobilization.","In view of the demands posed to a language to use in this concept formation process, the framework of a Design Process Language (DPL) is developed.","The basis for the language are linguistic categories that act as classes of relations used to combine concepts, containing relations used for describing process and object within the same general system, with some relations being process specific, others being object specific, and with the bulk being used both for process and object description.","Another outcome is the distinction of modal relations, or relations describing futurity, possibility, willingness, hypothetical events, and the like.","The design process almost always includes aspects such as these, and it is thus necessary for a language facilitating design process description to support such relationships to be constructed.","The DPL is argued to be a foundation whereupon to build a language that can be used for enabling computers to be more useful - act more intelligently - in the design process."],"url":"http://arxiv.org/abs/2404.13721v1","category":"cs.AI"}
{"created":"2024-04-21 17:15:43","title":"A Practical Multilevel Governance Framework for Autonomous and Intelligent Systems","abstract":"Autonomous and intelligent systems (AIS) facilitate a wide range of beneficial applications across a variety of different domains. However, technical characteristics such as unpredictability and lack of transparency, as well as potential unintended consequences, pose considerable challenges to the current governance infrastructure. Furthermore, the speed of development and deployment of applications outpaces the ability of existing governance institutions to put in place effective ethical-legal oversight. New approaches for agile, distributed and multilevel governance are needed. This work presents a practical framework for multilevel governance of AIS. The framework enables mapping actors onto six levels of decision-making including the international, national and organizational levels. Furthermore, it offers the ability to identify and evolve existing tools or create new tools for guiding the behavior of actors within the levels. Governance mechanisms enable actors to shape and enforce regulations and other tools, which when complemented with good practices contribute to effective and comprehensive governance.","sentences":["Autonomous and intelligent systems (AIS) facilitate a wide range of beneficial applications across a variety of different domains.","However, technical characteristics such as unpredictability and lack of transparency, as well as potential unintended consequences, pose considerable challenges to the current governance infrastructure.","Furthermore, the speed of development and deployment of applications outpaces the ability of existing governance institutions to put in place effective ethical-legal oversight.","New approaches for agile, distributed and multilevel governance are needed.","This work presents a practical framework for multilevel governance of AIS.","The framework enables mapping actors onto six levels of decision-making including the international, national and organizational levels.","Furthermore, it offers the ability to identify and evolve existing tools or create new tools for guiding the behavior of actors within the levels.","Governance mechanisms enable actors to shape and enforce regulations and other tools, which when complemented with good practices contribute to effective and comprehensive governance."],"url":"http://arxiv.org/abs/2404.13719v1","category":"cs.CY"}
{"created":"2024-04-21 16:35:16","title":"Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models","abstract":"Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.   Project page: https://cs-people.bu.edu/vpetsiuk/arc","sentences":["Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images.","Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them.","We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation.","This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible.","We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment.","We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary.","Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   ","Content Advisory:","This paper contains discussions and model-generated content that may be considered offensive.","Reader discretion is advised.   ","Project page: https://cs-people.bu.edu/vpetsiuk/arc"],"url":"http://arxiv.org/abs/2404.13706v1","category":"cs.CV"}
{"created":"2024-04-21 15:40:32","title":"A Complete System for Automated 3D Semantic-Geometric Mapping of Corrosion in Industrial Environments","abstract":"Corrosion, a naturally occurring process leading to the deterioration of metallic materials, demands diligent detection for quality control and the preservation of metal-based objects, especially within industrial contexts. Traditional techniques for corrosion identification, including ultrasonic testing, radio-graphic testing, and magnetic flux leakage, necessitate the deployment of expensive and bulky equipment on-site for effective data acquisition. An unexplored alternative involves employing lightweight, conventional camera systems, and state-of-the-art computer vision methods for its identification.   In this work, we propose a complete system for semi-automated corrosion identification and mapping in industrial environments. We leverage recent advances in LiDAR-based methods for localization and mapping, with vision-based semantic segmentation deep learning techniques, in order to build semantic-geometric maps of industrial environments. Unlike previous corrosion identification systems available in the literature, our designed multi-modal system is low-cost, portable, semi-autonomous and allows collecting large datasets by untrained personnel.   A set of experiments in an indoor laboratory environment, demonstrate quantitatively the high accuracy of the employed LiDAR based 3D mapping and localization system, with less then $0.05m$ and 0.02m average absolute and relative pose errors. Also, our data-driven semantic segmentation model, achieves around 70\\% precision when trained with our pixel-wise manually annotated dataset.","sentences":["Corrosion, a naturally occurring process leading to the deterioration of metallic materials, demands diligent detection for quality control and the preservation of metal-based objects, especially within industrial contexts.","Traditional techniques for corrosion identification, including ultrasonic testing, radio-graphic testing, and magnetic flux leakage, necessitate the deployment of expensive and bulky equipment on-site for effective data acquisition.","An unexplored alternative involves employing lightweight, conventional camera systems, and state-of-the-art computer vision methods for its identification.   ","In this work, we propose a complete system for semi-automated corrosion identification and mapping in industrial environments.","We leverage recent advances in LiDAR-based methods for localization and mapping, with vision-based semantic segmentation deep learning techniques, in order to build semantic-geometric maps of industrial environments.","Unlike previous corrosion identification systems available in the literature, our designed multi-modal system is low-cost, portable, semi-autonomous and allows collecting large datasets by untrained personnel.   ","A set of experiments in an indoor laboratory environment, demonstrate quantitatively the high accuracy of the employed LiDAR based 3D mapping and localization system, with less then $0.05m$ and 0.02m average absolute and relative pose errors.","Also, our data-driven semantic segmentation model, achieves around 70\\% precision when trained with our pixel-wise manually annotated dataset."],"url":"http://arxiv.org/abs/2404.13691v1","category":"cs.CV"}
{"created":"2024-04-21 14:43:31","title":"PoseAnimate: Zero-shot high fidelity pose controllable character animation","abstract":"Image-to-video(I2V) generation aims to create a video sequence from a single image, which requires high temporal coherence and visual fidelity with the source image.However, existing approaches suffer from character appearance inconsistency and poor preservation of fine details. Moreover, they require a large amount of video data for training, which can be computationally demanding.To address these limitations,we propose PoseAnimate, a novel zero-shot I2V framework for character animation.PoseAnimate contains three key components: 1) Pose-Aware Control Module (PACM) incorporates diverse pose signals into conditional embeddings, to preserve character-independent content and maintain precise alignment of actions.2) Dual Consistency Attention Module (DCAM) enhances temporal consistency, and retains character identity and intricate background details.3) Mask-Guided Decoupling Module (MGDM) refines distinct feature perception, improving animation fidelity by decoupling the character and background.We also propose a Pose Alignment Transition Algorithm (PATA) to ensure smooth action transition.Extensive experiment results demonstrate that our approach outperforms the state-of-the-art training-based methods in terms of character consistency and detail fidelity. Moreover, it maintains a high level of temporal coherence throughout the generated animations.","sentences":["Image-to-video(I2V) generation aims to create a video sequence from a single image, which requires high temporal coherence and visual fidelity with the source image.","However, existing approaches suffer from character appearance inconsistency and poor preservation of fine details.","Moreover, they require a large amount of video data for training, which can be computationally demanding.","To address these limitations,we propose PoseAnimate, a novel zero-shot I2V framework for character animation.","PoseAnimate contains three key components: 1) Pose-Aware Control Module (PACM) incorporates diverse pose signals into conditional embeddings, to preserve character-independent content and maintain precise alignment of actions.2) Dual Consistency Attention Module (DCAM) enhances temporal consistency, and retains character identity and intricate background details.3)","Mask-Guided Decoupling Module (MGDM) refines distinct feature perception, improving animation fidelity by decoupling the character and background.","We also propose a Pose Alignment Transition Algorithm (PATA) to ensure smooth action transition.","Extensive experiment results demonstrate that our approach outperforms the state-of-the-art training-based methods in terms of character consistency and detail fidelity.","Moreover, it maintains a high level of temporal coherence throughout the generated animations."],"url":"http://arxiv.org/abs/2404.13680v1","category":"cs.CV"}
{"created":"2024-04-21 14:36:57","title":"A Dataset and Model for Realistic License Plate Deblurring","abstract":"Vehicle license plate recognition is a crucial task in intelligent traffic management systems. However, the challenge of achieving accurate recognition persists due to motion blur from fast-moving vehicles. Despite the widespread use of image synthesis approaches in existing deblurring and recognition algorithms, their effectiveness in real-world scenarios remains unproven. To address this, we introduce the first large-scale license plate deblurring dataset named License Plate Blur (LPBlur), captured by a dual-camera system and processed through a post-processing pipeline to avoid misalignment issues. Then, we propose a License Plate Deblurring Generative Adversarial Network (LPDGAN) to tackle the license plate deblurring: 1) a Feature Fusion Module to integrate multi-scale latent codes; 2) a Text Reconstruction Module to restore structure through textual modality; 3) a Partition Discriminator Module to enhance the model's perception of details in each letter. Extensive experiments validate the reliability of the LPBlur dataset for both model training and testing, showcasing that our proposed model outperforms other state-of-the-art motion deblurring methods in realistic license plate deblurring scenarios. The dataset and code are available at https://github.com/haoyGONG/LPDGAN.","sentences":["Vehicle license plate recognition is a crucial task in intelligent traffic management systems.","However, the challenge of achieving accurate recognition persists due to motion blur from fast-moving vehicles.","Despite the widespread use of image synthesis approaches in existing deblurring and recognition algorithms, their effectiveness in real-world scenarios remains unproven.","To address this, we introduce the first large-scale license plate deblurring dataset named License Plate Blur (LPBlur), captured by a dual-camera system and processed through a post-processing pipeline to avoid misalignment issues.","Then, we propose a License Plate Deblurring Generative Adversarial Network (LPDGAN) to tackle the license plate deblurring: 1) a Feature Fusion Module to integrate multi-scale latent codes; 2) a Text Reconstruction Module to restore structure through textual modality; 3) a Partition Discriminator Module to enhance the model's perception of details in each letter.","Extensive experiments validate the reliability of the LPBlur dataset for both model training and testing, showcasing that our proposed model outperforms other state-of-the-art motion deblurring methods in realistic license plate deblurring scenarios.","The dataset and code are available at https://github.com/haoyGONG/LPDGAN."],"url":"http://arxiv.org/abs/2404.13677v2","category":"cs.CV"}
{"created":"2024-04-21 14:03:34","title":"MathNet: A Data-Centric Approach for Printed Mathematical Expression Recognition","abstract":"Printed mathematical expression recognition (MER) models are usually trained and tested using LaTeX-generated mathematical expressions (MEs) as input and the LaTeX source code as ground truth. As the same ME can be generated by various different LaTeX source codes, this leads to unwanted variations in the ground truth data that bias test performance results and hinder efficient learning. In addition, the use of only one font to generate the MEs heavily limits the generalization of the reported results to realistic scenarios. We propose a data-centric approach to overcome this problem, and present convincing experimental results: Our main contribution is an enhanced LaTeX normalization to map any LaTeX ME to a canonical form. Based on this process, we developed an improved version of the benchmark dataset im2latex-100k, featuring 30 fonts instead of one. Second, we introduce the real-world dataset realFormula, with MEs extracted from papers. Third, we developed a MER model, MathNet, based on a convolutional vision transformer, with superior results on all four test sets (im2latex-100k, im2latexv2, realFormula, and InftyMDB-1), outperforming the previous state of the art by up to 88.3%.","sentences":["Printed mathematical expression recognition (MER) models are usually trained and tested using LaTeX-generated mathematical expressions (MEs) as input and the LaTeX source code as ground truth.","As the same ME can be generated by various different LaTeX source codes, this leads to unwanted variations in the ground truth data that bias test performance results and hinder efficient learning.","In addition, the use of only one font to generate the MEs heavily limits the generalization of the reported results to realistic scenarios.","We propose a data-centric approach to overcome this problem, and present convincing experimental results: Our main contribution is an enhanced LaTeX normalization to map any LaTeX ME to a canonical form.","Based on this process, we developed an improved version of the benchmark dataset im2latex-100k, featuring 30 fonts instead of one.","Second, we introduce the real-world dataset realFormula, with MEs extracted from papers.","Third, we developed a MER model, MathNet, based on a convolutional vision transformer, with superior results on all four test sets (im2latex-100k, im2latexv2, realFormula, and InftyMDB-1), outperforming the previous state of the art by up to 88.3%."],"url":"http://arxiv.org/abs/2404.13667v1","category":"cs.CV"}
{"created":"2024-04-21 13:51:31","title":"Cumulative Hazard Function Based Efficient Multivariate Temporal Point Process Learning","abstract":"Most existing temporal point process models are characterized by conditional intensity function. These models often require numerical approximation methods for likelihood evaluation, which potentially hurts their performance. By directly modelling the integral of the intensity function, i.e., the cumulative hazard function (CHF), the likelihood can be evaluated accurately, making it a promising approach. However, existing CHF-based methods are not well-defined, i.e., the mathematical constraints of CHF are not completely satisfied, leading to untrustworthy results. For multivariate temporal point process, most existing methods model intensity (or density, etc.) functions for each variate, limiting the scalability. In this paper, we explore using neural networks to model a flexible but well-defined CHF and learning the multivariate temporal point process with low parameter complexity. Experimental results on six datasets show that the proposed model achieves the state-of-the-art performance on data fitting and event prediction tasks while having significantly fewer parameters and memory usage than the strong competitors. The source code and data can be obtained from https://github.com/lbq8942/NPP.","sentences":["Most existing temporal point process models are characterized by conditional intensity function.","These models often require numerical approximation methods for likelihood evaluation, which potentially hurts their performance.","By directly modelling the integral of the intensity function, i.e., the cumulative hazard function (CHF), the likelihood can be evaluated accurately, making it a promising approach.","However, existing CHF-based methods are not well-defined, i.e., the mathematical constraints of CHF are not completely satisfied, leading to untrustworthy results.","For multivariate temporal point process, most existing methods model intensity (or density, etc.) functions for each variate, limiting the scalability.","In this paper, we explore using neural networks to model a flexible but well-defined CHF and learning the multivariate temporal point process with low parameter complexity.","Experimental results on six datasets show that the proposed model achieves the state-of-the-art performance on data fitting and event prediction tasks while having significantly fewer parameters and memory usage than the strong competitors.","The source code and data can be obtained from https://github.com/lbq8942/NPP."],"url":"http://arxiv.org/abs/2404.13663v1","category":"cs.LG"}
{"created":"2024-04-21 13:25:46","title":"MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions","abstract":"In this paper, we address the unexplored question of temporal sentence localization in human motions (TSLM), aiming to locate a target moment from a 3D human motion that semantically corresponds to a text query. Considering that 3D human motions are captured using specialized motion capture devices, motions with only a few joints lack complex scene information like objects and lighting. Due to this character, motion data has low contextual richness and semantic ambiguity between frames, which limits the accuracy of predictions made by current video localization frameworks extended to TSLM to only a rough level. To refine this, we devise two novel label-prior-assisted training schemes: one embed prior knowledge of foreground and background to highlight the localization chances of target moments, and the other forces the originally rough predictions to overlap with the more accurate predictions obtained from the flipped start/end prior label sequences during recovery training. We show that injecting label-prior knowledge into the model is crucial for improving performance at high IoU. In our constructed TSLM benchmark, our model termed MLP achieves a recall of 44.13 at IoU@0.7 on the BABEL dataset and 71.17 on HumanML3D (Restore), outperforming prior works. Finally, we showcase the potential of our approach in corpus-level moment retrieval. Our source code is openly accessible at https://github.com/eanson023/mlp.","sentences":["In this paper, we address the unexplored question of temporal sentence localization in human motions (TSLM), aiming to locate a target moment from a 3D human motion that semantically corresponds to a text query.","Considering that 3D human motions are captured using specialized motion capture devices, motions with only a few joints lack complex scene information like objects and lighting.","Due to this character, motion data has low contextual richness and semantic ambiguity between frames, which limits the accuracy of predictions made by current video localization frameworks extended to TSLM to only a rough level.","To refine this, we devise two novel label-prior-assisted training schemes: one embed prior knowledge of foreground and background to highlight the localization chances of target moments, and the other forces the originally rough predictions to overlap with the more accurate predictions obtained from the flipped start/end prior label sequences during recovery training.","We show that injecting label-prior knowledge into the model is crucial for improving performance at high IoU.","In our constructed TSLM benchmark, our model termed MLP achieves a recall of 44.13 at IoU@0.7 on the BABEL dataset and 71.17 on HumanML3D (Restore), outperforming prior works.","Finally, we showcase the potential of our approach in corpus-level moment retrieval.","Our source code is openly accessible at https://github.com/eanson023/mlp."],"url":"http://arxiv.org/abs/2404.13657v1","category":"cs.CV"}
{"created":"2024-04-21 13:11:59","title":"SPGNN: Recognizing Salient Subgraph Patterns via Enhanced Graph Convolution and Pooling","abstract":"Graph neural networks (GNNs) have revolutionized the field of machine learning on non-Euclidean data such as graphs and networks. GNNs effectively implement node representation learning through neighborhood aggregation and achieve impressive results in many graph-related tasks. However, most neighborhood aggregation approaches are summation-based, which can be problematic as they may not be sufficiently expressive to encode informative graph structures. Furthermore, though the graph pooling module is also of vital importance for graph learning, especially for the task of graph classification, research on graph down-sampling mechanisms is rather limited.   To address the above challenges, we propose a concatenation-based graph convolution mechanism that injectively updates node representations to maximize the discriminative power in distinguishing non-isomorphic subgraphs. In addition, we design a novel graph pooling module, called WL-SortPool, to learn important subgraph patterns in a deep-learning manner. WL-SortPool layer-wise sorts node representations (i.e. continuous WL colors) to separately learn the relative importance of subtrees with different depths for the purpose of classification, thus better characterizing the complex graph topology and rich information encoded in the graph. We propose a novel Subgraph Pattern GNN (SPGNN) architecture that incorporates these enhancements. We test the proposed SPGNN architecture on many graph classification benchmarks. Experimental results show that our method can achieve highly competitive results with state-of-the-art graph kernels and other GNN approaches.","sentences":["Graph neural networks (GNNs) have revolutionized the field of machine learning on non-Euclidean data such as graphs and networks.","GNNs effectively implement node representation learning through neighborhood aggregation and achieve impressive results in many graph-related tasks.","However, most neighborhood aggregation approaches are summation-based, which can be problematic as they may not be sufficiently expressive to encode informative graph structures.","Furthermore, though the graph pooling module is also of vital importance for graph learning, especially for the task of graph classification, research on graph down-sampling mechanisms is rather limited.   ","To address the above challenges, we propose a concatenation-based graph convolution mechanism that injectively updates node representations to maximize the discriminative power in distinguishing non-isomorphic subgraphs.","In addition, we design a novel graph pooling module, called WL-SortPool, to learn important subgraph patterns in a deep-learning manner.","WL-SortPool layer-wise sorts node representations (i.e. continuous WL colors) to separately learn the relative importance of subtrees with different depths for the purpose of classification, thus better characterizing the complex graph topology and rich information encoded in the graph.","We propose a novel Subgraph Pattern GNN (SPGNN) architecture that incorporates these enhancements.","We test the proposed SPGNN architecture on many graph classification benchmarks.","Experimental results show that our method can achieve highly competitive results with state-of-the-art graph kernels and other GNN approaches."],"url":"http://arxiv.org/abs/2404.13655v1","category":"cs.LG"}
{"created":"2024-04-21 13:04:58","title":"BANSAI: Towards Bridging the AI Adoption Gap in Industrial Robotics with Neurosymbolic Programming","abstract":"Over the past decade, deep learning helped solve manipulation problems across all domains of robotics. At the same time, industrial robots continue to be programmed overwhelmingly using traditional program representations and interfaces. This paper undertakes an analysis of this \"AI adoption gap\" from an industry practitioner's perspective. In response, we propose the BANSAI approach (Bridging the AI Adoption Gap via Neurosymbolic AI). It systematically leverages principles of neurosymbolic AI to establish data-driven, subsymbolic program synthesis and optimization in modern industrial robot programming workflow. BANSAI conceptually unites several lines of prior research and proposes a path toward practical, real-world validation.","sentences":["Over the past decade, deep learning helped solve manipulation problems across all domains of robotics.","At the same time, industrial robots continue to be programmed overwhelmingly using traditional program representations and interfaces.","This paper undertakes an analysis of this \"AI adoption gap\" from an industry practitioner's perspective.","In response, we propose the BANSAI approach (Bridging the AI Adoption Gap via Neurosymbolic AI).","It systematically leverages principles of neurosymbolic AI to establish data-driven, subsymbolic program synthesis and optimization in modern industrial robot programming workflow.","BANSAI conceptually unites several lines of prior research and proposes a path toward practical, real-world validation."],"url":"http://arxiv.org/abs/2404.13652v1","category":"cs.RO"}
{"created":"2024-04-21 12:16:38","title":"Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks","abstract":"Synthetic data generation offers a promising solution to enhance the usefulness of Electronic Healthcare Records (EHR) by generating realistic de-identified data. However, the existing literature primarily focuses on the quality of synthetic health data, neglecting the crucial aspect of fairness in downstream predictions. Consequently, models trained on synthetic EHR have faced criticism for producing biased outcomes in target tasks. These biases can arise from either spurious correlations between features or the failure of models to accurately represent sub-groups. To address these concerns, we present Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based synthetic data generator specifically designed for the healthcare domain. In order to tackle spurious correlations (i), we propose an information-constrained Data Generation Process that enables the generator to learn a fair deterministic transformation based on a well-defined notion of algorithmic fairness. To overcome the challenge of capturing exact sub-group representations (ii), we incentivize the generator to preserve sub-group densities through score-based weighted sampling. This approach compels the generator to learn from underrepresented regions of the data manifold. We conduct extensive experiments using the MIMIC-III database. Our results demonstrate that Bt-GAN achieves SOTA accuracy while significantly improving fairness and minimizing bias amplification. We also perform an in-depth explainability analysis to provide additional evidence supporting the validity of our study. In conclusion, our research introduces a novel and professional approach to addressing the limitations of synthetic data generation in the healthcare domain. By incorporating fairness considerations and leveraging advanced techniques such as GANs, we pave the way for more reliable and unbiased predictions in healthcare applications.","sentences":["Synthetic data generation offers a promising solution to enhance the usefulness of Electronic Healthcare Records (EHR) by generating realistic de-identified data.","However, the existing literature primarily focuses on the quality of synthetic health data, neglecting the crucial aspect of fairness in downstream predictions.","Consequently, models trained on synthetic EHR have faced criticism for producing biased outcomes in target tasks.","These biases can arise from either spurious correlations between features or the failure of models to accurately represent sub-groups.","To address these concerns, we present Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based synthetic data generator specifically designed for the healthcare domain.","In order to tackle spurious correlations (i), we propose an information-constrained Data Generation Process that enables the generator to learn a fair deterministic transformation based on a well-defined notion of algorithmic fairness.","To overcome the challenge of capturing exact sub-group representations (ii), we incentivize the generator to preserve sub-group densities through score-based weighted sampling.","This approach compels the generator to learn from underrepresented regions of the data manifold.","We conduct extensive experiments using the MIMIC-III database.","Our results demonstrate that Bt-GAN achieves SOTA accuracy while significantly improving fairness and minimizing bias amplification.","We also perform an in-depth explainability analysis to provide additional evidence supporting the validity of our study.","In conclusion, our research introduces a novel and professional approach to addressing the limitations of synthetic data generation in the healthcare domain.","By incorporating fairness considerations and leveraging advanced techniques such as GANs, we pave the way for more reliable and unbiased predictions in healthcare applications."],"url":"http://arxiv.org/abs/2404.13634v1","category":"cs.LG"}
{"created":"2024-04-21 12:06:05","title":"Utilizing Deep Learning to Optimize Software Development Processes","abstract":"This study explores the application of deep learning technologies in software development processes, particularly in automating code reviews, error prediction, and test generation to enhance code quality and development efficiency. Through a series of empirical studies, experimental groups using deep learning tools and control groups using traditional methods were compared in terms of code error rates and project completion times. The results demonstrated significant improvements in the experimental group, validating the effectiveness of deep learning technologies. The research also discusses potential optimization points, methodologies, and technical challenges of deep learning in software development, as well as how to integrate these technologies into existing software development workflows.","sentences":["This study explores the application of deep learning technologies in software development processes, particularly in automating code reviews, error prediction, and test generation to enhance code quality and development efficiency.","Through a series of empirical studies, experimental groups using deep learning tools and control groups using traditional methods were compared in terms of code error rates and project completion times.","The results demonstrated significant improvements in the experimental group, validating the effectiveness of deep learning technologies.","The research also discusses potential optimization points, methodologies, and technical challenges of deep learning in software development, as well as how to integrate these technologies into existing software development workflows."],"url":"http://arxiv.org/abs/2404.13630v1","category":"cs.SE"}
{"created":"2024-04-21 11:51:13","title":"NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding","abstract":"Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.","sentences":["Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability.","Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios.","This poses a pressing demand to develop new real-world scenario benchmarks.","We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions).","Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models.","Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method."],"url":"http://arxiv.org/abs/2404.13627v1","category":"cs.CL"}
{"created":"2024-04-21 10:26:13","title":"CKGConv: General Graph Convolution with Continuous Kernels","abstract":"The existing definitions of graph convolution, either from spatial or spectral perspectives, are inflexible and not unified. Defining a general convolution operator in the graph domain is challenging due to the lack of canonical coordinates, the presence of irregular structures, and the properties of graph symmetries. In this work, we propose a novel graph convolution framework by parameterizing the kernels as continuous functions of pseudo-coordinates derived via graph positional encoding. We name this Continuous Kernel Graph Convolution (CKGConv). Theoretically, we demonstrate that CKGConv is flexible and expressive. CKGConv encompasses many existing graph convolutions, and exhibits the same expressiveness as graph transformers in terms of distinguishing non-isomorphic graphs. Empirically, we show that CKGConv-based Networks outperform existing graph convolutional networks and perform comparably to the best graph transformers across a variety of graph datasets.","sentences":["The existing definitions of graph convolution, either from spatial or spectral perspectives, are inflexible and not unified.","Defining a general convolution operator in the graph domain is challenging due to the lack of canonical coordinates, the presence of irregular structures, and the properties of graph symmetries.","In this work, we propose a novel graph convolution framework by parameterizing the kernels as continuous functions of pseudo-coordinates derived via graph positional encoding.","We name this Continuous Kernel Graph Convolution (CKGConv).","Theoretically, we demonstrate that CKGConv is flexible and expressive.","CKGConv encompasses many existing graph convolutions, and exhibits the same expressiveness as graph transformers in terms of distinguishing non-isomorphic graphs.","Empirically, we show that CKGConv-based Networks outperform existing graph convolutional networks and perform comparably to the best graph transformers across a variety of graph datasets."],"url":"http://arxiv.org/abs/2404.13604v1","category":"cs.LG"}
{"created":"2024-04-21 10:02:23","title":"Are We Ready for Planetary Exploration Robots? The TAIL-Plus Dataset for SLAM in Granular Environments","abstract":"So far, planetary surface exploration depends on various mobile robot platforms. The autonomous navigation and decision-making of these mobile robots in complex terrains largely rely on their terrain-aware perception, localization and mapping capabilities. In this paper we release the TAIL-Plus dataset, a new challenging dataset in deformable granular environments for planetary exploration robots, which is an extension to our previous work, TAIL (Terrain-Aware multI-modaL) dataset. We conducted field experiments on beaches that are considered as planetary surface analog environments for diverse sandy terrains. In TAIL-Plus dataset, we provide more sequences with multiple loops and expand the scene from day to night. Benefit from our sensor suite with modular design, we use both wheeled and quadruped robots for data collection. The sensors include a 3D LiDAR, three downward RGB-D cameras, a pair of global-shutter color cameras that can be used as a forward-looking stereo camera, an RTK-GPS device and an extra IMU. Our datasets are intended to help researchers developing multi-sensor simultaneous localization and mapping (SLAM) algorithms for robots in unstructured, deformable granular terrains. Our datasets and supplementary materials will be available at \\url{https://tailrobot.github.io/}.","sentences":["So far, planetary surface exploration depends on various mobile robot platforms.","The autonomous navigation and decision-making of these mobile robots in complex terrains largely rely on their terrain-aware perception, localization and mapping capabilities.","In this paper we release the TAIL-Plus dataset, a new challenging dataset in deformable granular environments for planetary exploration robots, which is an extension to our previous work, TAIL (Terrain-Aware multI-modaL) dataset.","We conducted field experiments on beaches that are considered as planetary surface analog environments for diverse sandy terrains.","In TAIL-Plus dataset, we provide more sequences with multiple loops and expand the scene from day to night.","Benefit from our sensor suite with modular design, we use both wheeled and quadruped robots for data collection.","The sensors include a 3D LiDAR, three downward RGB-D cameras, a pair of global-shutter color cameras that can be used as a forward-looking stereo camera, an RTK-GPS device and an extra IMU.","Our datasets are intended to help researchers developing multi-sensor simultaneous localization and mapping (SLAM) algorithms for robots in unstructured, deformable granular terrains.","Our datasets and supplementary materials will be available at \\url{https://tailrobot.github.io/}."],"url":"http://arxiv.org/abs/2404.13600v1","category":"cs.RO"}
{"created":"2024-04-21 09:35:49","title":"An Integrated Communication and Computing Scheme for Wi-Fi Networks based on Generative AI and Reinforcement Learning","abstract":"The continuous evolution of future mobile communication systems is heading towards the integration of communication and computing, with Mobile Edge Computing (MEC) emerging as a crucial means of implementing Artificial Intelligence (AI) computation. MEC could enhance the computational performance of wireless edge networks by offloading computing-intensive tasks to MEC servers. However, in edge computing scenarios, the sparse sample problem may lead to high costs of time-consuming model training. This paper proposes an MEC offloading decision and resource allocation solution that combines generative AI and deep reinforcement learning (DRL) for the communication-computing integration scenario in the 802.11ax Wi-Fi network. Initially, the optimal offloading policy is determined by the joint use of the Generative Diffusion Model (GDM) and the Twin Delayed DDPG (TD3) algorithm. Subsequently, resource allocation is accomplished by using the Hungarian algorithm. Simulation results demonstrate that the introduction of Generative AI significantly reduces model training costs, and the proposed solution exhibits significant reductions in system task processing latency and total energy consumption costs.","sentences":["The continuous evolution of future mobile communication systems is heading towards the integration of communication and computing, with Mobile Edge Computing (MEC) emerging as a crucial means of implementing Artificial Intelligence (AI) computation.","MEC could enhance the computational performance of wireless edge networks by offloading computing-intensive tasks to MEC servers.","However, in edge computing scenarios, the sparse sample problem may lead to high costs of time-consuming model training.","This paper proposes an MEC offloading decision and resource allocation solution that combines generative AI and deep reinforcement learning (DRL) for the communication-computing integration scenario in the 802.11ax Wi-Fi network.","Initially, the optimal offloading policy is determined by the joint use of the Generative Diffusion Model (GDM) and the Twin Delayed DDPG (TD3) algorithm.","Subsequently, resource allocation is accomplished by using the Hungarian algorithm.","Simulation results demonstrate that the introduction of Generative AI significantly reduces model training costs, and the proposed solution exhibits significant reductions in system task processing latency and total energy consumption costs."],"url":"http://arxiv.org/abs/2404.13598v1","category":"cs.NI"}
{"created":"2024-04-21 09:23:36","title":"Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers","abstract":"An effective method for combining frozen large language models (LLM) and visual encoders involves a resampler module that creates a `visual prompt' which is provided to the LLM, along with the textual prompt. While this approach has enabled impressive performance across many coarse-grained tasks like image captioning and visual question answering, more fine-grained tasks that require spatial understanding have not been thoroughly examined. In this paper, we use \\textit{diagnostic classifiers} to measure the extent to which the visual prompt produced by the resampler encodes spatial information. Our results show that this information is largely absent from the resampler output when kept frozen during training of the classifiers. However, when the resampler and classifier are trained jointly, we observe a significant performance boost. This shows that the compression achieved by the resamplers can in principle encode the requisite spatial information, but that more object-aware objectives are needed at the pretraining stage to facilitate this capability","sentences":["An effective method for combining frozen large language models (LLM) and visual encoders involves a resampler module that creates a `visual prompt' which is provided to the LLM, along with the textual prompt.","While this approach has enabled impressive performance across many coarse-grained tasks like image captioning and visual question answering, more fine-grained tasks that require spatial understanding have not been thoroughly examined.","In this paper, we use \\textit{diagnostic classifiers} to measure the extent to which the visual prompt produced by the resampler encodes spatial information.","Our results show that this information is largely absent from the resampler output when kept frozen during training of the classifiers.","However, when the resampler and classifier are trained jointly, we observe a significant performance boost.","This shows that the compression achieved by the resamplers can in principle encode the requisite spatial information, but that more object-aware objectives are needed at the pretraining stage to facilitate this capability"],"url":"http://arxiv.org/abs/2404.13594v1","category":"cs.CV"}
{"created":"2024-04-21 09:09:21","title":"Machine Unlearning via Null Space Calibration","abstract":"Machine unlearning aims to enable models to forget specific data instances when receiving deletion requests. Current research centres on efficient unlearning to erase the influence of data from the model and neglects the subsequent impacts on the remaining data. Consequently, existing unlearning algorithms degrade the model's performance after unlearning, known as \\textit{over-unlearning}. This paper addresses this critical yet under-explored issue by introducing machine \\underline{U}nlearning via \\underline{N}ull \\underline{S}pace \\underline{C}alibration (UNSC), which can accurately unlearn target samples without over-unlearning. On the contrary, by calibrating the decision space during unlearning, UNSC can significantly improve the model's performance on the remaining samples. In particular, our approach hinges on confining the unlearning process to a specified null space tailored to the remaining samples, which is augmented by strategically pseudo-labeling the unlearning samples. Comparative analyses against several established baselines affirm the superiority of our approach. Code is released at this \\href{https://github.com/HQC-ML/Machine-Unlearning-via-Null-Space-Calibration}{URL}.","sentences":["Machine unlearning aims to enable models to forget specific data instances when receiving deletion requests.","Current research centres on efficient unlearning to erase the influence of data from the model and neglects the subsequent impacts on the remaining data.","Consequently, existing unlearning algorithms degrade the model's performance after unlearning, known as \\textit{over-unlearning}.","This paper addresses this critical yet under-explored issue by introducing machine \\underline{U}nlearning via \\underline{N}ull \\underline{S}pace \\underline{C}alibration (UNSC), which can accurately unlearn target samples without over-unlearning.","On the contrary, by calibrating the decision space during unlearning, UNSC can significantly improve the model's performance on the remaining samples.","In particular, our approach hinges on confining the unlearning process to a specified null space tailored to the remaining samples, which is augmented by strategically pseudo-labeling the unlearning samples.","Comparative analyses against several established baselines affirm the superiority of our approach.","Code is released at this \\href{https://github.com/HQC-ML/Machine-Unlearning-via-Null-Space-Calibration}{URL}."],"url":"http://arxiv.org/abs/2404.13588v1","category":"cs.LG"}
{"created":"2024-04-21 08:51:52","title":"Radial Basis Function Neural Networks for Formation Control of Unmanned Aerial Vehicles","abstract":"This paper addresses the problem of controlling multiple unmanned aerial vehicles (UAVs) cooperating in a formation to carry out a complex task such as surface inspection. We first use the virtual leader-follower model to determine the topology and trajectory of the formation. A double-loop control system combining backstepping and sliding mode control techniques is then designed for the UAVs to track the trajectory. A radial basis function neural network (RBFNN) capable of estimating external disturbances is developed to enhance the robustness of the controller. The stability of the controller is proven by using the Lyapunov theorem. A number of comparisons and software-in-the-loop (SIL) tests have been conducted to evaluate the performance of the proposed controller. The results show that our controller not only outperforms other state-of-the-art controllers but is also sufficient for complex tasks of UAVs such as collecting surface data for inspection. The source code of our controller can be found at https://github.com/duynamrcv/rbf_bsmc","sentences":["This paper addresses the problem of controlling multiple unmanned aerial vehicles (UAVs) cooperating in a formation to carry out a complex task such as surface inspection.","We first use the virtual leader-follower model to determine the topology and trajectory of the formation.","A double-loop control system combining backstepping and sliding mode control techniques is then designed for the UAVs to track the trajectory.","A radial basis function neural network (RBFNN) capable of estimating external disturbances is developed to enhance the robustness of the controller.","The stability of the controller is proven by using the Lyapunov theorem.","A number of comparisons and software-in-the-loop (SIL) tests have been conducted to evaluate the performance of the proposed controller.","The results show that our controller not only outperforms other state-of-the-art controllers but is also sufficient for complex tasks of UAVs such as collecting surface data for inspection.","The source code of our controller can be found at https://github.com/duynamrcv/rbf_bsmc"],"url":"http://arxiv.org/abs/2404.13583v1","category":"cs.RO"}
{"created":"2024-04-21 08:41:21","title":"Preliminary Investigation of SSL for Complex Work Activity Recognition in Industrial Domain via MoIL","abstract":"In this study, we investigate a new self-supervised learning (SSL) approach for complex work activity recognition using wearable sensors. Owing to the cost of labeled sensor data collection, SSL methods for human activity recognition (HAR) that effectively use unlabeled data for pretraining have attracted attention. However, applying prior SSL to complex work activities such as packaging works is challenging because the observed data vary considerably depending on situations such as the number of items to pack and the size of the items in the case of packaging works. In this study, we focus on sensor data corresponding to characteristic and necessary actions (sensor data motifs) in a specific activity such as a stretching packing tape action in an assembling a box activity, and \\textcolor{black}{try} to train a neural network in self-supervised learning so that it identifies occurrences of the characteristic actions, i.e., Motif Identification Learning (MoIL). The feature extractor in the network is used in the downstream task, i.e., work activity recognition, enabling precise activity recognition containing characteristic actions with limited labeled training data. The MoIL approach was evaluated on real-world work activity data and it achieved state-of-the-art performance under limited training labels.","sentences":["In this study, we investigate a new self-supervised learning (SSL) approach for complex work activity recognition using wearable sensors.","Owing to the cost of labeled sensor data collection, SSL methods for human activity recognition (HAR) that effectively use unlabeled data for pretraining have attracted attention.","However, applying prior SSL to complex work activities such as packaging works is challenging because the observed data vary considerably depending on situations such as the number of items to pack and the size of the items in the case of packaging works.","In this study, we focus on sensor data corresponding to characteristic and necessary actions (sensor data motifs) in a specific activity such as a stretching packing tape action in an assembling a box activity, and \\textcolor{black}{try} to train a neural network in self-supervised learning so that it identifies occurrences of the characteristic actions, i.e., Motif Identification Learning (MoIL).","The feature extractor in the network is used in the downstream task, i.e., work activity recognition, enabling precise activity recognition containing characteristic actions with limited labeled training data.","The MoIL approach was evaluated on real-world work activity data and it achieved state-of-the-art performance under limited training labels."],"url":"http://arxiv.org/abs/2404.13581v1","category":"cs.HC"}
{"created":"2024-04-21 08:37:43","title":"LTOS: Layout-controllable Text-Object Synthesis via Adaptive Cross-attention Fusions","abstract":"Controllable text-to-image generation synthesizes visual text and objects in images with certain conditions, which are frequently applied to emoji and poster generation. Visual text rendering and layout-to-image generation tasks have been popular in controllable text-to-image generation. However, each of these tasks typically focuses on single modality generation or rendering, leaving yet-to-be-bridged gaps between the approaches correspondingly designed for each of the tasks. In this paper, we combine text rendering and layout-to-image generation tasks into a single task: layout-controllable text-object synthesis (LTOS) task, aiming at synthesizing images with object and visual text based on predefined object layout and text contents. As compliant datasets are not readily available for our LTOS task, we construct a layout-aware text-object synthesis dataset, containing elaborate well-aligned labels of visual text and object information. Based on the dataset, we propose a layout-controllable text-object adaptive fusion (TOF) framework, which generates images with clear, legible visual text and plausible objects. We construct a visual-text rendering module to synthesize text and employ an object-layout control module to generate objects while integrating the two modules to harmoniously generate and integrate text content and objects in images. To better the image-text integration, we propose a self-adaptive cross-attention fusion module that helps the image generation to attend more to important text information. Within such a fusion module, we use a self-adaptive learnable factor to learn to flexibly control the influence of cross-attention outputs on image generation. Experimental results show that our method outperforms the state-of-the-art in LTOS, text rendering, and layout-to-image tasks, enabling harmonious visual text rendering and object generation.","sentences":["Controllable text-to-image generation synthesizes visual text and objects in images with certain conditions, which are frequently applied to emoji and poster generation.","Visual text rendering and layout-to-image generation tasks have been popular in controllable text-to-image generation.","However, each of these tasks typically focuses on single modality generation or rendering, leaving yet-to-be-bridged gaps between the approaches correspondingly designed for each of the tasks.","In this paper, we combine text rendering and layout-to-image generation tasks into a single task: layout-controllable text-object synthesis (LTOS) task, aiming at synthesizing images with object and visual text based on predefined object layout and text contents.","As compliant datasets are not readily available for our LTOS task, we construct a layout-aware text-object synthesis dataset, containing elaborate well-aligned labels of visual text and object information.","Based on the dataset, we propose a layout-controllable text-object adaptive fusion (TOF) framework, which generates images with clear, legible visual text and plausible objects.","We construct a visual-text rendering module to synthesize text and employ an object-layout control module to generate objects while integrating the two modules to harmoniously generate and integrate text content and objects in images.","To better the image-text integration, we propose a self-adaptive cross-attention fusion module that helps the image generation to attend more to important text information.","Within such a fusion module, we use a self-adaptive learnable factor to learn to flexibly control the influence of cross-attention outputs on image generation.","Experimental results show that our method outperforms the state-of-the-art in LTOS, text rendering, and layout-to-image tasks, enabling harmonious visual text rendering and object generation."],"url":"http://arxiv.org/abs/2404.13579v1","category":"cs.CV"}
{"created":"2024-04-21 08:27:36","title":"FedMPQ: Secure and Communication-Efficient Federated Learning with Multi-codebook Product Quantization","abstract":"In federated learning, particularly in cross-device scenarios, secure aggregation has recently gained popularity as it effectively defends against inference attacks by malicious aggregators. However, secure aggregation often requires additional communication overhead and can impede the convergence rate of the global model, which is particularly challenging in wireless network environments with extremely limited bandwidth. Therefore, achieving efficient communication compression under the premise of secure aggregation presents a highly challenging and valuable problem. In this work, we propose a novel uplink communication compression method for federated learning, named FedMPQ, which is based on multi shared codebook product quantization.Specifically, we utilize updates from the previous round to generate sufficiently robust codebooks. Secure aggregation is then achieved through trusted execution environments (TEE) or a trusted third party (TTP).In contrast to previous works, our approach exhibits greater robustness in scenarios where data is not independently and identically distributed (non-IID) and there is a lack of sufficient public data. The experiments conducted on the LEAF dataset demonstrate that our proposed method achieves 99% of the baseline's final accuracy, while reducing uplink communications by 90-95%","sentences":["In federated learning, particularly in cross-device scenarios, secure aggregation has recently gained popularity as it effectively defends against inference attacks by malicious aggregators.","However, secure aggregation often requires additional communication overhead and can impede the convergence rate of the global model, which is particularly challenging in wireless network environments with extremely limited bandwidth.","Therefore, achieving efficient communication compression under the premise of secure aggregation presents a highly challenging and valuable problem.","In this work, we propose a novel uplink communication compression method for federated learning, named FedMPQ, which is based on multi shared codebook product quantization.","Specifically, we utilize updates from the previous round to generate sufficiently robust codebooks.","Secure aggregation is then achieved through trusted execution environments (TEE) or a trusted third party (TTP).In contrast to previous works, our approach exhibits greater robustness in scenarios where data is not independently and identically distributed (non-IID) and there is a lack of sufficient public data.","The experiments conducted on the LEAF dataset demonstrate that our proposed method achieves 99% of the baseline's final accuracy, while reducing uplink communications by 90-95%"],"url":"http://arxiv.org/abs/2404.13575v1","category":"cs.CR"}
{"created":"2024-04-21 08:27:20","title":"Exploring AIGC Video Quality: A Focus on Visual Harmony, Video-Text Consistency and Domain Distribution Gap","abstract":"The recent advancements in Text-to-Video Artificial Intelligence Generated Content (AIGC) have been remarkable. Compared with traditional videos, the assessment of AIGC videos encounters various challenges: visual inconsistency that defy common sense, discrepancies between content and the textual prompt, and distribution gap between various generative models, etc. Target at these challenges, in this work, we categorize the assessment of AIGC video quality into three dimensions: visual harmony, video-text consistency, and domain distribution gap. For each dimension, we design specific modules to provide a comprehensive quality assessment of AIGC videos. Furthermore, our research identifies significant variations in visual quality, fluidity, and style among videos generated by different text-to-video models. Predicting the source generative model can make the AIGC video features more discriminative, which enhances the quality assessment performance. The proposed method was used in the third-place winner of the NTIRE 2024 Quality Assessment for AI-Generated Content - Track 2 Video, demonstrating its effectiveness.","sentences":["The recent advancements in Text-to-Video Artificial Intelligence Generated Content (AIGC) have been remarkable.","Compared with traditional videos, the assessment of AIGC videos encounters various challenges: visual inconsistency that defy common sense, discrepancies between content and the textual prompt, and distribution gap between various generative models, etc. Target at these challenges, in this work, we categorize the assessment of AIGC video quality into three dimensions: visual harmony, video-text consistency, and domain distribution gap.","For each dimension, we design specific modules to provide a comprehensive quality assessment of AIGC videos.","Furthermore, our research identifies significant variations in visual quality, fluidity, and style among videos generated by different text-to-video models.","Predicting the source generative model can make the AIGC video features more discriminative, which enhances the quality assessment performance.","The proposed method was used in the third-place winner of the NTIRE 2024 Quality Assessment for AI-Generated Content - Track 2 Video, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2404.13573v1","category":"cs.CV"}
{"created":"2024-04-21 08:20:02","title":"Test-Time Training on Graphs with Large Language Models (LLMs)","abstract":"Graph Neural Networks have demonstrated great success in various fields of multimedia. However, the distribution shift between the training and test data challenges the effectiveness of GNNs. To mitigate this challenge, Test-Time Training (TTT) has been proposed as a promising approach. Traditional TTT methods require a demanding unsupervised training strategy to capture the information from test to benefit the main task. Inspired by the great annotation ability of Large Language Models (LLMs) on Text-Attributed Graphs (TAGs), we propose to enhance the test-time training on graphs with LLMs as annotators. In this paper, we design a novel Test-Time Training pipeline, LLMTTT, which conducts the test-time adaptation under the annotations by LLMs on a carefully-selected node set. Specifically, LLMTTT introduces a hybrid active node selection strategy that considers not only node diversity and representativeness, but also prediction signals from the pre-trained model. Given annotations from LLMs, a two-stage training strategy is designed to tailor the test-time model with the limited and noisy labels. A theoretical analysis ensures the validity of our method and extensive experiments demonstrate that the proposed LLMTTT can achieve a significant performance improvement compared to existing Out-of-Distribution (OOD) generalization methods.","sentences":["Graph Neural Networks have demonstrated great success in various fields of multimedia.","However, the distribution shift between the training and test data challenges the effectiveness of GNNs.","To mitigate this challenge, Test-Time Training (TTT) has been proposed as a promising approach.","Traditional TTT methods require a demanding unsupervised training strategy to capture the information from test to benefit the main task.","Inspired by the great annotation ability of Large Language Models (LLMs) on Text-Attributed Graphs (TAGs), we propose to enhance the test-time training on graphs with LLMs as annotators.","In this paper, we design a novel Test-Time Training pipeline, LLMTTT, which conducts the test-time adaptation under the annotations by LLMs on a carefully-selected node set.","Specifically, LLMTTT introduces a hybrid active node selection strategy that considers not only node diversity and representativeness, but also prediction signals from the pre-trained model.","Given annotations from LLMs, a two-stage training strategy is designed to tailor the test-time model with the limited and noisy labels.","A theoretical analysis ensures the validity of our method and extensive experiments demonstrate that the proposed LLMTTT can achieve a significant performance improvement compared to existing Out-of-Distribution (OOD) generalization methods."],"url":"http://arxiv.org/abs/2404.13571v1","category":"cs.LG"}
{"created":"2024-04-21 07:57:45","title":"On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis","abstract":"A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would help answer the question of what a deep learning system internally detects as relevant in the input, demystifying the otherwise black-box nature of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. This is particularly the case for approaches that can both draw explanations from substantial background knowledge, and that are based on inherently explainable (symbolic) methods.   In this paper, we introduce a novel model-agnostic post-hoc Explainable AI method demonstrating that it provides meaningful interpretations. Our approach is based on using a Wikipedia-derived concept hierarchy with approximately 2 million classes as background knowledge, and utilizes OWL-reasoning-based Concept Induction for explanation generation. Additionally, we explore and compare the capabilities of off-the-shelf pre-trained multimodal-based explainable methods.   Our results indicate that our approach can automatically attach meaningful class expressions as explanations to individual neurons in the dense layer of a Convolutional Neural Network. Evaluation through statistical analysis and degree of concept activation in the hidden layer show that our method provides a competitive edge in both quantitative and qualitative aspects compared to prior work.","sentences":["A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would help answer the question of what a deep learning system internally detects as relevant in the input, demystifying the otherwise black-box nature of deep learning systems.","The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored.","This is particularly the case for approaches that can both draw explanations from substantial background knowledge, and that are based on inherently explainable (symbolic) methods.   ","In this paper, we introduce a novel model-agnostic post-hoc Explainable AI method demonstrating that it provides meaningful interpretations.","Our approach is based on using a Wikipedia-derived concept hierarchy with approximately 2 million classes as background knowledge, and utilizes OWL-reasoning-based Concept Induction for explanation generation.","Additionally, we explore and compare the capabilities of off-the-shelf pre-trained multimodal-based explainable methods.   ","Our results indicate that our approach can automatically attach meaningful class expressions as explanations to individual neurons in the dense layer of a Convolutional Neural Network.","Evaluation through statistical analysis and degree of concept activation in the hidden layer show that our method provides a competitive edge in both quantitative and qualitative aspects compared to prior work."],"url":"http://arxiv.org/abs/2404.13567v1","category":"cs.AI"}
{"created":"2024-04-21 07:34:44","title":"Exploring Diverse Methods in Visual Question Answering","abstract":"This study explores innovative methods for improving Visual Question Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and attention mechanisms. Leveraging a balanced VQA dataset, we investigate three distinct strategies. Firstly, GAN-based approaches aim to generate answer embeddings conditioned on image and question inputs, showing potential but struggling with more complex tasks. Secondly, autoencoder-based techniques focus on learning optimal embeddings for questions and images, achieving comparable results with GAN due to better ability on complex questions. Lastly, attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB), address language priors and attention modeling, albeit with a complexity-performance trade-off. This study underscores the challenges and opportunities in VQA and suggests avenues for future research, including alternative GAN formulations and attentional mechanisms.","sentences":["This study explores innovative methods for improving Visual Question Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and attention mechanisms.","Leveraging a balanced VQA dataset, we investigate three distinct strategies.","Firstly, GAN-based approaches aim to generate answer embeddings conditioned on image and question inputs, showing potential but struggling with more complex tasks.","Secondly, autoencoder-based techniques focus on learning optimal embeddings for questions and images, achieving comparable results with GAN due to better ability on complex questions.","Lastly, attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB), address language priors and attention modeling, albeit with a complexity-performance trade-off.","This study underscores the challenges and opportunities in VQA and suggests avenues for future research, including alternative GAN formulations and attentional mechanisms."],"url":"http://arxiv.org/abs/2404.13565v1","category":"cs.CV"}
{"created":"2024-04-21 07:26:09","title":"Masked Latent Transformer with the Random Masking Ratio to Advance the Diagnosis of Dental Fluorosis","abstract":"Dental fluorosis is a chronic disease caused by long-term overconsumption of fluoride, which leads to changes in the appearance of tooth enamel. It is an important basis for early non-invasive diagnosis of endemic fluorosis. However, even dental professionals may not be able to accurately distinguish dental fluorosis and its severity based on tooth images. Currently, there is still a gap in research on applying deep learning to diagnosing dental fluorosis. Therefore, we construct the first open-source dental fluorosis image dataset (DFID), laying the foundation for deep learning research in this field. To advance the diagnosis of dental fluorosis, we propose a pioneering deep learning model called masked latent transformer with the random masking ratio (MLTrMR). MLTrMR introduces a mask latent modeling scheme based on Vision Transformer to enhance contextual learning of dental fluorosis lesion characteristics. Consisting of a latent embedder, encoder, and decoder, MLTrMR employs the latent embedder to extract latent tokens from the original image, whereas the encoder and decoder comprising the latent transformer (LT) block are used to process unmasked tokens and predict masked tokens, respectively. To mitigate the lack of inductive bias in Vision Transformer, which may result in performance degradation, the LT block introduces latent tokens to enhance the learning capacity of latent lesion features. Furthermore, we design an auxiliary loss function to constrain the parameter update direction of the model. MLTrMR achieves 80.19% accuracy, 75.79% F1, and 81.28% quadratic weighted kappa on DFID, making it state-of-the-art (SOTA).","sentences":["Dental fluorosis is a chronic disease caused by long-term overconsumption of fluoride, which leads to changes in the appearance of tooth enamel.","It is an important basis for early non-invasive diagnosis of endemic fluorosis.","However, even dental professionals may not be able to accurately distinguish dental fluorosis and its severity based on tooth images.","Currently, there is still a gap in research on applying deep learning to diagnosing dental fluorosis.","Therefore, we construct the first open-source dental fluorosis image dataset (DFID), laying the foundation for deep learning research in this field.","To advance the diagnosis of dental fluorosis, we propose a pioneering deep learning model called masked latent transformer with the random masking ratio (MLTrMR).","MLTrMR introduces a mask latent modeling scheme based on Vision Transformer to enhance contextual learning of dental fluorosis lesion characteristics.","Consisting of a latent embedder, encoder, and decoder, MLTrMR employs the latent embedder to extract latent tokens from the original image, whereas the encoder and decoder comprising the latent transformer (LT) block are used to process unmasked tokens and predict masked tokens, respectively.","To mitigate the lack of inductive bias in Vision Transformer, which may result in performance degradation, the LT block introduces latent tokens to enhance the learning capacity of latent lesion features.","Furthermore, we design an auxiliary loss function to constrain the parameter update direction of the model.","MLTrMR achieves 80.19% accuracy, 75.79% F1, and 81.28% quadratic weighted kappa on DFID, making it state-of-the-art (SOTA)."],"url":"http://arxiv.org/abs/2404.13564v1","category":"cs.CV"}
{"created":"2024-04-21 07:03:48","title":"Cell Phone Image-Based Persian Rice Detection and Classification Using Deep Learning Techniques","abstract":"This study introduces an innovative approach to classifying various types of Persian rice using image-based deep learning techniques, highlighting the practical application of everyday technology in food categorization. Recognizing the diversity of Persian rice and its culinary significance, we leveraged the capabilities of convolutional neural networks (CNNs), specifically by fine-tuning a ResNet model for accurate identification of different rice varieties and employing a U-Net architecture for precise segmentation of rice grains in bulk images. This dual-methodology framework allows for both individual grain classification and comprehensive analysis of bulk rice samples, addressing two crucial aspects of rice quality assessment. Utilizing images captured with consumer-grade cell phones reflects a realistic scenario in which individuals can leverage this technology for assistance with grocery shopping and meal preparation. The dataset, comprising various rice types photographed under natural conditions without professional lighting or equipment, presents a challenging yet practical classification problem. Our findings demonstrate the feasibility of using non-professional images for food classification and the potential of deep learning models, like ResNet and U-Net, to adapt to the nuances of everyday objects and textures. This study contributes to the field by providing insights into the applicability of image-based deep learning in daily life, specifically for enhancing consumer experiences and knowledge in food selection. Furthermore, it opens avenues for extending this approach to other food categories and practical applications, emphasizing the role of accessible technology in bridging the gap between sophisticated computational methods and everyday tasks.","sentences":["This study introduces an innovative approach to classifying various types of Persian rice using image-based deep learning techniques, highlighting the practical application of everyday technology in food categorization.","Recognizing the diversity of Persian rice and its culinary significance, we leveraged the capabilities of convolutional neural networks (CNNs), specifically by fine-tuning a ResNet model for accurate identification of different rice varieties and employing a U-Net architecture for precise segmentation of rice grains in bulk images.","This dual-methodology framework allows for both individual grain classification and comprehensive analysis of bulk rice samples, addressing two crucial aspects of rice quality assessment.","Utilizing images captured with consumer-grade cell phones reflects a realistic scenario in which individuals can leverage this technology for assistance with grocery shopping and meal preparation.","The dataset, comprising various rice types photographed under natural conditions without professional lighting or equipment, presents a challenging yet practical classification problem.","Our findings demonstrate the feasibility of using non-professional images for food classification and the potential of deep learning models, like ResNet and U-Net, to adapt to the nuances of everyday objects and textures.","This study contributes to the field by providing insights into the applicability of image-based deep learning in daily life, specifically for enhancing consumer experiences and knowledge in food selection.","Furthermore, it opens avenues for extending this approach to other food categories and practical applications, emphasizing the role of accessible technology in bridging the gap between sophisticated computational methods and everyday tasks."],"url":"http://arxiv.org/abs/2404.13555v1","category":"cs.CV"}
{"created":"2024-04-21 05:10:57","title":"Joint Transmit and Reflective Beamforming for Multi-Active-IRS-Assisted Cooperative Sensing","abstract":"This paper studies multi-active intelligent-reflecting-surface (IRS) cooperative sensing, in which multiple active IRSs are deployed in a distributed manner to help the base station (BS) provide multi-view sensing. We focus on the scenario where the sensing target is located in the non-line-of-sight (NLoS) area of the BS. Based on the received echo signal, the BS aims to estimate the target's direction-of-arrival (DoA) with respect to each IRS. In addition, we leverage active IRSs to overcome the severe path loss induced by multi-hop reflections. Under this setup, we minimize the maximum Cram\\'{e}r-Rao bound (CRB) among all IRSs by jointly optimizing the transmit beamforming at the BS and the reflective beamforming at the multiple IRSs, subject to the constraints on the maximum transmit power at the BS, as well as the maximum transmit power and the maximum power amplification gain at individual IRSs. To tackle the resulting highly non-convex max-CRB minimization problem, we propose an efficient algorithm based on alternating optimization, successive convex approximation, and semi-definite relaxation, to obtain a high-quality solution. Finally, numerical results are provided to verify the effectiveness of our proposed design and the benefits of active IRS-assisted sensing compared to the counterpart with passive IRSs.","sentences":["This paper studies multi-active intelligent-reflecting-surface (IRS) cooperative sensing, in which multiple active IRSs are deployed in a distributed manner to help the base station (BS) provide multi-view sensing.","We focus on the scenario where the sensing target is located in the non-line-of-sight (NLoS) area of the BS.","Based on the received echo signal, the BS aims to estimate the target's direction-of-arrival (DoA) with respect to each IRS.","In addition, we leverage active IRSs to overcome the severe path loss induced by multi-hop reflections.","Under this setup, we minimize the maximum Cram\\'{e}r-Rao bound (CRB) among all IRSs by jointly optimizing the transmit beamforming at the BS and the reflective beamforming at the multiple IRSs, subject to the constraints on the maximum transmit power at the BS, as well as the maximum transmit power and the maximum power amplification gain at individual IRSs.","To tackle the resulting highly non-convex max-CRB minimization problem, we propose an efficient algorithm based on alternating optimization, successive convex approximation, and semi-definite relaxation, to obtain a high-quality solution.","Finally, numerical results are provided to verify the effectiveness of our proposed design and the benefits of active IRS-assisted sensing compared to the counterpart with passive IRSs."],"url":"http://arxiv.org/abs/2404.13536v1","category":"cs.IT"}
{"created":"2024-04-21 04:55:13","title":"Listen Then See: Video Alignment with Speaker Attention","abstract":"Video-based Question Answering (Video QA) is a challenging task and becomes even more intricate when addressing Socially Intelligent Question Answering (SIQA). SIQA requires context understanding, temporal reasoning, and the integration of multimodal information, but in addition, it requires processing nuanced human behavior. Furthermore, the complexities involved are exacerbated by the dominance of the primary modality (text) over the others. Thus, there is a need to help the task's secondary modalities to work in tandem with the primary modality. In this work, we introduce a cross-modal alignment and subsequent representation fusion approach that achieves state-of-the-art results (82.06\\% accuracy) on the Social IQ 2.0 dataset for SIQA. Our approach exhibits an improved ability to leverage the video modality by using the audio modality as a bridge with the language modality. This leads to enhanced performance by reducing the prevalent issue of language overfitting and resultant video modality bypassing encountered by current existing techniques. Our code and models are publicly available at https://github.com/sts-vlcc/sts-vlcc","sentences":["Video-based Question Answering (Video QA) is a challenging task and becomes even more intricate when addressing Socially Intelligent Question Answering (SIQA).","SIQA requires context understanding, temporal reasoning, and the integration of multimodal information, but in addition, it requires processing nuanced human behavior.","Furthermore, the complexities involved are exacerbated by the dominance of the primary modality (text) over the others.","Thus, there is a need to help the task's secondary modalities to work in tandem with the primary modality.","In this work, we introduce a cross-modal alignment and subsequent representation fusion approach that achieves state-of-the-art results (82.06\\% accuracy) on the Social IQ 2.0 dataset for SIQA.","Our approach exhibits an improved ability to leverage the video modality by using the audio modality as a bridge with the language modality.","This leads to enhanced performance by reducing the prevalent issue of language overfitting and resultant video modality bypassing encountered by current existing techniques.","Our code and models are publicly available at https://github.com/sts-vlcc/sts-vlcc"],"url":"http://arxiv.org/abs/2404.13530v1","category":"cs.CV"}
{"created":"2024-04-21 04:47:26","title":"SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile","abstract":"This work is motivated by recent developments in Deep Neural Networks, particularly the Transformer architectures underlying applications such as ChatGPT, and the need for performing inference on mobile devices. Focusing on emerging transformers (specifically the ones with computationally efficient Swin-like architectures) and large models (e.g., Stable Diffusion and LLMs) based on transformers, we observe that layout transformations between the computational operators cause a significant slowdown in these applications. This paper presents SmartMem, a comprehensive framework for eliminating most layout transformations, with the idea that multiple operators can use the same tensor layout through careful choice of layout and implementation of operations. Our approach is based on classifying the operators into four groups, and considering combinations of producer-consumer edges between the operators. We develop a set of methods for searching such layouts. Another component of our work is developing efficient memory layouts for 2.5 dimensional memory commonly seen in mobile devices. Our experimental results show that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on mobile devices across 18 varied neural networks, including CNNs, Transformers with both local and global attention, as well as LLMs. In particular, compared to DNNFusion, SmartMem achieves an average speedup of 2.8$\\times$, and outperforms TVM and MNN with speedups of 6.9$\\times$ and 7.9$\\times$, respectively, on average.","sentences":["This work is motivated by recent developments in Deep Neural Networks, particularly the Transformer architectures underlying applications such as ChatGPT, and the need for performing inference on mobile devices.","Focusing on emerging transformers (specifically the ones with computationally efficient Swin-like architectures) and large models (e.g., Stable Diffusion and LLMs) based on transformers, we observe that layout transformations between the computational operators cause a significant slowdown in these applications.","This paper presents SmartMem, a comprehensive framework for eliminating most layout transformations, with the idea that multiple operators can use the same tensor layout through careful choice of layout and implementation of operations.","Our approach is based on classifying the operators into four groups, and considering combinations of producer-consumer edges between the operators.","We develop a set of methods for searching such layouts.","Another component of our work is developing efficient memory layouts for 2.5 dimensional memory commonly seen in mobile devices.","Our experimental results show that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on mobile devices across 18 varied neural networks, including CNNs, Transformers with both local and global attention, as well as LLMs.","In particular, compared to DNNFusion, SmartMem achieves an average speedup of 2.8$\\times$, and outperforms TVM and MNN with speedups of 6.9$\\times$ and 7.9$\\times$, respectively, on average."],"url":"http://arxiv.org/abs/2404.13528v1","category":"cs.LG"}
{"created":"2024-04-21 04:07:52","title":"Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective","abstract":"Shapley value attribution is an increasingly popular explainable AI (XAI) method, which quantifies the contribution of each feature to the model's output. However, recent work has shown that most existing methods to implement Shapley value attributions have some drawbacks. Due to these drawbacks, the resulting Shapley value attributions may provide biased or unreliable explanations, which fail to correctly capture the true intrinsic relationships between features and model outputs. Moreover, it is difficult to evaluate these explanation errors because the true underlying dependencies between features and model outputs are typically unknown. In this paper, we theoretically analyze the explanation errors of Shapley value attributions by decomposing the explanation error into two components: observation bias and structural bias. We also clarify the underlying causes of these two biases and demonstrate that there is a trade-off between them. Based on this error analysis framework, we develop two novel concepts: over-informative and under-informative explanations. We theoretically analyze the potential over-informativeness and under-informativeness of existing Shapley value attribution methods. Particularly for the widely deployed assumption-based Shapley value attributions, we affirm that they can easily be under-informative due to the distribution drift caused by distributional assumptions. We also propose a measurement tool to quantify the distribution drift that causes such errors.","sentences":["Shapley value attribution is an increasingly popular explainable AI (XAI) method, which quantifies the contribution of each feature to the model's output.","However, recent work has shown that most existing methods to implement Shapley value attributions have some drawbacks.","Due to these drawbacks, the resulting Shapley value attributions may provide biased or unreliable explanations, which fail to correctly capture the true intrinsic relationships between features and model outputs.","Moreover, it is difficult to evaluate these explanation errors because the true underlying dependencies between features and model outputs are typically unknown.","In this paper, we theoretically analyze the explanation errors of Shapley value attributions by decomposing the explanation error into two components: observation bias and structural bias.","We also clarify the underlying causes of these two biases and demonstrate that there is a trade-off between them.","Based on this error analysis framework, we develop two novel concepts: over-informative and under-informative explanations.","We theoretically analyze the potential over-informativeness and under-informativeness of existing Shapley value attribution methods.","Particularly for the widely deployed assumption-based Shapley value attributions, we affirm that they can easily be under-informative due to the distribution drift caused by distributional assumptions.","We also propose a measurement tool to quantify the distribution drift that causes such errors."],"url":"http://arxiv.org/abs/2404.13522v1","category":"cs.AI"}
{"created":"2024-04-21 04:06:09","title":"Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces","abstract":"Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. To seize machine learning's potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements' properties and their semantic-visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model's suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.","sentences":["Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up.","They do not encapsulate both semantic and visuo-spatial relationships among elements.","To seize machine learning's potential for GUIs more efficiently, Graph4GUI exploits graph neural networks to capture individual elements' properties and their semantic-visuo-spatial constraints in a layout.","The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI.","The new model's suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference.","Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in."],"url":"http://arxiv.org/abs/2404.13521v1","category":"cs.HC"}
{"created":"2024-04-21 03:38:20","title":"Reliable Model Watermarking: Defending Against Theft without Compromising on Evasion","abstract":"With the rise of Machine Learning as a Service (MLaaS) platforms,safeguarding the intellectual property of deep learning models is becoming paramount. Among various protective measures, trigger set watermarking has emerged as a flexible and effective strategy for preventing unauthorized model distribution. However, this paper identifies an inherent flaw in the current paradigm of trigger set watermarking: evasion adversaries can readily exploit the shortcuts created by models memorizing watermark samples that deviate from the main task distribution, significantly impairing their generalization in adversarial settings. To counteract this, we leverage diffusion models to synthesize unrestricted adversarial examples as trigger sets. By learning the model to accurately recognize them, unique watermark behaviors are promoted through knowledge injection rather than error memorization, thus avoiding exploitable shortcuts. Furthermore, we uncover that the resistance of current trigger set watermarking against removal attacks primarily relies on significantly damaging the decision boundaries during embedding, intertwining unremovability with adverse impacts. By optimizing the knowledge transfer properties of protected models, our approach conveys watermark behaviors to extraction surrogates without aggressively decision boundary perturbation. Experimental results on CIFAR-10/100 and Imagenette datasets demonstrate the effectiveness of our method, showing not only improved robustness against evasion adversaries but also superior resistance to watermark removal attacks compared to state-of-the-art solutions.","sentences":["With the rise of Machine Learning as a Service (MLaaS) platforms,safeguarding the intellectual property of deep learning models is becoming paramount.","Among various protective measures, trigger set watermarking has emerged as a flexible and effective strategy for preventing unauthorized model distribution.","However, this paper identifies an inherent flaw in the current paradigm of trigger set watermarking: evasion adversaries can readily exploit the shortcuts created by models memorizing watermark samples that deviate from the main task distribution, significantly impairing their generalization in adversarial settings.","To counteract this, we leverage diffusion models to synthesize unrestricted adversarial examples as trigger sets.","By learning the model to accurately recognize them, unique watermark behaviors are promoted through knowledge injection rather than error memorization, thus avoiding exploitable shortcuts.","Furthermore, we uncover that the resistance of current trigger set watermarking against removal attacks primarily relies on significantly damaging the decision boundaries during embedding, intertwining unremovability with adverse impacts.","By optimizing the knowledge transfer properties of protected models, our approach conveys watermark behaviors to extraction surrogates without aggressively decision boundary perturbation.","Experimental results on CIFAR-10/100 and Imagenette datasets demonstrate the effectiveness of our method, showing not only improved robustness against evasion adversaries but also superior resistance to watermark removal attacks compared to state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.13518v1","category":"cs.CR"}
{"created":"2024-04-21 03:31:01","title":"FedTrans: Efficient Federated Learning Over Heterogeneous Clients via Model Transformation","abstract":"Federated learning (FL) aims to train machine learning (ML) models across potentially millions of edge client devices. Yet, training and customizing models for FL clients is notoriously challenging due to the heterogeneity of client data, device capabilities, and the massive scale of clients, making individualized model exploration prohibitively expensive. State-of-the-art FL solutions personalize a globally trained model or concurrently train multiple models, but they often incur suboptimal model accuracy and huge training costs.   In this paper, we introduce FedTrans, a multi-model FL training framework that automatically produces and trains high-accuracy, hardware-compatible models for individual clients at scale. FedTrans begins with a basic global model, identifies accuracy bottlenecks in model architectures during training, and then employs model transformation to derive new models for heterogeneous clients on the fly. It judiciously assigns models to individual clients while performing soft aggregation on multi-model updates to minimize total training costs. Our evaluations using realistic settings show that FedTrans improves individual client model accuracy by 14% - 72% while slashing training costs by 1.6X - 20X over state-of-the-art solutions.","sentences":["Federated learning (FL) aims to train machine learning (ML) models across potentially millions of edge client devices.","Yet, training and customizing models for FL clients is notoriously challenging due to the heterogeneity of client data, device capabilities, and the massive scale of clients, making individualized model exploration prohibitively expensive.","State-of-the-art FL solutions personalize a globally trained model or concurrently train multiple models, but they often incur suboptimal model accuracy and huge training costs.   ","In this paper, we introduce FedTrans, a multi-model FL training framework that automatically produces and trains high-accuracy, hardware-compatible models for individual clients at scale.","FedTrans begins with a basic global model, identifies accuracy bottlenecks in model architectures during training, and then employs model transformation to derive new models for heterogeneous clients on the fly.","It judiciously assigns models to individual clients while performing soft aggregation on multi-model updates to minimize total training costs.","Our evaluations using realistic settings show that FedTrans improves individual client model accuracy by 14% - 72% while slashing training costs by 1.6X - 20X over state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.13515v1","category":"cs.LG"}
{"created":"2024-04-21 03:30:53","title":"A new iterative algorithm for comprehensive Grobner systems","abstract":"A Comprehensive Grobner system for a parametric ideal I in K(A)[X] represents the collection of all Grobner bases of the ideals I' in K[X] obtained as the values of the parameters A vary in K.   The recent algorithms for computing them comprehensive Grobner systems consider the corresponding ideal J in K[A,X], and are based on stability of Grobner bases of ideals under specializations of the parameters. Starting from a Grobner basis of J, the computation splits recursively depending on the vanishing of the evaluation of some ``coefficients'' in K[A].   In this paper, taking inspiration from the algorithm described by Nabeshima, we create a new iterative algorithm to compute comprehensive Grobner systems. We show how we keep track of the sub-cases to be considered, and how we avoid some redundant computation branches using ``comparatively-cheap'' ideal-membership tests, instead of radical-membership tests.","sentences":["A Comprehensive Grobner system for a parametric ideal I in K(A)[X] represents the collection of all Grobner bases of the ideals I' in K[X] obtained as the values of the parameters A vary in K.   The recent algorithms for computing them comprehensive Grobner systems consider the corresponding ideal J in K[A,X], and are based on stability of Grobner bases of ideals under specializations of the parameters.","Starting from a Grobner basis of J, the computation splits recursively depending on the vanishing of the evaluation of some ``coefficients'' in K[A].   ","In this paper, taking inspiration from the algorithm described by Nabeshima, we create a new iterative algorithm to compute comprehensive Grobner systems.","We show how we keep track of the sub-cases to be considered, and how we avoid some redundant computation branches using ``comparatively-cheap'' ideal-membership tests, instead of radical-membership tests."],"url":"http://arxiv.org/abs/2404.13514v1","category":"math.AC"}
{"created":"2024-04-21 02:44:17","title":"MFHCA: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion and Hierarchical Cooperative Attention","abstract":"Speech emotion recognition is crucial in human-computer interaction, but extracting and using emotional cues from audio poses challenges. This paper introduces MFHCA, a novel method for Speech Emotion Recognition using Multi-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and raw audio. We employ the Multi-Spatial Fusion module (MF) to efficiently identify emotion-related spectrogram regions and integrate Hubert features for higher-level acoustic information. Our approach also includes a Hierarchical Cooperative Attention module (HCA) to merge features from various auditory levels. We evaluate our method on the IEMOCAP dataset and achieve 2.6\\% and 1.87\\% improvements on the weighted accuracy and unweighted accuracy, respectively. Extensive experiments demonstrate the effectiveness of the proposed method.","sentences":["Speech emotion recognition is crucial in human-computer interaction, but extracting and using emotional cues from audio poses challenges.","This paper introduces MFHCA, a novel method for Speech Emotion Recognition using Multi-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and raw audio.","We employ the Multi-Spatial Fusion module (MF) to efficiently identify emotion-related spectrogram regions and integrate Hubert features for higher-level acoustic information.","Our approach also includes a Hierarchical Cooperative Attention module (HCA) to merge features from various auditory levels.","We evaluate our method on the IEMOCAP dataset and achieve 2.6\\% and 1.87\\% improvements on the weighted accuracy and unweighted accuracy, respectively.","Extensive experiments demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.13509v1","category":"cs.SD"}
{"created":"2024-04-21 02:26:15","title":"Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications","abstract":"The rise of deep learning has marked significant progress in fields such as computer vision, natural language processing, and medical imaging, primarily through the adaptation of pre-trained models for specific tasks. Traditional fine-tuning methods, involving adjustments to all parameters, face challenges due to high computational and memory demands. This has led to the development of Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update parameters to balance computational efficiency with performance. This review examines PEFT approaches, offering a detailed comparison of various strategies highlighting applications across different domains, including text generation, medical imaging, protein modeling, and speech synthesis. By assessing the effectiveness of PEFT methods in reducing computational load, speeding up training, and lowering memory usage, this paper contributes to making deep learning more accessible and adaptable, facilitating its wider application and encouraging innovation in model optimization. Ultimately, the paper aims to contribute towards insights into PEFT's evolving landscape, guiding researchers and practitioners in overcoming the limitations of conventional fine-tuning approaches.","sentences":["The rise of deep learning has marked significant progress in fields such as computer vision, natural language processing, and medical imaging, primarily through the adaptation of pre-trained models for specific tasks.","Traditional fine-tuning methods, involving adjustments to all parameters, face challenges due to high computational and memory demands.","This has led to the development of Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update parameters to balance computational efficiency with performance.","This review examines PEFT approaches, offering a detailed comparison of various strategies highlighting applications across different domains, including text generation, medical imaging, protein modeling, and speech synthesis.","By assessing the effectiveness of PEFT methods in reducing computational load, speeding up training, and lowering memory usage, this paper contributes to making deep learning more accessible and adaptable, facilitating its wider application and encouraging innovation in model optimization.","Ultimately, the paper aims to contribute towards insights into PEFT's evolving landscape, guiding researchers and practitioners in overcoming the limitations of conventional fine-tuning approaches."],"url":"http://arxiv.org/abs/2404.13506v1","category":"cs.LG"}
{"created":"2024-04-21 01:49:46","title":"A Survey on the Memory Mechanism of Large Language Model based Agents","abstract":"Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.","sentences":["Large language model (LLM) based agents have recently attracted much attention from the research and industry communities.","Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions.","The key component to support agent-environment interactions is the memory of the agents.","While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies.","To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents.","In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents.","Then, we systematically review previous studies on how to design and evaluate the memory module.","In addition, we also present many agent applications, where the memory module plays an important role.","At last, we analyze the limitations of existing work and show important future directions.","To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}."],"url":"http://arxiv.org/abs/2404.13501v1","category":"cs.AI"}
{"created":"2024-04-21 01:27:47","title":"Generalized Regression with Conditional GANs","abstract":"Regression is typically treated as a curve-fitting process where the goal is to fit a prediction function to data. With the help of conditional generative adversarial networks, we propose to solve this age-old problem in a different way; we aim to learn a prediction function whose outputs, when paired with the corresponding inputs, are indistinguishable from feature-label pairs in the training dataset. We show that this approach to regression makes fewer assumptions on the distribution of the data we are fitting to and, therefore, has better representation capabilities. We draw parallels with generalized linear models in statistics and show how our proposal serves as an extension of them to neural networks. We demonstrate the superiority of this new approach to standard regression with experiments on multiple synthetic and publicly available real-world datasets, finding encouraging results, especially with real-world heavy-tailed regression datasets. To make our work more reproducible, we release our source code. Link to repository: https://anonymous.4open.science/r/regressGAN-7B71/","sentences":["Regression is typically treated as a curve-fitting process where the goal is to fit a prediction function to data.","With the help of conditional generative adversarial networks, we propose to solve this age-old problem in a different way; we aim to learn a prediction function whose outputs, when paired with the corresponding inputs, are indistinguishable from feature-label pairs in the training dataset.","We show that this approach to regression makes fewer assumptions on the distribution of the data we are fitting to and, therefore, has better representation capabilities.","We draw parallels with generalized linear models in statistics and show how our proposal serves as an extension of them to neural networks.","We demonstrate the superiority of this new approach to standard regression with experiments on multiple synthetic and publicly available real-world datasets, finding encouraging results, especially with real-world heavy-tailed regression datasets.","To make our work more reproducible, we release our source code.","Link to repository: https://anonymous.4open.science/r/regressGAN-7B71/"],"url":"http://arxiv.org/abs/2404.13500v1","category":"cs.LG"}
{"created":"2024-04-21 01:26:18","title":"Unified Map Handling for Robotic Systems: Enhancing Interoperability and Efficiency Across Diverse Environments","abstract":"Mapping is a time-consuming process for deploying robotic systems to new environments. The handling of maps is also risk-adverse when not managed effectively. We propose here, a standardised approach to handling such maps in a manner which focuses on the information contained wherein such as global location, object positions, topology, and occupancy. As part of this approach, associated management scripts are able to assist with generation of maps both through direct and indirect information restructuring, and with template and procedural generation of missing data. These approaches are able to, when combined, improve the handling of maps to enable more efficient deployments and higher interoperability between platforms. Alongside this, a collection of sample datasets of fully-mapped environments are included covering areas such as agriculture, urban roadways, and indoor environments.","sentences":["Mapping is a time-consuming process for deploying robotic systems to new environments.","The handling of maps is also risk-adverse when not managed effectively.","We propose here, a standardised approach to handling such maps in a manner which focuses on the information contained wherein such as global location, object positions, topology, and occupancy.","As part of this approach, associated management scripts are able to assist with generation of maps both through direct and indirect information restructuring, and with template and procedural generation of missing data.","These approaches are able to, when combined, improve the handling of maps to enable more efficient deployments and higher interoperability between platforms.","Alongside this, a collection of sample datasets of fully-mapped environments are included covering areas such as agriculture, urban roadways, and indoor environments."],"url":"http://arxiv.org/abs/2404.13499v1","category":"cs.RO"}
{"created":"2024-04-21 00:57:13","title":"ODE-DPS: ODE-based Diffusion Posterior Sampling for Inverse Problems in Partial Differential Equation","abstract":"In recent years we have witnessed a growth in mathematics for deep learning, which has been used to solve inverse problems of partial differential equations (PDEs). However, most deep learning-based inversion methods either require paired data or necessitate retraining neural networks for modifications in the conditions of the inverse problem, significantly reducing the efficiency of inversion and limiting its applicability. To overcome this challenge, in this paper, leveraging the score-based generative diffusion model, we introduce a novel unsupervised inversion methodology tailored for solving inverse problems arising from PDEs. Our approach operates within the Bayesian inversion framework, treating the task of solving the posterior distribution as a conditional generation process achieved through solving a reverse-time stochastic differential equation. Furthermore, to enhance the accuracy of inversion results, we propose an ODE-based Diffusion Posterior Sampling inversion algorithm. The algorithm stems from the marginal probability density functions of two distinct forward generation processes that satisfy the same Fokker-Planck equation. Through a series of experiments involving various PDEs, we showcase the efficiency and robustness of our proposed method.","sentences":["In recent years we have witnessed a growth in mathematics for deep learning, which has been used to solve inverse problems of partial differential equations (PDEs).","However, most deep learning-based inversion methods either require paired data or necessitate retraining neural networks for modifications in the conditions of the inverse problem, significantly reducing the efficiency of inversion and limiting its applicability.","To overcome this challenge, in this paper, leveraging the score-based generative diffusion model, we introduce a novel unsupervised inversion methodology tailored for solving inverse problems arising from PDEs.","Our approach operates within the Bayesian inversion framework, treating the task of solving the posterior distribution as a conditional generation process achieved through solving a reverse-time stochastic differential equation.","Furthermore, to enhance the accuracy of inversion results, we propose an ODE-based Diffusion Posterior Sampling inversion algorithm.","The algorithm stems from the marginal probability density functions of two distinct forward generation processes that satisfy the same Fokker-Planck equation.","Through a series of experiments involving various PDEs, we showcase the efficiency and robustness of our proposed method."],"url":"http://arxiv.org/abs/2404.13496v1","category":"math.NA"}
{"created":"2024-04-22 17:59:56","title":"High Harmonic Tracking of Ultrafast Electron Dynamics across the Mott to Charge Density Wave Phase Transition","abstract":"Different insulator phases compete with each other in strongly correlated materials with simultaneous local and non-local interactions. It is known that the homogeneous Mott insulator converts into a charge density wave (CDW) phase when the non-local interactions are increased, but there is ongoing debate on whether and in which parameter regimes this transition is of first order, or of second order with an intermediate bond-order wave phase. Here we show that strong-field optics applied to an extended Fermi-Hubbard system can serve as a powerful tool to reveal the nature of the quantum phase transition. Specifically, we show that in the strongly interacting regime characteristic excitations such as excitons, biexcitons, excitonic strings, and charge droplets can be tracked by the non-linear optical response to an ultrafast and intense laser pulse. Subcycle analysis of high harmonic spectra unravels the ultrafast dynamics of these increasingly complex objects, which partially escape the scrutiny of linear optics. Their appearance in the high harmonic spectrum provides striking evidence of a first-order transition into the CDW phase, and makes a strong case for using strong-field optics as a powerful tool to reveal the nature of quasiparticles in strongly correlated matter, and to track the electron dynamics during a first-order quantum phase transition.","sentences":["Different insulator phases compete with each other in strongly correlated materials with simultaneous local and non-local interactions.","It is known that the homogeneous Mott insulator converts into a charge density wave (CDW) phase when the non-local interactions are increased, but there is ongoing debate on whether and in which parameter regimes this transition is of first order, or of second order with an intermediate bond-order wave phase.","Here we show that strong-field optics applied to an extended Fermi-Hubbard system can serve as a powerful tool to reveal the nature of the quantum phase transition.","Specifically, we show that in the strongly interacting regime characteristic excitations such as excitons, biexcitons, excitonic strings, and charge droplets can be tracked by the non-linear optical response to an ultrafast and intense laser pulse.","Subcycle analysis of high harmonic spectra unravels the ultrafast dynamics of these increasingly complex objects, which partially escape the scrutiny of linear optics.","Their appearance in the high harmonic spectrum provides striking evidence of a first-order transition into the CDW phase, and makes a strong case for using strong-field optics as a powerful tool to reveal the nature of quasiparticles in strongly correlated matter, and to track the electron dynamics during a first-order quantum phase transition."],"url":"http://arxiv.org/abs/2404.14411v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 17:59:18","title":"Hyp-OC: Hyperbolic One Class Classification for Face Anti-Spoofing","abstract":"Face recognition technology has become an integral part of modern security systems and user authentication processes. However, these systems are vulnerable to spoofing attacks and can easily be circumvented. Most prior research in face anti-spoofing (FAS) approaches it as a two-class classification task where models are trained on real samples and known spoof attacks and tested for detection performance on unknown spoof attacks. However, in practice, FAS should be treated as a one-class classification task where, while training, one cannot assume any knowledge regarding the spoof samples a priori. In this paper, we reformulate the face anti-spoofing task from a one-class perspective and propose a novel hyperbolic one-class classification framework. To train our network, we use a pseudo-negative class sampled from the Gaussian distribution with a weighted running mean and propose two novel loss functions: (1) Hyp-PC: Hyperbolic Pairwise Confusion loss, and (2) Hyp-CE: Hyperbolic Cross Entropy loss, which operate in the hyperbolic space. Additionally, we employ Euclidean feature clipping and gradient clipping to stabilize the training in the hyperbolic space. To the best of our knowledge, this is the first work extending hyperbolic embeddings for face anti-spoofing in a one-class manner. With extensive experiments on five benchmark datasets: Rose-Youtu, MSU-MFSD, CASIA-MFSD, Idiap Replay-Attack, and OULU-NPU, we demonstrate that our method significantly outperforms the state-of-the-art, achieving better spoof detection performance.","sentences":["Face recognition technology has become an integral part of modern security systems and user authentication processes.","However, these systems are vulnerable to spoofing attacks and can easily be circumvented.","Most prior research in face anti-spoofing (FAS) approaches it as a two-class classification task where models are trained on real samples and known spoof attacks and tested for detection performance on unknown spoof attacks.","However, in practice, FAS should be treated as a one-class classification task where, while training, one cannot assume any knowledge regarding the spoof samples a priori.","In this paper, we reformulate the face anti-spoofing task from a one-class perspective and propose a novel hyperbolic one-class classification framework.","To train our network, we use a pseudo-negative class sampled from the Gaussian distribution with a weighted running mean and propose two novel loss functions: (1) Hyp-PC: Hyperbolic Pairwise Confusion loss, and (2) Hyp-CE: Hyperbolic Cross Entropy loss, which operate in the hyperbolic space.","Additionally, we employ Euclidean feature clipping and gradient clipping to stabilize the training in the hyperbolic space.","To the best of our knowledge, this is the first work extending hyperbolic embeddings for face anti-spoofing in a one-class manner.","With extensive experiments on five benchmark datasets: Rose-Youtu, MSU-MFSD, CASIA-MFSD, Idiap Replay-Attack, and OULU-NPU, we demonstrate that our method significantly outperforms the state-of-the-art, achieving better spoof detection performance."],"url":"http://arxiv.org/abs/2404.14406v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:02","title":"Multipolar Skyrmion Crystals in Non-Kramers Doublet Systems","abstract":"We study the Kondo lattice model of multipolar magnetic moments interacting with conduction electrons on a triangular lattice. Bond-dependent electron hoppings induce a compass-like anisotropy in the effective Ruderman-Kittel-Kasuya-Yosida interaction between multipolar moments. This unique anisotropy stabilizes multipolar skyrmion crystals with a skyrmion topological charge of 1 and 2 at a zero magnetic field. Diverse multipolar phases in the phase diagram give rise to novel spontaneous Hall response of conduction electrons.","sentences":["We study the Kondo lattice model of multipolar magnetic moments interacting with conduction electrons on a triangular lattice.","Bond-dependent electron hoppings induce a compass-like anisotropy in the effective Ruderman-Kittel-Kasuya-Yosida interaction between multipolar moments.","This unique anisotropy stabilizes multipolar skyrmion crystals with a skyrmion topological charge of 1 and 2 at a zero magnetic field.","Diverse multipolar phases in the phase diagram give rise to novel spontaneous Hall response of conduction electrons."],"url":"http://arxiv.org/abs/2404.14404v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 17:58:29","title":"On the convergence rates of discrete solutions to the Wave Kinetic Equation","abstract":"In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE). This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation. Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times. Moreover, there is convergence to a single Dirac mass at long times. The first goal of this paper is to give quantitative rates for the speed of said convergence. In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions.","sentences":["In this paper, we consider the long-term behavior of some special solutions to the Wave Kinetic Equation (WKE).","This equation provides a mesoscopic description of wave systems interacting nonlinearly via the cubic NLS equation.","Escobedo and Vel\\'azquez showed that, starting with initial data given by countably many Dirac masses, solutions remain a linear combination of countably many Dirac masses at all times.","Moreover, there is convergence to a single Dirac mass at long times.","The first goal of this paper is to give quantitative rates for the speed of said convergence.","In order to study the optimality of the bounds we obtain, we introduce and analyze a toy model accounting only for the leading order quadratic interactions."],"url":"http://arxiv.org/abs/2404.14400v1","category":"math.AP"}
{"created":"2024-04-22 17:54:18","title":"Direct observation of Floquet-Bloch states in monolayer graphene","abstract":"Floquet engineering is a novel method of manipulating quantum phases of matter via periodic driving [1, 2]. It has successfully been utilized in different platforms ranging from photonic systems [3] to optical lattice of ultracold atoms [4, 5]. In solids, light can be used as the periodic drive via coherent light-matter interaction. This leads to hybridization of Bloch electrons with photons resulting in replica bands known as Floquet-Bloch states. After the direct observation of Floquet-Bloch states in a topological insulator [6], their manifestations have been seen in a number of other experiments [7-14]. By engineering the electronic band structure using Floquet-Bloch states, various exotic phase transitions have been predicted [15-22] to occur. To realize these phases, it is necessary to better understand the nature of Floquet-Bloch states in different materials. However, direct energy and momentum resolved observation of these states is still limited to only few material systems [6, 10, 14, 23, 24]. Here, we report direct observation of Floquet-Bloch states in monolayer epitaxial graphene which was the first proposed material platform [15] for Floquet engineering. By using time- and angle-resolved photoemission spectroscopy (trARPES) with mid-infrared (mid-IR) pump excitation, we detected replicas of the Dirac cone. Pump polarization dependence of these replica bands unequivocally shows that they originate from the scattering between Floquet-Bloch states and photon-dressed free-electron-like photoemission final states, called Volkov states. Beyond graphene, our method can potentially be used to directly observe Floquet-Bloch states in other systems paving the way for Floquet engineering in a wide range of quantum materials.","sentences":["Floquet engineering is a novel method of manipulating quantum phases of matter via periodic driving [1, 2].","It has successfully been utilized in different platforms ranging from photonic systems","[3] to optical lattice of ultracold atoms [4, 5].","In solids, light can be used as the periodic drive via coherent light-matter interaction.","This leads to hybridization of Bloch electrons with photons resulting in replica bands known as Floquet-Bloch states.","After the direct observation of Floquet-Bloch states in a topological insulator [6], their manifestations have been seen in a number of other experiments [7-14].","By engineering the electronic band structure using Floquet-Bloch states, various exotic phase transitions have been predicted [15-22] to occur.","To realize these phases, it is necessary to better understand the nature of Floquet-Bloch states in different materials.","However, direct energy and momentum resolved observation of these states is still limited to only few material systems","[6, 10, 14, 23, 24].","Here, we report direct observation of Floquet-Bloch states in monolayer epitaxial graphene which was the first proposed material platform [15] for Floquet engineering.","By using time- and angle-resolved photoemission spectroscopy (trARPES) with mid-infrared (mid-IR) pump excitation, we detected replicas of the Dirac cone.","Pump polarization dependence of these replica bands unequivocally shows that they originate from the scattering between Floquet-Bloch states and photon-dressed free-electron-like photoemission final states, called Volkov states.","Beyond graphene, our method can potentially be used to directly observe Floquet-Bloch states in other systems paving the way for Floquet engineering in a wide range of quantum materials."],"url":"http://arxiv.org/abs/2404.14392v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 17:53:14","title":"Strong Asymptotics of Multiple Orthogonal Polynomials for Angelesco Systems. Part I: Non-Marginal Directions","abstract":"In this work, we establish strong asymptotics of multiple orthogonal polynomials of the second type for Angelesco systems with measures that satisfy Szeg\\H{o} conditions. We consider multi-indices that converge to infinity in the non-marginal directions.","sentences":["In this work, we establish strong asymptotics of multiple orthogonal polynomials of the second type for Angelesco systems with measures that satisfy Szeg\\H{o} conditions.","We consider multi-indices that converge to infinity in the non-marginal directions."],"url":"http://arxiv.org/abs/2404.14391v1","category":"math.CA"}
{"created":"2024-04-22 17:50:27","title":"Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction","abstract":"Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data. This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems. Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated. In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge. We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension. Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines.","sentences":["Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data.","This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems.","Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated.","In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge.","We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension.","Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines."],"url":"http://arxiv.org/abs/2404.14389v1","category":"cs.NI"}
{"created":"2024-04-22 17:46:29","title":"STROOBnet Optimization via GPU-Accelerated Proximal Recurrence Strategies","abstract":"Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors. This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring. Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances. Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach. It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage.","sentences":["Spatiotemporal networks' observational capabilities are crucial for accurate data gathering and informed decisions across multiple sectors.","This study focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network (STROOBnet), linking observational nodes (e.g., surveillance cameras) to events within defined geographical regions, enabling efficient monitoring.","Using data from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New Orleans, where RTCC combats rising crime amidst reduced police presence, we address the network's initial observational imbalances.","Aiming for uniform observational efficacy, we propose the Proximal Recurrence approach.","It outperformed traditional clustering methods like k-means and DBSCAN by offering holistic event frequency and spatial consideration, enhancing observational coverage."],"url":"http://arxiv.org/abs/2404.14388v1","category":"cs.LG"}
{"created":"2024-04-22 17:42:12","title":"Flexibility Pricing in Distribution Systems: A Direct Method Aligned with Flexibility Models","abstract":"Effective quantification of costs and values of distributed energy resources (DERs) can enhance the power system's utilization of their flexibility. This paper studies the flexibility pricing of DERs in distribution systems. We first propose a flexibility cost formulation based on adjustment ranges in both power and accumulated energy consumption. Compared with traditional power range-based approaches, the proposed method can quantify the potential inconvenience caused to DER users by flexibility activation and directly capture the time-dependent characteristics of DER's flexibility regions. We then propose a flexibility cost formulation aligned with the aggregated flexibility model and design a DSO flexibility market model to activate the aggregated flexibility in the distribution system to participate in the transmission system operation for energy arbitrage and ancillary services provision. Furthermore, we propose the concept of marginal flexibility price for the settlement of the DSO flexibility market, ensuring incentive compatibility, revenue adequacy, and a non-profit distribution system operator. Numerical tests on the IEEE 33-node distribution system verify the proposed methods.","sentences":["Effective quantification of costs and values of distributed energy resources (DERs) can enhance the power system's utilization of their flexibility.","This paper studies the flexibility pricing of DERs in distribution systems.","We first propose a flexibility cost formulation based on adjustment ranges in both power and accumulated energy consumption.","Compared with traditional power range-based approaches, the proposed method can quantify the potential inconvenience caused to DER users by flexibility activation and directly capture the time-dependent characteristics of DER's flexibility regions.","We then propose a flexibility cost formulation aligned with the aggregated flexibility model and design a DSO flexibility market model to activate the aggregated flexibility in the distribution system to participate in the transmission system operation for energy arbitrage and ancillary services provision.","Furthermore, we propose the concept of marginal flexibility price for the settlement of the DSO flexibility market, ensuring incentive compatibility, revenue adequacy, and a non-profit distribution system operator.","Numerical tests on the IEEE 33-node distribution system verify the proposed methods."],"url":"http://arxiv.org/abs/2404.14386v1","category":"eess.SY"}
{"created":"2024-04-22 17:38:58","title":"Encoding Petri Nets into CCS (Technical Report)","abstract":"This paper explores the problem of determining which classes of Petri nets can be encoded into behaviourally-equivalent CCS processes. Most of the existing related literature focuses on the inverse problem (i.e., encoding process calculi belonging to the CCS family into Petri nets), or extends CCS with Petri net-like multi-synchronisation (Multi-CCS). In this work, our main focus are free-choice and workflow nets (which are widely used in process mining to describe system interactions) and our target is plain CCS. We present several novel encodings, including one from free-choice workflow nets (produced by process mining algorithms like the alpha-miner) into CCS processes, and we prove that our encodings produce CCS processes that are weakly bisimilar to the original net. Besides contributing new expressiveness results, our encodings open a door towards bringing analysis and verification techniques from the realm of process calculi into the realm of process mining.","sentences":["This paper explores the problem of determining which classes of Petri nets can be encoded into behaviourally-equivalent CCS processes.","Most of the existing related literature focuses on the inverse problem (i.e., encoding process calculi belonging to the CCS family into Petri nets), or extends CCS with Petri net-like multi-synchronisation (Multi-CCS).","In this work, our main focus are free-choice and workflow nets (which are widely used in process mining to describe system interactions) and our target is plain CCS.","We present several novel encodings, including one from free-choice workflow nets (produced by process mining algorithms like the alpha-miner) into CCS processes, and we prove that our encodings produce CCS processes that are weakly bisimilar to the original net.","Besides contributing new expressiveness results, our encodings open a door towards bringing analysis and verification techniques from the realm of process calculi into the realm of process mining."],"url":"http://arxiv.org/abs/2404.14385v1","category":"cs.PL"}
{"created":"2024-04-22 17:37:04","title":"Observational characterisation of large-scale transport and horizontal turbulent diffusivity in the quiet Sun","abstract":"The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection. This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields. Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle. Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc. We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface. These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion. We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed. This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations. Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems.","sentences":["The Sun is a magnetic star, and the only spatio-temporally resolved astrophysical system displaying turbulent MHD thermal convection.","This makes it a privileged object of study to understand fluid turbulence in extreme regimes and its interactions with magnetic fields.","Global analyses of high-resolution solar observations provided by the NASA Solar Dynamics Observatory can shed light on the physical processes underlying large-scale emergent phenomena such as the solar dynamo cycle.","Combining a Coherent Structure Tracking reconstruction of photospheric flows, based on photometric data, and a statistical analysis of virtual passive tracers trajectories advected by these flows, we characterise one of the most important such processes, turbulent diffusion, over an unprecedentedly long monitoring period of 6 consecutive days of a significant fraction of the solar disc.","We first confirm, and provide a new global view of the emergence of a remarkable dynamical pattern of Lagrangian Coherent Structures tiling the entire surface.","These structures act as transport barriers on the time and spatial scale of supergranulation and, by transiently accumulating particles and magnetic fields, regulate large-scale turbulent surface diffusion.","We then further statistically characterise the turbulent transport regime using two different methods, and obtain an effective horizontal turbulent diffusivity $D=2-3\\times10^8~\\mathrm{m}^2~\\mathrm{s}^{-1}$ on the longest timescales probed.","This estimate is consistent with the transport coefficients required in large-scale mean-field solar dynamo models, and is in broad agreement with the results of global simulations.","Our analysis may also have implications for understanding the connections between solar-surface, coronal and solar-wind dynamics, and provides valuable lessons to characterise turbulent transport in other, unresolved turbulent astrophysical systems."],"url":"http://arxiv.org/abs/2404.14383v1","category":"astro-ph.SR"}
{"created":"2024-04-22 17:29:43","title":"A Fresh Look into the Interaction of Exoplanets Magnetosphere with Stellar Winds using MHD Simulations","abstract":"Numerous numerical studies have been carried out in recent years that simulate different aspects of exoplanets' magnetosphere and stellar winds. These studies have focused primarily on hot Jupiters with sun-like stars. This study addresses the challenges inherent in utilizing existing MHD codes to model hot Jupiter-star systems. Due to the scaling of the system and the assumption of a uniformly flowing stellar wind at the outer boundary of the simulation, MHD codes necessitate a minimum distance of greater than 0.4 au for a Jupiter-like planet orbiting a sun-like star to avoid substantial violations of the code's assumptions. Additionally, employing the GAMERA (Grid Agnostic MHD for Extended Research Applications) MHD code, we simulate star-planet interactions considering various stellar types (Sun-like and M Dwarf stars) with both Jupiter-like and Earth-like planets positioned at varying orbital distances. Furthermore, we explore the impact of tidal locking on the total power within the magnetosphere-ionosphere systems.","sentences":["Numerous numerical studies have been carried out in recent years that simulate different aspects of exoplanets' magnetosphere and stellar winds.","These studies have focused primarily on hot Jupiters with sun-like stars.","This study addresses the challenges inherent in utilizing existing MHD codes to model hot Jupiter-star systems.","Due to the scaling of the system and the assumption of a uniformly flowing stellar wind at the outer boundary of the simulation, MHD codes necessitate a minimum distance of greater than 0.4 au for a Jupiter-like planet orbiting a sun-like star to avoid substantial violations of the code's assumptions.","Additionally, employing the GAMERA (Grid Agnostic MHD for Extended Research Applications) MHD code, we simulate star-planet interactions considering various stellar types (Sun-like and M Dwarf stars) with both Jupiter-like and Earth-like planets positioned at varying orbital distances.","Furthermore, we explore the impact of tidal locking on the total power within the magnetosphere-ionosphere systems."],"url":"http://arxiv.org/abs/2404.14377v1","category":"astro-ph.EP"}
{"created":"2024-04-22 17:16:40","title":"Bistable Organic Electrochemical Transistors: Enthalpy vs. Entropy","abstract":"Organic electrochemical transistors (OECTs) serve as the foundation for a wide range of emerging applications, from bioelectronic implants and smart sensor systems to neuromorphic computing. Their ascent originates from a distinctive switching mechanism based on the coupling of electronic and ionic charge carriers, which gives rise to a multitude of unique characteristics. Notably, various OECT systems have been reported with significant hysteresis in their transfer curve. While being a feature sought after as non-volatile memory in neuromorphic systems, no universal explanation has yet been given for its physical origin, impeding its advanced implementation. Herein, we present a thermodynamic framework that readily elucidates the emergence of bistable OECT operation through the interplay of enthalpy and entropy. We validate our model through three experimental approaches, covering temperature-resolved characterizations, targeted material manipulation, and thermal imaging. In this context, we demonstrate the exceptional scenario where the subthreshold swing deviates from Boltzmann statistics, and we provide an alternate view on existing data in literature, which further supports our model. Finally, we leverage the bistability in form of a single-OECT Schmitt trigger, thus compacting the complexity of a multi-component circuit into a single device. These insights offer a revised understanding of OECT physics and promote their application in non-conventional computing, where symmetry-breaking phenomena are pivotal to unlock novel paradigms.","sentences":["Organic electrochemical transistors (OECTs) serve as the foundation for a wide range of emerging applications, from bioelectronic implants and smart sensor systems to neuromorphic computing.","Their ascent originates from a distinctive switching mechanism based on the coupling of electronic and ionic charge carriers, which gives rise to a multitude of unique characteristics.","Notably, various OECT systems have been reported with significant hysteresis in their transfer curve.","While being a feature sought after as non-volatile memory in neuromorphic systems, no universal explanation has yet been given for its physical origin, impeding its advanced implementation.","Herein, we present a thermodynamic framework that readily elucidates the emergence of bistable OECT operation through the interplay of enthalpy and entropy.","We validate our model through three experimental approaches, covering temperature-resolved characterizations, targeted material manipulation, and thermal imaging.","In this context, we demonstrate the exceptional scenario where the subthreshold swing deviates from Boltzmann statistics, and we provide an alternate view on existing data in literature, which further supports our model.","Finally, we leverage the bistability in form of a single-OECT Schmitt trigger, thus compacting the complexity of a multi-component circuit into a single device.","These insights offer a revised understanding of OECT physics and promote their application in non-conventional computing, where symmetry-breaking phenomena are pivotal to unlock novel paradigms."],"url":"http://arxiv.org/abs/2404.14362v1","category":"physics.app-ph"}
{"created":"2024-04-22 17:13:44","title":"Infrared-Radio-follow-up Observations for Detection of the Magnetic Radio Emission of Extra-Solar Planets: A New Window to Detect Exoplanets","abstract":"There are several methods for indirectly detecting exoplanets, such as transit, radial velocity, astrometry, and the conventional gravitational microlensing approach. These methods rely on observing the effects of exoplanets on the emission or motion of observed stars. All these techniques have focused on the optical or infrared domains. However, an alternative method for exoplanet detection via microlensing events involves planets orbiting the source star, creating a binary source system. In this study, we explore a novel approach to detecting and studying exoplanets exclusively through their radio emissions resulting from magnetospheric processes. We propose utilizing the Roman telescope as a survey observer to detect microlensing events. Subsequently, we investigate the potential for detecting planetary radio signals through follow-up observations of these microlensing events in the radio band using the SKA telescope. This method is viable due to the comparable radio emission levels of exoplanets and their parent stars, unlike optical and infrared emissions. We conduct a Monte Carlo simulation to replicate the observations by the Nancy Roman Telescope, followed by a follow-up observation in radio frequencies using the SKA telescope. We determine that approximately 1155 exoplanets exhibit detectable signals by the SKA telescope during the 7-season observations by the Nancy Roman Telescope. This result indicates that such a method cannot only facilitate the direct detection of exoplanets but also enable the measurement of their magnetic field strength through analysis of their radio emissions.","sentences":["There are several methods for indirectly detecting exoplanets, such as transit, radial velocity, astrometry, and the conventional gravitational microlensing approach.","These methods rely on observing the effects of exoplanets on the emission or motion of observed stars.","All these techniques have focused on the optical or infrared domains.","However, an alternative method for exoplanet detection via microlensing events involves planets orbiting the source star, creating a binary source system.","In this study, we explore a novel approach to detecting and studying exoplanets exclusively through their radio emissions resulting from magnetospheric processes.","We propose utilizing the Roman telescope as a survey observer to detect microlensing events.","Subsequently, we investigate the potential for detecting planetary radio signals through follow-up observations of these microlensing events in the radio band using the SKA telescope.","This method is viable due to the comparable radio emission levels of exoplanets and their parent stars, unlike optical and infrared emissions.","We conduct a Monte Carlo simulation to replicate the observations by the Nancy Roman Telescope, followed by a follow-up observation in radio frequencies using the SKA telescope.","We determine that approximately 1155 exoplanets exhibit detectable signals by the SKA telescope during the 7-season observations by the Nancy Roman Telescope.","This result indicates that such a method cannot only facilitate the direct detection of exoplanets but also enable the measurement of their magnetic field strength through analysis of their radio emissions."],"url":"http://arxiv.org/abs/2404.14359v1","category":"astro-ph.EP"}
{"created":"2024-04-22 17:12:06","title":"A Stochastic Geo-spatiotemporal Bipartite Network to Optimize GCOOS Sensor Placement Strategies","abstract":"This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness. The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes. The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements. This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes. In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico. This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes. The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model.","sentences":["This paper proposes two new measures applicable in a spatial bipartite network model: coverage and coverage robustness.","The bipartite network must consist of observer nodes, observable nodes, and edges that connect observer nodes to observable nodes.","The coverage and coverage robustness scores evaluate the effectiveness of the observer node placements.","This measure is beneficial for stochastic data as it may be coupled with Monte Carlo simulations to identify optimal placements for new observer nodes.","In this paper, we construct a Geo-SpatioTemporal Bipartite Network (GSTBN) within the stochastic and dynamical environment of the Gulf of Mexico.","This GSTBN consists of GCOOS sensor nodes and HYCOM Region of Interest (RoI) event nodes.","The goal is to identify optimal placements to expand GCOOS to improve the forecasting outcomes by the HYCOM ocean prediction model."],"url":"http://arxiv.org/abs/2404.14357v1","category":"cs.MA"}
{"created":"2024-04-22 16:58:37","title":"Heterogeneous Face Recognition Using Domain Invariant Units","abstract":"Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra. However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data. In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap. The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework. This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods.","sentences":["Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra.","However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data.","In this work, we leverage a pretrained face recognition model as a teacher network to learn domaininvariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap.","The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework.","This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data.","We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.14343v1","category":"cs.CV"}
{"created":"2024-04-22 16:56:39","title":"Managing Expectations and Imbalanced Training Data in Reactive Force Field Development: an Application to Water Adsorption on Alumina","abstract":"ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems. When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce. ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others. To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set. One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set. This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure. The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters. Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed. The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina. This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training. We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model.","sentences":["ReaxFF is a computationally efficient model for reactive molecular dynamics simulations, which has been applied to a wide variety of chemical systems.","When ReaxFF parameters are not yet available for a chemistry of interest, they must be (re)optimized, for which one defines a set of training data that the new ReaxFF parameters should reproduce.","ReaxFF training sets typically contain diverse properties with different units, some of which are more abundant (by orders of magnitude) than others.","To find the best parameters, one conventionally minimizes a weighted sum of squared errors over all data in the training set.","One of the challenges in such numerical optimizations is to assign weights so that the optimized parameters represent a good compromise between all the requirements defined in the training set.","This work introduces a new loss function, called Balanced Loss, and a workflow that replaces weight assignment with a more manageable procedure.","The training data is divided into categories with corresponding \"tolerances\", i.e. acceptable root-mean-square errors for the categories, which define the expectations for the optimized ReaxFF parameters.","Through the Log-Sum-Exp form of Balanced Loss, the parameter optimization is also a validation of one's expectations, providing meaningful feedback that can be used to reconfigure the tolerances if needed.","The new methodology is demonstrated with a non-trivial parameterization of ReaxFF for water adsorption on alumina.","This results in a new force field that reproduces both rare and frequent properties of a validation set not used for training.","We also demonstrate the robustness of the new force field with a molecular dynamics simulation of water desorption from a $\\gamma$-Al$_2$O$_3$ slab model."],"url":"http://arxiv.org/abs/2404.14338v1","category":"physics.chem-ph"}
{"created":"2024-04-22 16:55:44","title":"Statistical Validation of Contagion Centrality in Financial Networks","abstract":"In this paper, we introduce a novel centrality measure to evaluate shock propagation on financial networks capturing a notion of contagion and systemic risk contributions. In comparison to many popular centrality metrics (e.g., eigenvector centrality) which provide only a relative centrality between nodes, our proposed measure is in an absolute scale permitting comparisons of contagion risk over time. In addition, we provide a statistical validation method when the network is estimated from data, as is done in practice. This statistical test allows us to reliably assess the computed centrality values. We validate our methodology on simulated data and conduct empirical case studies using financial data. We find that our proposed centrality measure increases significantly during times of financial distress and is able to provide insights in to the (market implied) risk-levels of different firms and sectors.","sentences":["In this paper, we introduce a novel centrality measure to evaluate shock propagation on financial networks capturing a notion of contagion and systemic risk contributions.","In comparison to many popular centrality metrics (e.g., eigenvector centrality) which provide only a relative centrality between nodes, our proposed measure is in an absolute scale permitting comparisons of contagion risk over time.","In addition, we provide a statistical validation method when the network is estimated from data, as is done in practice.","This statistical test allows us to reliably assess the computed centrality values.","We validate our methodology on simulated data and conduct empirical case studies using financial data.","We find that our proposed centrality measure increases significantly during times of financial distress and is able to provide insights in to the (market implied) risk-levels of different firms and sectors."],"url":"http://arxiv.org/abs/2404.14337v1","category":"q-fin.MF"}
{"created":"2024-04-22 16:55:35","title":"Transitions and Thermodynamics on Species Graphs of Chemical Reaction Networks","abstract":"Chemical reaction networks (CRNs) exhibit complex dynamics governed by their underlying network structure. In this paper, we propose a novel approach to study the dynamics of CRNs by representing them on species graphs (S-graphs). By scaling concentrations by conservation laws, we obtain a graph representation of transitions compatible with the S-graph, which allows us to treat the dynamics in CRNs as transitions between chemicals. We also define thermodynamic-like quantities on the S-graph from the introduced transitions and investigate their properties, including the relationship between specieswise forces, activities, and conventional thermodynamic quantities. Remarkably, we demonstrate that this formulation can be developed for a class of irreversible CRNs, while for reversible CRNs, it is related to conventional thermodynamic quantities associated with reactions. The behavior of these specieswise quantities is numerically validated using an oscillating system (Brusselator). Our work provides a novel methodology for studying dynamics on S-graphs, paving the way for a deeper understanding of the intricate interplay between the structure and dynamics of chemical reaction networks.","sentences":["Chemical reaction networks (CRNs) exhibit complex dynamics governed by their underlying network structure.","In this paper, we propose a novel approach to study the dynamics of CRNs by representing them on species graphs (S-graphs).","By scaling concentrations by conservation laws, we obtain a graph representation of transitions compatible with the S-graph, which allows us to treat the dynamics in CRNs as transitions between chemicals.","We also define thermodynamic-like quantities on the S-graph from the introduced transitions and investigate their properties, including the relationship between specieswise forces, activities, and conventional thermodynamic quantities.","Remarkably, we demonstrate that this formulation can be developed for a class of irreversible CRNs, while for reversible CRNs, it is related to conventional thermodynamic quantities associated with reactions.","The behavior of these specieswise quantities is numerically validated using an oscillating system (Brusselator).","Our work provides a novel methodology for studying dynamics on S-graphs, paving the way for a deeper understanding of the intricate interplay between the structure and dynamics of chemical reaction networks."],"url":"http://arxiv.org/abs/2404.14336v2","category":"q-bio.MN"}
{"created":"2024-04-22 16:38:23","title":"Thirteen New M Dwarf + T Dwarf Pairs Identified with WISE/NEOWISE","abstract":"We present the discovery of 13 new widely separated T dwarf companions to M dwarf primaries, identified using WISE/NEOWISE data by the CatWISE and Backyard Worlds: Planet 9 projects. This sample represents a $\\sim$60% increase in the number of known M+T systems, and allows us to probe the most extreme products of binary/planetary system formation, a discovery space made available by the CatWISE2020 catalog and the Backyard Worlds: Planet 9 effort. Highlights among the sample are WISEP J075108.79-763449.6, a previously known T9 thought to be old due to its SED, which we now find is part of a common-proper-motion pair with L 34-26 A, a well studied young M3 V star within 10 pc of the Sun; CWISE J054129.32-745021.5 B and 2MASS J05581644-4501559 B, two T8 dwarfs possibly associated with the very fast-rotating M4 V stars CWISE J054129.32-745021.5 A and 2MASS J05581644-4501559 A; and UCAC3 52-1038 B, which is among the widest late T companions to main sequence stars, with a projected separation of $\\sim$7100 au. The new benchmarks presented here are prime $JWST$ targets, and can help us place strong constraints on formation and evolution theory of substellar objects as well as on atmospheric models for these cold exoplanet analogs.","sentences":["We present the discovery of 13 new widely separated T dwarf companions to M dwarf primaries, identified using WISE/NEOWISE data by the CatWISE and Backyard Worlds:","Planet 9 projects.","This sample represents a $\\sim$60% increase in the number of known M+T systems, and allows us to probe the most extreme products of binary/planetary system formation, a discovery space made available by the CatWISE2020 catalog and the Backyard Worlds:","Planet 9 effort.","Highlights among the sample are WISEP J075108.79-763449.6, a previously known T9 thought to be old due to its SED, which we now find is part of a common-proper-motion pair with L 34-26 A, a well studied young M3 V star within 10 pc of the Sun; CWISE J054129.32-745021.5 B and 2MASS J05581644-4501559 B, two T8 dwarfs possibly associated with the very fast-rotating M4 V stars CWISE J054129.32-745021.5 A and","2MASS J05581644-4501559 A; and UCAC3 52-1038 B, which is among the widest late T companions to main sequence stars, with a projected separation of $\\sim$7100 au.","The new benchmarks presented here are prime $JWST$ targets, and can help us place strong constraints on formation and evolution theory of substellar objects as well as on atmospheric models for these cold exoplanet analogs."],"url":"http://arxiv.org/abs/2404.14324v1","category":"astro-ph.SR"}
{"created":"2024-04-22 16:33:31","title":"Quantum Coherence and Distinguishability: A Resource-Theoretic Perspective on Wave-Particle Duality","abstract":"Wave-particle duality, the cornerstone of quantum mechanics, illustrates essential trade-offs between two complementary aspects of quantum systems. Captured by Bohr's complementarity principle, the wave-particle duality relation indicates that perfect path discrimination in a multipath interferometer obliterates interference patterns and vice versa. In this work, from the perspective of coherence resource manipulation, we uncover a novel duality relation between quantum coherence and distinguishability in ensembles of mutually orthogonal pure states. We demonstrate the sum of `co-bits', coherence preserved after discrimination, and classical bits, distinguishability extracted through perfect discrimination is bounded. One cannot simultaneously extract all classical information and preserve coherence. Such duality relation exposes an inherent trade-off between quantum coherence and classical distinguishability resources. Our findings offer a fresh perspective and advance our understanding of the intrinsic complementary relationship between quantum and classical resources.","sentences":["Wave-particle duality, the cornerstone of quantum mechanics, illustrates essential trade-offs between two complementary aspects of quantum systems.","Captured by Bohr's complementarity principle, the wave-particle duality relation indicates that perfect path discrimination in a multipath interferometer obliterates interference patterns and vice versa.","In this work, from the perspective of coherence resource manipulation, we uncover a novel duality relation between quantum coherence and distinguishability in ensembles of mutually orthogonal pure states.","We demonstrate the sum of `co-bits', coherence preserved after discrimination, and classical bits, distinguishability extracted through perfect discrimination is bounded.","One cannot simultaneously extract all classical information and preserve coherence.","Such duality relation exposes an inherent trade-off between quantum coherence and classical distinguishability resources.","Our findings offer a fresh perspective and advance our understanding of the intrinsic complementary relationship between quantum and classical resources."],"url":"http://arxiv.org/abs/2404.14323v1","category":"quant-ph"}
{"created":"2024-04-22 16:30:03","title":"Multi-Agent Hybrid SAC for Joint SS-DSA in CRNs","abstract":"Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs). In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network. However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate. In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power. Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing. In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme. Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin. We also explore the impact of wireless variations such as coherence time on the efficacy of the system.","sentences":["Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs).","In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network.","However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate.","In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power.","Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing.","In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme.","Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin.","We also explore the impact of wireless variations such as coherence time on the efficacy of the system."],"url":"http://arxiv.org/abs/2404.14319v1","category":"eess.SY"}
{"created":"2024-04-22 16:28:09","title":"Automated Long Answer Grading with RiceChem Dataset","abstract":"We introduce a new area of study in the field of educational Natural Language Processing: Automated Long Answer Grading (ALAG). Distinguishing itself from Automated Short Answer Grading (ASAG) and Automated Essay Grading (AEG), ALAG presents unique challenges due to the complexity and multifaceted nature of fact-based long answers. To study ALAG, we introduce RiceChem, a dataset derived from a college chemistry course, featuring real student responses to long-answer questions with an average word count notably higher than typical ASAG datasets. We propose a novel approach to ALAG by formulating it as a rubric entailment problem, employing natural language inference models to verify whether each criterion, represented by a rubric item, is addressed in the student's response. This formulation enables the effective use of MNLI for transfer learning, significantly improving the performance of models on the RiceChem dataset. We demonstrate the importance of rubric-based formulation in ALAG, showcasing its superiority over traditional score-based approaches in capturing the nuances of student responses. We also investigate the performance of models in cold start scenarios, providing valuable insights into the practical deployment considerations in educational settings. Lastly, we benchmark state-of-the-art open-sourced Large Language Models (LLMs) on RiceChem and compare their results to GPT models, highlighting the increased complexity of ALAG compared to ASAG. Despite leveraging the benefits of a rubric-based approach and transfer learning from MNLI, the lower performance of LLMs on RiceChem underscores the significant difficulty posed by the ALAG task. With this work, we offer a fresh perspective on grading long, fact-based answers and introduce a new dataset to stimulate further research in this important area. Code: \\url{https://github.com/luffycodes/Automated-Long-Answer-Grading}.","sentences":["We introduce a new area of study in the field of educational Natural Language Processing: Automated Long Answer Grading (ALAG).","Distinguishing itself from Automated Short Answer Grading (ASAG) and Automated Essay Grading (AEG), ALAG presents unique challenges due to the complexity and multifaceted nature of fact-based long answers.","To study ALAG, we introduce RiceChem, a dataset derived from a college chemistry course, featuring real student responses to long-answer questions with an average word count notably higher than typical ASAG datasets.","We propose a novel approach to ALAG by formulating it as a rubric entailment problem, employing natural language inference models to verify whether each criterion, represented by a rubric item, is addressed in the student's response.","This formulation enables the effective use of MNLI for transfer learning, significantly improving the performance of models on the RiceChem dataset.","We demonstrate the importance of rubric-based formulation in ALAG, showcasing its superiority over traditional score-based approaches in capturing the nuances of student responses.","We also investigate the performance of models in cold start scenarios, providing valuable insights into the practical deployment considerations in educational settings.","Lastly, we benchmark state-of-the-art open-sourced Large Language Models (LLMs) on RiceChem and compare their results to GPT models, highlighting the increased complexity of ALAG compared to ASAG.","Despite leveraging the benefits of a rubric-based approach and transfer learning from MNLI, the lower performance of LLMs on RiceChem underscores the significant difficulty posed by the ALAG task.","With this work, we offer a fresh perspective on grading long, fact-based answers and introduce a new dataset to stimulate further research in this important area.","Code: \\url{https://github.com/luffycodes/Automated-Long-Answer-Grading}."],"url":"http://arxiv.org/abs/2404.14316v1","category":"cs.CL"}
{"created":"2024-04-22 16:16:06","title":"Structure-preserving neural networks for the regularzied entropy-based closure of the Boltzmann moment system","abstract":"The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations. In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time. We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures. The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure. We conduct a numerical analysis of this approximation and investigate optimal parameter choices. Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy. The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}.","sentences":["The main challenge of large-scale numerical simulation of radiation transport is the high memory and computation time requirements of discretization methods for kinetic equations.","In this work, we derive and investigate a neural network-based approximation to the entropy closure method to accurately compute the solution of the multi-dimensional moment system with a low memory footprint and competitive computational time.","We extend methods developed for the standard entropy-based closure to the context of regularized entropy-based closures.","The main idea is to interpret structure-preserving neural network approximations of the regularized entropy closure as a two-stage approximation to the original entropy closure.","We conduct a numerical analysis of this approximation and investigate optimal parameter choices.","Our numerical experiments demonstrate that the method has a much lower memory footprint than traditional methods with competitive computation times and simulation accuracy.","The code and all trained networks are provided on GitHub\\footnote{\\url{https://github.com/ScSteffen/neuralEntropyClosures}}$^,$\\footnote{\\url{https://github.com/CSMMLab/KiT-RT}}."],"url":"http://arxiv.org/abs/2404.14312v1","category":"math.NA"}
{"created":"2024-04-22 15:59:41","title":"A Cross-Platform Execution Engine for the Quantum Intermediate Representation","abstract":"Hybrid languages like the Quantum Intermediate Representation (QIR) are essential for programming systems that mix quantum and conventional computing models, while execution of these programs is often deferred to a system-specific implementation. Here, we describe and demonstrate the QIR Execution Engine (QIR-EE) for parsing, interpreting, and executing QIR across multiple hardware platforms. QIR-EE uses LLVM to execute hybrid instructions specifying quantum programs and, by design, presents extension points that support customized runtime and hardware environments. We demonstrate an implementation that uses the XACC quantum hardware-accelerator library to dispatch prototypical quantum programs on different commercial quantum platforms and numerical simulators, and we validate execution of QIR-EE on the IonQ Harmony and Quantinuum H1-1 hardware. Our results highlight the efficiency of hybrid executable architectures for handling mixed instructions, managing mixed data, and integrating with quantum computing frameworks to realize cross-platform execution.","sentences":["Hybrid languages like the Quantum Intermediate Representation (QIR) are essential for programming systems that mix quantum and conventional computing models, while execution of these programs is often deferred to a system-specific implementation.","Here, we describe and demonstrate the QIR Execution Engine (QIR-EE) for parsing, interpreting, and executing QIR across multiple hardware platforms.","QIR-EE uses LLVM to execute hybrid instructions specifying quantum programs and, by design, presents extension points that support customized runtime and hardware environments.","We demonstrate an implementation that uses the XACC quantum hardware-accelerator library to dispatch prototypical quantum programs on different commercial quantum platforms and numerical simulators, and we validate execution of QIR-EE on the IonQ Harmony and Quantinuum H1-1 hardware.","Our results highlight the efficiency of hybrid executable architectures for handling mixed instructions, managing mixed data, and integrating with quantum computing frameworks to realize cross-platform execution."],"url":"http://arxiv.org/abs/2404.14299v1","category":"quant-ph"}
{"created":"2024-04-22 15:47:00","title":"Navier-Stokes equations for nearly integrable quantum gases","abstract":"The Navier-Stokes equations are paradigmatic equations describing hydrodynamics of an interacting system with microscopic interactions encoded in transport coefficients. In this work we show how the Navier-Stokes equations arise from the microscopic dynamics of nearly integrable $1d$ quantum many-body systems. We build upon the recently developed hydrodynamics of integrable models to study the effective Boltzmann equation with collision integral taking into account the non-integrable interactions. We compute the transport coefficients and find that the resulting Navier-Stokes equations have two regimes, which differ in the viscous properties of the resulting fluid. We illustrate the method by computing the transport coefficients for an experimentally relevant case of coupled 1d cold-atomic gases.","sentences":["The Navier-Stokes equations are paradigmatic equations describing hydrodynamics of an interacting system with microscopic interactions encoded in transport coefficients.","In this work we show how the Navier-Stokes equations arise from the microscopic dynamics of nearly integrable $1d$ quantum many-body systems.","We build upon the recently developed hydrodynamics of integrable models to study the effective Boltzmann equation with collision integral taking into account the non-integrable interactions.","We compute the transport coefficients and find that the resulting Navier-Stokes equations have two regimes, which differ in the viscous properties of the resulting fluid.","We illustrate the method by computing the transport coefficients for an experimentally relevant case of coupled 1d cold-atomic gases."],"url":"http://arxiv.org/abs/2404.14292v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-22 15:33:53","title":"Affine laminations and coaffine representations","abstract":"We study surface subgroups of $\\mathrm{SL}(4,\\mathbb R)$ acting convex cocompactly on $\\mathbb R \\textrm P^3$ with image in the coaffine group. The boundary of the convex core is stratified, and the one dimensional strata form a pair of bending laminations. We show that the bending data on each component consist of a convex $\\mathbb R \\textrm P^2$ structure and an affine measured lamination depending on the underlying convex projective structure on $S$ with (Hitchin) holonomy $\\rho: \\pi_1S \\to \\mathrm{SL}(3,\\mathbb R)$. We study the space $\\mathcal {ML}^\\rho(S)$ of bending data compatible with $\\rho$ and prove that its projectivization is a sphere of dimension $6g-7$.","sentences":["We study surface subgroups of $\\mathrm{SL}(4,\\mathbb R)$ acting convex cocompactly on $\\mathbb R \\textrm P^3$ with image in the coaffine group.","The boundary of the convex core is stratified, and the one dimensional strata form a pair of bending laminations.","We show that the bending data on each component consist of a convex $\\mathbb R \\textrm P^2$ structure and an affine measured lamination depending on the underlying convex projective structure on $S$ with (Hitchin) holonomy $\\rho: \\pi_1S \\to \\mathrm{SL}(3,\\mathbb R)$. We study the space $\\mathcal {ML}^\\rho(S)$ of bending data compatible with $\\rho$ and prove that its projectivization is a sphere of dimension $6g-7$."],"url":"http://arxiv.org/abs/2404.14284v1","category":"math.GT"}
{"created":"2024-04-22 15:31:21","title":"Blockchain in a box: A portable blockchain network implementation on Raspberry Pi's","abstract":"In this paper we describe a prototype of a blockchain-in-a-box system which allows users to easily bootstrap the whole Ethereum Proof-of-Work (PoW) network running on multiple Raspberry Pi nodes - an inexpensive modular computers. Users are able to orchestrate the whole blockchain network using a single web based interface, for example they are able to set the topology of the peer-to-peer (P2P) connections and control the initialization parameters. Each Raspberry Pi has a screen attached which visualizes current state of local blockchain, allowing users to easily visualize the consensus of the network in real time. We show how this platform can be used to perform experiments on consensus quality while using different P2P topologies. Similar experiments can be used for demonstration purposes in a workshop or other educational settings.","sentences":["In this paper we describe a prototype of a blockchain-in-a-box system which allows users to easily bootstrap the whole Ethereum Proof-of-Work (PoW) network running on multiple Raspberry Pi nodes - an inexpensive modular computers.","Users are able to orchestrate the whole blockchain network using a single web based interface, for example they are able to set the topology of the peer-to-peer (P2P) connections and control the initialization parameters.","Each Raspberry Pi has a screen attached which visualizes current state of local blockchain, allowing users to easily visualize the consensus of the network in real time.","We show how this platform can be used to perform experiments on consensus quality while using different P2P topologies.","Similar experiments can be used for demonstration purposes in a workshop or other educational settings."],"url":"http://arxiv.org/abs/2404.14282v1","category":"cs.DC"}
{"created":"2024-04-22 15:29:28","title":"Fast and Robust Normal Estimation for Sparse LiDAR Scans","abstract":"Light Detection and Ranging (LiDAR) technology has proven to be an important part of many robotics systems. Surface normals estimated from LiDAR data are commonly used for a variety of tasks in such systems. As most of the today's mechanical LiDAR sensors produce sparse data, estimating normals from a single scan in a robust manner poses difficulties.   In this paper, we address the problem of estimating normals for sparse LiDAR data avoiding the typical issues of smoothing out the normals in high curvature areas.   Mechanical LiDARs rotate a set of rigidly mounted lasers. One firing of such a set of lasers produces an array of points where each point's neighbor is known due to the known firing pattern of the scanner. We use this knowledge to connect these points to their neighbors and label them using the angles of the lines connecting them. When estimating normals at these points, we only consider points with the same label as neighbors. This allows us to avoid estimating normals in high curvature areas.   We evaluate our approach on various data, both self-recorded and publicly available, acquired using various sparse LiDAR sensors. We show that using our method for normal estimation leads to normals that are more robust in areas with high curvature which leads to maps of higher quality. We also show that our method only incurs a constant factor runtime overhead with respect to a lightweight baseline normal estimation procedure and is therefore suited for operation in computationally demanding environments.","sentences":["Light Detection and Ranging (LiDAR) technology has proven to be an important part of many robotics systems.","Surface normals estimated from LiDAR data are commonly used for a variety of tasks in such systems.","As most of the today's mechanical LiDAR sensors produce sparse data, estimating normals from a single scan in a robust manner poses difficulties.   ","In this paper, we address the problem of estimating normals for sparse LiDAR data avoiding the typical issues of smoothing out the normals in high curvature areas.   ","Mechanical LiDARs rotate a set of rigidly mounted lasers.","One firing of such a set of lasers produces an array of points where each point's neighbor is known due to the known firing pattern of the scanner.","We use this knowledge to connect these points to their neighbors and label them using the angles of the lines connecting them.","When estimating normals at these points, we only consider points with the same label as neighbors.","This allows us to avoid estimating normals in high curvature areas.   ","We evaluate our approach on various data, both self-recorded and publicly available, acquired using various sparse LiDAR sensors.","We show that using our method for normal estimation leads to normals that are more robust in areas with high curvature which leads to maps of higher quality.","We also show that our method only incurs a constant factor runtime overhead with respect to a lightweight baseline normal estimation procedure and is therefore suited for operation in computationally demanding environments."],"url":"http://arxiv.org/abs/2404.14281v1","category":"cs.RO"}
{"created":"2024-04-22 15:22:56","title":"VAMP: Visual Analytics for Microservices Performance","abstract":"Analysis of microservices' performance is a considerably challenging task due to the multifaceted nature of these systems. Each request to a microservices system might raise several Remote Procedure Calls (RPCs) to services deployed on different servers and/or containers. Existing distributed tracing tools leverage swimlane visualizations as the primary means to support performance analysis of microservices. These visualizations are particularly effective when it is needed to investigate individual end-to-end requests' performance behaviors. Still, they are substantially limited when more complex analyses are required, as when understanding the system-wide performance trends is needed. To overcome this limitation, we introduce vamp, an innovative visual analytics tool that enables, at once, the performance analysis of multiple end-to-end requests of a microservices system. Vamp was built around the idea that having a wide set of interactive visualizations facilitates the analyses of the recurrent characteristics of requests and their relation w.r.t. the end-to-end performance behavior. Through an evaluation of 33 datasets from an established open-source microservices system, we demonstrate how vamp aids in identifying RPC execution time deviations with significant impact on end-to-end performance. Additionally, we show that vamp can support in pinpointing meaningful structural patterns in end-to-end requests and their relationship with microservice performance behaviors.","sentences":["Analysis of microservices' performance is a considerably challenging task due to the multifaceted nature of these systems.","Each request to a microservices system might raise several Remote Procedure Calls (RPCs) to services deployed on different servers and/or containers.","Existing distributed tracing tools leverage swimlane visualizations as the primary means to support performance analysis of microservices.","These visualizations are particularly effective when it is needed to investigate individual end-to-end requests' performance behaviors.","Still, they are substantially limited when more complex analyses are required, as when understanding the system-wide performance trends is needed.","To overcome this limitation, we introduce vamp, an innovative visual analytics tool that enables, at once, the performance analysis of multiple end-to-end requests of a microservices system.","Vamp was built around the idea that having a wide set of interactive visualizations facilitates the analyses of the recurrent characteristics of requests and their relation w.r.t.","the end-to-end performance behavior.","Through an evaluation of 33 datasets from an established open-source microservices system, we demonstrate how vamp aids in identifying RPC execution time deviations with significant impact on end-to-end performance.","Additionally, we show that vamp can support in pinpointing meaningful structural patterns in end-to-end requests and their relationship with microservice performance behaviors."],"url":"http://arxiv.org/abs/2404.14273v1","category":"cs.SE"}
{"created":"2024-04-22 15:20:23","title":"Electrical spin manipulation in double SrTiO$_3$/LaAlO$_3$ quantum dots","abstract":"The spin dynamics in two electron double quantum dots embedded in two dimensional electron gas at the interface between SrTiO$_3$ and LaAlO$_3$ is studied by an exact numerical solution of the time-dependent Schr\\\"odinger equation, in the context of the electric dipole spin resonance experiment. Based on the three band model of $3d$-electrons localized at Ti ions on the square lattice we analyze in details the singlet-triplet transition induced by the AC electric field, in the magnetic field range close to the avoided crossing which appears as a result of the spin-orbit coupling. Our calculations show that for symmetric double quantum dots the single photon spin-flip transitions is prohibited due to the parity symmetry and the transition can occur only by the higher order two-photon processes. For a weakly asymmetric system, when the first order singlet-triplet transitions are released due to the parity symmetry breaking, the spin-flip transition has a character of the Rabi oscillations for a low electric field amplitude. As the amplitude is increased the frequency of the transition is blueshifted (redshifted) for the magnetic field below (above) the single-triplet avoided crossing. Interestingly, for a sufficiently high magnetic field and high AC field amplitude the electric field drives the system across the avoided crossing inducing the spin-flip by the Landau-Zener-Stueckelberg-Majorana transitions with 100\\% spin flip probability for a slow sweep. Finally, the optimization of the geometrical parameters of the system with respect to the time of spin-flip of its fidelity is also presented.","sentences":["The spin dynamics in two electron double quantum dots embedded in two dimensional electron gas at the interface between SrTiO$_3$ and LaAlO$_3$ is studied by an exact numerical solution of the time-dependent Schr\\\"odinger equation, in the context of the electric dipole spin resonance experiment.","Based on the three band model of $3d$-electrons localized at Ti ions on the square lattice we analyze in details the singlet-triplet transition induced by the AC electric field, in the magnetic field range close to the avoided crossing which appears as a result of the spin-orbit coupling.","Our calculations show that for symmetric double quantum dots the single photon spin-flip transitions is prohibited due to the parity symmetry and the transition can occur only by the higher order two-photon processes.","For a weakly asymmetric system, when the first order singlet-triplet transitions are released due to the parity symmetry breaking, the spin-flip transition has a character of the Rabi oscillations for a low electric field amplitude.","As the amplitude is increased the frequency of the transition is blueshifted (redshifted) for the magnetic field below (above) the single-triplet avoided crossing.","Interestingly, for a sufficiently high magnetic field and high AC field amplitude the electric field drives the system across the avoided crossing inducing the spin-flip by the Landau-Zener-Stueckelberg-Majorana transitions with 100\\% spin flip probability for a slow sweep.","Finally, the optimization of the geometrical parameters of the system with respect to the time of spin-flip of its fidelity is also presented."],"url":"http://arxiv.org/abs/2404.14272v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 15:16:59","title":"Sparse Explanations of Neural Networks Using Pruned Layer-Wise Relevance Propagation","abstract":"Explainability is a key component in many applications involving deep neural networks (DNNs). However, current explanation methods for DNNs commonly leave it to the human observer to distinguish relevant explanations from spurious noise. This is not feasible anymore when going from easily human-accessible data such as images to more complex data such as genome sequences. To facilitate the accessibility of DNN outputs from such complex data and to increase explainability, we present a modification of the widely used explanation method layer-wise relevance propagation. Our approach enforces sparsity directly by pruning the relevance propagation for the different layers. Thereby, we achieve sparser relevance attributions for the input features as well as for the intermediate layers. As the relevance propagation is input-specific, we aim to prune the relevance propagation rather than the underlying model architecture. This allows to prune different neurons for different inputs and hence, might be more appropriate to the local nature of explanation methods. To demonstrate the efficacy of our method, we evaluate it on two types of data, images and genomic sequences. We show that our modification indeed leads to noise reduction and concentrates relevance on the most important features compared to the baseline.","sentences":["Explainability is a key component in many applications involving deep neural networks (DNNs).","However, current explanation methods for DNNs commonly leave it to the human observer to distinguish relevant explanations from spurious noise.","This is not feasible anymore when going from easily human-accessible data such as images to more complex data such as genome sequences.","To facilitate the accessibility of DNN outputs from such complex data and to increase explainability, we present a modification of the widely used explanation method layer-wise relevance propagation.","Our approach enforces sparsity directly by pruning the relevance propagation for the different layers.","Thereby, we achieve sparser relevance attributions for the input features as well as for the intermediate layers.","As the relevance propagation is input-specific, we aim to prune the relevance propagation rather than the underlying model architecture.","This allows to prune different neurons for different inputs and hence, might be more appropriate to the local nature of explanation methods.","To demonstrate the efficacy of our method, we evaluate it on two types of data, images and genomic sequences.","We show that our modification indeed leads to noise reduction and concentrates relevance on the most important features compared to the baseline."],"url":"http://arxiv.org/abs/2404.14271v1","category":"cs.LG"}
{"created":"2024-04-22 15:08:08","title":"The impact of a top-heavy IMF on the formation and evolution of dark star clusters","abstract":"The Spitzer instability leads to the formation of a black hole sub-system (BHSub) at the center of a star cluster providing energy to luminous stars (LSs) and increasing their rate of evaporation. When the self-depletion time of the BHSub exceeds the evaporation time of the LSs, a dark star cluster (DSC) will appear. Using the NBODY7 code, we performed a comprehensive set of direct \\Nbody simulations over a wide range of initial conditions to study the pure effect of the top-heaviness of the IMF on the formation of the DSC phase. In the Galactic tidal field, top-heavy IMFs lead to the fast evaporation of LSs and the formation of DSCs. Therefore, DSCs can be present even in the outer region of the Milky Way (MW). To successfully transition to the DSC phase, the MW Globular Clusters (GCs) must possess an initial BH mass fraction of $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)>0.05$. For star clusters with $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)>0.08$, the DSC phase will be created for any given initial density of the cluster and Galactocentric distance. The duration of the cluster's lifetime spent in the DSC phase shows a negative (positive) correlation with the initial density, and Galactocentric distance of the star cluster if $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)\\leq 0.12$ ($\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)\\geq 0.15$). Considering the canonical IMF, it is unlikely for any MW GCs to enter the DSC phase. We discuss the BH retention fraction in view of the observed properties of the GCs of the MW.","sentences":["The Spitzer instability leads to the formation of a black hole sub-system (BHSub) at the center of a star cluster providing energy to luminous stars (LSs) and increasing their rate of evaporation.","When the self-depletion time of the BHSub exceeds the evaporation time of the LSs, a dark star cluster (DSC) will appear.","Using the NBODY7 code, we performed a comprehensive set of direct \\Nbody simulations over a wide range of initial conditions to study the pure effect of the top-heaviness of the IMF on the formation of the DSC phase.","In the Galactic tidal field, top-heavy IMFs lead to the fast evaporation of LSs and the formation of DSCs.","Therefore, DSCs can be present even in the outer region of the Milky Way (MW).","To successfully transition to the DSC phase, the MW Globular Clusters (GCs) must possess an initial BH mass fraction of $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)>0.05$. For star clusters with $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)>0.08$, the DSC phase will be created for any given initial density of the cluster and Galactocentric distance.","The duration of the cluster's lifetime spent in the DSC phase shows a negative (positive) correlation with the initial density, and Galactocentric distance of the star cluster if $\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)\\leq 0.12$ ($\\widetilde{\\mathit{M}}_\\mathrm{BH}(0)\\geq 0.15$).","Considering the canonical IMF, it is unlikely for any MW GCs to enter the DSC phase.","We discuss the BH retention fraction in view of the observed properties of the GCs of the MW."],"url":"http://arxiv.org/abs/2404.14259v1","category":"astro-ph.GA"}
{"created":"2024-04-22 15:02:41","title":"Frosty: Bringing strong liveness guarantees to the Snow family of consensus protocols","abstract":"Snowman is the consensus protocol implemented by the Avalanche blockchain and is part of the Snow family of protocols, first introduced through the original Avalanche leaderless consensus protocol. A major advantage of Snowman is that each consensus decision only requires an expected constant communication overhead per processor in the `common' case that the protocol is not under substantial Byzantine attack, i.e. it provides a solution to the scalability problem which ensures that the expected communication overhead per processor is independent of the total number of processors $n$ during normal operation. This is the key property that would enable a consensus protocol to scale to 10,000 or more independent validators (i.e. processors). On the other hand, the two following concerns have remained:   (1) Providing formal proofs of consistency for Snowman has presented a formidable challenge.   (2) Liveness attacks exist in the case that a Byzantine adversary controls more than $O(\\sqrt{n})$ processors, slowing termination to more than a logarithmic number of steps.   In this paper, we address the two issues above. We consider a Byzantine adversary that controls at most $f<n/5$ processors. First, we provide a simple proof of consistency for Snowman. Then we supplement Snowman with a `liveness module' that can be triggered in the case that a substantial adversary launches a liveness attack, and which guarantees liveness in this event by temporarily forgoing the communication complexity advantages of Snowman, but without sacrificing these low communication complexity advantages during normal operation.","sentences":["Snowman is the consensus protocol implemented by the Avalanche blockchain and is part of the Snow family of protocols, first introduced through the original Avalanche leaderless consensus protocol.","A major advantage of Snowman is that each consensus decision only requires an expected constant communication overhead per processor in the `common' case that the protocol is not under substantial Byzantine attack, i.e. it provides a solution to the scalability problem which ensures that the expected communication overhead per processor is independent of the total number of processors $n$ during normal operation.","This is the key property that would enable a consensus protocol to scale to 10,000 or more independent validators (i.e. processors).","On the other hand, the two following concerns have remained:   (1) Providing formal proofs of consistency for Snowman has presented a formidable challenge.   ","(2) Liveness attacks exist in the case that a Byzantine adversary controls more than $O(\\sqrt{n})$ processors, slowing termination to more than a logarithmic number of steps.   ","In this paper, we address the two issues above.","We consider a Byzantine adversary that controls at most $f<n/5$ processors.","First, we provide a simple proof of consistency for Snowman.","Then we supplement Snowman with a `liveness module' that can be triggered in the case that a substantial adversary launches a liveness attack, and which guarantees liveness in this event by temporarily forgoing the communication complexity advantages of Snowman, but without sacrificing these low communication complexity advantages during normal operation."],"url":"http://arxiv.org/abs/2404.14250v1","category":"cs.DC"}
{"created":"2024-04-22 15:00:51","title":"From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition","abstract":"Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios. However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch. In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap. We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems. The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap. Our method enables end-to-end training using a small set of paired samples. We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available","sentences":["Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios.","However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch.","In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap.","We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems.","The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap.","Our method enables end-to-end training using a small set of paired samples.","We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods.","The source code and protocols for reproducing the findings will be made publicly available"],"url":"http://arxiv.org/abs/2404.14247v1","category":"cs.CV"}
{"created":"2024-04-22 14:59:35","title":"Chain of trust: Unraveling the references among Common Criteria certified products","abstract":"With 5394 security certificates of IT products and systems, the Common Criteria for Information Technology Security Evaluation have bred an ecosystem entangled with various kind of relations between the certified products. Yet, the prevalence and nature of dependencies among Common Criteria certified products remains largely unexplored. This study devises a novel method for building the graph of references among the Common Criteria certified products, determining the different contexts of references with a supervised machine-learning algorithm, and measuring how often the references constitute actual dependencies between the certified products. With the help of the resulting reference graph, this work identifies just a dozen of certified components that are relied on by at least 10% of the whole ecosystem -- making them a prime target for malicious actors. The impact of their compromise is assessed and potentially problematic references to archived products are discussed.","sentences":["With 5394 security certificates of IT products and systems, the Common Criteria for Information Technology Security Evaluation have bred an ecosystem entangled with various kind of relations between the certified products.","Yet, the prevalence and nature of dependencies among Common Criteria certified products remains largely unexplored.","This study devises a novel method for building the graph of references among the Common Criteria certified products, determining the different contexts of references with a supervised machine-learning algorithm, and measuring how often the references constitute actual dependencies between the certified products.","With the help of the resulting reference graph, this work identifies just a dozen of certified components that are relied on by at least 10% of the whole ecosystem -- making them a prime target for malicious actors.","The impact of their compromise is assessed and potentially problematic references to archived products are discussed."],"url":"http://arxiv.org/abs/2404.14246v1","category":"cs.CR"}
{"created":"2024-04-22 14:57:37","title":"Functional Closure Properties of Finite $\\mathbb{N}$-weighted Automata","abstract":"We determine all functional closure properties of finite $\\mathbb{N}$-weighted automata, even all multivariate ones, and in particular all multivariate polynomials. We also determine all univariate closure properties in the promise setting, and all multivariate closure properties under certain assumptions on the promise, in particular we determine all multivariate closure properties where the output vector lies on a monotone algebraic graph variety.","sentences":["We determine all functional closure properties of finite $\\mathbb{N}$-weighted automata, even all multivariate ones, and in particular all multivariate polynomials.","We also determine all univariate closure properties in the promise setting, and all multivariate closure properties under certain assumptions on the promise, in particular we determine all multivariate closure properties where the output vector lies on a monotone algebraic graph variety."],"url":"http://arxiv.org/abs/2404.14245v1","category":"cs.CC"}
{"created":"2024-04-22 14:42:54","title":"Multi-mem behavior at reduced voltages in La$_{1/2}$Sr$_{1/2}$Mn$_{1/2}$Co$_{1/2}$O$_{3-x}$ perovskite modified with Sm:CeO$_2$","abstract":"Neuromorphic computing aims to mimic the architecture and the information processing mechanisms of the mammalian brain, appearing as the only avenue that offers significant energy savings compared to the standard digital computers. Memcapacitive devices (which can change their capacitance between different non-volatile states upon the application of electrical stimulation) can significantly reduce the energy consumption of bioinspired circuitry. In the present work, we study the multimem (memristive and memcapacitive) behavior of devices based on thin films of the topotactic redox La$_{1/2}$Sr$_{1/2}$Mn$_{1/2}$Co$_{1/2}$O$_{3-x}$ (LSMCO) perovskite modified with Sm:Ce$O_2$ (SCO), grown on Nb:SrTiO$_{3}$ with (001) and (110) out of plane orientations. Either the self assembling at the nanoscale of both LSMCO and SCO phases or the doping with Ce(Sm) of the LSMCO perovskite were observed for different fabrication conditions and out of plane orientations. The impact of these changes on the device electrical behavior was determined. The optimum devices resulted those with (110) orientation and Ce(Sm) doping the perovskite. These devices displayed a multimem behavior with robust memcapacitance and significantly lower operation voltages (especially the RESET voltage) in comparison with devices based on pristine LSMCO. In addition, they were able to endure electrical cycling (and the concomitant perovskite topotactic redox transition between oxidized and reduced phases) without suffering nanostructural or chemical changes. We link these properties to an enhanced perovskite reducibility upon Ce(Sm) doping. Our work contributes to increase the reliability of LSMCO based multimem systems and to reduce their operating voltages closer to the 1 V threshold, which are key issues for the development of nanodevices for neuromorphic or in memory computing.","sentences":["Neuromorphic computing aims to mimic the architecture and the information processing mechanisms of the mammalian brain, appearing as the only avenue that offers significant energy savings compared to the standard digital computers.","Memcapacitive devices (which can change their capacitance between different non-volatile states upon the application of electrical stimulation) can significantly reduce the energy consumption of bioinspired circuitry.","In the present work, we study the multimem (memristive and memcapacitive) behavior of devices based on thin films of the topotactic redox La$_{1/2}$Sr$_{1/2}$Mn$_{1/2}$Co$_{1/2}$O$_{3-x}$ (LSMCO) perovskite modified with Sm:Ce$O_2$ (SCO), grown on Nb:SrTiO$_{3}$ with (001) and (110) out of plane orientations.","Either the self assembling at the nanoscale of both LSMCO and SCO phases or the doping with Ce(Sm) of the LSMCO perovskite were observed for different fabrication conditions and out of plane orientations.","The impact of these changes on the device electrical behavior was determined.","The optimum devices resulted those with (110) orientation and Ce(Sm) doping the perovskite.","These devices displayed a multimem behavior with robust memcapacitance and significantly lower operation voltages (especially the RESET voltage) in comparison with devices based on pristine LSMCO.","In addition, they were able to endure electrical cycling (and the concomitant perovskite topotactic redox transition between oxidized and reduced phases) without suffering nanostructural or chemical changes.","We link these properties to an enhanced perovskite reducibility upon Ce(Sm) doping.","Our work contributes to increase the reliability of LSMCO based multimem systems and to reduce their operating voltages closer to the 1 V threshold, which are key issues for the development of nanodevices for neuromorphic or in memory computing."],"url":"http://arxiv.org/abs/2404.14231v1","category":"physics.app-ph"}
{"created":"2024-04-22 14:32:46","title":"Robust electrothermal switching of optical phase change materials through computer-aided adaptive pulse optimization","abstract":"Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states. This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage. Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states. Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging. In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events. To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices. Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films.","sentences":["Electrically tunable optical devices present diverse functionalities for manipulating electromagnetic waves by leveraging elements capable of reversibly switching between different optical states.","This adaptability in adjusting their responses to electromagnetic waves after fabrication is crucial for developing more efficient and compact optical systems for a broad range of applications including sensing, imaging, telecommunications, and data storage.","Chalcogenide-based phase change materials (PCMs) have shown great promise due to their stable, non-volatile phase transition between amorphous and crystalline states.","Nonetheless, optimizing the switching parameters of PCM devices and maintaining their stable operation over thousands of cycles with minimal variation can be challenging.","In this paper, we report on the critical role of PCM pattern as well as electrical pulse form in achieving reliable and stable switching, extending the operational lifetime of the device beyond 13,000 switching events.","To achieve this, we have developed a computer-aided algorithm that monitors optical changes in the device and adjusts the applied voltage in accordance with the phase transformation process, thereby significantly enhancing the lifetime of these reconfigurable devices.","Our findings reveal that patterned PCM structures show significantly higher endurance compared to blanket PCM thin films."],"url":"http://arxiv.org/abs/2404.14220v1","category":"physics.optics"}
{"created":"2024-04-22 14:32:21","title":"Designing Safe and Engaging AI Experiences for Children: Towards the Definition of Best Practices in UI/UX Design","abstract":"This workshop proposal focuses on best practices in UI/UX design for AI applications aimed at children, emphasising safety, engagement, and ethics. It aims to address the challenge of measuring the safety, trustworthiness, and reliability of interactions between children and AI systems. Through collaborative discussions, participants will explore effective design strategies and ethical guidelines while developing methodologies for assessing the safety and reliability of AI interactions with children. This proposal seeks to foster responsible and child-centered AI design practices within the CHI community.","sentences":["This workshop proposal focuses on best practices in UI/UX design for AI applications aimed at children, emphasising safety, engagement, and ethics.","It aims to address the challenge of measuring the safety, trustworthiness, and reliability of interactions between children and AI systems.","Through collaborative discussions, participants will explore effective design strategies and ethical guidelines while developing methodologies for assessing the safety and reliability of AI interactions with children.","This proposal seeks to foster responsible and child-centered AI design practices within the CHI community."],"url":"http://arxiv.org/abs/2404.14218v1","category":"cs.HC"}
{"created":"2024-04-22 14:31:40","title":"Security flaws from time-varying active encoding in high-speed measurement-device-independent quantum key distribution","abstract":"Quantum key distribution (QKD) can transmit secret keys with, in principle, information-theoretic security. However, bandwidth limitations in practical equipment threaten the security of high-speed (GHz) QKD systems. We propose and characterize a new side channel which arises when using active encoding. As an illustrative example, we focus on electro-optic phase modulation for polarization encoding at 1 GHz. We show that this side channel may reduce the maximum secure transmission distance by over 50% in a decoy state measurement-device-independent QKD protocol.","sentences":["Quantum key distribution (QKD) can transmit secret keys with, in principle, information-theoretic security.","However, bandwidth limitations in practical equipment threaten the security of high-speed (GHz) QKD systems.","We propose and characterize a new side channel which arises when using active encoding.","As an illustrative example, we focus on electro-optic phase modulation for polarization encoding at 1 GHz.","We show that this side channel may reduce the maximum secure transmission distance by over 50% in a decoy state measurement-device-independent QKD protocol."],"url":"http://arxiv.org/abs/2404.14216v1","category":"quant-ph"}
{"created":"2024-04-22 14:22:15","title":"An Exposure Model Framework for Signal Detection based on Electronic Healthcare Data","abstract":"Despite extensive safety assessments of drugs prior to their introduction to the market, certain adverse drug reactions (ADRs) remain undetected. The primary objective of pharmacovigilance is to identify these ADRs (i.e., signals). In addition to traditional spontaneous reporting systems (SRSs), electronic health (EHC) data is being used for signal detection as well. Unlike SRS, EHC data is longitudinal and thus requires assumptions about the patient's drug exposure history and its impact on ADR occurrences over time, which many current methods do implicitly.   We propose an exposure model framework that explicitly models the longitudinal relationship between the drug and the ADR. By considering multiple such models simultaneously, we can detect signals that might be missed by other approaches. The parameters of these models are estimated using maximum likelihood, and the Bayesian Information Criterion (BIC) is employed to select the most suitable model. Since BIC is connected to the posterior distribution, it servers the dual purpose of identifying the best-fitting model and determining the presence of a signal by evaluating the posterior probability of the null model.   We evaluate the effectiveness of this framework through a simulation study, for which we develop an EHC data simulator. Additionally, we conduct a case study applying our approach to four drug-ADR pairs using an EHC dataset comprising over 1.2 million insured individuals. Both the method and the EHC data simulator code are publicly accessible as part of the R package https://github.com/bips-hb/expard.","sentences":["Despite extensive safety assessments of drugs prior to their introduction to the market, certain adverse drug reactions (ADRs) remain undetected.","The primary objective of pharmacovigilance is to identify these ADRs (i.e., signals).","In addition to traditional spontaneous reporting systems (SRSs), electronic health (EHC) data is being used for signal detection as well.","Unlike SRS, EHC data is longitudinal and thus requires assumptions about the patient's drug exposure history and its impact on ADR occurrences over time, which many current methods do implicitly.   ","We propose an exposure model framework that explicitly models the longitudinal relationship between the drug and the ADR.","By considering multiple such models simultaneously, we can detect signals that might be missed by other approaches.","The parameters of these models are estimated using maximum likelihood, and the Bayesian Information Criterion (BIC) is employed to select the most suitable model.","Since BIC is connected to the posterior distribution, it servers the dual purpose of identifying the best-fitting model and determining the presence of a signal by evaluating the posterior probability of the null model.   ","We evaluate the effectiveness of this framework through a simulation study, for which we develop an EHC data simulator.","Additionally, we conduct a case study applying our approach to four drug-ADR pairs using an EHC dataset comprising over 1.2 million insured individuals.","Both the method and the EHC data simulator code are publicly accessible as part of the R package https://github.com/bips-hb/expard."],"url":"http://arxiv.org/abs/2404.14213v1","category":"stat.ME"}
{"created":"2024-04-22 14:19:00","title":"Protostellar spin-up and fast rotator formation through binary star formation","abstract":"(Edited) Many fast rotator stars (rotation periods of < 2 days) are found in unresolved binaries with separations of tens of au. This correlation leads to the question of whether the formation of binary stars inherently produces fast rotators. We aim to understand whether the formation of companions plays a role in spinning up stars. We use magneto-hydrodynamical simulations to study the formation of multiple star systems from turbulent and non-turbulent protostellar cores. We track the angular momentum accreted by individual star and inner disc systems by using a sink particle technique. We run a resolution study to extrapolate protostellar properties. We find in all simulations that the primary star can experience spin-up events that are correlated with the formation of companions. The primary star can spin up by up to 84% of its pre-fragmentation angular momentum and by up to 18% of its pre-fragmentation mass-specific angular momentum. The mechanism for the spin-up is gravitational disc instabilities in the circumstellar disc around the primary star, leading to the accretion of material with high specific angular momentum. The simulations that experience the strongest disc instabilities fragment to form companions. Simulations with weaker spin-up events experience disc instabilities triggered by a companion flyby, and the disc instability in these cases does not produce further fragments. We conclude that the primary star in multiple star systems may end up with a higher spin than single stars. This is because gravitational instabilities in the circumstellar disc around the primary star can trigger a spin-up event. In the strongest spin-up events, the instability is likely to cause disc fragmentation and the formation of companions. This companion formation coupled with shorter disc lifetimes, because the companion truncates the circumstellar disc, can help produce fast rotators.","sentences":["(Edited) Many fast rotator stars (rotation periods of < 2 days) are found in unresolved binaries with separations of tens of au.","This correlation leads to the question of whether the formation of binary stars inherently produces fast rotators.","We aim to understand whether the formation of companions plays a role in spinning up stars.","We use magneto-hydrodynamical simulations to study the formation of multiple star systems from turbulent and non-turbulent protostellar cores.","We track the angular momentum accreted by individual star and inner disc systems by using a sink particle technique.","We run a resolution study to extrapolate protostellar properties.","We find in all simulations that the primary star can experience spin-up events that are correlated with the formation of companions.","The primary star can spin up by up to 84% of its pre-fragmentation angular momentum and by up to 18% of its pre-fragmentation mass-specific angular momentum.","The mechanism for the spin-up is gravitational disc instabilities in the circumstellar disc around the primary star, leading to the accretion of material with high specific angular momentum.","The simulations that experience the strongest disc instabilities fragment to form companions.","Simulations with weaker spin-up events experience disc instabilities triggered by a companion flyby, and the disc instability in these cases does not produce further fragments.","We conclude that the primary star in multiple star systems may end up with a higher spin than single stars.","This is because gravitational instabilities in the circumstellar disc around the primary star can trigger a spin-up event.","In the strongest spin-up events, the instability is likely to cause disc fragmentation and the formation of companions.","This companion formation coupled with shorter disc lifetimes, because the companion truncates the circumstellar disc, can help produce fast rotators."],"url":"http://arxiv.org/abs/2404.14210v1","category":"astro-ph.SR"}
{"created":"2024-04-22 14:18:28","title":"Minimizing the Number of Tardy Jobs with Uniform Processing Times on Parallel Machines","abstract":"In this work, we study the computational (parameterized) complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$. Here, we are given $m$ identical parallel machines and $n$ jobs with equal processing time, each characterized by a release date, a due date, and a weight. The task is to find a feasible schedule, that is, an assignment of the jobs to starting times on machines, such that no job starts before its release date and no machine processes several jobs at the same time, that minimizes the weighted number of tardy jobs. A job is considered tardy if it finishes after its due date.   Our main contribution is showing that $P \\mid r_j, p_j=p \\mid \\sum_j U_j$ (the unweighted version of the problem) is NP-hard and W[2]-hard when parameterized by the number of machines. The former resolves an open problem in Note 2.1.19 by Kravchenko and Werner [Journal of Scheduling, 2011] and Open Problem 2 by Sgall [ESA, 2012], and the latter resolves Open Problem 7 by Mnich and van Bevern [Computers & Operations Research, 2018]. Furthermore, our result shows that the known XP-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ parameterized by the number of machines is optimal from a classification standpoint.   On the algorithmic side, we provide alternative running time bounds for the above-mentioned known XP-algorithm. Our analysis shows that $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ is contained in XP when parameterized by the processing time, and that it is contained in FPT when parameterized by the combination of the number of machines and the processing time. Finally, we give an FPT-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$ parameterized by the number of release dates or the number of due dates. With this work, we lay out the foundation for a systematic study of the parameterized complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j U_j$.","sentences":["In this work, we study the computational (parameterized) complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$. Here, we are given $m$ identical parallel machines and $n$ jobs with equal processing time, each characterized by a release date, a due date, and a weight.","The task is to find a feasible schedule, that is, an assignment of the jobs to starting times on machines, such that no job starts before its release date and no machine processes several jobs at the same time, that minimizes the weighted number of tardy jobs.","A job is considered tardy if it finishes after its due date.   ","Our main contribution is showing that $P \\mid r_j, p_j=p \\mid \\sum_j U_j$ (the unweighted version of the problem) is NP-hard and W[2]-hard when parameterized by the number of machines.","The former resolves an open problem in Note 2.1.19 by Kravchenko and Werner [Journal of Scheduling, 2011] and Open Problem 2 by Sgall [ESA, 2012], and the latter resolves Open Problem 7 by Mnich and van Bevern [Computers & Operations Research, 2018].","Furthermore, our result shows that the known XP-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ parameterized by the number of machines is optimal from a classification standpoint.   ","On the algorithmic side, we provide alternative running time bounds for the above-mentioned known XP-algorithm.","Our analysis shows that $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ is contained in XP when parameterized by the processing time, and that it is contained in FPT when parameterized by the combination of the number of machines and the processing time.","Finally, we give an FPT-algorithm for $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$ parameterized by the number of release dates or the number of due dates.","With this work, we lay out the foundation for a systematic study of the parameterized complexity of $P \\mid r_j, p_j=p \\mid \\sum_j w_j","U_j$."],"url":"http://arxiv.org/abs/2404.14208v1","category":"cs.DS"}
{"created":"2024-04-22 14:06:35","title":"SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion","abstract":"Multivariate time series forecasting plays a crucial role in various fields such as finance, traffic management, energy, and healthcare. Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements. Several methods utilize mechanisms like attention or mixer to address this by capturing channel correlations, but they either introduce excessive complexity or rely too heavily on the correlation to achieve satisfactory results under distribution drifts, particularly with a large number of channels. Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Dispatch (STAD) module. Unlike traditional approaches that manage channel interactions through distributed structures, e.g., attention, STAD employs a centralized strategy. It aggregates all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions effectively. SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity. The broad applicability of the STAD module across different forecasting models is also demonstrated empirically. For further research and development, we have made our code publicly available at https://github.com/Secilia-Cxy/SOFTS.","sentences":["Multivariate time series forecasting plays a crucial role in various fields such as finance, traffic management, energy, and healthcare.","Recent studies have highlighted the advantages of channel independence to resist distribution drift but neglect channel correlations, limiting further enhancements.","Several methods utilize mechanisms like attention or mixer to address this by capturing channel correlations, but they either introduce excessive complexity or rely too heavily on the correlation to achieve satisfactory results under distribution drifts, particularly with a large number of channels.","Addressing this gap, this paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS), which incorporates a novel STar Aggregate-Dispatch (STAD) module.","Unlike traditional approaches that manage channel interactions through distributed structures, e.g., attention, STAD employs a centralized strategy.","It aggregates all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions effectively.","SOFTS achieves superior performance over existing state-of-the-art methods with only linear complexity.","The broad applicability of the STAD module across different forecasting models is also demonstrated empirically.","For further research and development, we have made our code publicly available at https://github.com/Secilia-Cxy/SOFTS."],"url":"http://arxiv.org/abs/2404.14197v1","category":"cs.LG"}
{"created":"2024-04-22 14:01:09","title":"Swap distance minimization beyond entropy minimization in word order variation","abstract":"Here we consider the problem of all the possible orders of a linguistic structure formed by $n$ elements, for instance, subject, direct object and verb ($n=3$) or subject, direct object, indirect object and verb ($n=4$). We investigate if the frequency of the $n!$ possible orders is constrained by two principles. First, entropy minimization, a principle that has been suggested to shape natural communication systems at distinct levels of organization. Second, swap distance minimization, namely a preference for word orders that require fewer swaps of adjacent elements to be produced from a source order. Here we present average swap distance, a novel score for research on swap distance minimization, and investigate the theoretical distribution of that score for any $n$: its minimum and maximum values and its expected value in die rolling experiments or when the word order frequencies are shuffled. We investigate whether entropy and average swap distance are significantly small in distinct linguistic structures with $n=3$ or $n=4$ in agreement with the corresponding minimization principles. We find strong evidence of entropy minimization and swap distance minimization with respect to a die rolling experiment. The evidence of these two forces with respect to a Polya urn process is strong for $n=4$ but weaker for $n=3$. We still find evidence of swap distance minimization when word order frequencies are shuffled, indicating that swap distance minimization effects are beyond pressure to minimize word order entropy.","sentences":["Here we consider the problem of all the possible orders of a linguistic structure formed by $n$ elements, for instance, subject, direct object and verb ($n=3$) or subject, direct object, indirect object and verb ($n=4$).","We investigate if the frequency of the $n!$ possible orders is constrained by two principles.","First, entropy minimization, a principle that has been suggested to shape natural communication systems at distinct levels of organization.","Second, swap distance minimization, namely a preference for word orders that require fewer swaps of adjacent elements to be produced from a source order.","Here we present average swap distance, a novel score for research on swap distance minimization, and investigate the theoretical distribution of that score for any $n$: its minimum and maximum values and its expected value in die rolling experiments or when the word order frequencies are shuffled.","We investigate whether entropy and average swap distance are significantly small in distinct linguistic structures with $n=3$ or $n=4$ in agreement with the corresponding minimization principles.","We find strong evidence of entropy minimization and swap distance minimization with respect to a die rolling experiment.","The evidence of these two forces with respect to a Polya urn process is strong for $n=4$ but weaker for $n=3$. We still find evidence of swap distance minimization when word order frequencies are shuffled, indicating that swap distance minimization effects are beyond pressure to minimize word order entropy."],"url":"http://arxiv.org/abs/2404.14192v1","category":"cs.CL"}
{"created":"2024-04-22 13:57:00","title":"Modeling the focusing of a radially-polarized laser beam with an initially flat-top intensity profile","abstract":"Radially-polarized light beams present very interesting and useful behavior for creating small intensity spots when tightly-focused, and manipulating nanostructures or charged particles. The modeling of the propagation of such vector beams, however, is almost always done using the lowest-order fundamental radially-polarized beam due to the complexity of vector diffraction theory. We show how a flat-top radially-polarized beam can be modeled analytically using a sum of higher-order beams, and describe a number of interesting qualities, and compare to numerically-solved integral descriptions.","sentences":["Radially-polarized light beams present very interesting and useful behavior for creating small intensity spots when tightly-focused, and manipulating nanostructures or charged particles.","The modeling of the propagation of such vector beams, however, is almost always done using the lowest-order fundamental radially-polarized beam due to the complexity of vector diffraction theory.","We show how a flat-top radially-polarized beam can be modeled analytically using a sum of higher-order beams, and describe a number of interesting qualities, and compare to numerically-solved integral descriptions."],"url":"http://arxiv.org/abs/2404.14185v1","category":"physics.optics"}
{"created":"2024-04-22 13:56:31","title":"Kinematic morphology of low-mass galaxies in IllustrisTNG","abstract":"The origin of diverse kinematic morphologies observed in low-mass galaxies is unclear. In this study, we investigate the kinematic morphologies of central galaxies with stellar mass $10^{8.5-9.0} M_{\\odot}$ at $z=0$ in the TNG50-1 cosmological simulation. The majority of the low-mass galaxies in TNG50-1 are dispersion-dominated, consistent with observations. By tracing the evolutionary histories of simulated low-mass galaxies, we find that while most stars form in rotating cold gas discs, the orientation of the star-forming discs relative to the galaxies may evolve with cosmic time. If the cold gas disc remains aligning with the galaxy during its evolution, stars formed at different times share the same rotational direction, leading to a rotation-dominated system. On the contrary, frequent misalignment of cold gas disc would result in a dispersion-dominated system. In addition, we also find that the two-body scattering can have a non-negligible numerical heating effect on the simulated galaxy morphology, especially at central regions of galaxies and for relatively low-mass galaxies. By comparing results of simulations with different resolutions, our results suggest that the simulated morphology of galaxies is reliable when their number of stellar particles exceeds about $10^{4}$, and bulge morphology of galaxies can not be resolved robustly at the resolution level of TNG50-1.","sentences":["The origin of diverse kinematic morphologies observed in low-mass galaxies is unclear.","In this study, we investigate the kinematic morphologies of central galaxies with stellar mass $10^{8.5-9.0} M_{\\odot}$ at $z=0$ in the TNG50-1 cosmological simulation.","The majority of the low-mass galaxies in TNG50-1 are dispersion-dominated, consistent with observations.","By tracing the evolutionary histories of simulated low-mass galaxies, we find that while most stars form in rotating cold gas discs, the orientation of the star-forming discs relative to the galaxies may evolve with cosmic time.","If the cold gas disc remains aligning with the galaxy during its evolution, stars formed at different times share the same rotational direction, leading to a rotation-dominated system.","On the contrary, frequent misalignment of cold gas disc would result in a dispersion-dominated system.","In addition, we also find that the two-body scattering can have a non-negligible numerical heating effect on the simulated galaxy morphology, especially at central regions of galaxies and for relatively low-mass galaxies.","By comparing results of simulations with different resolutions, our results suggest that the simulated morphology of galaxies is reliable when their number of stellar particles exceeds about $10^{4}$, and bulge morphology of galaxies can not be resolved robustly at the resolution level of TNG50-1."],"url":"http://arxiv.org/abs/2404.14184v1","category":"astro-ph.GA"}
{"created":"2024-04-22 13:49:42","title":"Face2Face: Label-driven Facial Retouching Restoration","abstract":"With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs. This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media. By altering facial images, users can easily create deceptive images, leading to the dissemination of false information. This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud. To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching. To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN). Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees. Then FaceR restores the retouched image based on the predicted retouching label. Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models. Extensive experiments demonstrate the effectiveness of our framework and each module.","sentences":["With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs.","This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media.","By altering facial images, users can easily create deceptive images, leading to the dissemination of false information.","This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud.","To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching.","To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN).","Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees.","Then FaceR restores the retouched image based on the predicted retouching label.","Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models.","Extensive experiments demonstrate the effectiveness of our framework and each module."],"url":"http://arxiv.org/abs/2404.14177v1","category":"cs.CV"}
{"created":"2024-04-22 13:47:19","title":"Non-equilibrium structure and relaxation in active microemulsions","abstract":"Microphase separation is common in active biological systems as exemplified by the separation of RNA and DNA-rich phases in the cell nucleus driven by the transcriptional activity of polymerase enzymes acting similarly to amphiphiles in a microemulsion. Here we propose an analytically tractable model of an active microemulsion to investigate how the activity affects its structure and relaxation dynamics. Continuum theory derived from a lattice model exhibits two distinct regimes of the relaxation dynamics and is linked to the broken detailed balance due to intermittent activity of the amphiphiles.","sentences":["Microphase separation is common in active biological systems as exemplified by the separation of RNA and DNA-rich phases in the cell nucleus driven by the transcriptional activity of polymerase enzymes acting similarly to amphiphiles in a microemulsion.","Here we propose an analytically tractable model of an active microemulsion to investigate how the activity affects its structure and relaxation dynamics.","Continuum theory derived from a lattice model exhibits two distinct regimes of the relaxation dynamics and is linked to the broken detailed balance due to intermittent activity of the amphiphiles."],"url":"http://arxiv.org/abs/2404.14176v1","category":"cond-mat.soft"}
{"created":"2024-04-22 13:43:38","title":"Nonadiabatic excited-state dynamics and energy gradients in the framework of FMO-LC-TDDFTB","abstract":"We introduce a novel methodology for simulating the excited-state dynamics of extensive molecular aggregates in the framework of the long-range corrected time-dependent density-functional tight-binding fragment molecular orbital method (FMO-LC-TDDFTB) combined with the mean-field Ehrenfest method. The electronic structure of the system is described in a quasi-diabatic basis composed of locally excited and charge-transfer states of all fragments. In order to carry out nonadiabatic molecular dynamics simulatios, we derive and implement the excited-state gradients of the locally excited and charge-transfer states. Subsequently, the accuracy of the analytical excited-state gradients is evaluated. The applicability to the simulation of exciton transport in organic semiconductors is illustrated on a large cluster of anthracene molecules. Additionally, nonadiabatic molecular dynamics simulations of a model system of benzothieno-benzothiophene molecules highlight the methods utility in studying charge-transfer dynamics in organic materials. Our new methodology will facilitate the investigation of excitonic transfer in extensive biological systems, nanomaterials and other complex molecular systems consisting of thousands of atoms.","sentences":["We introduce a novel methodology for simulating the excited-state dynamics of extensive molecular aggregates in the framework of the long-range corrected time-dependent density-functional tight-binding fragment molecular orbital method (FMO-LC-TDDFTB) combined with the mean-field Ehrenfest method.","The electronic structure of the system is described in a quasi-diabatic basis composed of locally excited and charge-transfer states of all fragments.","In order to carry out nonadiabatic molecular dynamics simulatios, we derive and implement the excited-state gradients of the locally excited and charge-transfer states.","Subsequently, the accuracy of the analytical excited-state gradients is evaluated.","The applicability to the simulation of exciton transport in organic semiconductors is illustrated on a large cluster of anthracene molecules.","Additionally, nonadiabatic molecular dynamics simulations of a model system of benzothieno-benzothiophene molecules highlight the methods utility in studying charge-transfer dynamics in organic materials.","Our new methodology will facilitate the investigation of excitonic transfer in extensive biological systems, nanomaterials and other complex molecular systems consisting of thousands of atoms."],"url":"http://arxiv.org/abs/2404.14174v1","category":"physics.chem-ph"}
{"created":"2024-04-22 13:41:38","title":"Noiseless linear amplification-based quantum Ziv-Zakai bound for phase estimation and its Heisenberg error limits in noisy scenarios","abstract":"In this work, we address the central problem about how to effectively find the available precision limit of unknown parameters. In the framework of the quantum Ziv-Zakai bound (QZZB), we employ noiseless linear amplification (NLA)techniques to an initial coherent state (CS) as the probe state, and focus on whether the phase estimation performance is improved significantly in noisy scenarios, involving the photon-loss and phase-diffusion cases. More importantly, we also obtain two kinds of Heisenberg error limits of the QZZB with the NLA-based CS in these noisy scenarios, making comparisons with both the Margolus-Levitin (ML) type bound and the Mandelstam-Tamm (MT) type bound. Our analytical results show that in cases of photon loss and phase diffusion, the phase estimation performance of the QZZB can be improved remarkably by increasing the NLA gain factor. Particularly, the improvement is more pronounced with severe photon losses. Furthermore in minimal photon losses, our Heisenberg error limit shows better compactness than the cases of the ML-type and MT-type bounds. Our findings will provide an useful guidance for accomplishing more complex quantum information processing tasks.","sentences":["In this work, we address the central problem about how to effectively find the available precision limit of unknown parameters.","In the framework of the quantum Ziv-Zakai bound (QZZB), we employ noiseless linear amplification (NLA)techniques to an initial coherent state (CS) as the probe state, and focus on whether the phase estimation performance is improved significantly in noisy scenarios, involving the photon-loss and phase-diffusion cases.","More importantly, we also obtain two kinds of Heisenberg error limits of the QZZB with the NLA-based CS in these noisy scenarios, making comparisons with both the Margolus-Levitin (ML) type bound and the Mandelstam-Tamm (MT) type bound.","Our analytical results show that in cases of photon loss and phase diffusion, the phase estimation performance of the QZZB can be improved remarkably by increasing the NLA gain factor.","Particularly, the improvement is more pronounced with severe photon losses.","Furthermore in minimal photon losses, our Heisenberg error limit shows better compactness than the cases of the ML-type and MT-type bounds.","Our findings will provide an useful guidance for accomplishing more complex quantum information processing tasks."],"url":"http://arxiv.org/abs/2404.14173v1","category":"quant-ph"}
{"created":"2024-04-22 13:39:36","title":"A Simple Molecular Model for Hydrated Silicate Ionic Liquids, a Realistic Zeolite Precursor","abstract":"Despite the widespread use of zeolites in chemical industry, their formation process is not fully understood due to the complex and heterogeneous structure of traditional synthesis media. Hydrated silicate ionic liquids (HSILs) have been proposed as an alternative. They are truly homogeneous and transparent mixtures with low viscosity, facilitating experimental characterization. Interestingly, their homogeneous nature and simple speciation brings realistic molecular models of a zeolite growth liquid within reach for the first time. In this work, a simple molecular model is developed that gives insight into the crucial role of the alkali cations (sodium, potassium, rubidium and cesium). Thereby molecular dynamics simulations are combined with experimental measurements to demonstrate that the HSIL liquid structure strongly depends on the charge density and concentration of the alkali cation. As the water content increases, it transitions from a glassy network with fast ion exchange to an aqueous solution containing long-lasting, solvated ion pairs. Furthermore, simulations reveal that the cation is capable of bringing several silicate monomers together in the glassy network, displaying perfect orientations for condensation reactions that underlie zeolite formation. This work is an important step towards the development of molecular models that can fully describe the early nucleation process of zeolites in combination with experiments.","sentences":["Despite the widespread use of zeolites in chemical industry, their formation process is not fully understood due to the complex and heterogeneous structure of traditional synthesis media.","Hydrated silicate ionic liquids (HSILs) have been proposed as an alternative.","They are truly homogeneous and transparent mixtures with low viscosity, facilitating experimental characterization.","Interestingly, their homogeneous nature and simple speciation brings realistic molecular models of a zeolite growth liquid within reach for the first time.","In this work, a simple molecular model is developed that gives insight into the crucial role of the alkali cations (sodium, potassium, rubidium and cesium).","Thereby molecular dynamics simulations are combined with experimental measurements to demonstrate that the HSIL liquid structure strongly depends on the charge density and concentration of the alkali cation.","As the water content increases, it transitions from a glassy network with fast ion exchange to an aqueous solution containing long-lasting, solvated ion pairs.","Furthermore, simulations reveal that the cation is capable of bringing several silicate monomers together in the glassy network, displaying perfect orientations for condensation reactions that underlie zeolite formation.","This work is an important step towards the development of molecular models that can fully describe the early nucleation process of zeolites in combination with experiments."],"url":"http://arxiv.org/abs/2404.14170v1","category":"physics.chem-ph"}
{"created":"2024-04-22 13:32:29","title":"Sliding window-aided ordered statistics decoding for short LDPC codes","abstract":"This paper introduces an innovative approach to the design of efficient decoders that meet the rigorous requirements of modern communication systems, particularly in terms of ultra-reliability and low latency. We enhance an established hybrid decoding framework by proposing an ordered statistical decoding scheme augmented with a sliding window technique. This novel component replaces a key element of the current architecture, significantly reducing average complexity. A critical aspect of our scheme is the integration of a pre-trained neural network model that dynamically determines the progression or halt of the sliding window process. Furthermore, we present a user-defined soft margin mechanism that adeptly balances the trade-off between decoding accuracy and complexity. Empirical results, supported by a thorough complexity analysis, demonstrate that the proposed scheme holds a competitive advantage over existing state-of-the-art decoders, notably in addressing the decoding failures prevalent in neural min-sum decoders. Additionally, our research uncovers that short LDPC codes can deliver performance comparable to that of short classical linear codes within the critical waterfall region of the SNR, highlighting their potential for practical applications.","sentences":["This paper introduces an innovative approach to the design of efficient decoders that meet the rigorous requirements of modern communication systems, particularly in terms of ultra-reliability and low latency.","We enhance an established hybrid decoding framework by proposing an ordered statistical decoding scheme augmented with a sliding window technique.","This novel component replaces a key element of the current architecture, significantly reducing average complexity.","A critical aspect of our scheme is the integration of a pre-trained neural network model that dynamically determines the progression or halt of the sliding window process.","Furthermore, we present a user-defined soft margin mechanism that adeptly balances the trade-off between decoding accuracy and complexity.","Empirical results, supported by a thorough complexity analysis, demonstrate that the proposed scheme holds a competitive advantage over existing state-of-the-art decoders, notably in addressing the decoding failures prevalent in neural min-sum decoders.","Additionally, our research uncovers that short LDPC codes can deliver performance comparable to that of short classical linear codes within the critical waterfall region of the SNR, highlighting their potential for practical applications."],"url":"http://arxiv.org/abs/2404.14165v1","category":"cs.IT"}
{"created":"2024-04-22 13:13:14","title":"Autonomous Forest Inventory with Legged Robots: System Design and Field Deployment","abstract":"We present a solution for autonomous forest inventory with a legged robotic platform. Compared to their wheeled and aerial counterparts, legged platforms offer an attractive balance of endurance and low soil impact for forest applications. In this paper, we present the complete system architecture of our forest inventory solution which includes state estimation, navigation, mission planning, and real-time tree segmentation and trait estimation. We present preliminary results for three campaigns in forests in Finland and the UK and summarize the main outcomes, lessons, and challenges. Our UK experiment at the Forest of Dean with the ANYmal D legged platform, achieved an autonomous survey of a 0.96 hectare plot in 20 min, identifying over 100 trees with typical DBH accuracy of 2 cm.","sentences":["We present a solution for autonomous forest inventory with a legged robotic platform.","Compared to their wheeled and aerial counterparts, legged platforms offer an attractive balance of endurance and low soil impact for forest applications.","In this paper, we present the complete system architecture of our forest inventory solution which includes state estimation, navigation, mission planning, and real-time tree segmentation and trait estimation.","We present preliminary results for three campaigns in forests in Finland and the UK and summarize the main outcomes, lessons, and challenges.","Our UK experiment at the Forest of Dean with the ANYmal D legged platform, achieved an autonomous survey of a 0.96 hectare plot in 20 min, identifying over 100 trees with typical DBH accuracy of 2 cm."],"url":"http://arxiv.org/abs/2404.14157v1","category":"cs.RO"}
{"created":"2024-04-22 12:56:22","title":"Solutions of local and nonlocal discrete complex modified Korteweg-de Vries equations and continuum limits","abstract":"Cauchy matrix approach for the discrete Ablowitz-Kaup-Newell-Segur equations is reconsidered, where two `proper' discrete Ablowitz-Kaup-Newell-Segur equations and two `unproper' discrete Ablowitz-Kaup-Newell-Segur equations are derived. The `proper' equations admit local reduction, while the `unproper' equations admit nonlocal reduction. By imposing the local and nonlocal complex reductions on the obtained discrete Ablowitz-Kaup-Newell-Segur equations, two local and nonlocal discrete complex modified Korteweg-de Vries equations are constructed. For the obtained local and nonlocal discrete complex modified Korteweg-de Vries equations, soliton solutions and Jordan-block solutions are presented by solving the determining equation set. The dynamical behaviors of 1-soliton solution are analyzed and illustrated. Continuum limits of the resulting local and nonlocal discrete complex modified Korteweg-de Vries equations are discussed.","sentences":["Cauchy matrix approach for the discrete Ablowitz-Kaup-Newell-Segur equations is reconsidered, where two `proper' discrete Ablowitz-Kaup-Newell-Segur equations and two `unproper' discrete Ablowitz-Kaup-Newell-Segur equations are derived.","The `proper' equations admit local reduction, while the `unproper' equations admit nonlocal reduction.","By imposing the local and nonlocal complex reductions on the obtained discrete Ablowitz-Kaup-Newell-Segur equations, two local and nonlocal discrete complex modified Korteweg-de Vries equations are constructed.","For the obtained local and nonlocal discrete complex modified Korteweg-de Vries equations, soliton solutions and Jordan-block solutions are presented by solving the determining equation set.","The dynamical behaviors of 1-soliton solution are analyzed and illustrated.","Continuum limits of the resulting local and nonlocal discrete complex modified Korteweg-de Vries equations are discussed."],"url":"http://arxiv.org/abs/2404.14150v1","category":"nlin.SI"}
{"created":"2024-04-22 12:55:33","title":"Cavity-enhanced Kondo effect","abstract":"In metals containing magnetic impurities, conduction electrons screen the magnetic impurities and induce the Kondo effect, i.e., the enhancement of the electrical resistance at low temperatures. Motivated by recent advances in manipulating quantum materials by cavity confinement, we study how the ultrastrong light-matter coupling can affect the Kondo effect. We show that the ultrastrong coupling can enhance the Kondo temperature and give rise to several notable phenomena, including universal scalings of the cavity-modified Kondo effect, the photon occupation number, and the entanglement entropy between the cavity and electrons. The origin of the cavity enhancement can be understood from the mass renormalization due to the cavity-mediated nonlocal electron-electron interaction, which is akin to the polaronic mass enhancement. We combine the unitary transformations and the Gaussian variational states to analyze the quantum impurity system confined in the cavity. Our nonperturbative framework can be applied to a variety of quantum impurity problems influenced by structured quantum electromagnetic environment.","sentences":["In metals containing magnetic impurities, conduction electrons screen the magnetic impurities and induce the Kondo effect, i.e., the enhancement of the electrical resistance at low temperatures.","Motivated by recent advances in manipulating quantum materials by cavity confinement, we study how the ultrastrong light-matter coupling can affect the Kondo effect.","We show that the ultrastrong coupling can enhance the Kondo temperature and give rise to several notable phenomena, including universal scalings of the cavity-modified Kondo effect, the photon occupation number, and the entanglement entropy between the cavity and electrons.","The origin of the cavity enhancement can be understood from the mass renormalization due to the cavity-mediated nonlocal electron-electron interaction, which is akin to the polaronic mass enhancement.","We combine the unitary transformations and the Gaussian variational states to analyze the quantum impurity system confined in the cavity.","Our nonperturbative framework can be applied to a variety of quantum impurity problems influenced by structured quantum electromagnetic environment."],"url":"http://arxiv.org/abs/2404.14148v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 12:55:21","title":"Travelling waves in an ensemble of excitable oscillators: the interplay of memristive coupling and noise","abstract":"Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model. First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling. In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction. In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves.","sentences":["Using methods of numerical simulation, we demonstrate the constructive role of memristive coupling in the context of the travelling waves formation and robustness in an ensemble of excitable oscillators described by the FitzHugh-Nagumo neuron model.","First, the revealed aspects of the memristive coupling action are shown on an example of the deterministic model where the memristive properties of the coupling elements provide for achieving travelling waves at lower coupling strength as compared to non-adaptive diffusive coupling.","In the presence of noise, the positive role of memristive coupling is manifested as significant increasing a noise intensity critical value corresponding to the noise-induced destruction of travelling waves as compared to classical diffusive interaction.","In addition, we point out the second constructive factor, the L{\\'e}vy noise whose properties provide for inducing travelling waves."],"url":"http://arxiv.org/abs/2404.14147v1","category":"nlin.AO"}
{"created":"2024-04-22 12:55:04","title":"Physics-based reward driven image analysis in microscopy","abstract":"The rise of electron microscopy has expanded our ability to acquire nanometer and atomically resolved images of complex materials. The resulting vast datasets are typically analyzed by human operators, an intrinsically challenging process due to the multiple possible analysis steps and the corresponding need to build and optimize complex analysis workflows. We present a methodology based on the concept of a Reward Function coupled with Bayesian Optimization, to optimize image analysis workflows dynamically. The Reward Function is engineered to closely align with the experimental objectives and broader context and is quantifiable upon completion of the analysis. Here, cross-section, high-angle annular dark field (HAADF) images of ion-irradiated $(Y, Dy)Ba_2Cu_3O_{7-\\delta}$ thin-films were used as a model system. The reward functions were formed based on the expected materials density and atomic spacings and used to drive multi-objective optimization of the classical Laplacian-of-Gaussian (LoG) method. These results can be benchmarked against the DCNN segmentation. This optimized LoG* compares favorably against DCNN in the presence of the additional noise. We further extend the reward function approach towards the identification of partially-disordered regions, creating a physics-driven reward function and action space of high-dimensional clustering. We pose that with correct definition, the reward function approach allows real-time optimization of complex analysis workflows at much higher speeds and lower computational costs than classical DCNN-based inference, ensuring the attainment of results that are both precise and aligned with the human-defined objectives.","sentences":["The rise of electron microscopy has expanded our ability to acquire nanometer and atomically resolved images of complex materials.","The resulting vast datasets are typically analyzed by human operators, an intrinsically challenging process due to the multiple possible analysis steps and the corresponding need to build and optimize complex analysis workflows.","We present a methodology based on the concept of a Reward Function coupled with Bayesian Optimization, to optimize image analysis workflows dynamically.","The Reward Function is engineered to closely align with the experimental objectives and broader context and is quantifiable upon completion of the analysis.","Here, cross-section, high-angle annular dark field (HAADF) images of ion-irradiated $(Y, Dy)Ba_2Cu_3O_{7-\\delta}$ thin-films were used as a model system.","The reward functions were formed based on the expected materials density and atomic spacings and used to drive multi-objective optimization of the classical Laplacian-of-Gaussian (LoG) method.","These results can be benchmarked against the DCNN segmentation.","This optimized LoG* compares favorably against DCNN in the presence of the additional noise.","We further extend the reward function approach towards the identification of partially-disordered regions, creating a physics-driven reward function and action space of high-dimensional clustering.","We pose that with correct definition, the reward function approach allows real-time optimization of complex analysis workflows at much higher speeds and lower computational costs than classical DCNN-based inference, ensuring the attainment of results that are both precise and aligned with the human-defined objectives."],"url":"http://arxiv.org/abs/2404.14146v2","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 12:51:57","title":"Fast Inclusive Flavour Tagging at LHCb","abstract":"The task of identifying B meson flavor at the primary interaction point in the LHCb detector is crucial for measurements of mixing and time-dependent CP violation. Flavour tagging is usually done with a small number of expert systems that find important tracks to infer the B meson flavour from. Recent advances show that replacing all of those expert systems with one ML algorithm that considers all tracks in an event yields an increase in tagging power. However, training the current classifier takes a long time and is not suitable for use in real-time triggers. In this work we present a new classifier, based on the DeepSet architecture. With the right inductive bias of permutation invariance, we achieve great speedups in training (multiple hours vs 10 minutes), a factor of 4-5 speed-up in inference for use in real time environments like the trigger and less tagging asymmetry. For the first time we investigate and compare performances of these Inclusive Flavor Taggers on simulation of the upgraded LHCb detector for the third run of the LHC.","sentences":["The task of identifying B meson flavor at the primary interaction point in the LHCb detector is crucial for measurements of mixing and time-dependent CP violation.","Flavour tagging is usually done with a small number of expert systems that find important tracks to infer the B meson flavour from.","Recent advances show that replacing all of those expert systems with one ML algorithm that considers all tracks in an event yields an increase in tagging power.","However, training the current classifier takes a long time and is not suitable for use in real-time triggers.","In this work we present a new classifier, based on the DeepSet architecture.","With the right inductive bias of permutation invariance, we achieve great speedups in training (multiple hours vs 10 minutes), a factor of 4-5 speed-up in inference for use in real time environments like the trigger and less tagging asymmetry.","For the first time we investigate and compare performances of these Inclusive Flavor Taggers on simulation of the upgraded LHCb detector for the third run of the LHC."],"url":"http://arxiv.org/abs/2404.14145v1","category":"hep-ex"}
{"created":"2024-04-22 12:30:20","title":"X-shooter spectroscopy of Liller1 giant stars","abstract":"We present the first comprehensive chemical study of a representative sample of 27 luminous red giant branch (RGB) stars belonging to Liller 1, a complex stellar system in the Galactic bulge. This study is based on medium-resolution near-infrared spectra acquired with X-shooter at the Very Large Telescope. We found a subpopulation counting 22 stars with subsolar metallicity ($<$[Fe/H]$>=-0.31\\pm0.02$ and 1$\\sigma$ dispersion of 0.08 dex) and with enhanced [$\\alpha$/Fe], [Al/Fe], and [K/Fe] that likely formed early and quickly from gas that was mainly enriched by type II supernovae, and a metal-rich population counting 5 stars with supersolar metallicity ($<$[Fe/H]$>$=+0.22$\\pm$0.03 and 1$\\sigma$ dispersion of 0.06 dex) and roughly solar-scaled [$\\alpha$/Fe], [Al/Fe], and [K/Fe] that formed at later epochs from gas that was also enriched by type Ia supernovae. Moreover, both subpopulations show enhanced [Na/Fe], as in the bulge field, about solar-scaled [V/Fe], and depletion of [C/Fe] and $^{12}$C/$^{13}$C with respect to the solar values. This indicates that mixing and extra-mixing processes during the RGB evolution also occur at very high metallicities. Notably, no evidence of a Na-O anticorrelation, which is considered the fingerprint of genuine globular clusters, has been found. This challenges any formation scenarios that invoke the accretion of a molecular cloud or an additional stellar system onto a genuine globular cluster. The results of this study underline the strong chemical similarity between Liller 1 and Terzan 5 and support the hypothesis that these complex stellar systems might be fossil fragments of the epoch of Galactic bulge formation.","sentences":["We present the first comprehensive chemical study of a representative sample of 27 luminous red giant branch (RGB) stars belonging to Liller 1, a complex stellar system in the Galactic bulge.","This study is based on medium-resolution near-infrared spectra acquired with X-shooter at the Very Large Telescope.","We found a subpopulation counting 22 stars with subsolar metallicity ($<$[Fe/H]$>=-0.31\\pm0.02$ and 1$\\sigma$ dispersion of 0.08 dex) and with enhanced [$\\alpha$/Fe], [Al/Fe], and [K/Fe] that likely formed early and quickly from gas that was mainly enriched by type II supernovae, and a metal-rich population counting 5 stars with supersolar metallicity ($<$[Fe/H]$>$=+0.22$\\pm$0.03 and 1$\\sigma$ dispersion of 0.06 dex) and roughly solar-scaled [$\\alpha$/Fe], [Al/Fe], and [K/Fe] that formed at later epochs from gas that was also enriched by type Ia supernovae.","Moreover, both subpopulations show enhanced [Na/Fe], as in the bulge field, about solar-scaled [V/Fe], and depletion of [C/Fe] and $^{12}$C/$^{13}$C with respect to the solar values.","This indicates that mixing and extra-mixing processes during the RGB evolution also occur at very high metallicities.","Notably, no evidence of a Na-O anticorrelation, which is considered the fingerprint of genuine globular clusters, has been found.","This challenges any formation scenarios that invoke the accretion of a molecular cloud or an additional stellar system onto a genuine globular cluster.","The results of this study underline the strong chemical similarity between Liller 1 and Terzan 5 and support the hypothesis that these complex stellar systems might be fossil fragments of the epoch of Galactic bulge formation."],"url":"http://arxiv.org/abs/2404.14130v1","category":"astro-ph.GA"}
{"created":"2024-04-22 12:27:30","title":"Individual Rationality in Topological Distance Games is Surprisingly Hard","abstract":"In the recently introduced topological distance games, strategic agents need to be assigned to a subset of vertices of a topology. In the assignment, the utility of an agent depends on both the agent's inherent utilities for other agents and its distance from them on the topology. We study the computational complexity of finding individually rational outcomes; this notion is widely assumed to be the very minimal stability requirement and requires that the utility of every agent in a solution is non-negative. We perform a comprehensive study of the problem's complexity, and we prove that even in very basic cases, deciding whether an individually rational solution exists is intractable. To reach at least some tractability, one needs to combine multiple restrictions of the input instance, including the number of agents and the topology and the influence of distant agents on the utility.","sentences":["In the recently introduced topological distance games, strategic agents need to be assigned to a subset of vertices of a topology.","In the assignment, the utility of an agent depends on both the agent's inherent utilities for other agents and its distance from them on the topology.","We study the computational complexity of finding individually rational outcomes; this notion is widely assumed to be the very minimal stability requirement and requires that the utility of every agent in a solution is non-negative.","We perform a comprehensive study of the problem's complexity, and we prove that even in very basic cases, deciding whether an individually rational solution exists is intractable.","To reach at least some tractability, one needs to combine multiple restrictions of the input instance, including the number of agents and the topology and the influence of distant agents on the utility."],"url":"http://arxiv.org/abs/2404.14128v1","category":"cs.GT"}
{"created":"2024-04-22 12:05:34","title":"Anomalous dispersion via dissipative coupling in a quantum well exciton-polariton microcavity","abstract":"According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian. This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel. This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion. However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system. Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion. We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample. The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model. Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems.","sentences":["According to the principles of quantum mechanics the Hamiltonian describing a closed system's energies must be Hermitian.","This leads to an avoided crossing on resonance, as coupling between states causes the energy levels to repel.","This concept lies at the heart of exciton-polariton physics, where coherent exciton-photon interaction causes polariton branches to repel in momentum dispersion.","However, non-Hermitian physics predicts an opposite effect: level attraction, which occurs when significant energy dissipation is present in the system.","Here, we show a manifestation of dissipative coupling in a high-quality AlGaAs-based polariton microcavity, where two polariton branches attract, resulting in an anomalous, inverted dispersion of the lower branch in momentum dispersion.","We observe the evolution of the level attraction with exciton-photon detuning, leading to changes in anomalous dispersion shape within a single sample.","The dissipative coupling is explained by the interaction with an indirect exciton, acting as a highly dissipative channel in our system, and the observed dispersions are well captured within a phenomenological model.","Our results present a new mechanism of dissipative coupling in light-matter systems and offer a tunable and well-controlled AlGaAs-based platform for engineering the non-Hermitian and negative mass effects in polariton systems."],"url":"http://arxiv.org/abs/2404.14116v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 12:03:49","title":"Pricing of European Calls with the Quantum Fourier Transform","abstract":"The accurate valuation of financial derivatives plays a pivotal role in the finance industry. Although closed formulas for pricing are available for certain models and option types, exemplified by the European Call and Put options in the Black-Scholes Model, the use of either more complex models or more sophisticated options precludes the existence of such formulas, thereby requiring alternative approaches. The Monte Carlo simulation, an alternative approach effective in nearly all scenarios, has already been challenged by quantum computing techniques that leverage Amplitude Estimation. Despite its theoretical promise, this approach currently faces limitations due to the constraints of hardware in the Noisy Intermediate-Scale Quantum (NISQ) era.   In this study, we introduce and analyze a quantum algorithm for pricing European call options across a broad spectrum of asset models. This method transforms a classical approach, which utilizes the Fast Fourier Transform (FFT), into a quantum algorithm, leveraging the efficiency of the Quantum Fourier Transform (QFT). Furthermore, we compare this novel algorithm with existing quantum algorithms for option pricing.","sentences":["The accurate valuation of financial derivatives plays a pivotal role in the finance industry.","Although closed formulas for pricing are available for certain models and option types, exemplified by the European Call and Put options in the Black-Scholes Model, the use of either more complex models or more sophisticated options precludes the existence of such formulas, thereby requiring alternative approaches.","The Monte Carlo simulation, an alternative approach effective in nearly all scenarios, has already been challenged by quantum computing techniques that leverage Amplitude Estimation.","Despite its theoretical promise, this approach currently faces limitations due to the constraints of hardware in the Noisy Intermediate-Scale Quantum (NISQ) era.   ","In this study, we introduce and analyze a quantum algorithm for pricing European call options across a broad spectrum of asset models.","This method transforms a classical approach, which utilizes the Fast Fourier Transform (FFT), into a quantum algorithm, leveraging the efficiency of the Quantum Fourier Transform (QFT).","Furthermore, we compare this novel algorithm with existing quantum algorithms for option pricing."],"url":"http://arxiv.org/abs/2404.14115v1","category":"quant-ph"}
{"created":"2024-04-22 12:03:43","title":"Dionysos.jl: a Modular Platform for Smart Symbolic Control","abstract":"We introduce Dionysos.jl, a modular package for solving optimal control problems for complex dynamical systems using state-of-the-art and experimental techniques from symbolic control, optimization, and learning. More often than not with Cyber-Physical systems, the only sensible way of developing a controller is by discretizing the different variables, thus transforming the control task into a purely combinatorial problem on a finite-state mathematical object, called an abstraction of this system. Although this approach offers a safety-critical framework, the available techniques suffer important scalability issues. In order to render these techniques practical, it is necessary to construct smarter abstractions that differ from classical techniques by partitioning the state-space in a non trivial way.","sentences":["We introduce Dionysos.jl, a modular package for solving optimal control problems for complex dynamical systems using state-of-the-art and experimental techniques from symbolic control, optimization, and learning.","More often than not with Cyber-Physical systems, the only sensible way of developing a controller is by discretizing the different variables, thus transforming the control task into a purely combinatorial problem on a finite-state mathematical object, called an abstraction of this system.","Although this approach offers a safety-critical framework, the available techniques suffer important scalability issues.","In order to render these techniques practical, it is necessary to construct smarter abstractions that differ from classical techniques by partitioning the state-space in a non trivial way."],"url":"http://arxiv.org/abs/2404.14114v1","category":"eess.SY"}
{"created":"2024-04-22 11:52:40","title":"CKD: Contrastive Knowledge Distillation from A Sample-wise Perspective","abstract":"In this paper, we present a simple yet effective contrastive knowledge distillation approach, which can be formulated as a sample-wise alignment problem with intra- and inter-sample constraints. Unlike traditional knowledge distillation methods that concentrate on maximizing feature similarities or preserving class-wise semantic correlations between teacher and student features, our method attempts to recover the \"dark knowledge\" by aligning sample-wise teacher and student logits. Specifically, our method first minimizes logit differences within the same sample by considering their numerical values, thus preserving intra-sample similarities. Next, we bridge semantic disparities by leveraging dissimilarities across different samples. Note that constraints on intra-sample similarities and inter-sample dissimilarities can be efficiently and effectively reformulated into a contrastive learning framework with newly designed positive and negative pairs. The positive pair consists of the teacher's and student's logits derived from an identical sample, while the negative pairs are formed by using logits from different samples. With this formulation, our method benefits from the simplicity and efficiency of contrastive learning through the optimization of InfoNCE, yielding a run-time complexity that is far less than $O(n^2)$, where $n$ represents the total number of training samples. Furthermore, our method can eliminate the need for hyperparameter tuning, particularly related to temperature parameters and large batch sizes. We conduct comprehensive experiments on three datasets including CIFAR-100, ImageNet-1K, and MS COCO. Experimental results clearly confirm the effectiveness of the proposed method on both image classification and object detection tasks. Our source codes will be publicly available at https://github.com/wencheng-zhu/CKD.","sentences":["In this paper, we present a simple yet effective contrastive knowledge distillation approach, which can be formulated as a sample-wise alignment problem with intra- and inter-sample constraints.","Unlike traditional knowledge distillation methods that concentrate on maximizing feature similarities or preserving class-wise semantic correlations between teacher and student features, our method attempts to recover the \"dark knowledge\" by aligning sample-wise teacher and student logits.","Specifically, our method first minimizes logit differences within the same sample by considering their numerical values, thus preserving intra-sample similarities.","Next, we bridge semantic disparities by leveraging dissimilarities across different samples.","Note that constraints on intra-sample similarities and inter-sample dissimilarities can be efficiently and effectively reformulated into a contrastive learning framework with newly designed positive and negative pairs.","The positive pair consists of the teacher's and student's logits derived from an identical sample, while the negative pairs are formed by using logits from different samples.","With this formulation, our method benefits from the simplicity and efficiency of contrastive learning through the optimization of InfoNCE, yielding a run-time complexity that is far less than $O(n^2)$, where $n$ represents the total number of training samples.","Furthermore, our method can eliminate the need for hyperparameter tuning, particularly related to temperature parameters and large batch sizes.","We conduct comprehensive experiments on three datasets including CIFAR-100, ImageNet-1K, and MS COCO.","Experimental results clearly confirm the effectiveness of the proposed method on both image classification and object detection tasks.","Our source codes will be publicly available at https://github.com/wencheng-zhu/CKD."],"url":"http://arxiv.org/abs/2404.14109v1","category":"cs.CV"}
{"created":"2024-04-22 11:52:31","title":"Chemically Stable Group IV-V Transition Metal Carbide Thin Films in Hydrogen Radical Environments","abstract":"Hydrogen is playing a crucial role in the green energy transition. Yet, its tendency to react with and diffuse into surrounding materials poses a challenge. Therefore, it is critical to develop coatings that protect hydrogen-sensitive system components in reactive-hydrogen environments. In this work, we report group IV-V transition metal carbide (TMC) thin films as potential candidates for hydrogen-protective coatings in hydrogen radical (H*) environments at elevated temperatures. We identify three classes of TMCs based on the reduction of carbides and surface oxides (TMOx). HfC, ZrC, TiC, TaC, NbC, and VC (class A) are found to have a stable carbidic-C (TM-C) content, with a further sub-division into partial (class A1: HfC, ZrC, and TiC) and strong (class A2: TaC, NbC, and VC) surface TMOx reduction. In contrast to class A, a strong carbide reduction is observed in Co2C (class B), along with a strong surface TMOx reduction. The H*-TMC/TMOx interaction is hypothesized to entail three processes: (i) hydrogenation of surface C/O-atoms, (ii) formation of CHx/OHx species, and (iii) subsurface C/O-atoms diffusion to the surface vacancies. The number of adsorbed H-atoms required to form CHx/OHx species (i), and the corresponding energy barriers (ii) are estimated based on the change in the Gibbs free energy (DeltaG) for the reduction reactions of TMCs and TMOx. Hydrogenation of surface carbidic-C-atoms is proposed to limit the reduction of TMCs, whereas the reduction of surface TMOx is governed by the thermodynamic barrier for forming H2O.","sentences":["Hydrogen is playing a crucial role in the green energy transition.","Yet, its tendency to react with and diffuse into surrounding materials poses a challenge.","Therefore, it is critical to develop coatings that protect hydrogen-sensitive system components in reactive-hydrogen environments.","In this work, we report group IV-V transition metal carbide (TMC) thin films as potential candidates for hydrogen-protective coatings in hydrogen radical (H*) environments at elevated temperatures.","We identify three classes of TMCs based on the reduction of carbides and surface oxides (TMOx).","HfC, ZrC, TiC, TaC, NbC, and VC (class A) are found to have a stable carbidic-C (TM-C) content, with a further sub-division into partial (class A1: HfC, ZrC, and TiC) and strong (class A2: TaC, NbC, and VC) surface TMOx reduction.","In contrast to class A, a strong carbide reduction is observed in Co2C (class B), along with a strong surface TMOx reduction.","The H*-TMC/TMOx interaction is hypothesized to entail three processes: (i) hydrogenation of surface C/O-atoms, (ii) formation of CHx/OHx species, and (iii) subsurface C/O-atoms diffusion to the surface vacancies.","The number of adsorbed H-atoms required to form CHx/OHx species (i), and the corresponding energy barriers (ii) are estimated based on the change in the Gibbs free energy (DeltaG) for the reduction reactions of TMCs and TMOx.","Hydrogenation of surface carbidic-C-atoms is proposed to limit the reduction of TMCs, whereas the reduction of surface TMOx is governed by the thermodynamic barrier for forming H2O."],"url":"http://arxiv.org/abs/2404.14108v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 11:51:01","title":"Laser Frequency Stabilization Using Light Shift in Compact Atomic Clocks","abstract":"This paper describes the Light-Shift Laser-Lock (LSLL) technique, a novel method intended for compact atomic clocks that greatly simplifies the laser setup by stabilizing the pumping-laser frequency to the atoms involved in the clock, without the need of an external reference. By alternating two clock sequences with different light shifts, the method estimates and cancels out a controlled amount of induced light shift, acting on the laser frequency. The LSLL technique is compatible with state-of-the-art 3-level clocks and was demonstrated with FPGA-based electronics on a pulsed-optically-pumped (POP) vapor-cell clock developed at INRIM. The results have shown that the LSLL technique operates robustly, having a capture range of gigahertz without significantly compromising clock stability. In our tests, the clock exhibited a white frequency noise of $3.2 \\times 10^{-13}$ $\\tau^{-1/2}$ for averaging time up to 4000 s, reaching a floor below $1 \\times 10^{-14}$ up to 100 000 s. These performance levels meet the requirements of future Global Navigation Satellite Systems (GNSS) on-board clocks, and offer the added benefits of a reduced clock footprint, as well as increased reliability and robustness.","sentences":["This paper describes the Light-Shift Laser-Lock (LSLL) technique, a novel method intended for compact atomic clocks that greatly simplifies the laser setup by stabilizing the pumping-laser frequency to the atoms involved in the clock, without the need of an external reference.","By alternating two clock sequences with different light shifts, the method estimates and cancels out a controlled amount of induced light shift, acting on the laser frequency.","The LSLL technique is compatible with state-of-the-art 3-level clocks and was demonstrated with FPGA-based electronics on a pulsed-optically-pumped (POP) vapor-cell clock developed at INRIM.","The results have shown that the LSLL technique operates robustly, having a capture range of gigahertz without significantly compromising clock stability.","In our tests, the clock exhibited a white frequency noise of $3.2 \\times 10^{-13}$ $\\tau^{-1/2}$ for averaging time up to 4000 s, reaching a floor below $1 \\times 10^{-14}$ up to 100 000 s. These performance levels meet the requirements of future Global Navigation Satellite Systems (GNSS) on-board clocks, and offer the added benefits of a reduced clock footprint, as well as increased reliability and robustness."],"url":"http://arxiv.org/abs/2404.14105v1","category":"physics.atom-ph"}
{"created":"2024-04-22 11:44:48","title":"Dynamical quantum Ansatz tree approach for the heat equation","abstract":"Quantum computers can be used for the solution of various problems of mathematical physics. In the present paper, we consider a discretized version of the heat equation and address its solution on quantum computer using variational Anzats tree approach (ATA). We extend this method originally proposed for the system of linear equations to tackle full time dependent heat equation. The key ingredients of our method are (i) special probabilistic quantum circuit in order to add heat sources to temperature distribution, (ii) limiting auxiliary register in the preparation of quantum state, (iii) utilizing a robust cluster of repetitive nodes in the anzats tree structure. We suggest that our procedure provides an exponential speedup compared to the classical algorithms in the case of time dependent heat equation.","sentences":["Quantum computers can be used for the solution of various problems of mathematical physics.","In the present paper, we consider a discretized version of the heat equation and address its solution on quantum computer using variational Anzats tree approach (ATA).","We extend this method originally proposed for the system of linear equations to tackle full time dependent heat equation.","The key ingredients of our method are (i) special probabilistic quantum circuit in order to add heat sources to temperature distribution, (ii) limiting auxiliary register in the preparation of quantum state, (iii) utilizing a robust cluster of repetitive nodes in the anzats tree structure.","We suggest that our procedure provides an exponential speedup compared to the classical algorithms in the case of time dependent heat equation."],"url":"http://arxiv.org/abs/2404.14102v1","category":"quant-ph"}
{"created":"2024-04-22 11:37:47","title":"A Method of Joint Angle Estimation Using Only Relative Changes in Muscle Lengths for Tendon-driven Humanoids with Complex Musculoskeletal Structures","abstract":"Tendon-driven musculoskeletal humanoids typically have complex structures similar to those of human beings, such as ball joints and the scapula, in which encoders cannot be installed. Therefore, joint angles cannot be directly obtained and need to be estimated using the changes in muscle lengths. In previous studies, methods using table-search and extended kalman filter have been developed. These methods express the joint-muscle mapping, which is the nonlinear relationship between joint angles and muscle lengths, by using a data table, polynomials, or a neural network. However, due to computational complexity, these methods cannot consider the effects of polyarticular muscles. In this study, considering the limitation of the computational cost, we reduce unnecessary degrees of freedom, divide joints and muscles into several groups, and formulate a joint angle estimation method that takes into account polyarticular muscles. Also, we extend the estimation method to propose a joint angle estimation method using only the relative changes in muscle lengths. By this extension, which does not use absolute muscle lengths, we do not need to execute a difficult calibration of muscle lengths for tendon-driven musculoskeletal humanoids. Finally, we conduct experiments in simulation and actual environments, and verify the effectiveness of this study.","sentences":["Tendon-driven musculoskeletal humanoids typically have complex structures similar to those of human beings, such as ball joints and the scapula, in which encoders cannot be installed.","Therefore, joint angles cannot be directly obtained and need to be estimated using the changes in muscle lengths.","In previous studies, methods using table-search and extended kalman filter have been developed.","These methods express the joint-muscle mapping, which is the nonlinear relationship between joint angles and muscle lengths, by using a data table, polynomials, or a neural network.","However, due to computational complexity, these methods cannot consider the effects of polyarticular muscles.","In this study, considering the limitation of the computational cost, we reduce unnecessary degrees of freedom, divide joints and muscles into several groups, and formulate a joint angle estimation method that takes into account polyarticular muscles.","Also, we extend the estimation method to propose a joint angle estimation method using only the relative changes in muscle lengths.","By this extension, which does not use absolute muscle lengths, we do not need to execute a difficult calibration of muscle lengths for tendon-driven musculoskeletal humanoids.","Finally, we conduct experiments in simulation and actual environments, and verify the effectiveness of this study."],"url":"http://arxiv.org/abs/2404.14100v1","category":"cs.RO"}
{"created":"2024-04-22 11:33:21","title":"MMT: Mutation Testing of Java Bytecode with Model Transformation -- An Illustrative Demonstration","abstract":"Mutation testing is an approach to check the robustness of test suites. The program code is slightly changed by mutations to inject errors. A test suite is robust enough if it finds such errors. Tools for mutation testing usually integrate sets of mutation operators such as, for example, swapping arithmetic operators; modern tools typically work with compiled code such as Java bytecode. In this case, the mutations must be defined in such a way that the mutated program still can be loaded and executed. The results of mutation tests depend directly on the possible mutations. More advanced mutations and even domain-specific mutations can pose another challenge to the test suite. Since extending the classical approaches to more complex mutations is not well supported and is difficult, we propose a model-driven approach where mutations of Java bytecode can be flexibly defined by model transformation. The corresponding tool called MMT has been extended with advanced mutation operators for modifying object-oriented structures, Java-specific properties and method calls of APIs, making it the only mutation testing tool for Java bytecode that supports such mutations.","sentences":["Mutation testing is an approach to check the robustness of test suites.","The program code is slightly changed by mutations to inject errors.","A test suite is robust enough if it finds such errors.","Tools for mutation testing usually integrate sets of mutation operators such as, for example, swapping arithmetic operators; modern tools typically work with compiled code such as Java bytecode.","In this case, the mutations must be defined in such a way that the mutated program still can be loaded and executed.","The results of mutation tests depend directly on the possible mutations.","More advanced mutations and even domain-specific mutations can pose another challenge to the test suite.","Since extending the classical approaches to more complex mutations is not well supported and is difficult, we propose a model-driven approach where mutations of Java bytecode can be flexibly defined by model transformation.","The corresponding tool called MMT has been extended with advanced mutation operators for modifying object-oriented structures, Java-specific properties and method calls of APIs, making it the only mutation testing tool for Java bytecode that supports such mutations."],"url":"http://arxiv.org/abs/2404.14097v1","category":"cs.SE"}
{"created":"2024-04-22 11:26:56","title":"Quantum Information reveals that orbital-wise correlation is essentially classical in Natural Orbitals","abstract":"The intersection of Quantum Chemistry and Quantum Computing has led to significant advancements in understanding the potential of using quantum devices for the efficient calculation of molecular energies. Simultaneously, this intersection is enhancing the comprehension of quantum chemical properties through the use of quantum computing and quantum information tools. This paper tackles a key question in this relationship: Is the nature of the orbital-wise electron correlations in wavefunctions of realistic prototypical cases classical or quantum?   We delve into this inquiry with a comprehensive examination of molecular wavefunctions using Shannon and von Neumann entropies, alongside classical and quantum information theory. Our analysis reveals a notable distinction between classical and quantum mutual information in molecular systems when analyzed with Hartree-Fock canonical orbitals. However, this difference decreases dramatically, by approximately 100-fold, when Natural Orbitals are used as reference. This finding suggests that wavefunction correlations, when viewed through the appropriate orbital basis, are predominantly classical. This insight indicates that computational tasks in quantum chemistry could be significantly simplified by employing Natural Orbitals. Consequently, our study underscores the importance of using Natural Orbitals to accurately assess molecular wavefunction correlations and to avoid their overestimation. In summary, our results suggest a promising path for computational simplification in quantum chemistry, advocating for the wider adoption of Natural Orbitals and raising questions about the actual computational complexity of the multi-body problem in quantum chemistry.","sentences":["The intersection of Quantum Chemistry and Quantum Computing has led to significant advancements in understanding the potential of using quantum devices for the efficient calculation of molecular energies.","Simultaneously, this intersection is enhancing the comprehension of quantum chemical properties through the use of quantum computing and quantum information tools.","This paper tackles a key question in this relationship: Is the nature of the orbital-wise electron correlations in wavefunctions of realistic prototypical cases classical or quantum?   ","We delve into this inquiry with a comprehensive examination of molecular wavefunctions using Shannon and von Neumann entropies, alongside classical and quantum information theory.","Our analysis reveals a notable distinction between classical and quantum mutual information in molecular systems when analyzed with Hartree-Fock canonical orbitals.","However, this difference decreases dramatically, by approximately 100-fold, when Natural Orbitals are used as reference.","This finding suggests that wavefunction correlations, when viewed through the appropriate orbital basis, are predominantly classical.","This insight indicates that computational tasks in quantum chemistry could be significantly simplified by employing Natural Orbitals.","Consequently, our study underscores the importance of using Natural Orbitals to accurately assess molecular wavefunction correlations and to avoid their overestimation.","In summary, our results suggest a promising path for computational simplification in quantum chemistry, advocating for the wider adoption of Natural Orbitals and raising questions about the actual computational complexity of the multi-body problem in quantum chemistry."],"url":"http://arxiv.org/abs/2404.14093v1","category":"quant-ph"}
{"created":"2024-04-22 11:21:40","title":"Well-posedness and long-term behaviour of buffered flows in infinite networks","abstract":"We consider a transport problem on an infinite metric graph and discuss its well-posedness and long-term behaviour under the condition that the mass flow is buffered in at least one of the vertices. In order to show the well-posedness of the problem, we employ the theory of $C_0$-semigroups and prove a Desch--Schappacher type perturbation theorem for dispersive semigroups. Investigating the long-term behaviour of the system, we prove irreducibility of the semigroup under the assumption that the underlying graph is strongly connected and an additional spectral condition on its adjacency matrix. Moreover, we employ recent results about the convergence of stochastic semigroups that dominate a kernel operator to prove that the solutions converge strongly to equilibrium. Finally, we prove that the solutions converge uniformly under more restrictive assumptions.","sentences":["We consider a transport problem on an infinite metric graph and discuss its well-posedness and long-term behaviour under the condition that the mass flow is buffered in at least one of the vertices.","In order to show the well-posedness of the problem, we employ the theory of $C_0$-semigroups and prove a Desch--Schappacher type perturbation theorem for dispersive semigroups.","Investigating the long-term behaviour of the system, we prove irreducibility of the semigroup under the assumption that the underlying graph is strongly connected and an additional spectral condition on its adjacency matrix.","Moreover, we employ recent results about the convergence of stochastic semigroups that dominate a kernel operator to prove that the solutions converge strongly to equilibrium.","Finally, we prove that the solutions converge uniformly under more restrictive assumptions."],"url":"http://arxiv.org/abs/2404.14090v1","category":"math.AP"}
{"created":"2024-04-22 11:10:00","title":"Simulated structure and thermodynamics of decagonal Al-Co-Cu quasicrystals","abstract":"Atomic structures of Al-Co-Cu decagonal quasicrystals (QCs) are investigated using empirical oscillating pair potentials (EOPP) in molecular dynamic (MD) simulations that we enhance by Monte Carlo (MC) swapping of chemical species and replica exchange. Predicted structures exhibit planar decagonal tilin g patterns and are periodic along the perpendicular direction. We then recalculate the energies of promising structures using first-principles density functional theory (DFT), along with energies of competing phases. We find that our $\\tau$-inflated sequence of QC approximants are energetically unstable a t low temperature by at least 3 meV/atom. Extending our study to finite temperatures by calculating harmonic vibrational entropy, as well as anharmonic contributions that include chemical species swaps and tile flips, our results suggest that the quasicrystal phase is entropically stabilized at temperatur es in the range 600-800K and above. It decomposes into ordinary (though complex) crystal phases at low temperatures, including a partially disordered B2-type phase. We discuss the influence of density and composition on QC phase stability; we compare the structural differences between Co-rich and Cu-rich quasicrystals; and we analyze the role of entropy in stabilizing the quasicrystal, concluding with a discussion of the possible existence of \"high entropy\" quasicrystals.","sentences":["Atomic structures of Al-Co-Cu decagonal quasicrystals (QCs) are investigated using empirical oscillating pair potentials (EOPP) in molecular dynamic (MD) simulations that we enhance by Monte Carlo (MC) swapping of chemical species and replica exchange.","Predicted structures exhibit planar decagonal tilin g patterns and are periodic along the perpendicular direction.","We then recalculate the energies of promising structures using first-principles density functional theory (DFT), along with energies of competing phases.","We find that our $\\tau$-inflated sequence of QC approximants are energetically unstable a t low temperature by at least 3 meV/atom.","Extending our study to finite temperatures by calculating harmonic vibrational entropy, as well as anharmonic contributions that include chemical species swaps and tile flips, our results suggest that the quasicrystal phase is entropically stabilized at temperatur es in the range 600-800K and above.","It decomposes into ordinary (though complex) crystal phases at low temperatures, including a partially disordered B2-type phase.","We discuss the influence of density and composition on QC phase stability; we compare the structural differences between Co-rich and Cu-rich quasicrystals; and we analyze the role of entropy in stabilizing the quasicrystal, concluding with a discussion of the possible existence of \"high entropy\" quasicrystals."],"url":"http://arxiv.org/abs/2404.14086v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 11:06:06","title":"Evidence and concerns about a latent, embryonic phase tectonic evolution and the existence of the young subsurface ocean on Mimas","abstract":"New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean. Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean. Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation. In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs. Simple tectonic features are identified as young compared to complex structures. The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns. In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean. However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago.","sentences":["New models challenge the long-standing conclusion about Mimas, an icy satellite of Saturn, being an inactive snowball, suggesting the existence of a young stealth ocean.","Unfortunately, no observable evidence has been found yet implying tectonic activity and the theoretical subsurface ocean.","Here, we present the first structural geological map of the icy satellite, with the signs of various tectonic features, along with a simple crosscutting chronology of lineaments formation.","In accordance with the supposedly young age of the stealth ocean, the observed phenomena are described as putative lineaments, ridges, and troughs.","Simple tectonic features are identified as young compared to complex structures.","The pattern of the linear features seems to overlap with the allocation of various modeled global nonlinear tidal dissipation patterns.","In such a way, it may provide the first observed evidence for the existence of the theoretical subsurface stealth ocean.","However, the overlapping and crosscutting relation between craters and the observed features may raise concerns about the recent formation of such linear features, indicating possibly long-time dormant or already stopped tectonic processes at the early embryonic phase of lineament formation billions of years ago."],"url":"http://arxiv.org/abs/2404.14084v1","category":"astro-ph.EP"}
{"created":"2024-04-22 10:54:02","title":"TWIMP: Two-Wheel Inverted Musculoskeletal Pendulum as a Learning Control Platform in the Real World with Environmental Physical Contact","abstract":"By the recent spread of machine learning in the robotics field, a humanoid that can act, perceive, and learn in the real world through contact with the environment needs to be developed. In this study, as one of the choices, we propose a novel humanoid TWIMP, which combines a human mimetic musculoskeletal upper limb with a two-wheel inverted pendulum. By combining the benefit of a musculoskeletal humanoid, which can achieve soft contact with the external environment, and the benefit of a two-wheel inverted pendulum with a small footprint and high mobility, we can easily investigate learning control systems in environments with contact and sudden impact. We reveal our whole concept and system details of TWIMP, and execute several preliminary experiments to show its potential ability.","sentences":["By the recent spread of machine learning in the robotics field, a humanoid that can act, perceive, and learn in the real world through contact with the environment needs to be developed.","In this study, as one of the choices, we propose a novel humanoid TWIMP, which combines a human mimetic musculoskeletal upper limb with a two-wheel inverted pendulum.","By combining the benefit of a musculoskeletal humanoid, which can achieve soft contact with the external environment, and the benefit of a two-wheel inverted pendulum with a small footprint and high mobility, we can easily investigate learning control systems in environments with contact and sudden impact.","We reveal our whole concept and system details of TWIMP, and execute several preliminary experiments to show its potential ability."],"url":"http://arxiv.org/abs/2404.14080v1","category":"cs.RO"}
{"created":"2024-04-22 10:49:46","title":"Research on Robot Path Planning Based on Reinforcement Learning","abstract":"This project has conducted research on robot path planning based on Visual SLAM. The main work of this project is as follows: (1) Construction of Visual SLAM system. Research has been conducted on the basic architecture of Visual SLAM. A Visual SLAM system is developed based on ORB-SLAM3 system, which can conduct dense point cloud mapping. (2) The map suitable for two-dimensional path planning is obtained through map conversion. This part converts the dense point cloud map obtained by Visual SLAM system into an octomap and then performs projection transformation to the grid map. The map conversion converts the dense point cloud map containing a large amount of redundant map information into an extremely lightweight grid map suitable for path planning. (3) Research on path planning algorithm based on reinforcement learning. This project has conducted experimental comparisons between the Q-learning algorithm, the DQN algorithm, and the SARSA algorithm, and found that DQN is the algorithm with the fastest convergence and best performance in high-dimensional complex environments. This project has conducted experimental verification of the Visual SLAM system in a simulation environment. The experimental results obtained based on open-source dataset and self-made dataset prove the feasibility and effectiveness of the designed Visual SLAM system. At the same time, this project has also conducted comparative experiments on the three reinforcement learning algorithms under the same experimental condition to obtain the optimal algorithm under the experimental condition.","sentences":["This project has conducted research on robot path planning based on Visual SLAM.","The main work of this project is as follows: (1) Construction of Visual SLAM system.","Research has been conducted on the basic architecture of Visual SLAM.","A Visual SLAM system is developed based on ORB-SLAM3 system, which can conduct dense point cloud mapping.","(2) The map suitable for two-dimensional path planning is obtained through map conversion.","This part converts the dense point cloud map obtained by Visual SLAM system into an octomap and then performs projection transformation to the grid map.","The map conversion converts the dense point cloud map containing a large amount of redundant map information into an extremely lightweight grid map suitable for path planning.","(3) Research on path planning algorithm based on reinforcement learning.","This project has conducted experimental comparisons between the Q-learning algorithm, the DQN algorithm, and the SARSA algorithm, and found that DQN is the algorithm with the fastest convergence and best performance in high-dimensional complex environments.","This project has conducted experimental verification of the Visual SLAM system in a simulation environment.","The experimental results obtained based on open-source dataset and self-made dataset prove the feasibility and effectiveness of the designed Visual SLAM system.","At the same time, this project has also conducted comparative experiments on the three reinforcement learning algorithms under the same experimental condition to obtain the optimal algorithm under the experimental condition."],"url":"http://arxiv.org/abs/2404.14077v1","category":"cs.RO"}
{"created":"2024-04-22 10:35:26","title":"Towards Proxy Staking Accounts Based on NFTs in Ethereum","abstract":"Blockchain is a technology that is often used to share data and assets. However, in the decentralized ecosystem, blockchain-based systems can be utilized to share information and assets without the traditional barriers associated with solo responsibility, e.g., multi-sig wallets. This paper describes an innovative approach to blockchain networks based on a non-fungible token that behaves as an account (NFTAA). The key novelty of this article is using NFTAA to leverage the unique properties of NFTs to manage your ownership better and effectively isolate them to improve the security, transparency, and even interoperability possibilities. Additionally, the account-based solution gives us the ability and flexibility to cover regular use cases such as staking and liquid equities, but also practical composability. This article offers a simple implementation, which allows developers and researchers to choose the best solution for their needs in demand of abstract representation in any use case.","sentences":["Blockchain is a technology that is often used to share data and assets.","However, in the decentralized ecosystem, blockchain-based systems can be utilized to share information and assets without the traditional barriers associated with solo responsibility, e.g., multi-sig wallets.","This paper describes an innovative approach to blockchain networks based on a non-fungible token that behaves as an account (NFTAA).","The key novelty of this article is using NFTAA to leverage the unique properties of NFTs to manage your ownership better and effectively isolate them to improve the security, transparency, and even interoperability possibilities.","Additionally, the account-based solution gives us the ability and flexibility to cover regular use cases such as staking and liquid equities, but also practical composability.","This article offers a simple implementation, which allows developers and researchers to choose the best solution for their needs in demand of abstract representation in any use case."],"url":"http://arxiv.org/abs/2404.14074v1","category":"cs.DC"}
{"created":"2024-04-22 10:31:05","title":"Measure-valued death state and local sensitivity analysis for Winfree models with uncertain high-order couplings","abstract":"We study the measure-valued death state and local sensitivity analysis of the Winfree model and its mean-field counterpart with uncertain high-order couplings. The Winfree model is the first mathematical model for synchronization, and it can cast as the effective approximation of the pulse-coupled model for synchronization, and it exhibits diverse asymptotic patterns depending on system parameters and initial data. For the proposed models, we present several frameworks leading to oscillator death in terms of system parameters and initial data, and the propagation of regularity in random space. We also present several numerical tests and compare them with analytical results.","sentences":["We study the measure-valued death state and local sensitivity analysis of the Winfree model and its mean-field counterpart with uncertain high-order couplings.","The Winfree model is the first mathematical model for synchronization, and it can cast as the effective approximation of the pulse-coupled model for synchronization, and it exhibits diverse asymptotic patterns depending on system parameters and initial data.","For the proposed models, we present several frameworks leading to oscillator death in terms of system parameters and initial data, and the propagation of regularity in random space.","We also present several numerical tests and compare them with analytical results."],"url":"http://arxiv.org/abs/2404.14072v1","category":"math.AP"}
{"created":"2024-04-22 10:30:11","title":"From Rigid to Soft Robotic Approaches for Minimally Invasive Neurosurgery","abstract":"Robotic assistance has significantly improved the outcomes of open microsurgery and rigid endoscopic surgery, however is yet to make an impact in flexible endoscopic neurosurgery. Some of the most common intracranial procedures for treatment of hydrocephalus and tumors stand to benefit from increased dexterity and reduced invasiveness offered by robotic systems that can navigate in the deep ventricular system of the brain. We review a spectrum of flexible robotic devices, from the traditional highly actuated approach, to more novel and bio-inspired mechanisms for safe navigation. For each technology, we identify the operating principle and are able to evaluate the potential for minimally invasive surgical applications. Overall, rigid-type continuum robots have seen the most development, however, approaches combining rigid and soft robotic principles into innovative devices, are ideally situated to address safety and complexity limitations after future design evolution. We also observe a number of related challenges in the field, from surgeon-robot interfaces to robot evaluation procedures. Fundamentally, the challenges revolve around a guarantee of safety in robotic devices with the prerequisites to assist and improve upon surgical tasks. With innovative designs, materials and evaluation techniques emerging, we see potential impacts in the next 5--10 years.","sentences":["Robotic assistance has significantly improved the outcomes of open microsurgery and rigid endoscopic surgery, however is yet to make an impact in flexible endoscopic neurosurgery.","Some of the most common intracranial procedures for treatment of hydrocephalus and tumors stand to benefit from increased dexterity and reduced invasiveness offered by robotic systems that can navigate in the deep ventricular system of the brain.","We review a spectrum of flexible robotic devices, from the traditional highly actuated approach, to more novel and bio-inspired mechanisms for safe navigation.","For each technology, we identify the operating principle and are able to evaluate the potential for minimally invasive surgical applications.","Overall, rigid-type continuum robots have seen the most development, however, approaches combining rigid and soft robotic principles into innovative devices, are ideally situated to address safety and complexity limitations after future design evolution.","We also observe a number of related challenges in the field, from surgeon-robot interfaces to robot evaluation procedures.","Fundamentally, the challenges revolve around a guarantee of safety in robotic devices with the prerequisites to assist and improve upon surgical tasks.","With innovative designs, materials and evaluation techniques emerging, we see potential impacts in the next 5--10 years."],"url":"http://arxiv.org/abs/2404.14071v1","category":"cs.RO"}
{"created":"2024-04-22 10:25:04","title":"Quantum master equation for many-body systems: Derivation based on the Lieb-Robinson bound","abstract":"The local Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) quantum master equation is a powerful tool for the study of open quantum many-body systems. However, its microscopic derivation applicable to many-body systems is available only in limited cases of weak internal couplings, and it has yet to be fully understood under what microscopic conditions the local GKSL equation is valid. We derive the local GKSL equation on the basis of the Lieb-Robinson bound, which provides an upper bound of the propagation of information in quantum many-body systems. We numerically test the validity of the derived local GKSL equation for a one-dimensional tight-binding fermion chain.","sentences":["The local Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) quantum master equation is a powerful tool for the study of open quantum many-body systems.","However, its microscopic derivation applicable to many-body systems is available only in limited cases of weak internal couplings, and it has yet to be fully understood under what microscopic conditions the local GKSL equation is valid.","We derive the local GKSL equation on the basis of the Lieb-Robinson bound, which provides an upper bound of the propagation of information in quantum many-body systems.","We numerically test the validity of the derived local GKSL equation for a one-dimensional tight-binding fermion chain."],"url":"http://arxiv.org/abs/2404.14067v1","category":"quant-ph"}
{"created":"2024-04-22 10:19:16","title":"GatedLexiconNet: A Comprehensive End-to-End Handwritten Paragraph Text Recognition System","abstract":"The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition. Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem. Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition. However, this method has several drawbacks. These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation. Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies. Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder. The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models. The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line. During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step. In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network. Our results at line and page levels also favour our new GatedLexiconNet. This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets.","sentences":["The Handwritten Text Recognition problem has been a challenge for researchers for the last few decades, especially in the domain of computer vision, a subdomain of pattern recognition.","Variability of texts amongst writers, cursiveness, and different font styles of handwritten texts with degradation of historical text images make it a challenging problem.","Recognizing scanned document images in neural network-based systems typically involves a two-step approach: segmentation and recognition.","However, this method has several drawbacks.","These shortcomings encompass challenges in identifying text regions, analyzing layout diversity within pages, and establishing accurate ground truth segmentation.","Consequently, these processes are prone to errors, leading to bottlenecks in achieving high recognition accuracies.","Thus, in this study, we present an end-to-end paragraph recognition system that incorporates internal line segmentation and gated convolutional layers based encoder.","The gating is a mechanism that controls the flow of information and allows to adaptively selection of the more relevant features in handwritten text recognition models.","The attention module plays an important role in performing internal line segmentation, allowing the page to be processed line-by-line.","During the decoding step, we have integrated a connectionist temporal classification-based word beam search decoder as a post-processing step.","In this work, we have extended existing LexiconNet by carefully applying and utilizing gated convolutional layers in the existing deep neural network.","Our results at line and page levels also favour our new GatedLexiconNet.","This study reported character error rates of 2.27% on IAM, 0.9% on RIMES, and 2.13% on READ-16, and word error rates of 5.73% on IAM, 2.76% on RIMES, and 6.52% on READ-2016 datasets."],"url":"http://arxiv.org/abs/2404.14062v1","category":"cs.CV"}
{"created":"2024-04-22 10:18:28","title":"Discrete nonlinear Schr\u00f6dinger type equations: Solutions and continuum limits","abstract":"As local and nonlocal reductions of a discrete second-order Ablowitz-Kaup-Newell-Segur equation, two discrete nonlinear Schr\\\"odinger type equations are considered. Through the bilinearization reduction method, we construct double Casoratian solutions of the reduced discrete nonlinear Schr\\\"odinger type equations, including soliton solutions and Jordan-block solutions.Dynamics of the obtained one-soliton and two-soliton solutions are analyzed and illustrated. Moreover,both semi-continuous limit and full continuous limit, are applied to obtain solutions of the local and nonlocal semi-discrete nonlinear Schr\\\"odinger type equations, as well as the local and nonlocal continuous nonlinear Schr\\\"odinger type equations.","sentences":["As local and nonlocal reductions of a discrete second-order Ablowitz-Kaup-Newell-Segur equation, two discrete nonlinear Schr\\\"odinger type equations are considered.","Through the bilinearization reduction method, we construct double Casoratian solutions of the reduced discrete nonlinear Schr\\\"odinger type equations, including soliton solutions and Jordan-block solutions.","Dynamics of the obtained one-soliton and two-soliton solutions are analyzed and illustrated.","Moreover,both semi-continuous limit and full continuous limit, are applied to obtain solutions of the local and nonlocal semi-discrete nonlinear Schr\\\"odinger type equations, as well as the local and nonlocal continuous nonlinear Schr\\\"odinger type equations."],"url":"http://arxiv.org/abs/2404.14060v1","category":"nlin.SI"}
{"created":"2024-04-22 10:16:18","title":"Volumes of components of Lelong upper level sets II","abstract":"Let $X$ be a compact K\\\"ahler manifold of dimension $n$, and let $T$ be a closed positive $(1,1)$-current in a nef cohomology class on $X$. We establish an optimal upper bound for the volume of components of Lelong upper level sets of $T$ in terms of cohomology classes of non-pluripolar self-products of $T$.","sentences":["Let $X$ be a compact K\\\"ahler manifold of dimension $n$, and let $T$ be a closed positive $(1,1)$-current in a nef cohomology class on $X$. We establish an optimal upper bound for the volume of components of Lelong upper level sets of $T$ in terms of cohomology classes of non-pluripolar self-products of $T$."],"url":"http://arxiv.org/abs/2404.14058v1","category":"math.CV"}
{"created":"2024-04-22 10:14:42","title":"Covert Multi-Access Communication with a Non-Covert User","abstract":"In this paper, we characterize the fundamental limits of a communication system with three users (i.e., three transmitters) and a single receiver where communication from two covert users must remain undetectable to an external warden. Our results show a tradeoff between the highest rates that are simultaneously achievable for the three users. They further show that the presence of a non-covert user in the system can enhance the capacities of the covert users under stringent secret-key constraints. To derive our fundamental limits, we provide an information-theoretic converse proof and present a coding scheme that achieves the performance of our converse result. Our coding scheme is based on multiplexing different code phases, which seems to be essential to exhaust the entire tradeoff region between the rates at the covert and the two non-covert users. This property is reminiscent of the setup with multiple non-covert users, where multiplexing is also required to exhaust the entire rate-region.","sentences":["In this paper, we characterize the fundamental limits of a communication system with three users (i.e., three transmitters) and a single receiver where communication from two covert users must remain undetectable to an external warden.","Our results show a tradeoff between the highest rates that are simultaneously achievable for the three users.","They further show that the presence of a non-covert user in the system can enhance the capacities of the covert users under stringent secret-key constraints.","To derive our fundamental limits, we provide an information-theoretic converse proof and present a coding scheme that achieves the performance of our converse result.","Our coding scheme is based on multiplexing different code phases, which seems to be essential to exhaust the entire tradeoff region between the rates at the covert and the two non-covert users.","This property is reminiscent of the setup with multiple non-covert users, where multiplexing is also required to exhaust the entire rate-region."],"url":"http://arxiv.org/abs/2404.14056v1","category":"cs.IT"}
{"created":"2024-04-22 10:03:24","title":"Optimization-based Heuristic for Vehicle Dynamic Coordination in Mixed Traffic Intersections","abstract":"In this paper, we address a coordination problem for connected and autonomous vehicles (CAVs) in mixed traffic settings with human-driven vehicles (HDVs). The main objective is to have a safe and optimal crossing order for vehicles approaching unsignalized intersections. This problem results in a mixed-integer quadratic programming (MIQP) formulation which is unsuitable for real-time applications. Therefore, we propose a computationally tractable optimization-based heuristic that monitors platoons of CAVs and HDVs to evaluate whether alternative crossing orders can perform better. It first checks the future constraint violation that consistently occurs between pairs of platoons to determine a potential swap. Next, the costs of quadratic programming (QP) formulations associated with the current and alternative orders are compared in a depth-first branching fashion. In simulations, we show that the heuristic can be a hundred times faster than the original and simplified MIQPs and yields solutions that are close to optimal and have better order consistency.","sentences":["In this paper, we address a coordination problem for connected and autonomous vehicles (CAVs) in mixed traffic settings with human-driven vehicles (HDVs).","The main objective is to have a safe and optimal crossing order for vehicles approaching unsignalized intersections.","This problem results in a mixed-integer quadratic programming (MIQP) formulation which is unsuitable for real-time applications.","Therefore, we propose a computationally tractable optimization-based heuristic that monitors platoons of CAVs and HDVs to evaluate whether alternative crossing orders can perform better.","It first checks the future constraint violation that consistently occurs between pairs of platoons to determine a potential swap.","Next, the costs of quadratic programming (QP) formulations associated with the current and alternative orders are compared in a depth-first branching fashion.","In simulations, we show that the heuristic can be a hundred times faster than the original and simplified MIQPs and yields solutions that are close to optimal and have better order consistency."],"url":"http://arxiv.org/abs/2404.14048v1","category":"eess.SY"}
{"created":"2024-04-22 09:55:50","title":"CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction","abstract":"The increasing adoption of 3D point cloud data in various applications, such as autonomous vehicles, robotics, and virtual reality, has brought about significant advancements in object recognition and scene understanding. However, this progress is accompanied by new security challenges, particularly in the form of backdoor attacks. These attacks involve inserting malicious information into the training data of machine learning models, potentially compromising the model's behavior. In this paper, we propose CloudFort, a novel defense mechanism designed to enhance the robustness of 3D point cloud classifiers against backdoor attacks. CloudFort leverages spatial partitioning and ensemble prediction techniques to effectively mitigate the impact of backdoor triggers while preserving the model's performance on clean data. We evaluate the effectiveness of CloudFort through extensive experiments, demonstrating its strong resilience against the Point Cloud Backdoor Attack (PCBA). Our results show that CloudFort significantly enhances the security of 3D point cloud classification models without compromising their accuracy on benign samples. Furthermore, we explore the limitations of CloudFort and discuss potential avenues for future research in the field of 3D point cloud security. The proposed defense mechanism represents a significant step towards ensuring the trustworthiness and reliability of point-cloud-based systems in real-world applications.","sentences":["The increasing adoption of 3D point cloud data in various applications, such as autonomous vehicles, robotics, and virtual reality, has brought about significant advancements in object recognition and scene understanding.","However, this progress is accompanied by new security challenges, particularly in the form of backdoor attacks.","These attacks involve inserting malicious information into the training data of machine learning models, potentially compromising the model's behavior.","In this paper, we propose CloudFort, a novel defense mechanism designed to enhance the robustness of 3D point cloud classifiers against backdoor attacks.","CloudFort leverages spatial partitioning and ensemble prediction techniques to effectively mitigate the impact of backdoor triggers while preserving the model's performance on clean data.","We evaluate the effectiveness of CloudFort through extensive experiments, demonstrating its strong resilience against the Point Cloud Backdoor Attack (PCBA).","Our results show that CloudFort significantly enhances the security of 3D point cloud classification models without compromising their accuracy on benign samples.","Furthermore, we explore the limitations of CloudFort and discuss potential avenues for future research in the field of 3D point cloud security.","The proposed defense mechanism represents a significant step towards ensuring the trustworthiness and reliability of point-cloud-based systems in real-world applications."],"url":"http://arxiv.org/abs/2404.14042v1","category":"cs.CV"}
{"created":"2024-04-22 09:53:00","title":"Two-tone spectroscopy for the detection of two-level systems in superconducting qubits","abstract":"Two-level systems (TLS) of unclear physical origin are a major contributor to decoherence in superconducting qubits. The interactions of individual TLS with a qubit can be detected via various spectroscopic methods, most of which have relied on the tunability of the qubit frequency. We propose a novel method that requires only a microwave drive and dispersive readout, and thus also works fixed-frequency qubits. The proposed two-tone spectroscopy involves a microwave pulse of varying frequency and length to excite TLSs of unknown frequencies, followed by a second pulse at the qubit frequency. TLS parameters can be estimated from the qubit population as a function of the first pulse frequency and length.","sentences":["Two-level systems (TLS) of unclear physical origin are a major contributor to decoherence in superconducting qubits.","The interactions of individual TLS with a qubit can be detected via various spectroscopic methods, most of which have relied on the tunability of the qubit frequency.","We propose a novel method that requires only a microwave drive and dispersive readout, and thus also works fixed-frequency qubits.","The proposed two-tone spectroscopy involves a microwave pulse of varying frequency and length to excite TLSs of unknown frequencies, followed by a second pulse at the qubit frequency.","TLS parameters can be estimated from the qubit population as a function of the first pulse frequency and length."],"url":"http://arxiv.org/abs/2404.14039v1","category":"quant-ph"}
{"created":"2024-04-22 09:52:50","title":"Accurate Chemical Reaction Modeling on Noisy Intermediate-Scale Quantum Computers Using a Noise-Resilient Wavefunction Ansatz","abstract":"Quantum computing is of great potential for chemical system simulations. In this study, we propose an efficient protocol of quantum computer based simulation of chemical systems which enables accurate chemical reaction modeling on noisy intermediate-scale quantum (NISQ) devices. In this protocol, we combine an correlation energy-based active orbital selection, an effective Hamiltonian from the driven similarity renormalization group (DSRG) method, and a noise-resilient wavefunction ansatz. Such a combination gives a quantum resource-efficient way to accurately simulate chemical systems. The power of this protocol is demonstrated by numerical results for systems with up to tens of atoms. Modeling of a Diels-Alder (DA) reaction is also performed on a cloud-based superconducting quantum computer. These results represent an important step forward in realizing quantum utility in the NISQ era.","sentences":["Quantum computing is of great potential for chemical system simulations.","In this study, we propose an efficient protocol of quantum computer based simulation of chemical systems which enables accurate chemical reaction modeling on noisy intermediate-scale quantum (NISQ) devices.","In this protocol, we combine an correlation energy-based active orbital selection, an effective Hamiltonian from the driven similarity renormalization group (DSRG) method, and a noise-resilient wavefunction ansatz.","Such a combination gives a quantum resource-efficient way to accurately simulate chemical systems.","The power of this protocol is demonstrated by numerical results for systems with up to tens of atoms.","Modeling of a Diels-Alder (DA) reaction is also performed on a cloud-based superconducting quantum computer.","These results represent an important step forward in realizing quantum utility in the NISQ era."],"url":"http://arxiv.org/abs/2404.14038v1","category":"quant-ph"}
{"created":"2024-04-22 09:51:43","title":"Optimal Structure of Receive Beamforming for Over-the-Air Computation","abstract":"We investigate fast data aggregation via over-the-air computation (AirComp) over wireless networks. In this scenario, an access point (AP) with multiple antennas aims to recover the arithmetic mean of sensory data from multiple wireless devices. To minimize estimation distortion, we formulate a mean-squared-error (MSE) minimization problem that considers joint optimization of transmit scalars at wireless devices, denoising factor, and receive beamforming vector at the AP. We derive closed-form expressions for the transmit scalars and denoising factor, resulting in a non-convex quadratic constrained quadratic programming (QCQP) problem concerning the receive beamforming vector. To tackle the computational complexity of the beamforming design, particularly relevant in massive multiple-input multiple-output (MIMO) AirComp systems, we explore the optimal structure of receive beamforming using successive convex approximation (SCA) and Lagrange duality. By leveraging the proposed optimal beamforming structure, we develop two efficient algorithms based on SCA and semi-definite relaxation (SDR). These algorithms enable fast wireless aggregation with low computational complexity and yield almost identical mean square error (MSE) performance compared to baseline algorithms. Simulation results validate the effectiveness of our proposed methods.","sentences":["We investigate fast data aggregation via over-the-air computation (AirComp) over wireless networks.","In this scenario, an access point (AP) with multiple antennas aims to recover the arithmetic mean of sensory data from multiple wireless devices.","To minimize estimation distortion, we formulate a mean-squared-error (MSE) minimization problem that considers joint optimization of transmit scalars at wireless devices, denoising factor, and receive beamforming vector at the AP.","We derive closed-form expressions for the transmit scalars and denoising factor, resulting in a non-convex quadratic constrained quadratic programming (QCQP) problem concerning the receive beamforming vector.","To tackle the computational complexity of the beamforming design, particularly relevant in massive multiple-input multiple-output (MIMO) AirComp systems, we explore the optimal structure of receive beamforming using successive convex approximation (SCA) and Lagrange duality.","By leveraging the proposed optimal beamforming structure, we develop two efficient algorithms based on SCA and semi-definite relaxation (SDR).","These algorithms enable fast wireless aggregation with low computational complexity and yield almost identical mean square error (MSE) performance compared to baseline algorithms.","Simulation results validate the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2404.14036v1","category":"cs.IT"}
{"created":"2024-04-22 09:50:02","title":"Stable coexistence in indefinitely large systems of competing species","abstract":"The Lotka-Volterra system is a set of ordinary differential equations describing growth of interacting ecological species. This model has gained renewed interest in the context of random interaction networks. One of the debated questions is understanding how the number of species in the system, $n$, influences the stability of the model. Robert May demonstrated that large systems become unstable, unless species-species interactions vanish. This outcome has frequently been interpreted as a universal phenomenon and summarised as \"large systems are unstable\". However, May's results were performed on a specific type of graphs (Erd\\H{o}s-R\\'enyi), whereas we explore a different class of networks and we show that the competitive Lotka-Volterra system maintains stability even in the limit of large $n$, despite non-vanishing interaction strength. We establish a lower bound on the interspecific interaction strength, formulated in terms of the maximum and minimum degrees of the ecological network, rather than being dependent upon the network's size. For values below this threshold, coexistence of all species is attained in the asymptotic limit. In other words, the outlier nodes with large degree cause instability, rather than the large number of species in the system. Our result refines May's bound, by showing that the type of network model is relevant and can lead to completely different results.","sentences":["The Lotka-Volterra system is a set of ordinary differential equations describing growth of interacting ecological species.","This model has gained renewed interest in the context of random interaction networks.","One of the debated questions is understanding how the number of species in the system, $n$, influences the stability of the model.","Robert May demonstrated that large systems become unstable, unless species-species interactions vanish.","This outcome has frequently been interpreted as a universal phenomenon and summarised as \"large systems are unstable\".","However, May's results were performed on a specific type of graphs (Erd\\H{o}s-R\\'enyi), whereas we explore a different class of networks and we show that the competitive Lotka-Volterra system maintains stability even in the limit of large $n$, despite non-vanishing interaction strength.","We establish a lower bound on the interspecific interaction strength, formulated in terms of the maximum and minimum degrees of the ecological network, rather than being dependent upon the network's size.","For values below this threshold, coexistence of all species is attained in the asymptotic limit.","In other words, the outlier nodes with large degree cause instability, rather than the large number of species in the system.","Our result refines May's bound, by showing that the type of network model is relevant and can lead to completely different results."],"url":"http://arxiv.org/abs/2404.14031v1","category":"math.DS"}
{"created":"2024-04-22 09:47:36","title":"Towards Using Behavior Trees in Industrial Automation Controllers","abstract":"The Industry 4.0 paradigm manifests the shift towards mass customization and cyber-physical production systems (CPPS) and sets new requirements for industrial automation software in terms of modularity, flexibility, and short development cycles of control programs. Though programmable logical controllers (PLCs) have been evolving into versatile and powerful edge devices, there is a lack of PLC software flexibility and integration between low-level programs and high-level task-oriented control frameworks. Behavior trees (BTs) is a novel framework, which enables rapid design of modular hierarchical control structures. It combines improved modularity with a simple and intuitive design of control logic. This paper proposes an approach for improving the industrial control software design by integrating BTs into PLC programs and separating hardware related functionalities from the coordination logic. Several strategies for integration of BTs into PLCs are shown. The first two integrate BTs with the IEC 61131 based PLCs and are based on the use of the PLCopen Common Behavior Model. The last one utilized event-based BTs and shows the integration with the IEC 61499 based controllers. An application example demonstrates the approach.   The paper contributes in the following ways. First, we propose a new PLC software design, which improves modularity, supports better separation of concerns, and enables rapid development and reconfiguration of the control software. Second, we show and evaluate the integration of the BT framework into both IEC 61131 and IEC 61499 based PLCs, as well as the integration of the PLCopen function blocks with the external BT library. This leads to better integration of the low-level PLC code and the AI-based task-oriented frameworks. It also improves the skill-based programming approach for PLCs by using BTs for skills composition.","sentences":["The Industry 4.0 paradigm manifests the shift towards mass customization and cyber-physical production systems (CPPS) and sets new requirements for industrial automation software in terms of modularity, flexibility, and short development cycles of control programs.","Though programmable logical controllers (PLCs) have been evolving into versatile and powerful edge devices, there is a lack of PLC software flexibility and integration between low-level programs and high-level task-oriented control frameworks.","Behavior trees (BTs) is a novel framework, which enables rapid design of modular hierarchical control structures.","It combines improved modularity with a simple and intuitive design of control logic.","This paper proposes an approach for improving the industrial control software design by integrating BTs into PLC programs and separating hardware related functionalities from the coordination logic.","Several strategies for integration of BTs into PLCs are shown.","The first two integrate BTs with the IEC 61131 based PLCs and are based on the use of the PLCopen Common Behavior Model.","The last one utilized event-based BTs","and shows the integration with the IEC 61499 based controllers.","An application example demonstrates the approach.   ","The paper contributes in the following ways.","First, we propose a new PLC software design, which improves modularity, supports better separation of concerns, and enables rapid development and reconfiguration of the control software.","Second, we show and evaluate the integration of the BT framework into both IEC 61131 and IEC 61499 based PLCs, as well as the integration of the PLCopen function blocks with the external BT library.","This leads to better integration of the low-level PLC code and the AI-based task-oriented frameworks.","It also improves the skill-based programming approach for PLCs by using BTs for skills composition."],"url":"http://arxiv.org/abs/2404.14030v1","category":"cs.SE"}
{"created":"2024-04-22 09:17:29","title":"Human behavior-driven epidemic surveillance in urban landscapes","abstract":"We introduce a surveillance strategy specifically designed for urban areas to enhance preparedness and response to disease outbreaks by leveraging the unique characteristics of human behavior within urban contexts. By integrating data on individual residences and travel patterns, we construct a Mixing matrix that facilitates the identification of critical pathways that ease pathogen transmission across urban landscapes enabling targeted testing strategies. Our approach not only enhances public health systems' ability to provide early epidemiological alerts but also underscores the variability in strategy effectiveness based on urban layout. We prove the feasibility of our mobility-informed policies by mapping essential mobility flows to major transit stations, showing that few resources focused on specific stations yields a more effective surveillance than non-targeted approaches. This study emphasizes the critical role of integrating human behavioral patterns into epidemic management strategies to improve the preparedness and resilience of major cities against future outbreaks.","sentences":["We introduce a surveillance strategy specifically designed for urban areas to enhance preparedness and response to disease outbreaks by leveraging the unique characteristics of human behavior within urban contexts.","By integrating data on individual residences and travel patterns, we construct a Mixing matrix that facilitates the identification of critical pathways that ease pathogen transmission across urban landscapes enabling targeted testing strategies.","Our approach not only enhances public health systems' ability to provide early epidemiological alerts but also underscores the variability in strategy effectiveness based on urban layout.","We prove the feasibility of our mobility-informed policies by mapping essential mobility flows to major transit stations, showing that few resources focused on specific stations yields a more effective surveillance than non-targeted approaches.","This study emphasizes the critical role of integrating human behavioral patterns into epidemic management strategies to improve the preparedness and resilience of major cities against future outbreaks."],"url":"http://arxiv.org/abs/2404.14009v1","category":"physics.soc-ph"}
{"created":"2024-04-22 09:06:28","title":"Revolutionizing student course selection: Exploring the application prospects and challenges of blockchain token voting technology","abstract":"This paper explores the utilization of blockchain token voting technology in student course selection systems. The current course selection systems face various issues, which can be mitigated through the implementation of blockchain technology. The advantages of blockchain technology, including consensus mechanisms and smart contracts, are discussed in detail. The token voting mechanism, encompassing concepts, token issuance and distribution, and voting rules and procedures, is also explained. The system design takes into account the system architecture, user roles and permissions, course information on the blockchain, student course selection voting process, and course selection result statistics and public display. The technology offers advantages such as transparency, fairness, data security and privacy protection, and system efficiency improvement. However, it also poses several challenges, such as technological and regulatory hurdles. The prospects for the application of blockchain token voting technology in student course selection systems and its potential impact on other fields are summarized. Overall, the utilization of blockchain token voting technology in student course selection systems holds promising future implications, which could revolutionize the education sector.","sentences":["This paper explores the utilization of blockchain token voting technology in student course selection systems.","The current course selection systems face various issues, which can be mitigated through the implementation of blockchain technology.","The advantages of blockchain technology, including consensus mechanisms and smart contracts, are discussed in detail.","The token voting mechanism, encompassing concepts, token issuance and distribution, and voting rules and procedures, is also explained.","The system design takes into account the system architecture, user roles and permissions, course information on the blockchain, student course selection voting process, and course selection result statistics and public display.","The technology offers advantages such as transparency, fairness, data security and privacy protection, and system efficiency improvement.","However, it also poses several challenges, such as technological and regulatory hurdles.","The prospects for the application of blockchain token voting technology in student course selection systems and its potential impact on other fields are summarized.","Overall, the utilization of blockchain token voting technology in student course selection systems holds promising future implications, which could revolutionize the education sector."],"url":"http://arxiv.org/abs/2404.14000v1","category":"cs.CY"}
{"created":"2024-04-22 09:01:14","title":"Challenges in automatic and selective plant-clearing","abstract":"With the advent of multispectral imagery and AI, there have been numerous works on automatic plant segmentation for purposes such as counting, picking, health monitoring, localized pesticide delivery, etc. In this paper, we tackle the related problem of automatic and selective plant-clearing in a sustainable forestry context, where an autonomous machine has to detect and avoid specific plants while clearing any weeds which may compete with the species being cultivated. Such an autonomous system requires a high level of robustness to weather conditions, plant variability, terrain and weeds while remaining cheap and easy to maintain. We notably discuss the lack of robustness of spectral imagery, investigate the impact of the reference database's size and discuss issues specific to AI systems operating in uncontrolled environments.","sentences":["With the advent of multispectral imagery and AI, there have been numerous works on automatic plant segmentation for purposes such as counting, picking, health monitoring, localized pesticide delivery, etc.","In this paper, we tackle the related problem of automatic and selective plant-clearing in a sustainable forestry context, where an autonomous machine has to detect and avoid specific plants while clearing any weeds which may compete with the species being cultivated.","Such an autonomous system requires a high level of robustness to weather conditions, plant variability, terrain and weeds while remaining cheap and easy to maintain.","We notably discuss the lack of robustness of spectral imagery, investigate the impact of the reference database's size and discuss issues specific to AI systems operating in uncontrolled environments."],"url":"http://arxiv.org/abs/2404.13996v1","category":"cs.CV"}
{"created":"2024-04-22 09:01:13","title":"Efficient calculation of transient eddy current response from multilayer cylindrical conductive media","abstract":"The transient response from a transmitter-receiver coil system inside a multi-layer cylindrical conductive configuration is obtained. The particular set-up applies to well logging as well as to eddy current tube testing. In this work, a number of improvements are presented to existing models for an efficient calculation of the induced voltage. These include: domain truncation, novel treatment of arbitrary number of layers in order to avoid computational overflows and efficient time response calculation. The latter is based on a combination of Laplace inversion techniques for short- and long-time transient responses. This article is part of the theme issue 'Advanced electromagnetic non-destructive evaluation and smart monitoring'.","sentences":["The transient response from a transmitter-receiver coil system inside a multi-layer cylindrical conductive configuration is obtained.","The particular set-up applies to well logging as well as to eddy current tube testing.","In this work, a number of improvements are presented to existing models for an efficient calculation of the induced voltage.","These include: domain truncation, novel treatment of arbitrary number of layers in order to avoid computational overflows and efficient time response calculation.","The latter is based on a combination of Laplace inversion techniques for short- and long-time transient responses.","This article is part of the theme issue 'Advanced electromagnetic non-destructive evaluation and smart monitoring'."],"url":"http://arxiv.org/abs/2404.13995v1","category":"math.AP"}
{"created":"2024-04-22 08:53:34","title":"Current fluctuations in finite-sized one-dimensional non-interacting passive and active systems","abstract":"We investigate the problem of effusion of particles initially confined in a finite one-dimensional box of size $L$. We study both passive as well active scenarios, involving non-interacting diffusive particles and run-and-tumble particles, respectively. We derive analytic results for the fluctuations in the number of particles exiting the boundaries of the finite confining box. The statistical properties of this quantity crucially depend on how the system is prepared initially. Two common types of averages employed to understand the impact of initial conditions in stochastic systems are annealed and quenched averages. It is well known that for an infinitely extended system, these different initial conditions produce quantitatively different fluctuations, even in the infinite time limit. We demonstrate explicitly that in finite systems, annealed and quenched fluctuations become equal beyond a system-size dependent timescale, $t \\sim L^2$. For diffusing particles, the fluctuations exhibit a $\\sqrt{t}$ growth at short times and decay as $1/\\sqrt{t}$ for time scales, $t \\gg L^2/D$, where $D$ is the diffusion constant. Meanwhile, for run-and-tumble particles, the fluctuations grow linearly at short times and then decay as $1/\\sqrt{t}$ for time scales, $t \\gg L^2/D_{\\text{eff}}$, where $D_{\\text{eff}}$ represents the effective diffusive constant for run-and-tumble particles. To study the effect of confinement in detail, we also analyze two different setups (i) with one reflecting boundary and (ii) with both boundaries open.","sentences":["We investigate the problem of effusion of particles initially confined in a finite one-dimensional box of size $L$. We study both passive as well active scenarios, involving non-interacting diffusive particles and run-and-tumble particles, respectively.","We derive analytic results for the fluctuations in the number of particles exiting the boundaries of the finite confining box.","The statistical properties of this quantity crucially depend on how the system is prepared initially.","Two common types of averages employed to understand the impact of initial conditions in stochastic systems are annealed and quenched averages.","It is well known that for an infinitely extended system, these different initial conditions produce quantitatively different fluctuations, even in the infinite time limit.","We demonstrate explicitly that in finite systems, annealed and quenched fluctuations become equal beyond a system-size dependent timescale, $t \\sim L^2$.","For diffusing particles, the fluctuations exhibit a $\\sqrt{t}$ growth at short times and decay as $1/\\sqrt{t}$ for time scales, $t \\gg L^2/D$, where $D$ is the diffusion constant.","Meanwhile, for run-and-tumble particles, the fluctuations grow linearly at short times and then decay as $1/\\sqrt{t}$ for time scales, $t \\gg L^2/D_{\\text{eff}}$, where $D_{\\text{eff}}$ represents the effective diffusive constant for run-and-tumble particles.","To study the effect of confinement in detail, we also analyze two different setups (i) with one reflecting boundary and (ii) with both boundaries open."],"url":"http://arxiv.org/abs/2404.13988v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-22 08:44:07","title":"Liquid-Graph Time-Constant Network for Multi-Agent Systems Control","abstract":"In this paper, we propose the Liquid-Graph Time-constant (LGTC) network, a continuous graph neural network(GNN) model for control of multi-agent systems based on therecent Liquid Time Constant (LTC) network. We analyse itsstability leveraging contraction analysis and propose a closed-form model that preserves the model contraction rate and doesnot require solving an ODE at each iteration. Compared todiscrete models like Graph Gated Neural Networks (GGNNs),the higher expressivity of the proposed model guaranteesremarkable performance while reducing the large amountof communicated variables normally required by GNNs. Weevaluate our model on a distributed multi-agent control casestudy (flocking) taking into account variable communicationrange and scalability under non-instantaneous communication","sentences":["In this paper, we propose the Liquid-Graph Time-constant (LGTC) network, a continuous graph neural network(GNN) model for control of multi-agent systems based on therecent Liquid Time Constant (LTC) network.","We analyse itsstability leveraging contraction analysis and propose a closed-form model that preserves the model contraction rate and doesnot require solving an ODE at each iteration.","Compared todiscrete models like Graph Gated Neural Networks (GGNNs),the higher expressivity of the proposed model guaranteesremarkable performance while reducing the large amountof communicated variables normally required by GNNs.","Weevaluate our model on a distributed multi-agent control casestudy (flocking) taking into account variable communicationrange and scalability under non-instantaneous communication"],"url":"http://arxiv.org/abs/2404.13982v1","category":"cs.MA"}
{"created":"2024-04-22 08:39:34","title":"Pour une interop{\u00e9}rabilit{\u00e9} s{\u00e9}mantique en {\u00e9}ducation : les mod{\u00e8}les normatifs de l'ISO/IEC JTC1 SC36","abstract":"The semantics of content is one of the essential constituents of models of innovative educational systems. It is gradually built based on normative efforts carried out by different actors in the fields of the technological industry, telecommunications, IT, linguistic engineering, information sciences documentation, etc. Semantics in networks and digital information systems represent, in fact, an advanced link in a long process of processing digital information in which terminological work occupies an important part. This process is also very consolidated by a wide range of norms and standards which ensure very high levels of technical, organizational, and semantic interoperability.","sentences":["The semantics of content is one of the essential constituents of models of innovative educational systems.","It is gradually built based on normative efforts carried out by different actors in the fields of the technological industry, telecommunications, IT, linguistic engineering, information sciences documentation, etc.","Semantics in networks and digital information systems represent, in fact, an advanced link in a long process of processing digital information in which terminological work occupies an important part.","This process is also very consolidated by a wide range of norms and standards which ensure very high levels of technical, organizational, and semantic interoperability."],"url":"http://arxiv.org/abs/2404.13978v1","category":"cs.CY"}
{"created":"2024-04-22 08:34:26","title":"Strategic geometric graphs through mean field games","abstract":"We exploit the structure of geometric graphs on Riemannian manifolds to analyze strategic dynamic graphs at the limit, when the number of nodes tends to infinity. This framework allows to preserve intrinsic geometrical information about the limiting graph structure, such as the Ollivier curvature. After introducing the setting, we derive a mean field game system, which models a strategic equilibrium between the nodes. It has the usual structure with the distinction of being set on a manifold. Finally, we establish existence and uniqueness of solutions to the system when the Hamiltonian is quadratic for a class of non-necessarily compact Riemannian manifolds, referred to as manifolds of bounded geometry.","sentences":["We exploit the structure of geometric graphs on Riemannian manifolds to analyze strategic dynamic graphs at the limit, when the number of nodes tends to infinity.","This framework allows to preserve intrinsic geometrical information about the limiting graph structure, such as the Ollivier curvature.","After introducing the setting, we derive a mean field game system, which models a strategic equilibrium between the nodes.","It has the usual structure with the distinction of being set on a manifold.","Finally, we establish existence and uniqueness of solutions to the system when the Hamiltonian is quadratic for a class of non-necessarily compact Riemannian manifolds, referred to as manifolds of bounded geometry."],"url":"http://arxiv.org/abs/2404.13975v1","category":"math.AP"}
{"created":"2024-04-22 08:27:14","title":"HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers","abstract":"In this paper, we introduce HamilToniQ, an open-source, and application-oriented benchmarking toolkit for the comprehensive evaluation of Quantum Processing Units (QPUs). Designed to navigate the complexities of quantum computations, HamilToniQ incorporates a methodological framework assessing QPU types, topologies, and multi-QPU systems. The toolkit facilitates the evaluation of QPUs' performance through multiple steps including quantum circuit compilation and quantum error mitigation (QEM), integrating strategies that are unique to each stage. HamilToniQ's standardized score, H-Score, quantifies the fidelity and reliability of QPUs, providing a multidimensional perspective of QPU performance. With a focus on the Quantum Approximate Optimization Algorithm (QAOA), the toolkit enables direct, comparable analysis of QPUs, enhancing transparency and equity in benchmarking. Demonstrated in this paper, HamilToniQ has been validated on various IBM QPUs, affirming its effectiveness and robustness. Overall, HamilToniQ significantly contributes to the advancement of the quantum computing field by offering precise and equitable benchmarking metrics.","sentences":["In this paper, we introduce HamilToniQ, an open-source, and application-oriented benchmarking toolkit for the comprehensive evaluation of Quantum Processing Units (QPUs).","Designed to navigate the complexities of quantum computations, HamilToniQ incorporates a methodological framework assessing QPU types, topologies, and multi-QPU systems.","The toolkit facilitates the evaluation of QPUs' performance through multiple steps including quantum circuit compilation and quantum error mitigation (QEM), integrating strategies that are unique to each stage.","HamilToniQ's standardized score, H-Score, quantifies the fidelity and reliability of QPUs, providing a multidimensional perspective of QPU performance.","With a focus on the Quantum Approximate Optimization Algorithm (QAOA), the toolkit enables direct, comparable analysis of QPUs, enhancing transparency and equity in benchmarking.","Demonstrated in this paper, HamilToniQ has been validated on various IBM QPUs, affirming its effectiveness and robustness.","Overall, HamilToniQ significantly contributes to the advancement of the quantum computing field by offering precise and equitable benchmarking metrics."],"url":"http://arxiv.org/abs/2404.13971v1","category":"quant-ph"}
{"created":"2024-04-22 08:14:49","title":"Neural Control Systems","abstract":"We propose a function-learning methodology with a control-theoretical foundation. We parametrise the approximating function as the solution to a control system on a reproducing-kernel Hilbert space, and propose several methods to find the set of controls which bring the initial function as close as possible to the target function. At first, we derive the expression for the gradient of the cost function with respect to the controls that parametrise the difference equations. This allows us to find the optimal controls by means of gradient descent. In addition, we show how to compute derivatives of the approximating functions with respect to the controls and describe two optimisation methods relying on linear approximations of the approximating functions. We show how the assumptions we make lead to results which are coherent with Pontryagin's maximum principle. We test the optimisation methods on two toy examples and on two higher-dimensional real-world problems, showing that the approaches succeed in learning from real data and are versatile enough to tackle learning tasks of different nature.","sentences":["We propose a function-learning methodology with a control-theoretical foundation.","We parametrise the approximating function as the solution to a control system on a reproducing-kernel Hilbert space, and propose several methods to find the set of controls which bring the initial function as close as possible to the target function.","At first, we derive the expression for the gradient of the cost function with respect to the controls that parametrise the difference equations.","This allows us to find the optimal controls by means of gradient descent.","In addition, we show how to compute derivatives of the approximating functions with respect to the controls and describe two optimisation methods relying on linear approximations of the approximating functions.","We show how the assumptions we make lead to results which are coherent with Pontryagin's maximum principle.","We test the optimisation methods on two toy examples and on two higher-dimensional real-world problems, showing that the approaches succeed in learning from real data and are versatile enough to tackle learning tasks of different nature."],"url":"http://arxiv.org/abs/2404.13967v1","category":"math.OC"}
{"created":"2024-04-22 08:12:30","title":"The complex landslide flow and the method of integrable systems","abstract":"We investigate a connection between the complex landslide flow, defined on a pair of Teichm\\\"uller spaces, and the integrable system method's approach to harmonic maps into a symmetric space. We will prove that the holonomy of the complex landslide flow can be derived from the holonomy of the family of flat connections determined by a harmonic map into the hyperbolic two-space.","sentences":["We investigate a connection between the complex landslide flow, defined on a pair of Teichm\\\"uller spaces, and the integrable system method's approach to harmonic maps into a symmetric space.","We will prove that the holonomy of the complex landslide flow can be derived from the holonomy of the family of flat connections determined by a harmonic map into the hyperbolic two-space."],"url":"http://arxiv.org/abs/2404.13966v1","category":"math.DG"}
{"created":"2024-04-22 07:55:03","title":"GNSS Measurement-Based Context Recognition for Vehicle Navigation using Gated Recurrent Unit","abstract":"Recent years, people have put forward higher and higher requirements for context-adaptive navigation (CAN). CAN system realizes seamless navigation in complex environments by recognizing the ambient surroundings of vehicles, and it is crucial to develop a fast, reliable, and robust navigational context recognition (NCR) method to enable CAN systems to operate effectively. Environmental context recognition based on Global Navigation Satellite System (GNSS) measurements has attracted widespread attention due to its low cost because it does not require additional infrastructure. The performance and application value of NCR methods depend on three main factors: context categorization, feature extraction, and classification models. In this paper, a fine-grained context categorization framework comprising seven environment categories (open sky, tree-lined avenue, semi-outdoor, urban canyon, viaduct-down, shallow indoor, and deep indoor) is proposed, which currently represents the most elaborate context categorization framework known in this research domain. To improve discrimination between categories, a new feature called the C/N0-weighted azimuth distribution factor, is designed. Then, to ensure real-time performance, a lightweight gated recurrent unit (GRU) network is adopted for its excellent sequence data processing capabilities. A dataset containing 59,996 samples is created and made publicly available to researchers in the NCR community on Github. Extensive experiments have been conducted on the dataset, and the results show that the proposed method achieves an overall recognition accuracy of 99.41\\% for isolated scenarios and 94.95\\% for transition scenarios, with an average transition delay of 2.14 seconds.","sentences":["Recent years, people have put forward higher and higher requirements for context-adaptive navigation (CAN).","CAN system realizes seamless navigation in complex environments by recognizing the ambient surroundings of vehicles, and it is crucial to develop a fast, reliable, and robust navigational context recognition (NCR) method to enable CAN systems to operate effectively.","Environmental context recognition based on Global Navigation Satellite System (GNSS) measurements has attracted widespread attention due to its low cost because it does not require additional infrastructure.","The performance and application value of NCR methods depend on three main factors: context categorization, feature extraction, and classification models.","In this paper, a fine-grained context categorization framework comprising seven environment categories (open sky, tree-lined avenue, semi-outdoor, urban canyon, viaduct-down, shallow indoor, and deep indoor) is proposed, which currently represents the most elaborate context categorization framework known in this research domain.","To improve discrimination between categories, a new feature called the C/N0-weighted azimuth distribution factor, is designed.","Then, to ensure real-time performance, a lightweight gated recurrent unit (GRU) network is adopted for its excellent sequence data processing capabilities.","A dataset containing 59,996 samples is created and made publicly available to researchers in the NCR community on Github.","Extensive experiments have been conducted on the dataset, and the results show that the proposed method achieves an overall recognition accuracy of 99.41\\% for isolated scenarios and 94.95\\% for transition scenarios, with an average transition delay of 2.14 seconds."],"url":"http://arxiv.org/abs/2404.13955v1","category":"eess.SP"}
{"created":"2024-04-22 07:52:22","title":"Program Environment Fuzzing","abstract":"Computer programs are not executed in isolation, but rather interact with the execution environment which drives the program behaviours. Software validation and verification methods, such as greybox fuzzing, thus need to capture the effect of possibly complex environmental interactions, including files, databases, configurations, network sockets, human-user interactions, and more. Conventional approaches for environment capture in symbolic execution and model checking employ environment modelling, which involves manual effort. In this paper, we take a different approach based on an extension of greybox fuzzing. Given a program, we first record all observed environmental interactions at the kernel/user-mode boundary in the form of system calls. Next, we replay the program under the original recorded interactions, but this time with selective mutations applied, in order to get the effect of different program environments -- all without environment modelling. Via repeated (feedback-driven) mutations over a fuzzing campaign, we can search for program environments that induce crashing behaviour. Our EFuzz tool found 33 zero-day bugs in well-known real-world protocol implementations and GUI applications. Many of these are security vulnerabilities and 14 CVEs were assigned.","sentences":["Computer programs are not executed in isolation, but rather interact with the execution environment which drives the program behaviours.","Software validation and verification methods, such as greybox fuzzing, thus need to capture the effect of possibly complex environmental interactions, including files, databases, configurations, network sockets, human-user interactions, and more.","Conventional approaches for environment capture in symbolic execution and model checking employ environment modelling, which involves manual effort.","In this paper, we take a different approach based on an extension of greybox fuzzing.","Given a program, we first record all observed environmental interactions at the kernel/user-mode boundary in the form of system calls.","Next, we replay the program under the original recorded interactions, but this time with selective mutations applied, in order to get the effect of different program environments -- all without environment modelling.","Via repeated (feedback-driven) mutations over a fuzzing campaign, we can search for program environments that induce crashing behaviour.","Our EFuzz tool found 33 zero-day bugs in well-known real-world protocol implementations and GUI applications.","Many of these are security vulnerabilities and 14 CVEs were assigned."],"url":"http://arxiv.org/abs/2404.13951v1","category":"cs.SE"}
{"created":"2024-04-22 07:50:24","title":"PeLiCal: Targetless Extrinsic Calibration via Penetrating Lines for RGB-D Cameras with Limited Co-visibility","abstract":"RGB-D cameras are crucial in robotic perception, given their ability to produce images augmented with depth data. However, their limited FOV often requires multiple cameras to cover a broader area. In multi-camera RGB-D setups, the goal is typically to reduce camera overlap, optimizing spatial coverage with as few cameras as possible. The extrinsic calibration of these systems introduces additional complexities. Existing methods for extrinsic calibration either necessitate specific tools or highly depend on the accuracy of camera motion estimation. To address these issues, we present PeLiCal, a novel line-based calibration approach for RGB-D camera systems exhibiting limited overlap. Our method leverages long line features from surroundings, and filters out outliers with a novel convergence voting algorithm, achieving targetless, real-time, and outlier-robust performance compared to existing methods. We open source our implementation on https://github.com/joomeok/PeLiCal.git.","sentences":["RGB-D cameras are crucial in robotic perception, given their ability to produce images augmented with depth data.","However, their limited FOV often requires multiple cameras to cover a broader area.","In multi-camera RGB-D setups, the goal is typically to reduce camera overlap, optimizing spatial coverage with as few cameras as possible.","The extrinsic calibration of these systems introduces additional complexities.","Existing methods for extrinsic calibration either necessitate specific tools or highly depend on the accuracy of camera motion estimation.","To address these issues, we present PeLiCal, a novel line-based calibration approach for RGB-D camera systems exhibiting limited overlap.","Our method leverages long line features from surroundings, and filters out outliers with a novel convergence voting algorithm, achieving targetless, real-time, and outlier-robust performance compared to existing methods.","We open source our implementation on https://github.com/joomeok/PeLiCal.git."],"url":"http://arxiv.org/abs/2404.13949v2","category":"cs.CV"}
{"created":"2024-04-22 07:36:46","title":"Dominic Welsh (1938-2023)","abstract":"This biographical and scientific memoir of Dominic Welsh includes summaries of his important contributions to probability and combinatorics. With John Hammersley, he introduced first-passage percolation, and in so doing they formulated and proved the first subadditive ergodic theorem. Welsh has numerous results in matroid theory, and wrote the first monograph on the topic. He worked on computational complexity and particularly the complexity of computing the Tutte polynomial. He was an inspirational teacher and advisor who helped to develop a community of scholars in combinatorics.","sentences":["This biographical and scientific memoir of Dominic Welsh includes summaries of his important contributions to probability and combinatorics.","With John Hammersley, he introduced first-passage percolation, and in so doing they formulated and proved the first subadditive ergodic theorem.","Welsh has numerous results in matroid theory, and wrote the first monograph on the topic.","He worked on computational complexity and particularly the complexity of computing the Tutte polynomial.","He was an inspirational teacher and advisor who helped to develop a community of scholars in combinatorics."],"url":"http://arxiv.org/abs/2404.13942v1","category":"math.HO"}
{"created":"2024-04-22 07:19:27","title":"Comparison of On-Orbit Manual Attitude Control Methods for Non-Docking Spacecraft Through Virtual Reality Simulation","abstract":"On-orbit manual attitude control of manned spacecraft is accomplished using external visual references and some method of three axis attitude control. All past, present, and developmental spacecraft feature the capability to manually control attitude for deorbit. National Aeronautics and Space Administration (NASA) spacecraft permit an aircraft windshield type front view, wherein an arc of the Earths horizon is visible to the crew in deorbit attitude. Russian and Chinese spacecraft permit the crew a bottom view wherein the entire circular Earth horizon disk is visible to the crew in deorbit attitude. Our study compared these two types of external views for efficiency in achievement of deorbit attitude. We used a Unity Virtual Reality (VR) spacecraft simulator that we built in house. The task was to accurately achieve deorbit attitude while in a 400 km circular orbit. Six military test pilots and six civilians with gaming experience flew the task using two methods of visual reference. Comparison was based on time taken, fuel consumed, cognitive workload assessment and user preference. We used ocular parameters, EEG, NASA TLX and IBM SUS to quantify our results. Our study found that the bottom view was easier to operate for manual deorbit task. Additionally, we realized that a VR based system can work as a training simulator for manual on-orbit flight path control tasks by pilots and non pilots. Results from our study can be used for design of manual on orbit attitude control of present and future spacecrafts.","sentences":["On-orbit manual attitude control of manned spacecraft is accomplished using external visual references and some method of three axis attitude control.","All past, present, and developmental spacecraft feature the capability to manually control attitude for deorbit.","National Aeronautics and Space Administration (NASA) spacecraft permit an aircraft windshield type front view, wherein an arc of the Earths horizon is visible to the crew in deorbit attitude.","Russian and Chinese spacecraft permit the crew a bottom view wherein the entire circular Earth horizon disk is visible to the crew in deorbit attitude.","Our study compared these two types of external views for efficiency in achievement of deorbit attitude.","We used a Unity Virtual Reality (VR) spacecraft simulator that we built in house.","The task was to accurately achieve deorbit attitude while in a 400 km circular orbit.","Six military test pilots and six civilians with gaming experience flew the task using two methods of visual reference.","Comparison was based on time taken, fuel consumed, cognitive workload assessment and user preference.","We used ocular parameters, EEG, NASA TLX and IBM SUS to quantify our results.","Our study found that the bottom view was easier to operate for manual deorbit task.","Additionally, we realized that a VR based system can work as a training simulator for manual on-orbit flight path control tasks by pilots and non pilots.","Results from our study can be used for design of manual on orbit attitude control of present and future spacecrafts."],"url":"http://arxiv.org/abs/2404.13933v1","category":"cs.HC"}
{"created":"2024-04-22 07:15:51","title":"Extreme Elastic Deformation of Atoms and Pressure-Induced Superconductivity in Silicon","abstract":"Change in the interatomic spacing of a two-atom system under tension and compression has been modelled by the elastic deformation of atoms. The critical elastic strain of atoms before separation or cracking from tension was estimated by the Griffith theory together with a recent mechanics model, then extended to the lateral elastic expansion under uniaxial compression. The hypothesis of deformable atoms has led to astonishing predictions of the critical elastic strain, around 10 and 20 percent for silicon in the 110 and 100 crystal directions. Superimposed by the substantial reduction of interatomic spacing in the direction of uniaxial compression above 20 GPa, these severely deformed silicon atoms or metastable new variants have acquired unforeseeable characteristics and properties, vastly different from those of silicon atoms under moderate stresses. Under extreme pressure, the natural repulsive reaction of atoms is intensified due to the increasing alignment of electron orbitals along the pressure direction and formation of metastable pressure-resistant phases. An opportunity for creation of a superconductive band has thus arisen at the edge of the laterally elastically expanded region away from the nuclei, where more space is available for free electron movement. Diamond results were also used to validate the new mechanics model, including the effects of atomic scale defects on fracture strain and strength, critical to elastic strain engineering.","sentences":["Change in the interatomic spacing of a two-atom system under tension and compression has been modelled by the elastic deformation of atoms.","The critical elastic strain of atoms before separation or cracking from tension was estimated by the Griffith theory together with a recent mechanics model, then extended to the lateral elastic expansion under uniaxial compression.","The hypothesis of deformable atoms has led to astonishing predictions of the critical elastic strain, around 10 and 20 percent for silicon in the 110 and 100 crystal directions.","Superimposed by the substantial reduction of interatomic spacing in the direction of uniaxial compression above 20 GPa, these severely deformed silicon atoms or metastable new variants have acquired unforeseeable characteristics and properties, vastly different from those of silicon atoms under moderate stresses.","Under extreme pressure, the natural repulsive reaction of atoms is intensified due to the increasing alignment of electron orbitals along the pressure direction and formation of metastable pressure-resistant phases.","An opportunity for creation of a superconductive band has thus arisen at the edge of the laterally elastically expanded region away from the nuclei, where more space is available for free electron movement.","Diamond results were also used to validate the new mechanics model, including the effects of atomic scale defects on fracture strain and strength, critical to elastic strain engineering."],"url":"http://arxiv.org/abs/2404.13932v1","category":"cond-mat.supr-con"}
{"created":"2024-04-22 07:14:56","title":"Polynomial effective density in quotient of $\\mathrm{SL}_2(\\mathbb{Q}_p) \\times \\mathrm{SL}_2(\\mathbb{Q}_p)$","abstract":"We prove an effective density theorem with polynomial error rate for orbits of upper triangular subgroup of $\\mathrm{SL}_2(\\mathbb{Q}_p)$ in $\\mathrm{SL}_2(\\mathbb{Q}_p) \\times \\mathrm{SL}_2(\\mathbb{Q}_p)$ for prime number $p > 3$. The proof is based on the use of Margulis function, a restricted projection theorem on $\\mathbb{Q}_p^3$, and spectral gap of the ambient space.","sentences":["We prove an effective density theorem with polynomial error rate for orbits of upper triangular subgroup of $\\mathrm{SL}_2(\\mathbb{Q}_p)$ in $\\mathrm{SL}_2(\\mathbb{Q}_p)","\\times \\mathrm{SL}_2(\\mathbb{Q}_p)$ for prime number $p >","3$.","The proof is based on the use of Margulis function, a restricted projection theorem on $\\mathbb{Q}_p^3$, and spectral gap of the ambient space."],"url":"http://arxiv.org/abs/2404.13931v1","category":"math.DS"}
{"created":"2024-04-22 07:04:47","title":"Projectivity criteria for K\u00e4hler morphisms","abstract":"In this short note we prove two projectivity criteria for fibrations between mildly singular compact K\\\"ahler spaces. They are the relative versions of the celebrated criteria of Kodaira and Moishezon. As an application we obtain that the MRC fibration always has a model that is a projective morphism.","sentences":["In this short note we prove two projectivity criteria for fibrations between mildly singular compact K\\\"ahler spaces.","They are the relative versions of the celebrated criteria of Kodaira and Moishezon.","As an application we obtain that the MRC fibration always has a model that is a projective morphism."],"url":"http://arxiv.org/abs/2404.13927v1","category":"math.AG"}
{"created":"2024-04-22 07:03:44","title":"MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit","abstract":"Large language models (LLMs) have been explored in a variety of reasoning tasks including solving of mathematical problems. Each math dataset typically includes its own specially designed evaluation script, which, while suitable for its intended use, lacks generalizability across different datasets. Consequently, updates and adaptations to these evaluation tools tend to occur without being systematically reported, leading to inconsistencies and obstacles to fair comparison across studies. To bridge this gap, we introduce a comprehensive mathematical evaluation toolkit that not only utilizes a python computer algebra system (CAS) for its numerical accuracy, but also integrates an optional LLM, known for its considerable natural language processing capabilities. To validate the effectiveness of our toolkit, we manually annotated two distinct datasets. Our experiments demonstrate that the toolkit yields more robust evaluation results compared to prior works, even without an LLM. Furthermore, when an LLM is incorporated, there is a notable enhancement. The code for our method will be made available at \\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}.","sentences":["Large language models (LLMs) have been explored in a variety of reasoning tasks including solving of mathematical problems.","Each math dataset typically includes its own specially designed evaluation script, which, while suitable for its intended use, lacks generalizability across different datasets.","Consequently, updates and adaptations to these evaluation tools tend to occur without being systematically reported, leading to inconsistencies and obstacles to fair comparison across studies.","To bridge this gap, we introduce a comprehensive mathematical evaluation toolkit that not only utilizes a python computer algebra system (CAS) for its numerical accuracy, but also integrates an optional LLM, known for its considerable natural language processing capabilities.","To validate the effectiveness of our toolkit, we manually annotated two distinct datasets.","Our experiments demonstrate that the toolkit yields more robust evaluation results compared to prior works, even without an LLM.","Furthermore, when an LLM is incorporated, there is a notable enhancement.","The code for our method will be made available at \\url{https://github.com/MARIO-Math-Reasoning/math_evaluation}."],"url":"http://arxiv.org/abs/2404.13925v1","category":"cs.CL"}
{"created":"2024-04-22 06:57:33","title":"Emerging Advancements in 6G NTN Radio Access Technologies: An Overview","abstract":"The efforts on the development, standardization and improvements to communication systems towards 5G Advanced and 6G are on track to provide benefits such as an unprecedented level of connectivity and performance, enabling a diverse range of vertical services. The full integration of non-terrestrial components into 6G plays a pivotal role in realizing this paradigm shift towards ubiquitous communication and global coverage. However, this integration into 6G brings forth a set of its own challenges, particularly in Radio Access Technologies (RATs). To this end, this paper comprehensively discusses those challenges at different levels of RATs and proposes the corresponding potential emerging advancements in the realm of 6G NTN. In particular, the focus is on advancing the prospective aspects of Radio Resource Management (RRM), spectral coexistence in terrestrial and non-terrestrial components and flexible waveform design solutions to combat the impediments. This discussion with a specific focus on emerging advancements in 6G NTN RATs is critical for shaping the next generation networks and potentially relevant in contributing the part in standardization in forthcoming releases","sentences":["The efforts on the development, standardization and improvements to communication systems towards 5G Advanced and 6G are on track to provide benefits such as an unprecedented level of connectivity and performance, enabling a diverse range of vertical services.","The full integration of non-terrestrial components into 6G plays a pivotal role in realizing this paradigm shift towards ubiquitous communication and global coverage.","However, this integration into 6G brings forth a set of its own challenges, particularly in Radio Access Technologies (RATs).","To this end, this paper comprehensively discusses those challenges at different levels of RATs and proposes the corresponding potential emerging advancements in the realm of 6G NTN.","In particular, the focus is on advancing the prospective aspects of Radio Resource Management (RRM), spectral coexistence in terrestrial and non-terrestrial components and flexible waveform design solutions to combat the impediments.","This discussion with a specific focus on emerging advancements in 6G NTN RATs is critical for shaping the next generation networks and potentially relevant in contributing the part in standardization in forthcoming releases"],"url":"http://arxiv.org/abs/2404.13918v1","category":"eess.SP"}
{"created":"2024-04-22 06:53:13","title":"Angle-Aware Coverage with Camera Rotational Motion Control","abstract":"This paper presents a novel control strategy for drone networks to improve the quality of 3D structures reconstructed from aerial images by drones. Unlike the existing coverage control strategies for this purpose, our proposed approach simultaneously controls both the camera orientation and drone translational motion, enabling more comprehensive perspectives and enhancing the map's overall quality. Subsequently, we present a novel problem formulation, including a new performance function to evaluate the drone positions and camera orientations. We then design a QP-based controller with a control barrier-like function for a constraint on the decay rate of the objective function. The present problem formulation poses a new challenge, requiring significantly greater computational efforts than the case involving only translational motion control. We approach this issue technologically, namely by introducing JAX, utilizing just-in-time (JIT) compilation and Graphical Processing Unit (GPU) acceleration. We finally conduct extensive verifications through simulation in ROS (Robot Operating System) and show the real-time feasibility of the controller and the superiority of the present controller to the conventional method.","sentences":["This paper presents a novel control strategy for drone networks to improve the quality of 3D structures reconstructed from aerial images by drones.","Unlike the existing coverage control strategies for this purpose, our proposed approach simultaneously controls both the camera orientation and drone translational motion, enabling more comprehensive perspectives and enhancing the map's overall quality.","Subsequently, we present a novel problem formulation, including a new performance function to evaluate the drone positions and camera orientations.","We then design a QP-based controller with a control barrier-like function for a constraint on the decay rate of the objective function.","The present problem formulation poses a new challenge, requiring significantly greater computational efforts than the case involving only translational motion control.","We approach this issue technologically, namely by introducing JAX, utilizing just-in-time (JIT) compilation and Graphical Processing Unit (GPU) acceleration.","We finally conduct extensive verifications through simulation in ROS (Robot Operating System) and show the real-time feasibility of the controller and the superiority of the present controller to the conventional method."],"url":"http://arxiv.org/abs/2404.13915v1","category":"math.OC"}
{"created":"2024-04-22 06:41:26","title":"Physics-informed neural networks with curriculum training for poroelastic flow and deformation processes","abstract":"Physics-Informed Neural Networks (PINNs) have emerged as a highly active research topic across multiple disciplines in science and engineering, including computational geomechanics. PINNs offer a promising approach in different applications where faster, near real-time or real-time numerical prediction is required. Examples of such areas in geomechanics include geotechnical design optimization, digital twins of geo-structures and stability prediction of monitored slopes. But there remain challenges in training of PINNs, especially for problems with high spatial and temporal complexity. In this paper, we study how the training of PINNs can be improved by using an ideal-ized poroelasticity problem as a demonstration example. A curriculum training strat-egy is employed where the PINN model is trained gradually by dividing the training data into intervals along the temporal dimension. We find that the PINN model with curriculum training takes nearly half the time required for training compared to con-ventional training over the whole solution domain. For the particular example here, the quality of the predicted solution was found to be good in both training approach-es, but it is anticipated that the curriculum training approach has the potential to offer a better prediction capability for more complex problems, a subject for further research.","sentences":["Physics-Informed Neural Networks (PINNs) have emerged as a highly active research topic across multiple disciplines in science and engineering, including computational geomechanics.","PINNs offer a promising approach in different applications where faster, near real-time or real-time numerical prediction is required.","Examples of such areas in geomechanics include geotechnical design optimization, digital twins of geo-structures and stability prediction of monitored slopes.","But there remain challenges in training of PINNs, especially for problems with high spatial and temporal complexity.","In this paper, we study how the training of PINNs can be improved by using an ideal-ized poroelasticity problem as a demonstration example.","A curriculum training strat-egy is employed where the PINN model is trained gradually by dividing the training data into intervals along the temporal dimension.","We find that the PINN model with curriculum training takes nearly half the time required for training compared to con-ventional training over the whole solution domain.","For the particular example here, the quality of the predicted solution was found to be good in both training approach-es, but it is anticipated that the curriculum training approach has the potential to offer a better prediction capability for more complex problems, a subject for further research."],"url":"http://arxiv.org/abs/2404.13909v1","category":"cs.CE"}
{"created":"2024-04-22 06:37:03","title":"Normalized grounded states for a coupled nonlinear schr\u00f6dinger system on $\\mathbb{R}^3$","abstract":"We investigate the existence of normalized ground states to the system of coupled Schr\\\"odinger equations: \\begin{equation}\\label{eq:0.1}   \\begin{cases}   -\\Delta u_1 + \\lambda_1 u_1 = \\mu_1 |u_1|^{p_1-2}u_1 + \\beta r_1|u_1|^{r_1-2}u_1|u_2|^{r_2} & \\text{ in } \\mathbb{R}^{3},   -\\Delta u_2 + \\lambda_2 u_2 = \\mu_2|u_2|^{p_2-2}u_2 + \\beta r_2|u_1|^{r_1}|u_2|^{r_2-2}u_2 & \\text{ in } \\mathbb{R}^3,   \\end{cases}   \\end{equation} subject to the constraints $\\mathcal{S}_{a_1} \\times \\mathcal{S}_{a_2} = \\{(u_1 \\in H^1(\\mathbb{R}^3))|\\int_{\\mathbb{R}^3} u_1^2 dx = a_1^2\\} \\times \\{(u_2 \\in H^1(\\mathbb{R}^3))|\\int_{\\mathbb{R}^3} u_2^2 dx = a_2^2\\}$, where $\\mu_1, \\mu_2 > 0$, $r_1, r_2 > 1$, and $\\beta \\geq 0$. Our focus is on the coupled mass super-critical case, specifically, $$\\frac{10}{3} < p_1, p_2, r_1 + r_2 < 2^* = 6.$$ We demonstrate that there exists a $\\tilde{\\beta} \\geq 0$ such that equation (\\ref{eq:0.1}) admits positive, radially symmetric, normalized ground state solutions when $\\beta > \\tilde{\\beta}$. Furthermore, this result can be generalized to systems with an arbitrary number of components, and the corresponding standing wave is orbitally unstable.","sentences":["We investigate the existence of normalized ground states to the system of coupled Schr\\\"odinger equations: \\begin{equation}\\label{eq:0.1}   \\begin{cases}   -\\Delta u_1 + \\lambda_1 u_1 = \\mu_1 |u_1|^{p_1-2}u_1 + \\beta r_1|u_1|^{r_1-2}u_1|u_2|^{r_2} & \\text{ in } \\mathbb{R}^{3},   -\\Delta u_2 + \\lambda_2 u_2 = \\mu_2|u_2|^{p_2-2}u_2 + \\beta r_2|u_1|^{r_1}|u_2|^{r_2-2}u_2 & \\text{ in } \\mathbb{R}^3,   \\end{cases}   \\end{equation} subject to the constraints $\\mathcal{S}_{a_1} \\times \\mathcal{S}_{a_2} = \\{(u_1 \\in H^1(\\mathbb{R}^3))|\\int_{\\mathbb{R}^3} u_1^2 dx = a_1^2\\} \\times \\{(u_2 \\in H^1(\\mathbb{R}^3))|\\int_{\\mathbb{R}^3} u_2^2 dx = a_2^2\\}$, where $\\mu_1, \\mu_2 > 0$, $r_1, r_2 > 1$, and $\\beta \\geq 0$.","Our focus is on the coupled mass super-critical case, specifically, $$\\frac{10}{3} < p_1, p_2, r_1 + r_2 <","2^*","= 6.$$","We demonstrate that there exists a $\\tilde{\\beta} \\geq 0$ such that equation (\\ref{eq:0.1}) admits positive, radially symmetric, normalized ground state solutions when $\\beta > \\tilde{\\beta}$.","Furthermore, this result can be generalized to systems with an arbitrary number of components, and the corresponding standing wave is orbitally unstable."],"url":"http://arxiv.org/abs/2404.13908v2","category":"math.AP"}
{"created":"2024-04-22 06:28:41","title":"Deep Regression Representation Learning with Topology","abstract":"Most works studying representation learning focus only on classification and neglect regression. Yet, the learning objectives and therefore the representation topologies of the two tasks are fundamentally different: classification targets class separation, leading to disconnected representations, whereas regression requires ordinality with respect to the target, leading to continuous representations. We thus wonder how the effectiveness of a regression representation is influenced by its topology, with evaluation based on the Information Bottleneck (IB) principle.   The IB principle is an important framework that provides principles for learning effectiveness representations. We establish two connections between it and the topology of regression representations. The first connection reveals that a lower intrinsic dimension of the feature space implies a reduced complexity of the representation Z. This complexity can be quantified as the conditional entropy of Z on the target space Y and serves as an upper bound on the generalization error. The second connection suggests learning a feature space that is topologically similar to the target space will better align with the IB principle. Based on these two connections, we introduce PH-Reg, a regularizer specific to regression that matches the intrinsic dimension and topology of the feature space with the target space. Experiments on synthetic and real-world regression tasks demonstrate the benefits of PH-Reg.","sentences":["Most works studying representation learning focus only on classification and neglect regression.","Yet, the learning objectives and therefore the representation topologies of the two tasks are fundamentally different: classification targets class separation, leading to disconnected representations, whereas regression requires ordinality with respect to the target, leading to continuous representations.","We thus wonder how the effectiveness of a regression representation is influenced by its topology, with evaluation based on the Information Bottleneck (IB) principle.   ","The IB principle is an important framework that provides principles for learning effectiveness representations.","We establish two connections between it and the topology of regression representations.","The first connection reveals that a lower intrinsic dimension of the feature space implies a reduced complexity of the representation Z.","This complexity can be quantified as the conditional entropy of Z on the target space Y and serves as an upper bound on the generalization error.","The second connection suggests learning a feature space that is topologically similar to the target space will better align with the IB principle.","Based on these two connections, we introduce PH-Reg, a regularizer specific to regression that matches the intrinsic dimension and topology of the feature space with the target space.","Experiments on synthetic and real-world regression tasks demonstrate the benefits of PH-Reg."],"url":"http://arxiv.org/abs/2404.13904v1","category":"cs.LG"}
{"created":"2024-04-22 06:24:00","title":"Evaluating experiences in a digital nutrition education program for people with multiple sclerosis: a qualitative study","abstract":"Background Multiple sclerosis (MS) is a complex immune-mediated disease with no currently known cure. There is growing evidence to support the role of diet in reducing some of the symptoms and disease progression in MS, and we previously developed and tested the feasibility of a digital nutrition education program for people with MS. Objective The aim of this study was to explore factors that influenced engagement in the digital nutrition education program, including features influencing capability, opportunity, and motivation to change their dietary behaviours. Methods Semi-structured interviews were conducted with people who MS who completed some or all of the program, until data saturation was reached. Interviews were analysed inductively using thematic analysis. Themes were deductively mapped against the COM-B behaviour change model. Results 16 interviews were conducted with participants who completed all (n=10) or some of the program (n=6). Four themes emerged: 1) Acquiring and validating nutrition knowledge; 2) Influence of time and social support; 3) Getting in early to improve health; and 4) Accounting for food literacy experiences. Discussion This is the first online nutrition program with suitable behavioural supports for people with MS. It highlights the importance of disease-specific and evidence-based nutrition education to support people with MS to make dietary changes. Acquiring nutrition knowledge, coupled with practical support mechanisms such as recipe booklets and goal-setting, emerged as crucial for facilitating engagement with the program. Conclusions When designing education programs for people with MS and other neurological conditions, healthcare professionals and program designers should consider flexible delivery and building peer support to address the needs and challenges faced by participants.","sentences":["Background Multiple sclerosis (MS) is a complex immune-mediated disease with no currently known cure.","There is growing evidence to support the role of diet in reducing some of the symptoms and disease progression in MS, and we previously developed and tested the feasibility of a digital nutrition education program for people with MS.","Objective The aim of this study was to explore factors that influenced engagement in the digital nutrition education program, including features influencing capability, opportunity, and motivation to change their dietary behaviours.","Methods Semi-structured interviews were conducted with people who MS who completed some or all of the program, until data saturation was reached.","Interviews were analysed inductively using thematic analysis.","Themes were deductively mapped against the COM-B behaviour change model.","Results 16 interviews were conducted with participants who completed all (n=10) or some of the program (n=6).","Four themes emerged: 1) Acquiring and validating nutrition knowledge; 2) Influence of time and social support; 3) Getting in early to improve health; and 4) Accounting for food literacy experiences.","Discussion This is the first online nutrition program with suitable behavioural supports for people with MS.","It highlights the importance of disease-specific and evidence-based nutrition education to support people with MS to make dietary changes.","Acquiring nutrition knowledge, coupled with practical support mechanisms such as recipe booklets and goal-setting, emerged as crucial for facilitating engagement with the program.","Conclusions When designing education programs for people with MS and other neurological conditions, healthcare professionals and program designers should consider flexible delivery and building peer support to address the needs and challenges faced by participants."],"url":"http://arxiv.org/abs/2404.13902v1","category":"q-bio.OT"}
{"created":"2024-04-22 06:16:29","title":"Residual Entropy of Hexagonal Ice and Cubic Ice: A Transfer Matrix Description","abstract":"Residual entropy of ice systems has long been a significant and intriguing issue in condensed matter physics and statistical mechanics. The exact solutions for the residual entropy of realistic three-dimensional ice systems remain unknown. In this study, we focus on two typical realistic ice systems, namely the hexagonal ice (ice Ih) and cubic ice (ice Ic). We present a transfer matrix description of the number of ice-ruled configurations for these two systems. First, a transfer matrix M is constructed for ice Ic, where each element is the number of ice-ruled configurations of a hexagonal monolayer under certain condition. The product of M and its transpose corresponds to a bilayer unit in ice Ih lattice, therefore is exactly a transfer matrix for ice Ih. Making use of this, we simply show that the residual entropy of ice Ih is not less than that of ice Ic in the thermodynamic limit, which was first proved by Onsager in 1960s. Furthermore, we find an alternative transfer matrix M' for ice Ih, which is based on a monolayer periodic unit. Some interesting properties of M, MMT and M' are illustrated, specifically the summation of all elements, the element in the first row and first column, and the trace. Each property is equivalent with the residual entropy of a two-dimensional ice model. Our work rediscovers the relationship between the residual entropies of ice Ih and ice Ic, and provides an effective description for various two-dimensional ice models.","sentences":["Residual entropy of ice systems has long been a significant and intriguing issue in condensed matter physics and statistical mechanics.","The exact solutions for the residual entropy of realistic three-dimensional ice systems remain unknown.","In this study, we focus on two typical realistic ice systems, namely the hexagonal ice (ice Ih) and cubic ice (ice Ic).","We present a transfer matrix description of the number of ice-ruled configurations for these two systems.","First, a transfer matrix M is constructed for ice Ic, where each element is the number of ice-ruled configurations of a hexagonal monolayer under certain condition.","The product of M and its transpose corresponds to a bilayer unit in ice Ih lattice, therefore is exactly a transfer matrix for ice Ih.","Making use of this, we simply show that the residual entropy of ice Ih is not less than that of ice Ic in the thermodynamic limit, which was first proved by Onsager in 1960s.","Furthermore, we find an alternative transfer matrix M' for ice Ih, which is based on a monolayer periodic unit.","Some interesting properties of M, MMT and M' are illustrated, specifically the summation of all elements, the element in the first row and first column, and the trace.","Each property is equivalent with the residual entropy of a two-dimensional ice model.","Our work rediscovers the relationship between the residual entropies of ice Ih and ice Ic, and provides an effective description for various two-dimensional ice models."],"url":"http://arxiv.org/abs/2404.13897v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-22 06:07:06","title":"CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with Complex Trajectory","abstract":"Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction. However, NeRF heavily relies on precise camera poses. While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes. Existing methods struggle while tackling complex trajectories involving large rotations. To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input. In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure. Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs. Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories. We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories. Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy.","sentences":["Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction.","However, NeRF heavily relies on precise camera poses.","While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes.","Existing methods struggle while tackling complex trajectories involving large rotations.","To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input.","In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure.","Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs.","Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories.","We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories.","Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy."],"url":"http://arxiv.org/abs/2404.13896v1","category":"cs.CV"}
{"created":"2024-04-22 05:32:47","title":"Cell Balancing for the Transportation Sector: Techniques, Challenges, and Future Research Directions","abstract":"Efficient and reliable energy systems are key to progress of society. High performance batteries are essential for widely used technologies like Electric Vehicles (EVs) and portable electronics. Additionally, an effective Battery Management System (BMS) is crucial to oversee vital parameters of battery. However, BMS can experience cell imbalance due to charging/discharging dynamics, which reduce battery capacity, lifespan, and efficiency, and raise critical safety concerns. This calls for effective cell-balancing techniques. Notably, the existing literature on cell balancing is limited, urgently necessitating a thorough survey to pinpoint key research gaps and suggest prompt solutions. In this article, cell balancing and corresponding techniques are reviewed. Initially, we detail comparison of passive cell balancing techniques and assess their respective advantages, drawbacks, and practical applications. Then, we discuss the strengths and weaknesses of active cell balancing methods and applicability of cell balancing for both, series and parallel-connected cells. Additionally, we examine the need for cell balancing in commonly used batteries, and applications in EVs. Lastly, we present detailed prospects which include challenges and directions for future research.","sentences":["Efficient and reliable energy systems are key to progress of society.","High performance batteries are essential for widely used technologies like Electric Vehicles (EVs) and portable electronics.","Additionally, an effective Battery Management System (BMS) is crucial to oversee vital parameters of battery.","However, BMS can experience cell imbalance due to charging/discharging dynamics, which reduce battery capacity, lifespan, and efficiency, and raise critical safety concerns.","This calls for effective cell-balancing techniques.","Notably, the existing literature on cell balancing is limited, urgently necessitating a thorough survey to pinpoint key research gaps and suggest prompt solutions.","In this article, cell balancing and corresponding techniques are reviewed.","Initially, we detail comparison of passive cell balancing techniques and assess their respective advantages, drawbacks, and practical applications.","Then, we discuss the strengths and weaknesses of active cell balancing methods and applicability of cell balancing for both, series and parallel-connected cells.","Additionally, we examine the need for cell balancing in commonly used batteries, and applications in EVs.","Lastly, we present detailed prospects which include challenges and directions for future research."],"url":"http://arxiv.org/abs/2404.13890v1","category":"eess.SY"}
{"created":"2024-04-22 05:23:25","title":"Taming Server Memory TCO with Multiple Software-Defined Compressed Tiers","abstract":"Memory accounts for 33 - 50% of the total cost of ownership (TCO) in modern data centers. We propose a novel solution to tame memory TCO through the novel creation and judicious management of multiple software-defined compressed memory tiers.   As opposed to the state-of-the-art solutions that employ a 2-Tier solution, a single compressed tier along with DRAM, we define multiple compressed tiers implemented through a combination of different compression algorithms, memory allocators for compressed objects, and backing media to store compressed objects. These compressed memory tiers represent distinct points in the access latency, data compressibility, and unit memory usage cost spectrum, allowing rich and flexible trade-offs between memory TCO savings and application performance impact. A key advantage with ntier is that it enables aggressive memory TCO saving opportunities by placing warm data in low latency compressed tiers with a reasonable performance impact while simultaneously placing cold data in the best memory TCO saving tiers. We believe our work represents an important server system configuration and optimization capability to achieve the best SLA-aware performance per dollar for applications hosted in production data center environments.   We present a comprehensive and rigorous analytical cost model for performance and TCO trade-off based on continuous monitoring of the application's data access profile. Guided by this model, our placement model takes informed actions to dynamically manage the placement and migration of application data across multiple software-defined compressed tiers. On real-world benchmarks, our solution increases memory TCO savings by 22% - 40% percentage points while maintaining performance parity or improves performance by 2% - 10% percentage points while maintaining memory TCO parity compared to state-of-the-art 2-Tier solutions.","sentences":["Memory accounts for 33 - 50% of the total cost of ownership (TCO) in modern data centers.","We propose a novel solution to tame memory TCO through the novel creation and judicious management of multiple software-defined compressed memory tiers.   ","As opposed to the state-of-the-art solutions that employ a 2-Tier solution, a single compressed tier along with DRAM, we define multiple compressed tiers implemented through a combination of different compression algorithms, memory allocators for compressed objects, and backing media to store compressed objects.","These compressed memory tiers represent distinct points in the access latency, data compressibility, and unit memory usage cost spectrum, allowing rich and flexible trade-offs between memory TCO savings and application performance impact.","A key advantage with ntier is that it enables aggressive memory TCO saving opportunities by placing warm data in low latency compressed tiers with a reasonable performance impact while simultaneously placing cold data in the best memory TCO saving tiers.","We believe our work represents an important server system configuration and optimization capability to achieve the best SLA-aware performance per dollar for applications hosted in production data center environments.   ","We present a comprehensive and rigorous analytical cost model for performance and TCO trade-off based on continuous monitoring of the application's data access profile.","Guided by this model, our placement model takes informed actions to dynamically manage the placement and migration of application data across multiple software-defined compressed tiers.","On real-world benchmarks, our solution increases memory TCO savings by 22% - 40% percentage points while maintaining performance parity or improves performance by 2% - 10% percentage points while maintaining memory TCO parity compared to state-of-the-art 2-Tier solutions."],"url":"http://arxiv.org/abs/2404.13886v1","category":"cs.OS"}
{"created":"2024-04-22 05:12:11","title":"MambaUIE&SR: Unraveling the Ocean's Secrets with Only 2.8 FLOPs","abstract":"Underwater Image Enhancement (UIE) techniques aim to address the problem of underwater image degradation due to light absorption and scattering. In recent years, both Convolution Neural Network (CNN)-based and Transformer-based methods have been widely explored. In addition, combining CNN and Transformer can effectively combine global and local information for enhancement. However, this approach is still affected by the secondary complexity of the Transformer and cannot maximize the performance. Recently, the state-space model (SSM) based architecture Mamba has been proposed, which excels in modeling long distances while maintaining linear complexity. This paper explores the potential of this SSM-based model for UIE from both efficiency and effectiveness perspectives. However, the performance of directly applying Mamba is poor because local fine-grained features, which are crucial for image enhancement, cannot be fully utilized. Specifically, we customize the MambaUIE architecture for efficient UIE. Specifically, we introduce visual state space (VSS) blocks to capture global contextual information at the macro level while mining local information at the micro level. Also, for these two kinds of information, we propose a Dynamic Interaction Block (DIB) and Spatial feed-forward Network (SGFN) for intra-block feature aggregation. MambaUIE is able to efficiently synthesize global and local information and maintains a very small number of parameters with high accuracy. Experiments on UIEB datasets show that our method reduces GFLOPs by 67.4% (2.715G) relative to the SOTA method. To the best of our knowledge, this is the first UIE model constructed based on SSM that breaks the limitation of FLOPs on accuracy in UIE. The official repository of MambaUIE at https://github.com/1024AILab/MambaUIE.","sentences":["Underwater Image Enhancement (UIE) techniques aim to address the problem of underwater image degradation due to light absorption and scattering.","In recent years, both Convolution Neural Network (CNN)-based and Transformer-based methods have been widely explored.","In addition, combining CNN and Transformer can effectively combine global and local information for enhancement.","However, this approach is still affected by the secondary complexity of the Transformer and cannot maximize the performance.","Recently, the state-space model (SSM) based architecture Mamba has been proposed, which excels in modeling long distances while maintaining linear complexity.","This paper explores the potential of this SSM-based model for UIE from both efficiency and effectiveness perspectives.","However, the performance of directly applying Mamba is poor because local fine-grained features, which are crucial for image enhancement, cannot be fully utilized.","Specifically, we customize the MambaUIE architecture for efficient UIE.","Specifically, we introduce visual state space (VSS) blocks to capture global contextual information at the macro level while mining local information at the micro level.","Also, for these two kinds of information, we propose a Dynamic Interaction Block (DIB) and Spatial feed-forward Network (SGFN) for intra-block feature aggregation.","MambaUIE is able to efficiently synthesize global and local information and maintains a very small number of parameters with high accuracy.","Experiments on UIEB datasets show that our method reduces GFLOPs by 67.4% (2.715G) relative to the SOTA method.","To the best of our knowledge, this is the first UIE model constructed based on SSM that breaks the limitation of FLOPs on accuracy in UIE.","The official repository of MambaUIE at https://github.com/1024AILab/MambaUIE."],"url":"http://arxiv.org/abs/2404.13884v1","category":"eess.IV"}
{"created":"2024-04-22 05:10:02","title":"Decoherence of a charged Brownian particle in a magnetic field : an analysis of the roles of coupling via position and momentum variables","abstract":"The study of decoherence plays a key role in our understanding of the transition from the quantum to the classical world. Typically, one considers a system coupled to an external bath which forms a model for an open quantum system. While most of the studies pertain to a position coupling between the system and the environment, some involve a momentum coupling, giving rise to an anomalous diffusive model. Here we have gone beyond existing studies and analysed the quantum Langevin dynamics of a harmonically oscillating charged Brownian particle in the presence of a magnetic field and coupled to an Ohmic heat bath via both position and momentum couplings. The presence of both position and momentum couplings leads to a stronger interaction with the environment, resulting in a faster loss of coherence compared to a situation where only position coupling is present. The rate of decoherence can be tuned by controlling the relative strengths of the position and momentum coupling parameters. In addition, the magnetic field results in the slowing down of the loss of information from the system, irrespective of the nature of coupling between the system and the bath. Our results can be experimentally verified by designing a suitable ion trap setup.","sentences":["The study of decoherence plays a key role in our understanding of the transition from the quantum to the classical world.","Typically, one considers a system coupled to an external bath which forms a model for an open quantum system.","While most of the studies pertain to a position coupling between the system and the environment, some involve a momentum coupling, giving rise to an anomalous diffusive model.","Here we have gone beyond existing studies and analysed the quantum Langevin dynamics of a harmonically oscillating charged Brownian particle in the presence of a magnetic field and coupled to an Ohmic heat bath via both position and momentum couplings.","The presence of both position and momentum couplings leads to a stronger interaction with the environment, resulting in a faster loss of coherence compared to a situation where only position coupling is present.","The rate of decoherence can be tuned by controlling the relative strengths of the position and momentum coupling parameters.","In addition, the magnetic field results in the slowing down of the loss of information from the system, irrespective of the nature of coupling between the system and the bath.","Our results can be experimentally verified by designing a suitable ion trap setup."],"url":"http://arxiv.org/abs/2404.13883v2","category":"quant-ph"}
{"created":"2024-04-22 05:07:16","title":"Maxwell's and Stokes' operators associated with elliptic differential complexes","abstract":"We propose a new technique to generate reasonable systems of partial differential equations (PDE) that could be potential candidates for depicting models in natural sciences related to quasi-linear equations. Such systems appear within typical constructions of the Homological Algebra as complexes of differential operators describing compatibility conditions for overdetermined systems of PDE's. The related models can be both steady and evolutionary. Additional assumptions on the ellipticity of the differential complex provide a wide class of elliptic, parabolic and hyperbolic operators that could be generated in this way. In particular, it appears that an essentially large amount of equations related to the modern Mathematical Physics is generated by the de Rham complex of differentials on the exterior differential forms. These includes the elliptic Laplace and Lam\\'e type operators; the parabolic heat transfer equation; the Euler type and Navier-Stokes type equations in Hydrodynamics; the hyperbolic wave equation and the Maxwell equations in Electrodynamics; the Klein-Gordon equation in Relativistic Quantum Mechanics; and so on. Our model generation method covers a broad class of generating systems, especially in higher spatial dimensions, due to different basic algebraic structures at play.","sentences":["We propose a new technique to generate reasonable systems of partial differential equations (PDE) that could be potential candidates for depicting models in natural sciences related to quasi-linear equations.","Such systems appear within typical constructions of the Homological Algebra as complexes of differential operators describing compatibility conditions for overdetermined systems of PDE's.","The related models can be both steady and evolutionary.","Additional assumptions on the ellipticity of the differential complex provide a wide class of elliptic, parabolic and hyperbolic operators that could be generated in this way.","In particular, it appears that an essentially large amount of equations related to the modern Mathematical Physics is generated by the de Rham complex of differentials on the exterior differential forms.","These includes the elliptic Laplace and Lam\\'e type operators; the parabolic heat transfer equation; the Euler type and Navier-Stokes type equations in Hydrodynamics; the hyperbolic wave equation and the Maxwell equations in Electrodynamics; the Klein-Gordon equation in Relativistic Quantum Mechanics; and so on.","Our model generation method covers a broad class of generating systems, especially in higher spatial dimensions, due to different basic algebraic structures at play."],"url":"http://arxiv.org/abs/2404.13881v1","category":"math-ph"}
{"created":"2024-04-22 17:58:36","title":"A mean curvature flow arising in adversarial training","abstract":"We connect adversarial training for binary classification to a geometric evolution equation for the decision boundary. Relying on a perspective that recasts adversarial training as a regularization problem, we introduce a modified training scheme that constitutes a minimizing movements scheme for a nonlocal perimeter functional. We prove that the scheme is monotone and consistent as the adversarial budget vanishes and the perimeter localizes, and as a consequence we rigorously show that the scheme approximates a weighted mean curvature flow. This highlights that the efficacy of adversarial training may be due to locally minimizing the length of the decision boundary. In our analysis, we introduce a variety of tools for working with the subdifferential of a supremal-type nonlocal total variation and its regularity properties.","sentences":["We connect adversarial training for binary classification to a geometric evolution equation for the decision boundary.","Relying on a perspective that recasts adversarial training as a regularization problem, we introduce a modified training scheme that constitutes a minimizing movements scheme for a nonlocal perimeter functional.","We prove that the scheme is monotone and consistent as the adversarial budget vanishes and the perimeter localizes, and as a consequence we rigorously show that the scheme approximates a weighted mean curvature flow.","This highlights that the efficacy of adversarial training may be due to locally minimizing the length of the decision boundary.","In our analysis, we introduce a variety of tools for working with the subdifferential of a supremal-type nonlocal total variation and its regularity properties."],"url":"http://arxiv.org/abs/2404.14402v1","category":"math.AP"}
{"created":"2024-04-22 17:19:09","title":"Lessons Learned in Performing a Trustworthy AI and Fundamental Rights Assessment","abstract":"This report shares the experiences, results and lessons learned in conducting a pilot project ``Responsible use of AI'' in cooperation with the Province of Friesland, Rijks ICT Gilde-part of the Ministry of the Interior and Kingdom Relations (BZK) (both in The Netherlands) and a group of members of the Z-Inspection$^{\\small{\\circledR}}$ Initiative. The pilot project took place from May 2022 through January 2023. During the pilot, the practical application of a deep learning algorithm from the province of Fr\\^yslan was assessed. The AI maps heathland grassland by means of satellite images for monitoring nature reserves. Environmental monitoring is one of the crucial activities carried on by society for several purposes ranging from maintaining standards on drinkable water to quantifying the CO2 emissions of a particular state or region. Using satellite imagery and machine learning to support decisions is becoming an important part of environmental monitoring. The main focus of this report is to share the experiences, results and lessons learned from performing both a Trustworthy AI assessment using the Z-Inspection$^{\\small{\\circledR}}$ process and the EU framework for Trustworthy AI, and combining it with a Fundamental Rights assessment using the Fundamental Rights and Algorithms Impact Assessment (FRAIA) as recommended by the Dutch government for the use of AI algorithms by the Dutch public authorities.","sentences":["This report shares the experiences, results and lessons learned in conducting a pilot project ``Responsible use of AI'' in cooperation with the Province of Friesland, Rijks ICT Gilde-part of the Ministry of the Interior and Kingdom Relations (BZK) (both in The Netherlands) and a group of members of the Z-Inspection$^{\\small{\\circledR}}$ Initiative.","The pilot project took place from May 2022 through January 2023.","During the pilot, the practical application of a deep learning algorithm from the province of Fr\\^yslan was assessed.","The AI maps heathland grassland by means of satellite images for monitoring nature reserves.","Environmental monitoring is one of the crucial activities carried on by society for several purposes ranging from maintaining standards on drinkable water to quantifying the CO2 emissions of a particular state or region.","Using satellite imagery and machine learning to support decisions is becoming an important part of environmental monitoring.","The main focus of this report is to share the experiences, results and lessons learned from performing both a Trustworthy AI assessment using the Z-Inspection$^{\\small{\\circledR}}$ process and the EU framework for Trustworthy AI, and combining it with a Fundamental Rights assessment using the Fundamental Rights and Algorithms Impact Assessment (FRAIA) as recommended by the Dutch government for the use of AI algorithms by the Dutch public authorities."],"url":"http://arxiv.org/abs/2404.14366v1","category":"cs.CY"}
{"created":"2024-04-22 16:58:05","title":"Quantifying Scalar Field Dynamics with DESI 2024 Y1 BAO measurements","abstract":"Quintessence scalar fields are a natural candidate for evolving dark energy. Unlike the phenomenological $w_0w_a$ parameterization of the dark energy equation of state, they cannot accommodate the phantom regime of dark energy $w(z) < -1$, or crossings into the phantom regime. Recent baryon acoustic oscillation (BAO) measurements by the Dark Energy Spectroscopic Instrument (DESI) indicate a preference for evolving dark energy over a cosmological constant, ranging from $2.6\\sigma-3.9\\sigma$ when fitting to $w_0w_a$, and combining the DESI BAO measurements with other cosmological probes. In this work, we directly fit three simple scalar field models to the DESI BAO data, combined with cosmic microwave background anisotropy measurements and supernova data sets. Quantifying the preference for scalar field dynamics exhibited by the data, we find that $2-4\\%$ of kinetic scalar field energy $\\Omega_{\\rm scf,k}$, is preferred over $\\Lambda$CDM at the $95\\%$ confidence level, for a canonical scalar field with a quadratic or linear potential. Fitting to the supernova data sets Pantheon, Pantheon+, DES-Y5, and Union3, we show that the mild tension ($n_{\\sigma}< 3.4 $) under $\\Lambda$CDM emerges from a BAO preference for smaller values of fractional mass-energy density $\\Omega_m < 0.29$, while all supernova data sets, except for Pantheon, prefer larger values, $\\Omega_m > 0.3$. The tension under $\\Lambda$CDM remains noticeable ($n_{\\sigma} <2.8$), when replacing two of the DESI BAO measurements redshift bins with effective redshifts $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.706$ with comparable BOSS DR 12 BAO measurements at $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.61$. Canonical scalar fields as dark energy are successful in mitigating that tension.","sentences":["Quintessence scalar fields are a natural candidate for evolving dark energy.","Unlike the phenomenological $w_0w_a$ parameterization of the dark energy equation of state, they cannot accommodate the phantom regime of dark energy $w(z) <","-1$, or crossings into the phantom regime.","Recent baryon acoustic oscillation (BAO) measurements by the Dark Energy Spectroscopic Instrument (DESI) indicate a preference for evolving dark energy over a cosmological constant, ranging from $2.6\\sigma-3.9\\sigma$ when fitting to $w_0w_a$, and combining the DESI BAO measurements with other cosmological probes.","In this work, we directly fit three simple scalar field models to the DESI BAO data, combined with cosmic microwave background anisotropy measurements and supernova data sets.","Quantifying the preference for scalar field dynamics exhibited by the data, we find that $2-4\\%$ of kinetic scalar field energy $\\Omega_{\\rm scf,k}$, is preferred over $\\Lambda$CDM at the $95\\%$ confidence level, for a canonical scalar field with a quadratic or linear potential.","Fitting to the supernova data sets Pantheon, Pantheon+, DES-Y5, and Union3, we show that the mild tension ($n_{\\sigma}< 3.4 $) under $\\Lambda$CDM emerges from a BAO preference for smaller values of fractional mass-energy density $\\Omega_m < 0.29$, while all supernova data sets, except for Pantheon, prefer larger values, $\\Omega_m > 0.3$.","The tension under $\\Lambda$CDM remains noticeable ($n_{\\sigma} <2.8$), when replacing two of the DESI BAO measurements redshift bins with effective redshifts $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.706$ with comparable BOSS DR 12 BAO measurements at $z_{\\text{eff}} =0.51$, and $z_{\\text{eff}}= 0.61$. Canonical scalar fields as dark energy are successful in mitigating that tension."],"url":"http://arxiv.org/abs/2404.14341v1","category":"astro-ph.CO"}
{"created":"2024-04-22 16:56:43","title":"Zero-shot Cross-lingual Stance Detection via Adversarial Language Adaptation","abstract":"Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian. We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub.","sentences":["Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines.","Research in stance detection has however often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model.","This paper makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, Multilingual Translation-Augmented BERT (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages.","Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy.","Through experiments on datasets labeled for stance towards vaccines in four languages English, German, French, Italian.","We demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model.","Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model.","We have made our source code accessible on GitHub."],"url":"http://arxiv.org/abs/2404.14339v1","category":"cs.CL"}
{"created":"2024-04-22 15:44:08","title":"Laser-synthesized TiN nanoparticles as novel efficient sorbent for environmental water cleaning","abstract":"Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems. Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection. In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect. TiN nanoparticles were synthesized by laser ablation method in liquid. Water, acetone and acetonitrile are used as solvent. Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x). TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles. Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states. The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g.","sentences":["Dyes used in industries such as textile, paper, and leather are known to be harmful to both human health and aquatic ecosystems.","Therefore, finding effective and sustainable methods to remove dyes from wastewater is crucial for mitigating the detrimental effects of pollution.","TiN nanoparticles have good absorption and conversion of light energy into thermal energy in the visible range of the spectrum, which makes them promising in various applications, from biomedical to environmental protection.","In this work, it is shown that titanium nitride nanoparticles also possess promising adsorption capabilitieseffect.","TiN nanoparticles were synthesized by laser ablation method in liquid.","Water, acetone and acetonitrile are used as solvent.","Nanoparticles were characterized by scanning and transmission microscopy, Raman spectroscopy, which showed the formation of the under-stoichiometric titanium nitride (TiN1-x).","TiN nanoparticles are investigated as a promising object for high adsorption It is shown that adsorption of TiN nanoparticles is associated with the electrostatic effect and the presence of pores in the synthesized nanoparticles.","Optimal dye absorption capabilities were found to be associated with a low amount of Ti vacancies and high amount of N vacancies acting as donor states.","The particles synthesized in water have the highest sorption capacity of dye achieving the value of 136.5 mg/g."],"url":"http://arxiv.org/abs/2404.14289v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 15:32:42","title":"Cascade Radiations of $e^\\pm$ from $\u03b3\u03b3$-annihilation process as an extra component of the Early Optical/X-Ray Afterglows of Gamma-Ray Bursts","abstract":"Chromatic break and/or plateau observed in the early optical and X-ray afterglow lightcurves challenge the conventional external shock models of gamma-ray bursts (GRBs). Detection of TeV gamma-ray afterglows indicates strong gamma-ray production within the afterglow jets. We investigate the cascade radiations of the $e^\\pm$ production via the $\\gamma\\gamma$ interaction in the jets. Our numerical calculations show that the cascade synchrotron emission can make a significant contribution to the early optical/X-ray afterglows. The combination of the primary and cascade emission fluxes can shape a chromatic break and/or plateau in the early optical/X-ray lightcurves, depending on the jet properties. Applying our model to GRBs 050801 and 080310, we found that their optical plateaus and the late X-ray/optical lightcurves can be explained with our model in reasonable parameter values. We suggest that such a chromatic optical plateau could be a signature of strong $e^\\pm$ production in GRB afterglow jets. The TeV gamma-ray flux of such kind GRBs should be significantly reduced, hence tends to be detectable for those GRBs that have a single power-law decaying optical afterglow lightcurve.","sentences":["Chromatic break and/or plateau observed in the early optical and X-ray afterglow lightcurves challenge the conventional external shock models of gamma-ray bursts (GRBs).","Detection of TeV gamma-ray afterglows indicates strong gamma-ray production within the afterglow jets.","We investigate the cascade radiations of the $e^\\pm$ production via the $\\gamma\\gamma$ interaction in the jets.","Our numerical calculations show that the cascade synchrotron emission can make a significant contribution to the early optical/X-ray afterglows.","The combination of the primary and cascade emission fluxes can shape a chromatic break and/or plateau in the early optical/X-ray lightcurves, depending on the jet properties.","Applying our model to GRBs 050801 and 080310, we found that their optical plateaus and the late X-ray/optical lightcurves can be explained with our model in reasonable parameter values.","We suggest that such a chromatic optical plateau could be a signature of strong $e^\\pm$ production in GRB afterglow jets.","The TeV gamma-ray flux of such kind GRBs should be significantly reduced, hence tends to be detectable for those GRBs that have a single power-law decaying optical afterglow lightcurve."],"url":"http://arxiv.org/abs/2404.14283v1","category":"astro-ph.HE"}
{"created":"2024-04-22 15:15:06","title":"Hybrid Fusion for 802.11ax Wi-Fi-based Passive Radars Exploiting Beamforming Feedbacks","abstract":"Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of targets using Wi-Fi signals of opportunity transmitted by an access point. Unlike active radars that optimize their transmitted waveform for localization, PWRs align with the 802.11 amendments. Specifically, during the channel sounding session preceding a multi-user multiple-input multiple-output downlink transmission, an access point isotropically transmits a null data packet (NDP) with a known preamble. From these known symbols, client user equipments derive their channel state information and transmit an unencrypted beamforming feedback (BFF) back to the access point. The BFF comprises the right singular matrix of the channel and the corresponding stream gain for each subcarrier, which allows the computation of a beamforming matrix at the access point. In a classical PWR processing, only the preamble symbols from the NDP are exploited during the channel sounding session. In this study, we investigate multiple target localization by a PWR exploiting hybrid information sources. On one hand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP. On another hand, the line-of-sight angle-of-departures inferred from the BFFs. The processing steps at the PWR are defined and an optimal hybrid fusion rule is derived in the maximum likelihood framework. Monte-Carlo simulations assess the enhanced accuracy of the proposed combination method compared to classical PWR processing based solely on the NDP, and compare the localisation performance between client and non-client targets.","sentences":["Passive Wi-Fi-based radars (PWRs) are devices that enable the localization of targets using Wi-Fi signals of opportunity transmitted by an access point.","Unlike active radars that optimize their transmitted waveform for localization, PWRs align with the 802.11 amendments.","Specifically, during the channel sounding session preceding a multi-user multiple-input multiple-output downlink transmission, an access point isotropically transmits a null data packet (NDP) with a known preamble.","From these known symbols, client user equipments derive their channel state information and transmit an unencrypted beamforming feedback (BFF) back to the access point.","The BFF comprises the right singular matrix of the channel and the corresponding stream gain for each subcarrier, which allows the computation of a beamforming matrix at the access point.","In a classical PWR processing, only the preamble symbols from the NDP are exploited during the channel sounding session.","In this study, we investigate multiple target localization by a PWR exploiting hybrid information sources.","On one hand, the joint angle-of-departure and angle-of-arrival evaluated from the NDP.","On another hand, the line-of-sight angle-of-departures inferred from the BFFs.","The processing steps at the PWR are defined and an optimal hybrid fusion rule is derived in the maximum likelihood framework.","Monte-Carlo simulations assess the enhanced accuracy of the proposed combination method compared to classical PWR processing based solely on the NDP, and compare the localisation performance between client and non-client targets."],"url":"http://arxiv.org/abs/2404.14269v1","category":"eess.SP"}
{"created":"2024-04-22 15:08:56","title":"Investigation of [KSF2015] 1381-19L, a WC9-type star in the high extinction Galactic region","abstract":"We report a multi-wavelength study of the Wolf Rayet (WR) star: [KSF2015] 1381-19L, which is located in the solar metallicity region (Z=0.014) of the Milky Way Galaxy, strongly obscured by the interstellar dust. We perform a detailed characterization of the stellar atmosphere by fitting the spectral emission lines observed in the Optical and Near-InfraRed (NIR) bands, using CMFGEN. The best-fitted spectroscopic model indicates a highly luminous ($10^{5.89}L_{\\odot}$) star with a larger radius ($15\\,R_{\\odot}$) and effective temperature, wind terminal velocity, and chemical composition similar to that of Galactic WC9-dusty (WC9d)-type stars. The atmospheric ionization structure shows coexisting ionization states of different elements, simultaneously affecting the opacity and thermal electron balance. Fitting of the spectral energy data (SED) reveals high interstellar optical extinction ($A_{V}=$ 8.87) while the IR extinction is found to be comparatively lower ($A_{K_{s}}=$ 0.98). We do not detect any excess emission at near-IR wavelengths due to dust. Upon comparison of our results with the GENEVA single star evolutionary models (Z=0.014), we identify the best possible progenitors ( a rotating star of $67\\,M_{\\odot}$ and a non-rotating star of $90\\,M_{\\odot}$).","sentences":["We report a multi-wavelength study of the Wolf Rayet (WR) star:","[KSF2015] 1381-19L, which is located in the solar metallicity region (Z=0.014) of the Milky Way Galaxy, strongly obscured by the interstellar dust.","We perform a detailed characterization of the stellar atmosphere by fitting the spectral emission lines observed in the Optical and Near-InfraRed (NIR) bands, using CMFGEN.","The best-fitted spectroscopic model indicates a highly luminous ($10^{5.89}L_{\\odot}$) star with a larger radius ($15\\,R_{\\odot}$) and effective temperature, wind terminal velocity, and chemical composition similar to that of Galactic WC9-dusty (WC9d)-type stars.","The atmospheric ionization structure shows coexisting ionization states of different elements, simultaneously affecting the opacity and thermal electron balance.","Fitting of the spectral energy data (SED) reveals high interstellar optical extinction ($A_{V}=$ 8.87) while the IR extinction is found to be comparatively lower ($A_{K_{s}}=$ 0.98).","We do not detect any excess emission at near-IR wavelengths due to dust.","Upon comparison of our results with the GENEVA single star evolutionary models (Z=0.014), we identify the best possible progenitors ( a rotating star of $67\\,M_{\\odot}$ and a non-rotating star of $90\\,M_{\\odot}$)."],"url":"http://arxiv.org/abs/2404.14260v1","category":"astro-ph.SR"}
{"created":"2024-04-22 14:34:14","title":"Error Credits: Resourceful Reasoning about Error Bounds for Higher-Order Probabilistic Programs","abstract":"Probabilistic programs often trade accuracy for efficiency, and are thus only approximately correct. It is important to obtain precise error bounds for these approximations, but existing approaches rely on simplifications that make the error bounds excesively coarse, or only apply to first-order programs. In this paper we present Eris, a higher-order separation logic for probabilistic programs written in an expressive higher-order language.   Our key novelty is the introduction of error credits, a separation logic resource that tracks the error bound of a program. By representing error bounds as a resource, we recover the benefits of separation logic, including compositionality, modularity, and dependency between errors and program terms, allowing for more precise specifications. Moreover, we enable novel reasoning principles such as expectation-preserving error composition, amortized error reasoning, and proving almost-sure termination by induction on the error.   We illustrate the advantages of our approach by proving amortized error bounds on a range of examples, including collision probabilities in hash functions, which allows us to write more modular specifications for data structures that use them as clients. We also use our logic to prove correctness and almost-sure termination of rejection sampling algorithms. All of our results have been mechanized in the Coq proof assistant using the Iris separation logic framework and the Coquelicot real analysis library.","sentences":["Probabilistic programs often trade accuracy for efficiency, and are thus only approximately correct.","It is important to obtain precise error bounds for these approximations, but existing approaches rely on simplifications that make the error bounds excesively coarse, or only apply to first-order programs.","In this paper we present Eris, a higher-order separation logic for probabilistic programs written in an expressive higher-order language.   ","Our key novelty is the introduction of error credits, a separation logic resource that tracks the error bound of a program.","By representing error bounds as a resource, we recover the benefits of separation logic, including compositionality, modularity, and dependency between errors and program terms, allowing for more precise specifications.","Moreover, we enable novel reasoning principles such as expectation-preserving error composition, amortized error reasoning, and proving almost-sure termination by induction on the error.   ","We illustrate the advantages of our approach by proving amortized error bounds on a range of examples, including collision probabilities in hash functions, which allows us to write more modular specifications for data structures that use them as clients.","We also use our logic to prove correctness and almost-sure termination of rejection sampling algorithms.","All of our results have been mechanized in the Coq proof assistant using the Iris separation logic framework and the Coquelicot real analysis library."],"url":"http://arxiv.org/abs/2404.14223v1","category":"cs.LO"}
{"created":"2024-04-22 14:17:21","title":"ETROC1: The First Full Chain Precision Timing Prototype ASIC for CMS MTD Endcap Timing Layer Upgrade","abstract":"we present the design and characterization of the first full chain precision timing prototype ASIC, named ETL Readout Chip version 1 (ETROC1) for the CMS MTD endcap timing layer (ETL) upgrade. The ETL utilizes Low Gain Avalanche Diode (LGAD) sensors to detect charged particles, with the goal to achieve a time resolution of 40 - 50 ps per hit, and 30 - 40 ps per track with hits from two detector layers. The ETROC1 is composed of a 5 x 5 pixel array and peripheral circuits. The pixel array includes a 4 x 4 active pixel array with an H-tree shaped network delivering clock and charge injection signals. Each active pixel is composed of various components, including a bump pad, a charge injection circuit, a pre-amplifier, a discriminator, a digital-to-analog converter, and a time-to-digital converter. These components play essential roles as the front-end link in processing LGAD signals and measuring timing-related information. The peripheral circuits provide clock signals and readout functionalities. The size of the ETROC1 chip is 7 mm x 9 mm. ETROC1 has been fabricated in a 65 nm CMOS process, and extensively tested under stimuli of charge injection, infrared laser, and proton beam. The time resolution of bump-bonded ETROC1 + LGAD chipsets reaches 42 - 46 ps per hit in the beam test.","sentences":["we present the design and characterization of the first full chain precision timing prototype ASIC, named ETL Readout Chip version 1 (ETROC1) for the CMS MTD endcap timing layer (ETL) upgrade.","The ETL utilizes Low Gain Avalanche Diode (LGAD) sensors to detect charged particles, with the goal to achieve a time resolution of 40 - 50 ps per hit, and 30 - 40 ps per track with hits from two detector layers.","The ETROC1 is composed of a 5 x 5 pixel array and peripheral circuits.","The pixel array includes a 4 x 4 active pixel array with an H-tree shaped network delivering clock and charge injection signals.","Each active pixel is composed of various components, including a bump pad, a charge injection circuit, a pre-amplifier, a discriminator, a digital-to-analog converter, and a time-to-digital converter.","These components play essential roles as the front-end link in processing LGAD signals and measuring timing-related information.","The peripheral circuits provide clock signals and readout functionalities.","The size of the ETROC1 chip is 7 mm x 9 mm.","ETROC1 has been fabricated in a 65 nm CMOS process, and extensively tested under stimuli of charge injection, infrared laser, and proton beam.","The time resolution of bump-bonded ETROC1 + LGAD chipsets reaches 42 - 46 ps per hit in the beam test."],"url":"http://arxiv.org/abs/2404.14207v1","category":"physics.ins-det"}
{"created":"2024-04-22 13:56:05","title":"Record high superconducting transition temperature in Ti$_{1-x}$Mn$_x$ alloy with rich magnetic element Mn","abstract":"It is well-known that magnetic moments are very harmful to superconductivity. A typical example is the element Mn whose compounds usually exhibit strong magnetism. Thus, it is very hard to achieve superconductivity in materials containing Mn. Here, we report enhanced superconductivity with the superconducting transition temperature ($T_\\text{c}$) up to a record high-value of about 26 K in a beta-phase Ti$_{1-x}$Mn$_x$ alloy containing rich magnetic element Mn under high pressures. This is contrary to the intuition that the magnetic moments always suppress superconductivity. Under high pressures, we also found that in the middle-pressure regime, the Pauli limit of the upper critical field is surpassed. The synchrotron X-ray diffraction data shows an unchanged beta-phase with a continuous contraction of the cell volume, which is well supported by the first-principles calculations. Although the theoretical results based on electron-phonon coupling (EPC) can interpret the $T_\\text{c}$ value in a certain pressure region, the monotonic enhancement of superconductivity by pressure cannot seek support from the theory. Our results show a surprising enhancement of superconductivity in Ti$_{1-x}$Mn$_x$ alloy with a considerable Mn content.","sentences":["It is well-known that magnetic moments are very harmful to superconductivity.","A typical example is the element Mn whose compounds usually exhibit strong magnetism.","Thus, it is very hard to achieve superconductivity in materials containing Mn.","Here, we report enhanced superconductivity with the superconducting transition temperature ($T_\\text{c}$) up to a record high-value of about 26 K in a beta-phase Ti$_{1-x}$Mn$_x$ alloy containing rich magnetic element Mn under high pressures.","This is contrary to the intuition that the magnetic moments always suppress superconductivity.","Under high pressures, we also found that in the middle-pressure regime, the Pauli limit of the upper critical field is surpassed.","The synchrotron X-ray diffraction data shows an unchanged beta-phase with a continuous contraction of the cell volume, which is well supported by the first-principles calculations.","Although the theoretical results based on electron-phonon coupling (EPC) can interpret the $T_\\text{c}$ value in a certain pressure region, the monotonic enhancement of superconductivity by pressure cannot seek support from the theory.","Our results show a surprising enhancement of superconductivity in Ti$_{1-x}$Mn$_x$ alloy with a considerable Mn content."],"url":"http://arxiv.org/abs/2404.14182v1","category":"cond-mat.supr-con"}
{"created":"2024-04-22 13:54:17","title":"Metric Distortion under Group-Fair Objectives","abstract":"We consider a voting problem in which a set of agents have metric preferences over a set of alternatives, and are also partitioned into disjoint groups. Given information about the preferences of the agents and their groups, our goal is to decide an alternative to approximately minimize an objective function that takes the groups of agents into account. We consider two natural group-fair objectives known as Max-of-Avg and Avg-of-Max which are different combinations of the max and the average cost in and out of the groups. We show tight bounds on the best possible distortion that can be achieved by various classes of mechanisms depending on the amount of information they have access to. In particular, we consider group-oblivious full-information mechanisms that do not know the groups but have access to the exact distances between agents and alternatives in the metric space, group-oblivious ordinal-information mechanisms that again do not know the groups but are given the ordinal preferences of the agents, and group-aware mechanisms that have full knowledge of the structure of the agent groups and also ordinal information about the metric space.","sentences":["We consider a voting problem in which a set of agents have metric preferences over a set of alternatives, and are also partitioned into disjoint groups.","Given information about the preferences of the agents and their groups, our goal is to decide an alternative to approximately minimize an objective function that takes the groups of agents into account.","We consider two natural group-fair objectives known as Max-of-Avg and Avg-of-Max which are different combinations of the max and the average cost in and out of the groups.","We show tight bounds on the best possible distortion that can be achieved by various classes of mechanisms depending on the amount of information they have access to.","In particular, we consider group-oblivious full-information mechanisms that do not know the groups but have access to the exact distances between agents and alternatives in the metric space, group-oblivious ordinal-information mechanisms that again do not know the groups but are given the ordinal preferences of the agents, and group-aware mechanisms that have full knowledge of the structure of the agent groups and also ordinal information about the metric space."],"url":"http://arxiv.org/abs/2404.14180v1","category":"cs.GT"}
{"created":"2024-04-22 13:40:07","title":"Edge-selective reconfiguration in polarized lattices with magnet-enabled bistability","abstract":"The signature topological feature of Maxwell lattices is their polarization, which manifests as an unbalance in stiffness between opposite edges of a finite domain. The manifestation of this asymmetry is especially dramatic in the case of soft lattices undergoing large nonlinear deformation under concentrated loads, where the excess of softness at the soft edge can result in the activation of sharp indentations. This study explores how this mechanical dichotomy between edges can be tuned and possibly extremized by working with soft magneto-mechanical metamaterials. The magneto-mechanical coupling is obtained by endowing the lattice sites with permanent magnets, which activate a network of magnetic forces that can interact with (either augmenting or competing with) the elasticity of the material. Specifically, under sufficiently large deformation that macroscopically alters the equilibrium positions of the sites, the attractive forces between the magnets can trigger bistable reconfiguration mechanisms. The strength of such mechanisms depends on the landscapes of elastic reaction forces exhibited by the edges, which are different due to the polarization, and is therefore inherently edge-selective. We show that, on the soft edge, the addition of magnets simply enhances the softness of the edge. In contrast, on the stiff edge, the magnets activate snapping mechanisms that locally reconfigure the cells and produce a lattice response reminiscent of plasticity, characterized by residual deformation that persists upon unloading.","sentences":["The signature topological feature of Maxwell lattices is their polarization, which manifests as an unbalance in stiffness between opposite edges of a finite domain.","The manifestation of this asymmetry is especially dramatic in the case of soft lattices undergoing large nonlinear deformation under concentrated loads, where the excess of softness at the soft edge can result in the activation of sharp indentations.","This study explores how this mechanical dichotomy between edges can be tuned and possibly extremized by working with soft magneto-mechanical metamaterials.","The magneto-mechanical coupling is obtained by endowing the lattice sites with permanent magnets, which activate a network of magnetic forces that can interact with (either augmenting or competing with) the elasticity of the material.","Specifically, under sufficiently large deformation that macroscopically alters the equilibrium positions of the sites, the attractive forces between the magnets can trigger bistable reconfiguration mechanisms.","The strength of such mechanisms depends on the landscapes of elastic reaction forces exhibited by the edges, which are different due to the polarization, and is therefore inherently edge-selective.","We show that, on the soft edge, the addition of magnets simply enhances the softness of the edge.","In contrast, on the stiff edge, the magnets activate snapping mechanisms that locally reconfigure the cells and produce a lattice response reminiscent of plasticity, characterized by residual deformation that persists upon unloading."],"url":"http://arxiv.org/abs/2404.14171v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 13:06:27","title":"Spin-dependent photodynamics of boron-vacancy centers in hexagonal boron nitride","abstract":"The negatively-charged boron vacancy (V$_\\text{B}^-$) center in hexagonal boron nitride (hBN) is currently garnering considerable attention for the design of two-dimensional (2D) quantum sensing units. Such developments require a precise understanding of the spin-dependent optical response of V$_\\text{B}^-$ centers, which still remains poorly documented despite its key role for sensing applications. Here we investigate the spin-dependent photodynamics of V$_\\text{B}^-$ centers in hBN by a series of time-resolved photoluminescence (PL) measurements. We first introduce a robust all-optical method to infer the spin-dependent lifetime of the excited states and the electron spin polarization of V$_\\text{B}^-$ centers under optical pumping. Using these results, we then analyze PL time traces recorded at different optical excitation powers with a seven-level model of the V$_\\text{B}^-$ center and we extract all the rates involved in the spin-dependent optical cycles, both under ambient conditions and at liquid helium temperature. These findings are finally used to study the impact of a vector magnetic field on the optical response. More precisely, we analyze PL quenching effects resulting from electron spin mixing induced by the magnetic field component perpendicular to the V$_\\text{B}^-$ quantization axis. All experimental results are well reproduced by the seven-level model, illustrating its robustness to describe the spin-dependent photodymanics of V$_\\text{B}^-$ centers. This work provides important insights into the properties of V$_\\text{B}^-$ centers in hBN, which are valuable for future developments of 2D quantum sensing units.","sentences":["The negatively-charged boron vacancy (V$_\\text{B}^-$) center in hexagonal boron nitride (hBN) is currently garnering considerable attention for the design of two-dimensional (2D) quantum sensing units.","Such developments require a precise understanding of the spin-dependent optical response of V$_\\text{B}^-$ centers, which still remains poorly documented despite its key role for sensing applications.","Here we investigate the spin-dependent photodynamics of V$_\\text{B}^-$ centers in hBN by a series of time-resolved photoluminescence (PL) measurements.","We first introduce a robust all-optical method to infer the spin-dependent lifetime of the excited states and the electron spin polarization of V$_\\text{B}^-$ centers under optical pumping.","Using these results, we then analyze PL time traces recorded at different optical excitation powers with a seven-level model of the V$_\\text{B}^-$ center and we extract all the rates involved in the spin-dependent optical cycles, both under ambient conditions and at liquid helium temperature.","These findings are finally used to study the impact of a vector magnetic field on the optical response.","More precisely, we analyze PL quenching effects resulting from electron spin mixing induced by the magnetic field component perpendicular to the V$_\\text{B}^-$ quantization axis.","All experimental results are well reproduced by the seven-level model, illustrating its robustness to describe the spin-dependent photodymanics of V$_\\text{B}^-$ centers.","This work provides important insights into the properties of V$_\\text{B}^-$ centers in hBN, which are valuable for future developments of 2D quantum sensing units."],"url":"http://arxiv.org/abs/2404.14155v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 13:01:52","title":"Complete $CP$ Eigen-bases of Mesonic Chiral Lagrangian up to $p^8$-order","abstract":"Chiral perturbation theory systematically describes the low energy dynamics of meson and baryons using nonlinear Nambu-Goldstone fields. Using the Young tensor technique, we construct the pure mesonic effective operators up to $p^8$-order, one-to-one corresponding to contact amplitudes with the on-shell Adler zero condition. The off-shell external sources, non-vanishing under equation-of-motion conditions, are also added to the operator bases. We also show the invariant tensor bases using the Young tableau is equivalent to the trace bases with Cayley-Hamilton relations. Separated into different $CP$ eigenstates, at $\\mathcal{O}(p^8)$ we obtain the operator lists of the 567 $C$+$P$+ operators, 483 $C$+$P$- operators, 376 $C$-$P$+ operators, and 408 $C$-$P$- operators for $SU(2)$ case, while there are 1959 $C$+$P$+ operators, 1809 $C$+$P$- operators, 1520 $C$-$P$+ operators, and 1594 $C$-$P$- operators for $SU(3)$ case, consistent with results using the Hilbert series.","sentences":["Chiral perturbation theory systematically describes the low energy dynamics of meson and baryons using nonlinear Nambu-Goldstone fields.","Using the Young tensor technique, we construct the pure mesonic effective operators up to $p^8$-order, one-to-one corresponding to contact amplitudes with the on-shell Adler zero condition.","The off-shell external sources, non-vanishing under equation-of-motion conditions, are also added to the operator bases.","We also show the invariant tensor bases using the Young tableau is equivalent to the trace bases with Cayley-Hamilton relations.","Separated into different $CP$ eigenstates, at $\\mathcal{O}(p^8)$ we obtain the operator lists of the 567 $C$+$P$+ operators, 483 $C$+$P$- operators, 376 $C$-$P$+ operators, and 408 $C$-$P$- operators for $SU(2)$ case, while there are 1959 $C$+$P$+ operators, 1809 $C$+$P$- operators, 1520 $C$-$P$+ operators, and 1594 $C$-$P$- operators for $SU(3)$ case, consistent with results using the Hilbert series."],"url":"http://arxiv.org/abs/2404.14152v1","category":"hep-ph"}
{"created":"2024-04-22 12:45:04","title":"Human Orientation Estimation under Partial Observation","abstract":"Reliable human orientation estimation (HOE) is critical for autonomous agents to understand human intention and perform human-robot interaction (HRI) tasks. Great progress has been made in HOE under full observation. However, the existing methods easily make a wrong prediction under partial observation and give it an unexpectedly high probability. To solve the above problems, this study first develops a method that estimates orientation from the visible joints of a target person so that it is able to handle partial observation. Subsequently, we introduce a confidence-aware orientation estimation method, enabling more accurate orientation estimation and reasonable confidence estimation under partial observation. The effectiveness of our method is validated on both public and custom-built datasets, and it showed great accuracy and reliability improvement in partial observation scenarios. In particular, we show in real experiments that our method can benefit the robustness and consistency of the robot person following (RPF) task.","sentences":["Reliable human orientation estimation (HOE) is critical for autonomous agents to understand human intention and perform human-robot interaction (HRI) tasks.","Great progress has been made in HOE under full observation.","However, the existing methods easily make a wrong prediction under partial observation and give it an unexpectedly high probability.","To solve the above problems, this study first develops a method that estimates orientation from the visible joints of a target person so that it is able to handle partial observation.","Subsequently, we introduce a confidence-aware orientation estimation method, enabling more accurate orientation estimation and reasonable confidence estimation under partial observation.","The effectiveness of our method is validated on both public and custom-built datasets, and it showed great accuracy and reliability improvement in partial observation scenarios.","In particular, we show in real experiments that our method can benefit the robustness and consistency of the robot person following (RPF) task."],"url":"http://arxiv.org/abs/2404.14139v1","category":"cs.RO"}
{"created":"2024-04-22 12:40:38","title":"Offensive AI: Enhancing Directory Brute-forcing Attack with the Use of Language Models","abstract":"Web Vulnerability Assessment and Penetration Testing (Web VAPT) is a comprehensive cybersecurity process that uncovers a range of vulnerabilities which, if exploited, could compromise the integrity of web applications. In a VAPT, it is common to perform a \\textit{Directory brute-forcing Attack}, aiming at the identification of accessible directories of a target website. Current commercial solutions are inefficient as they are based on brute-forcing strategies that use wordlists, resulting in enormous quantities of trials for a small amount of success. Offensive AI is a recent paradigm that integrates AI-based technologies in cyber attacks. In this work, we explore whether AI can enhance the directory enumeration process and propose a novel Language Model-based framework. Our experiments -- conducted in a testbed consisting of 1 million URLs from different web application domains (universities, hospitals, government, companies) -- demonstrate the superiority of the LM-based attack, with an average performance increase of 969%.","sentences":["Web Vulnerability Assessment and Penetration Testing (Web VAPT) is a comprehensive cybersecurity process that uncovers a range of vulnerabilities which, if exploited, could compromise the integrity of web applications.","In a VAPT, it is common to perform a \\textit{Directory brute-forcing Attack}, aiming at the identification of accessible directories of a target website.","Current commercial solutions are inefficient as they are based on brute-forcing strategies that use wordlists, resulting in enormous quantities of trials for a small amount of success.","Offensive AI is a recent paradigm that integrates AI-based technologies in cyber attacks.","In this work, we explore whether AI can enhance the directory enumeration process and propose a novel Language Model-based framework.","Our experiments -- conducted in a testbed consisting of 1 million URLs from different web application domains (universities, hospitals, government, companies) -- demonstrate the superiority of the LM-based attack, with an average performance increase of 969%."],"url":"http://arxiv.org/abs/2404.14138v1","category":"cs.CR"}
{"created":"2024-04-22 12:39:12","title":"Text in the Dark: Extremely Low-Light Text Image Enhancement","abstract":"Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging. One solution is to enhance these images using low-light image enhancement methods before text extraction. However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks. Further research is also hindered by the lack of extremely low-light text datasets. To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement. Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction. Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15). We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks. Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets. Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark.","sentences":["Extremely low-light text images are common in natural scenes, making scene text detection and recognition challenging.","One solution is to enhance these images using low-light image enhancement methods before text extraction.","However, previous methods often do not try to particularly address the significance of low-level features, which are crucial for optimal performance on downstream scene text tasks.","Further research is also hindered by the lack of extremely low-light text datasets.","To address these limitations, we propose a novel encoder-decoder framework with an edge-aware attention module to focus on scene text regions during enhancement.","Our proposed method uses novel text detection and edge reconstruction losses to emphasize low-level scene text features, leading to successful text extraction.","Additionally, we present a Supervised Deep Curve Estimation (Supervised-DCE) model to synthesize extremely low-light images based on publicly available scene text datasets such as ICDAR15 (IC15).","We also labeled texts in the extremely low-light See In the Dark (SID) and ordinary LOw-Light (LOL) datasets to allow for objective assessment of extremely low-light image enhancement through scene text tasks.","Extensive experiments show that our model outperforms state-of-the-art methods in terms of both image quality and scene text metrics on the widely-used LOL, SID, and synthetic IC15 datasets.","Code and dataset will be released publicly at https://github.com/chunchet-ng/Text-in-the-Dark."],"url":"http://arxiv.org/abs/2404.14135v1","category":"cs.CV"}
{"created":"2024-04-22 12:21:12","title":"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?","abstract":"Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality. In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of all these factors. We find that LLMs display strong translation capability after being fine-tuned on as few as 32 training instances, and that fine-tuning on a single translation direction effectively enables LLMs to translate in multiple directions. However, the choice of direction is critical: fine-tuning LLMs with English on the target side can lead to task misinterpretation, which hinders translations into non-English languages. A similar problem arises when noise is introduced into the target side of parallel data, especially when the target language is well-represented in the LLM's pre-training. In contrast, noise in an under-represented language has a less pronounced effect. Our findings suggest that attaining successful alignment hinges on teaching the model to maintain a \"superficial\" focus, thereby avoiding the learning of erroneous biases beyond translation.","sentences":["Traditionally, success in multilingual machine translation can be attributed to three key factors in training data: large volume, diverse translation directions, and high quality.","In the current practice of fine-tuning large language models (LLMs) for translation, we revisit the importance of all these factors.","We find that LLMs display strong translation capability after being fine-tuned on as few as 32 training instances, and that fine-tuning on a single translation direction effectively enables LLMs to translate in multiple directions.","However, the choice of direction is critical: fine-tuning LLMs with English on the target side can lead to task misinterpretation, which hinders translations into non-English languages.","A similar problem arises when noise is introduced into the target side of parallel data, especially when the target language is well-represented in the LLM's pre-training.","In contrast, noise in an under-represented language has a less pronounced effect.","Our findings suggest that attaining successful alignment hinges on teaching the model to maintain a \"superficial\" focus, thereby avoiding the learning of erroneous biases beyond translation."],"url":"http://arxiv.org/abs/2404.14122v1","category":"cs.CL"}
{"created":"2024-04-22 11:37:35","title":"DynaMMo: Dynamic Model Merging for Efficient Class Incremental Learning for Medical Images","abstract":"Continual learning, the ability to acquire knowledge from new data while retaining previously learned information, is a fundamental challenge in machine learning. Various approaches, including memory replay, knowledge distillation, model regularization, and dynamic network expansion, have been proposed to address this issue. Thus far, dynamic network expansion methods have achieved state-of-the-art performance at the cost of incurring significant computational overhead. This is due to the need for additional model buffers, which makes it less feasible in resource-constrained settings, particularly in the medical domain. To overcome this challenge, we propose Dynamic Model Merging, DynaMMo, a method that merges multiple networks at different stages of model training to achieve better computational efficiency. Specifically, we employ lightweight learnable modules for each task and combine them into a unified model to minimize computational overhead. DynaMMo achieves this without compromising performance, offering a cost-effective solution for continual learning in medical applications. We evaluate DynaMMo on three publicly available datasets, demonstrating its effectiveness compared to existing approaches. DynaMMo offers around 10-fold reduction in GFLOPS with a small drop of 2.76 in average accuracy when compared to state-of-the-art dynamic-based approaches. The code implementation of this work will be available upon the acceptance of this work at https://github.com/BioMedIA-MBZUAI/DynaMMo.","sentences":["Continual learning, the ability to acquire knowledge from new data while retaining previously learned information, is a fundamental challenge in machine learning.","Various approaches, including memory replay, knowledge distillation, model regularization, and dynamic network expansion, have been proposed to address this issue.","Thus far, dynamic network expansion methods have achieved state-of-the-art performance at the cost of incurring significant computational overhead.","This is due to the need for additional model buffers, which makes it less feasible in resource-constrained settings, particularly in the medical domain.","To overcome this challenge, we propose Dynamic Model Merging, DynaMMo, a method that merges multiple networks at different stages of model training to achieve better computational efficiency.","Specifically, we employ lightweight learnable modules for each task and combine them into a unified model to minimize computational overhead.","DynaMMo achieves this without compromising performance, offering a cost-effective solution for continual learning in medical applications.","We evaluate DynaMMo on three publicly available datasets, demonstrating its effectiveness compared to existing approaches.","DynaMMo offers around 10-fold reduction in GFLOPS with a small drop of 2.76 in average accuracy when compared to state-of-the-art dynamic-based approaches.","The code implementation of this work will be available upon the acceptance of this work at https://github.com/BioMedIA-MBZUAI/DynaMMo."],"url":"http://arxiv.org/abs/2404.14099v1","category":"cs.CV"}
{"created":"2024-04-22 11:12:36","title":"Imaging through scattering media by exploiting the optical memory effect: a tutorial","abstract":"Scattering, especially multiple scattering, is a well known problem in imaging, ranging from astronomy to medicine. In particular it is often desirable to be able to perform non-invasive imaging through turbid and/or opaque media. Many different approaches have been proposed and tested through the years, each with their own advantages, disadvantages, and specific situations in which they work. In this tutorial we will show how knowledge of the correlations arising from the multiple scattering of light allows for non-invasive imaging through a strongly scattering layer, with particular attention on the practicalities of how to make such an experiment work.","sentences":["Scattering, especially multiple scattering, is a well known problem in imaging, ranging from astronomy to medicine.","In particular it is often desirable to be able to perform non-invasive imaging through turbid and/or opaque media.","Many different approaches have been proposed and tested through the years, each with their own advantages, disadvantages, and specific situations in which they work.","In this tutorial we will show how knowledge of the correlations arising from the multiple scattering of light allows for non-invasive imaging through a strongly scattering layer, with particular attention on the practicalities of how to make such an experiment work."],"url":"http://arxiv.org/abs/2404.14088v1","category":"physics.optics"}
{"created":"2024-04-22 11:12:00","title":"A Tight Subexponential-time Algorithm for Two-Page Book Embedding","abstract":"A book embedding of a graph is a drawing that maps vertices onto a line and edges to simple pairwise non-crossing curves drawn into pages, which are half-planes bounded by that line. Two-page book embeddings, i.e., book embeddings into 2 pages, are of special importance as they are both NP-hard to compute and have specific applications. We obtain a 2^(O(\\sqrt{n})) algorithm for computing a book embedding of an n-vertex graph on two pages -- a result which is asymptotically tight under the Exponential Time Hypothesis. As a key tool in our approach, we obtain a single-exponential fixed-parameter algorithm for the same problem when parameterized by the treewidth of the input graph. We conclude by establishing the fixed-parameter tractability of computing minimum-page book embeddings when parameterized by the feedback edge number, settling an open question arising from previous work on the problem.","sentences":["A book embedding of a graph is a drawing that maps vertices onto a line and edges to simple pairwise non-crossing curves drawn into pages, which are half-planes bounded by that line.","Two-page book embeddings, i.e., book embeddings into 2 pages, are of special importance as they are both NP-hard to compute and have specific applications.","We obtain a 2^(O(\\sqrt{n}))","algorithm for computing a book embedding of an n-vertex graph on two pages -- a result which is asymptotically tight under the Exponential Time Hypothesis.","As a key tool in our approach, we obtain a single-exponential fixed-parameter algorithm for the same problem when parameterized by the treewidth of the input graph.","We conclude by establishing the fixed-parameter tractability of computing minimum-page book embeddings when parameterized by the feedback edge number, settling an open question arising from previous work on the problem."],"url":"http://arxiv.org/abs/2404.14087v1","category":"cs.DS"}
{"created":"2024-04-22 11:07:02","title":"High efficient sunlight-driven CO2 hydrogenation to methanol over NiZn intermetallic catalysts under atmospheric pressure","abstract":"The synthesis of solar methanol through direct CO2 hydrogenation using solar energy is of great importance in advancing a sustainable energy economy. In this study, non-precious NiZn intermetallic/ZnO catalyst is reported to catalyze the hydrogenation of CO2 to methanol using sunlight irradiation (1sun). The NiZn-ZnO interface is identified as the active site to stabilize the key intermediates of HxCO*. At ambient pressure, the NiZn-ZnO catalyst demonstrates a methanol production rate of 127.5 umol g-1h-1 from solar driven CO2 hydrogenation, with a remarkable 100% selectivity towards methanol in the total organic products. Notably, this production rate stands as the highest record for photothermic CO2 hydrogenation to methanol in continuous-flow reactors with sunlight as the only requisite energy input. This discovery not only paves the way for the development of novel catalysts for CO2 hydrogenation to methanol but also marks a significant stride towards a full solar-driven chemical energy storage.","sentences":["The synthesis of solar methanol through direct CO2 hydrogenation using solar energy is of great importance in advancing a sustainable energy economy.","In this study, non-precious NiZn intermetallic/ZnO catalyst is reported to catalyze the hydrogenation of CO2 to methanol using sunlight irradiation (1sun).","The NiZn-ZnO interface is identified as the active site to stabilize the key intermediates of HxCO*.","At ambient pressure, the NiZn-ZnO catalyst demonstrates a methanol production rate of 127.5 umol g-1h-1 from solar driven CO2 hydrogenation, with a remarkable 100% selectivity towards methanol in the total organic products.","Notably, this production rate stands as the highest record for photothermic CO2 hydrogenation to methanol in continuous-flow reactors with sunlight as the only requisite energy input.","This discovery not only paves the way for the development of novel catalysts for CO2 hydrogenation to methanol but also marks a significant stride towards a full solar-driven chemical energy storage."],"url":"http://arxiv.org/abs/2404.14085v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 10:03:03","title":"How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study","abstract":"Meta's LLaMA family has become one of the most powerful open-source Large Language Model (LLM) series. Notably, LLaMA3 models have recently been released and achieve impressive performance across various with super-large scale pre-training on over 15T tokens of data. Given the wide application of low-bit quantization for LLMs in resource-limited scenarios, we explore LLaMA3's capabilities when quantized to low bit-width. This exploration holds the potential to unveil new insights and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs, especially in addressing performance degradation problems that suffer in LLM compression. Specifically, we evaluate the 10 existing post-training quantization and LoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's low-bit quantization performance. Our experiment results indicate that LLaMA3 still suffers non-negligent degradation in these scenarios, especially in ultra-low bit-width. This highlights the significant performance gap under low bit-width that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, pushing the LLMs to lower bit-width with higher accuracy for being practical. Our project is released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized LLaMA3 models are released in https://huggingface.co/LLMQ.","sentences":["Meta's LLaMA family has become one of the most powerful open-source Large Language Model (LLM) series.","Notably, LLaMA3 models have recently been released and achieve impressive performance across various with super-large scale pre-training on over 15T tokens of data.","Given the wide application of low-bit quantization for LLMs in resource-limited scenarios, we explore LLaMA3's capabilities when quantized to low bit-width.","This exploration holds the potential to unveil new insights and challenges for low-bit quantization of LLaMA3 and other forthcoming LLMs, especially in addressing performance degradation problems that suffer in LLM compression.","Specifically, we evaluate the 10 existing post-training quantization and LoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's low-bit quantization performance.","Our experiment results indicate that LLaMA3 still suffers non-negligent degradation in these scenarios, especially in ultra-low bit-width.","This highlights the significant performance gap under low bit-width that needs to be bridged in future developments.","We expect that this empirical study will prove valuable in advancing future models, pushing the LLMs to lower bit-width with higher accuracy for being practical.","Our project is released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized LLaMA3 models are released in https://huggingface.co/LLMQ."],"url":"http://arxiv.org/abs/2404.14047v1","category":"cs.LG"}
{"created":"2024-04-22 09:50:12","title":"PointDifformer: Robust Point Cloud Registration With Neural Diffusion and Transformer","abstract":"Point cloud registration is a fundamental technique in 3-D computer vision with applications in graphics, autonomous driving, and robotics. However, registration tasks under challenging conditions, under which noise or perturbations are prevalent, can be difficult. We propose a robust point cloud registration approach that leverages graph neural partial differential equations (PDEs) and heat kernel signatures. Our method first uses graph neural PDE modules to extract high dimensional features from point clouds by aggregating information from the 3-D point neighborhood, thereby enhancing the robustness of the feature representations. Then, we incorporate heat kernel signatures into an attention mechanism to efficiently obtain corresponding keypoints. Finally, a singular value decomposition (SVD) module with learnable weights is used to predict the transformation between two point clouds. Empirical experiments on a 3-D point cloud dataset demonstrate that our approach not only achieves state-of-the-art performance for point cloud registration but also exhibits better robustness to additive noise or 3-D shape perturbations.","sentences":["Point cloud registration is a fundamental technique in 3-D computer vision with applications in graphics, autonomous driving, and robotics.","However, registration tasks under challenging conditions, under which noise or perturbations are prevalent, can be difficult.","We propose a robust point cloud registration approach that leverages graph neural partial differential equations (PDEs) and heat kernel signatures.","Our method first uses graph neural PDE modules to extract high dimensional features from point clouds by aggregating information from the 3-D point neighborhood, thereby enhancing the robustness of the feature representations.","Then, we incorporate heat kernel signatures into an attention mechanism to efficiently obtain corresponding keypoints.","Finally, a singular value decomposition (SVD) module with learnable weights is used to predict the transformation between two point clouds.","Empirical experiments on a 3-D point cloud dataset demonstrate that our approach not only achieves state-of-the-art performance for point cloud registration but also exhibits better robustness to additive noise or 3-D shape perturbations."],"url":"http://arxiv.org/abs/2404.14034v1","category":"cs.CV"}
{"created":"2024-04-22 09:29:14","title":"Ungeneralizable Examples","abstract":"The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy. Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios. In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce \\textbf{U}n\\textbf{G}eneralizable \\textbf{E}xamples (UGEs). UGEs exhibit learnability for authorized users while maintaining unlearnability for potential hackers. The protector defines the authorized network and optimizes UGEs to match the gradients of the original data and its ungeneralizable version, ensuring learnability. To prevent unauthorized learning, UGEs are trained by maximizing a designated distance loss in a common feature space. Additionally, to further safeguard the authorized side from potential attacks, we introduce additional undistillation optimization. Experimental results on multiple datasets and various networks demonstrate that the proposed UGEs framework preserves data usability while reducing training performance on hacker networks, even under different types of attacks.","sentences":["The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy.","Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios.","In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce \\textbf{U}n\\textbf{G}eneralizable \\textbf{E}xamples (UGEs).","UGEs exhibit learnability for authorized users while maintaining unlearnability for potential hackers.","The protector defines the authorized network and optimizes UGEs to match the gradients of the original data and its ungeneralizable version, ensuring learnability.","To prevent unauthorized learning, UGEs are trained by maximizing a designated distance loss in a common feature space.","Additionally, to further safeguard the authorized side from potential attacks, we introduce additional undistillation optimization.","Experimental results on multiple datasets and various networks demonstrate that the proposed UGEs framework preserves data usability while reducing training performance on hacker networks, even under different types of attacks."],"url":"http://arxiv.org/abs/2404.14016v1","category":"cs.LG"}
{"created":"2024-04-22 09:29:09","title":"Origin of the Apparent Electric-Field Dependence of Electrostrictive Coefficients","abstract":"Electrostrictive materials exhibit a strain that is proportional to the square of the induced polarization. In linear dielectrics where the permittivity is constant, this electromechanical strain is also proportional to the square of the electric field. However, under increasing amplitudes of the driving field, the electromechanical strain sometimes saturates; the electrostrictive coefficients therefore appear to depend on the amplitude of the electric field used to measure them. Here, we present a methodology showing that this apparent field dependence is a consequence of neglecting higher-order electromechanical phenomena. When these are taken into account, not only do the electrostrictive coefficients remain constant but the signs of the high-order coefficients enable the prediction of the saturation behavior from a single measurement. We illustrate this approach on both classical and non-classical (so-called ``giant'') electrostrictors.","sentences":["Electrostrictive materials exhibit a strain that is proportional to the square of the induced polarization.","In linear dielectrics where the permittivity is constant, this electromechanical strain is also proportional to the square of the electric field.","However, under increasing amplitudes of the driving field, the electromechanical strain sometimes saturates; the electrostrictive coefficients therefore appear to depend on the amplitude of the electric field used to measure them.","Here, we present a methodology showing that this apparent field dependence is a consequence of neglecting higher-order electromechanical phenomena.","When these are taken into account, not only do the electrostrictive coefficients remain constant but the signs of the high-order coefficients enable the prediction of the saturation behavior from a single measurement.","We illustrate this approach on both classical and non-classical (so-called ``giant'') electrostrictors."],"url":"http://arxiv.org/abs/2404.14015v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 09:18:33","title":"A Stochastic Rounding-Enabled Low-Precision Floating-Point MAC for DNN Training","abstract":"Training Deep Neural Networks (DNNs) can be computationally demanding, particularly when dealing with large models. Recent work has aimed to mitigate this computational challenge by introducing 8-bit floating-point (FP8) formats for multiplication. However, accumulations are still done in either half (16-bit) or single (32-bit) precision arithmetic. In this paper, we investigate lowering accumulator word length while maintaining the same model accuracy. We present a multiply-accumulate (MAC) unit with FP8 multiplier inputs and FP12 accumulations, which leverages an optimized stochastic rounding (SR) implementation to mitigate swamping errors that commonly arise during low precision accumulations. We investigate the hardware implications and accuracy impact associated with varying the number of random bits used for rounding operations. We additionally attempt to reduce MAC area and power by proposing a new scheme to support SR in floating-point MAC and by removing support for subnormal values. Our optimized eager SR unit significantly reduces delay and area when compared to a classic lazy SR design. Moreover, when compared to MACs utilizing single-or half-precision adders, our design showcases notable savings in all metrics. Furthermore, our approach consistently maintains near baseline accuracy across a diverse range of computer vision tasks, making it a promising alternative for low-precision DNN training.","sentences":["Training Deep Neural Networks (DNNs) can be computationally demanding, particularly when dealing with large models.","Recent work has aimed to mitigate this computational challenge by introducing 8-bit floating-point (FP8) formats for multiplication.","However, accumulations are still done in either half (16-bit) or single (32-bit) precision arithmetic.","In this paper, we investigate lowering accumulator word length while maintaining the same model accuracy.","We present a multiply-accumulate (MAC) unit with FP8 multiplier inputs and FP12 accumulations, which leverages an optimized stochastic rounding (SR) implementation to mitigate swamping errors that commonly arise during low precision accumulations.","We investigate the hardware implications and accuracy impact associated with varying the number of random bits used for rounding operations.","We additionally attempt to reduce MAC area and power by proposing a new scheme to support SR in floating-point MAC and by removing support for subnormal values.","Our optimized eager SR unit significantly reduces delay and area when compared to a classic lazy SR design.","Moreover, when compared to MACs utilizing single-or half-precision adders, our design showcases notable savings in all metrics.","Furthermore, our approach consistently maintains near baseline accuracy across a diverse range of computer vision tasks, making it a promising alternative for low-precision DNN training."],"url":"http://arxiv.org/abs/2404.14010v1","category":"cs.AR"}
{"created":"2024-04-22 09:00:21","title":"Finite element analysis of a spectral problem on curved meshes occurring in diffusion with high order boundary conditions","abstract":"In this work is considered a spectral problem, involving a second order term on the domain boundary: the Laplace-Beltrami operator. A variational formulation is presented, leading to a finite element discretization. For the Laplace-Beltrami operator to make sense on the boundary, the domain is smooth: consequently the computational domain (classically a polygonal domain) will not match the physical one. Thus, the physical domain is discretized using high order curved meshes so as to reduce the \\textit{geometric error}. The \\textit{lift operator}, which is aimed to transform a function defined on the mesh domain into a function defined on the physical one, is recalled. This \\textit{lift} is a key ingredient in estimating errors on eigenvalues and eigenfunctions. A bootstrap method is used to prove the error estimates, which are expressed both in terms of \\textit{finite element approximation error} and of \\textit{geometric error}, respectively associated to the finite element degree $k\\ge 1$ and to the mesh order~$r\\ge 1$. Numerical experiments are led on various smooth domains in 2D and 3D, which allow us to validate the presented theoretical results.","sentences":["In this work is considered a spectral problem, involving a second order term on the domain boundary: the Laplace-Beltrami operator.","A variational formulation is presented, leading to a finite element discretization.","For the Laplace-Beltrami operator to make sense on the boundary, the domain is smooth: consequently the computational domain (classically a polygonal domain) will not match the physical one.","Thus, the physical domain is discretized using high order curved meshes so as to reduce the \\textit{geometric error}.","The \\textit{lift operator}, which is aimed to transform a function defined on the mesh domain into a function defined on the physical one, is recalled.","This \\textit{lift} is a key ingredient in estimating errors on eigenvalues and eigenfunctions.","A bootstrap method is used to prove the error estimates, which are expressed both in terms of \\textit{finite element approximation error} and of \\textit{geometric error}, respectively associated to the finite element degree $k\\ge 1$ and to the mesh order~$r\\ge 1$. Numerical experiments are led on various smooth domains in 2D and 3D, which allow us to validate the presented theoretical results."],"url":"http://arxiv.org/abs/2404.13994v1","category":"math.NA"}
{"created":"2024-04-22 08:57:46","title":"QCore: Data-Efficient, On-Device Continual Calibration for Quantized Models -- Extended Version","abstract":"We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes. It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers. To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits. The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy. This one-time calibration works for deployments in static environments. However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions. The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices. The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive. We propose QCore to enable continual calibration on the edge. First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths. We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data. Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation. An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods.","sentences":["We are witnessing an increasing availability of streaming data that may contain valuable information on the underlying processes.","It is thus attractive to be able to deploy machine learning models on edge devices near sensors such that decisions can be made instantaneously, rather than first having to transmit incoming data to servers.","To enable deployment on edge devices with limited storage and computational capabilities, the full-precision parameters in standard models can be quantized to use fewer bits.","The resulting quantized models are then calibrated using back-propagation and full training data to ensure accuracy.","This one-time calibration works for deployments in static environments.","However, model deployment in dynamic edge environments call for continual calibration to adaptively adjust quantized models to fit new incoming data, which may have different distributions.","The first difficulty in enabling continual calibration on the edge is that the full training data may be too large and thus not always available on edge devices.","The second difficulty is that the use of back-propagation on the edge for repeated calibration is too expensive.","We propose QCore to enable continual calibration on the edge.","First, it compresses the full training data into a small subset to enable effective calibration of quantized models with different bit-widths.","We also propose means of updating the subset when new streaming data arrives to reflect changes in the environment, while not forgetting earlier training data.","Second, we propose a small bit-flipping network that works with the subset to update quantized model parameters, thus enabling efficient continual calibration without back-propagation.","An experimental study, conducted with real-world data in a continual learning setting, offers insight into the properties of QCore and shows that it is capable of outperforming strong baseline methods."],"url":"http://arxiv.org/abs/2404.13990v1","category":"cs.LG"}
{"created":"2024-04-22 08:42:12","title":"Asymptotic stability of solitons for near-cubic NLS equation with an internal mode","abstract":"We consider perturbations of the one-dimensional cubic Schr\\\"odinger equation, of the form $i \\, \\partial_t \\psi + \\partial_x^2 \\psi + |\\psi|^2 \\psi + g( |\\psi|^2 ) \\psi = 0$. Under hypotheses on the function $g$ that can be easily verified in some cases (such as $g(s) = s^\\sigma$ with $\\sigma >1$), we show that the linearized problem around a small solitary wave presents a unique internal mode. Moreover, under an additional hypothesis (the Fermi golden rule) that can also be verified in the case of powers $g(s) = s^\\sigma$, we prove the asymptotic stability of the solitary waves with small frequencies.","sentences":["We consider perturbations of the one-dimensional cubic Schr\\\"odinger equation, of the form $i \\, \\partial_t \\psi + \\partial_x^2 \\psi + |\\psi|^2 \\psi + g( |\\psi|^2 ) \\psi = 0$. Under hypotheses on the function $g$ that can be easily verified in some cases (such as $g(s) = s^\\sigma$ with $\\sigma >1$), we show that the linearized problem around a small solitary wave presents a unique internal mode.","Moreover, under an additional hypothesis (the Fermi golden rule) that can also be verified in the case of powers $g(s) = s^\\sigma$, we prove the asymptotic stability of the solitary waves with small frequencies."],"url":"http://arxiv.org/abs/2404.13980v1","category":"math.AP"}
{"created":"2024-04-22 08:19:31","title":"Importance of the semimetallic state for the quantum Hall effect in HfTe$_{5}$","abstract":"At ambient pressure, HfTe$_{5}$ is a material at the boundary between a weak and a strong topological phase, which can be tuned by changes in its crystalline structure or by the application of high magnetic fields. It exhibits a Lifshitz transition upon cooling, and three-dimensional (3D) quantum Hall effect (QHE) plateaus can be observed at low temperatures. Here, we have investigated the electrical transport properties of HfTe$_{5}$ under hydrostatic pressure up to 3 GPa. We find a pressure-induced crossover from a semimetallic phase at low pressures to an insulating phase at about 1.5 GPa. Our data suggest the presence of a pressure-induced Lifshitz transition at low temperatures within the insulating phase around 2 GPa. The quasi-3D QHE is confined to the low-pressure region in the semimetallic phase. This reveals the importance of the semimetallic groundstate for the emergence of the QHE in HfTe$_{5}$ and thus favors a scenario based on a low carrier density metal in the quantum limit for the observed signatures of the quasi-quantized 3D QHE.","sentences":["At ambient pressure, HfTe$_{5}$ is a material at the boundary between a weak and a strong topological phase, which can be tuned by changes in its crystalline structure or by the application of high magnetic fields.","It exhibits a Lifshitz transition upon cooling, and three-dimensional (3D) quantum","Hall effect (QHE) plateaus can be observed at low temperatures.","Here, we have investigated the electrical transport properties of HfTe$_{5}$ under hydrostatic pressure up to 3 GPa.","We find a pressure-induced crossover from a semimetallic phase at low pressures to an insulating phase at about 1.5 GPa.","Our data suggest the presence of a pressure-induced Lifshitz transition at low temperatures within the insulating phase around 2 GPa.","The quasi-3D QHE is confined to the low-pressure region in the semimetallic phase.","This reveals the importance of the semimetallic groundstate for the emergence of the QHE in HfTe$_{5}$ and thus favors a scenario based on a low carrier density metal in the quantum limit for the observed signatures of the quasi-quantized 3D QHE."],"url":"http://arxiv.org/abs/2404.13969v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 08:07:12","title":"A Geometric Perspective on Double Robustness by Semiparametric Theory and Information Geometry","abstract":"Double robustness (DR) is a widely-used property of estimators that provides protection against model misspecification and slow convergence of nuisance functions. While DR is a global property on the probability distribution manifold, it often coincides with influence curves, which only ensure orthogonality to nuisance directions locally. This apparent discrepancy raises fundamental questions about the theoretical underpinnings of DR.   In this short communication, we address two key questions: (1) Why do influence curves frequently imply DR \"for free\"? (2) Under what conditions do DR estimators exist for a given statistical model and parameterization? Using tools from semiparametric theory, we show that convexity is the crucial property that enables influence curves to imply DR. We then derive necessary and sufficient conditions for the existence of DR estimators under a mean squared differentiable path-connected parameterization.   Our main contribution also lies in the novel geometric interpretation of DR using information geometry. By leveraging concepts such as parallel transport, m-flatness, and m-curvature freeness, we characterize DR in terms of invariance along submanifolds. This geometric perspective deepens the understanding of when and why DR estimators exist.   The results not only resolve apparent mysteries surrounding DR but also have practical implications for the construction and analysis of DR estimators. The geometric insights open up new connections and directions for future research. Our findings aim to solidify the theoretical foundations of a fundamental concept and contribute to the broader understanding of robust estimation in statistics.","sentences":["Double robustness (DR) is a widely-used property of estimators that provides protection against model misspecification and slow convergence of nuisance functions.","While DR is a global property on the probability distribution manifold, it often coincides with influence curves, which only ensure orthogonality to nuisance directions locally.","This apparent discrepancy raises fundamental questions about the theoretical underpinnings of DR.   ","In this short communication, we address two key questions: (1) Why do influence curves frequently imply DR \"for free\"?","(2) Under what conditions do DR estimators exist for a given statistical model and parameterization?","Using tools from semiparametric theory, we show that convexity is the crucial property that enables influence curves to imply DR.","We then derive necessary and sufficient conditions for the existence of DR estimators under a mean squared differentiable path-connected parameterization.   ","Our main contribution also lies in the novel geometric interpretation of DR using information geometry.","By leveraging concepts such as parallel transport, m-flatness, and m-curvature freeness, we characterize DR in terms of invariance along submanifolds.","This geometric perspective deepens the understanding of when and why DR estimators exist.   ","The results not only resolve apparent mysteries surrounding DR but also have practical implications for the construction and analysis of DR estimators.","The geometric insights open up new connections and directions for future research.","Our findings aim to solidify the theoretical foundations of a fundamental concept and contribute to the broader understanding of robust estimation in statistics."],"url":"http://arxiv.org/abs/2404.13960v1","category":"math.ST"}
{"created":"2024-04-22 07:08:13","title":"Exploring Kinetic Curves Features for the Classification of Benign and Malignant Breast Lesions in DCE-MRI","abstract":"Breast cancer is the most common malignant tumor among women and the second cause of cancer-related death. Early diagnosis in clinical practice is crucial for timely treatment and prognosis. Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has revealed great usability in the preoperative diagnosis and assessing therapy effects thanks to its capability to reflect the morphology and dynamic characteristics of breast lesions. However, most existing computer-assisted diagnosis algorithms only consider conventional radiomic features when classifying benign and malignant lesions in DCE-MRI. In this study, we propose to fully leverage the dynamic characteristics from the kinetic curves as well as the radiomic features to boost the classification accuracy of benign and malignant breast lesions. The proposed method is a fully automated solution by directly analyzing the 3D features from the DCE-MRI. The proposed method is evaluated on an in-house dataset including 200 DCE-MRI scans with 298 breast tumors (172 benign and 126 malignant tumors), achieving favorable classification accuracy with an area under curve (AUC) of 0.94. By simultaneously considering the dynamic and radiomic features, it is beneficial to effectively distinguish between benign and malignant breast lesions.","sentences":["Breast cancer is the most common malignant tumor among women and the second cause of cancer-related death.","Early diagnosis in clinical practice is crucial for timely treatment and prognosis.","Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has revealed great usability in the preoperative diagnosis and assessing therapy effects thanks to its capability to reflect the morphology and dynamic characteristics of breast lesions.","However, most existing computer-assisted diagnosis algorithms only consider conventional radiomic features when classifying benign and malignant lesions in DCE-MRI.","In this study, we propose to fully leverage the dynamic characteristics from the kinetic curves as well as the radiomic features to boost the classification accuracy of benign and malignant breast lesions.","The proposed method is a fully automated solution by directly analyzing the 3D features from the DCE-MRI.","The proposed method is evaluated on an in-house dataset including 200 DCE-MRI scans with 298 breast tumors (172 benign and 126 malignant tumors), achieving favorable classification accuracy with an area under curve (AUC) of 0.94.","By simultaneously considering the dynamic and radiomic features, it is beneficial to effectively distinguish between benign and malignant breast lesions."],"url":"http://arxiv.org/abs/2404.13929v1","category":"eess.IV"}
{"created":"2024-04-22 07:03:49","title":"Using Polyvinyl Alcohol as Polymeric Adhesive to Enhance the Water Stability of Soil and its Performance","abstract":"Soil degradation threatens agricultural productivity and food supply, leading to hunger issues in some developing regions. To address this challenge, we developed a low-cost, highly efficient, and long-term stable soil improvement method. We chose polyvinyl alcohol (PVA), a commercially available polymer that is safe and non-degradable, to serve as a soil adhesive. We mixed PVA solution into the soil and applied a drying treatment to enhance the bonding between PVA and the soil, achieving highly water-stable soil. This PVA-stabilized soil exhibits low bulk density, high porosity, and high permeability, making it an ideal substrate for planting. In a germination test, the PVA-stabilized soil revealed a higher germination rate and growth rate compared to those of the non-treated soil. We believe this simple and efficient soil improvement method can restore degraded soil and contribute to sustainable agriculture.","sentences":["Soil degradation threatens agricultural productivity and food supply, leading to hunger issues in some developing regions.","To address this challenge, we developed a low-cost, highly efficient, and long-term stable soil improvement method.","We chose polyvinyl alcohol (PVA), a commercially available polymer that is safe and non-degradable, to serve as a soil adhesive.","We mixed PVA solution into the soil and applied a drying treatment to enhance the bonding between PVA and the soil, achieving highly water-stable soil.","This PVA-stabilized soil exhibits low bulk density, high porosity, and high permeability, making it an ideal substrate for planting.","In a germination test, the PVA-stabilized soil revealed a higher germination rate and growth rate compared to those of the non-treated soil.","We believe this simple and efficient soil improvement method can restore degraded soil and contribute to sustainable agriculture."],"url":"http://arxiv.org/abs/2404.13926v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 06:59:03","title":"NeRF-DetS: Enhancing Multi-View 3D Object Detection with Sampling-adaptive Network of Continuous NeRF-based Representation","abstract":"As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively.","sentences":["As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection.","Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement.","To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS.","The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine.","Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion.","This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs.","NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively."],"url":"http://arxiv.org/abs/2404.13921v1","category":"cs.CV"}
{"created":"2024-04-22 06:56:47","title":"Basis Function Dependence of Estimation Precision for Synchrotron-Radiation-Based M\u00f6ssbauer Spectroscopy","abstract":"M\\\"ossbauer spectroscopy is a technique employed to investigate the microscopic properties of materials using transitions between energy levels in the nuclei. Conventionally, in synchrotron-radiation-based M\\\"ossbauer spectroscopy, the measurement window is decided by the researcher heuristically, although this decision has a significant impact on the shape of the measurement spectra. In this paper, we propose a method for evaluating the precision of the spectral position by introducing Bayesian estimation. The proposed method makes it possible to select the best measurement window by calculating the precision of M\\\"ossbauer spectroscopy from the data. Based on the results, the precision of the M\\\"ossbauer center shifts improved by more than three times compared with the results achieved with the conventional simple fitting method using the Lorentzian function.","sentences":["M\\\"ossbauer spectroscopy is a technique employed to investigate the microscopic properties of materials using transitions between energy levels in the nuclei.","Conventionally, in synchrotron-radiation-based M\\\"ossbauer spectroscopy, the measurement window is decided by the researcher heuristically, although this decision has a significant impact on the shape of the measurement spectra.","In this paper, we propose a method for evaluating the precision of the spectral position by introducing Bayesian estimation.","The proposed method makes it possible to select the best measurement window by calculating the precision of M\\\"ossbauer spectroscopy from the data.","Based on the results, the precision of the M\\\"ossbauer center shifts improved by more than three times compared with the results achieved with the conventional simple fitting method using the Lorentzian function."],"url":"http://arxiv.org/abs/2404.13916v1","category":"physics.comp-ph"}
{"created":"2024-04-22 06:32:17","title":"SI-FID: Only One Objective Indicator for Evaluating Stitched Images","abstract":"Image quality evaluation accurately is vital in developing image stitching algorithms as it directly reflects the algorithms progress. However, commonly used objective indicators always produce inconsistent and even conflicting results with subjective indicators. To enhance the consistency between objective and subjective evaluations, this paper introduces a novel indicator the Frechet Distance for Stitched Images (SI-FID). To be specific, our training network employs the contrastive learning architecture overall. We employ data augmentation approaches that serve as noise to distort images in the training set. Both the initial and distorted training sets are then input into the pre-training model for fine-tuning. We then evaluate the altered FID after introducing interference to the test set and examine if the noise can improve the consistency between objective and subjective evaluation results. The rank correlation coefficient is utilized to measure the consistency. SI-FID is an altered FID that generates the highest rank correlation coefficient under the effect of a certain noise. The experimental results demonstrate that the rank correlation coefficient obtained by SI-FID is at least 25% higher than other objective indicators, which means achieving evaluation results closer to human subjective evaluation.","sentences":["Image quality evaluation accurately is vital in developing image stitching algorithms as it directly reflects the algorithms progress.","However, commonly used objective indicators always produce inconsistent and even conflicting results with subjective indicators.","To enhance the consistency between objective and subjective evaluations, this paper introduces a novel indicator the Frechet Distance for Stitched Images (SI-FID).","To be specific, our training network employs the contrastive learning architecture overall.","We employ data augmentation approaches that serve as noise to distort images in the training set.","Both the initial and distorted training sets are then input into the pre-training model for fine-tuning.","We then evaluate the altered FID after introducing interference to the test set and examine if the noise can improve the consistency between objective and subjective evaluation results.","The rank correlation coefficient is utilized to measure the consistency.","SI-FID is an altered FID that generates the highest rank correlation coefficient under the effect of a certain noise.","The experimental results demonstrate that the rank correlation coefficient obtained by SI-FID is at least 25% higher than other objective indicators, which means achieving evaluation results closer to human subjective evaluation."],"url":"http://arxiv.org/abs/2404.13905v1","category":"eess.IV"}
{"created":"2024-04-22 06:21:19","title":"Lipschitz stability for an elliptic inverse problem with a single measurement","abstract":"We consider the problem of determining the unknown boundary values of a solution of an elliptic equation outside a bounded domain $B$ from the knowledge of the values of this solution on a boundary of an arbitrary bounded domain surrounding $B$. We obtain for this inverse problem Lipschitz stability under an additional hypothesis on the unknown boundary function. This result can be also interpreted as quantitative uniqueness of continuation from the Cauchy data on the boundary of the domain surrounding $B$. Our analysis also applies to an interior problem.","sentences":["We consider the problem of determining the unknown boundary values of a solution of an elliptic equation outside a bounded domain $B$ from the knowledge of the values of this solution on a boundary of an arbitrary bounded domain surrounding $B$. We obtain for this inverse problem Lipschitz stability under an additional hypothesis on the unknown boundary function.","This result can be also interpreted as quantitative uniqueness of continuation from the Cauchy data on the boundary of the domain surrounding $B$. Our analysis also applies to an interior problem."],"url":"http://arxiv.org/abs/2404.13901v1","category":"math.AP"}
{"created":"2024-04-22 06:18:43","title":"Feasibility of a co-designed online nutrition education program for people with multiple sclerosis","abstract":"Objective: Diet quality is important for people with multiple sclerosis (MS), but conflicting online information causes them confusion. People with MS want evidence-based MS-specific information to help them make healthy dietary changes, and we co-designed an asynchronous, online nutrition education program (Eating Well with MS) with the MS community. Our aim was to determine the feasibility of Eating Well with MS. Methods: We used a single-arm pre-post design. The feasibility trial was a nine-week intervention with adults with confirmed MS. Feasibility outcomes: 1) demand (recruitment); 2) practicality (completion); 3) acceptability (Intrinsic Motivation Inventory: interest/enjoyment and value/usefulness subscales); and 4) limited efficacy testing (Diet Habits Questionnaire (DHQ); Critical Nutrition Literacy Tool (CNLT); Food Literacy Behaviour Checklist (FLBC)). Results: The recruitment target (n=70) was reached. 87% completed at least one module and 57% completed the full program (five modules). The median interest/enjoyment rating was 5 out of 7 and median value/usefulness rating was 6 out of 7 (where 7 = very true). Compared to pre-program, participants who completed any of the program had statistically significantly improved DHQ, CNLT, and FLBC scores. Conclusion: Eating Well with MS was well received by the MS community and improved their dietary behaviours; demonstrating feasibility. Our findings support the use of co-design methods when developing resources to improve dietary behaviours.","sentences":["Objective: Diet quality is important for people with multiple sclerosis (MS), but conflicting online information causes them confusion.","People with MS want evidence-based MS-specific information to help them make healthy dietary changes, and we co-designed an asynchronous, online nutrition education program (Eating Well with MS) with the MS community.","Our aim was to determine the feasibility of Eating Well with MS.","Methods: We used a single-arm pre-post design.","The feasibility trial was a nine-week intervention with adults with confirmed MS.","Feasibility outcomes: 1) demand (recruitment); 2) practicality (completion); 3) acceptability (Intrinsic Motivation Inventory: interest/enjoyment and value/usefulness subscales); and 4) limited efficacy testing (Diet Habits Questionnaire (DHQ); Critical Nutrition Literacy Tool (CNLT); Food Literacy Behaviour Checklist (FLBC)).","Results:","The recruitment target (n=70) was reached.","87% completed at least one module and 57% completed the full program (five modules).","The median interest/enjoyment rating was 5 out of 7 and median value/usefulness rating was 6 out of 7 (where 7 = very true).","Compared to pre-program, participants who completed any of the program had statistically significantly improved DHQ, CNLT, and FLBC scores.","Conclusion: Eating Well with MS was well received by the MS community and improved their dietary behaviours; demonstrating feasibility.","Our findings support the use of co-design methods when developing resources to improve dietary behaviours."],"url":"http://arxiv.org/abs/2404.13900v1","category":"q-bio.OT"}
{"created":"2024-04-22 05:31:40","title":"Quantitative Analysis of Roles of Direct and Indirect Pathways for Action Selection in The Basal Ganglia","abstract":"The basal ganglia (BG) show diverse functions for motor and cognition. Here, we are concerned about action selection performed by the BG. Particularly, we make quantitative analysis of roles of direct pathway (DP) and indirect pathway (IP) for action selection in a spiking neural network with 3 competing channels. For such quantitative work, in each channel, we get the competition degree ${\\cal C}_d$, given by the ratio of strength of DP (${\\cal S}_{DP}$) to strength of IP (${\\cal S}_{IP}$) (i.e., ${\\cal C}_d = {\\cal S}_{DP} / {\\cal S}_{IP}$). Then, desired action is selected in the channel with the largest ${\\cal C}_d$. Desired action selection is made mainly due to strong focused inhibitory projection to the output nucleus, SNr (substantia nigra pars reticulata) via the \"Go\" DP in the corresponding channel. Unlike the case of DP, there are two types of IPs; intra-channel IP and inter-channel IP, due to widespread diffusive excitation from the STN (subthalamic nucleus). The intra-channel \"No-Go\" IP plays a role of brake to suppress the desired action selection. On the other hand, the inter-channel IP to the SNr in the neighboring channels suppresses competing actions, leading to spotlight the desired action selection. In this way, role of the inter-channel IP is opposite to that of the intra-channel IP. But, to the best of our knowledge, no quantitative analysis for such roles of the DP and the two IPs was made. Here, by direct calculations of the DP and the intra- and the inter-channel IP presynaptic currents into the SNr in each channel, we get the competition degree of each channel to determine a desired action, and then roles of the DP and the intra- and inter-channel IPs are quantitatively made clear.","sentences":["The basal ganglia (BG) show diverse functions for motor and cognition.","Here, we are concerned about action selection performed by the BG.","Particularly, we make quantitative analysis of roles of direct pathway (DP) and indirect pathway (IP) for action selection in a spiking neural network with 3 competing channels.","For such quantitative work, in each channel, we get the competition degree ${\\cal C}_d$, given by the ratio of strength of DP (${\\cal S}_{DP}$) to strength of IP (${\\cal S}_{IP}$) (i.e., ${\\cal C}_d = {\\cal S}_{DP} / {\\cal S}_{IP}$).","Then, desired action is selected in the channel with the largest ${\\cal C}_d$. Desired action selection is made mainly due to strong focused inhibitory projection to the output nucleus, SNr (substantia nigra pars reticulata) via the \"Go\" DP in the corresponding channel.","Unlike the case of DP, there are two types of IPs; intra-channel IP and inter-channel IP, due to widespread diffusive excitation from the STN (subthalamic nucleus).","The intra-channel \"No-Go\" IP plays a role of brake to suppress the desired action selection.","On the other hand, the inter-channel IP to the SNr in the neighboring channels suppresses competing actions, leading to spotlight the desired action selection.","In this way, role of the inter-channel IP is opposite to that of the intra-channel IP.","But, to the best of our knowledge, no quantitative analysis for such roles of the DP and the two IPs was made.","Here, by direct calculations of the DP and the intra- and the inter-channel IP presynaptic currents into the SNr in each channel, we get the competition degree of each channel to determine a desired action, and then roles of the DP and the intra- and inter-channel IPs are quantitatively made clear."],"url":"http://arxiv.org/abs/2404.13888v1","category":"q-bio.NC"}
{"created":"2024-04-22 05:01:29","title":"Explicit Lipschitz Value Estimation Enhances Policy Robustness Against Perturbation","abstract":"In robotic control tasks, policies trained by reinforcement learning (RL) in simulation often experience a performance drop when deployed on physical hardware, due to modeling error, measurement error, and unpredictable perturbations in the real world. Robust RL methods account for this issue by approximating a worst-case value function during training, but they can be sensitive to approximation errors in the value function and its gradient before training is complete. In this paper, we hypothesize that Lipschitz regularization can help condition the approximated value function gradients, leading to improved robustness after training. We test this hypothesis by combining Lipschitz regularization with an application of Fast Gradient Sign Method to reduce approximation errors when evaluating the value function under adversarial perturbations. Our empirical results demonstrate the benefits of this approach over prior work on a number of continuous control benchmarks.","sentences":["In robotic control tasks, policies trained by reinforcement learning (RL) in simulation often experience a performance drop when deployed on physical hardware, due to modeling error, measurement error, and unpredictable perturbations in the real world.","Robust RL methods account for this issue by approximating a worst-case value function during training, but they can be sensitive to approximation errors in the value function and its gradient before training is complete.","In this paper, we hypothesize that Lipschitz regularization can help condition the approximated value function gradients, leading to improved robustness after training.","We test this hypothesis by combining Lipschitz regularization with an application of Fast Gradient Sign Method to reduce approximation errors when evaluating the value function under adversarial perturbations.","Our empirical results demonstrate the benefits of this approach over prior work on a number of continuous control benchmarks."],"url":"http://arxiv.org/abs/2404.13879v1","category":"cs.LG"}
{"created":"2024-04-22 04:41:42","title":"FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge","abstract":"Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection. Existing methods typically generate these faces by blending real or fake faces in color space. While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth. To address this, this paper introduces {\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge. Specifically, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces. Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces. Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process. Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods.","sentences":["Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection.","Existing methods typically generate these faces by blending real or fake faces in color space.","While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth.","To address this, this paper introduces {\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge.","Specifically, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces.","Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces.","Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process.","Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods."],"url":"http://arxiv.org/abs/2404.13872v1","category":"cs.CV"}
{"created":"2024-04-22 04:31:09","title":"Plug-and-Play Algorithm Convergence Analysis From The Standpoint of Stochastic Differential Equation","abstract":"The Plug-and-Play (PnP) algorithm is popular for inverse image problem-solving. However, this algorithm lacks theoretical analysis of its convergence with more advanced plug-in denoisers. We demonstrate that discrete PnP iteration can be described by a continuous stochastic differential equation (SDE). We can also achieve this transformation through Markov process formulation of PnP. Then, we can take a higher standpoint of PnP algorithms from stochastic differential equations, and give a unified framework for the convergence property of PnP according to the solvability condition of its corresponding SDE. We reveal that a much weaker condition, bounded denoiser with Lipschitz continuous measurement function would be enough for its convergence guarantee, instead of previous Lipschitz continuous denoiser condition.","sentences":["The Plug-and-Play (PnP) algorithm is popular for inverse image problem-solving.","However, this algorithm lacks theoretical analysis of its convergence with more advanced plug-in denoisers.","We demonstrate that discrete PnP iteration can be described by a continuous stochastic differential equation (SDE).","We can also achieve this transformation through Markov process formulation of PnP. Then, we can take a higher standpoint of PnP algorithms from stochastic differential equations, and give a unified framework for the convergence property of PnP according to the solvability condition of its corresponding SDE.","We reveal that a much weaker condition, bounded denoiser with Lipschitz continuous measurement function would be enough for its convergence guarantee, instead of previous Lipschitz continuous denoiser condition."],"url":"http://arxiv.org/abs/2404.13866v1","category":"cs.CV"}
{"created":"2024-04-22 04:19:24","title":"Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis","abstract":"What is agency, and why does it matter? In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent. The first view, which we term mechanistic, is commonly--and implicitly--assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI. Under the second view, which we term volitional, AI can no longer be considered an ethical agent. We discuss the implications of each of these views for two critical questions: first, what the ideal system ought to look like, and second, how accountability may be achieved. In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes.","sentences":["What is agency, and why does it matter?","In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent.","The first view, which we term mechanistic, is commonly--and implicitly--assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI.","Under the second view, which we term volitional, AI can no longer be considered an ethical agent.","We discuss the implications of each of these views for two critical questions: first, what the ideal system ought to look like, and second, how accountability may be achieved.","In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes."],"url":"http://arxiv.org/abs/2404.13861v1","category":"cs.CY"}
{"created":"2024-04-22 03:47:00","title":"Understanding the role of FFNs in driving multilingual behaviour in LLMs","abstract":"Multilingualism in Large Language Models (LLMs) is an yet under-explored area. In this paper, we conduct an in-depth analysis of the multilingual capabilities of a family of a Large Language Model, examining its architecture, activation patterns, and processing mechanisms across languages. We introduce novel metrics to probe the model's multilingual behaviour at different layers and shed light on the impact of architectural choices on multilingual processing.   Our findings reveal different patterns of multilinugal processing in the sublayers of Feed-Forward Networks of the models. Furthermore, we uncover the phenomenon of \"over-layerization\" in certain model configurations, where increasing layer depth without corresponding adjustments to other parameters may degrade model performance. Through comparisons within and across languages, we demonstrate the interplay between model architecture, layer depth, and multilingual processing capabilities of LLMs trained on multiple languages.","sentences":["Multilingualism in Large Language Models (LLMs) is an yet under-explored area.","In this paper, we conduct an in-depth analysis of the multilingual capabilities of a family of a Large Language Model, examining its architecture, activation patterns, and processing mechanisms across languages.","We introduce novel metrics to probe the model's multilingual behaviour at different layers and shed light on the impact of architectural choices on multilingual processing.   ","Our findings reveal different patterns of multilinugal processing in the sublayers of Feed-Forward Networks of the models.","Furthermore, we uncover the phenomenon of \"over-layerization\" in certain model configurations, where increasing layer depth without corresponding adjustments to other parameters may degrade model performance.","Through comparisons within and across languages, we demonstrate the interplay between model architecture, layer depth, and multilingual processing capabilities of LLMs trained on multiple languages."],"url":"http://arxiv.org/abs/2404.13855v1","category":"cs.CL"}
{"created":"2024-04-22 03:39:03","title":"Self-Supervised Monocular Depth Estimation in the Dark: Towards Data Distribution Compensation","abstract":"Nighttime self-supervised monocular depth estimation has received increasing attention in recent years. However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions. Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks. In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training. Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences. With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner. Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods.","sentences":["Nighttime self-supervised monocular depth estimation has received increasing attention in recent years.","However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions.","Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks.","In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training.","Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences.","With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner.","Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods."],"url":"http://arxiv.org/abs/2404.13854v1","category":"cs.CV"}
{"created":"2024-04-22 03:05:32","title":"EventLens: Leveraging Event-Aware Pretraining and Cross-modal Linking Enhances Visual Commonsense Reasoning","abstract":"Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct. With emergence of Large Language Models (LLMs), it is natural and imperative to explore their applicability to VCR. However, VCR task demands more external knowledge to tackle its challenging questions, necessitating special designs to activate LLMs' commonsense reasoning abilities. Also, most existing Multimodal LLMs adopted an abstraction of entire input image, which makes it difficult to comprehend VCR's unique co-reference tags between image regions and text, posing challenges for fine-grained alignment. To address these issues, we propose EventLens that leverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR. First, by emulating the cognitive process of human reasoning, an Event-Aware Pretraining auxiliary task is introduced to better activate LLM's global comprehension of intricate scenarios. Second, during fine-tuning, we further utilize reference tags to bridge RoI features with texts, while preserving both modality semantics. Finally, we use instruct-style prompts to narrow the gap between pretraining and fine-tuning, and task-specific adapters to better integrate LLM's inherent knowledge with new commonsense. Experimental results show the effectiveness of our proposed auxiliary task and fine-grained linking strategy.","sentences":["Visual Commonsense Reasoning (VCR) is a cognitive task, challenging models to answer visual questions requiring human commonsense, and to provide rationales explaining why the answers are correct.","With emergence of Large Language Models (LLMs), it is natural and imperative to explore their applicability to VCR.","However, VCR task demands more external knowledge to tackle its challenging questions, necessitating special designs to activate LLMs' commonsense reasoning abilities.","Also, most existing Multimodal LLMs adopted an abstraction of entire input image, which makes it difficult to comprehend VCR's unique co-reference tags between image regions and text, posing challenges for fine-grained alignment.","To address these issues, we propose EventLens that leverages Event-Aware Pretraining and Cross-modal Linking and EnhanceS VCR.","First, by emulating the cognitive process of human reasoning, an Event-Aware Pretraining auxiliary task is introduced to better activate LLM's global comprehension of intricate scenarios.","Second, during fine-tuning, we further utilize reference tags to bridge RoI features with texts, while preserving both modality semantics.","Finally, we use instruct-style prompts to narrow the gap between pretraining and fine-tuning, and task-specific adapters to better integrate LLM's inherent knowledge with new commonsense.","Experimental results show the effectiveness of our proposed auxiliary task and fine-grained linking strategy."],"url":"http://arxiv.org/abs/2404.13847v1","category":"cs.CV"}
{"created":"2024-04-22 02:30:25","title":"MultiFun-DAG: Multivariate Functional Directed Acyclic Graph","abstract":"Directed Acyclic Graphical (DAG) models efficiently formulate causal relationships in complex systems. Traditional DAGs assume nodes to be scalar variables, characterizing complex systems under a facile and oversimplified form. This paper considers that nodes can be multivariate functional data and thus proposes a multivariate functional DAG (MultiFun-DAG). It constructs a hidden bilinear multivariate function-to-function regression to describe the causal relationships between different nodes. Then an Expectation-Maximum algorithm is used to learn the graph structure as a score-based algorithm with acyclic constraints. Theoretical properties are diligently derived. Prudent numerical studies and a case study from urban traffic congestion analysis are conducted to show MultiFun-DAG's effectiveness.","sentences":["Directed Acyclic Graphical (DAG) models efficiently formulate causal relationships in complex systems.","Traditional DAGs assume nodes to be scalar variables, characterizing complex systems under a facile and oversimplified form.","This paper considers that nodes can be multivariate functional data and thus proposes a multivariate functional DAG (MultiFun-DAG).","It constructs a hidden bilinear multivariate function-to-function regression to describe the causal relationships between different nodes.","Then an Expectation-Maximum algorithm is used to learn the graph structure as a score-based algorithm with acyclic constraints.","Theoretical properties are diligently derived.","Prudent numerical studies and a case study from urban traffic congestion analysis are conducted to show MultiFun-DAG's effectiveness."],"url":"http://arxiv.org/abs/2404.13836v1","category":"stat.ME"}
{"created":"2024-04-22 02:28:28","title":"Finding accreted stars in the Milky Way: clues from NIHAO simulations","abstract":"Exploring the marks left by galactic accretion in the Milky Way helps us understand how our Galaxy was formed. However, finding and studying accreted stars and the galaxies they came from has been challenging. This study uses a simulation from the NIHAO project, which now includes a wider range of chemical compositions, to find better ways to spot these accreted stars. By comparing our findings with data from the GALAH spectroscopic survey, we confirm that the observationally established diagnostics of [Al/Fe] vs. [Mg/Mn] also show a separation of in-situ and accreted stars in the simulation, but stars from different accretion events tend to overlap in this plane even without observational uncertainties. Looking at the relationship between stellar age and linear or logarithmic abundances, such as [Fe/H], we can clearly separate different groups of these stars if the uncertainties in their chemical makeup are less than 0.15 dex and less than 20% for their ages. This method shows promise for studying the history of the Milky Way and other galaxies. Our work highlights how important it is to have accurate measurements of stellar ages and chemical content. It also shows how simulations can help us understand the complex process of galaxies merging and suggest how these events might relate to the differences we see between our Galaxy's thin and thick disk stars. This study provides a way to compare theoretical models with real observations, opening new paths for research in both our own Galaxy and beyond.","sentences":["Exploring the marks left by galactic accretion in the Milky Way helps us understand how our Galaxy was formed.","However, finding and studying accreted stars and the galaxies they came from has been challenging.","This study uses a simulation from the NIHAO project, which now includes a wider range of chemical compositions, to find better ways to spot these accreted stars.","By comparing our findings with data from the GALAH spectroscopic survey, we confirm that the observationally established diagnostics of [Al/Fe] vs. [Mg/Mn] also show a separation of in-situ and accreted stars in the simulation, but stars from different accretion events tend to overlap in this plane even without observational uncertainties.","Looking at the relationship between stellar age and linear or logarithmic abundances, such as [Fe/H], we can clearly separate different groups of these stars if the uncertainties in their chemical makeup are less than 0.15 dex and less than 20% for their ages.","This method shows promise for studying the history of the Milky Way and other galaxies.","Our work highlights how important it is to have accurate measurements of stellar ages and chemical content.","It also shows how simulations can help us understand the complex process of galaxies merging and suggest how these events might relate to the differences we see between our Galaxy's thin and thick disk stars.","This study provides a way to compare theoretical models with real observations, opening new paths for research in both our own Galaxy and beyond."],"url":"http://arxiv.org/abs/2404.13835v1","category":"astro-ph.GA"}
{"created":"2024-04-22 02:02:58","title":"Unique multistable states in periodic structures with saturable nonlinearity. II. Broken $\\mathcal{PT}$-symmetric regime","abstract":"In this work, we observe that the $\\mathcal{PT}$-symmetric fiber Bragg gratings (PTFBGs) with saturable nonlinearity (SNL) exhibit ramp-like, mixed, optical multistability (OM) in the broken regime. The interplay between nonlinearity and detuning parameter plays a central role in transforming the characteristics of the hysteresis curves and facilitates the realization of different OM curves. Also, it plays a crucial role in reducing the switch-up and down intensities of various stable branches of an OM curve. In a mixed OM curve, either the ramp-like hysteresis curves or S-like hysteresis curves can appear predominantly depending on the magnitude of the detuning parameter. An increase in the device length or nonlinearity increases the number of stable states for fixed values of input intensity. Under a reversal in the direction of light incidence, the ramp-like OM and mixed OM curves assume an unusual vortex-like envelope at lower intensities. Numerical simulations reveal that the switch-up and down intensities of different stable branches of a ramp-like OM and mixed OM curves drift towards the higher and lower intensity sides, respectively (opposite direction). The drift is severe to the extent that an intermediate hysteresis curve features switch-down action at near-zero switching intensities. Also, the input intensities required to realize ramp-like, and mixed OM curves reduce dramatically under a reversal in the direction of light incidence.","sentences":["In this work, we observe that the $\\mathcal{PT}$-symmetric fiber Bragg gratings (PTFBGs) with saturable nonlinearity (SNL) exhibit ramp-like, mixed, optical multistability (OM) in the broken regime.","The interplay between nonlinearity and detuning parameter plays a central role in transforming the characteristics of the hysteresis curves and facilitates the realization of different OM curves.","Also, it plays a crucial role in reducing the switch-up and down intensities of various stable branches of an OM curve.","In a mixed OM curve, either the ramp-like hysteresis curves or S-like hysteresis curves can appear predominantly depending on the magnitude of the detuning parameter.","An increase in the device length or nonlinearity increases the number of stable states for fixed values of input intensity.","Under a reversal in the direction of light incidence, the ramp-like OM and mixed OM curves assume an unusual vortex-like envelope at lower intensities.","Numerical simulations reveal that the switch-up and down intensities of different stable branches of a ramp-like OM and mixed OM curves drift towards the higher and lower intensity sides, respectively (opposite direction).","The drift is severe to the extent that an intermediate hysteresis curve features switch-down action at near-zero switching intensities.","Also, the input intensities required to realize ramp-like, and mixed OM curves reduce dramatically under a reversal in the direction of light incidence."],"url":"http://arxiv.org/abs/2404.13828v1","category":"physics.optics"}
{"created":"2024-04-22 01:51:22","title":"Robotic Blended Sonification: Consequential Robot Sound as Creative Material for Human-Robot Interaction","abstract":"Current research in robotic sounds generally focuses on either masking the consequential sound produced by the robot or on sonifying data about the robot to create a synthetic robot sound. We propose to capture, modify, and utilise rather than mask the sounds that robots are already producing. In short, this approach relies on capturing a robot's sounds, processing them according to contextual information (e.g., collaborators' proximity or particular work sequences), and playing back the modified sound. Previous research indicates the usefulness of non-semantic, and even mechanical, sounds as a communication tool for conveying robotic affect and function. Adding to this, this paper presents a novel approach which makes two key contributions: (1) a technique for real-time capture and processing of consequential robot sounds, and (2) an approach to explore these sounds through direct human-robot interaction. Drawing on methodologies from design, human-robot interaction, and creative practice, the resulting 'Robotic Blended Sonification' is a concept which transforms the consequential robot sounds into a creative material that can be explored artistically and within application-based studies.","sentences":["Current research in robotic sounds generally focuses on either masking the consequential sound produced by the robot or on sonifying data about the robot to create a synthetic robot sound.","We propose to capture, modify, and utilise rather than mask the sounds that robots are already producing.","In short, this approach relies on capturing a robot's sounds, processing them according to contextual information (e.g., collaborators' proximity or particular work sequences), and playing back the modified sound.","Previous research indicates the usefulness of non-semantic, and even mechanical, sounds as a communication tool for conveying robotic affect and function.","Adding to this, this paper presents a novel approach which makes two key contributions: (1) a technique for real-time capture and processing of consequential robot sounds, and (2) an approach to explore these sounds through direct human-robot interaction.","Drawing on methodologies from design, human-robot interaction, and creative practice, the resulting 'Robotic Blended Sonification' is a concept which transforms the consequential robot sounds into a creative material that can be explored artistically and within application-based studies."],"url":"http://arxiv.org/abs/2404.13821v1","category":"cs.HC"}
{"created":"2024-04-22 01:40:37","title":"Joint Liability Model with Adaptation to Climate Change","abstract":"This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change. Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score. This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships. Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits. Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions.","sentences":["This paper extends the application of ESG score assessment methodologies from large corporations to individual farmers' production, within the context of climate change.","Our proposal involves the integration of crucial agricultural sustainability variables into conventional personal credit evaluation frameworks, culminating in the formulation of a holistic sustainable credit rating referred to as the Environmental, Social, Economics (ESE) score.","This ESE score is integrated into theoretical joint liability models, to gain valuable insights into optimal group sizes and individual-ESE score relationships.","Additionally, we adopt a mean-variance utility function for farmers to effectively capture the risk associated with anticipated profits.","Through a set of simulation exercises, the paper investigates the implications of incorporating ESE scores into credit evaluation systems, offering a nuanced comprehension of the repercussions under various climatic conditions."],"url":"http://arxiv.org/abs/2404.13818v1","category":"q-fin.GN"}
{"created":"2024-04-22 01:08:06","title":"Quasicrystalline chiral soliton lattices in a Fibonacci helimagnet","abstract":"We investigate the ground state magnetic configurations of a Fibonacci chain of classical spins with nearest-neighbor ferromagnetic and monoaxial Dzyaloshinskii-Moriya exchange interactions. Our analysis reveals a diverse array of magnetic textures induced by an external magnetic field perpendicular to the Dzyaloshinskii-Moriya vector. These textures exhibit a spectrum ranging from a quasi-fully polarized non-collinear state under high magnetic fields, capable of maintaining metastable chiral soliton topological defects, to a variety of quasicrystalline chiral soliton lattices below a critical field $H_c$. For a range of magnetic fields below $H_c$, the ground state spin textures result from the interplay between an effective quasiperiodic potential influencing the solitons and their repulsive interactions. At lower magnetic fields, the system experiences a commensurate-incommensurate transition, signified by the appearance of discommensurations in the quasicrystalline soliton lattice. In the absence of an external magnetic field, the ground state assumes a helical configuration with a quasiperiodic pitch angle.","sentences":["We investigate the ground state magnetic configurations of a Fibonacci chain of classical spins with nearest-neighbor ferromagnetic and monoaxial Dzyaloshinskii-Moriya exchange interactions.","Our analysis reveals a diverse array of magnetic textures induced by an external magnetic field perpendicular to the Dzyaloshinskii-Moriya vector.","These textures exhibit a spectrum ranging from a quasi-fully polarized non-collinear state under high magnetic fields, capable of maintaining metastable chiral soliton topological defects, to a variety of quasicrystalline chiral soliton lattices below a critical field $H_c$. For a range of magnetic fields below $H_c$, the ground state spin textures result from the interplay between an effective quasiperiodic potential influencing the solitons and their repulsive interactions.","At lower magnetic fields, the system experiences a commensurate-incommensurate transition, signified by the appearance of discommensurations in the quasicrystalline soliton lattice.","In the absence of an external magnetic field, the ground state assumes a helical configuration with a quasiperiodic pitch angle."],"url":"http://arxiv.org/abs/2404.13810v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 01:07:29","title":"Open Cluster Dynamics under the Influence of Outflow-Ambient Interactions","abstract":"Outflowing stars impinging upon ambient gas experience accelerations due to the gravitational feedback from the morphology of the interaction betweem the outflow and the ambient gas. Such \"negative dynamical friction\" (NDF), in contrast to the conventional \"dynamical friction\" (DF), is studied for its impact on the dynamics of open clusters immersed in a uniform ambient gas. We modify the $N$-body integration code REBOUND with both NDF and DF implemented according to the outflow conditions of each star in a consistently constructed model open cluster. The evolution of stars is also involved in determining the gas-star interactions throughout their stellar lives. Compared to DF-only and gas-free models with identical initial conditions, the NDF-affected cluster is puffier and evaporates faster, as indicated by various diagnostics, including lower velocity dispersions and larger half-mass and half-light radii. Neutron stars with fast winds are expelled from the cluster due to their intensive NDF effect, even without the \"kicks\" by asymmetric supernovae. Exploration of parameter space confirms that the NDF effect is generally enhanced with higher ambient gas densities, in qualitatively agreement with the expression of acceleration. Outflow-ambient interactions should be considered for the proper interpretation of the evolution of stellar dynamics in clusters.","sentences":["Outflowing stars impinging upon ambient gas experience accelerations due to the gravitational feedback from the morphology of the interaction betweem the outflow and the ambient gas.","Such \"negative dynamical friction\" (NDF), in contrast to the conventional \"dynamical friction\" (DF), is studied for its impact on the dynamics of open clusters immersed in a uniform ambient gas.","We modify the $N$-body integration code REBOUND with both NDF and DF implemented according to the outflow conditions of each star in a consistently constructed model open cluster.","The evolution of stars is also involved in determining the gas-star interactions throughout their stellar lives.","Compared to DF-only and gas-free models with identical initial conditions, the NDF-affected cluster is puffier and evaporates faster, as indicated by various diagnostics, including lower velocity dispersions and larger half-mass and half-light radii.","Neutron stars with fast winds are expelled from the cluster due to their intensive NDF effect, even without the \"kicks\" by asymmetric supernovae.","Exploration of parameter space confirms that the NDF effect is generally enhanced with higher ambient gas densities, in qualitatively agreement with the expression of acceleration.","Outflow-ambient interactions should be considered for the proper interpretation of the evolution of stellar dynamics in clusters."],"url":"http://arxiv.org/abs/2404.13809v1","category":"astro-ph.GA"}
{"created":"2024-04-21 23:52:32","title":"The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment","abstract":"As more algorithmic systems have come under scrutiny for their potential to inflict societal harms, an increasing number of organizations that hold power over harmful algorithms have chosen (or were required under the law) to abandon them. While social movements and calls to abandon harmful algorithms have emerged across application domains, little academic attention has been paid to studying abandonment as a means to mitigate algorithmic harms. In this paper, we take a first step towards conceptualizing \"algorithm abandonment\" as an organization's decision to stop designing, developing, or using an algorithmic system due to its (potential) harms. We conduct a thematic analysis of real-world cases of algorithm abandonment to characterize the dynamics leading to this outcome. Our analysis of 40 cases reveals that campaigns to abandon an algorithm follow a common process of six iterative phases: discovery, diagnosis, dissemination, dialogue, decision, and death, which we term the \"6 D's of abandonment\". In addition, we highlight key factors that facilitate (or prohibit) abandonment, which include characteristics of both the technical and social systems that the algorithm is embedded within. We discuss implications for several stakeholders, including proprietors and technologists who have the power to influence an algorithm's (dis)continued use, FAccT researchers, and policymakers.","sentences":["As more algorithmic systems have come under scrutiny for their potential to inflict societal harms, an increasing number of organizations that hold power over harmful algorithms have chosen (or were required under the law) to abandon them.","While social movements and calls to abandon harmful algorithms have emerged across application domains, little academic attention has been paid to studying abandonment as a means to mitigate algorithmic harms.","In this paper, we take a first step towards conceptualizing \"algorithm abandonment\" as an organization's decision to stop designing, developing, or using an algorithmic system due to its (potential) harms.","We conduct a thematic analysis of real-world cases of algorithm abandonment to characterize the dynamics leading to this outcome.","Our analysis of 40 cases reveals that campaigns to abandon an algorithm follow a common process of six iterative phases: discovery, diagnosis, dissemination, dialogue, decision, and death, which we term the \"6 D's of abandonment\".","In addition, we highlight key factors that facilitate (or prohibit) abandonment, which include characteristics of both the technical and social systems that the algorithm is embedded within.","We discuss implications for several stakeholders, including proprietors and technologists who have the power to influence an algorithm's (dis)continued use, FAccT researchers, and policymakers."],"url":"http://arxiv.org/abs/2404.13802v1","category":"cs.HC"}
{"created":"2024-04-21 23:48:23","title":"Discovery of the Free Precession in the Magnetar SGR 1806$-$20 with the ASCA GIS","abstract":"Four X-ray data sets of the Soft Gamma Repeater SGR 1806-20, taken with the Gas Imaging Spectrometer (GIS) onboad ASCA, were analyzed. Three of them were acquired over 1993 October 9-20, whereas the last one in 1995 October. Epoch-folding analysis of the 2.8-12 keV signals confirmed the $\\sim 7.6$ s pulses in these data, which Kouveliotou et al. (1998) reported as one of the earliest pulse detections from this object. In the 1995 observation, 3-12 keV pulses were phase modulated with a period of $T =16.4 \\pm 0.4$ ks, and an amplitude of $\\sim 1$ s. This makes a fourth example of the behavior observed from magnetars. Like in the previous three sources, the pulse-phase modulation of SGR 1806-20 disappeared at $\\lesssim 2.5$ keV, where the soft X-ray component dominates. In the 1993 data sets, this periodic modulation was reconfirmed, and successfully phase-connected coherently across the 11 d interval. As a result, the modulation period was refined to $T =16.435 \\pm 0.024$ ks. The implied high stability of the phenomenon strengthens its interpretation in terms of free precession of the neutron star, which is deformed to an asphericity of $\\sim 10^{-4}$, presumably by the stress of toroidal magnetic fields reaching $\\sim 10^{16}$ G. Toroidal fields of this level can be common among magnetars.","sentences":["Four X-ray data sets of the Soft Gamma Repeater SGR 1806-20, taken with the Gas Imaging Spectrometer (GIS) onboad ASCA, were analyzed.","Three of them were acquired over 1993 October 9-20, whereas the last one in 1995 October.","Epoch-folding analysis of the 2.8-12 keV signals confirmed the $\\sim 7.6$ s pulses in these data, which Kouveliotou et al.","(1998) reported as one of the earliest pulse detections from this object.","In the 1995 observation, 3-12 keV pulses were phase modulated with a period of $T =16.4 \\pm 0.4$ ks, and an amplitude of $\\sim 1$ s. This makes a fourth example of the behavior observed from magnetars.","Like in the previous three sources, the pulse-phase modulation of SGR 1806-20 disappeared at $\\lesssim 2.5$ keV, where the soft X-ray component dominates.","In the 1993 data sets, this periodic modulation was reconfirmed, and successfully phase-connected coherently across the 11 d interval.","As a result, the modulation period was refined to $T =16.435 \\pm 0.024$ ks.","The implied high stability of the phenomenon strengthens its interpretation in terms of free precession of the neutron star, which is deformed to an asphericity of $\\sim 10^{-4}$, presumably by the stress of toroidal magnetic fields reaching $\\sim 10^{16}$ G. Toroidal fields of this level can be common among magnetars."],"url":"http://arxiv.org/abs/2404.13799v1","category":"astro-ph.HE"}
{"created":"2024-04-21 23:34:45","title":"Enforcing Conditional Independence for Fair Representation Learning and Causal Image Generation","abstract":"Conditional independence (CI) constraints are critical for defining and evaluating fairness in machine learning, as well as for learning unconfounded or causal representations. Traditional methods for ensuring fairness either blindly learn invariant features with respect to a protected variable (e.g., race when classifying sex from face images) or enforce CI relative to the protected attribute only on the model output (e.g., the sex label). Neither of these methods are effective in enforcing CI in high-dimensional feature spaces. In this paper, we focus on a nascent approach characterizing the CI constraint in terms of two Jensen-Shannon divergence terms, and we extend it to high-dimensional feature spaces using a novel dynamic sampling strategy. In doing so, we introduce a new training paradigm that can be applied to any encoder architecture. We are able to enforce conditional independence of the diffusion autoencoder latent representation with respect to any protected attribute under the equalized odds constraint and show that this approach enables causal image generation with controllable latent spaces. Our experimental results demonstrate that our approach can achieve high accuracy on downstream tasks while upholding equality of odds.","sentences":["Conditional independence (CI) constraints are critical for defining and evaluating fairness in machine learning, as well as for learning unconfounded or causal representations.","Traditional methods for ensuring fairness either blindly learn invariant features with respect to a protected variable (e.g., race when classifying sex from face images) or enforce CI relative to the protected attribute only on the model output (e.g., the sex label).","Neither of these methods are effective in enforcing CI in high-dimensional feature spaces.","In this paper, we focus on a nascent approach characterizing the CI constraint in terms of two Jensen-Shannon divergence terms, and we extend it to high-dimensional feature spaces using a novel dynamic sampling strategy.","In doing so, we introduce a new training paradigm that can be applied to any encoder architecture.","We are able to enforce conditional independence of the diffusion autoencoder latent representation with respect to any protected attribute under the equalized odds constraint and show that this approach enables causal image generation with controllable latent spaces.","Our experimental results demonstrate that our approach can achieve high accuracy on downstream tasks while upholding equality of odds."],"url":"http://arxiv.org/abs/2404.13798v1","category":"cs.CV"}
{"created":"2024-04-21 23:20:11","title":"A Cyclic Spectroscopy Scintillation Study of PSR B1937+21 I. Demonstration of Improved Scintillometry","abstract":"We use cyclic spectroscopy to perform high frequency-resolution analyses of multi-hour baseband Arecibo observations of the millisecond pulsar PSR B1937+21. This technique allows for the examination of scintillation features in far greater detail than is otherwise possible under most pulsar timing array observing setups. We measure scintillation bandwidths and timescales in each of eight subbands across a 200 MHz observing band in each observation. Through these measurements we obtain robust, intra-epoch estimates of the frequency scalings for scintillation bandwidth and timescale. Thanks to our high frequency resolution and the narrow scintles of this pulsar, we resolve scintillation arcs in the secondary spectra due to the increased Nyquist limit, which would not have been resolved at the same observing frequency with a traditional filterbank spectrum using NANOGrav's current time and frequency resolutions, and the frequency-dependent evolution of scintillation arc features within individual observations. We observe the dimming of prominent arc features at higher frequencies, possibly due to a combination of decreasing flux density and undetermined effects due to the interstellar medium. We also find agreement with arc curvature frequency dependence predicted by Stinebring et al. (2001) in some epochs. Thanks to the frequency resolution improvement provided by cyclic spectroscopy, these results show strong promise for future such analyses with millisecond pulsars, particularly for pulsar timing arrays, where such techniques can allow for detailed studies of the interstellar medium in highly scattered pulsars without sacrificing the timing resolution that is crucial to their gravitational wave detection efforts.","sentences":["We use cyclic spectroscopy to perform high frequency-resolution analyses of multi-hour baseband Arecibo observations of the millisecond pulsar PSR B1937+21.","This technique allows for the examination of scintillation features in far greater detail than is otherwise possible under most pulsar timing array observing setups.","We measure scintillation bandwidths and timescales in each of eight subbands across a 200 MHz observing band in each observation.","Through these measurements we obtain robust, intra-epoch estimates of the frequency scalings for scintillation bandwidth and timescale.","Thanks to our high frequency resolution and the narrow scintles of this pulsar, we resolve scintillation arcs in the secondary spectra due to the increased Nyquist limit, which would not have been resolved at the same observing frequency with a traditional filterbank spectrum using NANOGrav's current time and frequency resolutions, and the frequency-dependent evolution of scintillation arc features within individual observations.","We observe the dimming of prominent arc features at higher frequencies, possibly due to a combination of decreasing flux density and undetermined effects due to the interstellar medium.","We also find agreement with arc curvature frequency dependence predicted by Stinebring et al.","(2001) in some epochs.","Thanks to the frequency resolution improvement provided by cyclic spectroscopy, these results show strong promise for future such analyses with millisecond pulsars, particularly for pulsar timing arrays, where such techniques can allow for detailed studies of the interstellar medium in highly scattered pulsars without sacrificing the timing resolution that is crucial to their gravitational wave detection efforts."],"url":"http://arxiv.org/abs/2404.13796v1","category":"astro-ph.HE"}
{"created":"2024-04-21 22:19:31","title":"Fourier transform on the locus of cyclic spectral curves in the Hitchin base","abstract":"We compute the Fourier transform of some of the summands of the push-forward of the constant sheaf under the Hitchin map for $SL_n$ restricted to the locus of cyclic spectral curves inside the Hitchin base (for $SL_2$ all spectral curves are cyclic) and give an estimate on the support of the Fourier transforms of the other summands.","sentences":["We compute the Fourier transform of some of the summands of the push-forward of the constant sheaf under the Hitchin map for $SL_n$ restricted to the locus of cyclic spectral curves inside the Hitchin base (for $SL_2$ all spectral curves are cyclic) and give an estimate on the support of the Fourier transforms of the other summands."],"url":"http://arxiv.org/abs/2404.13787v1","category":"math.AG"}
{"created":"2024-04-21 21:23:15","title":"The Characterization of Abstract Truth and its Factorization","abstract":"Human knowledge is made up of the conceptual structures of many communities of interest. In order to establish coherence in human knowledge representation, it is important to enable communication between the conceptual structures of different communities The conceptual structures of any particular community is representable in an ontology. Such a ontology provides a formal linguistic standard for that community. However, a standard community ontology is established for various purposes, and makes choices that force a given interpretation, while excluding others that may be equally valid for other purposes. Hence, a given representation is relative to the purpose for that representation. Due to this relativity of representation, in the larger scope of all human knowledge it is more important to standardize methods and frameworks for relating ontologies than to standardize any particular choice of ontology. The standardization of methods and frameworks is called the semantic integration of ontologies.","sentences":["Human knowledge is made up of the conceptual structures of many communities of interest.","In order to establish coherence in human knowledge representation, it is important to enable communication between the conceptual structures of different communities The conceptual structures of any particular community is representable in an ontology.","Such a ontology provides a formal linguistic standard for that community.","However, a standard community ontology is established for various purposes, and makes choices that force a given interpretation, while excluding others that may be equally valid for other purposes.","Hence, a given representation is relative to the purpose for that representation.","Due to this relativity of representation, in the larger scope of all human knowledge it is more important to standardize methods and frameworks for relating ontologies than to standardize any particular choice of ontology.","The standardization of methods and frameworks is called the semantic integration of ontologies."],"url":"http://arxiv.org/abs/2404.13782v1","category":"cs.LO"}
{"created":"2024-04-21 21:13:46","title":"Explainable Interfaces for Rapid Gaze-Based Interactions in Mixed Reality","abstract":"Gaze-based interactions offer a potential way for users to naturally engage with mixed reality (XR) interfaces. Black-box machine learning models enabled higher accuracy for gaze-based interactions. However, due to the black-box nature of the model, users might not be able to understand and effectively adapt their gaze behaviour to achieve high quality interaction. We posit that explainable AI (XAI) techniques can facilitate understanding of and interaction with gaze-based model-driven system in XR. To study this, we built a real-time, multi-level XAI interface for gaze-based interaction using a deep learning model, and evaluated it during a visual search task in XR. A between-subjects study revealed that participants who interacted with XAI made more accurate selections compared to those who did not use the XAI system (i.e., F1 score increase of 10.8%). Additionally, participants who used the XAI system adapted their gaze behavior over time to make more effective selections. These findings suggest that XAI can potentially be used to assist users in more effective collaboration with model-driven interactions in XR.","sentences":["Gaze-based interactions offer a potential way for users to naturally engage with mixed reality (XR) interfaces.","Black-box machine learning models enabled higher accuracy for gaze-based interactions.","However, due to the black-box nature of the model, users might not be able to understand and effectively adapt their gaze behaviour to achieve high quality interaction.","We posit that explainable AI (XAI) techniques can facilitate understanding of and interaction with gaze-based model-driven system in XR.","To study this, we built a real-time, multi-level XAI interface for gaze-based interaction using a deep learning model, and evaluated it during a visual search task in XR.","A between-subjects study revealed that participants who interacted with XAI made more accurate selections compared to those who did not use the XAI system (i.e., F1 score increase of 10.8%).","Additionally, participants who used the XAI system adapted their gaze behavior over time to make more effective selections.","These findings suggest that XAI can potentially be used to assist users in more effective collaboration with model-driven interactions in XR."],"url":"http://arxiv.org/abs/2404.13777v1","category":"cs.HC"}
{"created":"2024-04-21 20:17:34","title":"Dumbell Fermions and Fermi-Pauli Duality","abstract":"We use the Kantor-Susskind\\cite{kantsuss} model of fermions as \"dumbbells\" connecting points on a cubic lattice to points on its dual, to define a duality between local fermionic models invariant under a $Z_2$ gauge symmetry and models of bosonic variables (generalizations of Pauli matrices) defined on the lattice.","sentences":["We use the Kantor-Susskind\\cite{kantsuss} model of fermions as \"dumbbells\" connecting points on a cubic lattice to points on its dual, to define a duality between local fermionic models invariant under a $Z_2$ gauge symmetry and models of bosonic variables (generalizations of Pauli matrices) defined on the lattice."],"url":"http://arxiv.org/abs/2404.13761v1","category":"hep-th"}
{"created":"2024-04-21 19:42:28","title":"BC-MRI-SEG: A Breast Cancer MRI Tumor Segmentation Benchmark","abstract":"Binary breast cancer tumor segmentation with Magnetic Resonance Imaging (MRI) data is typically trained and evaluated on private medical data, which makes comparing deep learning approaches difficult. We propose a benchmark (BC-MRI-SEG) for binary breast cancer tumor segmentation based on publicly available MRI datasets. The benchmark consists of four datasets in total, where two datasets are used for supervised training and evaluation, and two are used for zero-shot evaluation. Additionally we compare state-of-the-art (SOTA) approaches on our benchmark and provide an exhaustive list of available public breast cancer MRI datasets. The source code has been made available at https://irulenot.github.io/BC_MRI_SEG_Benchmark.","sentences":["Binary breast cancer tumor segmentation with Magnetic Resonance Imaging (MRI) data is typically trained and evaluated on private medical data, which makes comparing deep learning approaches difficult.","We propose a benchmark (BC-MRI-SEG) for binary breast cancer tumor segmentation based on publicly available MRI datasets.","The benchmark consists of four datasets in total, where two datasets are used for supervised training and evaluation, and two are used for zero-shot evaluation.","Additionally we compare state-of-the-art (SOTA) approaches on our benchmark and provide an exhaustive list of available public breast cancer MRI datasets.","The source code has been made available at https://irulenot.github.io/BC_MRI_SEG_Benchmark."],"url":"http://arxiv.org/abs/2404.13756v1","category":"eess.IV"}
{"created":"2024-04-21 19:37:14","title":"Combining and Decoupling Rigid and Soft Grippers to Enhance Robotic Manipulation","abstract":"For robot arms to perform everyday tasks in unstructured environments, these robots must be able to manipulate a diverse range of objects. Today's robots often grasp objects with either soft grippers or rigid end-effectors. However, purely rigid or purely soft grippers have fundamental limitations: soft grippers struggle with irregular, heavy objects, while rigid grippers often cannot grasp small, numerous items. In this paper we therefore introduce RISOs, a mechanics and controls approach for unifying traditional RIgid end-effectors with a novel class of SOft adhesives. When grasping an object, RISOs can use either the rigid end-effector (pinching the item between non-deformable fingers) and/or the soft materials (attaching and releasing items with switchable adhesives). This enhances manipulation capabilities by combining and decoupling rigid and soft mechanisms. With RISOs robots can perform grasps along a spectrum from fully rigid, to fully soft, to rigid-soft, enabling real time object manipulation across a 1 million times range in weight (from 2 mg to 2 kg). To develop RISOs we first model and characterize the soft switchable adhesives. We then mount sheets of these soft adhesives on the surfaces of rigid end-effectors, and develop control strategies that make it easier for robot arms and human operators to utilize RISOs. The resulting RISO grippers were able to pick-up, carry, and release a larger set of objects than existing grippers, and participants also preferred using RISO. Overall, our experimental and user study results suggest that RISOs provide an exceptional gripper range in both capacity and object diversity. See videos of our user studies here: https://youtu.be/du085R0gPFI","sentences":["For robot arms to perform everyday tasks in unstructured environments, these robots must be able to manipulate a diverse range of objects.","Today's robots often grasp objects with either soft grippers or rigid end-effectors.","However, purely rigid or purely soft grippers have fundamental limitations: soft grippers struggle with irregular, heavy objects, while rigid grippers often cannot grasp small, numerous items.","In this paper we therefore introduce RISOs, a mechanics and controls approach for unifying traditional RIgid end-effectors with a novel class of SOft adhesives.","When grasping an object, RISOs can use either the rigid end-effector (pinching the item between non-deformable fingers) and/or the soft materials (attaching and releasing items with switchable adhesives).","This enhances manipulation capabilities by combining and decoupling rigid and soft mechanisms.","With RISOs robots can perform grasps along a spectrum from fully rigid, to fully soft, to rigid-soft, enabling real time object manipulation across a 1 million times range in weight (from 2 mg to 2 kg).","To develop RISOs we first model and characterize the soft switchable adhesives.","We then mount sheets of these soft adhesives on the surfaces of rigid end-effectors, and develop control strategies that make it easier for robot arms and human operators to utilize RISOs.","The resulting RISO grippers were able to pick-up, carry, and release a larger set of objects than existing grippers, and participants also preferred using RISO.","Overall, our experimental and user study results suggest that RISOs provide an exceptional gripper range in both capacity and object diversity.","See videos of our user studies here: https://youtu.be/du085R0gPFI"],"url":"http://arxiv.org/abs/2404.13755v1","category":"cs.RO"}
{"created":"2024-04-21 18:18:34","title":"Training-Conditional Coverage Bounds for Uniformly Stable Learning Algorithms","abstract":"The training-conditional coverage performance of the conformal prediction is known to be empirically sound. Recently, there have been efforts to support this observation with theoretical guarantees. The training-conditional coverage bounds for jackknife+ and full-conformal prediction regions have been established via the notion of $(m,n)$-stability by Liang and Barber~[2023]. Although this notion is weaker than uniform stability, it is not clear how to evaluate it for practical models. In this paper, we study the training-conditional coverage bounds of full-conformal, jackknife+, and CV+ prediction regions from a uniform stability perspective which is known to hold for empirical risk minimization over reproducing kernel Hilbert spaces with convex regularization. We derive coverage bounds for finite-dimensional models by a concentration argument for the (estimated) predictor function, and compare the bounds with existing ones under ridge regression.","sentences":["The training-conditional coverage performance of the conformal prediction is known to be empirically sound.","Recently, there have been efforts to support this observation with theoretical guarantees.","The training-conditional coverage bounds for jackknife+ and full-conformal prediction regions have been established via the notion of $(m,n)$-stability by Liang and Barber~[2023].","Although this notion is weaker than uniform stability, it is not clear how to evaluate it for practical models.","In this paper, we study the training-conditional coverage bounds of full-conformal, jackknife+, and CV+ prediction regions from a uniform stability perspective which is known to hold for empirical risk minimization over reproducing kernel Hilbert spaces with convex regularization.","We derive coverage bounds for finite-dimensional models by a concentration argument for the (estimated) predictor function, and compare the bounds with existing ones under ridge regression."],"url":"http://arxiv.org/abs/2404.13731v1","category":"stat.ML"}
{"created":"2024-04-21 18:14:46","title":"Frequency-dependent returns in nonlinear public goods games","abstract":"When individuals interact in groups, the evolution of cooperation is traditionally modeled using the framework of public goods games. Overwhelmingly, these models assume that the return of the public good depends linearly on the fraction of contributors. In contrast, it seems natural that in real life public goods interactions the return most likely depends on the size of the investor pool as well. Here, we consider a model to account for such nonlinearities in which the multiplication factor (marginal per capita return) for the public good depends on how many contribute. We find that nonlinear public goods interactions can break the curse of dominant defection in linear public goods interactions and give rise to richer dynamical outcomes in evolutionary settings. We provide an in-depth analysis of the more varied decisions by the classical rational player in nonlinear public goods interactions as well as a mechanistic, microscopic derivation of the evolutionary outcomes for the stochastic dynamics in finite populations and in the deterministic limit of infinite populations. This kind of nonlinearity provides a natural way to model public goods with diminishing returns as well as economies of scale.","sentences":["When individuals interact in groups, the evolution of cooperation is traditionally modeled using the framework of public goods games.","Overwhelmingly, these models assume that the return of the public good depends linearly on the fraction of contributors.","In contrast, it seems natural that in real life public goods interactions the return most likely depends on the size of the investor pool as well.","Here, we consider a model to account for such nonlinearities in which the multiplication factor (marginal per capita return) for the public good depends on how many contribute.","We find that nonlinear public goods interactions can break the curse of dominant defection in linear public goods interactions and give rise to richer dynamical outcomes in evolutionary settings.","We provide an in-depth analysis of the more varied decisions by the classical rational player in nonlinear public goods interactions as well as a mechanistic, microscopic derivation of the evolutionary outcomes for the stochastic dynamics in finite populations and in the deterministic limit of infinite populations.","This kind of nonlinearity provides a natural way to model public goods with diminishing returns as well as economies of scale."],"url":"http://arxiv.org/abs/2404.13728v1","category":"q-bio.PE"}
{"created":"2024-04-21 17:50:25","title":"Unique multistable states in periodic structures with saturable nonlinearity. I. Conventional case and unbroken $\\mathcal{PT}$-symmetric regime","abstract":"In this work, we predict that periodic structures without gain and loss do not exhibit an S-shaped hysteresis curve in the presence of saturable nonlinearity (SNL). Instead, the input-output characteristics of the system admit ramp-like optical bistability (OB) and multistability (OM) curves that are unprecedented in the context of conventional periodic structures in the literature. An increase in the nonlinearity (NL) or the gain-loss parameter increases the switch-up and down intensities of different stable branches in a ramp-like OM curve. Revival of the typical S-shaped hysteresis curve requires the device to work under the combined influence of frequency detuning and $\\mathcal{PT}$-symmetry. An increase in the detuning, NL and gain-loss parameters reduces the switching intensities of the S-shaped OB (OM) curves. During the process, mixed OM curves that feature a fusion between ramp-like and S-shaped OM curves emanate at low values of the detuning parameter in the input-output characteristics. The detuning parameter values for which ramp-like, S-shaped, and mixed OM appear varies with the NL coefficient. For a given range of input intensities, the number of stable states admitted by the system increases with the device length or NL. When the laser light enters the device from the opposite end of the grating, nonreciprocal switching occurs at ultra-low intensities via an interplay between NL, detuning, and gain-loss parameters.","sentences":["In this work, we predict that periodic structures without gain and loss do not exhibit an S-shaped hysteresis curve in the presence of saturable nonlinearity (SNL).","Instead, the input-output characteristics of the system admit ramp-like optical bistability (OB) and multistability (OM) curves that are unprecedented in the context of conventional periodic structures in the literature.","An increase in the nonlinearity (NL) or the gain-loss parameter increases the switch-up and down intensities of different stable branches in a ramp-like OM curve.","Revival of the typical S-shaped hysteresis curve requires the device to work under the combined influence of frequency detuning and $\\mathcal{PT}$-symmetry.","An increase in the detuning, NL and gain-loss parameters reduces the switching intensities of the S-shaped OB (OM) curves.","During the process, mixed OM curves that feature a fusion between ramp-like and S-shaped OM curves emanate at low values of the detuning parameter in the input-output characteristics.","The detuning parameter values for which ramp-like, S-shaped, and mixed OM appear varies with the NL coefficient.","For a given range of input intensities, the number of stable states admitted by the system increases with the device length or NL.","When the laser light enters the device from the opposite end of the grating, nonreciprocal switching occurs at ultra-low intensities via an interplay between NL, detuning, and gain-loss parameters."],"url":"http://arxiv.org/abs/2404.13724v1","category":"physics.optics"}
{"created":"2024-04-21 17:07:15","title":"Variable-Stepsize Implicit Peer Triplets in ODE Constrained Optimal Control","abstract":"This paper is concerned with the theory, construction and application of implicit Peer two-step methods that are super-convergent for variable stepsizes, i.e., preserve their classical order achieved for uniform stepsizes when applied to ODE constrained optimal control problems in a first-discretize-then-optimize setting. We upgrade our former implicit two-step Peer triplets constructed in [Algorithms, 15:310, 2022] to get ready for dynamical systems with varying time scales without loosing efficiency. Peer triplets consist of a standard Peer method for interior time steps supplemented by matching methods for the starting and end steps. A decisive advantage of Peer methods is their absence of order reduction since they use stages of the same high stage order. The consistency analysis of variable-stepsize implicit Peer methods results in additional order conditions and severe new difficulties for uniform zero-stability, which intensifies the demands on the Peer triplet. Further, we discuss the construction of 4-stage methods with order pairs (4,3) and (3,3) for state and adjoint variables in detail and provide four Peer triplets of practical interest. We rigorously prove convergence of order $s-1$ for $s$-stage Peer methods applied on grids with bounded or smoothly changing stepsize ratios. Numerical tests show the expected order of convergence for the new variable-stepsize Peer triplets.","sentences":["This paper is concerned with the theory, construction and application of implicit Peer two-step methods that are super-convergent for variable stepsizes, i.e., preserve their classical order achieved for uniform stepsizes when applied to ODE constrained optimal control problems in a first-discretize-then-optimize setting.","We upgrade our former implicit two-step Peer triplets constructed in [Algorithms, 15:310, 2022] to get ready for dynamical systems with varying time scales without loosing efficiency.","Peer triplets consist of a standard Peer method for interior time steps supplemented by matching methods for the starting and end steps.","A decisive advantage of Peer methods is their absence of order reduction since they use stages of the same high stage order.","The consistency analysis of variable-stepsize implicit Peer methods results in additional order conditions and severe new difficulties for uniform zero-stability, which intensifies the demands on the Peer triplet.","Further, we discuss the construction of 4-stage methods with order pairs (4,3) and (3,3) for state and adjoint variables in detail and provide four Peer triplets of practical interest.","We rigorously prove convergence of order $s-1$ for $s$-stage Peer methods applied on grids with bounded or smoothly changing stepsize ratios.","Numerical tests show the expected order of convergence for the new variable-stepsize Peer triplets."],"url":"http://arxiv.org/abs/2404.13716v1","category":"math.OC"}
{"created":"2024-04-21 16:40:33","title":"Vector-substrate-based Josephson junctions","abstract":"We present a way to We present a way to fabricate bicrystal Josephson junctions of high-Tc cuprate superconductors that does not require bulk bicrystalline substrates. Based on vector substrate technology, this novel approach makes use of a few tens-of-nanometers-thick bicrystalline membranes transferred onto conventional substrates.We demonstrate 24{\\deg} YBa2Cu3O7-x Josephson junctions fabricated on sapphire single crystals by utilizing 10-nm-thick bicrystalline membranes of SrTiO3. This technique allows one to manufacture bicrystalline Josephson junctions of high-Tc superconductors on a large variety of bulk substrate materials, providing novel degrees of freedom in designing the junctions and their electronic properties. It furthermore offers the capability to replace the fabrication of bulk bicrystalline substrates with thin-film growth methods","sentences":["We present a way to We present a way to fabricate bicrystal Josephson junctions of high-Tc cuprate superconductors that does not require bulk bicrystalline substrates.","Based on vector substrate technology, this novel approach makes use of a few tens-of-nanometers-thick bicrystalline membranes transferred onto conventional substrates.","We demonstrate 24{\\deg} YBa2Cu3O7-x Josephson junctions fabricated on sapphire single crystals by utilizing 10-nm-thick bicrystalline membranes of SrTiO3.","This technique allows one to manufacture bicrystalline Josephson junctions of high-Tc superconductors on a large variety of bulk substrate materials, providing novel degrees of freedom in designing the junctions and their electronic properties.","It furthermore offers the capability to replace the fabrication of bulk bicrystalline substrates with thin-film growth methods"],"url":"http://arxiv.org/abs/2404.13708v1","category":"cond-mat.supr-con"}
{"created":"2024-04-21 16:35:24","title":"Robust inference for the unification of confidence intervals in meta-analysis","abstract":"Traditional meta-analysis assumes that the effect sizes estimated in individual studies follow a Gaussian distribution. However, this distributional assumption is not always satisfied in practice, leading to potentially biased results. In the situation when the number of studies, denoted as K, is large, the cumulative Gaussian approximation errors from each study could make the final estimation unreliable. In the situation when K is small, it is not realistic to assume the random-effect follows Gaussian distribution. In this paper, we present a novel empirical likelihood method for combining confidence intervals under the meta-analysis framework. This method is free of the Gaussian assumption in effect size estimates from individual studies and from the random-effects. We establish the large-sample properties of the non-parametric estimator, and introduce a criterion governing the relationship between the number of studies, K, and the sample size of each study, n_i. Our methodology supersedes conventional meta-analysis techniques in both theoretical robustness and computational efficiency. We assess the performance of our proposed methods using simulation studies, and apply our proposed methods to two examples.","sentences":["Traditional meta-analysis assumes that the effect sizes estimated in individual studies follow a Gaussian distribution.","However, this distributional assumption is not always satisfied in practice, leading to potentially biased results.","In the situation when the number of studies, denoted as K, is large, the cumulative Gaussian approximation errors from each study could make the final estimation unreliable.","In the situation when K is small, it is not realistic to assume the random-effect follows Gaussian distribution.","In this paper, we present a novel empirical likelihood method for combining confidence intervals under the meta-analysis framework.","This method is free of the Gaussian assumption in effect size estimates from individual studies and from the random-effects.","We establish the large-sample properties of the non-parametric estimator, and introduce a criterion governing the relationship between the number of studies, K, and the sample size of each study, n_i.","Our methodology supersedes conventional meta-analysis techniques in both theoretical robustness and computational efficiency.","We assess the performance of our proposed methods using simulation studies, and apply our proposed methods to two examples."],"url":"http://arxiv.org/abs/2404.13707v1","category":"stat.ME"}
{"created":"2024-04-21 16:34:39","title":"Controlling quantum vortex dynamics and vortex-antivortex annihilation in Bose-Einstein condensates with optical lattices","abstract":"Superfluids with strong spatial modulation can be experimentally produced in the area of cold atoms under the influence of optical lattices. Here we address $^{87}$Rb bosons at T=0 K in a flat geometry under the influence of a periodic potential with the Gross-Pitaevskii theory. The statics and dynamics of vortex excitations are studied in the case of one dimensional (1D) and of two dimensional (2D) optical lattices, as function of the intensity of the optical lattice. We compute how the vortex energy depends on the position of its core and the energy barrier that a vortex has to surmount in order to move in the superfluid. The dynamics of a vortex dipole, a pair of vortices of opposite chirality, differ profoundly from the case of a uniform superfluid. In the 1D case, when parallel ridges of density are present, the dynamics depends on the positions of the two vortices. If they are in the same channel between two ridges, then the two vortices approach each other until they annihilate each other in a short time. If the two vortices are in distinct channels the dipole undergoes a rigid translation but with a velocity depending on the intensity of the optical lattice and this translation velocity can even change sign with respect to the case of the uniform superfluid. Superimposed on this translation an oscillatory motion is also present. These oscillatory motions can be both longitudinal, i.e. along the channel, as well as transverse. In all cases the transverse motions are one-side, in the sense that the vortex core never crosses the equilibrium position nearest the starting position. In the case of the 2D lattices we study (square, triangular and honeycomb), the two vortices of a dipole move mainly by jumps between equilibrium positions and approach each other until annihilation.","sentences":["Superfluids with strong spatial modulation can be experimentally produced in the area of cold atoms under the influence of optical lattices.","Here we address $^{87}$Rb bosons at T=0 K in a flat geometry under the influence of a periodic potential with the Gross-Pitaevskii theory.","The statics and dynamics of vortex excitations are studied in the case of one dimensional (1D) and of two dimensional (2D) optical lattices, as function of the intensity of the optical lattice.","We compute how the vortex energy depends on the position of its core and the energy barrier that a vortex has to surmount in order to move in the superfluid.","The dynamics of a vortex dipole, a pair of vortices of opposite chirality, differ profoundly from the case of a uniform superfluid.","In the 1D case, when parallel ridges of density are present, the dynamics depends on the positions of the two vortices.","If they are in the same channel between two ridges, then the two vortices approach each other until they annihilate each other in a short time.","If the two vortices are in distinct channels the dipole undergoes a rigid translation but with a velocity depending on the intensity of the optical lattice and this translation velocity can even change sign with respect to the case of the uniform superfluid.","Superimposed on this translation an oscillatory motion is also present.","These oscillatory motions can be both longitudinal, i.e. along the channel, as well as transverse.","In all cases the transverse motions are one-side, in the sense that the vortex core never crosses the equilibrium position nearest the starting position.","In the case of the 2D lattices we study (square, triangular and honeycomb), the two vortices of a dipole move mainly by jumps between equilibrium positions and approach each other until annihilation."],"url":"http://arxiv.org/abs/2404.13705v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-21 16:16:56","title":"Learning Galaxy Intrinsic Alignment Correlations","abstract":"The intrinsic alignments (IA) of galaxies, regarded as a contaminant in weak lensing analyses, represents the correlation of galaxy shapes due to gravitational tidal interactions and galaxy formation processes. As such, understanding IA is paramount for accurate cosmological inferences from weak lensing surveys; however, one limitation to our understanding and mitigation of IA is expensive simulation-based modeling. In this work, we present a deep learning approach to emulate galaxy position-position ($\\xi$), position-orientation ($\\omega$), and orientation-orientation ($\\eta$) correlation function measurements and uncertainties from halo occupation distribution-based mock galaxy catalogs. We find strong Pearson correlation values with the model across all three correlation functions and further predict aleatoric uncertainties through a mean-variance estimation training procedure. $\\xi(r)$ predictions are generally accurate to $\\leq10\\%$. Our model also successfully captures the underlying signal of the noisier correlations $\\omega(r)$ and $\\eta(r)$, although with a lower average accuracy. We find that the model performance is inhibited by the stochasticity of the data, and will benefit from correlations averaged over multiple data realizations. Our code will be made open source upon journal publication.","sentences":["The intrinsic alignments (IA) of galaxies, regarded as a contaminant in weak lensing analyses, represents the correlation of galaxy shapes due to gravitational tidal interactions and galaxy formation processes.","As such, understanding IA is paramount for accurate cosmological inferences from weak lensing surveys; however, one limitation to our understanding and mitigation of IA is expensive simulation-based modeling.","In this work, we present a deep learning approach to emulate galaxy position-position ($\\xi$), position-orientation ($\\omega$), and orientation-orientation ($\\eta$) correlation function measurements and uncertainties from halo occupation distribution-based mock galaxy catalogs.","We find strong Pearson correlation values with the model across all three correlation functions and further predict aleatoric uncertainties through a mean-variance estimation training procedure.","$\\xi(r)$ predictions are generally accurate to $\\leq10\\%$. Our model also successfully captures the underlying signal of the noisier correlations $\\omega(r)$ and $\\eta(r)$, although with a lower average accuracy.","We find that the model performance is inhibited by the stochasticity of the data, and will benefit from correlations averaged over multiple data realizations.","Our code will be made open source upon journal publication."],"url":"http://arxiv.org/abs/2404.13702v1","category":"astro-ph.CO"}
{"created":"2024-04-21 15:55:00","title":"Exponential Quantum One-Wayness and EFI Pairs","abstract":"In classical cryptography, one-way functions are widely considered to be the minimal computational assumption. However, when taking quantum information into account, the situation is more nuanced. There are currently two major candidates for the minimal assumption: the search quantum generalization of one-way functions are one-way state generators (OWSG), whereas the decisional variant are EFI pairs. A well-known open problem in quantum cryptography is to understand how these two primitives are related. A recent breakthrough result of Khurana and Tomer (STOC'24) shows that OWSGs imply EFI pairs, for the restricted case of pure states.   In this work, we make progress towards understanding the general case. To this end, we define the notion of inefficiently-verifiable one-way state generators (IV-OWSGs), where the verification algorithm is not required to be efficient, and show that these are precisely equivalent to EFI pairs, with an exponential loss in the reduction. Significantly, this equivalence holds also for mixed states. Thus our work establishes the following relations among these fundamental primitives of quantum cryptography: (mixed) OWSGs => (mixed) IV-OWSGs $\\equiv_{\\rm exp}$ EFI pairs, where $\\equiv_{\\rm exp}$ denotes equivalence up to exponential security of the primitives.","sentences":["In classical cryptography, one-way functions are widely considered to be the minimal computational assumption.","However, when taking quantum information into account, the situation is more nuanced.","There are currently two major candidates for the minimal assumption: the search quantum generalization of one-way functions are one-way state generators (OWSG), whereas the decisional variant are EFI pairs.","A well-known open problem in quantum cryptography is to understand how these two primitives are related.","A recent breakthrough result of Khurana and Tomer (STOC'24) shows that OWSGs imply EFI pairs, for the restricted case of pure states.   ","In this work, we make progress towards understanding the general case.","To this end, we define the notion of inefficiently-verifiable one-way state generators (IV-OWSGs), where the verification algorithm is not required to be efficient, and show that these are precisely equivalent to EFI pairs, with an exponential loss in the reduction.","Significantly, this equivalence holds also for mixed states.","Thus our work establishes the following relations among these fundamental primitives of quantum cryptography: (mixed) OWSGs => (mixed) IV-OWSGs $\\equiv_{\\rm exp}$ EFI pairs, where $\\equiv_{\\rm exp}$ denotes equivalence up to exponential security of the primitives."],"url":"http://arxiv.org/abs/2404.13699v1","category":"quant-ph"}
{"created":"2024-04-21 15:50:11","title":"Tunable Nanoislands Decorated Tapered Optical Fibers Reveal Concurrent Contributions in Through-Fiber SERS Detection","abstract":"Creating plasmonic nanoparticles on a tapered optical fiber tip enables a remote SERS sensing probe, ideal for challenging sampling scenarios like biological tissue, specific cells, on-site environmental monitoring, and deep brain structures. However, nanoparticle patterns fabricated from current bottom-up methods are mostly random, making geometry control difficult. Uneven statistical distribution, clustering, and multilayer deposition introduce uncertainty in correlating device performance with morphology. Here, we employ a tunable solid-state dewetting method to create densely packed monolayer Au nanoislands (NIs) with varied geometric parameters, directly contacting the silica TF surface. These patterns exhibit analyzable nanoparticle sizes, densities, and uniform distribution across the entire taper surface, enabling a systematic investigation of particle size, density, and analyte effects on the SERS performance of the through-fiber detection system. The study is focused on the SERS response of a widely employed benchmark Rhodamine 6G molecule and Serotonin, a neurotransmitter with high relevance for the neuroscience field. The numerical simulations and limit of detection (LOD) experiments on R6G show that the increase of the total near-field enhancement volume promotes the SERS sensitivity of the probe. However, for serotonin we observed a different behavior linked to its interaction with the nanoparticle's surface. The obtained LOD is as low as 10-7 M, a value not achieved so far in a through-fiber detection scheme. Therefore, we believe our work offers a strategy to design nanoparticle-based remote SERS sensing probes and provide new clues to discover and understand the intricate plasmonic-driven chemical reactions.","sentences":["Creating plasmonic nanoparticles on a tapered optical fiber tip enables a remote SERS sensing probe, ideal for challenging sampling scenarios like biological tissue, specific cells, on-site environmental monitoring, and deep brain structures.","However, nanoparticle patterns fabricated from current bottom-up methods are mostly random, making geometry control difficult.","Uneven statistical distribution, clustering, and multilayer deposition introduce uncertainty in correlating device performance with morphology.","Here, we employ a tunable solid-state dewetting method to create densely packed monolayer Au nanoislands (NIs) with varied geometric parameters, directly contacting the silica TF surface.","These patterns exhibit analyzable nanoparticle sizes, densities, and uniform distribution across the entire taper surface, enabling a systematic investigation of particle size, density, and analyte effects on the SERS performance of the through-fiber detection system.","The study is focused on the SERS response of a widely employed benchmark Rhodamine 6G molecule and Serotonin, a neurotransmitter with high relevance for the neuroscience field.","The numerical simulations and limit of detection (LOD) experiments on R6G show that the increase of the total near-field enhancement volume promotes the SERS sensitivity of the probe.","However, for serotonin we observed a different behavior linked to its interaction with the nanoparticle's surface.","The obtained LOD is as low as 10-7 M, a value not achieved so far in a through-fiber detection scheme.","Therefore, we believe our work offers a strategy to design nanoparticle-based remote SERS sensing probes and provide new clues to discover and understand the intricate plasmonic-driven chemical reactions."],"url":"http://arxiv.org/abs/2404.13695v1","category":"physics.optics"}
{"created":"2024-04-21 15:42:56","title":"PV-S3: Advancing Automatic Photovoltaic Defect Detection using Semi-Supervised Semantic Segmentation of Electroluminescence Images","abstract":"Photovoltaic (PV) systems allow us to tap into all abundant solar energy, however they require regular maintenance for high efficiency and to prevent degradation. Traditional manual health check, using Electroluminescence (EL) imaging, is expensive and logistically challenging making automated defect detection essential. Current automation approaches require extensive manual expert labeling, which is time-consuming, expensive, and prone to errors. We propose PV-S3 (Photovoltaic-Semi Supervised Segmentation), a Semi-Supervised Learning approach for semantic segmentation of defects in EL images that reduces reliance on extensive labeling. PV-S3 is a Deep learning model trained using a few labeled images along with numerous unlabeled images. We introduce a novel Semi Cross-Entropy loss function to train PV-S3 which addresses the challenges specific to automated PV defect detection, such as diverse defect types and class imbalance. We evaluate PV-S3 on multiple datasets and demonstrate its effectiveness and adaptability. With merely 20% labeled samples, we achieve an absolute improvement of 9.7% in IoU, 29.9% in Precision, 12.75% in Recall, and 20.42% in F1-Score over prior state-of-the-art supervised method (which uses 100% labeled samples) on UCF-EL dataset (largest dataset available for semantic segmentation of EL images) showing improvement in performance while reducing the annotation costs by 80%.","sentences":["Photovoltaic (PV) systems allow us to tap into all abundant solar energy, however they require regular maintenance for high efficiency and to prevent degradation.","Traditional manual health check, using Electroluminescence (EL) imaging, is expensive and logistically challenging making automated defect detection essential.","Current automation approaches require extensive manual expert labeling, which is time-consuming, expensive, and prone to errors.","We propose PV-S3 (Photovoltaic-Semi Supervised Segmentation), a Semi-Supervised Learning approach for semantic segmentation of defects in EL images that reduces reliance on extensive labeling.","PV-S3 is a Deep learning model trained using a few labeled images along with numerous unlabeled images.","We introduce a novel Semi Cross-Entropy loss function to train PV-S3 which addresses the challenges specific to automated PV defect detection, such as diverse defect types and class imbalance.","We evaluate PV-S3 on multiple datasets and demonstrate its effectiveness and adaptability.","With merely 20% labeled samples, we achieve an absolute improvement of 9.7% in IoU, 29.9% in Precision, 12.75% in Recall, and 20.42% in F1-Score over prior state-of-the-art supervised method (which uses 100% labeled samples) on UCF-EL dataset (largest dataset available for semantic segmentation of EL images) showing improvement in performance while reducing the annotation costs by 80%."],"url":"http://arxiv.org/abs/2404.13693v1","category":"eess.IV"}
{"created":"2024-04-21 14:45:51","title":"$\\mathsf{QuITO}$ $\\textsf{v.2}$: Trajectory Optimization with Uniform Error Guarantees under Path Constraints","abstract":"This article introduces a new transcription, change point localization, and mesh refinement scheme for direct optimization-based solutions and for uniform approximation of optimal control trajectories associated with a class of nonlinear constrained optimal control problems (OCPs). The base transcription algorithm for which we establish the refinement algorithm is a $\\textit{direct multiple shooting technique}$ -- $\\mathsf{QuITO}$ $\\textsf{v.2}$ (Quasi-Interpolation based Trajectory Optimization). The mesh refinement technique consists of two steps -- localization of certain irregular regions in an optimal control trajectory via wavelets, followed by a targeted $h$-refinement approach around such regions of irregularity. Theoretical approximation guarantees on uniform grids are presented for optimal controls with certain regularity properties along with guarantees of localization of change points by wavelet transform. Numerical illustrations are provided for control profiles involving discontinuities to show the effectiveness of the localization and refinement strategy. We also announce and make freely available a new software package developed based on $\\mathsf{QuITO}$ $\\textsf{v.2}$ along with its functionalities to make the article complete.","sentences":["This article introduces a new transcription, change point localization, and mesh refinement scheme for direct optimization-based solutions and for uniform approximation of optimal control trajectories associated with a class of nonlinear constrained optimal control problems (OCPs).","The base transcription algorithm for which we establish the refinement algorithm is a $\\textit{direct multiple shooting technique}$ -- $\\mathsf{QuITO}$ $\\textsf{v.2}$ (Quasi-Interpolation based Trajectory Optimization).","The mesh refinement technique consists of two steps -- localization of certain irregular regions in an optimal control trajectory via wavelets, followed by a targeted $h$-refinement approach around such regions of irregularity.","Theoretical approximation guarantees on uniform grids are presented for optimal controls with certain regularity properties along with guarantees of localization of change points by wavelet transform.","Numerical illustrations are provided for control profiles involving discontinuities to show the effectiveness of the localization and refinement strategy.","We also announce and make freely available a new software package developed based on $\\mathsf{QuITO}$ $\\textsf{v.2}$ along with its functionalities to make the article complete."],"url":"http://arxiv.org/abs/2404.13681v1","category":"math.OC"}
{"created":"2024-04-21 14:24:57","title":"In-situ process monitoring and adaptive quality enhancement in laser additive manufacturing: a critical review","abstract":"Laser Additive Manufacturing (LAM) presents unparalleled opportunities for fabricating complex, high-performance structures and components with unique material properties. Despite these advancements, achieving consistent part quality and process repeatability remains challenging. This paper provides a comprehensive review of various state-of-the-art in-situ process monitoring techniques, including optical-based monitoring, acoustic-based sensing, laser line scanning, and operando X-ray monitoring. These techniques are evaluated for their capabilities and limitations in detecting defects within Laser Powder Bed Fusion (LPBF) and Laser Directed Energy Deposition (LDED) processes. Furthermore, the review discusses emerging multisensor monitoring and machine learning (ML)-assisted defect detection methods, benchmarking ML models tailored for in-situ defect detection. The paper also discusses in-situ adaptive defect remediation strategies that advance LAM towards zero-defect autonomous operations, focusing on real-time closed-loop feedback control and defect correction methods. Research gaps such as the need for standardization, improved reliability and sensitivity, and decision-making strategies beyond early stopping are highlighted. Future directions are proposed, with an emphasis on multimodal sensor fusion for multiscale defect prediction and fault diagnosis, ultimately enabling self-adaptation in LAM processes. This paper aims to equip researchers and industry professionals with a holistic understanding of the current capabilities, limitations, and future directions in in-situ process monitoring and adaptive quality enhancement in LAM.","sentences":["Laser Additive Manufacturing (LAM) presents unparalleled opportunities for fabricating complex, high-performance structures and components with unique material properties.","Despite these advancements, achieving consistent part quality and process repeatability remains challenging.","This paper provides a comprehensive review of various state-of-the-art in-situ process monitoring techniques, including optical-based monitoring, acoustic-based sensing, laser line scanning, and operando X-ray monitoring.","These techniques are evaluated for their capabilities and limitations in detecting defects within Laser Powder Bed Fusion (LPBF) and Laser Directed Energy Deposition (LDED) processes.","Furthermore, the review discusses emerging multisensor monitoring and machine learning (ML)-assisted defect detection methods, benchmarking ML models tailored for in-situ defect detection.","The paper also discusses in-situ adaptive defect remediation strategies that advance LAM towards zero-defect autonomous operations, focusing on real-time closed-loop feedback control and defect correction methods.","Research gaps such as the need for standardization, improved reliability and sensitivity, and decision-making strategies beyond early stopping are highlighted.","Future directions are proposed, with an emphasis on multimodal sensor fusion for multiscale defect prediction and fault diagnosis, ultimately enabling self-adaptation in LAM processes.","This paper aims to equip researchers and industry professionals with a holistic understanding of the current capabilities, limitations, and future directions in in-situ process monitoring and adaptive quality enhancement in LAM."],"url":"http://arxiv.org/abs/2404.13673v1","category":"eess.SP"}
{"created":"2024-04-21 14:18:49","title":"Rate Analysis of Coupled Distributed Stochastic Approximation for Misspecified Optimization","abstract":"We consider an $n$ agents distributed optimization problem with imperfect information characterized in a parametric sense, where the unknown parameter can be solved by a distinct distributed parameter learning problem. Though each agent only has access to its local parameter learning and computational problem, they mean to collaboratively minimize the average of their local cost functions. To address the special optimization problem, we propose a coupled distributed stochastic approximation algorithm, in which every agent updates the current beliefs of its unknown parameter and decision variable by stochastic approximation method; and then averages the beliefs and decision variables of its neighbors over network in consensus protocol. Our interest lies in the convergence analysis of this algorithm. We quantitatively characterize the factors that affect the algorithm performance, and prove that the mean-squared error of the decision variable is bounded by $\\mathcal{O}(\\frac{1}{nk})+\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}(1-\\rho_w)}\\right)\\frac{1}{k^{1.5}}+\\mathcal{O}\\big(\\frac{1}{(1-\\rho_w)^2} \\big)\\frac{1}{k^2}$, where $k$ is the iteration count and $(1-\\rho_w)$ is the spectral gap of the network weighted adjacency matrix. It reveals that the network connectivity characterized by $(1-\\rho_w)$ only influences the high order of convergence rate, while the domain rate still acts the same as the centralized algorithm. In addition, we analyze that the transient iteration needed for reaching its dominant rate $\\mathcal{O}(\\frac{1}{nk})$ is $\\mathcal{O}(\\frac{n}{(1-\\rho_w)^2})$. Numerical experiments are carried out to demonstrate the theoretical results by taking different CPUs as agents, which is more applicable to real-world distributed scenarios.","sentences":["We consider an $n$ agents distributed optimization problem with imperfect information characterized in a parametric sense, where the unknown parameter can be solved by a distinct distributed parameter learning problem.","Though each agent only has access to its local parameter learning and computational problem, they mean to collaboratively minimize the average of their local cost functions.","To address the special optimization problem, we propose a coupled distributed stochastic approximation algorithm, in which every agent updates the current beliefs of its unknown parameter and decision variable by stochastic approximation method; and then averages the beliefs and decision variables of its neighbors over network in consensus protocol.","Our interest lies in the convergence analysis of this algorithm.","We quantitatively characterize the factors that affect the algorithm performance, and prove that the mean-squared error of the decision variable is bounded by $\\mathcal{O}(\\frac{1}{nk})+\\mathcal{O}\\left(\\frac{1}{\\sqrt{n}(1-\\rho_w)}\\right)\\frac{1}{k^{1.5}}+\\mathcal{O}\\big(\\frac{1}{(1-\\rho_w)^2} \\big)\\frac{1}{k^2}$, where $k$ is the iteration count and $(1-\\rho_w)$ is the spectral gap of the network weighted adjacency matrix.","It reveals that the network connectivity characterized by $(1-\\rho_w)$ only influences the high order of convergence rate, while the domain rate still acts the same as the centralized algorithm.","In addition, we analyze that the transient iteration needed for reaching its dominant rate $\\mathcal{O}(\\frac{1}{nk})$ is $\\mathcal{O}(\\frac{n}{(1-\\rho_w)^2})$. Numerical experiments are carried out to demonstrate the theoretical results by taking different CPUs as agents, which is more applicable to real-world distributed scenarios."],"url":"http://arxiv.org/abs/2404.13669v1","category":"math.OC"}
{"created":"2024-04-21 13:26:25","title":"Tripartite multiphoton Jaynes-Cummings model: analytical solution and Wigner nonclassicalities","abstract":"We investigate a generic tripartite quantum system featuring a single qubit interacting concurrently with two quantized harmonic oscillators via nonlinear multiphoton Jaynes-Cummings (MPJC) interactions. Assuming the qubit is initially prepared in a superposition state and the two oscillators are in arbitrary Fock states, we analytically trace the temporal evolution of this tripartite pure initial state. We identify four broad cases, each further divided into two subcases, and derive exact analytical solutions for most cases. Notably, we obtain perfect swapping of arbitrary Fock states between the oscillators by carefully selecting system parameters. In addition, we extensively examine the manner in which the nonclassicalities of various initial oscillator Fock states, quantified by the volume of negative regions in the associated Wigner functions, evolve under the MPJC Hamiltonian, considering diverse system parameters including environmentally induced effects. Besides producing substantial enhancements in the initial value for higher photon number states, our analysis reveals that driven solely by the initial qubit energy, with both oscillators initialized in the vacuum state, the nonlinear MPJC interaction yields nontrivial Wigner negativities in the oscillators. The additional nonlinearity introduced by the multiphoton process plays a pivotal role in surpassing the initial nonclassicalities of the photon number states.","sentences":["We investigate a generic tripartite quantum system featuring a single qubit interacting concurrently with two quantized harmonic oscillators via nonlinear multiphoton Jaynes-Cummings (MPJC) interactions.","Assuming the qubit is initially prepared in a superposition state and the two oscillators are in arbitrary Fock states, we analytically trace the temporal evolution of this tripartite pure initial state.","We identify four broad cases, each further divided into two subcases, and derive exact analytical solutions for most cases.","Notably, we obtain perfect swapping of arbitrary Fock states between the oscillators by carefully selecting system parameters.","In addition, we extensively examine the manner in which the nonclassicalities of various initial oscillator Fock states, quantified by the volume of negative regions in the associated Wigner functions, evolve under the MPJC Hamiltonian, considering diverse system parameters including environmentally induced effects.","Besides producing substantial enhancements in the initial value for higher photon number states, our analysis reveals that driven solely by the initial qubit energy, with both oscillators initialized in the vacuum state, the nonlinear MPJC interaction yields nontrivial Wigner negativities in the oscillators.","The additional nonlinearity introduced by the multiphoton process plays a pivotal role in surpassing the initial nonclassicalities of the photon number states."],"url":"http://arxiv.org/abs/2404.13658v1","category":"quant-ph"}
{"created":"2024-04-21 13:05:29","title":"Ergodic and chaotic properties in Tavis-Cummings dimer: quantum and classical limit","abstract":"We investigate two key aspects of quantum systems by using the Tavis-Cummings dimer system as a platform. The first aspect involves unraveling the relationship between the phenomenon of self-trapping (or lack thereof) and integrability (or quantum chaos). Secondly, we uncover {the possibility of} mixed behavior in this quantum system using diagnostics based on random matrix theory and make an in-depth study of classical-quantum correspondence. The setup chosen for the study is precisely suited as it (i) enables a transition from delocalized to self-trapped states and (ii) has a well-defined classical limit, thereby amenable to studies involving classical-quantum conjectures. The obtained classical model in itself has rich chaotic and ergodic properties which were probed via maximal Lyapunov exponents. Furthermore, we present aspects of chaos in the corresponding open quantum system and make connections with non-Hermitian random matrix theory.","sentences":["We investigate two key aspects of quantum systems by using the Tavis-Cummings dimer system as a platform.","The first aspect involves unraveling the relationship between the phenomenon of self-trapping (or lack thereof) and integrability (or quantum chaos).","Secondly, we uncover {the possibility of} mixed behavior in this quantum system using diagnostics based on random matrix theory and make an in-depth study of classical-quantum correspondence.","The setup chosen for the study is precisely suited as it (i) enables a transition from delocalized to self-trapped states and (ii) has a well-defined classical limit, thereby amenable to studies involving classical-quantum conjectures.","The obtained classical model in itself has rich chaotic and ergodic properties which were probed via maximal Lyapunov exponents.","Furthermore, we present aspects of chaos in the corresponding open quantum system and make connections with non-Hermitian random matrix theory."],"url":"http://arxiv.org/abs/2404.13653v1","category":"quant-ph"}
{"created":"2024-04-21 12:49:12","title":"Mean Aggregator Is More Robust Than Robust Aggregators Under Label Poisoning Attacks","abstract":"Robustness to malicious attacks is of paramount importance for distributed learning. Existing works often consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process. To defend against such worst-case Byzantine attacks, various robust aggregators have been proven effective and much superior to the often-used mean aggregator. In this paper, we show that robust aggregators are too conservative for a class of weak but practical malicious attacks, as known as label poisoning attacks, where the sample labels of some workers are poisoned. Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous. In fact, the learning error of the mean aggregator is proven to be optimal in order. Experimental results corroborate our theoretical findings, demonstrating the superiority of the mean aggregator under label poisoning attacks.","sentences":["Robustness to malicious attacks is of paramount importance for distributed learning.","Existing works often consider the classical Byzantine attacks model, which assumes that some workers can send arbitrarily malicious messages to the server and disturb the aggregation steps of the distributed learning process.","To defend against such worst-case Byzantine attacks, various robust aggregators have been proven effective and much superior to the often-used mean aggregator.","In this paper, we show that robust aggregators are too conservative for a class of weak but practical malicious attacks, as known as label poisoning attacks, where the sample labels of some workers are poisoned.","Surprisingly, we are able to show that the mean aggregator is more robust than the state-of-the-art robust aggregators in theory, given that the distributed data are sufficiently heterogeneous.","In fact, the learning error of the mean aggregator is proven to be optimal in order.","Experimental results corroborate our theoretical findings, demonstrating the superiority of the mean aggregator under label poisoning attacks."],"url":"http://arxiv.org/abs/2404.13647v1","category":"cs.LG"}
{"created":"2024-04-21 12:41:02","title":"PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical Structure","abstract":"In this work, we propose a novel tree-based explanation technique, PEACH (Pretrained-embedding Explanation Across Contextual and Hierarchical Structure), that can explain how text-based documents are classified by using any pretrained contextual embeddings in a tree-based human-interpretable manner. Note that PEACH can adopt any contextual embeddings of the PLMs as a training input for the decision tree. Using the proposed PEACH, we perform a comprehensive analysis of several contextual embeddings on nine different NLP text classification benchmarks. This analysis demonstrates the flexibility of the model by applying several PLM contextual embeddings, its attribute selections, scaling, and clustering methods. Furthermore, we show the utility of explanations by visualising the feature selection and important trend of text classification via human-interpretable word-cloud-based trees, which clearly identify model mistakes and assist in dataset debugging. Besides interpretability, PEACH outperforms or is similar to those from pretrained models.","sentences":["In this work, we propose a novel tree-based explanation technique, PEACH (Pretrained-embedding Explanation Across Contextual and Hierarchical Structure), that can explain how text-based documents are classified by using any pretrained contextual embeddings in a tree-based human-interpretable manner.","Note that PEACH can adopt any contextual embeddings of the PLMs as a training input for the decision tree.","Using the proposed PEACH, we perform a comprehensive analysis of several contextual embeddings on nine different NLP text classification benchmarks.","This analysis demonstrates the flexibility of the model by applying several PLM contextual embeddings, its attribute selections, scaling, and clustering methods.","Furthermore, we show the utility of explanations by visualising the feature selection and important trend of text classification via human-interpretable word-cloud-based trees, which clearly identify model mistakes and assist in dataset debugging.","Besides interpretability, PEACH outperforms or is similar to those from pretrained models."],"url":"http://arxiv.org/abs/2404.13645v1","category":"cs.CL"}
{"created":"2024-04-21 12:40:18","title":"Well behaved class of Heintzmann's solution within $f(R,\\,T)$ framework","abstract":"The primary objective of this paper is to develop a well-behaved class of Heintzmann IIa [{\\em H. Heintzmann, Z. Physik 228, 489-493 (1969)}] solution in the context of $f(R,\\, T)$ gravity. In the $f(R, T)$ framework, the gravitational action includes both the Ricci scalar ($R$) and the trace of the energy-momentum tensor ($T$). We chose a particular $f(R,\\,T)$ model s.t. $f(R,\\,T) = R+2 \\chi T$, where $\\chi$ is known as the coupling parameter. This solution describes a novel isotropic compact fluid sphere with positively finite central pressure and density in this extended theory of gravity. The results obtained analytically are better described by graphical representations of the physical parameters for various values of the coupling parameter $\\chi$. The solution for a specific compact object, Vela X-1, with radius $\\mathfrak{R} = 9.56_{-0.08}^{+0.08}$ km and mass $\\mathcal{M} = 1.77 \\pm 0.08~\\mathcal{M}_{\\odot}$ [{\\em M. L. Rawls et al. ApJ, 730, 25 (2011)}], is shown here. We analyze the fundamental physical attributes of the star, which reveals the influence of the coupling parameter $\\chi$ on the values of substance parameters. This helps us to make a fruitful comparison of this modified $f(R,\\, T)$ gravity with the standard GR and notice that it holds good for stable compact objects. In this framework, the star under our consideration exhibits a stable structure consistent with the Heintzmann IIa {\\em ansatz}. From all of our obtained graphical and numerical results, we can ultimately conclude that our reported model is physically admissible and satisfies all the physical criteria for an acceptable model.","sentences":["The primary objective of this paper is to develop a well-behaved class of Heintzmann IIa [{\\em H. Heintzmann, Z. Physik 228, 489-493 (1969)}] solution in the context of $f(R,\\, T)$ gravity.","In the $f(R, T)$ framework, the gravitational action includes both the Ricci scalar ($R$) and the trace of the energy-momentum tensor ($T$).","We chose a particular $f(R,\\,T)$ model s.t. $f(R,\\,T) = R+2 \\chi T$, where $\\chi$ is known as the coupling parameter.","This solution describes a novel isotropic compact fluid sphere with positively finite central pressure and density in this extended theory of gravity.","The results obtained analytically are better described by graphical representations of the physical parameters for various values of the coupling parameter $\\chi$. The solution for a specific compact object, Vela X-1, with radius $\\mathfrak{R} = 9.56_{-0.08}^{+0.08}$ km and mass $\\mathcal{M} = 1.77 \\pm 0.08~\\mathcal{M}_{\\odot}$","[{\\em M. L. Rawls et al. ApJ, 730, 25 (2011)}], is shown here.","We analyze the fundamental physical attributes of the star, which reveals the influence of the coupling parameter $\\chi$ on the values of substance parameters.","This helps us to make a fruitful comparison of this modified $f(R,\\, T)$ gravity with the standard GR and notice that it holds good for stable compact objects.","In this framework, the star under our consideration exhibits a stable structure consistent with the Heintzmann IIa {\\em ansatz}.","From all of our obtained graphical and numerical results, we can ultimately conclude that our reported model is physically admissible and satisfies all the physical criteria for an acceptable model."],"url":"http://arxiv.org/abs/2404.13643v1","category":"gr-qc"}
{"created":"2024-04-21 12:33:57","title":"A critical drift-diffusion equation: intermittent behavior","abstract":"We consider a drift-diffusion process with a time-independent and divergence-free random drift that is of white-noise character. We are interested in the critical case of two space dimensions, where one has to impose a small-scale cut-off for well-posedness, and is interested in the marginally super-diffusive behavior on large scales.   In the presence of an (artificial) large-scale cut-off at scale L, as a consequence of standard stochastic homogenization theory, there exist harmonic coordinates with a stationary gradient $F_L$; the merit of these coordinates being that under their lens, the drift-diffusion process turns into a martingale.   It has recently been established that the second moments diverge as $\\mathbb{E}|F_L|^2\\sim\\sqrt{\\ln L}$ for $L\\uparrow\\infty$. We quantitatively show that in this limit, and in the regime of small P\\'eclet number, $|F_L|^2/\\mathbb{E}|F_L|^2$ is not equi-integrable, and that $\\mathbb{E}|{\\rm det}F_L|/\\mathbb{E}|F_L|^2 $ is small. Hence the Jacobian matrix of the harmonic coordinates is very peaked and non-conformal.   We establish this asymptotic behavior by characterizing a proxy $\\tilde F_L$ introduced in previous work as the solution of an It\\^{o} SDE w. r. t. the variable $\\ln L$, and which implements the concept of a scale-by-scale homogenization based on a variance decomposition and admits an efficient calculus. For this proxy, we establish $\\mathbb{E}|\\tilde F_L|^4\\gg(\\mathbb{E}|\\tilde F_L|^2)^2$ and $\\mathbb{E}({\\rm det}\\tilde F_L-1)^2\\ll 1$. In view of the former property, we assimilate this phenomenon to intermittency. In fact, $\\tilde F_L$ behaves like a tensorial stochastic exponential, and as a field can be assimilated to multiplicative Gaussian chaos.","sentences":["We consider a drift-diffusion process with a time-independent and divergence-free random drift that is of white-noise character.","We are interested in the critical case of two space dimensions, where one has to impose a small-scale cut-off for well-posedness, and is interested in the marginally super-diffusive behavior on large scales.   ","In the presence of an (artificial) large-scale cut-off at scale L, as a consequence of standard stochastic homogenization theory, there exist harmonic coordinates with a stationary gradient $F_L$; the merit of these coordinates being that under their lens, the drift-diffusion process turns into a martingale.   ","It has recently been established that the second moments diverge as $\\mathbb{E}|F_L|^2\\sim\\sqrt{\\ln L}$ for $L\\uparrow\\infty$. We quantitatively show that in this limit, and in the regime of small P\\'eclet number, $|F_L|^2/\\mathbb{E}|F_L|^2$ is not equi-integrable, and that $\\mathbb{E}|{\\rm det}F_L|/\\mathbb{E}|F_L|^2 $ is small.","Hence the Jacobian matrix of the harmonic coordinates is very peaked and non-conformal.   ","We establish this asymptotic behavior by characterizing a proxy $\\tilde F_L$ introduced in previous work as the solution of an It\\^{o} SDE w. r. t. the variable $\\ln L$, and which implements the concept of a scale-by-scale homogenization based on a variance decomposition and admits an efficient calculus.","For this proxy, we establish $\\mathbb{E}|\\tilde F_L|^4\\gg(\\mathbb{E}|\\tilde F_L|^2)^2$ and $\\mathbb{E}({\\rm det}\\tilde","F_L-1)^2\\ll 1$.","In view of the former property, we assimilate this phenomenon to intermittency.","In fact, $\\tilde F_L$ behaves like a tensorial stochastic exponential, and as a field can be assimilated to multiplicative Gaussian chaos."],"url":"http://arxiv.org/abs/2404.13641v1","category":"math.PR"}
{"created":"2024-04-21 12:15:44","title":"Incorporating Different Verbal Cues to Improve Text-Based Computer-Delivered Health Messaging","abstract":"The ubiquity of smartphones has led to an increase in on demand healthcare being supplied. For example, people can share their illness-related experiences with others similar to themselves, and healthcare experts can offer advice for better treatment and care for remediable, terminal and mental illnesses. As well as this human-to-human communication, there has been an increased use of human-to-computer digital health messaging, such as chatbots. These can prove advantageous as they offer synchronous and anonymous feedback without the need for a human conversational partner. However, there are many subtleties involved in human conversation that a computer agent may not properly exhibit. For example, there are various conversational styles, etiquettes, politeness strategies or empathic responses that need to be chosen appropriately for the conversation. Encouragingly, computers are social actors (CASA) posits that people apply the same social norms to computers as they would do to people. On from this, previous studies have focused on applying conversational strategies to computer agents to make them embody more favourable human characteristics. However, if a computer agent fails in this regard it can lead to negative reactions from users. Therefore, in this dissertation we describe a series of studies we carried out to lead to more effective human-to-computer digital health messaging.   In our first study, we use the crowd [...]   Our second study investigates the effect of a health chatbot's conversational style [...]   In our final study, we investigate the format used by a chatbot when [...]   In summary, we have researched how to create more effective digital health interventions starting from generating health messages, to choosing an appropriate formality of messaging, and finally to formatting messages which reference a user's previous utterances.","sentences":["The ubiquity of smartphones has led to an increase in on demand healthcare being supplied.","For example, people can share their illness-related experiences with others similar to themselves, and healthcare experts can offer advice for better treatment and care for remediable, terminal and mental illnesses.","As well as this human-to-human communication, there has been an increased use of human-to-computer digital health messaging, such as chatbots.","These can prove advantageous as they offer synchronous and anonymous feedback without the need for a human conversational partner.","However, there are many subtleties involved in human conversation that a computer agent may not properly exhibit.","For example, there are various conversational styles, etiquettes, politeness strategies or empathic responses that need to be chosen appropriately for the conversation.","Encouragingly, computers are social actors (CASA) posits that people apply the same social norms to computers as they would do to people.","On from this, previous studies have focused on applying conversational strategies to computer agents to make them embody more favourable human characteristics.","However, if a computer agent fails in this regard it can lead to negative reactions from users.","Therefore, in this dissertation we describe a series of studies we carried out to lead to more effective human-to-computer digital health messaging.   ","In our first study, we use the crowd","[...]   Our second study investigates the effect of a health chatbot's conversational style","[...]   In our final study, we investigate the format used by a chatbot when [...]   In summary, we have researched how to create more effective digital health interventions starting from generating health messages, to choosing an appropriate formality of messaging, and finally to formatting messages which reference a user's previous utterances."],"url":"http://arxiv.org/abs/2404.13633v1","category":"cs.HC"}
{"created":"2024-04-21 11:51:07","title":"Safe Force/Position Tracking Control via Control Barrier Functions for Floating Base Mobile Manipulator Systems","abstract":"This paper introduces a safe force/position tracking control strategy designed for Free-Floating Mobile Manipulator Systems (MMSs) engaging in compliant contact with planar surfaces. The strategy uniquely integrates the Control Barrier Function (CBF) to manage operational limitations and safety concerns. It effectively addresses safety-critical aspects in the kinematic as well as dynamic level, such as manipulator joint limits, system velocity constraints, and inherent system dynamic uncertainties. The proposed strategy remains robust to the uncertainties of the MMS dynamic model, external disturbances, or variations in the contact stiffness model. The proposed control method has low computational demand ensures easy implementation on onboard computing systems, endorsing real-time operations. Simulation results verify the strategy's efficacy, reflecting enhanced system performance and safety.","sentences":["This paper introduces a safe force/position tracking control strategy designed for Free-Floating Mobile Manipulator Systems (MMSs) engaging in compliant contact with planar surfaces.","The strategy uniquely integrates the Control Barrier Function (CBF) to manage operational limitations and safety concerns.","It effectively addresses safety-critical aspects in the kinematic as well as dynamic level, such as manipulator joint limits, system velocity constraints, and inherent system dynamic uncertainties.","The proposed strategy remains robust to the uncertainties of the MMS dynamic model, external disturbances, or variations in the contact stiffness model.","The proposed control method has low computational demand ensures easy implementation on onboard computing systems, endorsing real-time operations.","Simulation results verify the strategy's efficacy, reflecting enhanced system performance and safety."],"url":"http://arxiv.org/abs/2404.13626v1","category":"cs.RO"}
{"created":"2024-04-21 11:50:29","title":"Sup-norm bounds for Jacobi cusp forms","abstract":"In this article, we give bounds for the natural invariant norm of cusp forms of real weight $k$ and character $\\chi$ for any cofinite Fuchsian subgroup $\\Gamma\\subset\\mathrm{SL}_{2}(\\mathbb{R})$. Using the representation of Jacobi cusp forms of integral weight $k$ and index $m$ for the modular group $\\Gamma_{0}=\\mathrm{SL}_{2}(\\mathbb{Z})$ as linear combinations of modular forms of weight $k-\\frac{1}{2}$ for some congruence subgroup of $\\Gamma_{0}$ (depending on $m$) and suitable Jacobi theta functions, we derive bounds for the natural invariant norm of these Jacobi cusp forms. More specifically, letting $J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$ denote the complex vector space of Jacobi cusp forms under consideration and $\\Vert\\cdot\\Vert_{\\mathrm{Pet}}$ the pointwise Petersson norm on $J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$, we prove that for given $\\epsilon>0$, the bound \\begin{align*} \\sup_{(\\tau,z)\\in\\mathbb{H}\\times\\mathbb{C}}\\Vert f(\\tau,z)\\Vert_{\\mathrm{Pet}}=O_{\\epsilon}\\big(k^{\\frac{3}{4}}m^{\\frac{3}{2}+\\epsilon}\\big) \\end{align*} holds for any $f\\in J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$, which is normalized with respect to the Petersson inner product, where the implied constant depends only on the choice of $\\epsilon>0$.","sentences":["In this article, we give bounds for the natural invariant norm of cusp forms of real weight $k$ and character $\\chi$ for any cofinite Fuchsian subgroup $\\Gamma\\subset\\mathrm{SL}_{2}(\\mathbb{R})$. Using the representation of Jacobi cusp forms of integral weight $k$ and index $m$ for the modular group $\\Gamma_{0}=\\mathrm{SL}_{2}(\\mathbb{Z})$ as linear combinations of modular forms of weight $k-\\frac{1}{2}$ for some congruence subgroup of $\\Gamma_{0}$ (depending on $m$) and suitable Jacobi theta functions, we derive bounds for the natural invariant norm of these Jacobi cusp forms.","More specifically, letting $J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$ denote the complex vector space of Jacobi cusp forms under consideration and $\\Vert\\cdot\\Vert_{\\mathrm{Pet}}$ the pointwise Petersson norm on $J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$, we prove that for given $\\epsilon>0$, the bound \\begin{align*} \\sup_{(\\tau,z)\\in\\mathbb{H}\\times\\mathbb{C}}\\Vert f(\\tau,z)\\Vert_{\\mathrm{Pet}}=O_{\\epsilon}\\big(k^{\\frac{3}{4}}m^{\\frac{3}{2}+\\epsilon}\\big) \\end{align*} holds for any $f\\in J_{k,m}^{\\mathrm{cusp}}(\\Gamma_{0})$, which is normalized with respect to the Petersson inner product, where the implied constant depends only on the choice of $\\epsilon>0$."],"url":"http://arxiv.org/abs/2404.13625v1","category":"math.NT"}
{"created":"2024-04-21 11:28:05","title":"Research in teaching and learning sequence design. To what extent do designers theoretical orientations about learning and the nature of science shape design decisions","abstract":"Over the last three decades, various didactic proposals have been published in an attempt to connect theory and research findings with the design of Teaching-Learning Sequences (TLS) in various contexts. Many studies have analysed the process of designing teaching-learning sequences as a research activity. This line of research aims to increase the impact and transferability of educational practice.   However, the information usually provided about the relation between the theory and research findings with the design of the TLS is insufficiently detailed to provide the basis for a critique. Furthermore, not all TLS proposals include evaluation in terms of learning outcomes and very rarely are these learning outcomes specifically related to the design process. This lack of detailed information on the design and evaluation of proposed TLS makes it difficult to properly assess their potential effectiveness or to systematically discuss and improve their design. In this chapter we want to contribute to make the rationale for design decisions explicit. The aim of this paper is to describe in detail how the theoretical orientations of designers of teaching materials towards cognition and learning can shape the structure and pedagogical strategies of the resulting TLS. We will analyse the relationship of two design tools (Epistemological analysis and Learning demands) to theoretical assumptions about learning and the nature of science. We want to highlight the benefits of reflecting on and discussing theoretical elements and their links to design decisions, which makes TLS design more productive on a practical level to broaden the teaching and learning perspectives of TLS. Finally, we will explore the question to what extent the theoretical orientations of curriculum designers towards cognition and learning can influence the structure and pedagogical strategies of the resulting TLS","sentences":["Over the last three decades, various didactic proposals have been published in an attempt to connect theory and research findings with the design of Teaching-Learning Sequences (TLS) in various contexts.","Many studies have analysed the process of designing teaching-learning sequences as a research activity.","This line of research aims to increase the impact and transferability of educational practice.   ","However, the information usually provided about the relation between the theory and research findings with the design of the TLS is insufficiently detailed to provide the basis for a critique.","Furthermore, not all TLS proposals include evaluation in terms of learning outcomes and very rarely are these learning outcomes specifically related to the design process.","This lack of detailed information on the design and evaluation of proposed TLS makes it difficult to properly assess their potential effectiveness or to systematically discuss and improve their design.","In this chapter we want to contribute to make the rationale for design decisions explicit.","The aim of this paper is to describe in detail how the theoretical orientations of designers of teaching materials towards cognition and learning can shape the structure and pedagogical strategies of the resulting TLS.","We will analyse the relationship of two design tools (Epistemological analysis and Learning demands) to theoretical assumptions about learning and the nature of science.","We want to highlight the benefits of reflecting on and discussing theoretical elements and their links to design decisions, which makes TLS design more productive on a practical level to broaden the teaching and learning perspectives of TLS.","Finally, we will explore the question to what extent the theoretical orientations of curriculum designers towards cognition and learning can influence the structure and pedagogical strategies of the resulting TLS"],"url":"http://arxiv.org/abs/2404.13623v1","category":"physics.ed-ph"}
{"created":"2024-04-21 11:24:35","title":"On the CR Nirenberg problem: density and multiplicity of solutions","abstract":"We prove some results on the density and multiplicity of positive solutions to the prescribed Webster scalar curvature problem on the $(2n+1)$-dimensional standard unit CR sphere $(\\mathbb{S} ^{2n+1},\\theta_0)$. Specifically, we construct arbitrarily many multi-bump solutions via the variational gluing method. In particular, we show the Webster scalar curvature functions of contact forms conformal to $\\theta_0$ are $C^{0}$-dense among bounded functions which are positive somewhere. Existence results of infinitely many positive solutions to the related equation $-\\Delta_{\\mathbb{H}} u=R(\\xi) u^{(n+2) /n}$ on the Heisenberg group $\\Hn $ with $R(\\xi)$ being asymptotically periodic with respect to left translation are also obtained. Our proofs make use of a refined analysis of bubbling behavior, gradient flow, Pohozaev identity, as well as blow up arguments.","sentences":["We prove some results on the density and multiplicity of positive solutions to the prescribed Webster scalar curvature problem on the $(2n+1)$-dimensional standard unit CR sphere $(\\mathbb{S} ^{2n+1},\\theta_0)$.","Specifically, we construct arbitrarily many multi-bump solutions via the variational gluing method.","In particular, we show the Webster scalar curvature functions of contact forms conformal to $\\theta_0$ are $C^{0}$-dense among bounded functions which are positive somewhere.","Existence results of infinitely many positive solutions to the related equation $-\\Delta_{\\mathbb{H}} u=R(\\xi) u^{(n+2) /n}$ on the Heisenberg group $\\Hn $ with $R(\\xi)$ being asymptotically periodic with respect to left translation are also obtained.","Our proofs make use of a refined analysis of bubbling behavior, gradient flow, Pohozaev identity, as well as blow up arguments."],"url":"http://arxiv.org/abs/2404.13622v1","category":"math.AP"}
{"created":"2024-04-21 10:49:41","title":"The Branch Not Taken: Predicting Branching in Online Conversations","abstract":"Multi-participant discussions tend to unfold in a tree structure rather than a chain structure. Branching may occur for multiple reasons -- from the asynchronous nature of online platforms to a conscious decision by an interlocutor to disengage with part of the conversation. Predicting branching and understanding the reasons for creating new branches is important for many downstream tasks such as summarization and thread disentanglement and may help develop online spaces that encourage users to engage in online discussions in more meaningful ways. In this work, we define the novel task of branch prediction and propose GLOBS (Global Branching Score) -- a deep neural network model for predicting branching. GLOBS is evaluated on three large discussion forums from Reddit, achieving significant improvements over an array of competitive baselines and demonstrating better transferability. We affirm that structural, temporal, and linguistic features contribute to GLOBS success and find that branching is associated with a greater number of conversation participants and tends to occur in earlier levels of the conversation tree. We publicly release GLOBS and our implementation of all baseline models to allow reproducibility and promote further research on this important task.","sentences":["Multi-participant discussions tend to unfold in a tree structure rather than a chain structure.","Branching may occur for multiple reasons -- from the asynchronous nature of online platforms to a conscious decision by an interlocutor to disengage with part of the conversation.","Predicting branching and understanding the reasons for creating new branches is important for many downstream tasks such as summarization and thread disentanglement and may help develop online spaces that encourage users to engage in online discussions in more meaningful ways.","In this work, we define the novel task of branch prediction and propose GLOBS (Global Branching Score) -- a deep neural network model for predicting branching.","GLOBS is evaluated on three large discussion forums from Reddit, achieving significant improvements over an array of competitive baselines and demonstrating better transferability.","We affirm that structural, temporal, and linguistic features contribute to GLOBS success and find that branching is associated with a greater number of conversation participants and tends to occur in earlier levels of the conversation tree.","We publicly release GLOBS and our implementation of all baseline models to allow reproducibility and promote further research on this important task."],"url":"http://arxiv.org/abs/2404.13613v1","category":"cs.CL"}
{"created":"2024-04-21 10:40:23","title":"Uncertainty Assessment of Probabilistic Cellular Automata Simulations in Microstructure Evolution","abstract":"The probabilistic cellular automaton (PCA) method is highlighted for its relatively simple numerical algorithm and low computational cost in the simulation of microstructural evolution. In this method, probabilistic state change rules are implemented to compute the evolution of cell states at each time step. The stochastic nature of this simulation method leads to non-repeatable simulation results, introducing inherent uncertainty. In this study, the uncertainty and dispersion in PCA simulations of microstructural evolution were investigated. Hence, the probabilistic transformations of cell states were meticulously considered at each time step, and discrete probability distribution functions (dPDF) were introduced to analyze the frequency distribution of simulation outcomes. To evaluate the performance of the proposed dPDFs, cellular automaton models were developed with various numbers of cells and distribution of transformation probabilities. Multiple iterations of these simulations were conducted, and the validity of the presented distribution functions was assessed through statistical analysis of the simulations outcomes. Comparisons between PCA simulation results and distribution functions demonstrate consistency, emphasizing the predictive capability of the proposed models. Also, the effects of modeling parameters on the uncertainty of simulation results in two and three-dimensional PCA modeling were studied, introducing the coefficient of variation as a measure of dispersion. Results indicate that increasing the number of boundary cells, cellular resolution, and model size reduces uncertainty, enhancing the repeatability of PCA simulation outcomes.","sentences":["The probabilistic cellular automaton (PCA) method is highlighted for its relatively simple numerical algorithm and low computational cost in the simulation of microstructural evolution.","In this method, probabilistic state change rules are implemented to compute the evolution of cell states at each time step.","The stochastic nature of this simulation method leads to non-repeatable simulation results, introducing inherent uncertainty.","In this study, the uncertainty and dispersion in PCA simulations of microstructural evolution were investigated.","Hence, the probabilistic transformations of cell states were meticulously considered at each time step, and discrete probability distribution functions (dPDF) were introduced to analyze the frequency distribution of simulation outcomes.","To evaluate the performance of the proposed dPDFs, cellular automaton models were developed with various numbers of cells and distribution of transformation probabilities.","Multiple iterations of these simulations were conducted, and the validity of the presented distribution functions was assessed through statistical analysis of the simulations outcomes.","Comparisons between PCA simulation results and distribution functions demonstrate consistency, emphasizing the predictive capability of the proposed models.","Also, the effects of modeling parameters on the uncertainty of simulation results in two and three-dimensional PCA modeling were studied, introducing the coefficient of variation as a measure of dispersion.","Results indicate that increasing the number of boundary cells, cellular resolution, and model size reduces uncertainty, enhancing the repeatability of PCA simulation outcomes."],"url":"http://arxiv.org/abs/2404.13610v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 10:28:34","title":"Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence","abstract":"Tackling image degradation due to atmospheric turbulence, particularly in dynamic environment, remains a challenge for long-range imaging systems. Existing techniques have been primarily designed for static scenes or scenes with small motion. This paper presents the first segment-then-restore pipeline for restoring the videos of dynamic scenes in turbulent environment. We leverage mean optical flow with an unsupervised motion segmentation method to separate dynamic and static scene components prior to restoration. After camera shake compensation and segmentation, we introduce foreground/background enhancement leveraging the statistics of turbulence strength and a transformer model trained on a novel noise-based procedural turbulence generator for fast dataset augmentation. Benchmarked against existing restoration methods, our approach restores most of the geometric distortion and enhances sharpness for videos. We make our code, simulator, and data publicly available to advance the field of video restoration from turbulence: riponcs.github.io/TurbSegRes","sentences":["Tackling image degradation due to atmospheric turbulence, particularly in dynamic environment, remains a challenge for long-range imaging systems.","Existing techniques have been primarily designed for static scenes or scenes with small motion.","This paper presents the first segment-then-restore pipeline for restoring the videos of dynamic scenes in turbulent environment.","We leverage mean optical flow with an unsupervised motion segmentation method to separate dynamic and static scene components prior to restoration.","After camera shake compensation and segmentation, we introduce foreground/background enhancement leveraging the statistics of turbulence strength and a transformer model trained on a novel noise-based procedural turbulence generator for fast dataset augmentation.","Benchmarked against existing restoration methods, our approach restores most of the geometric distortion and enhances sharpness for videos.","We make our code, simulator, and data publicly available to advance the field of video restoration from turbulence: riponcs.github.io/TurbSegRes"],"url":"http://arxiv.org/abs/2404.13605v1","category":"cs.CV"}
{"created":"2024-04-21 09:32:02","title":"New galaxy UV luminosity constraints on warm dark matter from JWST","abstract":"We exploit the recent {\\it James Webb Space Telescope} (JWST) determination of galaxy UV luminosity functions over the redshift range $z=9-14.5$ to derive constraints on warm dark matter (WDM) models. The delayed structure formation in WDM universe makes high redshift observations to be a powerful probe to set limits on the particle mass $m_\\mathrm{x}$ of WDM candidates. By integrating these observations with blank-field surveys conducted by {\\it Hubble Space Telescope} (HST) at redshifts $z=4-8$, we impose constraints on both astrophysical and WDM parameters simultaneously. We find a new limit of $m_\\mathrm{x} \\geq 3.2$ keV for the mass of thermal relic WDM particles at $95\\%$ confidence level. This bound is tighter than the most stringent result derived using HST data before. Future JWST observations could further reduce the observation uncertainties and improve this constraint.","sentences":["We exploit the recent {\\it James Webb Space Telescope} (JWST) determination of galaxy UV luminosity functions over the redshift range $z=9-14.5$ to derive constraints on warm dark matter (WDM) models.","The delayed structure formation in WDM universe makes high redshift observations to be a powerful probe to set limits on the particle mass $m_\\mathrm{x}$ of WDM candidates.","By integrating these observations with blank-field surveys conducted by {\\it Hubble Space Telescope} (HST) at redshifts $z=4-8$, we impose constraints on both astrophysical and WDM parameters simultaneously.","We find a new limit of $m_\\mathrm{x} \\geq 3.2$ keV for the mass of thermal relic WDM particles at $95\\%$ confidence level.","This bound is tighter than the most stringent result derived using HST data before.","Future JWST observations could further reduce the observation uncertainties and improve this constraint."],"url":"http://arxiv.org/abs/2404.13596v1","category":"astro-ph.CO"}
{"created":"2024-04-22 16:41:14","title":"Comparison of Empirical Models of Ionospheric Heating to Global Simulations","abstract":"Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating. This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively. Several empirical models estimate Joule heating based on ionospheric currents using the AE index. In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models. We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating. We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations.","sentences":["Intense currents produced during geomagnetic storms dissipate energy in the ionosphere through Joule heating.","This dissipation has significant space weather effects, and thus it is important to determine the ability of physics-based simulations to replicate real events quantitatively.","Several empirical models estimate Joule heating based on ionospheric currents using the AE index.","In this study, we select 11 magnetic storm simulations from the CCMC database and compare the integrated Joule heating in the simulations with the results of empirical models.","We also use the SWMF global magnetohydrodynamic simulations for 12 storms to reproduce the correlation between the simulated AE index and simulated Joule heating.","We find that the scale factors in the empirical models are half what is predicted by the SWMF simulations."],"url":"http://arxiv.org/abs/2404.14330v1","category":"astro-ph.EP"}
{"created":"2024-04-22 16:11:12","title":"Cryogenic sapphire optical reference cavity with crystalline coatings at $\\mathrm{ 1 \\times 10^{-16}}$ fractional instability","abstract":"The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length. These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties. Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented. Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail. The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s.","sentences":["The frequency stability of a laser locked to an optical reference cavity is fundamentally limited by thermal noise in the cavity length.","These fluctuations are linked to material dissipation, which depends both on the temperature of the optical components and the material properties.","Here, the design and experimental characterization of a sapphire optical cavity operated at 10 K with crystalline coatings at 1069 nm is presented.","Theoretical estimates of the thermo-mechanical noise indicate a thermal noise floor below $\\mathrm{4.5\\times10^{-18}}$. Major technical noise contributions including vibrations, temperature fluctuations, and residual amplitude modulation are characterized in detail.","The short-term performance is measured via a three-cornered hat analysis with two other cavity-stabilized lasers, yielding a noise floor of $1\\times10^{-16}$. The long-term performance is measured against an optical lattice clock, indicating cavity stability at the level of $2\\times10^{-15}$ for averaging times up to 10,000 s."],"url":"http://arxiv.org/abs/2404.14310v1","category":"physics.optics"}
{"created":"2024-04-22 14:11:54","title":"Rotting Infinitely Many-armed Bandits beyond the Worst-case Rotting: An Adaptive Approach","abstract":"In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithms using synthetic datasets.","sentences":["In this study, we consider the infinitely many armed bandit problems in rotting environments, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged.","We explore two scenarios capturing problem-dependent characteristics regarding the decay of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting scenario, and the other in which the number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting scenario.","To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards.","Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios.","Lastly, we demonstrate the performance of our algorithms using synthetic datasets."],"url":"http://arxiv.org/abs/2404.14202v1","category":"cs.LG"}
{"created":"2024-04-22 13:41:03","title":"The effects of turbulence modeling on dynamic stall","abstract":"A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented. The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models. After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks. Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex. In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted. Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one. Finally, a comparison of the proposed approaches with other numerical simulations is given.","sentences":["A numerical investigation of the flow evolution over a pitching NACA 0012 airfoil incurring in deep dynamic stall phenomena is presented.","The experimental data at Reynolds number Re = 135 000 and reduced frequency k = 0.1, provided by Lee and Gerontakos, are compared to numerical simulations using different turbulence models.","After a preliminary space and time convergence study, two- and three-dimensional URANS with different turbulence models are explored, highlighting the advantages and the drawbacks.","Then, the turbulence-resolving capabilities of hybrid RANS/LES strategies are exploited to recover and better represent the dynamic stall vortex.","In detail, Scale-Adaptive Simulations (SAS) and Stress-Blended Eddy Simulations (SBES) are adopted.","Furthermore, the LES resolved portion allows a spectral analysis of the force and moment coefficients to investigate the contribution of frequency lower than the pitching one.","Finally, a comparison of the proposed approaches with other numerical simulations is given."],"url":"http://arxiv.org/abs/2404.14172v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 10:52:52","title":"Dynamical scaling and Planckian dissipation due to heavy-fermion quantum criticality","abstract":"We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT). In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling. We propose a scaling Ansatz that describes this behavior. It also implies Planckian dissipation for the longest-lived excitations. The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling. This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point. Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities. Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$.","sentences":["We study dynamical scaling associated with a Kondo-breakdown quantum critical point (KB-QCP) of the periodic Anderson model, treated by two-site cellular dynamical mean-field theory (2CDMFT).","In the quantum critical region, the staggered spin exhibits SYK-like slow dynamics and its dynamical susceptibility shows $\\omega/T$ scaling.","We propose a scaling Ansatz that describes this behavior.","It also implies Planckian dissipation for the longest-lived excitations.","The current susceptibility follows the same scaling ansatz, leading to strange-metal scaling.","This demonstrates that the KB-QCP described by 2CDMFT is an intrinsic (i.e., disorder-free) strange-metal fixed point.","Surprisingly, the SYK-like dynamics and scaling are driven by strong vertex contributions to the susceptibilities.","Our results for the optical conductivity match experimental observations on YbRh${}_2$Si${}_2$ and CeCoIn${}_5$."],"url":"http://arxiv.org/abs/2404.14079v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 09:57:53","title":"HashPoint: Accelerated Point Searching and Sampling for Neural Rendering","abstract":"In this paper, we address the problem of efficient point searching and sampling for volume neural rendering. Within this realm, two typical approaches are employed: rasterization and ray tracing. The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity. In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time. We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering. Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches. Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray. Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets. The code will be available at https://jiahao-ma.github.io/hashpoint/.","sentences":["In this paper, we address the problem of efficient point searching and sampling for volume neural rendering.","Within this realm, two typical approaches are employed: rasterization and ray tracing.","The rasterization-based methods enable real-time rendering at the cost of increased memory and lower fidelity.","In contrast, the ray-tracing-based methods yield superior quality but demand longer rendering time.","We solve this problem by our HashPoint method combining these two strategies, leveraging rasterization for efficient point searching and sampling, and ray marching for rendering.","Our method optimizes point searching by rasterizing points within the camera's view, organizing them in a hash table, and facilitating rapid searches.","Notably, we accelerate the rendering process by adaptive sampling on the primary surface encountered by the ray.","Our approach yields substantial speed-up for a range of state-of-the-art ray-tracing-based methods, maintaining equivalent or superior accuracy across synthetic and real test datasets.","The code will be available at https://jiahao-ma.github.io/hashpoint/."],"url":"http://arxiv.org/abs/2404.14044v1","category":"cs.CV"}
{"created":"2024-04-22 09:40:07","title":"Exploring neural oscillations during speech perception via surrogate gradient spiking neural networks","abstract":"Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs. Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance. Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology.","sentences":["Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales.","We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network.","Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs.","Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance.","Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology."],"url":"http://arxiv.org/abs/2404.14024v1","category":"cs.CL"}
{"created":"2024-04-22 09:35:48","title":"Physics-Informed Neural Networks and Beyond: Enforcing Physical Constraints in Quantum Dissipative Dynamics","abstract":"Neural networks (NNs) accelerate simulations of quantum dissipative dynamics. Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches. We show that this may lead to implausible results measured by violation of the trace conservation. To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend. Beyond that, we introduce an approach enforcing the perfect trace conservation by design.","sentences":["Neural networks (NNs) accelerate simulations of quantum dissipative dynamics.","Ensuring that these simulations adhere to fundamental physical laws is crucial, but has been largely ignored in the state-of-the-art NN approaches.","We show that this may lead to implausible results measured by violation of the trace conservation.","To recover the correct physical behavior, we develop physics-informed NNs that mitigate the violations to a good extend.","Beyond that, we introduce an approach enforcing the perfect trace conservation by design."],"url":"http://arxiv.org/abs/2404.14021v1","category":"physics.chem-ph"}
{"created":"2024-04-22 05:52:33","title":"Ultralow Dissipation Nanomechanical Devices from Monocrystalline Silicon Carbide","abstract":"Due to their low mass and long coherence times, nanomechanical resonators have many applications, from biomolecule mass sensing to hybrid quantum interfaces. In many instances the performance is limited by internal material damping. Crystalline materials promise lower material dissipation, however due to fabrication challenges, amorphous materials are more commonly utilized. Crystalline silicon carbide (SiC) is particularly appealing due to its exquisite mechanical, electrical and optical properties, but to-date exhibits higher nanomechanical dissipation than both amorphous and other crystalline materials. To address this, we fabricate nanomechanical resonators thinned from bulk monocrystalline 4H-SiC. Characterization of multiple resonators of different sizes and thicknesses, allows us to discern the surface and volumetric contributions to dissipation. We measure mechanical dissipation rates as low as 2.7 mHz, more than an order-of-magnitude lower than any previous crystalline SiC resonator, yielding quality factors as high as 20 million at room temperature. We also quantify the nonlinear dissipation of SiC nanomechanical resonators for the first time, finding that it is lower than other materials. This promises higher sensitivity in applications such as mass sensing. By achieving exceptionally low dissipation in SiC resonators, our work provides a path towards improved performance in sensing and other applications.","sentences":["Due to their low mass and long coherence times, nanomechanical resonators have many applications, from biomolecule mass sensing to hybrid quantum interfaces.","In many instances the performance is limited by internal material damping.","Crystalline materials promise lower material dissipation, however due to fabrication challenges, amorphous materials are more commonly utilized.","Crystalline silicon carbide (SiC) is particularly appealing due to its exquisite mechanical, electrical and optical properties, but to-date exhibits higher nanomechanical dissipation than both amorphous and other crystalline materials.","To address this, we fabricate nanomechanical resonators thinned from bulk monocrystalline 4H-SiC. Characterization of multiple resonators of different sizes and thicknesses, allows us to discern the surface and volumetric contributions to dissipation.","We measure mechanical dissipation rates as low as 2.7 mHz, more than an order-of-magnitude lower than any previous crystalline SiC resonator, yielding quality factors as high as 20 million at room temperature.","We also quantify the nonlinear dissipation of SiC nanomechanical resonators for the first time, finding that it is lower than other materials.","This promises higher sensitivity in applications such as mass sensing.","By achieving exceptionally low dissipation in SiC resonators, our work provides a path towards improved performance in sensing and other applications."],"url":"http://arxiv.org/abs/2404.13893v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-22 03:31:34","title":"Toward Robust LiDAR based 3D Object Detection via Density-Aware Adaptive Thresholding","abstract":"Robust 3D object detection is a core challenge for autonomous mobile systems in field robotics. To tackle this issue, many researchers have demonstrated improvements in 3D object detection performance in datasets. However, real-world urban scenarios with unstructured and dynamic situations can still lead to numerous false positives, posing a challenge for robust 3D object detection models. This paper presents a post-processing algorithm that dynamically adjusts object detection thresholds based on the distance from the ego-vehicle. 3D object detection models usually perform well in detecting nearby objects but may exhibit suboptimal performance for distant ones. While conventional perception algorithms typically employ a single threshold in post-processing, the proposed algorithm addresses this issue by employing adaptive thresholds based on the distance from the ego-vehicle, minimizing false negatives and reducing false positives in urban scenarios. The results show performance enhancements in 3D object detection models across a range of scenarios, not only in dynamic urban road conditions but also in scenarios involving adverse weather conditions.","sentences":["Robust 3D object detection is a core challenge for autonomous mobile systems in field robotics.","To tackle this issue, many researchers have demonstrated improvements in 3D object detection performance in datasets.","However, real-world urban scenarios with unstructured and dynamic situations can still lead to numerous false positives, posing a challenge for robust 3D object detection models.","This paper presents a post-processing algorithm that dynamically adjusts object detection thresholds based on the distance from the ego-vehicle.","3D object detection models usually perform well in detecting nearby objects but may exhibit suboptimal performance for distant ones.","While conventional perception algorithms typically employ a single threshold in post-processing, the proposed algorithm addresses this issue by employing adaptive thresholds based on the distance from the ego-vehicle, minimizing false negatives and reducing false positives in urban scenarios.","The results show performance enhancements in 3D object detection models across a range of scenarios, not only in dynamic urban road conditions but also in scenarios involving adverse weather conditions."],"url":"http://arxiv.org/abs/2404.13852v1","category":"cs.RO"}
{"created":"2024-04-22 03:15:42","title":"DSDRNet: Disentangling Representation and Reconstruct Network for Domain Generalization","abstract":"Domain generalization faces challenges due to the distribution shift between training and testing sets, and the presence of unseen target domains. Common solutions include domain alignment, meta-learning, data augmentation, or ensemble learning, all of which rely on domain labels or domain adversarial techniques. In this paper, we propose a Dual-Stream Separation and Reconstruction Network, dubbed DSDRNet. It is a disentanglement-reconstruction approach that integrates features of both inter-instance and intra-instance through dual-stream fusion. The method introduces novel supervised signals by combining inter-instance semantic distance and intra-instance similarity. Incorporating Adaptive Instance Normalization (AdaIN) into a two-stage cyclic reconstruction process enhances self-disentangled reconstruction signals to facilitate model convergence. Extensive experiments on four benchmark datasets demonstrate that DSDRNet outperforms other popular methods in terms of domain generalization capabilities.","sentences":["Domain generalization faces challenges due to the distribution shift between training and testing sets, and the presence of unseen target domains.","Common solutions include domain alignment, meta-learning, data augmentation, or ensemble learning, all of which rely on domain labels or domain adversarial techniques.","In this paper, we propose a Dual-Stream Separation and Reconstruction Network, dubbed DSDRNet.","It is a disentanglement-reconstruction approach that integrates features of both inter-instance and intra-instance through dual-stream fusion.","The method introduces novel supervised signals by combining inter-instance semantic distance and intra-instance similarity.","Incorporating Adaptive Instance Normalization (AdaIN) into a two-stage cyclic reconstruction process enhances self-disentangled reconstruction signals to facilitate model convergence.","Extensive experiments on four benchmark datasets demonstrate that DSDRNet outperforms other popular methods in terms of domain generalization capabilities."],"url":"http://arxiv.org/abs/2404.13848v1","category":"cs.CV"}
{"created":"2024-04-22 02:04:34","title":"GazeIntent: Adapting dwell-time selection in VR interaction with real-time intent modeling","abstract":"The use of ML models to predict a user's cognitive state from behavioral data has been studied for various applications which includes predicting the intent to perform selections in VR. We developed a novel technique that uses gaze-based intent models to adapt dwell-time thresholds to aid gaze-only selection. A dataset of users performing selection in arithmetic tasks was used to develop intent prediction models (F1 = 0.94). We developed GazeIntent to adapt selection dwell times based on intent model outputs and conducted an end-user study with returning and new users performing additional tasks with varied selection frequencies. Personalized models for returning users effectively accounted for prior experience and were preferred by 63% of users. Our work provides the field with methods to adapt dwell-based selection to users, account for experience over time, and consider tasks that vary by selection frequency","sentences":["The use of ML models to predict a user's cognitive state from behavioral data has been studied for various applications which includes predicting the intent to perform selections in VR.","We developed a novel technique that uses gaze-based intent models to adapt dwell-time thresholds to aid gaze-only selection.","A dataset of users performing selection in arithmetic tasks was used to develop intent prediction models (F1 = 0.94).","We developed GazeIntent to adapt selection dwell times based on intent model outputs and conducted an end-user study with returning and new users performing additional tasks with varied selection frequencies.","Personalized models for returning users effectively accounted for prior experience and were preferred by 63% of users.","Our work provides the field with methods to adapt dwell-based selection to users, account for experience over time, and consider tasks that vary by selection frequency"],"url":"http://arxiv.org/abs/2404.13829v1","category":"cs.HC"}
{"created":"2024-04-22 00:16:18","title":"Adaptive Heterogeneous Client Sampling for Federated Learning over Wireless Networks","abstract":"Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited. Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity. This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time. We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability. Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem. We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem. Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design. Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth. Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes.","sentences":["Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited.","Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity.","This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time.","We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability.","Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem.","We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem.","Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design.","Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth.","Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes."],"url":"http://arxiv.org/abs/2404.13804v1","category":"cs.DC"}
{"created":"2024-04-21 23:48:28","title":"Dynamics of Polar-Core Spin Vortices in Inhomogeneous Spin-1 Bose-Einstein Condensates","abstract":"In the easy-plane phase, a ferromagnetic spin-1 Bose-Einstein condensate is magnetized in a plane transverse to the applied Zeeman field. This phase supports polar-core spin vortices (PCVs), which consist of phase windings of transverse magnetization. Here we show that spin-changing collisions cause a PCV to accelerate down density gradients in an inhomogeneous condensate. The dynamics is well-described by a simplified model adapted from scalar systems, which predicts the dependence of the dynamics on trap tightness and quadratic Zeeman energy. In a harmonic trap, a PCV accelerates radially to the condensate boundary, in stark contrast to the azimuthal motion of vortices in a scalar condensate. In a trap that has a local potential maximum at the centre, the PCV exhibits oscillations around the trap centre, which persist for a remarkably long time. The oscillations coincide with the emission and reabsorption of axial spin waves, which reflect off the condensate boundary.","sentences":["In the easy-plane phase, a ferromagnetic spin-1 Bose-Einstein condensate is magnetized in a plane transverse to the applied Zeeman field.","This phase supports polar-core spin vortices (PCVs), which consist of phase windings of transverse magnetization.","Here we show that spin-changing collisions cause a PCV to accelerate down density gradients in an inhomogeneous condensate.","The dynamics is well-described by a simplified model adapted from scalar systems, which predicts the dependence of the dynamics on trap tightness and quadratic Zeeman energy.","In a harmonic trap, a PCV accelerates radially to the condensate boundary, in stark contrast to the azimuthal motion of vortices in a scalar condensate.","In a trap that has a local potential maximum at the centre, the PCV exhibits oscillations around the trap centre, which persist for a remarkably long time.","The oscillations coincide with the emission and reabsorption of axial spin waves, which reflect off the condensate boundary."],"url":"http://arxiv.org/abs/2404.13800v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-21 20:21:24","title":"Using Adaptive Empathetic Responses for Teaching English","abstract":"Existing English-teaching chatbots rarely incorporate empathy explicitly in their feedback, but empathetic feedback could help keep students engaged and reduce learner anxiety. Toward this end, we propose the task of negative emotion detection via audio, for recognizing empathetic feedback opportunities in language learning. We then build the first spoken English-teaching chatbot with adaptive, empathetic feedback. This feedback is synthesized through automatic prompt optimization of ChatGPT and is evaluated with English learners. We demonstrate the effectiveness of our system through a preliminary user study.","sentences":["Existing English-teaching chatbots rarely incorporate empathy explicitly in their feedback, but empathetic feedback could help keep students engaged and reduce learner anxiety.","Toward this end, we propose the task of negative emotion detection via audio, for recognizing empathetic feedback opportunities in language learning.","We then build the first spoken English-teaching chatbot with adaptive, empathetic feedback.","This feedback is synthesized through automatic prompt optimization of ChatGPT and is evaluated with English learners.","We demonstrate the effectiveness of our system through a preliminary user study."],"url":"http://arxiv.org/abs/2404.13764v1","category":"cs.CL"}
{"created":"2024-04-21 18:14:59","title":"Strong Existence and Uniqueness for Singular SDEs Driven by Stable Processes","abstract":"We consider the one-dimensional stochastic differential equation   \\begin{equation*}   X_t = x_0 + L_t + \\int_0^t \\mu(X_s)ds, \\quad t \\geq 0,   \\end{equation*} where $\\mu$ is a finite measure of Kato class $K_{\\eta}$ with $\\eta \\in (0,\\alpha-1]$ and $(L_t)_{t \\geq 0}$ is a symmetric $\\alpha$-stable process with $\\alpha \\in (1,2)$. We derive weak and strong well posedness for this equation when $\\eta \\leq\\alpha-1$ and $\\eta < \\alpha-1$, respectively, and show that the condition $\\eta \\leq \\alpha-1$ is sharp for weak existence. We furthermore reformulate the equation in terms of the local time of the solution $(X_{t})_{t \\geq 0}$ and prove its well posedness. To this end, we also derive a Tanaka-type formula for a symmetric, $\\alpha$-stable processes with $\\alpha \\in (1,2)$ that is perturbed by an adapted, right-continuous process of finite variation.","sentences":["We consider the one-dimensional stochastic differential equation   \\begin{equation*}   X_t = x_0 + L_t + \\int_0^t \\mu(X_s)ds, \\quad t \\geq 0,   \\end{equation*} where $\\mu$ is a finite measure of Kato class $K_{\\eta}$ with $\\eta \\in (0,\\alpha-1]$ and $(L_t)_{t \\geq 0}$ is a symmetric $\\alpha$-stable process with $\\alpha \\in (1,2)$. We derive weak and strong well posedness for this equation","when $\\eta \\leq\\alpha-1$ and $\\eta < \\alpha-1$, respectively, and show that the condition $\\eta \\leq \\alpha-1$ is sharp for weak existence.","We furthermore reformulate the equation in terms of the local time of the solution $(X_{t})_{t \\geq 0}$ and prove its well posedness.","To this end, we also derive a Tanaka-type formula for a symmetric, $\\alpha$-stable processes with $\\alpha \\in (1,2)$ that is perturbed by an adapted, right-continuous process of finite variation."],"url":"http://arxiv.org/abs/2404.13729v1","category":"math.PR"}
{"created":"2024-04-21 17:35:04","title":"MatInf -- an Extensible Open-Source Solution for Research Digitalisation in Materials Science","abstract":"Information technology and data science development stimulate transformation in many fields of scientific knowledge. In recent years, a large number of specialized systems for information and knowledge management have been created in materials science. However, the development and deployment of open adaptive systems for research support in materials science based on the acquisition, storage, and processing of different types of information remains unsolved. We propose MatInf - an extensible, open-source solution for research digitalisation in materials science based on an adaptive, flexible information management system for heterogeneous data sources. MatInf can be easily adapted to any materials science laboratory and is especially useful for collaborative projects between several labs. As an example, we demonstrate its application in high-throughput experimentation.","sentences":["Information technology and data science development stimulate transformation in many fields of scientific knowledge.","In recent years, a large number of specialized systems for information and knowledge management have been created in materials science.","However, the development and deployment of open adaptive systems for research support in materials science based on the acquisition, storage, and processing of different types of information remains unsolved.","We propose MatInf - an extensible, open-source solution for research digitalisation in materials science based on an adaptive, flexible information management system for heterogeneous data sources.","MatInf can be easily adapted to any materials science laboratory and is especially useful for collaborative projects between several labs.","As an example, we demonstrate its application in high-throughput experimentation."],"url":"http://arxiv.org/abs/2404.13722v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 16:45:35","title":"ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis","abstract":"Recent advances in generative visual models and neural radiance fields have greatly boosted 3D-aware image synthesis and stylization tasks. However, previous NeRF-based work is limited to single scene stylization, training a model to generate 3D-aware cartoon faces with arbitrary styles remains unsolved. We propose ArtNeRF, a novel face stylization framework derived from 3D-aware GAN to tackle this problem. In this framework, we utilize an expressive generator to synthesize stylized faces and a triple-branch discriminator module to improve the visual quality and style consistency of the generated faces. Specifically, a style encoder based on contrastive learning is leveraged to extract robust low-dimensional embeddings of style images, empowering the generator with the knowledge of various styles. To smooth the training process of cross-domain transfer learning, we propose an adaptive style blending module which helps inject style information and allows users to freely tune the level of stylization. We further introduce a neural rendering module to achieve efficient real-time rendering of images with higher resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in generating high-quality 3D-aware cartoon faces with arbitrary styles.","sentences":["Recent advances in generative visual models and neural radiance fields have greatly boosted 3D-aware image synthesis and stylization tasks.","However, previous NeRF-based work is limited to single scene stylization, training a model to generate 3D-aware cartoon faces with arbitrary styles remains unsolved.","We propose ArtNeRF, a novel face stylization framework derived from 3D-aware GAN to tackle this problem.","In this framework, we utilize an expressive generator to synthesize stylized faces and a triple-branch discriminator module to improve the visual quality and style consistency of the generated faces.","Specifically, a style encoder based on contrastive learning is leveraged to extract robust low-dimensional embeddings of style images, empowering the generator with the knowledge of various styles.","To smooth the training process of cross-domain transfer learning, we propose an adaptive style blending module which helps inject style information and allows users to freely tune the level of stylization.","We further introduce a neural rendering module to achieve efficient real-time rendering of images with higher resolutions.","Extensive experiments demonstrate that ArtNeRF is versatile in generating high-quality 3D-aware cartoon faces with arbitrary styles."],"url":"http://arxiv.org/abs/2404.13711v1","category":"cs.CV"}
{"created":"2024-04-21 16:29:49","title":"PEMMA: Parameter-Efficient Multi-Modal Adaptation for Medical Image Segmentation","abstract":"Imaging modalities such as Computed Tomography (CT) and Positron Emission Tomography (PET) are key in cancer detection, inspiring Deep Neural Networks (DNN) models that merge these scans for tumor segmentation. When both CT and PET scans are available, it is common to combine them as two channels of the input to the segmentation model. However, this method requires both scan types during training and inference, posing a challenge due to the limited availability of PET scans, thereby sometimes limiting the process to CT scans only. Hence, there is a need to develop a flexible DNN architecture that can be trained/updated using only CT scans but can effectively utilize PET scans when they become available. In this work, we propose a parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight upgrading of a transformer-based segmentation model trained only on CT scans to also incorporate PET scans. The benefits of the proposed approach are two-fold. Firstly, we leverage the inherent modularity of the transformer architecture and perform low-rank adaptation (LoRA) of the attention weights to achieve parameter-efficient adaptation. Secondly, since the PEMMA framework attempts to minimize cross modal entanglement, it is possible to subsequently update the combined model using only one modality, without causing catastrophic forgetting of the other modality. Our proposed method achieves comparable results with the performance of early fusion techniques with just 8% of the trainable parameters, especially with a remarkable +28% improvement on the average dice score on PET scans when trained on a single modality.","sentences":["Imaging modalities such as Computed Tomography (CT) and Positron Emission Tomography (PET) are key in cancer detection, inspiring Deep Neural Networks (DNN) models that merge these scans for tumor segmentation.","When both CT and PET scans are available, it is common to combine them as two channels of the input to the segmentation model.","However, this method requires both scan types during training and inference, posing a challenge due to the limited availability of PET scans, thereby sometimes limiting the process to CT scans only.","Hence, there is a need to develop a flexible DNN architecture that can be trained/updated using only CT scans but can effectively utilize PET scans when they become available.","In this work, we propose a parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight upgrading of a transformer-based segmentation model trained only on CT scans to also incorporate PET scans.","The benefits of the proposed approach are two-fold.","Firstly, we leverage the inherent modularity of the transformer architecture and perform low-rank adaptation (LoRA) of the attention weights to achieve parameter-efficient adaptation.","Secondly, since the PEMMA framework attempts to minimize cross modal entanglement, it is possible to subsequently update the combined model using only one modality, without causing catastrophic forgetting of the other modality.","Our proposed method achieves comparable results with the performance of early fusion techniques with just 8% of the trainable parameters, especially with a remarkable +28% improvement on the average dice score on PET scans when trained on a single modality."],"url":"http://arxiv.org/abs/2404.13704v1","category":"eess.IV"}
{"created":"2024-04-21 16:04:43","title":"A short review of the pulsar magnetic inclination angles (II)","abstract":"The pulsar magnetic inclination angle is a key parameter for pulsar physics. It influences the observable properties of pulsars, such as the pulse beam width, braking index, polarisation, and emission geometry. In this study, we give a brief overview of the current state of knowledge and research on this parameter and its implications for the internal physics of pulsars. We use the observed pulsar data of magnetic inclination angle and braking index to constrain the star's number of precession cycles, $\\xi$, which reflects the interaction between superfluid neutrons and other particles inside a neutron star\\,(NS). We apply the method proposed by Cheng et al. (2019) to analyse the data of PSR J2013+3845 and obtain the constraints for $\\xi$ ranging from $2.393\\times 10^{5}$ to $1.268\\times10^{6}$. And further analysis suggests that the internal magnetic field structure of PSR J2013+3845 is likely dominated by toroidal component. This study may help us understand the process of internal viscous dissipation and the related evolution of the inclination angles of pulsars, and may have important implications for the study of continuous gravitational wave emissions from NS.","sentences":["The pulsar magnetic inclination angle is a key parameter for pulsar physics.","It influences the observable properties of pulsars, such as the pulse beam width, braking index, polarisation, and emission geometry.","In this study, we give a brief overview of the current state of knowledge and research on this parameter and its implications for the internal physics of pulsars.","We use the observed pulsar data of magnetic inclination angle and braking index to constrain the star's number of precession cycles, $\\xi$, which reflects the interaction between superfluid neutrons and other particles inside a neutron star\\,(NS).","We apply the method proposed by Cheng et al. (2019) to analyse the data of PSR J2013+3845 and obtain the constraints for $\\xi$ ranging from $2.393\\times 10^{5}$ to $1.268\\times10^{6}$. And further analysis suggests that the internal magnetic field structure of PSR J2013+3845 is likely dominated by toroidal component.","This study may help us understand the process of internal viscous dissipation and the related evolution of the inclination angles of pulsars, and may have important implications for the study of continuous gravitational wave emissions from NS."],"url":"http://arxiv.org/abs/2404.13700v2","category":"astro-ph.HE"}
{"created":"2024-04-21 14:41:40","title":"Adaptive Social Force Window Planner with Reinforcement Learning","abstract":"Human-aware navigation is a complex task for mobile robots, requiring an autonomous navigation system capable of achieving efficient path planning together with socially compliant behaviors. Social planners usually add costs or constraints to the objective function, leading to intricate tuning processes or tailoring the solution to the specific social scenario. Machine Learning can enhance planners' versatility and help them learn complex social behaviors from data. This work proposes an adaptive social planner, using a Deep Reinforcement Learning agent to dynamically adjust the weighting parameters of the cost function used to evaluate trajectories. The resulting planner combines the robustness of the classic Dynamic Window Approach, integrated with a social cost based on the Social Force Model, and the flexibility of learning methods to boost the overall performance on social navigation tasks. Our extensive experimentation on different environments demonstrates the general advantage of the proposed method over static cost planners.","sentences":["Human-aware navigation is a complex task for mobile robots, requiring an autonomous navigation system capable of achieving efficient path planning together with socially compliant behaviors.","Social planners usually add costs or constraints to the objective function, leading to intricate tuning processes or tailoring the solution to the specific social scenario.","Machine Learning can enhance planners' versatility and help them learn complex social behaviors from data.","This work proposes an adaptive social planner, using a Deep Reinforcement Learning agent to dynamically adjust the weighting parameters of the cost function used to evaluate trajectories.","The resulting planner combines the robustness of the classic Dynamic Window Approach, integrated with a social cost based on the Social Force Model, and the flexibility of learning methods to boost the overall performance on social navigation tasks.","Our extensive experimentation on different environments demonstrates the general advantage of the proposed method over static cost planners."],"url":"http://arxiv.org/abs/2404.13678v1","category":"cs.RO"}
{"created":"2024-04-21 14:22:04","title":"FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization","abstract":"Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing \"normal\" or \"abnormal\" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of \"abnormal\" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset.","sentences":["Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories.","Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing \"normal\" or \"abnormal\" semantics and image features to detect anomalies and localize anomalous patches.","However, the generic descriptions of \"abnormal\" often fail to precisely match diverse types of anomalies across different object categories.","Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales.","To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc).","FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection.","HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes.","Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset."],"url":"http://arxiv.org/abs/2404.13671v1","category":"cs.CV"}
{"created":"2024-04-21 12:52:04","title":"Distributional Principal Autoencoders","abstract":"Dimension reduction techniques usually lose information in the sense that reconstructed data are not identical to the original data. However, we argue that it is possible to have reconstructed data identically distributed as the original data, irrespective of the retained dimension or the specific mapping. This can be achieved by learning a distributional model that matches the conditional distribution of data given its low-dimensional latent variables. Motivated by this, we propose Distributional Principal Autoencoder (DPA) that consists of an encoder that maps high-dimensional data to low-dimensional latent variables and a decoder that maps the latent variables back to the data space. For reducing the dimension, the DPA encoder aims to minimise the unexplained variability of the data with an adaptive choice of the latent dimension. For reconstructing data, the DPA decoder aims to match the conditional distribution of all data that are mapped to a certain latent value, thus ensuring that the reconstructed data retains the original data distribution. Our numerical results on climate data, single-cell data, and image benchmarks demonstrate the practical feasibility and success of the approach in reconstructing the original distribution of the data. DPA embeddings are shown to preserve meaningful structures of data such as the seasonal cycle for precipitations and cell types for gene expression.","sentences":["Dimension reduction techniques usually lose information in the sense that reconstructed data are not identical to the original data.","However, we argue that it is possible to have reconstructed data identically distributed as the original data, irrespective of the retained dimension or the specific mapping.","This can be achieved by learning a distributional model that matches the conditional distribution of data given its low-dimensional latent variables.","Motivated by this, we propose Distributional Principal Autoencoder (DPA) that consists of an encoder that maps high-dimensional data to low-dimensional latent variables and a decoder that maps the latent variables back to the data space.","For reducing the dimension, the DPA encoder aims to minimise the unexplained variability of the data with an adaptive choice of the latent dimension.","For reconstructing data, the DPA decoder aims to match the conditional distribution of all data that are mapped to a certain latent value, thus ensuring that the reconstructed data retains the original data distribution.","Our numerical results on climate data, single-cell data, and image benchmarks demonstrate the practical feasibility and success of the approach in reconstructing the original distribution of the data.","DPA embeddings are shown to preserve meaningful structures of data such as the seasonal cycle for precipitations and cell types for gene expression."],"url":"http://arxiv.org/abs/2404.13649v1","category":"stat.ML"}
{"created":"2024-04-21 08:52:22","title":"Rethink Arbitrary Style Transfer with Transformer and Contrastive Learning","abstract":"Arbitrary style transfer holds widespread attention in research and boasts numerous practical applications. The existing methods, which either employ cross-attention to incorporate deep style attributes into content attributes or use adaptive normalization to adjust content features, fail to generate high-quality stylized images. In this paper, we introduce an innovative technique to improve the quality of stylized images. Firstly, we propose Style Consistency Instance Normalization (SCIN), a method to refine the alignment between content and style features. In addition, we have developed an Instance-based Contrastive Learning (ICL) approach designed to understand the relationships among various styles, thereby enhancing the quality of the resulting stylized images. Recognizing that VGG networks are more adept at extracting classification features and need to be better suited for capturing style features, we have also introduced the Perception Encoder (PE) to capture style features. Extensive experiments demonstrate that our proposed method generates high-quality stylized images and effectively prevents artifacts compared with the existing state-of-the-art methods.","sentences":["Arbitrary style transfer holds widespread attention in research and boasts numerous practical applications.","The existing methods, which either employ cross-attention to incorporate deep style attributes into content attributes or use adaptive normalization to adjust content features, fail to generate high-quality stylized images.","In this paper, we introduce an innovative technique to improve the quality of stylized images.","Firstly, we propose Style Consistency Instance Normalization (SCIN), a method to refine the alignment between content and style features.","In addition, we have developed an Instance-based Contrastive Learning (ICL) approach designed to understand the relationships among various styles, thereby enhancing the quality of the resulting stylized images.","Recognizing that VGG networks are more adept at extracting classification features and need to be better suited for capturing style features, we have also introduced the Perception Encoder (PE) to capture style features.","Extensive experiments demonstrate that our proposed method generates high-quality stylized images and effectively prevents artifacts compared with the existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.13584v1","category":"cs.CV"}
{"created":"2024-04-21 07:03:55","title":"ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval","abstract":"Conversational search requires accurate interpretation of user intent from complex multi-turn contexts. This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval. To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data. Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever substantially outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts. Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction.","sentences":["Conversational search requires accurate interpretation of user intent from complex multi-turn contexts.","This paper presents ChatRetriever, which inherits the strong generalization capability of large language models to robustly represent complex conversational sessions for dense retrieval.","To achieve this, we propose a simple and effective dual-learning approach that adapts LLM for retrieval via contrastive learning while enhancing the complex session understanding through masked instruction tuning on high-quality conversational instruction tuning data.","Extensive experiments on five conversational search benchmarks demonstrate that ChatRetriever substantially outperforms existing conversational dense retrievers, achieving state-of-the-art performance on par with LLM-based rewriting approaches.","Furthermore, ChatRetriever exhibits superior robustness in handling diverse conversational contexts.","Our work highlights the potential of adapting LLMs for retrieval with complex inputs like conversational search sessions and proposes an effective approach to advance this research direction."],"url":"http://arxiv.org/abs/2404.13556v1","category":"cs.IR"}
{"created":"2024-04-21 06:52:45","title":"A stable and accurate discretization for fractional-order adaptive exponential integrate-and-fire models","abstract":"We introduce an efficient discretization of a novel fractional-order adaptive exponential (FrAdEx) integrate-and-fire model, which is used to study the fractional-order dynamics of neuronal activities. The discretization is based on extension of L1-type methods that can accurately handle the exponential growth and the spiking mechanism of the model. This new method is implicit and uses adaptive time stepping to robustly handle the stiff system that arises due to the exponential term. The implicit nonlinear system can be solved exactly, without the need for iterative methods, making the scheme efficient while maintaining accuracy. We present a complete error model for the numerical scheme that can be extended to other integrate-and-fire models with minor changes. To show the feasibility of our approach, the numerical method has been rigorously validated and used to investigate several different spiking oscillations of the model. We observed that the fractional-order model is capable of predicting biophysical activities, which are interpreted through phase diagrams describing the transition from one firing type to another. This simple model shows significant promise, as it has sufficient expressive dynamics to reproduce several features qualitatively from a biophysical dynamical perspective.","sentences":["We introduce an efficient discretization of a novel fractional-order adaptive exponential (FrAdEx) integrate-and-fire model, which is used to study the fractional-order dynamics of neuronal activities.","The discretization is based on extension of L1-type methods that can accurately handle the exponential growth and the spiking mechanism of the model.","This new method is implicit and uses adaptive time stepping to robustly handle the stiff system that arises due to the exponential term.","The implicit nonlinear system can be solved exactly, without the need for iterative methods, making the scheme efficient while maintaining accuracy.","We present a complete error model for the numerical scheme that can be extended to other integrate-and-fire models with minor changes.","To show the feasibility of our approach, the numerical method has been rigorously validated and used to investigate several different spiking oscillations of the model.","We observed that the fractional-order model is capable of predicting biophysical activities, which are interpreted through phase diagrams describing the transition from one firing type to another.","This simple model shows significant promise, as it has sufficient expressive dynamics to reproduce several features qualitatively from a biophysical dynamical perspective."],"url":"http://arxiv.org/abs/2404.13554v1","category":"physics.bio-ph"}
{"created":"2024-04-21 06:33:04","title":"AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition","abstract":"Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio recognition tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.","sentences":["Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms.","However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices.","Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture.","Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions.","The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers.","This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms.","The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities.","We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy.","Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy.","It also learns robustly across a variety of audio recognition tasks.","Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt."],"url":"http://arxiv.org/abs/2404.13551v1","category":"cs.SD"}
{"created":"2024-04-21 06:01:03","title":"Simultaneous action of a single photon at two remote places","abstract":"Motivated by Einstein's thought experiment that a single quantum particle diffracted after a pinhole could in principle produce an action in two or several places on a hemispherical imaging screen, here we explore theoretically the possibility to simultaneously detect the action of a single photon at two remote places. This is considered in a cascade quantum system composed of two spatially distant cavities each coupled to a qubit in the ultrastrong coupling regime. We show that a single-photon pulse incident on the two cavities can simultaneously excite the two remote qubits and lead to two subsequent detection events even when the separation between them is comparable to the spatial length of the photon pulse. Our results not only uncover new facets of quantum mechanics at a fundamental level but also have practical applications, such as the generation of remote entanglement through a dissipative channel which is otherwise unattainable in the strong-coupling regime.","sentences":["Motivated by Einstein's thought experiment that a single quantum particle diffracted after a pinhole could in principle produce an action in two or several places on a hemispherical imaging screen, here we explore theoretically the possibility to simultaneously detect the action of a single photon at two remote places.","This is considered in a cascade quantum system composed of two spatially distant cavities each coupled to a qubit in the ultrastrong coupling regime.","We show that a single-photon pulse incident on the two cavities can simultaneously excite the two remote qubits and lead to two subsequent detection events even when the separation between them is comparable to the spatial length of the photon pulse.","Our results not only uncover new facets of quantum mechanics at a fundamental level but also have practical applications, such as the generation of remote entanglement through a dissipative channel which is otherwise unattainable in the strong-coupling regime."],"url":"http://arxiv.org/abs/2404.13545v2","category":"quant-ph"}
{"created":"2024-04-21 05:06:16","title":"Splitting Techniques for DAEs with port-Hamiltonian Applications","abstract":"In the simulation of differential-algebraic equations (DAEs), it is essential to employ numerical schemes that take into account the inherent structure and maintain explicit or hidden algebraic constraints without altering them. This paper focuses on operator-splitting techniques for coupled systems and aims at preserving the structure in the port-Hamiltonian framework. The study explores two decomposition strategies: one considering the underlying coupled subsystem structure and the other addressing energy-associated properties such as conservation and dissipation. We show that for coupled index-$1$ DAEs with and without private index-2 variables, the splitting schemes on top of a dimension-reducing decomposition achieve the same convergence rate as in the case of ordinary differential equations. Additionally, we discuss an energy-associated decomposition for index-1 pH-DAEs and introduce generalized Cayley transforms to uphold energy conservation. The effectiveness of both strategies is evaluated using port-Hamiltonian benchmark examples from electric circuits.","sentences":["In the simulation of differential-algebraic equations (DAEs), it is essential to employ numerical schemes that take into account the inherent structure and maintain explicit or hidden algebraic constraints without altering them.","This paper focuses on operator-splitting techniques for coupled systems and aims at preserving the structure in the port-Hamiltonian framework.","The study explores two decomposition strategies: one considering the underlying coupled subsystem structure and the other addressing energy-associated properties such as conservation and dissipation.","We show that for coupled index-$1$ DAEs with and without private index-2 variables, the splitting schemes on top of a dimension-reducing decomposition achieve the same convergence rate as in the case of ordinary differential equations.","Additionally, we discuss an energy-associated decomposition for index-1 pH-DAEs and introduce generalized Cayley transforms to uphold energy conservation.","The effectiveness of both strategies is evaluated using port-Hamiltonian benchmark examples from electric circuits."],"url":"http://arxiv.org/abs/2404.13531v1","category":"math.NA"}
{"created":"2024-04-21 04:08:24","title":"FSGe: A fast and strongly-coupled 3D fluid-solid-growth interaction method","abstract":"Equilibrated fluid-solid-growth (FSGe) is a fast, open source, three-dimensional (3D) computational platform for simulating interactions between instantaneous hemodynamics and long-term vessel wall adaptation through growth and remodeling (G&R). Such models are crucial for capturing adaptations in health and disease and following clinical interventions. In traditional G&R models, this feedback is modeled through highly simplified fluid models, neglecting local variations in blood pressure and wall shear stress (WSS). FSGe overcomes these inherent limitations by strongly coupling the 3D Navier-Stokes equations for blood flow with a 3D equilibrated constrained mixture model (CMMe) for vascular tissue G&R. CMMe allows one to predict long-term evolved mechanobiological equilibria from an original homeostatic state at a computational cost equivalent to that of a standard hyperelastic material model. In illustrative computational examples, we focus on the development of a stable aortic aneurysm in a mouse model to highlight key differences in growth patterns and fluid-solid feedback between FSGe and solid-only G&R models. We show that FSGe is especially important in blood vessels with asymmetric stimuli. Simulation results reveal greater local variation in fluid-derived WSS than in intramural stress (IMS). Thus, differences between FSGe and G&R models became more pronounced with the growing influence of WSS relative to pressure. Future applications in highly localized disease processes, such as for lesion formation in atherosclerosis, can now include spatial and temporal variations of WSS.","sentences":["Equilibrated fluid-solid-growth (FSGe) is a fast, open source, three-dimensional (3D) computational platform for simulating interactions between instantaneous hemodynamics and long-term vessel wall adaptation through growth and remodeling (G&R).","Such models are crucial for capturing adaptations in health and disease and following clinical interventions.","In traditional G&R models, this feedback is modeled through highly simplified fluid models, neglecting local variations in blood pressure and wall shear stress (WSS).","FSGe overcomes these inherent limitations by strongly coupling the 3D Navier-Stokes equations for blood flow with a 3D equilibrated constrained mixture model (CMMe) for vascular tissue G&R.","CMMe allows one to predict long-term evolved mechanobiological equilibria from an original homeostatic state at a computational cost equivalent to that of a standard hyperelastic material model.","In illustrative computational examples, we focus on the development of a stable aortic aneurysm in a mouse model to highlight key differences in growth patterns and fluid-solid feedback between FSGe and solid-only G&R models.","We show that FSGe is especially important in blood vessels with asymmetric stimuli.","Simulation results reveal greater local variation in fluid-derived WSS than in intramural stress (IMS).","Thus, differences between FSGe and G&R models became more pronounced with the growing influence of WSS relative to pressure.","Future applications in highly localized disease processes, such as for lesion formation in atherosclerosis, can now include spatial and temporal variations of WSS."],"url":"http://arxiv.org/abs/2404.13523v1","category":"cs.CE"}
{"created":"2024-04-21 01:52:21","title":"Optimal Non-Adaptive Tolerant Junta Testing via Local Estimators","abstract":"We give a non-adaptive algorithm that makes $2^{\\tilde{O}(\\sqrt{k\\log(1/\\varepsilon_2 - \\varepsilon_1)})}$ queries to a Boolean function $f:\\{\\pm 1\\}^n \\rightarrow \\{\\pm 1\\}$ and distinguishes between $f$ being $\\varepsilon_1$-close to some $k$-junta versus $\\varepsilon_2$-far from every $k$-junta. At the heart of our algorithm is a local mean estimation procedure for Boolean functions that may be of independent interest. We complement our upper bound with a matching lower bound, improving a recent lower bound obtained by Chen et al. We thus obtain the first tight bounds for a natural property of Boolean functions in the tolerant testing model.","sentences":["We give a non-adaptive algorithm that makes $2^{\\tilde{O}(\\sqrt{k\\log(1/\\varepsilon_2 - \\varepsilon_1)})}$ queries to a Boolean function $f:\\{\\pm 1\\}^n \\rightarrow \\{\\pm 1\\}$ and distinguishes between $f$ being $\\varepsilon_1$-close to some $k$-junta versus $\\varepsilon_2$-far from every $k$-junta.","At the heart of our algorithm is a local mean estimation procedure for Boolean functions that may be of independent interest.","We complement our upper bound with a matching lower bound, improving a recent lower bound obtained by Chen et al.","We thus obtain the first tight bounds for a natural property of Boolean functions in the tolerant testing model."],"url":"http://arxiv.org/abs/2404.13502v1","category":"cs.DS"}
{"created":"2024-04-22 17:59:24","title":"A covariant formulation for cosmological radiative transfer of the 21-cm line","abstract":"The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation. In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion. These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium. We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation. The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer. Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data.","sentences":["The 21-cm hyperfine line of neutral hydrogen is a useful tool to probe the conditions of the Universe during the Dark Ages, Cosmic Dawn, and the Epoch of Reionisation.","In most of the current calculations, the 21-cm line signals at given frequencies are computed, using an integrated line-of-sight line opacity, with the correction for cosmological expansion.","These calculations have not fully captured the line and continuum interactions in the radiative transfer, in response to evolution of the radiation field and the variations of thermal and dynamic properties of the line-of-sight medium.","We construct a covariant formulation for the radiative transfer of the 21-cm line and derive the cosmological 21-cm line radiative transfer (C21LRT) equation.","The formulation properly accounts for local emission and absorption processes and the interaction between the line and continuum when the radiation propagates across the expanding Universe to the present observer.","Our C21LRT calculations show that methods simply summing the line optical depth could lead to error of $5\\%$ in the 21-cm signals for redshift $z \\sim 12-35$ and of $>10\\%$ for redshift $z \\lesssim 8$. Proper covariant radiative transfer is therefore necessary for producing correct theoretical templates for extracting information of the structural evolution of the Universe through the Epoch of Reionisation from the 21-cm tomographic data."],"url":"http://arxiv.org/abs/2404.14407v1","category":"astro-ph.CO"}
{"created":"2024-04-22 17:36:34","title":"A unified theory of tunneling times promoted by Ramsey clocks","abstract":"What time does a clock tell after quantum tunneling? Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time. Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories. Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities. Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling.","sentences":["What time does a clock tell after quantum tunneling?","Predictions and indirect measurements range from superluminal or instantaneous tunneling to finite durations, depending on the specific experiment and the precise definition of the elapsed time.","Proposals and implementations utilize the atomic motion to define this delay, even though the inherent quantum nature of atoms implies a delocalization and is in sharp contrast to classical trajectories.","Here, we rely on an operational approach: we prepare atoms in a coherent superposition of internal states and study the time read off via a Ramsey sequence after the tunneling process without the notion of classical trajectories or velocities.","Our operational framework (a) unifies definitions of tunneling delay within one approach; (b) connects the time to a frequency standard given by a conventional atomic clock which can be boosted by differential light shifts; and (c) highlights that there exists no superluminal or instantaneous tunneling."],"url":"http://arxiv.org/abs/2404.14382v1","category":"quant-ph"}
{"created":"2024-04-22 16:46:14","title":"Divergence-free framings of three-manifolds via eigenspinors","abstract":"Gromov used convex integration to prove that any closed orientable three-manifold equipped with a volume form admits three divergence-free vector fields which are linearly independent at every point. We provide an alternative proof of this (inspired by Seiberg-Witten theory) using geometric properties of eigenspinors in three dimensions. In fact, our proof shows that for any Riemannian metric, one can find three divergence-free vector fields such that at every point they are orthogonal and have the same non-zero length.","sentences":["Gromov used convex integration to prove that any closed orientable three-manifold equipped with a volume form admits three divergence-free vector fields which are linearly independent at every point.","We provide an alternative proof of this (inspired by Seiberg-Witten theory) using geometric properties of eigenspinors in three dimensions.","In fact, our proof shows that for any Riemannian metric, one can find three divergence-free vector fields such that at every point they are orthogonal and have the same non-zero length."],"url":"http://arxiv.org/abs/2404.14331v1","category":"math.DG"}
{"created":"2024-04-22 15:39:25","title":"The asymptotic stability on the line of ground states of the pure power NLS with $0<|p-3|\\ll 1$","abstract":"For exponents $p$ satisfying $0<|p-3|\\ll 1$ and only in the context of spatially even solutions we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable. The proof is similar to a related result of Martel, preprint arXiv:2312.11016, for a cubic quintic NLS. Here we modify the second part of Martel's argument, replacing the second virial inequality for a transformed problem with a smoothing estimate on the initial problem, appropriately tamed by multiplying the initial variables and equations by a cutoff.","sentences":["For exponents $p$ satisfying $0<|p-3|\\ll 1$ and only in the context of spatially even solutions we prove that the ground states of the nonlinear Schr\\\"odinger equation (NLS) with pure power nonlinearity of exponent $p$ in the line are asymptotically stable.","The proof is similar to a related result of Martel, preprint arXiv:2312.11016, for a cubic quintic NLS.","Here we modify the second part of Martel's argument, replacing the second virial inequality for a transformed problem with a smoothing estimate on the initial problem, appropriately tamed by multiplying the initial variables and equations by a cutoff."],"url":"http://arxiv.org/abs/2404.14287v1","category":"math.AP"}
{"created":"2024-04-22 15:12:04","title":"Out-of-equilibrium Chiral Magnetic Effect from simulations on Euclidean lattices","abstract":"We introduce the Euclidean-time correlator of axial charge and electric current as an observable that can be used to study the out-of-equilibrium Chiral Magnetic Effect (CME) in first-principle lattice QCD simulations with background magnetic field. This observable directly reflects the fact that in the background magnetic field, a state with nonzero axial charge features nonzero electric current. For free fermions, the axial-vector correlator only receives contributions from the Lowest Landau Level, and features a linear dependence on both magnetic field and temperature with a universal coefficient. With an appropriate regularization, non-vanishing axial-vector correlator is compatible with the vanishing of the CME current in thermal equilibrium state with nonzero chiral chemical potential $\\mu_5$. We demonstrate that the real-time counterpart of the Euclidean-time axial-vector correlator is intimately related to the real-time form of the axial anomaly equation, which strongly limits possible corrections in full QCD. We present numerical results for the Euclidean-time axial-vector correlator in $SU(2)$ lattice gauge theory with $N_f = 2$ light quark flavours, demonstrating perfect agreement with free fermion result on both sides of the chiral crossover. The proposed methodology should help to answer the question whether the QCD corrections might be responsible for non-observation of CME in RHIC isobar run.","sentences":["We introduce the Euclidean-time correlator of axial charge and electric current as an observable that can be used to study the out-of-equilibrium Chiral Magnetic Effect (CME) in first-principle lattice QCD simulations with background magnetic field.","This observable directly reflects the fact that in the background magnetic field, a state with nonzero axial charge features nonzero electric current.","For free fermions, the axial-vector correlator only receives contributions from the Lowest Landau Level, and features a linear dependence on both magnetic field and temperature with a universal coefficient.","With an appropriate regularization, non-vanishing axial-vector correlator is compatible with the vanishing of the CME current in thermal equilibrium state with nonzero chiral chemical potential $\\mu_5$. We demonstrate that the real-time counterpart of the Euclidean-time axial-vector correlator is intimately related to the real-time form of the axial anomaly equation, which strongly limits possible corrections in full QCD.","We present numerical results for the Euclidean-time axial-vector correlator in $SU(2)$ lattice gauge theory with $N_f = 2$ light quark flavours, demonstrating perfect agreement with free fermion result on both sides of the chiral crossover.","The proposed methodology should help to answer the question whether the QCD corrections might be responsible for non-observation of CME in RHIC isobar run."],"url":"http://arxiv.org/abs/2404.14263v1","category":"hep-lat"}
{"created":"2024-04-22 15:07:19","title":"Multielectron effect in strong-field ionization of CO","abstract":"We investigate the effects of the multielectron polarization of the ion described by the induced dipole potential in photoelectron momentum distributions produced in ionization of the CO molecule by a strong laser field. We present results of the numerical solution of the time-dependent Schr\\\"{o}dinger equation in three spatial dimensions and semiclassical simulations accounting for quantum interference. We predict the change of the asymmetry and interference patterns in two-dimensional photoelectron momentum distributions as well as longitudinal momentum distributions. By using a semiclassical model we identify the mechanism responsible for the observed effects. It is shown that the modifications of electron momentum distributions are caused by a combined effect of the force acting on photoelectrons due to the induced dipole potential and the linear Stark-shift of the ionization potential.","sentences":["We investigate the effects of the multielectron polarization of the ion described by the induced dipole potential in photoelectron momentum distributions produced in ionization of the CO molecule by a strong laser field.","We present results of the numerical solution of the time-dependent Schr\\\"{o}dinger equation in three spatial dimensions and semiclassical simulations accounting for quantum interference.","We predict the change of the asymmetry and interference patterns in two-dimensional photoelectron momentum distributions as well as longitudinal momentum distributions.","By using a semiclassical model we identify the mechanism responsible for the observed effects.","It is shown that the modifications of electron momentum distributions are caused by a combined effect of the force acting on photoelectrons due to the induced dipole potential and the linear Stark-shift of the ionization potential."],"url":"http://arxiv.org/abs/2404.14254v1","category":"physics.atom-ph"}
{"created":"2024-04-22 14:46:20","title":"Good, but not very good orbifolds","abstract":"We construct examples of (effective) closed orbifolds which are covered by manifolds, but not finitely so.","sentences":["We construct examples of (effective) closed orbifolds which are covered by manifolds, but not finitely so."],"url":"http://arxiv.org/abs/2404.14234v1","category":"math.GT"}
{"created":"2024-04-22 14:38:58","title":"A Survey of Decomposition-Based Evolutionary Multi-Objective Optimization: Part II -- A Data Science Perspective","abstract":"This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature related to multi-objective evolutionary algorithms based on decomposition (MOEA/D). Complementary to the first part, here we employ a series of advanced data mining approaches to provide a comprehensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol. In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5,400 papers, 10,000 authors, 400 venues, and 1,600 institutions for MOEA/D research. We start our analysis with basic descriptive statistics. Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships. We also explored the collaboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers. Our data mining results here, combined with the expert review in Part I, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective.","sentences":["This paper presents the second part of the two-part survey series on decomposition-based evolutionary multi-objective optimization where we mainly focus on discussing the literature related to multi-objective evolutionary algorithms based on decomposition (MOEA/D).","Complementary to the first part, here we employ a series of advanced data mining approaches to provide a comprehensive anatomy of the enormous landscape of MOEA/D research, which is far beyond the capacity of classic manual literature review protocol.","In doing so, we construct a heterogeneous knowledge graph that encapsulates more than 5,400 papers, 10,000 authors, 400 venues, and 1,600 institutions for MOEA/D research.","We start our analysis with basic descriptive statistics.","Then we delve into prominent research/application topics pertaining to MOEA/D with state-of-the-art topic modeling techniques and interrogate their sptial-temporal and bilateral relationships.","We also explored the collaboration and citation networks of MOEA/D, uncovering hidden patterns in the growth of literature as well as collaboration between researchers.","Our data mining results here, combined with the expert review in Part I, together offer a holistic view of the MOEA/D research, and demonstrate the potential of an exciting new paradigm for conducting scientific surveys from a data science perspective."],"url":"http://arxiv.org/abs/2404.14228v1","category":"cs.NE"}
{"created":"2024-04-22 14:09:53","title":"Generalizable Neural Human Renderer","abstract":"While recent advancements in animatable human rendering have achieved remarkable results, they require test-time optimization for each subject which can be a significant limitation for real-world applications. To address this, we tackle the challenging task of learning a Generalizable Neural Human Renderer (GNH), a novel method for rendering animatable humans from monocular video without any test-time optimization. Our core method focuses on transferring appearance information from the input video to the output image plane by utilizing explicit body priors and multi-view geometry. To render the subject in the intended pose, we utilize a straightforward CNN-based image renderer, foregoing the more common ray-sampling or rasterizing-based rendering modules. Our GNH achieves remarkable generalizable, photorealistic rendering with unseen subjects with a three-stage process. We quantitatively and qualitatively demonstrate that GNH significantly surpasses current state-of-the-art methods, notably achieving a 31.3% improvement in LPIPS.","sentences":["While recent advancements in animatable human rendering have achieved remarkable results, they require test-time optimization for each subject which can be a significant limitation for real-world applications.","To address this, we tackle the challenging task of learning a Generalizable Neural Human Renderer (GNH), a novel method for rendering animatable humans from monocular video without any test-time optimization.","Our core method focuses on transferring appearance information from the input video to the output image plane by utilizing explicit body priors and multi-view geometry.","To render the subject in the intended pose, we utilize a straightforward CNN-based image renderer, foregoing the more common ray-sampling or rasterizing-based rendering modules.","Our GNH achieves remarkable generalizable, photorealistic rendering with unseen subjects with a three-stage process.","We quantitatively and qualitatively demonstrate that GNH significantly surpasses current state-of-the-art methods, notably achieving a 31.3% improvement in LPIPS."],"url":"http://arxiv.org/abs/2404.14199v1","category":"cs.CV"}
{"created":"2024-04-22 12:19:00","title":"Prediction of flow and elastic stresses in a viscoelastic turbulent channel flow using convolutional neural networks","abstract":"Neural-network models have been employed to predict the instantaneous flow close to the wall in a viscoelastic turbulent channel flow. The numerical simulation data at the wall is utilized to predict the instantaneous velocity fluctuations and polymeric-stress fluctuations at three different wall-normal positions. Apart from predicting the velocity fluctuations well in a hibernating flow, the neural-network models are also shown to predict the polymeric shear stress and the trace of the polymeric stresses at a given wall-normal location with reasonably good accuracy. These non-intrusive sensing models can be integrated in an experimental setting to construct the polymeric-stress field in turbulent flows, which otherwise may not be directly quantifiable in experimental measurements.","sentences":["Neural-network models have been employed to predict the instantaneous flow close to the wall in a viscoelastic turbulent channel flow.","The numerical simulation data at the wall is utilized to predict the instantaneous velocity fluctuations and polymeric-stress fluctuations at three different wall-normal positions.","Apart from predicting the velocity fluctuations well in a hibernating flow, the neural-network models are also shown to predict the polymeric shear stress and the trace of the polymeric stresses at a given wall-normal location with reasonably good accuracy.","These non-intrusive sensing models can be integrated in an experimental setting to construct the polymeric-stress field in turbulent flows, which otherwise may not be directly quantifiable in experimental measurements."],"url":"http://arxiv.org/abs/2404.14121v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 12:13:40","title":"Non-degeneracy of the bubble in a fractional and singular 1D Liouville equation","abstract":"We prove the non-degeneracy of solutions to a fractional and singular Liouville equation defined on the whole real line in presence of a singular term. We use conformal transformations to rewrite the linearized equation as a Steklov eigenvalue problem posed in a bounded domain, which is defined either by an intersection or a union of two disks. We conclude by proving the simplicity of the corresponding eigenvalue.","sentences":["We prove the non-degeneracy of solutions to a fractional and singular Liouville equation defined on the whole real line in presence of a singular term.","We use conformal transformations to rewrite the linearized equation as a Steklov eigenvalue problem posed in a bounded domain, which is defined either by an intersection or a union of two disks.","We conclude by proving the simplicity of the corresponding eigenvalue."],"url":"http://arxiv.org/abs/2404.14119v1","category":"math.AP"}
{"created":"2024-04-22 12:07:51","title":"Unconstrained Lagrangian Formulation for Bosonic Continuous Spin Theory in Flat Spacetime of Arbitrary Dimension","abstract":"We have discovered two unconstrained forms of free Lagrangian for continuous spin(CS) theory in arbitrary flat spacetime dimension for bosonic case. These Lagrangians, unlike that by Schuster and Toro, do not include delta functions and are conventional. The first form consists of five kinds of totally symmetric helicity fields and one kind of gauge parameter. By introducing auxiliary creation and annihilation operators, each is combined into a state vector in Fock space, including all ranks one by one. The Lagrangian imposes no constraints, such as trace conditions, on these fields or the gauge parameter field. Additionally, the Lagrangian does not contain higher-order derivative terms. In the limit as CS parameter $\\mu$ approaches zero, it naturally reproduces a Lagrangian for helicity fields in higher spin(HS) theory, known as unconstrained quartet formulation. Permitting third-order derivatives, we also obtain the second unconstrained form of Lagrangian that can be written in terms of three kinds of fields, including $\\mu$, similar to the formulation by Francia and Sagnotti. Partial gauge fixing and partial use of equation of motion(EOM) on this Lagrangian yield a Fronsdal-like Lagrangian with a single double-traceless field, including $\\mu$. By imposing further gauge fixing on the field in the EOM with respect to divergence and trace, we confirm the reproduction of the modified Wigner equations already known in literature.","sentences":["We have discovered two unconstrained forms of free Lagrangian for continuous spin(CS) theory in arbitrary flat spacetime dimension for bosonic case.","These Lagrangians, unlike that by Schuster and Toro, do not include delta functions and are conventional.","The first form consists of five kinds of totally symmetric helicity fields and one kind of gauge parameter.","By introducing auxiliary creation and annihilation operators, each is combined into a state vector in Fock space, including all ranks one by one.","The Lagrangian imposes no constraints, such as trace conditions, on these fields or the gauge parameter field.","Additionally, the Lagrangian does not contain higher-order derivative terms.","In the limit as CS parameter $\\mu$ approaches zero, it naturally reproduces a Lagrangian for helicity fields in higher spin(HS) theory, known as unconstrained quartet formulation.","Permitting third-order derivatives, we also obtain the second unconstrained form of Lagrangian that can be written in terms of three kinds of fields, including $\\mu$, similar to the formulation by Francia and Sagnotti.","Partial gauge fixing and partial use of equation of motion(EOM) on this Lagrangian yield a Fronsdal-like Lagrangian with a single double-traceless field, including $\\mu$. By imposing further gauge fixing on the field in the EOM with respect to divergence and trace, we confirm the reproduction of the modified Wigner equations already known in literature."],"url":"http://arxiv.org/abs/2404.14118v1","category":"hep-th"}
{"created":"2024-04-22 11:58:49","title":"Desynchronization of temporal solitons in Kerr cavities with pulsed injection","abstract":"A numerical and analytical study was conducted to investigate the bifurcation mechanisms that cause desynchronization between the soliton repetition frequency and the frequency of external pulsed injection in a Kerr cavity described by the Lugiato-Lefever equation. The results suggest that desynchronization typically occurs through an Andronov-Hopf bifurcation. Additionally, a simple and intuitive criterion for this bifurcation to occur is proposed.","sentences":["A numerical and analytical study was conducted to investigate the bifurcation mechanisms that cause desynchronization between the soliton repetition frequency and the frequency of external pulsed injection in a Kerr cavity described by the Lugiato-Lefever equation.","The results suggest that desynchronization typically occurs through an Andronov-Hopf bifurcation.","Additionally, a simple and intuitive criterion for this bifurcation to occur is proposed."],"url":"http://arxiv.org/abs/2404.14113v1","category":"physics.optics"}
{"created":"2024-04-22 11:35:09","title":"Asymptotic Fermat's Last Theorem for a family of equations of signature $(2, 2n, n)$","abstract":"In this paper, we study the integer solutions of a family of Fermat-type equations of signature $(2, 2n, n)$, $Cx^2 + q^ky^{2n} = z^n$. We provide an algorithmically testable set of conditions which, if satisfied, imply the existence of a constant $B_{C, q}$ such that if $n > B_{C,q}$, there are no solutions $(x, y, z, n)$ of the equation. Our methods use the modular method for Diophantine equations, along with level lowering and Galois theory.","sentences":["In this paper, we study the integer solutions of a family of Fermat-type equations of signature $(2, 2n, n)$, $Cx^2 + q^ky^{2n} = z^n$. We provide an algorithmically testable set of conditions which, if satisfied, imply the existence of a constant $B_{C, q}$ such that if $n > B_{C,q}$, there are no solutions $(x, y, z, n)$ of the equation.","Our methods use the modular method for Diophantine equations, along with level lowering and Galois theory."],"url":"http://arxiv.org/abs/2404.14098v1","category":"math.NT"}
{"created":"2024-04-22 11:27:20","title":"Smoothing estimate for the heat semigroup with a homogeneous weight on Morrey spaces","abstract":"We study the smoothing estimate for the heat semigroup which is related to the nonlinear term of the Hardy-H\\'enon parabolic equation on Morrey spaces. This result is improvement of \\cite[Proposition 3.3]{Tayachi20}, which is proved by using weak Lebesgue and Lorentz spaces.","sentences":["We study the smoothing estimate for the heat semigroup which is related to the nonlinear term of the Hardy-H\\'enon parabolic equation on Morrey spaces.","This result is improvement of \\cite[Proposition 3.3]{Tayachi20}, which is proved by using weak Lebesgue and Lorentz spaces."],"url":"http://arxiv.org/abs/2404.14094v1","category":"math.AP"}
{"created":"2024-04-22 10:06:21","title":"Differential contributions of machine learning and statistical analysis to language and cognitive sciences","abstract":"Data-driven approaches have revolutionized scientific research. Machine learning and statistical analysis are commonly utilized in this type of research. Despite their widespread use, these methodologies differ significantly in their techniques and objectives. Few studies have utilized a consistent dataset to demonstrate these differences within the social sciences, particularly in language and cognitive sciences. This study leverages the Buckeye Speech Corpus to illustrate how both machine learning and statistical analysis are applied in data-driven research to obtain distinct insights. This study significantly enhances our understanding of the diverse approaches employed in data-driven strategies.","sentences":["Data-driven approaches have revolutionized scientific research.","Machine learning and statistical analysis are commonly utilized in this type of research.","Despite their widespread use, these methodologies differ significantly in their techniques and objectives.","Few studies have utilized a consistent dataset to demonstrate these differences within the social sciences, particularly in language and cognitive sciences.","This study leverages the Buckeye Speech Corpus to illustrate how both machine learning and statistical analysis are applied in data-driven research to obtain distinct insights.","This study significantly enhances our understanding of the diverse approaches employed in data-driven strategies."],"url":"http://arxiv.org/abs/2404.14052v1","category":"cs.CL"}
{"created":"2024-04-22 10:02:00","title":"Logarithmic convexity of non-symmetric time-fractional diffusion equations","abstract":"We consider a class of diffusion equations with the Caputo time-fractional derivative $\\partial_t^\\alpha u=L u$ subject to the homogeneous Dirichlet boundary conditions. Here, we consider a fractional order $0<\\alpha < 1$ and a second-order operator $L$ which is elliptic and non-symmetric. In this paper, we show that the logarithmic convexity extends to this non-symmetric case provided that the drift coefficient is given by a gradient vector field. Next, we perform some numerical experiments to validate the theoretical results in both symmetric and non-symmetric cases. Finally, some conclusions and open problems will be mentioned.","sentences":["We consider a class of diffusion equations with the Caputo time-fractional derivative $\\partial_t^\\alpha u=L u$ subject to the homogeneous Dirichlet boundary conditions.","Here, we consider a fractional order $0<\\alpha < 1$ and a second-order operator $L$ which is elliptic and non-symmetric.","In this paper, we show that the logarithmic convexity extends to this non-symmetric case provided that the drift coefficient is given by a gradient vector field.","Next, we perform some numerical experiments to validate the theoretical results in both symmetric and non-symmetric cases.","Finally, some conclusions and open problems will be mentioned."],"url":"http://arxiv.org/abs/2404.14046v1","category":"math.AP"}
{"created":"2024-04-22 09:51:19","title":"A stochastic population model with hierarchic size-structure","abstract":"We consider a hierarchically structured population in which the amount of resources an individual has access to is affected by individuals that are larger, and that the intake of resources by an individual only affects directly the growth rate of the individual. We formulate a deterministic model, which takes the form of a delay equation for the population birth rate. We also formulate an individual based stochastic model, and study the relationship between the two models. In particular the stationary birth rate of the deterministic model is compared to that of the quasi-stationary birth rate of the stochastic model. Since the quasi-stationary birth rate cannot be obtained explicitly, we derive a formula to approximate it. We show that the stationary birth rate of the deterministic model can be obtained as the large population limit of the quasi-stationary birth rate of the stochastic model. This relation suggests that the deterministic model is a good approximation of the stochastic model when the number of individuals is sufficiently large.","sentences":["We consider a hierarchically structured population in which the amount of resources an individual has access to is affected by individuals that are larger, and that the intake of resources by an individual only affects directly the growth rate of the individual.","We formulate a deterministic model, which takes the form of a delay equation for the population birth rate.","We also formulate an individual based stochastic model, and study the relationship between the two models.","In particular the stationary birth rate of the deterministic model is compared to that of the quasi-stationary birth rate of the stochastic model.","Since the quasi-stationary birth rate cannot be obtained explicitly, we derive a formula to approximate it.","We show that the stationary birth rate of the deterministic model can be obtained as the large population limit of the quasi-stationary birth rate of the stochastic model.","This relation suggests that the deterministic model is a good approximation of the stochastic model when the number of individuals is sufficiently large."],"url":"http://arxiv.org/abs/2404.14035v1","category":"math.AP"}
{"created":"2024-04-22 09:37:15","title":"A topological sphere theorem for submanifolds of the hyperbolic space","abstract":"We identify as topological spheres those complete submanifolds lying with any codimension in hyperbolic space whose Ricci curvature satisfies a lower bound contingent solely upon the length of the mean curvature vector of the immersion.","sentences":["We identify as topological spheres those complete submanifolds lying with any codimension in hyperbolic space whose Ricci curvature satisfies a lower bound contingent solely upon the length of the mean curvature vector of the immersion."],"url":"http://arxiv.org/abs/2404.14023v1","category":"math.DG"}
{"created":"2024-04-22 09:33:44","title":"A Multimodal Feature Distillation with CNN-Transformer Network for Brain Tumor Segmentation with Incomplete Modalities","abstract":"Existing brain tumor segmentation methods usually utilize multiple Magnetic Resonance Imaging (MRI) modalities in brain tumor images for segmentation, which can achieve better segmentation performance. However, in clinical applications, some modalities are missing due to resource constraints, leading to severe degradation in the performance of methods applying complete modality segmentation. In this paper, we propose a Multimodal feature distillation with Convolutional Neural Network (CNN)-Transformer hybrid network (MCTSeg) for accurate brain tumor segmentation with missing modalities. We first design a Multimodal Feature Distillation (MFD) module to distill feature-level multimodal knowledge into different unimodality to extract complete modality information. We further develop a Unimodal Feature Enhancement (UFE) module to model the relationship between global and local information semantically. Finally, we build a Cross-Modal Fusion (CMF) module to explicitly align the global correlations among different modalities even when some modalities are missing. Complementary features within and across different modalities are refined via the CNN-Transformer hybrid architectures in both the UFE and CMF modules, where local and global dependencies are both captured. Our ablation study demonstrates the importance of the proposed modules with CNN-Transformer networks and the convolutional blocks in Transformer for improving the performance of brain tumor segmentation with missing modalities. Extensive experiments on the BraTS2018 and BraTS2020 datasets show that the proposed MCTSeg framework outperforms the state-of-the-art methods in missing modalities cases. Our code is available at: https://github.com/mkang315/MCTSeg.","sentences":["Existing brain tumor segmentation methods usually utilize multiple Magnetic Resonance Imaging (MRI) modalities in brain tumor images for segmentation, which can achieve better segmentation performance.","However, in clinical applications, some modalities are missing due to resource constraints, leading to severe degradation in the performance of methods applying complete modality segmentation.","In this paper, we propose a Multimodal feature distillation with Convolutional Neural Network (CNN)-Transformer hybrid network (MCTSeg) for accurate brain tumor segmentation with missing modalities.","We first design a Multimodal Feature Distillation (MFD) module to distill feature-level multimodal knowledge into different unimodality to extract complete modality information.","We further develop a Unimodal Feature Enhancement (UFE) module to model the relationship between global and local information semantically.","Finally, we build a Cross-Modal Fusion (CMF) module to explicitly align the global correlations among different modalities even when some modalities are missing.","Complementary features within and across different modalities are refined via the CNN-Transformer hybrid architectures in both the UFE and CMF modules, where local and global dependencies are both captured.","Our ablation study demonstrates the importance of the proposed modules with CNN-Transformer networks and the convolutional blocks in Transformer for improving the performance of brain tumor segmentation with missing modalities.","Extensive experiments on the BraTS2018 and BraTS2020 datasets show that the proposed MCTSeg framework outperforms the state-of-the-art methods in missing modalities cases.","Our code is available at: https://github.com/mkang315/MCTSeg."],"url":"http://arxiv.org/abs/2404.14019v1","category":"cs.CV"}
{"created":"2024-04-22 09:17:18","title":"Carleman estimates for higher order partial differential operators and its applications","abstract":"In this paper, we obtain a Carleman estimate for the higher order partial differential operator. In the process of establishing this estimate, we developed a new method, which is called the back-propagation method (the BPM, for short). This method can also be used to build up Carleman estimates for some other partial differential operators, and might provide assistance with corresponding numerical analyses. As an application of the above-mentioned Carleman estimate, we proved the conditional stability of a Cauchy problem for a time fractional diffusion equation.","sentences":["In this paper, we obtain a Carleman estimate for the higher order partial differential operator.","In the process of establishing this estimate, we developed a new method, which is called the back-propagation method (the BPM, for short).","This method can also be used to build up Carleman estimates for some other partial differential operators, and might provide assistance with corresponding numerical analyses.","As an application of the above-mentioned Carleman estimate, we proved the conditional stability of a Cauchy problem for a time fractional diffusion equation."],"url":"http://arxiv.org/abs/2404.14008v1","category":"math.AP"}
{"created":"2024-04-22 07:24:50","title":"A bound preserving cut discontinuous Galerkin method for one dimensional hyperbolic conservation laws","abstract":"In this paper we present a family of high order cut finite element methods with bound preserving properties for hyperbolic conservation laws in one space dimension. The methods are based on the discontinuous Galerkin framework and use a regular background mesh, where interior boundaries are allowed to cut through the mesh arbitrarily. Our methods include ghost penalty stabilization to handle small cut elements and a new reconstruction of the approximation on macro-elements, which are local patches consisting of cut and un-cut neighboring elements that are connected by stabilization. We show that the reconstructed solution retains conservation and optimal order of accuracy. Our lowest order scheme results in a piecewise constant solution that satisfies a maximum principle for scalar hyperbolic conservation laws. When the lowest order scheme is applied to the Euler equations, the scheme is positivity preserving in the sense that positivity of pressure and density are retained. For the high order schemes, suitable bound preserving limiters are applied to the reconstructed solution on macro-elements. In the scalar case, a maximum principle limiter is applied, which ensures that the limited approximation satisfies the maximum principle. Correspondingly, we use a positivity preserving limiter for the Euler equations, and show that our scheme is positivity preserving. In the presence of shocks additional limiting is needed to avoid oscillations, hence we apply a standard TVB limiter to the reconstructed solution. The time step restrictions are of the same order as for the corresponding discontinuous Galerkin methods on the background mesh. Numerical computations illustrate accuracy, bound preservation, and shock capturing capabilities of the proposed schemes.","sentences":["In this paper we present a family of high order cut finite element methods with bound preserving properties for hyperbolic conservation laws in one space dimension.","The methods are based on the discontinuous Galerkin framework and use a regular background mesh, where interior boundaries are allowed to cut through the mesh arbitrarily.","Our methods include ghost penalty stabilization to handle small cut elements and a new reconstruction of the approximation on macro-elements, which are local patches consisting of cut and un-cut neighboring elements that are connected by stabilization.","We show that the reconstructed solution retains conservation and optimal order of accuracy.","Our lowest order scheme results in a piecewise constant solution that satisfies a maximum principle for scalar hyperbolic conservation laws.","When the lowest order scheme is applied to the Euler equations, the scheme is positivity preserving in the sense that positivity of pressure and density are retained.","For the high order schemes, suitable bound preserving limiters are applied to the reconstructed solution on macro-elements.","In the scalar case, a maximum principle limiter is applied, which ensures that the limited approximation satisfies the maximum principle.","Correspondingly, we use a positivity preserving limiter for the Euler equations, and show that our scheme is positivity preserving.","In the presence of shocks additional limiting is needed to avoid oscillations, hence we apply a standard TVB limiter to the reconstructed solution.","The time step restrictions are of the same order as for the corresponding discontinuous Galerkin methods on the background mesh.","Numerical computations illustrate accuracy, bound preservation, and shock capturing capabilities of the proposed schemes."],"url":"http://arxiv.org/abs/2404.13936v1","category":"math.NA"}
{"created":"2024-04-22 07:23:18","title":"A whole-body mathematical model of cholesterol metabolism and transport","abstract":"Cardiovascular diseases are the leading cause of death. Increased levels of plasma cholesterol are consistently associated with an increased risk of cardiovascular disease. As a result, it is imperative that studies are conducted to determine the best course of action to reduce whole-body cholesterol levels. A whole-body mathematical model for cholesterol metabolism and transport is proposed. The model can simulate the effects of lipid-lowering drugs like statins and anti-PCSK9. The model is based on ordinary differential equations and kinetic functions. It has been validated against literature data. It offers a versatile platform for designing personalized interventions for cardiovascular health management.","sentences":["Cardiovascular diseases are the leading cause of death.","Increased levels of plasma cholesterol are consistently associated with an increased risk of cardiovascular disease.","As a result, it is imperative that studies are conducted to determine the best course of action to reduce whole-body cholesterol levels.","A whole-body mathematical model for cholesterol metabolism and transport is proposed.","The model can simulate the effects of lipid-lowering drugs like statins and anti-PCSK9.","The model is based on ordinary differential equations and kinetic functions.","It has been validated against literature data.","It offers a versatile platform for designing personalized interventions for cardiovascular health management."],"url":"http://arxiv.org/abs/2404.13934v1","category":"q-bio.QM"}
{"created":"2024-04-22 06:51:11","title":"Solutions to quantum tetrahedron equation with two colors and nonnegative matrix entries","abstract":"In this short note, we construct solutions to quantum tetrahedron equation in which there are two $\\mathcal R$-operators each depending on one parameter, while these parameters are independent from each other. The equation is of the kind \"with variables on the edges\", and these variables take just two values (called sometimes \"colors\"). All matrix entries of our $\\mathcal R$-operators are nonnegative, if the parameters are chosen properly, and the number of strictly positive entries is either 12 or 14.","sentences":["In this short note, we construct solutions to quantum tetrahedron equation in which there are two $\\mathcal R$-operators each depending on one parameter, while these parameters are independent from each other.","The equation is of the kind \"with variables on the edges\", and these variables take just two values (called sometimes \"colors\").","All matrix entries of our $\\mathcal R$-operators are nonnegative, if the parameters are chosen properly, and the number of strictly positive entries is either 12 or 14."],"url":"http://arxiv.org/abs/2404.13913v1","category":"math.QA"}
{"created":"2024-04-22 05:32:10","title":"Producing Fully-Charmed Tetraquarks via Charm Quark Fragmentation in Colliders","abstract":"Within the framework of nonrelativistic QCD (NRQCD), we calculate the fragmentation function for a charm quark into an $S$-wave fully-charmed tetraquark, denoted as $T_{4c}$. The charm-to-$T_{4c}$ fragmentation function can be expressed as a sum of products of the perturbatively calculable short-distance coefficients and the nonperturbative long-distance matrix elements. The short-distance coefficients are ascertained through the perturbative matching procedure at lowest order in $\\alpha_{s}$ and $v$ expansion. Incorporating the celebrated QCD factorization and the charm-to-$T_{4c}$ fragmentation function, we are able to predict the $T_{4c}$ production rate at high transverse momentum $p_T$ in colliders. In phenomenology, we approximate the NRQCD matrix elements with phenomenological four-body wave functions at the origin. After applying appropriate kinematic constraints, both the differential distribution over $p_T$ and the integrated cross sections are predicted at the \\texttt{LHC}. The cross section can reach approximately $100$ fb, indicating the potential for a significant number of $T_{4c}$ events to be produced at the \\texttt{LHC}. Additionally, we provide predictions for the photoproduction of $T_{4c}$ in electron-proton ($ep$) collisions. However, it is observed that the cross sections for these processes are relatively small at the \\texttt{HERA}, \\texttt{EIC} as well as \\texttt{EicC}. Given these findings, the prospect of detecting these fully heavy tetraquarks at $ep$ colliders appears to be rather challenging.","sentences":["Within the framework of nonrelativistic QCD (NRQCD), we calculate the fragmentation function for a charm quark into an $S$-wave fully-charmed tetraquark, denoted as $T_{4c}$. The charm-to-$T_{4c}$ fragmentation function can be expressed as a sum of products of the perturbatively calculable short-distance coefficients and the nonperturbative long-distance matrix elements.","The short-distance coefficients are ascertained through the perturbative matching procedure at lowest order in $\\alpha_{s}$ and $v$ expansion.","Incorporating the celebrated QCD factorization and the charm-to-$T_{4c}$ fragmentation function, we are able to predict the $T_{4c}$ production rate at high transverse momentum $p_T$ in colliders.","In phenomenology, we approximate the NRQCD matrix elements with phenomenological four-body wave functions at the origin.","After applying appropriate kinematic constraints, both the differential distribution over $p_T$ and the integrated cross sections are predicted at the \\texttt{LHC}.","The cross section can reach approximately $100$ fb, indicating the potential for a significant number of $T_{4c}$ events to be produced at the \\texttt{LHC}.","Additionally, we provide predictions for the photoproduction of $T_{4c}$ in electron-proton ($ep$) collisions.","However, it is observed that the cross sections for these processes are relatively small at the \\texttt{HERA}, \\texttt{EIC} as well as \\texttt{EicC}.","Given these findings, the prospect of detecting these fully heavy tetraquarks at $ep$ colliders appears to be rather challenging."],"url":"http://arxiv.org/abs/2404.13889v1","category":"hep-ph"}
{"created":"2024-04-22 05:23:53","title":"The complete exterior spacetime of spherical Brans-Dicke stars","abstract":"We derive the complete expression for the Brans Class I exterior spacetime explicitly in terms of the energy and pressures profiles of a stationary spherisymmetric gravity source. This novel and generic expression is achieved in a $\\textit{parsimonious}$ manner, requiring only a subset of the Brans-Dicke field equation and the scalar equation. For distant orbiting test particles, this expression promptly provides a simple, closed and exact formula of the $\\gamma$ Eddington parameter, which reads $\\gamma_{\\,\\text{exact}}=\\frac{\\omega+1+(\\omega+2)\\,\\varTheta}{\\omega+2+(\\omega+1)\\,\\varTheta}$, where $\\varTheta$ is the ratio of the star's \"total pressure\" integral over its energy integral. This $\\textit{non-perturbative}$ result reproduces the usual Post-Newtonian $\\frac{\\omega+1}{\\omega+2}$ expression in the case of a \"Newtonian star\", in which the pressure is negligible with respect to the energy density. Furthermore, it converges to the General Relativity value $\\gamma_{\\,\\text{GR}}=1$ as the star's equation of state approaches that of ultra-relativistic matter (in which case $\\varTheta$ approaches 1), a behavior consistent with broader studies on scalar-tensor gravity. Our derivation underscores the essence of these results involving (1) the key relevant portion of the Brans-Dicke field equations, (2) the uniqueness of the Brans Class I vacuum solution for the non-phantom action, viz. $\\omega>-3/2$, and (3) the involvement of only two free parameters in this solution. From a practical standpoint, it elucidates how a given stellar interior structure model determines the star's exterior gravitational field and impacts the motions of light objects (such as planets and accretion disks) orbiting it.","sentences":["We derive the complete expression for the Brans Class I exterior spacetime explicitly in terms of the energy and pressures profiles of a stationary spherisymmetric gravity source.","This novel and generic expression is achieved in a $\\textit{parsimonious}$ manner, requiring only a subset of the Brans-Dicke field equation and the scalar equation.","For distant orbiting test particles, this expression promptly provides a simple, closed and exact formula of the $\\gamma$ Eddington parameter, which reads $\\gamma_{\\,\\text{exact}}=\\frac{\\omega+1+(\\omega+2)\\,\\varTheta}{\\omega+2+(\\omega+1)\\,\\varTheta}$, where $\\varTheta$ is the ratio of the star's \"total pressure\" integral over its energy integral.","This $\\textit{non-perturbative}$ result reproduces the usual Post-Newtonian $\\frac{\\omega+1}{\\omega+2}$ expression in the case of a \"Newtonian star\", in which the pressure is negligible with respect to the energy density.","Furthermore, it converges to the General Relativity value $\\gamma_{\\,\\text{GR}}=1$ as the star's equation of state approaches that of ultra-relativistic matter (in which case $\\varTheta$ approaches 1), a behavior consistent with broader studies on scalar-tensor gravity.","Our derivation underscores the essence of these results involving (1) the key relevant portion of the Brans-Dicke field equations, (2) the uniqueness of the Brans Class I vacuum solution for the non-phantom action, viz.","$\\omega>-3/2$, and (3) the involvement of only two free parameters in this solution.","From a practical standpoint, it elucidates how a given stellar interior structure model determines the star's exterior gravitational field and impacts the motions of light objects (such as planets and accretion disks) orbiting it."],"url":"http://arxiv.org/abs/2404.13887v1","category":"gr-qc"}
{"created":"2024-04-22 05:08:03","title":"Spin Wave Optics","abstract":"Similarity between Walker and Helmholtz equations encouraged many to search for analogies between optical and spin wave phenomena. In the present article we demonstrate that one to one relationship can be established by formalizing the concept of magnetic refractive index and deriving on its basis Eikonal equations for magnetic media, formally proving that a very substantial portion of optical devices: lenses, mirrors, waveguides and so on, can be implemented as magnetic devices operating on spin waves instead of optical radiation. Controlling the refractive index is accomplished by changing the environmental variables such as magnetic bias field or temperature. Functionality of the above mentioned devices is confirmed with micromagnetic simulations, which also demonstrate a substantial agreement with the analytical model introduced in the present manuscript.","sentences":["Similarity between Walker and Helmholtz equations encouraged many to search for analogies between optical and spin wave phenomena.","In the present article we demonstrate that one to one relationship can be established by formalizing the concept of magnetic refractive index and deriving on its basis Eikonal equations for magnetic media, formally proving that a very substantial portion of optical devices: lenses, mirrors, waveguides and so on, can be implemented as magnetic devices operating on spin waves instead of optical radiation.","Controlling the refractive index is accomplished by changing the environmental variables such as magnetic bias field or temperature.","Functionality of the above mentioned devices is confirmed with micromagnetic simulations, which also demonstrate a substantial agreement with the analytical model introduced in the present manuscript."],"url":"http://arxiv.org/abs/2404.13882v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 04:22:30","title":"PGAHum: Prior-Guided Geometry and Appearance Learning for High-Fidelity Animatable Human Reconstruction","abstract":"Recent techniques on implicit geometry representation learning and neural rendering have shown promising results for 3D clothed human reconstruction from sparse video inputs. However, it is still challenging to reconstruct detailed surface geometry and even more difficult to synthesize photorealistic novel views with animated human poses. In this work, we introduce PGAHum, a prior-guided geometry and appearance learning framework for high-fidelity animatable human reconstruction. We thoroughly exploit 3D human priors in three key modules of PGAHum to achieve high-quality geometry reconstruction with intricate details and photorealistic view synthesis on unseen poses. First, a prior-based implicit geometry representation of 3D human, which contains a delta SDF predicted by a tri-plane network and a base SDF derived from the prior SMPL model, is proposed to model the surface details and the body shape in a disentangled manner. Second, we introduce a novel prior-guided sampling strategy that fully leverages the prior information of the human pose and body to sample the query points within or near the body surface. By avoiding unnecessary learning in the empty 3D space, the neural rendering can recover more appearance details. Last, we propose a novel iterative backward deformation strategy to progressively find the correspondence for the query point in observation space. A skinning weights prediction model is learned based on the prior provided by the SMPL model to achieve the iterative backward LBS deformation. Extensive quantitative and qualitative comparisons on various datasets are conducted and the results demonstrate the superiority of our framework. Ablation studies also verify the effectiveness of each scheme for geometry and appearance learning.","sentences":["Recent techniques on implicit geometry representation learning and neural rendering have shown promising results for 3D clothed human reconstruction from sparse video inputs.","However, it is still challenging to reconstruct detailed surface geometry and even more difficult to synthesize photorealistic novel views with animated human poses.","In this work, we introduce PGAHum, a prior-guided geometry and appearance learning framework for high-fidelity animatable human reconstruction.","We thoroughly exploit 3D human priors in three key modules of PGAHum to achieve high-quality geometry reconstruction with intricate details and photorealistic view synthesis on unseen poses.","First, a prior-based implicit geometry representation of 3D human, which contains a delta SDF predicted by a tri-plane network and a base SDF derived from the prior SMPL model, is proposed to model the surface details and the body shape in a disentangled manner.","Second, we introduce a novel prior-guided sampling strategy that fully leverages the prior information of the human pose and body to sample the query points within or near the body surface.","By avoiding unnecessary learning in the empty 3D space, the neural rendering can recover more appearance details.","Last, we propose a novel iterative backward deformation strategy to progressively find the correspondence for the query point in observation space.","A skinning weights prediction model is learned based on the prior provided by the SMPL model to achieve the iterative backward LBS deformation.","Extensive quantitative and qualitative comparisons on various datasets are conducted and the results demonstrate the superiority of our framework.","Ablation studies also verify the effectiveness of each scheme for geometry and appearance learning."],"url":"http://arxiv.org/abs/2404.13862v1","category":"cs.CV"}
{"created":"2024-04-22 02:46:58","title":"Co-evolution of dust grains and protoplanetary disks II: structure and evolution of protoplanetary disks; an analytical approach","abstract":"In our previous study (Tsukamoto {\\it et al.} 2023), we investigated formation and early evolution of protoplanetary disks with 3D non-ideal magnetohydrodynamics simulations considering dust growth, and found that the modified equations of the conventional steady accretion disk model which consider the magnetic braking, { dust growth} and ambipolar diffusion reproduce the disk structure obtained from simulations very well. In this paper, as a sequel of the our previous study, we analytically investigate the structure and evolution of protoplanetary disks corresponding to Class 0/I young stellar objects using the modified steady accretion disk model combining an analytical model of envelope accretion. We estimate that the disk radius is several AU at disk formation epoch and increases to several 100 AU at the end of the accretion phase. The disk mass is estimated to be $0.01 M_\\odot \\lesssim M_{\\rm disk} \\lesssim 0.1 M_\\odot$ for a disk with radius of several 10 AU and mass accretion rate of $\\dot{M}_{\\rm disk} \\sim 10^{-6} M_\\odot {\\rm yr^{-1}}$. We also found that, with typical disk ionization rates and moderate mass accretion rate ($\\dot{M}_{\\rm disk}\\gtrsim10^{-8} M_\\odot {\\rm yr^{-1}}$), magneto-rotational instability is suppressed in the disk because of low plasma $\\beta$ and efficient ambipolar diffusion. We argue that the radial profile of specific angular momentum (or rotational velocity) at the disk outer edge should be continuously connected to that of the envelope if the disk evolves by magnetic braking, and should be discontinuous if the disk evolves by internal angular momentum transport process such as gravitational instability or magneto-rotational instability. Future detailed observations of the specific angular momentum profile around the disk outer edge are important for understanding the angular momentum transport mechanism of protoplanetary disks.","sentences":["In our previous study (Tsukamoto {\\it et al.} 2023), we investigated formation and early evolution of protoplanetary disks with 3D non-ideal magnetohydrodynamics simulations considering dust growth, and found that the modified equations of the conventional steady accretion disk model which consider the magnetic braking, { dust growth} and ambipolar diffusion reproduce the disk structure obtained from simulations very well.","In this paper, as a sequel of the our previous study, we analytically investigate the structure and evolution of protoplanetary disks corresponding to Class 0/I young stellar objects using the modified steady accretion disk model combining an analytical model of envelope accretion.","We estimate that the disk radius is several AU at disk formation epoch and increases to several 100 AU at the end of the accretion phase.","The disk mass is estimated to be $0.01 M_\\odot \\lesssim M_{\\rm disk} \\lesssim 0.1 M_\\odot$ for a disk with radius of several 10 AU and mass accretion rate of $\\dot{M}_{\\rm disk} \\sim 10^{-6} M_\\odot {\\rm yr^{-1}}$. We also found that, with typical disk ionization rates and moderate mass accretion rate ($\\dot{M}_{\\rm disk}\\gtrsim10^{-8} M_\\odot {\\rm yr^{-1}}$), magneto-rotational instability is suppressed in the disk because of low plasma $\\beta$ and efficient ambipolar diffusion.","We argue that the radial profile of specific angular momentum (or rotational velocity) at the disk outer edge should be continuously connected to that of the envelope if the disk evolves by magnetic braking, and should be discontinuous if the disk evolves by internal angular momentum transport process such as gravitational instability or magneto-rotational instability.","Future detailed observations of the specific angular momentum profile around the disk outer edge are important for understanding the angular momentum transport mechanism of protoplanetary disks."],"url":"http://arxiv.org/abs/2404.13843v1","category":"astro-ph.SR"}
{"created":"2024-04-22 02:14:46","title":"The Self-Consistency of DESI Analysis and Comment on \"Does DESI 2024 Confirm $\u039b$CDM?\"","abstract":"We demonstrate that the constraints on the evolution of dark energy implemented by the DESI collaboration may be insufficient or incomplete using their own BAO data. Using large enough prior ranges for the present-day equation of state of dark energy $\\omega_0$ and amplitude of dark energy evolution $\\omega_a$, we obtain the complete $1\\,\\sigma$ and $2\\,\\sigma$ constraints $\\omega_0=1.04^{+0.91+2.00}_{-1.00-1.90}$ and $\\omega_a=-7.4^{+3.8+6.8}_{-3.2-7.3}$ indicating a beyond $2\\,\\sigma$ preference of quintessence-like dark energy today and an evidence of evolving dark energy at beyond $2\\,\\sigma$ CL, respectively. Our results are different from $\\omega_0=-0.55^{+0.39}_{-0.21}$ and the $2\\,\\sigma$ upper limit $\\omega_a<-1.32$ reported by the DESI collaboration \\cite{DESI:2024mwx}. Employing a data combination of cosmic microwave background, DESI BAO and type Ia supernova, we obtain the $1\\,\\sigma$, $2\\,\\sigma$ and $3\\,\\sigma$ constraints $\\omega_0=-0.707^{+0.089+0.18+0.24}_{-0.089-0.17-0.22}$ and $\\omega_a=-1.09^{+0.38+0.67+0.82}_{-0.31-0.72-1.00}$, which reveals a $\\sim4\\,\\sigma$ evidence of dynamical dark energy when the redshift $z\\lesssim0.1$. We verify that the BAO data point from luminous red galaxies at the effective redshift $z_{\\rm eff}=0.51$ hardly affects the joint constraint from the data combination of cosmic microwave background, DESI BAO and type Ia supernova. We also point out the shortcomings and advantages of the binning method widely used in cosmological analyses.","sentences":["We demonstrate that the constraints on the evolution of dark energy implemented by the DESI collaboration may be insufficient or incomplete using their own BAO data.","Using large enough prior ranges for the present-day equation of state of dark energy $\\omega_0$ and amplitude of dark energy evolution $\\omega_a$, we obtain the complete $1\\,\\sigma$ and $2\\,\\sigma$ constraints $\\omega_0=1.04^{+0.91+2.00}_{-1.00-1.90}$ and $\\omega_a=-7.4^{+3.8+6.8}_{-3.2-7.3}$ indicating a beyond $2\\,\\sigma$ preference of quintessence-like dark energy today and an evidence of evolving dark energy at beyond $2\\,\\sigma$ CL, respectively.","Our results are different from $\\omega_0=-0.55^{+0.39}_{-0.21}$ and the $2\\,\\sigma$ upper limit $\\omega_a<-1.32$ reported by the DESI collaboration \\cite{DESI:2024mwx}.","Employing a data combination of cosmic microwave background, DESI BAO and type Ia supernova, we obtain the $1\\,\\sigma$, $2\\,\\sigma$ and $3\\,\\sigma$ constraints $\\omega_0=-0.707^{+0.089+0.18+0.24}_{-0.089-0.17-0.22}$ and $\\omega_a=-1.09^{+0.38+0.67+0.82}_{-0.31-0.72-1.00}$, which reveals a $\\sim4\\,\\sigma$ evidence of dynamical dark energy when the redshift $z\\lesssim0.1$. We verify that the BAO data point from luminous red galaxies at the effective redshift $z_{\\rm eff}=0.51$ hardly affects the joint constraint from the data combination of cosmic microwave background, DESI BAO and type Ia supernova.","We also point out the shortcomings and advantages of the binning method widely used in cosmological analyses."],"url":"http://arxiv.org/abs/2404.13833v1","category":"astro-ph.CO"}
{"created":"2024-04-22 01:56:33","title":"General-Relativistic Magnetohydrodynamic Equations: the bare essential","abstract":"Recent years have seen a significant progress in the development of general relativistic codes for the numerical solution of the equations of magnetohydrodynamics in spacetimes with high and dynamical curvature. These codes are valuable tools to explore the large-scale plasma dynamics such as that takes place when two neutron stars collide or when matter accretes onto a supermassive black hole. This chapter is meant to provide a very brief but complete overview of the set of equations that are normally solved in modern numerical codes after they are cast into a conservative formulation within a 3+1 split of spacetime.","sentences":["Recent years have seen a significant progress in the development of general relativistic codes for the numerical solution of the equations of magnetohydrodynamics in spacetimes with high and dynamical curvature.","These codes are valuable tools to explore the large-scale plasma dynamics such as that takes place when two neutron stars collide or when matter accretes onto a supermassive black hole.","This chapter is meant to provide a very brief but complete overview of the set of equations that are normally solved in modern numerical codes after they are cast into a conservative formulation within a 3+1 split of spacetime."],"url":"http://arxiv.org/abs/2404.13824v1","category":"astro-ph.HE"}
{"created":"2024-04-22 01:48:58","title":"Prove Symbolic Regression is NP-hard by Symbol Graph","abstract":"Symbolic regression (SR) is the task of discovering a symbolic expression that fits a given data set from the space of mathematical expressions. Despite the abundance of research surrounding the SR problem, there's a scarcity of works that confirm its NP-hard nature. Therefore, this paper introduces the concept of a symbol graph as a comprehensive representation of the entire mathematical expression space, effectively illustrating the NP-hard characteristics of the SR problem. Leveraging the symbol graph, we establish a connection between the SR problem and the task of identifying an optimally fitted degree-constrained Steiner Arborescence (DCSAP). The complexity of DCSAP, which is proven to be NP-hard, directly implies the NP-hard nature of the SR problem.","sentences":["Symbolic regression (SR) is the task of discovering a symbolic expression that fits a given data set from the space of mathematical expressions.","Despite the abundance of research surrounding the SR problem, there's a scarcity of works that confirm its NP-hard nature.","Therefore, this paper introduces the concept of a symbol graph as a comprehensive representation of the entire mathematical expression space, effectively illustrating the NP-hard characteristics of the SR problem.","Leveraging the symbol graph, we establish a connection between the SR problem and the task of identifying an optimally fitted degree-constrained Steiner Arborescence (DCSAP).","The complexity of DCSAP, which is proven to be NP-hard, directly implies the NP-hard nature of the SR problem."],"url":"http://arxiv.org/abs/2404.13820v1","category":"cs.CC"}
{"created":"2024-04-22 01:36:50","title":"Neural Radiance Field in Autonomous Driving: A Survey","abstract":"Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF's applications in the context of AD. Our survey is structured to categorize NeRF's applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain.","sentences":["Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities.","With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD).","However, a conspicuous void is apparent within the current literature.","To bridge this gap, this paper conducts a comprehensive survey of NeRF's applications in the context of AD.","Our survey is structured to categorize NeRF's applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation.","We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field.","We hope this paper serves as a comprehensive reference for researchers in this domain.","To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain."],"url":"http://arxiv.org/abs/2404.13816v1","category":"cs.CV"}
{"created":"2024-04-22 00:44:13","title":"FaceFolds: Meshed Radiance Manifolds for Efficient Volumetric Rendering of Dynamic Faces","abstract":"3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts$\\unicode{x2014}$photorealism, efficiency, compatibility, and configurability. We present a novel representation that enables high-quality volumetric rendering of an actor's dynamic facial performances with minimal compute and memory footprint. It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency. Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects. We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture. We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration. We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates.","sentences":["3D rendering of dynamic face captures is a challenging problem, and it demands improvements on several fronts$\\unicode{x2014}$photorealism, efficiency, compatibility, and configurability.","We present a novel representation that enables high-quality volumetric rendering of an actor's dynamic facial performances with minimal compute and memory footprint.","It runs natively on commodity graphics soft- and hardware, and allows for a graceful trade-off between quality and efficiency.","Our method utilizes recent advances in neural rendering, particularly learning discrete radiance manifolds to sparsely sample the scene to model volumetric effects.","We achieve efficient modeling by learning a single set of manifolds for the entire dynamic sequence, while implicitly modeling appearance changes as temporal canonical texture.","We export a single layered mesh and view-independent RGBA texture video that is compatible with legacy graphics renderers without additional ML integration.","We demonstrate our method by rendering dynamic face captures of real actors in a game engine, at comparable photorealism to state-of-the-art neural rendering techniques at previously unseen frame rates."],"url":"http://arxiv.org/abs/2404.13807v1","category":"cs.CV"}
{"created":"2024-04-22 00:42:19","title":"On the existence of ground states to Hartree-type equations in $\\mathbb{R}^3$ with a delta potential","abstract":"Consider the Hartree-type equation in $\\mathbb{R}^3$ with a delta potential formally described by $$ i \\partial_t \\psi = - \\Delta_x \\psi + \\alpha \\delta_0 \\psi - (I_\\beta \\ast |\\psi|^p) |\\psi|^{p - 2} \\psi $$ where $\\alpha \\in \\mathbb{R}$; $0 < \\beta < 3$ and we want to solve for $\\psi \\colon \\mathbb{R}^3 \\times \\mathbb{R} \\to \\mathbb{C}$. By means of a Poho\\v{z}aev identity, we show that if $p = (3 + \\beta) / 3$ and $\\alpha \\geq 0$, then the problem has no ground state at any mass $\\mu > 0$. We also prove that if $$ \\frac{3 + \\beta}{3} < p < \\min \\left(   \\frac{5 + \\beta}{3}, \\frac{5 + 2 \\beta}{4} \\right), $$ which includes the physically-relevant case $p = \\beta = 2$, then the problem admits a ground state at any mass $\\mu > 0$.","sentences":["Consider the Hartree-type equation in $\\mathbb{R}^3$ with a delta potential formally described by $$ i \\partial_t \\psi = - \\Delta_x \\psi + \\alpha \\delta_0 \\psi - (I_\\beta \\ast |\\psi|^p) |\\psi|^{p - 2} \\psi $$ where $\\alpha \\in \\mathbb{R}$; $0 <","\\beta < 3$ and we want to solve for $\\psi \\colon \\mathbb{R}^3 \\times \\mathbb{R} \\to \\mathbb{C}$. By means of a Poho\\v{z}aev identity, we show that if $p = (3 + \\beta) / 3$ and $\\alpha \\geq 0$, then the problem has no ground state at any mass $\\mu >","0$.","We also prove that if $$ \\frac{3 + \\beta}{3} < p < \\min \\left(   \\frac{5 + \\beta}{3}, \\frac{5 + 2 \\beta}{4} \\right), $$ which includes the physically-relevant case $p = \\beta = 2$, then the problem admits a ground state at any mass $\\mu > 0$."],"url":"http://arxiv.org/abs/2404.13806v1","category":"math.AP"}
{"created":"2024-04-21 23:27:47","title":"On pseudo-Riemannian Ricci-parallel Lie groups which are not Einstein","abstract":"In this paper, we mainly study left invariant pseudo-Riemannian Ricci-parallel metrics on connected Lie groups which are not Einstein. Following a result of Boubel and B\\'{e}rard Bergery, there are two typical types of such metrics, which are characterized by the minimal polynomial of the Ricci operator. Namely, its form is either $(X-\\alpha)(X-\\bar{\\alpha})$ (type I), where $\\alpha\\in \\mathbb{C}\\setminus \\mathbb{R}$, or $X^{2}$ (type II). Firstly, we obtain a complete description of Ricci-parallel metrics of type I. In particular, such a Ricci-parallel metric is uniquely determined by an Einstein metric and an invariant symmetric parallel complex structure up to isometry and scaling. Then we study Ricci-parallel metric Lie algebras of type II by using double extension process. Surprisingly, we find that every double extension of a metric Abelian Lie algebra is Ricci-parallel and the converse holds for Lorentz Ricci-parallel metric nilpotent Lie algebras of type II. Moreover, we construct infinitely many new explicit examples of Ricci-parallel metric Lie algebras which are not Einstein.","sentences":["In this paper, we mainly study left invariant pseudo-Riemannian Ricci-parallel metrics on connected Lie groups which are not Einstein.","Following a result of Boubel and B\\'{e}rard Bergery, there are two typical types of such metrics, which are characterized by the minimal polynomial of the Ricci operator.","Namely, its form is either $(X-\\alpha)(X-\\bar{\\alpha})$ (type I), where $\\alpha\\in \\mathbb{C}\\setminus \\mathbb{R}$, or $X^{2}$ (type II).","Firstly, we obtain a complete description of Ricci-parallel metrics of type I.","In particular, such a Ricci-parallel metric is uniquely determined by an Einstein metric and an invariant symmetric parallel complex structure up to isometry and scaling.","Then we study Ricci-parallel metric Lie algebras of type II by using double extension process.","Surprisingly, we find that every double extension of a metric Abelian Lie algebra is Ricci-parallel and the converse holds for Lorentz Ricci-parallel metric nilpotent Lie algebras of type II.","Moreover, we construct infinitely many new explicit examples of Ricci-parallel metric Lie algebras which are not Einstein."],"url":"http://arxiv.org/abs/2404.13797v1","category":"math.DG"}
{"created":"2024-04-21 23:14:02","title":"Lightweight Connective Detection Using Gradient Boosting","abstract":"In this work, we introduce a lightweight discourse connective detection system. Employing gradient boosting trained on straightforward, low-complexity features, this proposed approach sidesteps the computational demands of the current approaches that rely on deep neural networks. Considering its simplicity, our approach achieves competitive results while offering significant gains in terms of time even on CPU. Furthermore, the stable performance across two unrelated languages suggests the robustness of our system in the multilingual scenario. The model is designed to support the annotation of discourse relations, particularly in scenarios with limited resources, while minimizing performance loss.","sentences":["In this work, we introduce a lightweight discourse connective detection system.","Employing gradient boosting trained on straightforward, low-complexity features, this proposed approach sidesteps the computational demands of the current approaches that rely on deep neural networks.","Considering its simplicity, our approach achieves competitive results while offering significant gains in terms of time even on CPU.","Furthermore, the stable performance across two unrelated languages suggests the robustness of our system in the multilingual scenario.","The model is designed to support the annotation of discourse relations, particularly in scenarios with limited resources, while minimizing performance loss."],"url":"http://arxiv.org/abs/2404.13793v1","category":"cs.CL"}
{"created":"2024-04-21 22:56:38","title":"On illposedness of the Hall and electron magnetohydrodynamic equations without resistivity on the whole space","abstract":"It has been shown in our previous work that the incompressible and irresistive Hall- and electron-magnetohydrodynamic (MHD) equations are illposed on flat domains $M = \\mathbb{R}^k \\times \\mathbb{T}^{3-k}$ for $0 \\le k \\le 2$. The data and solutions therein were assumed to be independent of one coordinate, which not only significantly simplifies the systems but also allows for a large class of steady states. In this work, we remove the assumption of independence and conclude strong illposedness for compactly supported data in $\\mathbb{R}^3$. This is achieved by constructing degenerating wave packets for linearized systems around time-dependent axisymmetric magnetic fields. A few main additional ingredients are: a more systematic application of the generalized energy estimate, use of the Bogovski\\v{i} operator, and a priori estimates for axisymmetric solutions to the Hall- and electron-MHD systems.","sentences":["It has been shown in our previous work that the incompressible and irresistive Hall- and electron-magnetohydrodynamic (MHD) equations are illposed on flat domains $M = \\mathbb{R}^k \\times \\mathbb{T}^{3-k}$ for $0 \\le k \\le 2$.","The data and solutions therein were assumed to be independent of one coordinate, which not only significantly simplifies the systems but also allows for a large class of steady states.","In this work, we remove the assumption of independence and conclude strong illposedness for compactly supported data in $\\mathbb{R}^3$. This is achieved by constructing degenerating wave packets for linearized systems around time-dependent axisymmetric magnetic fields.","A few main additional ingredients are: a more systematic application of the generalized energy estimate, use of the Bogovski\\v{i} operator, and a priori estimates for axisymmetric solutions to the Hall- and electron-MHD systems."],"url":"http://arxiv.org/abs/2404.13790v1","category":"math.AP"}
{"created":"2024-04-21 21:36:42","title":"How to Inverting the Leverage Score Distribution?","abstract":"Leverage score is a fundamental problem in machine learning and theoretical computer science. It has extensive applications in regression analysis, randomized algorithms, and neural network inversion. Despite leverage scores are widely used as a tool, in this paper, we study a novel problem, namely the inverting leverage score problem. We analyze to invert the leverage score distributions back to recover model parameters. Specifically, given a leverage score $\\sigma \\in \\mathbb{R}^n$, the matrix $A \\in \\mathbb{R}^{n \\times d}$, and the vector $b \\in \\mathbb{R}^n$, we analyze the non-convex optimization problem of finding $x \\in \\mathbb{R}^d$ to minimize $\\| \\mathrm{diag}( \\sigma ) - I_n \\circ (A(x) (A(x)^\\top A(x) )^{-1} A(x)^\\top ) \\|_F$, where $A(x):= S(x)^{-1} A \\in \\mathbb{R}^{n \\times d} $, $S(x) := \\mathrm{diag}(s(x)) \\in \\mathbb{R}^{n \\times n}$ and $s(x) : = Ax - b \\in \\mathbb{R}^n$. Our theoretical studies include computing the gradient and Hessian, demonstrating that the Hessian matrix is positive definite and Lipschitz, and constructing first-order and second-order algorithms to solve this regression problem. Our work combines iterative shrinking and the induction hypothesis to ensure global convergence rates for the Newton method, as well as the properties of Lipschitz and strong convexity to guarantee the performance of gradient descent. This important study on inverting statistical leverage opens up numerous new applications in interpretation, data recovery, and security.","sentences":["Leverage score is a fundamental problem in machine learning and theoretical computer science.","It has extensive applications in regression analysis, randomized algorithms, and neural network inversion.","Despite leverage scores are widely used as a tool, in this paper, we study a novel problem, namely the inverting leverage score problem.","We analyze to invert the leverage score distributions back to recover model parameters.","Specifically, given a leverage score $\\sigma \\in \\mathbb{R}^n$, the matrix $A \\in \\mathbb{R}^{n \\times d}$, and the vector $b \\in \\mathbb{R}^n$, we analyze the non-convex optimization problem of finding $x \\in \\mathbb{R}^d$ to minimize $\\| \\mathrm{diag}( \\sigma ) - I_n \\circ (A(x) (A(x)^\\top A(x) )^{-1} A(x)^\\top )","\\|_F$, where $A(x):= S(x)^{-1} A \\in \\mathbb{R}^{n \\times d} $, $S(x) := \\mathrm{diag}(s(x))","\\in \\mathbb{R}^{n \\times n}$ and $s(x) :","= Ax - b \\in \\mathbb{R}^n$.","Our theoretical studies include computing the gradient and Hessian, demonstrating that the Hessian matrix is positive definite and Lipschitz, and constructing first-order and second-order algorithms to solve this regression problem.","Our work combines iterative shrinking and the induction hypothesis to ensure global convergence rates for the Newton method, as well as the properties of Lipschitz and strong convexity to guarantee the performance of gradient descent.","This important study on inverting statistical leverage opens up numerous new applications in interpretation, data recovery, and security."],"url":"http://arxiv.org/abs/2404.13785v1","category":"cs.LG"}
{"created":"2024-04-21 21:28:13","title":"Spin Theory Based on the Extended Least Action Principle and Information Metrics: Quantization, Entanglement, and Bell Test With Time Delay","abstract":"A theory of electron spin is developed here based on the extended least action principle and assumptions of intrinsic angular momentum of an electron with random orientations. By incorporating appropriate relative entropy for the random orientations of intrinsic angular momentum in the extended least action principle, the theory recovers the quantum formulation of electron spin. The two-level quantization of spin measurement is a natural mathematical consequence instead of a postulate. The formulation of measurement probability when a second Stern-Gerlach apparatus is rotated relative to the first Stern-Gerlach apparatus, and the Schr\\\"{o}dinger-Pauli equation, are also derived successfully. Furthermore, we provide an intuitive physical model and formulation to explain the entanglement phenomenon between two electron spins. In this model, spin entanglement is the consequence of correlation between the random orientations of the intrinsic angular momenta of the two electrons. Since the orientation is an intrinsic local property of electron, the correlation of orientations can be preserved even when the two electrons are remotely separated. Such a correlation can be manifested without causal effect. Owing to this orientation correlation, the Bell-CHSH inequality is shown to be violated in a Bell test. The standard quantum theory of electron spin can be considered as an ideal approximation of the present theory when certain conditions are taken to the limits. A potential experiment is proposed to test the difference between the present theory and the standard quantum theory. In a typical Bell test that confirms the violation of Bell-CHSH inequality, the theory suggests that by adding a sufficiently large time delay before Bob's measurement, the Bell-CHSH inequality can become non-violated.","sentences":["A theory of electron spin is developed here based on the extended least action principle and assumptions of intrinsic angular momentum of an electron with random orientations.","By incorporating appropriate relative entropy for the random orientations of intrinsic angular momentum in the extended least action principle, the theory recovers the quantum formulation of electron spin.","The two-level quantization of spin measurement is a natural mathematical consequence instead of a postulate.","The formulation of measurement probability when a second Stern-Gerlach apparatus is rotated relative to the first Stern-Gerlach apparatus, and the Schr\\\"{o}dinger-Pauli equation, are also derived successfully.","Furthermore, we provide an intuitive physical model and formulation to explain the entanglement phenomenon between two electron spins.","In this model, spin entanglement is the consequence of correlation between the random orientations of the intrinsic angular momenta of the two electrons.","Since the orientation is an intrinsic local property of electron, the correlation of orientations can be preserved even when the two electrons are remotely separated.","Such a correlation can be manifested without causal effect.","Owing to this orientation correlation, the Bell-CHSH inequality is shown to be violated in a Bell test.","The standard quantum theory of electron spin can be considered as an ideal approximation of the present theory when certain conditions are taken to the limits.","A potential experiment is proposed to test the difference between the present theory and the standard quantum theory.","In a typical Bell test that confirms the violation of Bell-CHSH inequality, the theory suggests that by adding a sufficiently large time delay before Bob's measurement, the Bell-CHSH inequality can become non-violated."],"url":"http://arxiv.org/abs/2404.13783v1","category":"physics.gen-ph"}
{"created":"2024-04-21 21:07:40","title":"Constant energy families of harmonic maps","abstract":"For a negatively curved manifold $M$ and a continuous map $\\psi:\\Sigma\\to M$ from a closed surface $\\Sigma$, we study complex submanifolds of Teichm\\\"uller space $\\mathcal{S}\\subset\\mathcal{T}(\\Sigma)$ such that the harmonic maps $\\{h_X:X\\to M\\text{ for }X\\in\\mathcal{S}\\}$ in the homotopy class of $\\psi$ all have equal energy. When $M$ is real analytic with negative Hermitian sectional curvature, we show that for any such $\\mathcal{S}$, there exists a closed Riemann surface $Y$, such that any $h_X$ for $X\\in\\mathcal{S}$ factors as a holomorphic map $\\phi_X:X\\to Y$ followed by a fixed harmonic map $h:Y\\to M$. This answers a question posed by both Toledo and Gromov. As a first application, we show a factorization result for harmonic maps from normal projective varieties to $M$. As a second application, we study homomorphisms from finite index subgroups of mapping class groups to $\\pi_1(M)$.","sentences":["For a negatively curved manifold $M$ and a continuous map $\\psi:\\Sigma\\to M$ from a closed surface $\\Sigma$, we study complex submanifolds of Teichm\\\"uller space $\\mathcal{S}\\subset\\mathcal{T}(\\Sigma)$ such that the harmonic maps $\\{h_X:X\\to","M\\text{ for }X\\in\\mathcal{S}\\}$ in the homotopy class of $\\psi$ all have equal energy.","When $M$ is real analytic with negative Hermitian sectional curvature, we show that for any such $\\mathcal{S}$, there exists a closed Riemann surface $Y$, such that any $h_X$ for $X\\in\\mathcal{S}$ factors as a holomorphic map $\\phi_X:X\\to Y$ followed by a fixed harmonic map $h:Y\\to","M$.","This answers a question posed by both Toledo and Gromov.","As a first application, we show a factorization result for harmonic maps from normal projective varieties to $M$. As a second application, we study homomorphisms from finite index subgroups of mapping class groups to $\\pi_1(M)$."],"url":"http://arxiv.org/abs/2404.13774v1","category":"math.DG"}
{"created":"2024-04-21 20:49:13","title":"CMC foliations on Euclidean spaces are minimal foliations","abstract":"In this article, we give complete answers to some classical problems and conjectures on differential geometry (of foliations). For instance, we give a complete positive answer to the classical conjecture that states that every foliation on $\\mathbb{R}^{n+1}$ by (possibly varying) CMC hypersurfaces is a foliation by minimal hypersurfaces. Moreover, if $n\\leq 4$ such a CMC foliation must consist of parallel hyperplanes. We prove also that such conjecture holds true in much more general situations, for instance, when the ambient space is a complete Riemannian manifold with non-negative Ricci curvature. We prove also that for a foliation by CMC hypersurfaces on a complete Riemannian manifold $M$ with sectional curvature bounded from below by $-K_0\\leq 0$, then the mean curvature $H$ of the leaves of the foliation satisfies $|H|\\leq \\sqrt{K_0}$. This gives a complete positive answer to a conjecture due to Meeks III, P\\'erez and Ros. We give some answers to several other problems.","sentences":["In this article, we give complete answers to some classical problems and conjectures on differential geometry (of foliations).","For instance, we give a complete positive answer to the classical conjecture that states that every foliation on $\\mathbb{R}^{n+1}$ by (possibly varying) CMC hypersurfaces is a foliation by minimal hypersurfaces.","Moreover, if $n\\leq 4$ such a CMC foliation must consist of parallel hyperplanes.","We prove also that such conjecture holds true in much more general situations, for instance, when the ambient space is a complete Riemannian manifold with non-negative Ricci curvature.","We prove also that for a foliation by CMC hypersurfaces on a complete Riemannian manifold $M$ with sectional curvature bounded from below by $-K_0\\leq 0$, then the mean curvature $H$ of the leaves of the foliation satisfies $|H|\\leq \\sqrt{K_0}$.","This gives a complete positive answer to a conjecture due to Meeks III, P\\'erez and Ros.","We give some answers to several other problems."],"url":"http://arxiv.org/abs/2404.13772v1","category":"math.DG"}
{"created":"2024-04-21 19:51:40","title":"On commutative set-theoretic solutions of the Pentagon Equation","abstract":"We extend the so-called retract relation given in [6] for involutive set-theoretic solutions of the Pentagon Equation and we introduce the notion of associated permutation group to study the family of the commutative non-degenerate ones. Moreover, we develop a machinery to construct all these solutions and we use it to give a quite explicit classification of the irretractable ones. Finally, non-degenerate solutions on left-zero semigroup are studied in detail, with an emphasis on the ones with cyclic associated permutation group and on the ones having small size.","sentences":["We extend the so-called retract relation given in [6] for involutive set-theoretic solutions of the Pentagon Equation and we introduce the notion of associated permutation group to study the family of the commutative non-degenerate ones.","Moreover, we develop a machinery to construct all these solutions and we use it to give a quite explicit classification of the irretractable ones.","Finally, non-degenerate solutions on left-zero semigroup are studied in detail, with an emphasis on the ones with cyclic associated permutation group and on the ones having small size."],"url":"http://arxiv.org/abs/2404.13758v1","category":"math.QA"}
{"created":"2024-04-21 19:12:21","title":"Application of Kalman Filter in Stochastic Differential Equations","abstract":"In areas such as finance, engineering, and science, we often face situations that change quickly and unpredictably. These situations are tough to handle and require special tools and methods capable of understanding and predicting what might happen next. Stochastic Differential Equations (SDEs) are renowned for modeling and analyzing real-world dynamical systems. However, obtaining the parameters, boundary conditions, and closed-form solutions of SDEs can often be challenging. In this paper, we will discuss the application of Kalman filtering theory to SDEs, including Extended Kalman filtering and Particle Extended Kalman filtering. We will explore how to fit existing SDE systems through filtering and track the original SDEs by fitting the obtained closed-form solutions. This approach aims to gather more information about these SDEs, which could be used in various ways, such as incorporating them into parameters of data-based SDE models.","sentences":["In areas such as finance, engineering, and science, we often face situations that change quickly and unpredictably.","These situations are tough to handle and require special tools and methods capable of understanding and predicting what might happen next.","Stochastic Differential Equations (SDEs) are renowned for modeling and analyzing real-world dynamical systems.","However, obtaining the parameters, boundary conditions, and closed-form solutions of SDEs can often be challenging.","In this paper, we will discuss the application of Kalman filtering theory to SDEs, including Extended Kalman filtering and Particle Extended Kalman filtering.","We will explore how to fit existing SDE systems through filtering and track the original SDEs by fitting the obtained closed-form solutions.","This approach aims to gather more information about these SDEs, which could be used in various ways, such as incorporating them into parameters of data-based SDE models."],"url":"http://arxiv.org/abs/2404.13748v1","category":"eess.SY"}
{"created":"2024-04-21 18:32:15","title":"Curvature and harmonic analysis on compact manifolds","abstract":"We discuss problems that relate curvature and concentration properties of eigenfunctions and quasimodes on compact boundaryless Riemannian manifolds. These include new sharp $L^q$-estimates, $q\\in (2,q_c]$, $q_c=2(n+1)/(n-1)$, of log-quasimodes that characterize compact connected space forms in terms of the growth rate of $L^q$-norms of such quasimode for these relatively small Lebesgue exponents $q$.   No such characterization is possible for any exponent $q> q_c$.","sentences":["We discuss problems that relate curvature and concentration properties of eigenfunctions and quasimodes on compact boundaryless Riemannian manifolds.","These include new sharp $L^q$-estimates, $q\\in (2,q_c]$, $q_c=2(n+1)/(n-1)$, of log-quasimodes that characterize compact connected space forms in terms of the growth rate of $L^q$-norms of such quasimode for these relatively small Lebesgue exponents $q$.   No such characterization is possible for any exponent $q> q_c$."],"url":"http://arxiv.org/abs/2404.13739v1","category":"math.AP"}
{"created":"2024-04-21 18:27:35","title":"Quasimode concentration on compact space forms","abstract":"We show that the upper bounds for the $L^2$-norms of $L^1$-normalized quasimodes that we obtained in [9] are always sharp on any compact space form. This allows us to characterize compact manifolds of constant sectional curvature using the decay rates of lower bounds of $L^1$-norms of $L^2$-normalized log-quasimodes fully resolving a problem initiated by the second author and Zelditch [15]. We are also able to characterize such manifolds by the concentration of quasimodes near periodic geodesics as measured by $L^2$-norms over thin geodesic tubes.","sentences":["We show that the upper bounds for the $L^2$-norms of $L^1$-normalized quasimodes that we obtained in [9] are always sharp on any compact space form.","This allows us to characterize compact manifolds of constant sectional curvature using the decay rates of lower bounds of $L^1$-norms of $L^2$-normalized log-quasimodes fully resolving a problem initiated by the second author and Zelditch","[15].","We are also able to characterize such manifolds by the concentration of quasimodes near periodic geodesics as measured by $L^2$-norms over thin geodesic tubes."],"url":"http://arxiv.org/abs/2404.13738v1","category":"math.AP"}
{"created":"2024-04-21 18:22:08","title":"Identification and Estimation of Nonseparable Triangular Equations with Mismeasured Instruments","abstract":"In this paper, I study the nonparametric identification and estimation of the marginal effect of an endogenous variable $X$ on the outcome variable $Y$, given a potentially mismeasured instrument variable $W^*$, without assuming linearity or separability of the functions governing the relationship between observables and unobservables. To address the challenges arising from the co-existence of measurement error and nonseparability, I first employ the deconvolution technique from the measurement error literature to identify the joint distribution of $Y, X, W^*$ using two error-laden measurements of $W^*$. I then recover the structural derivative of the function of interest and the \"Local Average Response\" (LAR) from the joint distribution via the \"unobserved instrument\" approach in Matzkin (2016). I also propose nonparametric estimators for these parameters and derive their uniform rates of convergence. Monte Carlo exercises show evidence that the estimators I propose have good finite sample performance.","sentences":["In this paper, I study the nonparametric identification and estimation of the marginal effect of an endogenous variable $X$ on the outcome variable $Y$, given a potentially mismeasured instrument variable $W^*$, without assuming linearity or separability of the functions governing the relationship between observables and unobservables.","To address the challenges arising from the co-existence of measurement error and nonseparability, I first employ the deconvolution technique from the measurement error literature to identify the joint distribution of $Y, X, W^*$ using two error-laden measurements of $W^*$. I then recover the structural derivative of the function of interest and the \"Local Average Response\" (LAR) from the joint distribution via the \"unobserved instrument\" approach in Matzkin (2016).","I also propose nonparametric estimators for these parameters and derive their uniform rates of convergence.","Monte Carlo exercises show evidence that the estimators I propose have good finite sample performance."],"url":"http://arxiv.org/abs/2404.13735v1","category":"econ.EM"}
{"created":"2024-04-21 18:21:05","title":"Curvature and sharp growth rates of log-quasimodes on compact manifolds","abstract":"We obtain new optimal estimates for the $L^2(M)\\to L^q(M)$, $q\\in (2,q_c]$, $q_c=2(n+1)/(n-1)$, operator norms of spectral projection operators associated with spectral windows $[\\lambda,\\lambda+\\delta(\\lambda)]$, with $\\delta(\\lambda)=O((\\log\\lambda)^{-1})$ on compact Riemannian manifolds $(M,g)$ of dimension $n\\ge2$ all of whose sectional curvatures are nonpositive or negative. We show that these two different types of estimates are saturated on flat manifolds or manifolds all of whose sectional curvatures are negative. This allows us to classify compact space forms in terms of the size of $L^q$-norms of quasimodes for each Lebesgue exponent $q\\in (2,q_c]$, even though it is impossible to distinguish between ones of negative or zero curvature sectional curvature for any $q>q_c$.","sentences":["We obtain new optimal estimates for the $L^2(M)\\to L^q(M)$, $q\\in (2,q_c]$, $q_c=2(n+1)/(n-1)$, operator norms of spectral projection operators associated with spectral windows $[\\lambda,\\lambda+\\delta(\\lambda)]$, with $\\delta(\\lambda)=O((\\log\\lambda)^{-1})$ on compact Riemannian manifolds $(M,g)$ of dimension $n\\ge2$ all of whose sectional curvatures are nonpositive or negative.","We show that these two different types of estimates are saturated on flat manifolds or manifolds all of whose sectional curvatures are negative.","This allows us to classify compact space forms in terms of the size of $L^q$-norms of quasimodes for each Lebesgue exponent $q\\in (2,q_c]$, even though it is impossible to distinguish between ones of negative or zero curvature sectional curvature for any $q>q_c$."],"url":"http://arxiv.org/abs/2404.13734v1","category":"math.AP"}
{"created":"2024-04-21 17:55:22","title":"Polyharmonic helices","abstract":"The main aim of this paper is to investigate the existence of Frenet helices which are polyharmonic of order $r$, shortly, $r$-harmonic. We shall obtain existence, non-existence and classification results. More specifically, we obtain a complete classification of proper $r$-harmonic helices into the $3$-dimensional solvable Lie group Sol$_3$. Next, we investigate the existence of proper $r$-harmonic helices into Bianchi-Cartan-Vranceanu spaces and, in this context, we find new examples. Finally, we shall establish some non-existence results both for Frenet curves and Frenet helices of order $n \\geq 4$ when the ambient space is the Euclidean sphere $\\s^m$.","sentences":["The main aim of this paper is to investigate the existence of Frenet helices which are polyharmonic of order $r$, shortly, $r$-harmonic.","We shall obtain existence, non-existence and classification results.","More specifically, we obtain a complete classification of proper $r$-harmonic helices into the $3$-dimensional solvable Lie group Sol$_3$.","Next, we investigate the existence of proper $r$-harmonic helices into Bianchi-Cartan-Vranceanu spaces and, in this context, we find new examples.","Finally, we shall establish some non-existence results both for Frenet curves and Frenet helices of order $n \\geq 4$ when the ambient space is the Euclidean sphere $\\s^m$."],"url":"http://arxiv.org/abs/2404.13726v1","category":"math.DG"}
{"created":"2024-04-21 17:16:59","title":"Transverse structure of the proton beyond leading twist: A light-front Hamiltonian approach","abstract":"Within the Basis Light-Front Quantization framework, we systematically investigate the subleading twist (twist-3) transverse-momentum-dependent parton distribution functions (TMDs) of the proton beyond the Wandzura-Wilczek (WW) approximation. The subleading twist TMDs are not independent and can be decomposed into twist-2 and genuine twist-3 terms from the equations of motion. The latter involves quark-quark-gluon correlations and contains interferences between two light-front Fock sectors, $|qqq\\rangle$ and $|qqqg\\rangle$, which are usually neglected in the WW approximation.","sentences":["Within the Basis Light-Front Quantization framework, we systematically investigate the subleading twist (twist-3) transverse-momentum-dependent parton distribution functions (TMDs) of the proton beyond the Wandzura-Wilczek (WW) approximation.","The subleading twist TMDs are not independent and can be decomposed into twist-2 and genuine twist-3 terms from the equations of motion.","The latter involves quark-quark-gluon correlations and contains interferences between two light-front Fock sectors, $|qqq\\rangle$ and $|qqqg\\rangle$, which are usually neglected in the WW approximation."],"url":"http://arxiv.org/abs/2404.13720v1","category":"hep-ph"}
{"created":"2024-04-21 16:51:47","title":"The Lambda-CDM-NG cosmological model: A possible resolution of the Hubble tension","abstract":"This paper is based on two insights: (1) that general relativity alone does not specify how much of the matter density contributes to the source term in Friedmann's equation, and how much contributes as the source of the gravitational potential of condensed objects; and (2) the source of the gravitational potential in the currently accepted cosmological model, for the small scales of condensed objects, is not what one would expect from Newtonian theory. Here insight (1) is used to restore the source of the gravitational potential to its Newtonian value, giving a new cosmological model. This model uses only conventional general relativity (no speculative physics) and the usual cosmological sources of gravitation, and there is no Hubble tension in this model if the total gravitational potential of condensed objects falls in a certain range. The deceleration parameter in the new model differs from that in the currently favored onel, suggesting a test to distinguish between the two models.","sentences":["This paper is based on two insights: (1) that general relativity alone does not specify how much of the matter density contributes to the source term in Friedmann's equation, and how much contributes as the source of the gravitational potential of condensed objects; and (2) the source of the gravitational potential in the currently accepted cosmological model, for the small scales of condensed objects, is not what one would expect from Newtonian theory.","Here insight (1) is used to restore the source of the gravitational potential to its Newtonian value, giving a new cosmological model.","This model uses only conventional general relativity (no speculative physics) and the usual cosmological sources of gravitation, and there is no Hubble tension in this model if the total gravitational potential of condensed objects falls in a certain range.","The deceleration parameter in the new model differs from that in the currently favored onel, suggesting a test to distinguish between the two models."],"url":"http://arxiv.org/abs/2404.13712v1","category":"gr-qc"}
{"created":"2024-04-21 16:18:14","title":"Classical solutions of a mean field system for pulse-coupled oscillators: long time asymptotics versus blowup","abstract":"We introduce a novel reformulation of the mean-field system for pulse-coupled oscillators. It is based on writing a closed equation for the inverse distribution function associated to the probability density of oscillators with a given phase in a suitable time scale. This new framework allows to show a hidden contraction/expansion of certain distances leading to a full clarification of the long-time behavior, existence of steady states, rates of convergence, and finite time blow-up of classical solutions for a large class of monotone phase response functions. In the process, we get insights about the origin of obstructions to global-in-time existence and uniform in time estimates on the firing rate of the oscillators.","sentences":["We introduce a novel reformulation of the mean-field system for pulse-coupled oscillators.","It is based on writing a closed equation for the inverse distribution function associated to the probability density of oscillators with a given phase in a suitable time scale.","This new framework allows to show a hidden contraction/expansion of certain distances leading to a full clarification of the long-time behavior, existence of steady states, rates of convergence, and finite time blow-up of classical solutions for a large class of monotone phase response functions.","In the process, we get insights about the origin of obstructions to global-in-time existence and uniform in time estimates on the firing rate of the oscillators."],"url":"http://arxiv.org/abs/2404.13703v1","category":"math.AP"}
{"created":"2024-04-21 14:21:47","title":"A Minkowski type inequality in warped cylinders","abstract":"We prove a Minkowski type inequality for weakly mean convex and star-shaped hypersurfaces in warped cylinders which are asymptotically flat or hyperbolic. In particular, we show that this sharp inequality holds for outward minimizing hypersurfaces in the Schwarzschild manifold or the hyperbolic space using the weak solution of the inverse mean curvature flow.","sentences":["We prove a Minkowski type inequality for weakly mean convex and star-shaped hypersurfaces in warped cylinders which are asymptotically flat or hyperbolic.","In particular, we show that this sharp inequality holds for outward minimizing hypersurfaces in the Schwarzschild manifold or the hyperbolic space using the weak solution of the inverse mean curvature flow."],"url":"http://arxiv.org/abs/2404.13670v1","category":"math.DG"}
{"created":"2024-04-21 14:17:56","title":"Contraction properties and differentiability of $p$-energy forms with applications to nonlinear potential theory on self-similar sets","abstract":"We introduce new contraction properties called the generalized $p$-contraction property for $p$-energy forms as generalizations of many well-known inequalities, such as Clarkson's inequalities, the strong subadditivity and the ``Markov property'' in the theory of nonlinear Dirichlet forms, and show that any $p$-energy form satisfying Clarkson's inequalities is Fr\\'{e}chet differentiable. We also verify the generalized $p$-contraction property for $p$-energy forms constructed by Kigami [Mem. Eur. Math. Soc. 5 (2023)] and by Cao--Gu--Qiu [Adv. Math. 405 (2022), no. 108517]. As a general framework of $p$-energy forms taking into consideration the generalized $p$-contraction property, we introduce the notion of $p$-resistance form and investigate fundamental properties for $p$-harmonic functions with respect to $p$-resistance forms. In particular, some new estimates on scaling factors of $p$-energy forms are obtained by establishing H\\\"{o}lder regularity estimates for harmonic functions, and the $p$-walk dimensions of the generalized Sierpi\\'{n}ski carpets and $D$-dimensional level-$l$ Sierpi\\'{n}ski gasket are shown to be strictly greater than $p$.","sentences":["We introduce new contraction properties called the generalized $p$-contraction property for $p$-energy forms as generalizations of many well-known inequalities, such as Clarkson's inequalities, the strong subadditivity and the ``Markov property'' in the theory of nonlinear Dirichlet forms, and show that any $p$-energy form satisfying Clarkson's inequalities is Fr\\'{e}chet differentiable.","We also verify the generalized $p$-contraction property for $p$-energy forms constructed by Kigami","[Mem.","Eur.","Math.","Soc. 5 (2023)] and by Cao--Gu--Qiu","[Adv.","Math. 405","(2022), no. 108517].","As a general framework of $p$-energy forms taking into consideration the generalized $p$-contraction property, we introduce the notion of $p$-resistance form and investigate fundamental properties for $p$-harmonic functions with respect to $p$-resistance forms.","In particular, some new estimates on scaling factors of $p$-energy forms are obtained by establishing H\\\"{o}lder regularity estimates for harmonic functions, and the $p$-walk dimensions of the generalized Sierpi\\'{n}ski carpets and $D$-dimensional level-$l$ Sierpi\\'{n}ski gasket are shown to be strictly greater than $p$."],"url":"http://arxiv.org/abs/2404.13668v1","category":"math.AP"}
{"created":"2024-04-21 13:31:50","title":"Equational theory of ordinals with addition and left multiplication by $\u03c9$","abstract":"We show that the equational theory of the structure $\\langle \\omega^{\\omega}: (x,y)\\mapsto x+y, x\\mapsto \\omega x \\rangle $ is finitely axiomatizable and give a simple axiom schema when the domain is the set of transfinite ordinals.","sentences":["We show that the equational theory of the structure $\\langle \\omega^{\\omega}: (x,y)\\mapsto x+y, x\\mapsto \\omega x \\rangle $ is finitely axiomatizable and give a simple axiom schema when the domain is the set of transfinite ordinals."],"url":"http://arxiv.org/abs/2404.13661v1","category":"math.LO"}
{"created":"2024-04-21 12:56:21","title":"Surfaces with concentric or parallel $K$-contours","abstract":"Surfaces with concentric $K$-contours and parallel $K$-contours in Euclidean $3$-space are defined. Crucial examples are presented and characterization of them are given.","sentences":["Surfaces with concentric $K$-contours and parallel $K$-contours in Euclidean $3$-space are defined.","Crucial examples are presented and characterization of them are given."],"url":"http://arxiv.org/abs/2404.13650v1","category":"math.DG"}
{"created":"2024-04-21 12:41:30","title":"Physics-informed Mesh-independent Deep Compositional Operator Network","abstract":"Solving parametric Partial Differential Equations (PDEs) for a broad range of parameters is a critical challenge in scientific computing. To this end, neural operators, which learn mappings from parameters to solutions, have been successfully used. However, the training of neural operators typically demands large training datasets, the acquisition of which can be prohibitively expensive. To address this challenge, physics-informed training can offer a cost-effective strategy. However, current physics-informed neural operators face limitations, either in handling irregular domain shapes or in generalization to various discretizations of PDE parameters with variable mesh sizes. In this research, we introduce a novel physics-informed model architecture which can generalize to parameter discretizations of variable size and irregular domain shapes. Particularly, inspired by deep operator neural networks, our model involves a discretization-independent learning of parameter embedding repeatedly, and this parameter embedding is integrated with the response embeddings through multiple compositional layers, for more expressivity. Numerical results demonstrate the accuracy and efficiency of the proposed method.","sentences":["Solving parametric Partial Differential Equations (PDEs) for a broad range of parameters is a critical challenge in scientific computing.","To this end, neural operators, which learn mappings from parameters to solutions, have been successfully used.","However, the training of neural operators typically demands large training datasets, the acquisition of which can be prohibitively expensive.","To address this challenge, physics-informed training can offer a cost-effective strategy.","However, current physics-informed neural operators face limitations, either in handling irregular domain shapes or in generalization to various discretizations of PDE parameters with variable mesh sizes.","In this research, we introduce a novel physics-informed model architecture which can generalize to parameter discretizations of variable size and irregular domain shapes.","Particularly, inspired by deep operator neural networks, our model involves a discretization-independent learning of parameter embedding repeatedly, and this parameter embedding is integrated with the response embeddings through multiple compositional layers, for more expressivity.","Numerical results demonstrate the accuracy and efficiency of the proposed method."],"url":"http://arxiv.org/abs/2404.13646v1","category":"math.NA"}
{"created":"2024-04-21 12:40:55","title":"Error Estimation in the Mean-Field Limit of Kinetic Flocking Models with Local Alignments","abstract":"In this paper, we present an innovative particle system characterized by moderate interactions, designed to accurately approximate kinetic flocking models that incorporate singular interaction forces and local alignment mechanisms. We establish the existence of weak solutions to the corresponding flocking equations and provide an error estimate for the mean-field limit. This is achieved through the regularization of singular forces and a nonlocal approximation strategy for local alignments. We show that, by selecting the regularization and localization parameters logarithmically with respect to the number of particles, the particle system effectively approximates the mean-field equation.","sentences":["In this paper, we present an innovative particle system characterized by moderate interactions, designed to accurately approximate kinetic flocking models that incorporate singular interaction forces and local alignment mechanisms.","We establish the existence of weak solutions to the corresponding flocking equations and provide an error estimate for the mean-field limit.","This is achieved through the regularization of singular forces and a nonlocal approximation strategy for local alignments.","We show that, by selecting the regularization and localization parameters logarithmically with respect to the number of particles, the particle system effectively approximates the mean-field equation."],"url":"http://arxiv.org/abs/2404.13644v1","category":"math.AP"}
{"created":"2024-04-21 12:22:10","title":"Emergence of chirality by multipole interconversion","abstract":"A clear understanding of chirality in spin-active electronic states is discussed in order to address confusions about chiral effects recently discovered in materials science. Electronic toroidal monopole $G_0$ can serve as a measure of chirality in this categorization, which can be clearly related to the chiral density operator in the Dirac equation. We extend the concepts of chirality not only to those of materials but also to those of physical fields, and to material-field composites. Additionally, we illustrate specific examples from physics and chemistry that demonstrate the process of acquiring chirality through the combination of seemingly achiral degrees of freedom, which we term the emergence of chirality. Interference among multiple chiralities exhibiting phenomena specific to handedness is also discussed.","sentences":["A clear understanding of chirality in spin-active electronic states is discussed in order to address confusions about chiral effects recently discovered in materials science.","Electronic toroidal monopole $G_0$ can serve as a measure of chirality in this categorization, which can be clearly related to the chiral density operator in the Dirac equation.","We extend the concepts of chirality not only to those of materials but also to those of physical fields, and to material-field composites.","Additionally, we illustrate specific examples from physics and chemistry that demonstrate the process of acquiring chirality through the combination of seemingly achiral degrees of freedom, which we term the emergence of chirality.","Interference among multiple chiralities exhibiting phenomena specific to handedness is also discussed."],"url":"http://arxiv.org/abs/2404.13636v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 12:21:49","title":"Radial and orbital decomposition of charge radii of Ca nuclei:Comparative study of Skyrme and Fayans functionals","abstract":"We investigate the charge and point-proton radii of the Ca nuclei in detail in the density functional theory framework. As the Fayans energy density functional provides characteristic $N$-dependence, successfully describing the parabolic behavior of the differential charge radii in $20\\leq N\\leq 28$, we pose our particular focus on its physics origin, by decomposing them into the radial and orbital contributions. The results are compared with those from the Skyrme plus usual pairing functional, which is taken as a representative of the functionals having normal pairing channels. We point out that, because the enhancement of the differential charge radii in $N<20$ with the Fayans functional, which is contradictory with the data, has the origin parallel to the parabolic behavior in $20\\leq N\\leq 28$, it is significant to describe both $N$ regions simultaneously.","sentences":["We investigate the charge and point-proton radii of the Ca nuclei in detail in the density functional theory framework.","As the Fayans energy density functional provides characteristic $N$-dependence, successfully describing the parabolic behavior of the differential charge radii in $20\\leq N\\leq 28$, we pose our particular focus on its physics origin, by decomposing them into the radial and orbital contributions.","The results are compared with those from the Skyrme plus usual pairing functional, which is taken as a representative of the functionals having normal pairing channels.","We point out that, because the enhancement of the differential charge radii in $N<20$ with the Fayans functional, which is contradictory with the data, has the origin parallel to the parabolic behavior in $20\\leq N\\leq 28$, it is significant to describe both $N$ regions simultaneously."],"url":"http://arxiv.org/abs/2404.13635v1","category":"nucl-th"}
{"created":"2024-04-21 12:11:03","title":"Fermi-Bose Machine","abstract":"Distinct from human cognitive processing, deep neural networks trained by backpropagation can be easily fooled by adversarial examples. To design a semantically meaningful representation learning, we discard backpropagation, and instead, propose a local contrastive learning, where the representation for the inputs bearing the same label shrink (akin to boson) in hidden layers, while those of different labels repel (akin to fermion). This layer-wise learning is local in nature, being biological plausible. A statistical mechanics analysis shows that the target fermion-pair-distance is a key parameter. Moreover, the application of this local contrastive learning to MNIST benchmark dataset demonstrates that the adversarial vulnerability of standard perceptron can be greatly mitigated by tuning the target distance, i.e., controlling the geometric separation of prototype manifolds.","sentences":["Distinct from human cognitive processing, deep neural networks trained by backpropagation can be easily fooled by adversarial examples.","To design a semantically meaningful representation learning, we discard backpropagation, and instead, propose a local contrastive learning, where the representation for the inputs bearing the same label shrink (akin to boson) in hidden layers, while those of different labels repel (akin to fermion).","This layer-wise learning is local in nature, being biological plausible.","A statistical mechanics analysis shows that the target fermion-pair-distance is a key parameter.","Moreover, the application of this local contrastive learning to MNIST benchmark dataset demonstrates that the adversarial vulnerability of standard perceptron can be greatly mitigated by tuning the target distance, i.e., controlling the geometric separation of prototype manifolds."],"url":"http://arxiv.org/abs/2404.13631v1","category":"cs.LG"}
{"created":"2024-04-21 11:21:27","title":"Attack on Scene Flow using Point Clouds","abstract":"Deep neural networks have made significant advancements in accurately estimating scene flow using point clouds, which is vital for many applications like video analysis, action recognition, and navigation. Robustness of these techniques, however, remains a concern, particularly in the face of adversarial attacks that have been proven to deceive state-of-the-art deep neural networks in many domains. Surprisingly, the robustness of scene flow networks against such attacks has not been thoroughly investigated. To address this problem, the proposed approach aims to bridge this gap by introducing adversarial white-box attacks specifically tailored for scene flow networks. Experimental results show that the generated adversarial examples obtain up to 33.7 relative degradation in average end-point error on the KITTI and FlyingThings3D datasets. The study also reveals the significant impact that attacks targeting point clouds in only one dimension or color channel have on average end-point error. Analyzing the success and failure of these attacks on the scene flow networks and their 2D optical flow network variants show a higher vulnerability for the optical flow networks.","sentences":["Deep neural networks have made significant advancements in accurately estimating scene flow using point clouds, which is vital for many applications like video analysis, action recognition, and navigation.","Robustness of these techniques, however, remains a concern, particularly in the face of adversarial attacks that have been proven to deceive state-of-the-art deep neural networks in many domains.","Surprisingly, the robustness of scene flow networks against such attacks has not been thoroughly investigated.","To address this problem, the proposed approach aims to bridge this gap by introducing adversarial white-box attacks specifically tailored for scene flow networks.","Experimental results show that the generated adversarial examples obtain up to 33.7 relative degradation in average end-point error on the KITTI and FlyingThings3D datasets.","The study also reveals the significant impact that attacks targeting point clouds in only one dimension or color channel have on average end-point error.","Analyzing the success and failure of these attacks on the scene flow networks and their 2D optical flow network variants show a higher vulnerability for the optical flow networks."],"url":"http://arxiv.org/abs/2404.13621v1","category":"cs.CV"}
{"created":"2024-04-21 11:13:57","title":"Numerical solution to the PML problem of the biharmonic wave scattering in periodic structures","abstract":"Consider the interaction of biharmonic waves with a periodic array of cavities, characterized by the Kirchhoff--Love model. This paper investigates the perfectly matched layer (PML) formulation and its numerical soution to the governing biharmonic wave equation. The study establishes the well-posedness of the associated variational problem employing the Fredholm alternative theorem. Based on the examination of an auxiliary problem in the PML layer, exponential convergence of the PML solution is attained. Moreover, it develops and compares three decomposition methods alongside their corresponding mixed finite element formulations, incorporating interior penalty techniques for solving the PML problem. Numerical experiments validate the effectiveness of the proposed methods in absorbing outgoing waves within the PML layers and suppressing oscillations in the bending moment of biharmonic waves near the cavity's surface.","sentences":["Consider the interaction of biharmonic waves with a periodic array of cavities, characterized by the Kirchhoff--Love model.","This paper investigates the perfectly matched layer (PML) formulation and its numerical soution to the governing biharmonic wave equation.","The study establishes the well-posedness of the associated variational problem employing the Fredholm alternative theorem.","Based on the examination of an auxiliary problem in the PML layer, exponential convergence of the PML solution is attained.","Moreover, it develops and compares three decomposition methods alongside their corresponding mixed finite element formulations, incorporating interior penalty techniques for solving the PML problem.","Numerical experiments validate the effectiveness of the proposed methods in absorbing outgoing waves within the PML layers and suppressing oscillations in the bending moment of biharmonic waves near the cavity's surface."],"url":"http://arxiv.org/abs/2404.13620v1","category":"math.NA"}
{"created":"2024-04-21 11:01:38","title":"Towards Unified Representation of Multi-Modal Pre-training for 3D Understanding via Differentiable Rendering","abstract":"State-of-the-art 3D models, which excel in recognition tasks, typically depend on large-scale datasets and well-defined category sets. Recent advances in multi-modal pre-training have demonstrated potential in learning 3D representations by aligning features from 3D shapes with their 2D RGB or depth counterparts. However, these existing frameworks often rely solely on either RGB or depth images, limiting their effectiveness in harnessing a comprehensive range of multi-modal data for 3D applications. To tackle this challenge, we present DR-Point, a tri-modal pre-training framework that learns a unified representation of RGB images, depth images, and 3D point clouds by pre-training with object triplets garnered from each modality. To address the scarcity of such triplets, DR-Point employs differentiable rendering to obtain various depth images. This approach not only augments the supply of depth images but also enhances the accuracy of reconstructed point clouds, thereby promoting the representative learning of the Transformer backbone. Subsequently, using a limited number of synthetically generated triplets, DR-Point effectively learns a 3D representation space that aligns seamlessly with the RGB-Depth image space. Our extensive experiments demonstrate that DR-Point outperforms existing self-supervised learning methods in a wide range of downstream tasks, including 3D object classification, part segmentation, point cloud completion, semantic segmentation, and detection. Additionally, our ablation studies validate the effectiveness of DR-Point in enhancing point cloud understanding.","sentences":["State-of-the-art 3D models, which excel in recognition tasks, typically depend on large-scale datasets and well-defined category sets.","Recent advances in multi-modal pre-training have demonstrated potential in learning 3D representations by aligning features from 3D shapes with their 2D RGB or depth counterparts.","However, these existing frameworks often rely solely on either RGB or depth images, limiting their effectiveness in harnessing a comprehensive range of multi-modal data for 3D applications.","To tackle this challenge, we present DR-Point, a tri-modal pre-training framework that learns a unified representation of RGB images, depth images, and 3D point clouds by pre-training with object triplets garnered from each modality.","To address the scarcity of such triplets, DR-Point employs differentiable rendering to obtain various depth images.","This approach not only augments the supply of depth images but also enhances the accuracy of reconstructed point clouds, thereby promoting the representative learning of the Transformer backbone.","Subsequently, using a limited number of synthetically generated triplets, DR-Point effectively learns a 3D representation space that aligns seamlessly with the RGB-Depth image space.","Our extensive experiments demonstrate that DR-Point outperforms existing self-supervised learning methods in a wide range of downstream tasks, including 3D object classification, part segmentation, point cloud completion, semantic segmentation, and detection.","Additionally, our ablation studies validate the effectiveness of DR-Point in enhancing point cloud understanding."],"url":"http://arxiv.org/abs/2404.13619v1","category":"cs.MM"}
{"created":"2024-04-21 10:45:48","title":"Stochastic Thermodynamics of Micromagnetics","abstract":"In this work, we study the stochastic thermodynamics of micro-magnetic systems. We first formulate the stochastic dynamics of micro-magnetic systems by incorporating noises into Landau-Lifshitz (LL) equation, which describes the irreversible and deterministic dynamics of magnetic moments. The resulting stochastic Landau-Lifshitz (sLL) equation obeys detailed balance, which guarantees that, with the external field fixed, the system converges to thermodynamic equilibrium with vanishing entropy production and with non-vanishing probability current. We then discuss various thermodynamic variables both at the trajectory level and at the ensemble level, and further establish both the first and the second laws of thermodynamics. Finally, we establish fluctuation theorems, and verify them using numerical simulations.","sentences":["In this work, we study the stochastic thermodynamics of micro-magnetic systems.","We first formulate the stochastic dynamics of micro-magnetic systems by incorporating noises into Landau-Lifshitz (LL) equation, which describes the irreversible and deterministic dynamics of magnetic moments.","The resulting stochastic Landau-Lifshitz (sLL) equation obeys detailed balance, which guarantees that, with the external field fixed, the system converges to thermodynamic equilibrium with vanishing entropy production and with non-vanishing probability current.","We then discuss various thermodynamic variables both at the trajectory level and at the ensemble level, and further establish both the first and the second laws of thermodynamics.","Finally, we establish fluctuation theorems, and verify them using numerical simulations."],"url":"http://arxiv.org/abs/2404.13612v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-21 09:33:17","title":"Fully Tunable Fano Resonances in Chiral Electronic Transport","abstract":"Fano resonance arises from interference between a direct path and a resonant path. Predictions and observations of Fano profiles of conductance have been reported yet only in nonchiral electron transport. We propose an electronic Mach-Zehnder-Fano interferometer(\\textcolor{blue}{MZFI}) that integrates a quantum dot to an electronic Mach-Zehnder interferometer and investigate various transport spectra of this type of device. Electron motion in the device is chiral and backscattering is absent, enabling the transmission, linear conductance and differential spectra such as differential conductance, differential shot noise and differential Fano factor to possess perfect Fano profiles that can be fully tuned by an external magnetic flux. For a symmetric interferometer of(two arms with the same length, even the current and shot noise demonstrate fully tunable resonances and are bestowed different hallmarks unique to Fano resonances. All the transport spectra display the same evolution pattern in an evolution cycle, which along with the profiles are robust against variation of the parameters defining the device.","sentences":["Fano resonance arises from interference between a direct path and a resonant path.","Predictions and observations of Fano profiles of conductance have been reported yet only in nonchiral electron transport.","We propose an electronic Mach-Zehnder-Fano interferometer(\\textcolor{blue}{MZFI}) that integrates a quantum dot to an electronic Mach-Zehnder interferometer and investigate various transport spectra of this type of device.","Electron motion in the device is chiral and backscattering is absent, enabling the transmission, linear conductance and differential spectra such as differential conductance, differential shot noise and differential Fano factor to possess perfect Fano profiles that can be fully tuned by an external magnetic flux.","For a symmetric interferometer of(two arms with the same length, even the current and shot noise demonstrate fully tunable resonances and are bestowed different hallmarks unique to Fano resonances.","All the transport spectra display the same evolution pattern in an evolution cycle, which along with the profiles are robust against variation of the parameters defining the device."],"url":"http://arxiv.org/abs/2404.13597v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-21 09:29:20","title":"Unsupervised Social Bot Detection via Structural Information Theory","abstract":"Research on social bot detection plays a crucial role in maintaining the order and reliability of information dissemination while increasing trust in social interactions. The current mainstream social bot detection models rely on black-box neural network technology, e.g., Graph Neural Network, Transformer, etc., which lacks interpretability. In this work, we present UnDBot, a novel unsupervised, interpretable, yet effective and practical framework for detecting social bots. This framework is built upon structural information theory. We begin by designing three social relationship metrics that capture various aspects of social bot behaviors: Posting Type Distribution, Posting Influence, and Follow-to-follower Ratio. Three new relationships are utilized to construct a new, unified, and weighted social multi-relational graph, aiming to model the relevance of social user behaviors and discover long-distance correlations between users. Second, we introduce a novel method for optimizing heterogeneous structural entropy. This method involves the personalized aggregation of edge information from the social multi-relational graph to generate a two-dimensional encoding tree. The heterogeneous structural entropy facilitates decoding of the substantial structure of the social bots network and enables hierarchical clustering of social bots. Thirdly, a new community labeling method is presented to distinguish social bot communities by computing the user's stationary distribution, measuring user contributions to network structure, and counting the intensity of user aggregation within the community. Compared with ten representative social bot detection approaches, comprehensive experiments demonstrate the advantages of effectiveness and interpretability of UnDBot on four real social network datasets.","sentences":["Research on social bot detection plays a crucial role in maintaining the order and reliability of information dissemination while increasing trust in social interactions.","The current mainstream social bot detection models rely on black-box neural network technology, e.g., Graph Neural Network, Transformer, etc., which lacks interpretability.","In this work, we present UnDBot, a novel unsupervised, interpretable, yet effective and practical framework for detecting social bots.","This framework is built upon structural information theory.","We begin by designing three social relationship metrics that capture various aspects of social bot behaviors:","Posting Type Distribution, Posting Influence, and Follow-to-follower Ratio.","Three new relationships are utilized to construct a new, unified, and weighted social multi-relational graph, aiming to model the relevance of social user behaviors and discover long-distance correlations between users.","Second, we introduce a novel method for optimizing heterogeneous structural entropy.","This method involves the personalized aggregation of edge information from the social multi-relational graph to generate a two-dimensional encoding tree.","The heterogeneous structural entropy facilitates decoding of the substantial structure of the social bots network and enables hierarchical clustering of social bots.","Thirdly, a new community labeling method is presented to distinguish social bot communities by computing the user's stationary distribution, measuring user contributions to network structure, and counting the intensity of user aggregation within the community.","Compared with ten representative social bot detection approaches, comprehensive experiments demonstrate the advantages of effectiveness and interpretability of UnDBot on four real social network datasets."],"url":"http://arxiv.org/abs/2404.13595v1","category":"cs.SI"}
{"created":"2024-04-21 09:18:56","title":"Hysteretic dynamics of phase interfaces in bilinear forward-backward diffusion equations","abstract":"We study single-interface solutions to a free boundary problem that couples bilinear bulk diffusion to the Stefan condition and a hysteretic flow rule for phase boundaries. We introduce a time-discrete approximation scheme and establish its convergence in the limit of vanishing step size. The main difficulty in our proof are strong microscopic oscillations which are produced by a propagating phase interface and need to be controlled on the macroscopic scale. We also present numerical simulations and discuss the link to the viscous regularization of ill-posed diffusion equations.","sentences":["We study single-interface solutions to a free boundary problem that couples bilinear bulk diffusion to the Stefan condition and a hysteretic flow rule for phase boundaries.","We introduce a time-discrete approximation scheme and establish its convergence in the limit of vanishing step size.","The main difficulty in our proof are strong microscopic oscillations which are produced by a propagating phase interface and need to be controlled on the macroscopic scale.","We also present numerical simulations and discuss the link to the viscous regularization of ill-posed diffusion equations."],"url":"http://arxiv.org/abs/2404.13592v1","category":"math.AP"}
{"created":"2024-04-21 08:53:27","title":"Quantum simulation of the Fokker-Planck equation via Schrodingerization","abstract":"This paper studies a quantum simulation technique for solving the Fokker-Planck equation. Traditional semi-discretization methods often fail to preserve the underlying Hamiltonian dynamics and may even modify the Hamiltonian structure, particularly when incorporating boundary conditions. We address this challenge by employing the Schrodingerization method-it converts any linear partial and ordinary differential equation with non-Hermitian dynamics into systems of Schrodinger-type equations. We explore the application in two distinct forms of the Fokker-Planck equation. For the conservation form, we show that the semi-discretization-based Schrodingerization is preferable, especially when dealing with non-periodic boundary conditions. Additionally, we analyze the Schrodingerization approach for unstable systems that possess positive eigenvalues in the real part of the coefficient matrix or differential operator. Our analysis reveals that the direct use of Schrodingerization has the same effect as a stabilization procedure. For the heat equation form, we propose a quantum simulation procedure based on the time-splitting technique. We discuss the relationship between operator splitting in the Schrodingerization method and its application directly to the original problem, illustrating how the Schrodingerization method accurately reproduces the time-splitting solutions at each step. Furthermore, we explore finite difference discretizations of the heat equation form using shift operators. Utilizing Fourier bases, we diagonalize the shift operators, enabling efficient simulation in the frequency space. Providing additional guidance on implementing the diagonal unitary operators, we conduct a comparative analysis between diagonalizations in the Bell and the Fourier bases, and show that the former generally exhibits greater efficiency than the latter.","sentences":["This paper studies a quantum simulation technique for solving the Fokker-Planck equation.","Traditional semi-discretization methods often fail to preserve the underlying Hamiltonian dynamics and may even modify the Hamiltonian structure, particularly when incorporating boundary conditions.","We address this challenge by employing the Schrodingerization method-it converts any linear partial and ordinary differential equation with non-Hermitian dynamics into systems of Schrodinger-type equations.","We explore the application in two distinct forms of the Fokker-Planck equation.","For the conservation form, we show that the semi-discretization-based Schrodingerization is preferable, especially when dealing with non-periodic boundary conditions.","Additionally, we analyze the Schrodingerization approach for unstable systems that possess positive eigenvalues in the real part of the coefficient matrix or differential operator.","Our analysis reveals that the direct use of Schrodingerization has the same effect as a stabilization procedure.","For the heat equation form, we propose a quantum simulation procedure based on the time-splitting technique.","We discuss the relationship between operator splitting in the Schrodingerization method and its application directly to the original problem, illustrating how the Schrodingerization method accurately reproduces the time-splitting solutions at each step.","Furthermore, we explore finite difference discretizations of the heat equation form using shift operators.","Utilizing Fourier bases, we diagonalize the shift operators, enabling efficient simulation in the frequency space.","Providing additional guidance on implementing the diagonal unitary operators, we conduct a comparative analysis between diagonalizations in the Bell and the Fourier bases, and show that the former generally exhibits greater efficiency than the latter."],"url":"http://arxiv.org/abs/2404.13585v1","category":"quant-ph"}
{"created":"2024-04-21 08:38:40","title":"Self-consistency, relativism and many-particle system","abstract":"The interrelation between the concepts of self-consistency, relativism and many-particle systems is considered within the framework of a unified consideration of classical and quantum physics based on the first principle of the probability conservation law. The probability conservation law underlies the Vlasov equation chain. From the first Vlasov equation, the Schr\\\"odinger equation, the Hamilton-Jacobi equation, the equation of motion of a charged particle in an electromagnetic field, the Maxwell equations, the Pauli equation and the Dirac equation are constructed. The paper shows with mathematical rigor that quantum systems with a time independent function of quasi-density probability in phase space are not capable to emit electromagnetic radiation. It is shown that at the micro-level a quantum object may be considered rather as an {\\guillemotleft}extended{\\guillemotright} object than a point one. And the hydrodynamic description of continuum mechanics is applicable for such an object. A number of exact solutions of quantum and classical model systems is considered, demonstrating a new insight at the quantum mechanics representation.","sentences":["The interrelation between the concepts of self-consistency, relativism and many-particle systems is considered within the framework of a unified consideration of classical and quantum physics based on the first principle of the probability conservation law.","The probability conservation law underlies the Vlasov equation chain.","From the first Vlasov equation, the Schr\\\"odinger equation, the Hamilton-Jacobi equation, the equation of motion of a charged particle in an electromagnetic field, the Maxwell equations, the Pauli equation and the Dirac equation are constructed.","The paper shows with mathematical rigor that quantum systems with a time independent function of quasi-density probability in phase space are not capable to emit electromagnetic radiation.","It is shown that at the micro-level a quantum object may be considered rather as an {\\guillemotleft}extended{\\guillemotright} object than a point one.","And the hydrodynamic description of continuum mechanics is applicable for such an object.","A number of exact solutions of quantum and classical model systems is considered, demonstrating a new insight at the quantum mechanics representation."],"url":"http://arxiv.org/abs/2404.13580v1","category":"quant-ph"}
{"created":"2024-04-21 07:05:38","title":"Preconditioned Neural Posterior Estimation for Likelihood-free Inference","abstract":"Simulation based inference (SBI) methods enable the estimation of posterior distributions when the likelihood function is intractable, but where model simulation is feasible. Popular neural approaches to SBI are the neural posterior estimator (NPE) and its sequential version (SNPE). These methods can outperform statistical SBI approaches such as approximate Bayesian computation (ABC), particularly for relatively small numbers of model simulations. However, we show in this paper that the NPE methods are not guaranteed to be highly accurate, even on problems with low dimension. In such settings the posterior cannot be accurately trained over the prior predictive space, and even the sequential extension remains sub-optimal. To overcome this, we propose preconditioned NPE (PNPE) and its sequential version (PSNPE), which uses a short run of ABC to effectively eliminate regions of parameter space that produce large discrepancy between simulations and data and allow the posterior emulator to be more accurately trained. We present comprehensive empirical evidence that this melding of neural and statistical SBI methods improves performance over a range of examples, including a motivating example involving a complex agent-based model applied to real tumour growth data.","sentences":["Simulation based inference (SBI) methods enable the estimation of posterior distributions when the likelihood function is intractable, but where model simulation is feasible.","Popular neural approaches to SBI are the neural posterior estimator (NPE) and its sequential version (SNPE).","These methods can outperform statistical SBI approaches such as approximate Bayesian computation (ABC), particularly for relatively small numbers of model simulations.","However, we show in this paper that the NPE methods are not guaranteed to be highly accurate, even on problems with low dimension.","In such settings the posterior cannot be accurately trained over the prior predictive space, and even the sequential extension remains sub-optimal.","To overcome this, we propose preconditioned NPE (PNPE) and its sequential version (PSNPE), which uses a short run of ABC to effectively eliminate regions of parameter space that produce large discrepancy between simulations and data and allow the posterior emulator to be more accurately trained.","We present comprehensive empirical evidence that this melding of neural and statistical SBI methods improves performance over a range of examples, including a motivating example involving a complex agent-based model applied to real tumour growth data."],"url":"http://arxiv.org/abs/2404.13557v1","category":"stat.ML"}
{"created":"2024-04-21 06:42:39","title":"Filtered Stokes G-local Systems in Nonabelian Hodge Theory on Curves","abstract":"In the wild nonabelian Hodge correspondence on curves, filtered Stokes G-local systems are regarded as the objects on the Betti side. In this paper, we demonstrate a construction of the moduli space of them, called the Betti moduli space, it will reduce to the wild character variety when the Betti weights are trivial. We study some particular examples including Eguchi-Hanson space and the Airy equation together with the corresponding moduli spaces. Furthermore, we provide a proof of the correspondence among irregular singular G-connections, Stokes G-local systems, and Stokes G-representations to fill a gap in the literature. This correspondence can be viewed as the G-version of irregular Rieman-Hilbert correspondence on curves.","sentences":["In the wild nonabelian Hodge correspondence on curves, filtered Stokes G-local systems are regarded as the objects on the Betti side.","In this paper, we demonstrate a construction of the moduli space of them, called the Betti moduli space, it will reduce to the wild character variety when the Betti weights are trivial.","We study some particular examples including Eguchi-Hanson space and the Airy equation together with the corresponding moduli spaces.","Furthermore, we provide a proof of the correspondence among irregular singular G-connections, Stokes G-local systems, and Stokes G-representations to fill a gap in the literature.","This correspondence can be viewed as the G-version of irregular Rieman-Hilbert correspondence on curves."],"url":"http://arxiv.org/abs/2404.13553v1","category":"math.AG"}
{"created":"2024-04-21 06:36:00","title":"Focal Volume, Acoustic Radiation Force, and Strain in Two-Transducer Regimes","abstract":"Transcranial focused ultrasound stimulation (TUS) holds promise for non-invasive neural modulation in treating neurological disorders. Most clinically relevant targets are deep within the brain, surrounded by other sensitive regions that need to be spared clinical intervention. However, in TUS, increasing frequency with the goal of improving spatial resolution reduces the effective penetration depth. We show that by using a pair of 1 MHz, orthogonally arranged transducers we improve the spatial resolution afforded by each of the transducers individually, by nearly $40$ fold, achieving a sub-cubic millimeter target volume of $0.24\\ mm^3$ deep within the brain. We show that orthogonally placed transducers generate highly localized standing waves with Acoustic Radiation Force (ARF) arranged into periodic regions of compression and tension near the target. We further present an extended capability of the orthogonal setup, which is to impart selective pressures--either positive or negative, but not both--on the target. Lastly, we share our preliminary findings that strain can arise from both particle motion and ARF with the former reaching its maximum value at the focus, and the latter remaining null at the focus and reaching its maximum around the focus.   As the field is investigating the mechanism of interaction in TUS by way of elucidating the mapping between ultrasound parameters and neural response, orthogonal transducers expand our toolbox by making it possible to conduct these investigations at much finer spatial resolutions, with localized and directed (compression vs. tension) ARF and the capability of applying selective pressures at the target.","sentences":["Transcranial focused ultrasound stimulation (TUS) holds promise for non-invasive neural modulation in treating neurological disorders.","Most clinically relevant targets are deep within the brain, surrounded by other sensitive regions that need to be spared clinical intervention.","However, in TUS, increasing frequency with the goal of improving spatial resolution reduces the effective penetration depth.","We show that by using a pair of 1 MHz, orthogonally arranged transducers we improve the spatial resolution afforded by each of the transducers individually, by nearly $40$ fold, achieving a sub-cubic millimeter target volume of $0.24\\ mm^3$ deep within the brain.","We show that orthogonally placed transducers generate highly localized standing waves with Acoustic Radiation Force (ARF) arranged into periodic regions of compression and tension near the target.","We further present an extended capability of the orthogonal setup, which is to impart selective pressures--either positive or negative, but not both--on the target.","Lastly, we share our preliminary findings that strain can arise from both particle motion and ARF with the former reaching its maximum value at the focus, and the latter remaining null at the focus and reaching its maximum around the focus.   ","As the field is investigating the mechanism of interaction in TUS by way of elucidating the mapping between ultrasound parameters and neural response, orthogonal transducers expand our toolbox by making it possible to conduct these investigations at much finer spatial resolutions, with localized and directed (compression vs. tension) ARF and the capability of applying selective pressures at the target."],"url":"http://arxiv.org/abs/2404.13552v1","category":"physics.app-ph"}
{"created":"2024-04-21 06:31:29","title":"Pointsoup: High-Performance and Extremely Low-Decoding-Latency Learned Geometry Codec for Large-Scale Point Cloud Scenes","abstract":"Despite considerable progress being achieved in point cloud geometry compression, there still remains a challenge in effectively compressing large-scale scenes with sparse surfaces. Another key challenge lies in reducing decoding latency, a crucial requirement in real-world application. In this paper, we propose Pointsoup, an efficient learning-based geometry codec that attains high-performance and extremely low-decoding-latency simultaneously. Inspired by conventional Trisoup codec, a point model-based strategy is devised to characterize local surfaces. Specifically, skin features are embedded from local windows via an attention-based encoder, and dilated windows are introduced as cross-scale priors to infer the distribution of quantized features in parallel. During decoding, features undergo fast refinement, followed by a folding-based point generator that reconstructs point coordinates with fairly fast speed. Experiments show that Pointsoup achieves state-of-the-art performance on multiple benchmarks with significantly lower decoding complexity, i.e., up to 90$\\sim$160$\\times$ faster than the G-PCCv23 Trisoup decoder on a comparatively low-end platform (e.g., one RTX 2080Ti). Furthermore, it offers variable-rate control with a single neural model (2.9MB), which is attractive for industrial practitioners.","sentences":["Despite considerable progress being achieved in point cloud geometry compression, there still remains a challenge in effectively compressing large-scale scenes with sparse surfaces.","Another key challenge lies in reducing decoding latency, a crucial requirement in real-world application.","In this paper, we propose Pointsoup, an efficient learning-based geometry codec that attains high-performance and extremely low-decoding-latency simultaneously.","Inspired by conventional Trisoup codec, a point model-based strategy is devised to characterize local surfaces.","Specifically, skin features are embedded from local windows via an attention-based encoder, and dilated windows are introduced as cross-scale priors to infer the distribution of quantized features in parallel.","During decoding, features undergo fast refinement, followed by a folding-based point generator that reconstructs point coordinates with fairly fast speed.","Experiments show that Pointsoup achieves state-of-the-art performance on multiple benchmarks with significantly lower decoding complexity, i.e., up to 90$\\sim$160$\\times$ faster than the G-PCCv23 Trisoup decoder on a comparatively low-end platform (e.g., one RTX 2080Ti).","Furthermore, it offers variable-rate control with a single neural model (2.9MB), which is attractive for industrial practitioners."],"url":"http://arxiv.org/abs/2404.13550v1","category":"cs.CV"}
{"created":"2024-04-21 05:24:54","title":"Charged analog of anisotropic dark energy star in Rastall gravity","abstract":"Dark energy is one of the potential strategies for preventing compact objects from gravitationally collapsing into singularities. Because it is the cause of the accelerated expansion of our universe, it has the greatest impact on the cosmos. Thus, it is plausible that dark energy will interact with any stellar object that is compact in the universe [\\textit{Phys. Rev. D} \\textbf{103}, 084042 (2021)]. Our main goal in this work is to create a simplified model, in the Rastall gravitational framework, of a charged strange star coupled to anisotropic dark energy in Krori-Barua spacetime [\\textit{J. Phys. A, Math. Gen.} \\textbf{8}:508, 1975]. Here, we consider a specific strange star object, Her X-1, with observed values of mass $=(0.85 \\pm 0.15)M_{\\odot}$ and radius $= 8.1_{-0.41}^{+0.41}$ km., so that we can develop our model. In this context, we began by modeling dark energy using the equation of state (EoS), in which the dark energy density is proportional to the isotropic perfect fluid matter-energy density. The Darmois-Israel condition has been used to calculate the unknown constants that are present in the metric. We perform a detailed analysis of the model's physical properties, including the mass-radius relation, pressure, density, metric function, and dark energy parameters, by varying the Rastall coupling parameter. We also examine the stability and force equilibrium of our proposed stellar configuration. Following a comprehensive theoretical analysis, we discovered that our suggested model is both singularity-free and meets all stability requirements needed to be a stable, physically reasonable stellar model.","sentences":["Dark energy is one of the potential strategies for preventing compact objects from gravitationally collapsing into singularities.","Because it is the cause of the accelerated expansion of our universe, it has the greatest impact on the cosmos.","Thus, it is plausible that dark energy will interact with any stellar object that is compact in the universe [\\textit{Phys. Rev. D} \\textbf{103}, 084042 (2021)].","Our main goal in this work is to create a simplified model, in the Rastall gravitational framework, of a charged strange star coupled to anisotropic dark energy in Krori-Barua spacetime [\\textit{J. Phys.","A, Math. Gen.} \\textbf{8}:508, 1975].","Here, we consider a specific strange star object, Her X-1, with observed values of mass $=(0.85 \\pm 0.15)M_{\\odot}$ and radius $= 8.1_{-0.41}^{+0.41}$ km., so that we can develop our model.","In this context, we began by modeling dark energy using the equation of state (EoS), in which the dark energy density is proportional to the isotropic perfect fluid matter-energy density.","The Darmois-Israel condition has been used to calculate the unknown constants that are present in the metric.","We perform a detailed analysis of the model's physical properties, including the mass-radius relation, pressure, density, metric function, and dark energy parameters, by varying the Rastall coupling parameter.","We also examine the stability and force equilibrium of our proposed stellar configuration.","Following a comprehensive theoretical analysis, we discovered that our suggested model is both singularity-free and meets all stability requirements needed to be a stable, physically reasonable stellar model."],"url":"http://arxiv.org/abs/2404.13538v1","category":"gr-qc"}
{"created":"2024-04-21 05:07:51","title":"SpringGrasp: An optimization pipeline for robust and compliant dexterous pre-grasp synthesis","abstract":"Generating stable and robust grasps on arbitrary objects is critical for dexterous robotic hands, marking a significant step towards advanced dexterous manipulation. Previous studies have mostly focused on improving differentiable grasping metrics with the assumption of precisely known object geometry. However, shape uncertainty is ubiquitous due to noisy and partial shape observations, which introduce challenges in grasp planning. We propose, SpringGrasp planner, a planner that considers uncertain observations of the object surface for synthesizing compliant dexterous grasps. A compliant dexterous grasp could minimize the effect of unexpected contact with the object, leading to more stable grasp with shape-uncertain objects. We introduce an analytical and differentiable metric, SpringGrasp metric, that evaluates the dynamic behavior of the entire compliant grasping process. Planning with SpringGrasp planner, our method achieves a grasp success rate of 89% from two viewpoints and 84% from a single viewpoints in experiment with a real robot on 14 common objects. Compared with a force-closure based planner, our method achieves at least 18% higher grasp success rate.","sentences":["Generating stable and robust grasps on arbitrary objects is critical for dexterous robotic hands, marking a significant step towards advanced dexterous manipulation.","Previous studies have mostly focused on improving differentiable grasping metrics with the assumption of precisely known object geometry.","However, shape uncertainty is ubiquitous due to noisy and partial shape observations, which introduce challenges in grasp planning.","We propose, SpringGrasp planner, a planner that considers uncertain observations of the object surface for synthesizing compliant dexterous grasps.","A compliant dexterous grasp could minimize the effect of unexpected contact with the object, leading to more stable grasp with shape-uncertain objects.","We introduce an analytical and differentiable metric, SpringGrasp metric, that evaluates the dynamic behavior of the entire compliant grasping process.","Planning with SpringGrasp planner, our method achieves a grasp success rate of 89% from two viewpoints and 84% from a single viewpoints in experiment with a real robot on 14 common objects.","Compared with a force-closure based planner, our method achieves at least 18% higher grasp success rate."],"url":"http://arxiv.org/abs/2404.13532v1","category":"cs.RO"}
{"created":"2024-04-21 02:29:06","title":"Uncovering Obscured Phonon Dynamics from Powder Inelastic Neutron Scattering using Machine Learning","abstract":"The study of phonon dynamics is pivotal for understanding material properties, yet it faces challenges due to the irreversible information loss inherent in powder inelastic neutron scattering spectra and the limitations of traditional analysis methods. In this study, we present a machine learning framework designed to reveal obscured phonon dynamics from powder spectra. Using a variational autoencoder, we obtain a disentangled latent representation of spectra and successfully extract force constants for reconstructing phonon dispersions. Notably, our model demonstrates effective applicability to experimental data even when trained exclusively on physics-based simulations. The fine-tuning with experimental spectra further mitigates issues arising from domain shift. Analysis of latent space underscores the model's versatility and generalizability, affirming its suitability for complex system applications. Furthermore, our framework's two-stage design is promising for developing a universal pre-trained feature extractor. This approach has the potential to revolutionize neutron measurements of phonon dynamics, offering researchers a potent tool to decipher intricate spectra and gain valuable insights into the intrinsic physics of materials.","sentences":["The study of phonon dynamics is pivotal for understanding material properties, yet it faces challenges due to the irreversible information loss inherent in powder inelastic neutron scattering spectra and the limitations of traditional analysis methods.","In this study, we present a machine learning framework designed to reveal obscured phonon dynamics from powder spectra.","Using a variational autoencoder, we obtain a disentangled latent representation of spectra and successfully extract force constants for reconstructing phonon dispersions.","Notably, our model demonstrates effective applicability to experimental data even when trained exclusively on physics-based simulations.","The fine-tuning with experimental spectra further mitigates issues arising from domain shift.","Analysis of latent space underscores the model's versatility and generalizability, affirming its suitability for complex system applications.","Furthermore, our framework's two-stage design is promising for developing a universal pre-trained feature extractor.","This approach has the potential to revolutionize neutron measurements of phonon dynamics, offering researchers a potent tool to decipher intricate spectra and gain valuable insights into the intrinsic physics of materials."],"url":"http://arxiv.org/abs/2404.13507v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 00:35:24","title":"Global Bifurcation of Non-Radial Solutions for Symmetric Sub-linear Elliptic Systems on the Planar Unit Disc","abstract":"In this paper, we prove a global bifurcation result for the existence of non-radial branches of solutions to the paramterized family of $\\Gamma$-symmetric problems $-\\Delta u=f(\\alpha,z,u)$, $u|_{\\partial D}=0$ on the unit disc $D:=\\{z\\in \\mathbb{C} : |z|<1\\}$ with $u(z)\\in \\mathbb{R}^k$, where $\\mathbb{R}^k$ is an orthogonal $\\Gamma$-representation, $f: \\mathbb{R} \\times \\overline{D} \\times \\mathbb{R}^k\\to \\mathbb{R}^k$ is a sub-linear $\\Gamma$-equivariant continuous function, differentiable with respect to $u$ at zero and satisfying the conditions $f(\\alpha, e^{i\\theta}z,u)=f(\\alpha, z,u)$ for all $\\theta\\in \\mathbb{R}$ and $f(z,-u)=-f(z,u)$.","sentences":["In this paper, we prove a global bifurcation result for the existence of non-radial branches of solutions to the paramterized family of $\\Gamma$-symmetric problems $-\\Delta u=f(\\alpha,z,u)$, $u|_{\\partial D}=0$ on the unit disc $D:=\\{z\\in \\mathbb{C} : |z|<1\\}$ with $u(z)\\in \\mathbb{R}^k$, where $\\mathbb{R}^k$ is an orthogonal $\\Gamma$-representation, $f: \\mathbb{R} \\times \\overline{D} \\times \\mathbb{R}^k\\to \\mathbb{R}^k$ is a sub-linear $\\Gamma$-equivariant continuous function, differentiable with respect to $u$ at zero and satisfying the conditions $f(\\alpha, e^{i\\theta}z,u)=f(\\alpha, z,u)$ for all $\\theta\\in \\mathbb{R}$ and $f(z,-u)=-f(z,u)$."],"url":"http://arxiv.org/abs/2404.13495v1","category":"math.AP"}
{"created":"2024-04-22 17:58:13","title":"MLQAOA: Graph Learning Accelerated Hybrid Quantum-Classical Multilevel QAOA","abstract":"Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches. We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms. The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA. We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility: Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}","sentences":["Learning the problem structure at multiple levels of coarseness to inform the decomposition-based hybrid quantum-classical combinatorial optimization solvers is a promising approach to scaling up variational approaches.","We introduce a multilevel algorithm reinforced with the spectral graph representation learning-based accelerator to tackle large-scale graph maximum cut instances and fused with several versions of the quantum approximate optimization algorithm (QAOA) and QAOA-inspired algorithms.","The graph representation learning model utilizes the idea of QAOA variational parameters concentration and substantially improves the performance of QAOA.","We demonstrate the potential of using multilevel QAOA and representation learning-based approaches on very large graphs by achieving high-quality solutions in a much faster time.\\\\ Reproducibility:","Our source code and results are available at \\url{https://github.com/bachbao/MLQAOA}"],"url":"http://arxiv.org/abs/2404.14399v1","category":"quant-ph"}
{"created":"2024-04-22 17:56:26","title":"RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?","abstract":"Large language models (LLMs) and small language models (SLMs) are being adopted at remarkable speed, although their safety still remains a serious concern. With the advent of multilingual S/LLMs, the question now becomes a matter of scale: can we expand multilingual safety evaluations of these models with the same velocity at which they are deployed? To this end we introduce RTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and outputs in 28 languages. RTP-LX follows participatory design practices, and a portion of the corpus is especially designed to detect culturally-specific toxic language. We evaluate seven S/LLMs on their ability to detect toxic content in a culturally-sensitive, multilingual scenario. We find that, although they typically score acceptably in terms of accuracy, they have low agreement with human judges when judging holistically the toxicity of a prompt, and have difficulty discerning harm in context-dependent scenarios, particularly with subtle-yet-harmful content (e.g. microagressions, bias). We release of this dataset to contribute to further reduce harmful uses of these models and improve their safe deployment.","sentences":["Large language models (LLMs) and small language models (SLMs) are being adopted at remarkable speed, although their safety still remains a serious concern.","With the advent of multilingual S/LLMs, the question now becomes a matter of scale: can we expand multilingual safety evaluations of these models with the same velocity at which they are deployed?","To this end we introduce RTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and outputs in 28 languages.","RTP-LX follows participatory design practices, and a portion of the corpus is especially designed to detect culturally-specific toxic language.","We evaluate seven S/LLMs on their ability to detect toxic content in a culturally-sensitive, multilingual scenario.","We find that, although they typically score acceptably in terms of accuracy, they have low agreement with human judges when judging holistically the toxicity of a prompt, and have difficulty discerning harm in context-dependent scenarios, particularly with subtle-yet-harmful content (e.g. microagressions, bias).","We release of this dataset to contribute to further reduce harmful uses of these models and improve their safe deployment."],"url":"http://arxiv.org/abs/2404.14397v1","category":"cs.CL"}
{"created":"2024-04-22 16:38:41","title":"PLUTO: Pushing the Limit of Imitation Learning-based Planning for Autonomous Driving","abstract":"We present PLUTO, a powerful framework that pushes the limit of imitation learning-based planning for autonomous driving. Our improvements stem from three pivotal aspects: a longitudinal-lateral aware model architecture that enables flexible and diverse driving behaviors; An innovative auxiliary loss computation method that is broadly applicable and efficient for batch-wise calculation; A novel training framework that leverages contrastive learning, augmented by a suite of new data augmentations to regulate driving behaviors and facilitate the understanding of underlying interactions. We assessed our framework using the large-scale real-world nuPlan dataset and its associated standardized planning benchmark. Impressively, PLUTO achieves state-of-the-art closed-loop performance, beating other competing learning-based methods and surpassing the current top-performed rule-based planner for the first time. Results and code are available at https://jchengai.github.io/pluto.","sentences":["We present PLUTO, a powerful framework that pushes the limit of imitation learning-based planning for autonomous driving.","Our improvements stem from three pivotal aspects: a longitudinal-lateral aware model architecture that enables flexible and diverse driving behaviors; An innovative auxiliary loss computation method that is broadly applicable and efficient for batch-wise calculation; A novel training framework that leverages contrastive learning, augmented by a suite of new data augmentations to regulate driving behaviors and facilitate the understanding of underlying interactions.","We assessed our framework using the large-scale real-world nuPlan dataset and its associated standardized planning benchmark.","Impressively, PLUTO achieves state-of-the-art closed-loop performance, beating other competing learning-based methods and surpassing the current top-performed rule-based planner for the first time.","Results and code are available at https://jchengai.github.io/pluto."],"url":"http://arxiv.org/abs/2404.14327v1","category":"cs.RO"}
{"created":"2024-04-22 16:33:06","title":"A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module","abstract":"Lung segmentation in chest X-ray images is of paramount importance as it plays a crucial role in the diagnosis and treatment of various lung diseases. This paper presents a novel approach for lung segmentation in chest X-ray images by integrating U-net with attention mechanisms. The proposed method enhances the U-net architecture by incorporating a Convolutional Block Attention Module (CBAM), which unifies three distinct attention mechanisms: channel attention, spatial attention, and pixel attention. The channel attention mechanism enables the model to concentrate on the most informative features across various channels. The spatial attention mechanism enhances the model's precision in localization by focusing on significant spatial locations. Lastly, the pixel attention mechanism empowers the model to focus on individual pixels, further refining the model's focus and thereby improving the accuracy of segmentation. The adoption of the proposed CBAM in conjunction with the U-net architecture marks a significant advancement in the field of medical imaging, with potential implications for improving diagnostic precision and patient outcomes. The efficacy of this method is validated against contemporary state-of-the-art techniques, showcasing its superiority in segmentation performance.","sentences":["Lung segmentation in chest X-ray images is of paramount importance as it plays a crucial role in the diagnosis and treatment of various lung diseases.","This paper presents a novel approach for lung segmentation in chest X-ray images by integrating U-net with attention mechanisms.","The proposed method enhances the U-net architecture by incorporating a Convolutional Block Attention Module (CBAM), which unifies three distinct attention mechanisms: channel attention, spatial attention, and pixel attention.","The channel attention mechanism enables the model to concentrate on the most informative features across various channels.","The spatial attention mechanism enhances the model's precision in localization by focusing on significant spatial locations.","Lastly, the pixel attention mechanism empowers the model to focus on individual pixels, further refining the model's focus and thereby improving the accuracy of segmentation.","The adoption of the proposed CBAM in conjunction with the U-net architecture marks a significant advancement in the field of medical imaging, with potential implications for improving diagnostic precision and patient outcomes.","The efficacy of this method is validated against contemporary state-of-the-art techniques, showcasing its superiority in segmentation performance."],"url":"http://arxiv.org/abs/2404.14322v1","category":"eess.IV"}
{"created":"2024-04-22 15:15:50","title":"What do Transformers Know about Government?","abstract":"This paper investigates what insights about linguistic features and what knowledge about the structure of natural language can be obtained from the encodings in transformer language models.In particular, we explore how BERT encodes the government relation between constituents in a sentence. We use several probing classifiers, and data from two morphologically rich languages. Our experiments show that information about government is encoded across all transformer layers, but predominantly in the early layers of the model. We find that, for both languages, a small number of attention heads encode enough information about the government relations to enable us to train a classifier capable of discovering new, previously unknown types of government, never seen in the training data. Currently, data is lacking for the research community working on grammatical constructions, and government in particular. We release the Government Bank -- a dataset defining the government relations for thousands of lemmas in the languages in our experiments.","sentences":["This paper investigates what insights about linguistic features and what knowledge about the structure of natural language can be obtained from the encodings in transformer language models.","In particular, we explore how BERT encodes the government relation between constituents in a sentence.","We use several probing classifiers, and data from two morphologically rich languages.","Our experiments show that information about government is encoded across all transformer layers, but predominantly in the early layers of the model.","We find that, for both languages, a small number of attention heads encode enough information about the government relations to enable us to train a classifier capable of discovering new, previously unknown types of government, never seen in the training data.","Currently, data is lacking for the research community working on grammatical constructions, and government in particular.","We release the Government Bank -- a dataset defining the government relations for thousands of lemmas in the languages in our experiments."],"url":"http://arxiv.org/abs/2404.14270v1","category":"cs.CL"}
{"created":"2024-04-22 15:01:32","title":"CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and View-consistent 3D Semantic Understanding","abstract":"The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method.","sentences":["The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes.","Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes.","To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data.","In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach.","SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (>100 FPS).","Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model.","3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results.","Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed.","Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method."],"url":"http://arxiv.org/abs/2404.14249v1","category":"cs.CV"}
{"created":"2024-04-22 14:46:47","title":"EcoPull: Sustainable IoT Image Retrieval Empowered by TinyML Models","abstract":"This paper introduces EcoPull, a sustainable Internet of Things (IoT) framework empowered by tiny machine learning (TinyML) models for fetching images from wireless visual sensor networks. Two types of learnable TinyML models are installed in the IoT devices: i) a behavior model and ii) an image compressor model. The first filters out irrelevant images for the current task, reducing unnecessary transmission and resource competition among the devices. The second allows IoT devices to communicate with the receiver via latent representations of images, reducing communication bandwidth usage. However, integrating learnable modules into IoT devices comes at the cost of increased energy consumption due to inference. The numerical results show that the proposed framework can save > 70% energy compared to the baseline while maintaining the quality of the retrieved images at the ES.","sentences":["This paper introduces EcoPull, a sustainable Internet of Things (IoT) framework empowered by tiny machine learning (TinyML) models for fetching images from wireless visual sensor networks.","Two types of learnable TinyML models are installed in the IoT devices: i) a behavior model and ii) an image compressor model.","The first filters out irrelevant images for the current task, reducing unnecessary transmission and resource competition among the devices.","The second allows IoT devices to communicate with the receiver via latent representations of images, reducing communication bandwidth usage.","However, integrating learnable modules into IoT devices comes at the cost of increased energy consumption due to inference.","The numerical results show that the proposed framework can save > 70% energy compared to the baseline while maintaining the quality of the retrieved images at the ES."],"url":"http://arxiv.org/abs/2404.14236v1","category":"cs.NI"}
{"created":"2024-04-22 10:21:41","title":"Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras","abstract":"The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images. Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL. However, hardware constraints may limit the availability of multiple cameras in real-world deployment. Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training. To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that achieves zero-shot generalisation to any single camera from the training set. Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific. We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera.","sentences":["The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images.","Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL.","However, hardware constraints may limit the availability of multiple cameras in real-world deployment.","Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training.","To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that achieves zero-shot generalisation to any single camera from the training set.","Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific.","We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera."],"url":"http://arxiv.org/abs/2404.14064v1","category":"cs.LG"}
{"created":"2024-04-22 09:44:44","title":"Teaching Scrum with a focus on compliance assessment","abstract":"The Scrum framework has gained widespread adoption in the industry for its emphasis on collaboration and continuous improvement. However, it has not reached a similar relevance in Software Engineering (SE) curricula. This work reports the experience of five editions of a SE course within an M.Sc. Degree in Computer Engineering. The course primary educational objective is to provide students with the skills to manage software development projects with Scrum. The course is based on the execution of a team project and on the definition of qualitative and quantitative means of assessment of the application of Scrum. The conduction of five editions of the course allowed us to identify several lessons learned about time budgeting and team compositions in agile student projects and its evidence of the applicability of the framework to software development courses.","sentences":["The Scrum framework has gained widespread adoption in the industry for its emphasis on collaboration and continuous improvement.","However, it has not reached a similar relevance in Software Engineering (SE) curricula.","This work reports the experience of five editions of a SE course within an M.Sc. Degree in Computer Engineering.","The course primary educational objective is to provide students with the skills to manage software development projects with Scrum.","The course is based on the execution of a team project and on the definition of qualitative and quantitative means of assessment of the application of Scrum.","The conduction of five editions of the course allowed us to identify several lessons learned about time budgeting and team compositions in agile student projects and its evidence of the applicability of the framework to software development courses."],"url":"http://arxiv.org/abs/2404.14029v1","category":"cs.SE"}
{"created":"2024-04-22 09:43:03","title":"OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining BEV Segmentation Networks","abstract":"We introduce a self-supervised pretraining method, called OcFeat, for camera-only Bird's-Eye-View (BEV) segmentation networks. With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks. Occupancy prediction provides a 3D geometric understanding of the scene to the model. However, the geometry learned is class-agnostic. Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model. Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in low-data scenarios. Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach.","sentences":["We introduce a self-supervised pretraining method, called OcFeat, for camera-only Bird's-Eye-View (BEV) segmentation networks.","With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks.","Occupancy prediction provides a 3D geometric understanding of the scene to the model.","However, the geometry learned is class-agnostic.","Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model.","Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in low-data scenarios.","Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach."],"url":"http://arxiv.org/abs/2404.14027v1","category":"cs.CV"}
{"created":"2024-04-22 09:16:14","title":"Distilled Datamodel with Reverse Gradient Matching","abstract":"The proliferation of large-scale AI models trained on extensive datasets has revolutionized machine learning. With these models taking on increasingly central roles in various applications, the need to understand their behavior and enhance interpretability has become paramount. To investigate the impact of changes in training data on a pre-trained model, a common approach is leave-one-out retraining. This entails systematically altering the training dataset by removing specific samples to observe resulting changes within the model. However, retraining the model for each altered dataset presents a significant computational challenge, given the need to perform this operation for every dataset variation. In this paper, we introduce an efficient framework for assessing data impact, comprising offline training and online evaluation stages. During the offline training phase, we approximate the influence of training data on the target model through a distilled synset, formulated as a reversed gradient matching problem. For online evaluation, we expedite the leave-one-out process using the synset, which is then utilized to compute the attribution matrix based on the evaluation objective. Experimental evaluations, including training data attribution and assessments of data quality, demonstrate that our proposed method achieves comparable model behavior evaluation while significantly speeding up the process compared to the direct retraining method.","sentences":["The proliferation of large-scale AI models trained on extensive datasets has revolutionized machine learning.","With these models taking on increasingly central roles in various applications, the need to understand their behavior and enhance interpretability has become paramount.","To investigate the impact of changes in training data on a pre-trained model, a common approach is leave-one-out retraining.","This entails systematically altering the training dataset by removing specific samples to observe resulting changes within the model.","However, retraining the model for each altered dataset presents a significant computational challenge, given the need to perform this operation for every dataset variation.","In this paper, we introduce an efficient framework for assessing data impact, comprising offline training and online evaluation stages.","During the offline training phase, we approximate the influence of training data on the target model through a distilled synset, formulated as a reversed gradient matching problem.","For online evaluation, we expedite the leave-one-out process using the synset, which is then utilized to compute the attribution matrix based on the evaluation objective.","Experimental evaluations, including training data attribution and assessments of data quality, demonstrate that our proposed method achieves comparable model behavior evaluation while significantly speeding up the process compared to the direct retraining method."],"url":"http://arxiv.org/abs/2404.14006v1","category":"cs.LG"}
{"created":"2024-04-22 09:03:21","title":"CoFInAl: Enhancing Action Quality Assessment with Coarse-to-Fine Instruction Alignment","abstract":"Action Quality Assessment (AQA) is pivotal for quantifying actions across domains like sports and medical care. Existing methods often rely on pre-trained backbones from large-scale action recognition datasets to boost performance on smaller AQA datasets. However, this common strategy yields suboptimal results due to the inherent struggle of these backbones to capture the subtle cues essential for AQA. Moreover, fine-tuning on smaller datasets risks overfitting. To address these issues, we propose Coarse-to-Fine Instruction Alignment (CoFInAl). Inspired by recent advances in large language model tuning, CoFInAl aligns AQA with broader pre-trained tasks by reformulating it as a coarse-to-fine classification task. Initially, it learns grade prototypes for coarse assessment and then utilizes fixed sub-grade prototypes for fine-grained assessment. This hierarchical approach mirrors the judging process, enhancing interpretability within the AQA framework. Experimental results on two long-term AQA datasets demonstrate CoFInAl achieves state-of-the-art performance with significant correlation gains of 5.49% and 3.55% on Rhythmic Gymnastics and Fis-V, respectively. Our code is available at https://github.com/ZhouKanglei/CoFInAl_AQA.","sentences":["Action Quality Assessment (AQA) is pivotal for quantifying actions across domains like sports and medical care.","Existing methods often rely on pre-trained backbones from large-scale action recognition datasets to boost performance on smaller AQA datasets.","However, this common strategy yields suboptimal results due to the inherent struggle of these backbones to capture the subtle cues essential for AQA.","Moreover, fine-tuning on smaller datasets risks overfitting.","To address these issues, we propose Coarse-to-Fine Instruction Alignment (CoFInAl).","Inspired by recent advances in large language model tuning, CoFInAl aligns AQA with broader pre-trained tasks by reformulating it as a coarse-to-fine classification task.","Initially, it learns grade prototypes for coarse assessment and then utilizes fixed sub-grade prototypes for fine-grained assessment.","This hierarchical approach mirrors the judging process, enhancing interpretability within the AQA framework.","Experimental results on two long-term AQA datasets demonstrate CoFInAl achieves state-of-the-art performance with significant correlation gains of 5.49% and 3.55% on Rhythmic Gymnastics and Fis-V, respectively.","Our code is available at https://github.com/ZhouKanglei/CoFInAl_AQA."],"url":"http://arxiv.org/abs/2404.13999v1","category":"cs.CV"}
{"created":"2024-04-22 06:52:12","title":"Audio Anti-Spoofing Detection: A Survey","abstract":"The availability of smart devices leads to an exponential increase in multimedia content. However, the rapid advancements in deep learning have given rise to sophisticated algorithms capable of manipulating or creating multimedia fake content, known as Deepfake. Audio Deepfakes pose a significant threat by producing highly realistic voices, thus facilitating the spread of misinformation. To address this issue, numerous audio anti-spoofing detection challenges have been organized to foster the development of anti-spoofing countermeasures. This survey paper presents a comprehensive review of every component within the detection pipeline, including algorithm architectures, optimization techniques, application generalizability, evaluation metrics, performance comparisons, available datasets, and open-source availability. For each aspect, we conduct a systematic evaluation of the recent advancements, along with discussions on existing challenges. Additionally, we also explore emerging research topics on audio anti-spoofing, including partial spoofing detection, cross-dataset evaluation, and adversarial attack defence, while proposing some promising research directions for future work. This survey paper not only identifies the current state-of-the-art to establish strong baselines for future experiments but also guides future researchers on a clear path for understanding and enhancing the audio anti-spoofing detection mechanisms.","sentences":["The availability of smart devices leads to an exponential increase in multimedia content.","However, the rapid advancements in deep learning have given rise to sophisticated algorithms capable of manipulating or creating multimedia fake content, known as Deepfake.","Audio Deepfakes pose a significant threat by producing highly realistic voices, thus facilitating the spread of misinformation.","To address this issue, numerous audio anti-spoofing detection challenges have been organized to foster the development of anti-spoofing countermeasures.","This survey paper presents a comprehensive review of every component within the detection pipeline, including algorithm architectures, optimization techniques, application generalizability, evaluation metrics, performance comparisons, available datasets, and open-source availability.","For each aspect, we conduct a systematic evaluation of the recent advancements, along with discussions on existing challenges.","Additionally, we also explore emerging research topics on audio anti-spoofing, including partial spoofing detection, cross-dataset evaluation, and adversarial attack defence, while proposing some promising research directions for future work.","This survey paper not only identifies the current state-of-the-art to establish strong baselines for future experiments but also guides future researchers on a clear path for understanding and enhancing the audio anti-spoofing detection mechanisms."],"url":"http://arxiv.org/abs/2404.13914v1","category":"cs.SD"}
{"created":"2024-04-22 06:16:54","title":"Cross-Modal Generative Semantic Communications for Mobile AIGC: Joint Semantic Encoding and Prompt Engineering","abstract":"Employing massive Mobile AI-Generated Content (AIGC) Service Providers (MASPs) with powerful models, high-quality AIGC services can become accessible for resource-constrained end users. However, this advancement, referred to as mobile AIGC, also introduces a significant challenge: users should download large AIGC outputs from the MASPs, leading to substantial bandwidth consumption and potential transmission failures. In this paper, we apply cross-modal Generative Semantic Communications (G-SemCom) in mobile AIGC to overcome wireless bandwidth constraints. Specifically, we utilize a series of cross-modal attention maps to indicate the correlation between user prompts and each part of AIGC outputs. In this way, the MASP can analyze the prompt context and filter the most semantically important content efficiently. Only semantic information is transmitted, with which users can recover the entire AIGC output with high quality while saving mobile bandwidth. Since the transmitted information not only preserves the semantics but also prompts the recovery, we formulate a joint semantic encoding and prompt engineering problem to optimize the bandwidth allocation among users. Particularly, we present a human-perceptual metric named Joint Perpetual Similarity and Quality (JPSQ), which is fused by two learning-based measurements regarding semantic similarity and aesthetic quality, respectively. Furthermore, we develop the Attention-aware Deep Diffusion (ADD) algorithm, which learns attention maps and leverages the diffusion process to enhance the environment exploration ability. Extensive experiments demonstrate that our proposal can reduce the bandwidth consumption of mobile users by 49.4% on average, with almost no perceptual difference in AIGC output quality. Moreover, the ADD algorithm shows superior performance over baseline DRL methods, with 1.74x higher overall reward.","sentences":["Employing massive Mobile AI-Generated Content (AIGC) Service Providers (MASPs) with powerful models, high-quality AIGC services can become accessible for resource-constrained end users.","However, this advancement, referred to as mobile AIGC, also introduces a significant challenge: users should download large AIGC outputs from the MASPs, leading to substantial bandwidth consumption and potential transmission failures.","In this paper, we apply cross-modal Generative Semantic Communications (G-SemCom) in mobile AIGC to overcome wireless bandwidth constraints.","Specifically, we utilize a series of cross-modal attention maps to indicate the correlation between user prompts and each part of AIGC outputs.","In this way, the MASP can analyze the prompt context and filter the most semantically important content efficiently.","Only semantic information is transmitted, with which users can recover the entire AIGC output with high quality while saving mobile bandwidth.","Since the transmitted information not only preserves the semantics but also prompts the recovery, we formulate a joint semantic encoding and prompt engineering problem to optimize the bandwidth allocation among users.","Particularly, we present a human-perceptual metric named Joint Perpetual Similarity and Quality (JPSQ), which is fused by two learning-based measurements regarding semantic similarity and aesthetic quality, respectively.","Furthermore, we develop the Attention-aware Deep Diffusion (ADD) algorithm, which learns attention maps and leverages the diffusion process to enhance the environment exploration ability.","Extensive experiments demonstrate that our proposal can reduce the bandwidth consumption of mobile users by 49.4% on average, with almost no perceptual difference in AIGC output quality.","Moreover, the ADD algorithm shows superior performance over baseline DRL methods, with 1.74x higher overall reward."],"url":"http://arxiv.org/abs/2404.13898v1","category":"cs.NI"}
{"created":"2024-04-22 04:57:33","title":"Multi-Level Sequence Denoising with Cross-Signal Contrastive Learning for Sequential Recommendation","abstract":"Sequential recommender systems (SRSs) aim to suggest next item for a user based on her historical interaction sequences. Recently, many research efforts have been devoted to attenuate the influence of noisy items in sequences by either assigning them with lower attention weights or discarding them directly. The major limitation of these methods is that the former would still prone to overfit noisy items while the latter may overlook informative items. To the end, in this paper, we propose a novel model named Multi-level Sequence Denoising with Cross-signal Contrastive Learning (MSDCCL) for sequential recommendation. To be specific, we first introduce a target-aware user interest extractor to simultaneously capture users' long and short term interest with the guidance of target items. Then, we develop a multi-level sequence denoising module to alleviate the impact of noisy items by employing both soft and hard signal denoising strategies. Additionally, we extend existing curriculum learning by simulating the learning pattern of human beings. It is worth noting that our proposed model can be seamlessly integrated with a majority of existing recommendation models and significantly boost their effectiveness. Experimental studies on five public datasets are conducted and the results demonstrate that the proposed MSDCCL is superior to the state-of-the-art baselines. The source code is publicly available at https://github.com/lalunex/MSDCCL/tree/main.","sentences":["Sequential recommender systems (SRSs) aim to suggest next item for a user based on her historical interaction sequences.","Recently, many research efforts have been devoted to attenuate the influence of noisy items in sequences by either assigning them with lower attention weights or discarding them directly.","The major limitation of these methods is that the former would still prone to overfit noisy items while the latter may overlook informative items.","To the end, in this paper, we propose a novel model named Multi-level Sequence Denoising with Cross-signal Contrastive Learning (MSDCCL) for sequential recommendation.","To be specific, we first introduce a target-aware user interest extractor to simultaneously capture users' long and short term interest with the guidance of target items.","Then, we develop a multi-level sequence denoising module to alleviate the impact of noisy items by employing both soft and hard signal denoising strategies.","Additionally, we extend existing curriculum learning by simulating the learning pattern of human beings.","It is worth noting that our proposed model can be seamlessly integrated with a majority of existing recommendation models and significantly boost their effectiveness.","Experimental studies on five public datasets are conducted and the results demonstrate that the proposed MSDCCL is superior to the state-of-the-art baselines.","The source code is publicly available at https://github.com/lalunex/MSDCCL/tree/main."],"url":"http://arxiv.org/abs/2404.13878v1","category":"cs.IR"}
{"created":"2024-04-22 04:27:29","title":"Reproducible empirical evidence of cosmological-scale asymmetry in galaxy spin directions: comment on arXiv:2404.06617","abstract":"The distribution of the spin directions of galaxies has been a question in the past decade, with numerous Earth-based and space-based experiments showing that the distribution is not necessarily random. These experiments were based on different statistical methods, one of them was a simple and empirically verified open source $\\chi^2$ method. Patel & Desmond (2024) proposed that previous experiments showing non-random distribution are flawed since they assume Gaussian distribution. To address that, they apply a new complex ad-hoc statistical method to several datasets, none of them except for one were used in the past to claim for a dipole axis. The new method showed that all datasets except for one exhibit isotropy. This paper discusses the soundness of the contention that Gaussian distribution cannot be assumed for galaxy spin directions. More importantly, simple empirical analyses show that the new statistical method is not fully responsive to asymmetry in the distribution of galaxy spin directions, and does not identify non-random distribution even in situations where a dipole axis clearly exists in the data, or when an artificial bias is added to the data to create an extremely non-random dataset. Results using Monte Carlo simulation show substantial differences between the results of the simulation and the results of the new statistical method. Code and data to reproduce the experiments are available, and released in a manner that is easily reproducible. Possible reasons leading to the results are also discussed. The claims that the actual results are different from the results reported in the papers are examined in an open and transparent manner. These claims are found to be inaccurate, as the previous literature results are fully reproducible. These findings further reinforce the need to study astrophysical or cosmological explanations for the non-random distribution.","sentences":["The distribution of the spin directions of galaxies has been a question in the past decade, with numerous Earth-based and space-based experiments showing that the distribution is not necessarily random.","These experiments were based on different statistical methods, one of them was a simple and empirically verified open source $\\chi^2$ method.","Patel & Desmond (2024) proposed that previous experiments showing non-random distribution are flawed since they assume Gaussian distribution.","To address that, they apply a new complex ad-hoc statistical method to several datasets, none of them except for one were used in the past to claim for a dipole axis.","The new method showed that all datasets except for one exhibit isotropy.","This paper discusses the soundness of the contention that Gaussian distribution cannot be assumed for galaxy spin directions.","More importantly, simple empirical analyses show that the new statistical method is not fully responsive to asymmetry in the distribution of galaxy spin directions, and does not identify non-random distribution even in situations where a dipole axis clearly exists in the data, or when an artificial bias is added to the data to create an extremely non-random dataset.","Results using Monte Carlo simulation show substantial differences between the results of the simulation and the results of the new statistical method.","Code and data to reproduce the experiments are available, and released in a manner that is easily reproducible.","Possible reasons leading to the results are also discussed.","The claims that the actual results are different from the results reported in the papers are examined in an open and transparent manner.","These claims are found to be inaccurate, as the previous literature results are fully reproducible.","These findings further reinforce the need to study astrophysical or cosmological explanations for the non-random distribution."],"url":"http://arxiv.org/abs/2404.13864v1","category":"astro-ph.CO"}
{"created":"2024-04-22 04:18:38","title":"Distributional Black-Box Model Inversion Attack with Multi-Agent Reinforcement Learning","abstract":"A Model Inversion (MI) attack based on Generative Adversarial Networks (GAN) aims to recover the private training data from complex deep learning models by searching codes in the latent space. However, they merely search a deterministic latent space such that the found latent code is usually suboptimal. In addition, the existing distributional MI schemes assume that an attacker can access the structures and parameters of the target model, which is not always viable in practice. To overcome the above shortcomings, this paper proposes a novel Distributional Black-Box Model Inversion (DBB-MI) attack by constructing the probabilistic latent space for searching the target privacy data. Specifically, DBB-MI does not need the target model parameters or specialized GAN training. Instead, it finds the latent probability distribution by combining the output of the target model with multi-agent reinforcement learning techniques. Then, it randomly chooses latent codes from the latent probability distribution for recovering the private data. As the latent probability distribution closely aligns with the target privacy data in latent space, the recovered data will leak the privacy of training samples of the target model significantly. Abundant experiments conducted on diverse datasets and networks show that the present DBB-MI has better performance than state-of-the-art in attack accuracy, K-nearest neighbor feature distance, and Peak Signal-to-Noise Ratio.","sentences":["A Model Inversion (MI) attack based on Generative Adversarial Networks (GAN) aims to recover the private training data from complex deep learning models by searching codes in the latent space.","However, they merely search a deterministic latent space such that the found latent code is usually suboptimal.","In addition, the existing distributional MI schemes assume that an attacker can access the structures and parameters of the target model, which is not always viable in practice.","To overcome the above shortcomings, this paper proposes a novel Distributional Black-Box Model Inversion (DBB-MI) attack by constructing the probabilistic latent space for searching the target privacy data.","Specifically, DBB-MI does not need the target model parameters or specialized GAN training.","Instead, it finds the latent probability distribution by combining the output of the target model with multi-agent reinforcement learning techniques.","Then, it randomly chooses latent codes from the latent probability distribution for recovering the private data.","As the latent probability distribution closely aligns with the target privacy data in latent space, the recovered data will leak the privacy of training samples of the target model significantly.","Abundant experiments conducted on diverse datasets and networks show that the present DBB-MI has better performance than state-of-the-art in attack accuracy, K-nearest neighbor feature distance, and Peak Signal-to-Noise Ratio."],"url":"http://arxiv.org/abs/2404.13860v1","category":"cs.LG"}
{"created":"2024-04-22 04:14:14","title":"Machine Learning Prediction Models for Solid Electrolytes based on Lattice Dynamics Properties","abstract":"Recently, machine-learning approaches have accelerated computational materials design and the search for advanced solid electrolytes. However, the predictors are currently limited to static structural parameters, which may not fully account for the dynamic nature of ionic transport. In this study, we meticulously curated features considering dynamic properties and developed machine-learning models to predict the ionic conductivity of solid electrolytes. We compiled 14 phonon-related descriptors from first-principles phonon calculations along with 16 descriptors related to structure and electronic properties. Our logistic regression classifiers exhibit an accuracy of 93 %, while the random forest regression model yields a root mean square error of 1.179 S/cm and $R^2$ of 0.710. Notably, phonon-related features are essential for estimating the ionic conductivity in both models. Furthermore, we applied our prediction model to screen 264 Li-containing materials and identified 11 promising candidates as potential superionic conductors.","sentences":["Recently, machine-learning approaches have accelerated computational materials design and the search for advanced solid electrolytes.","However, the predictors are currently limited to static structural parameters, which may not fully account for the dynamic nature of ionic transport.","In this study, we meticulously curated features considering dynamic properties and developed machine-learning models to predict the ionic conductivity of solid electrolytes.","We compiled 14 phonon-related descriptors from first-principles phonon calculations along with 16 descriptors related to structure and electronic properties.","Our logistic regression classifiers exhibit an accuracy of 93 %, while the random forest regression model yields a root mean square error of 1.179 S/cm and $R^2$ of 0.710.","Notably, phonon-related features are essential for estimating the ionic conductivity in both models.","Furthermore, we applied our prediction model to screen 264 Li-containing materials and identified 11 promising candidates as potential superionic conductors."],"url":"http://arxiv.org/abs/2404.13858v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 02:34:50","title":"C2F-SemiCD: A Coarse-to-Fine Semi-Supervised Change Detection Method Based on Consistency Regularization in High-Resolution Remote Sensing Images","abstract":"A high-precision feature extraction model is crucial for change detection (CD). In the past, many deep learning-based supervised CD methods learned to recognize change feature patterns from a large number of labelled bi-temporal images, whereas labelling bi-temporal remote sensing images is very expensive and often time-consuming; therefore, we propose a coarse-to-fine semi-supervised CD method based on consistency regularization (C2F-SemiCD), which includes a coarse-to-fine CD network with a multiscale attention mechanism (C2FNet) and a semi-supervised update method. Among them, the C2FNet network gradually completes the extraction of change features from coarse-grained to fine-grained through multiscale feature fusion, channel attention mechanism, spatial attention mechanism, global context module, feature refine module, initial aggregation module, and final aggregation module. The semi-supervised update method uses the mean teacher method. The parameters of the student model are updated to the parameters of the teacher Model by using the exponential moving average (EMA) method. Through extensive experiments on three datasets and meticulous ablation studies, including crossover experiments across datasets, we verify the significant effectiveness and efficiency of the proposed C2F-SemiCD method. The code will be open at: https://github.com/ChengxiHAN/C2F-SemiCDand-C2FNet.","sentences":["A high-precision feature extraction model is crucial for change detection (CD).","In the past, many deep learning-based supervised CD methods learned to recognize change feature patterns from a large number of labelled bi-temporal images, whereas labelling bi-temporal remote sensing images is very expensive and often time-consuming; therefore, we propose a coarse-to-fine semi-supervised CD method based on consistency regularization (C2F-SemiCD), which includes a coarse-to-fine CD network with a multiscale attention mechanism (C2FNet) and a semi-supervised update method.","Among them, the C2FNet network gradually completes the extraction of change features from coarse-grained to fine-grained through multiscale feature fusion, channel attention mechanism, spatial attention mechanism, global context module, feature refine module, initial aggregation module, and final aggregation module.","The semi-supervised update method uses the mean teacher method.","The parameters of the student model are updated to the parameters of the teacher Model by using the exponential moving average (EMA) method.","Through extensive experiments on three datasets and meticulous ablation studies, including crossover experiments across datasets, we verify the significant effectiveness and efficiency of the proposed C2F-SemiCD method.","The code will be open at: https://github.com/ChengxiHAN/C2F-SemiCDand-C2FNet."],"url":"http://arxiv.org/abs/2404.13838v1","category":"cs.CV"}
{"created":"2024-04-22 02:06:35","title":"Data-Driven Performance Guarantees for Classical and Learned Optimizers","abstract":"We introduce a data-driven approach to analyze the performance of continuous optimization algorithms using generalization guarantees from statistical learning theory. We study classical and learned optimizers to solve families of parametric optimization problems. We build generalization guarantees for classical optimizers, using a sample convergence bound, and for learned optimizers, using the Probably Approximately Correct (PAC)-Bayes framework. To train learned optimizers, we use a gradient-based algorithm to directly minimize the PAC-Bayes upper bound. Numerical experiments in signal processing, control, and meta-learning showcase the ability of our framework to provide strong generalization guarantees for both classical and learned optimizers given a fixed budget of iterations. For classical optimizers, our bounds are much tighter than those that worst-case guarantees provide. For learned optimizers, our bounds outperform the empirical outcomes observed in their non-learned counterparts.","sentences":["We introduce a data-driven approach to analyze the performance of continuous optimization algorithms using generalization guarantees from statistical learning theory.","We study classical and learned optimizers to solve families of parametric optimization problems.","We build generalization guarantees for classical optimizers, using a sample convergence bound, and for learned optimizers, using the Probably Approximately Correct (PAC)-Bayes framework.","To train learned optimizers, we use a gradient-based algorithm to directly minimize the PAC-Bayes upper bound.","Numerical experiments in signal processing, control, and meta-learning showcase the ability of our framework to provide strong generalization guarantees for both classical and learned optimizers given a fixed budget of iterations.","For classical optimizers, our bounds are much tighter than those that worst-case guarantees provide.","For learned optimizers, our bounds outperform the empirical outcomes observed in their non-learned counterparts."],"url":"http://arxiv.org/abs/2404.13831v1","category":"math.OC"}
{"created":"2024-04-22 01:28:35","title":"Improving Group Robustness on Spurious Correlation Requires Preciser Group Inference","abstract":"Standard empirical risk minimization (ERM) models may prioritize learning spurious correlations between spurious features and true labels, leading to poor accuracy on groups where these correlations do not hold. Mitigating this issue often requires expensive spurious attribute (group) labels or relies on trained ERM models to infer group labels when group information is unavailable. However, the significant performance gap in worst-group accuracy between using pseudo group labels and using oracle group labels inspires us to consider further improving group robustness through preciser group inference. Therefore, we propose GIC, a novel method that accurately infers group labels, resulting in improved worst-group performance. GIC trains a spurious attribute classifier based on two key properties of spurious correlations: (1) high correlation between spurious attributes and true labels, and (2) variability in this correlation between datasets with different group distributions. Empirical studies on multiple datasets demonstrate the effectiveness of GIC in inferring group labels, and combining GIC with various downstream invariant learning methods improves worst-group accuracy, showcasing its powerful flexibility. Additionally, through analyzing the misclassifications in GIC, we identify an interesting phenomenon called semantic consistency, which may contribute to better decoupling the association between spurious attributes and labels, thereby mitigating spurious correlation.","sentences":["Standard empirical risk minimization (ERM) models may prioritize learning spurious correlations between spurious features and true labels, leading to poor accuracy on groups where these correlations do not hold.","Mitigating this issue often requires expensive spurious attribute (group) labels or relies on trained ERM models to infer group labels when group information is unavailable.","However, the significant performance gap in worst-group accuracy between using pseudo group labels and using oracle group labels inspires us to consider further improving group robustness through preciser group inference.","Therefore, we propose GIC, a novel method that accurately infers group labels, resulting in improved worst-group performance.","GIC trains a spurious attribute classifier based on two key properties of spurious correlations: (1) high correlation between spurious attributes and true labels, and (2) variability in this correlation between datasets with different group distributions.","Empirical studies on multiple datasets demonstrate the effectiveness of GIC in inferring group labels, and combining GIC with various downstream invariant learning methods improves worst-group accuracy, showcasing its powerful flexibility.","Additionally, through analyzing the misclassifications in GIC, we identify an interesting phenomenon called semantic consistency, which may contribute to better decoupling the association between spurious attributes and labels, thereby mitigating spurious correlation."],"url":"http://arxiv.org/abs/2404.13815v1","category":"cs.LG"}
{"created":"2024-04-21 21:45:23","title":"Soar: Design and Deployment of A Smart Roadside Infrastructure System for Autonomous Driving","abstract":"Recently,smart roadside infrastructure (SRI) has demonstrated the potential of achieving fully autonomous driving systems. To explore the potential of infrastructure-assisted autonomous driving, this paper presents the design and deployment of Soar, the first end-to-end SRI system specifically designed to support autonomous driving systems. Soar consists of both software and hardware components carefully designed to overcome various system and physical challenges. Soar can leverage the existing operational infrastructure like street lampposts for a lower barrier of adoption. Soar adopts a new communication architecture that comprises a bi-directional multi-hop I2I network and a downlink I2V broadcast service, which are designed based on off-the-shelf 802.11ac interfaces in an integrated manner. Soar also features a hierarchical DL task management framework to achieve desirable load balancing among nodes and enable them to collaborate efficiently to run multiple data-intensive autonomous driving applications. We deployed a total of 18 Soar nodes on existing lampposts on campus, which have been operational for over two years. Our real-world evaluation shows that Soar can support a diverse set of autonomous driving applications and achieve desirable real-time performance and high communication reliability. Our findings and experiences in this work offer key insights into the development and deployment of next-generation smart roadside infrastructure and autonomous driving systems.","sentences":["Recently,smart roadside infrastructure (SRI) has demonstrated the potential of achieving fully autonomous driving systems.","To explore the potential of infrastructure-assisted autonomous driving, this paper presents the design and deployment of Soar, the first end-to-end SRI system specifically designed to support autonomous driving systems.","Soar consists of both software and hardware components carefully designed to overcome various system and physical challenges.","Soar can leverage the existing operational infrastructure like street lampposts for a lower barrier of adoption.","Soar adopts a new communication architecture that comprises a bi-directional multi-hop I2I network and a downlink I2V broadcast service, which are designed based on off-the-shelf 802.11ac interfaces in an integrated manner.","Soar also features a hierarchical DL task management framework to achieve desirable load balancing among nodes and enable them to collaborate efficiently to run multiple data-intensive autonomous driving applications.","We deployed a total of 18 Soar nodes on existing lampposts on campus, which have been operational for over two years.","Our real-world evaluation shows that Soar can support a diverse set of autonomous driving applications and achieve desirable real-time performance and high communication reliability.","Our findings and experiences in this work offer key insights into the development and deployment of next-generation smart roadside infrastructure and autonomous driving systems."],"url":"http://arxiv.org/abs/2404.13786v1","category":"eess.SY"}
{"created":"2024-04-21 21:19:36","title":"Automated Text Mining of Experimental Methodologies from Biomedical Literature","abstract":"Biomedical literature is a rapidly expanding field of science and technology. Classification of biomedical texts is an essential part of biomedicine research, especially in the field of biology. This work proposes the fine-tuned DistilBERT, a methodology-specific, pre-trained generative classification language model for mining biomedicine texts. The model has proven its effectiveness in linguistic understanding capabilities and has reduced the size of BERT models by 40\\% but by 60\\% faster. The main objective of this project is to improve the model and assess the performance of the model compared to the non-fine-tuned model. We used DistilBert as a support model and pre-trained on a corpus of 32,000 abstracts and complete text articles; our results were impressive and surpassed those of traditional literature classification methods by using RNN or LSTM. Our aim is to integrate this highly specialised and specific model into different research industries.","sentences":["Biomedical literature is a rapidly expanding field of science and technology.","Classification of biomedical texts is an essential part of biomedicine research, especially in the field of biology.","This work proposes the fine-tuned DistilBERT, a methodology-specific, pre-trained generative classification language model for mining biomedicine texts.","The model has proven its effectiveness in linguistic understanding capabilities and has reduced the size of BERT models by 40\\% but by 60\\% faster.","The main objective of this project is to improve the model and assess the performance of the model compared to the non-fine-tuned model.","We used DistilBert as a support model and pre-trained on a corpus of 32,000 abstracts and complete text articles; our results were impressive and surpassed those of traditional literature classification methods by using RNN or LSTM.","Our aim is to integrate this highly specialised and specific model into different research industries."],"url":"http://arxiv.org/abs/2404.13779v1","category":"cs.CL"}
{"created":"2024-04-21 19:12:22","title":"Efficient Digital Twin Data Processing for Low-Latency Multicast Short Video Streaming","abstract":"In this paper, we propose a novel efficient digital twin (DT) data processing scheme to reduce service latency for multicast short video streaming. Particularly, DT is constructed to emulate and analyze user status for multicast group update and swipe feature abstraction. Then, a precise measurement model of DT data processing is developed to characterize the relationship among DT model size, user dynamics, and user clustering accuracy. A service latency model, consisting of DT data processing delay, video transcoding delay, and multicast transmission delay, is constructed by incorporating the impact of user clustering accuracy. Finally, a joint optimization problem of DT model size selection and bandwidth allocation is formulated to minimize the service latency. To efficiently solve this problem, a diffusion-based resource management algorithm is proposed, which utilizes the denoising technique to improve the action-generation process in the deep reinforcement learning algorithm. Simulation results based on the real-world dataset demonstrate that the proposed DT data processing scheme outperforms benchmark schemes in terms of service latency.","sentences":["In this paper, we propose a novel efficient digital twin (DT) data processing scheme to reduce service latency for multicast short video streaming.","Particularly, DT is constructed to emulate and analyze user status for multicast group update and swipe feature abstraction.","Then, a precise measurement model of DT data processing is developed to characterize the relationship among DT model size, user dynamics, and user clustering accuracy.","A service latency model, consisting of DT data processing delay, video transcoding delay, and multicast transmission delay, is constructed by incorporating the impact of user clustering accuracy.","Finally, a joint optimization problem of DT model size selection and bandwidth allocation is formulated to minimize the service latency.","To efficiently solve this problem, a diffusion-based resource management algorithm is proposed, which utilizes the denoising technique to improve the action-generation process in the deep reinforcement learning algorithm.","Simulation results based on the real-world dataset demonstrate that the proposed DT data processing scheme outperforms benchmark schemes in terms of service latency."],"url":"http://arxiv.org/abs/2404.13749v1","category":"cs.NI"}
{"created":"2024-04-21 19:05:02","title":"Empirical stability criteria for 3D hierarchical triple systems I: Circumbinary planets","abstract":"In this work we revisit the problem of the dynamical stability of hierarchical triple systems with applications to circumbinary planetary orbits. We carry out more than 3 10^8 numerical simulations of planets between the size of Mercury and the lower fusion boundary (13 Jupiter masses) which revolve around the center of mass of a stellar binary over long timescales. For the first time, three dimensional and eccentric planetary orbits are considered. We explore systems with a variety of binary and planetary mass ratios, binary and planetary eccentricities from 0 to 0.9 and orbital mutual inclinations ranging from 0 to 180 degrees. The simulation time is set to 10^6 planetary orbital periods. We classify the results of those long term numerical integrations into three categories: stable, unstable and mixed. We provide empirical expressions in the form of multidimensional, parameterized fits for the two borders that separate the three dynamical domains . In addition, we train a machine learning model on our data set in order to have an alternative tool of predicting the stability of circumbinary planets. Both the empirical fits and the machine learning model are tested against randomly generated circumbinary systems with very good results regarding the predictions of orbital stability. The empirical formulae are also applied to the Kepler and TESS circumbinary systems, confirming the stability of the planets in these systems. Finally, we present a REST API with a web based application for convenient access of our simulation data set.","sentences":["In this work we revisit the problem of the dynamical stability of hierarchical triple systems with applications to circumbinary planetary orbits.","We carry out more than 3 10^8 numerical simulations of planets between the size of Mercury and the lower fusion boundary (13 Jupiter masses) which revolve around the center of mass of a stellar binary over long timescales.","For the first time, three dimensional and eccentric planetary orbits are considered.","We explore systems with a variety of binary and planetary mass ratios, binary and planetary eccentricities from 0 to 0.9 and orbital mutual inclinations ranging from 0 to 180 degrees.","The simulation time is set to 10^6 planetary orbital periods.","We classify the results of those long term numerical integrations into three categories: stable, unstable and mixed.","We provide empirical expressions in the form of multidimensional, parameterized fits for the two borders that separate the three dynamical domains .","In addition, we train a machine learning model on our data set in order to have an alternative tool of predicting the stability of circumbinary planets.","Both the empirical fits and the machine learning model are tested against randomly generated circumbinary systems with very good results regarding the predictions of orbital stability.","The empirical formulae are also applied to the Kepler and TESS circumbinary systems, confirming the stability of the planets in these systems.","Finally, we present a REST API with a web based application for convenient access of our simulation data set."],"url":"http://arxiv.org/abs/2404.13746v1","category":"astro-ph.EP"}
{"created":"2024-04-21 17:04:53","title":"TF2AIF: Facilitating development and deployment of accelerated AI models on the cloud-edge continuum","abstract":"The B5G/6G evolution relies on connect-compute technologies and highly heterogeneous clusters with HW accelerators, which require specialized coding to be efficiently utilized. The current paper proposes a custom tool for generating multiple SW versions of a certain AI function input in high-level language, e.g., Python TensorFlow, while targeting multiple diverse HW+SW platforms. TF2AIF builds upon disparate tool-flows to create a plethora of relative containers and enable the system orchestrator to deploy the requested function on any peculiar node in the cloud-edge continuum, i.e., to leverage the performance/energy benefits of the underlying HW upon any circumstances. TF2AIF fills an identified gap in today's ecosystem and facilitates research on resource management or automated operations, by demanding minimal time or expertise from users.","sentences":["The B5G/6G evolution relies on connect-compute technologies and highly heterogeneous clusters with HW accelerators, which require specialized coding to be efficiently utilized.","The current paper proposes a custom tool for generating multiple SW versions of a certain AI function input in high-level language, e.g., Python TensorFlow, while targeting multiple diverse HW+SW platforms.","TF2AIF builds upon disparate tool-flows to create a plethora of relative containers and enable the system orchestrator to deploy the requested function on any peculiar node in the cloud-edge continuum, i.e., to leverage the performance/energy benefits of the underlying HW upon any circumstances.","TF2AIF fills an identified gap in today's ecosystem and facilitates research on resource management or automated operations, by demanding minimal time or expertise from users."],"url":"http://arxiv.org/abs/2404.13715v1","category":"cs.LG"}
{"created":"2024-04-21 16:05:38","title":"Semantic-Rearrangement-Based Multi-Level Alignment for Domain Generalized Segmentation","abstract":"Domain generalized semantic segmentation is an essential computer vision task, for which models only leverage source data to learn the capability of generalized semantic segmentation towards the unseen target domains. Previous works typically address this challenge by global style randomization or feature regularization. In this paper, we argue that given the observation that different local semantic regions perform different visual characteristics from the source domain to the target domain, methods focusing on global operations are hard to capture such regional discrepancies, thus failing to construct domain-invariant representations with the consistency from local to global level. Therefore, we propose the Semantic-Rearrangement-based Multi-Level Alignment (SRMA) to overcome this problem. SRMA first incorporates a Semantic Rearrangement Module (SRM), which conducts semantic region randomization to enhance the diversity of the source domain sufficiently. A Multi-Level Alignment module (MLA) is subsequently proposed with the help of such diversity to establish the global-regional-local consistent domain-invariant representations. By aligning features across randomized samples with domain-neutral knowledge at multiple levels, SRMA provides a more robust way to handle the source-target domain gap. Extensive experiments demonstrate the superiority of SRMA over the current state-of-the-art works on various benchmarks.","sentences":["Domain generalized semantic segmentation is an essential computer vision task, for which models only leverage source data to learn the capability of generalized semantic segmentation towards the unseen target domains.","Previous works typically address this challenge by global style randomization or feature regularization.","In this paper, we argue that given the observation that different local semantic regions perform different visual characteristics from the source domain to the target domain, methods focusing on global operations are hard to capture such regional discrepancies, thus failing to construct domain-invariant representations with the consistency from local to global level.","Therefore, we propose the Semantic-Rearrangement-based Multi-Level Alignment (SRMA) to overcome this problem.","SRMA first incorporates a Semantic Rearrangement Module (SRM), which conducts semantic region randomization to enhance the diversity of the source domain sufficiently.","A Multi-Level Alignment module (MLA) is subsequently proposed with the help of such diversity to establish the global-regional-local consistent domain-invariant representations.","By aligning features across randomized samples with domain-neutral knowledge at multiple levels, SRMA provides a more robust way to handle the source-target domain gap.","Extensive experiments demonstrate the superiority of SRMA over the current state-of-the-art works on various benchmarks."],"url":"http://arxiv.org/abs/2404.13701v1","category":"cs.CV"}
{"created":"2024-04-21 15:53:06","title":"Resampling-free Particle Filters in High-dimensions","abstract":"State estimation is crucial for the performance and safety of numerous robotic applications. Among the suite of estimation techniques, particle filters have been identified as a powerful solution due to their non-parametric nature. Yet, in high-dimensional state spaces, these filters face challenges such as 'particle deprivation' which hinders accurate representation of the true posterior distribution. This paper introduces a novel resampling-free particle filter designed to mitigate particle deprivation by forgoing the traditional resampling step. This ensures a broader and more diverse particle set, especially vital in high-dimensional scenarios. Theoretically, our proposed filter is shown to offer a near-accurate representation of the desired posterior distribution in high-dimensional contexts. Empirically, the effectiveness of our approach is underscored through a high-dimensional synthetic state estimation task and a 6D pose estimation derived from videos. We posit that as robotic systems evolve with greater degrees of freedom, particle filters tailored for high-dimensional state spaces will be indispensable.","sentences":["State estimation is crucial for the performance and safety of numerous robotic applications.","Among the suite of estimation techniques, particle filters have been identified as a powerful solution due to their non-parametric nature.","Yet, in high-dimensional state spaces, these filters face challenges such as 'particle deprivation' which hinders accurate representation of the true posterior distribution.","This paper introduces a novel resampling-free particle filter designed to mitigate particle deprivation by forgoing the traditional resampling step.","This ensures a broader and more diverse particle set, especially vital in high-dimensional scenarios.","Theoretically, our proposed filter is shown to offer a near-accurate representation of the desired posterior distribution in high-dimensional contexts.","Empirically, the effectiveness of our approach is underscored through a high-dimensional synthetic state estimation task and a 6D pose estimation derived from videos.","We posit that as robotic systems evolve with greater degrees of freedom, particle filters tailored for high-dimensional state spaces will be indispensable."],"url":"http://arxiv.org/abs/2404.13698v1","category":"cs.RO"}
{"created":"2024-04-21 15:49:12","title":"Solute segregation in polycrystalline aluminum from hybrid Monte Carlo and molecular dynamics simulations with a unified neuroevolution potential","abstract":"One of the most effective methods to enhance the strength of aluminum alloys involves modifying grain boundaries (GBs) through solute segregation. However, the fundamental mechanisms of solute segregation and their impacts on material properties remain elusive. In this study, we implemented highly efficient hybrid Monte Carlo and molecular dynamics (MCMD) algorithms in the graphics process units molecular dynamics (GPUMD) package. Using this efficient MCMD approach combined with a general-purpose machine-learning-based neuroevolution potential (NEP) for 16 elemental metals and their alloys, we simulated the segregation of 15 solutes in polycrystalline Al. Our results elucidate the segregation behavior and trends of 15 solutes in polycrystalline Al. Additionally, we investigated the impact of solutes on the strength of polycrystalline Al. The mechanisms underlying solute strengthening and embrittlement were analyzed at the atomistic level, revealing the importance of GB cohesion, as well as the nucleation and movement of Shockley dislocations, in determining the material's strength. We anticipate that our developed methods, along with our insights into solute segregation behavior in polycrystalline Al, will be valuable for the design of Al alloys and other multi-component materials, including medium-entropy materials, high-entropy materials, and complex concentrated alloys.","sentences":["One of the most effective methods to enhance the strength of aluminum alloys involves modifying grain boundaries (GBs) through solute segregation.","However, the fundamental mechanisms of solute segregation and their impacts on material properties remain elusive.","In this study, we implemented highly efficient hybrid Monte Carlo and molecular dynamics (MCMD) algorithms in the graphics process units molecular dynamics (GPUMD) package.","Using this efficient MCMD approach combined with a general-purpose machine-learning-based neuroevolution potential (NEP) for 16 elemental metals and their alloys, we simulated the segregation of 15 solutes in polycrystalline Al.","Our results elucidate the segregation behavior and trends of 15 solutes in polycrystalline Al.","Additionally, we investigated the impact of solutes on the strength of polycrystalline Al.","The mechanisms underlying solute strengthening and embrittlement were analyzed at the atomistic level, revealing the importance of GB cohesion, as well as the nucleation and movement of Shockley dislocations, in determining the material's strength.","We anticipate that our developed methods, along with our insights into solute segregation behavior in polycrystalline Al, will be valuable for the design of Al alloys and other multi-component materials, including medium-entropy materials, high-entropy materials, and complex concentrated alloys."],"url":"http://arxiv.org/abs/2404.13694v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-21 15:33:17","title":"Detecting Compromised IoT Devices Using Autoencoders with Sequential Hypothesis Testing","abstract":"IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.","sentences":["IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks.","Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices.","However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices.","In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices.","Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem.","CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly.","Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average."],"url":"http://arxiv.org/abs/2404.13690v1","category":"cs.CR"}
{"created":"2024-04-21 15:16:05","title":"Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis","abstract":"Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs). Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation. However, these approaches suffer from severe performance degradation or domain shifts. To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression. Firstly, we introduce Trajectory Segmented Consistency Distillation to progressively perform consistent distillation within pre-defined time-step segments, which facilitates the preservation of the original ODE trajectory from a higher-order perspective. Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process. Thirdly, we integrate score distillation to further improve the low-step generation capability of the model and offer the first attempt to leverage a unified LoRA to support the inference process at all steps. Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5. For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and +0.51 in Aes Score in the 1-step inference.","sentences":["Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs).","Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation.","However, these approaches suffer from severe performance degradation or domain shifts.","To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression.","Firstly, we introduce Trajectory Segmented Consistency Distillation to progressively perform consistent distillation within pre-defined time-step segments, which facilitates the preservation of the original ODE trajectory from a higher-order perspective.","Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process.","Thirdly, we integrate score distillation to further improve the low-step generation capability of the model and offer the first attempt to leverage a unified LoRA to support the inference process at all steps.","Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5.","For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and +0.51 in Aes Score in the 1-step inference."],"url":"http://arxiv.org/abs/2404.13686v1","category":"cs.CV"}
{"created":"2024-04-21 14:53:33","title":"Reproducible data science over data lakes: replayable data pipelines with Bauplan and Nessie","abstract":"As the Lakehouse architecture becomes more widespread, ensuring the reproducibility of data workloads over data lakes emerges as a crucial concern for data engineers. However, achieving reproducibility remains challenging. The size of data pipelines contributes to slow testing and iterations, while the intertwining of business logic and data management complicates debugging and increases error susceptibility. In this paper, we highlight recent advancements made at Bauplan in addressing this challenge. We introduce a system designed to decouple compute from data management, by leveraging a cloud runtime alongside Nessie, an open-source catalog with Git semantics. Demonstrating the system's capabilities, we showcase its ability to offer time-travel and branching semantics on top of object storage, and offer full pipeline reproducibility with a few CLI commands.","sentences":["As the Lakehouse architecture becomes more widespread, ensuring the reproducibility of data workloads over data lakes emerges as a crucial concern for data engineers.","However, achieving reproducibility remains challenging.","The size of data pipelines contributes to slow testing and iterations, while the intertwining of business logic and data management complicates debugging and increases error susceptibility.","In this paper, we highlight recent advancements made at Bauplan in addressing this challenge.","We introduce a system designed to decouple compute from data management, by leveraging a cloud runtime alongside Nessie, an open-source catalog with Git semantics.","Demonstrating the system's capabilities, we showcase its ability to offer time-travel and branching semantics on top of object storage, and offer full pipeline reproducibility with a few CLI commands."],"url":"http://arxiv.org/abs/2404.13682v1","category":"cs.DB"}
{"created":"2024-04-21 14:42:10","title":"GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal","abstract":"This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting. The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives. We introduce a robust framework specifically designed to overcome these obstacles. The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture. Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation. Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas. This innovative approach significantly refines the texture coherence within the final radiance field. Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds.","sentences":["This paper tackles the intricate challenge of object removal to update the radiance field using the 3D Gaussian Splatting.","The main challenges of this task lie in the preservation of geometric consistency and the maintenance of texture coherence in the presence of the substantial discrete nature of Gaussian primitives.","We introduce a robust framework specifically designed to overcome these obstacles.","The key insight of our approach is the enhancement of information exchange among visible and invisible areas, facilitating content restoration in terms of both geometry and texture.","Our methodology begins with optimizing the positioning of Gaussian primitives to improve geometric consistency across both removed and visible areas, guided by an online registration process informed by monocular depth estimation.","Following this, we employ a novel feature propagation mechanism to bolster texture coherence, leveraging a cross-attention design that bridges sampling Gaussians from both uncertain and certain areas.","This innovative approach significantly refines the texture coherence within the final radiance field.","Extensive experiments validate that our method not only elevates the quality of novel view synthesis for scenes undergoing object removal but also showcases notable efficiency gains in training and rendering speeds."],"url":"http://arxiv.org/abs/2404.13679v1","category":"cs.CV"}
{"created":"2024-04-21 13:09:30","title":"Multi-AUV Cooperative Underwater Multi-Target Tracking Based on Dynamic-Switching-enabled Multi-Agent Reinforcement Learning","abstract":"With the rapid development of underwater communication, sensing, automation, robot technologies, autonomous underwater vehicle (AUV) swarms are gradually becoming popular and have been widely promoted in ocean exploration and underwater tracking or surveillance, etc. However, the complex underwater environment poses significant challenges for AUV swarm-based accurate tracking for the underwater moving targets. In this paper, we aim at proposing a multi-AUV cooperative underwater multi-target tracking algorithm especially when the real underwater factors are taken into account.We first give normally modelling approach for the underwater sonar-based detection and the ocean current interference on the target tracking process.Then, we regard the AUV swarm as a underwater ad-hoc network and propose a novel Multi-Agent Reinforcement Learning (MARL) architecture towards the AUV swarm based on Software-Defined Networking (SDN).It enhances the flexibility and scalability of the AUV swarm through centralized management and distributed operations.Based on the proposed MARL architecture, we propose the \"dynamic-attention switching\" and \"dynamic-resampling switching\" mechanisms, to enhance the efficiency and accuracy of AUV swarm cooperation during task execution.Finally, based on a proposed AUV classification method, we propose an efficient cooperative tracking algorithm called ASMA.Evaluation results demonstrate that our proposed tracking algorithm can perform precise underwater multi-target tracking, comparing with many of recent research products in terms of convergence speed and tracking accuracy.","sentences":["With the rapid development of underwater communication, sensing, automation, robot technologies, autonomous underwater vehicle (AUV) swarms are gradually becoming popular and have been widely promoted in ocean exploration and underwater tracking or surveillance, etc.","However, the complex underwater environment poses significant challenges for AUV swarm-based accurate tracking for the underwater moving targets.","In this paper, we aim at proposing a multi-AUV cooperative underwater multi-target tracking algorithm especially when the real underwater factors are taken into account.","We first give normally modelling approach for the underwater sonar-based detection and the ocean current interference on the target tracking process.","Then, we regard the AUV swarm as a underwater ad-hoc network and propose a novel Multi-Agent Reinforcement Learning (MARL) architecture towards the AUV swarm based on Software-Defined Networking (SDN).It enhances the flexibility and scalability of the AUV swarm through centralized management and distributed operations.","Based on the proposed MARL architecture, we propose the \"dynamic-attention switching\" and \"dynamic-resampling switching\" mechanisms, to enhance the efficiency and accuracy of AUV swarm cooperation during task execution.","Finally, based on a proposed AUV classification method, we propose an efficient cooperative tracking algorithm called ASMA.Evaluation results demonstrate that our proposed tracking algorithm can perform precise underwater multi-target tracking, comparing with many of recent research products in terms of convergence speed and tracking accuracy."],"url":"http://arxiv.org/abs/2404.13654v2","category":"cs.MA"}
{"created":"2024-04-21 12:50:38","title":"Data-independent Module-aware Pruning for Hierarchical Vision Transformers","abstract":"Hierarchical vision transformers (ViTs) have two advantages over conventional ViTs. First, hierarchical ViTs achieve linear computational complexity with respect to image size by local self-attention. Second, hierarchical ViTs create hierarchical feature maps by merging image patches in deeper layers for dense prediction. However, existing pruning methods ignore the unique properties of hierarchical ViTs and use the magnitude value as the weight importance. This approach leads to two main drawbacks. First, the \"local\" attention weights are compared at a \"global\" level, which may cause some \"locally\" important weights to be pruned due to their relatively small magnitude \"globally\". The second issue with magnitude pruning is that it fails to consider the distinct weight distributions of the network, which are essential for extracting coarse to fine-grained features at various hierarchical levels.   To solve the aforementioned issues, we have developed a Data-independent Module-Aware Pruning method (DIMAP) to compress hierarchical ViTs. To ensure that \"local\" attention weights at different hierarchical levels are compared fairly in terms of their contribution, we treat them as a module and examine their contribution by analyzing their information distortion. Furthermore, we introduce a novel weight metric that is solely based on weights and does not require input images, thereby eliminating the dependence on the patch merging process. Our method validates its usefulness and strengths on Swin Transformers of different sizes on ImageNet-1k classification. Notably, the top-5 accuracy drop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B. When we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve a 0.8% higher relative top-5 accuracy than the original model. Code is available at: https://github.com/he-y/Data-independent-Module-Aware-Pruning","sentences":["Hierarchical vision transformers (ViTs) have two advantages over conventional ViTs.","First, hierarchical ViTs achieve linear computational complexity with respect to image size by local self-attention.","Second, hierarchical ViTs create hierarchical feature maps by merging image patches in deeper layers for dense prediction.","However, existing pruning methods ignore the unique properties of hierarchical ViTs and use the magnitude value as the weight importance.","This approach leads to two main drawbacks.","First, the \"local\" attention weights are compared at a \"global\" level, which may cause some \"locally\" important weights to be pruned due to their relatively small magnitude \"globally\".","The second issue with magnitude pruning is that it fails to consider the distinct weight distributions of the network, which are essential for extracting coarse to fine-grained features at various hierarchical levels.   ","To solve the aforementioned issues, we have developed a Data-independent Module-Aware Pruning method (DIMAP) to compress hierarchical ViTs.","To ensure that \"local\" attention weights at different hierarchical levels are compared fairly in terms of their contribution, we treat them as a module and examine their contribution by analyzing their information distortion.","Furthermore, we introduce a novel weight metric that is solely based on weights and does not require input images, thereby eliminating the dependence on the patch merging process.","Our method validates its usefulness and strengths on Swin Transformers of different sizes on ImageNet-1k classification.","Notably, the top-5 accuracy drop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B. When we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve a 0.8% higher relative top-5 accuracy than the original model.","Code is available at: https://github.com/he-y/Data-independent-Module-Aware-Pruning"],"url":"http://arxiv.org/abs/2404.13648v1","category":"cs.CV"}
{"created":"2024-04-21 11:59:53","title":"Mixture of LoRA Experts","abstract":"LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MoLE.","sentences":["LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques.","Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks.","Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges.","Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes.","On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs.","In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection.","The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively.","Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MoLE."],"url":"http://arxiv.org/abs/2404.13628v1","category":"cs.CL"}
{"created":"2024-04-21 10:41:04","title":"Video sentence grounding with temporally global textual knowledge","abstract":"Temporal sentence grounding involves the retrieval of a video moment with a natural language query. Many existing works directly incorporate the given video and temporally localized query for temporal grounding, overlooking the inherent domain gap between different modalities. In this paper, we utilize pseudo-query features containing extensive temporally global textual knowledge sourced from the same video-query pair, to enhance the bridging of domain gaps and attain a heightened level of similarity between multi-modal features. Specifically, we propose a Pseudo-query Intermediary Network (PIN) to achieve an improved alignment of visual and comprehensive pseudo-query features within the feature space through contrastive learning. Subsequently, we utilize learnable prompts to encapsulate the knowledge of pseudo-queries, propagating them into the textual encoder and multi-modal fusion module, further enhancing the feature alignment between visual and language for better temporal grounding. Extensive experiments conducted on the Charades-STA and ActivityNet-Captions datasets demonstrate the effectiveness of our method.","sentences":["Temporal sentence grounding involves the retrieval of a video moment with a natural language query.","Many existing works directly incorporate the given video and temporally localized query for temporal grounding, overlooking the inherent domain gap between different modalities.","In this paper, we utilize pseudo-query features containing extensive temporally global textual knowledge sourced from the same video-query pair, to enhance the bridging of domain gaps and attain a heightened level of similarity between multi-modal features.","Specifically, we propose a Pseudo-query Intermediary Network (PIN) to achieve an improved alignment of visual and comprehensive pseudo-query features within the feature space through contrastive learning.","Subsequently, we utilize learnable prompts to encapsulate the knowledge of pseudo-queries, propagating them into the textual encoder and multi-modal fusion module, further enhancing the feature alignment between visual and language for better temporal grounding.","Extensive experiments conducted on the Charades-STA and ActivityNet-Captions datasets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2404.13611v1","category":"cs.CV"}
{"created":"2024-04-21 09:42:05","title":"\"A good pun is its own reword\": Can Large Language Models Understand Puns?","abstract":"Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the \"lazy pun generation\" pattern and identify the primary challenges LLMs encounter in understanding puns.","sentences":["Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor.","However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation.","In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding.","In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs.","These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics.","Our findings reveal the \"lazy pun generation\" pattern and identify the primary challenges LLMs encounter in understanding puns."],"url":"http://arxiv.org/abs/2404.13599v1","category":"cs.CL"}
{"created":"2024-04-21 09:15:02","title":"MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning","abstract":"While multi-modal large language models (MLLMs) have shown significant progress on many popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only considered a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 by 3 matrices). To evaluate MLLMs' reasoning abilities comprehensively, we introduce MARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model accuracy is grounded in perception and reasoning, MARVEL complements the general AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with nine representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all models show near-random performance on the AVR question, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance) and even count the panels in the puzzle ( <45%), hindering their ability for abstract reasoning. We release our entire code and dataset.","sentences":["While multi-modal large language models (MLLMs) have shown significant progress on many popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question.","Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix).","However, existing AVR benchmarks only considered a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 by 3 matrices).","To evaluate MLLMs' reasoning abilities comprehensively, we introduce MARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations.","To inspect whether the model accuracy is grounded in perception and reasoning, MARVEL complements the general AVR question with perception questions in a hierarchical evaluation framework.","We conduct comprehensive experiments on MARVEL with nine representative MLLMs in zero-shot and few-shot settings.","Our experiments reveal that all models show near-random performance on the AVR question, with significant performance gaps (40%) compared to humans across all patterns and task configurations.","Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance) and even count the panels in the puzzle ( <45%), hindering their ability for abstract reasoning.","We release our entire code and dataset."],"url":"http://arxiv.org/abs/2404.13591v1","category":"cs.CV"}
{"created":"2024-04-21 08:28:52","title":"I2CANSAY:Inter-Class Analogical Augmentation and Intra-Class Significance Analysis for Non-Exemplar Online Task-Free Continual Learning","abstract":"Online task-free continual learning (OTFCL) is a more challenging variant of continual learning which emphasizes the gradual shift of task boundaries and learns in an online mode. Existing methods rely on a memory buffer composed of old samples to prevent forgetting. However,the use of memory buffers not only raises privacy concerns but also hinders the efficient learning of new samples. To address this problem, we propose a novel framework called I2CANSAY that gets rid of the dependence on memory buffers and efficiently learns the knowledge of new data from one-shot samples. Concretely, our framework comprises two main modules. Firstly, the Inter-Class Analogical Augmentation (ICAN) module generates diverse pseudo-features for old classes based on the inter-class analogy of feature distributions for different new classes, serving as a substitute for the memory buffer. Secondly, the Intra-Class Significance Analysis (ISAY) module analyzes the significance of attributes for each class via its distribution standard deviation, and generates the importance vector as a correction bias for the linear classifier, thereby enhancing the capability of learning from new samples. We run our experiments on four popular image classification datasets: CoRe50, CIFAR-10, CIFAR-100, and CUB-200, our approach outperforms the prior state-of-the-art by a large margin.","sentences":["Online task-free continual learning (OTFCL) is a more challenging variant of continual learning which emphasizes the gradual shift of task boundaries and learns in an online mode.","Existing methods rely on a memory buffer composed of old samples to prevent forgetting.","However,the use of memory buffers not only raises privacy concerns but also hinders the efficient learning of new samples.","To address this problem, we propose a novel framework called I2CANSAY that gets rid of the dependence on memory buffers and efficiently learns the knowledge of new data from one-shot samples.","Concretely, our framework comprises two main modules.","Firstly, the Inter-Class Analogical Augmentation (ICAN) module generates diverse pseudo-features for old classes based on the inter-class analogy of feature distributions for different new classes, serving as a substitute for the memory buffer.","Secondly, the Intra-Class Significance Analysis (ISAY) module analyzes the significance of attributes for each class via its distribution standard deviation, and generates the importance vector as a correction bias for the linear classifier, thereby enhancing the capability of learning from new samples.","We run our experiments on four popular image classification datasets: CoRe50, CIFAR-10, CIFAR-100, and CUB-200, our approach outperforms the prior state-of-the-art by a large margin."],"url":"http://arxiv.org/abs/2404.13576v1","category":"cs.CV"}
{"created":"2024-04-22 17:59:50","title":"Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses","abstract":"In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.","sentences":["In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input.","As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together.","In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world.","To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances.","We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations.","After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human.","Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches."],"url":"http://arxiv.org/abs/2404.14410v1","category":"cs.CV"}
{"created":"2024-04-22 17:37:17","title":"A New Optimization Model for Multiple-Control Toffoli Quantum Circuit Design","abstract":"As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research. This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing. While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently. This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used. Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks. Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees.","sentences":["As quantum technology is advancing, the efficient design of quantum circuits has become an important area of research.","This paper provides an introduction to the MCT quantum circuit design problem for reversible Boolean functions without assuming a prior background in quantum computing.","While this is a well-studied problem, optimization models that minimize the true objective have only been explored recently.","This paper introduces a new optimization model and symmetry-breaking constraints that improve solving time by up to two orders of magnitude compared to earlier work when a Constraint Programming solver is used.","Experiments with up to seven qubits and using up to 15 quantum gates result in several new best-known circuits for well-known benchmarks.","Finally, an extensive comparison with other approaches shows that optimization models may require more time but can provide superior circuits with optimality guarantees."],"url":"http://arxiv.org/abs/2404.14384v1","category":"math.OC"}
{"created":"2024-04-22 16:29:26","title":"Meta-GGAs vs. Hybrid Functionals for Point Defects: The Best of Both Worlds Applied to Layered MnO$_2$, NiO$_2$ and KCoO$_2$","abstract":"Defects in a material can significantly tune its properties and enhance its utility. Hybrid functionals like HSE06 are often used to describe solids with defects. However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study. The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides. We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state. The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0. K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion.","sentences":["Defects in a material can significantly tune its properties and enhance its utility.","Hybrid functionals like HSE06 are often used to describe solids with defects.","However, geometry optimization using hybrid functionals (e.g., HSE06), often used to describe solids with defects, is challenging for a large supercell, as needed for defect study.","The proposed r$^2$SCAN+rVV10+U+U$_d$ method, which is computationally much cheaper and faster than hybrid functionals, can successfully describe defects in materials with the proper choice of U (for the d orbitals of the host atom) and U$_d$ (for those of the defect atom), as shown here for small polarons in layered transition-metal oxides.","We use a literature value of U or U$_d$ appropriate to a given transition-metal ion and its oxidation state.","The materials MnO$_2$ and NiO$_2$, with one K atom intercalated between layers in a supercell, are found to have one localized occupied e$_g$ state on the transition metal ion that takes an electron from the K atom, when the geometry is calculated as above, for standard U values but not for U=U$_d$=0.","K-intercalated KCoO$_2$ is surprisingly different, due to a dramatic change of electronic configuration of the defected Co$^{+2}$ ion."],"url":"http://arxiv.org/abs/2404.14317v1","category":"physics.comp-ph"}
{"created":"2024-04-22 16:00:24","title":"Linear Search for an Escaping Target with Unknown Speed","abstract":"We consider linear search for an escaping target whose speed and initial position are unknown to the searcher. A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line. An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction. We consider two cases, depending on whether $d$ is known or unknown. The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings. This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown. Our results solve an open problem proposed in [Coleman et al., Proc. OPODIS 2022].","sentences":["We consider linear search for an escaping target whose speed and initial position are unknown to the searcher.","A searcher (an autonomous mobile agent) is initially placed at the origin of the real line and can move with maximum speed $1$ in either direction along the line.","An oblivious mobile target that is moving away from the origin with an unknown constant speed $v<1$ is initially placed by an adversary on the infinite line at distance $d$ from the origin in an unknown direction.","We consider two cases, depending on whether $d$ is known or unknown.","The main contribution of this paper is to prove a new lower bound and give algorithms leading to new upper bounds for search in these settings.","This results in an optimal (up to lower order terms in the exponent) competitive ratio in the case where $d$ is known and improved upper and lower bounds for the case where $d$ is unknown.","Our results solve an open problem proposed in [Coleman et al., Proc.","OPODIS 2022]."],"url":"http://arxiv.org/abs/2404.14300v1","category":"cs.DM"}
{"created":"2024-04-22 15:44:33","title":"Comparison of h-BN and graphene layers as grain boundary materials for granular FePt-$\\text{L}1_0$ thin films","abstract":"Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media. While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN. The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C. The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe. However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries. Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media.","sentences":["Granular $\\text{L}1_0$-FePt thin films with small columnar grains are essential for heat-assisted magnetic recording media.","While hexagonal boron nitride(h-BN) has proven effective for promoting columnar FePt grains, we explored multilayer graphene as an alternative grain boundary material leveraging its structural similarity to h-BN.","The FePt granular thin films with carbon-based grain boundary materials(GBMs) were deposited by cosputtering on Si/SiO2 substrates with substrate bias at 650{\\deg}C.","The RF bias and high temperature facilitated formation of interlinked graphene nanoribbons wrapping around FePt grains, yielding 7.5 nm diameter, 8 nm height grains with an order parameter of 0.78 and a perpendicular coercivity of 40 kOe.","However, the formation of graphene nanoribbons could not effectively promote columnar structures, likely due to co-existing amorphous carbon in grain boundaries.","Optimizing deposition to improve graphene grain boundary quality is necessary to realize this 2D material's potential for achieving desirable microstructures for HAMR media."],"url":"http://arxiv.org/abs/2404.14290v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 15:07:52","title":"Hierarchical NMPC for Obstacle Avoidance on Skid-steer Loaders","abstract":"This paper introduces a novel NMPC formulation for real-time obstacle avoidance on heavy equipment by modeling both vehicle and obstacles as convex superellipsoids. The combination of this approach with the separating hyperplane theorem and Optimization Engine (OpEn) allows to achieve efficient obstacle avoidance in autonomous heavy equipment and robotics. We demonstrate the efficacy of the approach through simulated and experimental results, showcasing a skid-steer loader's capability to navigate in obstructed environments.","sentences":["This paper introduces a novel NMPC formulation for real-time obstacle avoidance on heavy equipment by modeling both vehicle and obstacles as convex superellipsoids.","The combination of this approach with the separating hyperplane theorem and Optimization Engine (OpEn) allows to achieve efficient obstacle avoidance in autonomous heavy equipment and robotics.","We demonstrate the efficacy of the approach through simulated and experimental results, showcasing a skid-steer loader's capability to navigate in obstructed environments."],"url":"http://arxiv.org/abs/2404.14257v1","category":"math.OC"}
{"created":"2024-04-22 14:03:46","title":"Optimal Multiparameter Metrology: The Quantum Compass Solution","abstract":"We study optimal quantum sensing of multiple physical parameters using repeated measurements. In this scenario, the Fisher information framework sets the fundamental limits on sensing performance, yet the optimal states and corresponding measurements that attain these limits remain to be discovered. To address this, we extend the Fisher information approach with a second optimality requirement for a sensor to provide unambiguous estimation of unknown parameters. We propose a systematic method integrating Fisher information and Bayesian approaches to quantum metrology to identify the combination of input states and measurements that satisfies both optimality criteria. Specifically, we frame the optimal sensing problem as an optimization of an asymptotic Bayesian cost function that can be efficiently solved numerically and, in many cases, analytically. We refer to the resulting optimal sensor as a `quantum compass' solution, which serves as a direct multiparameter counterpart to the Greenberger-Horne-Zeilinger state-based interferometer, renowned for achieving the Heisenberg limit in single-parameter metrology. We provide exact quantum compass solutions for paradigmatic multiparameter problem of sensing two and three parameters using an SU(2) sensor. Our metrological cost function opens avenues for quantum variational techniques to design low-depth quantum circuits approaching the optimal sensing performance in the many-repetition scenario. We demonstrate this by constructing simple quantum circuits that achieve the Heisenberg limit for vector field and 3D rotations estimation using a limited set of gates available on a trapped-ion platform. Our work introduces and optimizes sensors for a practical notion of optimality, keeping in mind the ultimate goal of quantum sensors to precisely estimate unknown parameters.","sentences":["We study optimal quantum sensing of multiple physical parameters using repeated measurements.","In this scenario, the Fisher information framework sets the fundamental limits on sensing performance, yet the optimal states and corresponding measurements that attain these limits remain to be discovered.","To address this, we extend the Fisher information approach with a second optimality requirement for a sensor to provide unambiguous estimation of unknown parameters.","We propose a systematic method integrating Fisher information and Bayesian approaches to quantum metrology to identify the combination of input states and measurements that satisfies both optimality criteria.","Specifically, we frame the optimal sensing problem as an optimization of an asymptotic Bayesian cost function that can be efficiently solved numerically and, in many cases, analytically.","We refer to the resulting optimal sensor as a `quantum compass' solution, which serves as a direct multiparameter counterpart to the Greenberger-Horne-Zeilinger state-based interferometer, renowned for achieving the Heisenberg limit in single-parameter metrology.","We provide exact quantum compass solutions for paradigmatic multiparameter problem of sensing two and three parameters using an SU(2) sensor.","Our metrological cost function opens avenues for quantum variational techniques to design low-depth quantum circuits approaching the optimal sensing performance in the many-repetition scenario.","We demonstrate this by constructing simple quantum circuits that achieve the Heisenberg limit for vector field and 3D rotations estimation using a limited set of gates available on a trapped-ion platform.","Our work introduces and optimizes sensors for a practical notion of optimality, keeping in mind the ultimate goal of quantum sensors to precisely estimate unknown parameters."],"url":"http://arxiv.org/abs/2404.14194v1","category":"quant-ph"}
{"created":"2024-04-22 13:18:59","title":"Semirandom Planted Clique and the Restricted Isometry Property","abstract":"We give a simple, greedy $O(n^{\\omega+0.5})=O(n^{2.872})$-time algorithm to list-decode planted cliques in a semirandom model introduced in [CSV17] (following [FK01]) that succeeds whenever the size of the planted clique is $k\\geq O(\\sqrt{n} \\log^2 n)$. In the model, the edges touching the vertices in the planted $k$-clique are drawn independently with probability $p=1/2$ while the edges not touching the planted clique are chosen by an adversary in response to the random choices. Our result shows that the computational threshold in the semirandom setting is within a $O(\\log^2 n)$ factor of the information-theoretic one [Ste17] thus resolving an open question of Steinhardt. This threshold also essentially matches the conjectured computational threshold for the well-studied special case of fully random planted clique.   All previous algorithms [CSV17, MMT20, BKS23] in this model are based on rather sophisticated rounding algorithms for entropy-constrained semidefinite programming relaxations and their sum-of-squares strengthenings and the best known guarantee is a $n^{O(1/\\epsilon)}$-time algorithm to list-decode planted cliques of size $k \\geq \\tilde{O}(n^{1/2+\\epsilon})$. In particular, the guarantee trivializes to quasi-polynomial time if the planted clique is of size $O(\\sqrt{n} \\operatorname{polylog} n)$. Our algorithm achieves an almost optimal guarantee with a surprisingly simple greedy algorithm.   The prior state-of-the-art algorithmic result above is based on a reduction to certifying bounds on the size of unbalanced bicliques in random graphs -- closely related to certifying the restricted isometry property (RIP) of certain random matrices and known to be hard in the low-degree polynomial model. Our key idea is a new approach that relies on the truth of -- but not efficient certificates for -- RIP of a new class of matrices built from the input graphs.","sentences":["We give a simple, greedy $O(n^{\\omega+0.5})=O(n^{2.872})$-time algorithm to list-decode planted cliques in a semirandom model introduced in [CSV17] (following [FK01]) that succeeds whenever the size of the planted clique is $k\\geq O(\\sqrt{n} \\log^2","n)$. In the model, the edges touching the vertices in the planted $k$-clique are drawn independently with probability $p=1/2$ while the edges not touching the planted clique are chosen by an adversary in response to the random choices.","Our result shows that the computational threshold in the semirandom setting is within a $O(\\log^2","n)$ factor of the information-theoretic one [Ste17] thus resolving an open question of Steinhardt.","This threshold also essentially matches the conjectured computational threshold for the well-studied special case of fully random planted clique.   ","All previous algorithms [CSV17, MMT20, BKS23] in this model are based on rather sophisticated rounding algorithms for entropy-constrained semidefinite programming relaxations and their sum-of-squares strengthenings and the best known guarantee is a $n^{O(1/\\epsilon)}$-time algorithm to list-decode planted cliques of size $k \\geq \\tilde{O}(n^{1/2+\\epsilon})$.","In particular, the guarantee trivializes to quasi-polynomial time if the planted clique is of size $O(\\sqrt{n} \\operatorname{polylog} n)$. Our algorithm achieves an almost optimal guarantee with a surprisingly simple greedy algorithm.   ","The prior state-of-the-art algorithmic result above is based on a reduction to certifying bounds on the size of unbalanced bicliques in random graphs -- closely related to certifying the restricted isometry property (RIP) of certain random matrices and known to be hard in the low-degree polynomial model.","Our key idea is a new approach that relies on the truth of -- but not efficient certificates for -- RIP of a new class of matrices built from the input graphs."],"url":"http://arxiv.org/abs/2404.14159v1","category":"cs.DS"}
{"created":"2024-04-22 11:54:54","title":"Achieving binary topology optimization solutions via automatic projection parameter increase","abstract":"A method is created to automatically increase the threshold projection parameter in three-field density-based topology optimization to achieve a near binary design. The parameter increase each iteration is based on an exponential growth function, where the growth rate is dynamically changed during optimization by linking it to the change in objective function. This results in a method that does not need to be tuned for specific problems, or optimizers, and the same set of hyper-parameters can be used for a wide range of problems. The effectiveness of the method is demonstrated on several 2D benchmark problems, including linear buckling and geometrically nonlinear problems.","sentences":["A method is created to automatically increase the threshold projection parameter in three-field density-based topology optimization to achieve a near binary design.","The parameter increase each iteration is based on an exponential growth function, where the growth rate is dynamically changed during optimization by linking it to the change in objective function.","This results in a method that does not need to be tuned for specific problems, or optimizers, and the same set of hyper-parameters can be used for a wide range of problems.","The effectiveness of the method is demonstrated on several 2D benchmark problems, including linear buckling and geometrically nonlinear problems."],"url":"http://arxiv.org/abs/2404.14111v1","category":"math.OC"}
{"created":"2024-04-22 10:35:37","title":"Likelihood analysis of the newly observed $f_0(2020)$, $f_0(2330)$ and $f_0(2470)$ in $J/\u03c8\\to \u03b3\u03b7^\\prime\u03b7^\\prime$ as high-lying unflavored scalar mesons","abstract":"Inspired by the newly observed three scalar states $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ by the BESII Collaboration in $J/\\psi\\to \\gamma\\eta^\\prime\\eta^\\prime$, we carried out the study of spectroscopic behavior of these high-lying unflavored scalar mesonic states. In this work, using the Regge trajectory analysis and the quark pair creation model, we discussed the assignments of the $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ associated with the $f_0(2200)$ as the isoscalar high-lying scalar mesonic states and predicted the spectroscopic properties of their high-lying partners. The present study may provide valuable information for the construction of the scalar meson family.","sentences":["Inspired by the newly observed three scalar states $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ by the BESII Collaboration in $J/\\psi\\to \\gamma\\eta^\\prime\\eta^\\prime$, we carried out the study of spectroscopic behavior of these high-lying unflavored scalar mesonic states.","In this work, using the Regge trajectory analysis and the quark pair creation model, we discussed the assignments of the $f_0(2020)$, $f_0(2330)$, and $f_0(2470)$ associated with the $f_0(2200)$ as the isoscalar high-lying scalar mesonic states and predicted the spectroscopic properties of their high-lying partners.","The present study may provide valuable information for the construction of the scalar meson family."],"url":"http://arxiv.org/abs/2404.14075v1","category":"hep-ph"}
{"created":"2024-04-22 10:10:29","title":"Solving Combinatorial Optimization Problems with a Block Encoding Quantum Optimizer","abstract":"In the pursuit of achieving near-term quantum advantage for combinatorial optimization problems, the Quantum Approximate Optimization Algorithm (QAOA) and the Variational Quantum Eigensolver (VQE) are the primary methods of interest, but their practical effectiveness remains uncertain. Therefore, there is a persistent need to develop and evaluate alternative variational quantum algorithms. This study presents an investigation of the Block ENcoding Quantum Optimizer (BENQO), a hybrid quantum solver that uses block encoding to represent the cost function. BENQO is designed to be universally applicable across discrete optimization problems. Beyond Maximum Cut, we evaluate BENQO's performance in the context of the Traveling Salesperson Problem, which is of greater practical relevance. Our findings confirm that BENQO performs significantly better than QAOA and competes with VQE across a variety of performance metrics. We conclude that BENQO is a promising novel hybrid quantum-classical algorithm that should be further investigated and optimized to realize its full potential.","sentences":["In the pursuit of achieving near-term quantum advantage for combinatorial optimization problems, the Quantum Approximate Optimization Algorithm (QAOA) and the Variational Quantum Eigensolver (VQE) are the primary methods of interest, but their practical effectiveness remains uncertain.","Therefore, there is a persistent need to develop and evaluate alternative variational quantum algorithms.","This study presents an investigation of the Block ENcoding Quantum Optimizer (BENQO), a hybrid quantum solver that uses block encoding to represent the cost function.","BENQO is designed to be universally applicable across discrete optimization problems.","Beyond Maximum Cut, we evaluate BENQO's performance in the context of the Traveling Salesperson Problem, which is of greater practical relevance.","Our findings confirm that BENQO performs significantly better than QAOA and competes with VQE across a variety of performance metrics.","We conclude that BENQO is a promising novel hybrid quantum-classical algorithm that should be further investigated and optimized to realize its full potential."],"url":"http://arxiv.org/abs/2404.14054v1","category":"quant-ph"}
{"created":"2024-04-22 10:07:04","title":"Pressure gain combustion for gas turbines: Analysis of a fully coupled engine model","abstract":"The ``Shockless Explosion Combustion\" (SEC) concept for gas turbine combustors, introduced in 2014, approximates constant volume combustion (CVC) by harnessing acoustic confinement of autoigniting gas packets. The resulting pressure waves simultaneously transmit combustion energy to a turbine plenum and facilitate the combustor's recharging against an average pressure gain. Challenges in actualizing an SEC-driven gas turbine include i) the creation of charge stratifications for nearly homogeneous autoignition, ii) protecting the turbo components from combustion-induced pressure fluctuations, iii) providing evidence that efficiency gains comparable to those of CVC over deflagrative combustion can be realized, and iv) designing an effective one-way intake valve. This work addresses challenges i)-iii) utilizing computational engine models incorporating a quasi-one-dimensional combustor, zero- and two-dimensional compressor and turbine plena, and quasi-stationary turbo components. Two SEC operational modes are identified which fire at roughly one and two times the combustors' acoustic frequencies. Results for SEC-driven gas turbines with compressor pressure ratios of 6:1 and 20:1 reveal 1.5-fold mean pressure gains across the combustors. Assuming ideally efficient compressors and turbines, efficiency gains over engines with deflagration-based combustors of 30% and 18% are realized, respectively. With absolute values of 52% and 66%, the obtained efficiencies are close to the theoretical Humphrey cycle efficiencies of 54% and 65% for the mentioned pre-compression ratios. Detailed thermodynamic cycle analyses for individual gas parcels suggest that there is room for further efficiency gains through optimized plenum and combustor designs.","sentences":["The ``Shockless Explosion Combustion\" (SEC) concept for gas turbine combustors, introduced in 2014, approximates constant volume combustion (CVC) by harnessing acoustic confinement of autoigniting gas packets.","The resulting pressure waves simultaneously transmit combustion energy to a turbine plenum and facilitate the combustor's recharging against an average pressure gain.","Challenges in actualizing an SEC-driven gas turbine include i) the creation of charge stratifications for nearly homogeneous autoignition, ii) protecting the turbo components from combustion-induced pressure fluctuations, iii) providing evidence that efficiency gains comparable to those of CVC over deflagrative combustion can be realized, and iv) designing an effective one-way intake valve.","This work addresses challenges i)-iii) utilizing computational engine models incorporating a quasi-one-dimensional combustor, zero- and two-dimensional compressor and turbine plena, and quasi-stationary turbo components.","Two SEC operational modes are identified which fire at roughly one and two times the combustors' acoustic frequencies.","Results for SEC-driven gas turbines with compressor pressure ratios of 6:1 and 20:1 reveal 1.5-fold mean pressure gains across the combustors.","Assuming ideally efficient compressors and turbines, efficiency gains over engines with deflagration-based combustors of 30% and 18% are realized, respectively.","With absolute values of 52% and 66%, the obtained efficiencies are close to the theoretical Humphrey cycle efficiencies of 54% and 65% for the mentioned pre-compression ratios.","Detailed thermodynamic cycle analyses for individual gas parcels suggest that there is room for further efficiency gains through optimized plenum and combustor designs."],"url":"http://arxiv.org/abs/2404.14053v1","category":"physics.flu-dyn"}
{"created":"2024-04-22 06:51:08","title":"Linear Convergence Results for Inertial Type Projection Algorithm for Quasi-Variational Inequalities","abstract":"Many recently proposed gradient projection algorithms with inertial extrapolation step for solving quasi-variational inequalities in Hilbert spaces are proven to be strongly convergent with no linear rate given when the cost operator is strongly monotone and Lipschitz continuous. In this paper, our aim is to design an inertial type gradient projection algorithm for quasi-variational inequalities and obtain its linear rate of convergence. Therefore, our results fill in the gap for linear convergence results for inertial type gradient projection algorithms for quasi variational inequalities in Hilbert spaces. We perform numerical implementations of our proposed algorithm and give numerical comparisons with other related inertial type gradient projection algorithms for quasi variational inequalities in the literature.","sentences":["Many recently proposed gradient projection algorithms with inertial extrapolation step for solving quasi-variational inequalities in Hilbert spaces are proven to be strongly convergent with no linear rate given when the cost operator is strongly monotone and Lipschitz continuous.","In this paper, our aim is to design an inertial type gradient projection algorithm for quasi-variational inequalities and obtain its linear rate of convergence.","Therefore, our results fill in the gap for linear convergence results for inertial type gradient projection algorithms for quasi variational inequalities in Hilbert spaces.","We perform numerical implementations of our proposed algorithm and give numerical comparisons with other related inertial type gradient projection algorithms for quasi variational inequalities in the literature."],"url":"http://arxiv.org/abs/2404.13912v1","category":"math.OC"}
{"created":"2024-04-22 06:36:27","title":"Faster Algorithms for Dual-Failure Replacement Paths","abstract":"Given a simple weighted directed graph $G = (V, E, \\omega)$ on $n$ vertices as well as two designated terminals $s, t\\in V$, our goal is to compute the shortest path from $s$ to $t$ avoiding any pair of presumably failed edges $f_1, f_2\\in E$, which is a natural generalization of the classical replacement path problem which considers single edge failures only.   This dual failure replacement paths problem was recently studied by Vassilevska Williams, Woldeghebriel and Xu [FOCS 2022] who designed a cubic time algorithm for general weighted digraphs which is conditionally optimal; in the same paper, for unweighted graphs where $\\omega \\equiv 1$, the authors presented an algebraic algorithm with runtime $\\tilde{O}(n^{2.9146})$, as well as a conditional lower bound of $n^{8/3-o(1)}$ against combinatorial algorithms. However, it was unknown in their work whether fast matrix multiplication is necessary for a subcubic runtime in unweighted digraphs.   As our primary result, we present the first truly subcubic combinatorial algorithm for dual failure replacement paths in unweighted digraphs. Our runtime is $\\tilde{O}(n^{3-1/18})$. Besides, we also study algebraic algorithms for digraphs with small integer edge weights from $\\{-M, -M+1, \\cdots, M-1, M\\}$. As our secondary result, we obtained a runtime of $\\tilde{O}(Mn^{2.8716})$, which is faster than the previous bound of $\\tilde{O}(M^{2/3}n^{2.9144} + Mn^{2.8716})$ from [Vassilevska Williams, Woldeghebriela and Xu, 2022].","sentences":["Given a simple weighted directed graph $G = (V, E, \\omega)$ on $n$ vertices as well as two designated terminals $s, t\\in V$, our goal is to compute the shortest path from $s$ to $t$ avoiding any pair of presumably failed edges $f_1, f_2\\in E$, which is a natural generalization of the classical replacement path problem which considers single edge failures only.   ","This dual failure replacement paths problem was recently studied by Vassilevska Williams, Woldeghebriel and Xu","[FOCS 2022] who designed a cubic time algorithm for general weighted digraphs which is conditionally optimal; in the same paper, for unweighted graphs where $\\omega \\equiv 1$, the authors presented an algebraic algorithm with runtime $\\tilde{O}(n^{2.9146})$, as well as a conditional lower bound of $n^{8/3-o(1)}$ against combinatorial algorithms.","However, it was unknown in their work whether fast matrix multiplication is necessary for a subcubic runtime in unweighted digraphs.   ","As our primary result, we present the first truly subcubic combinatorial algorithm for dual failure replacement paths in unweighted digraphs.","Our runtime is $\\tilde{O}(n^{3-1/18})$.","Besides, we also study algebraic algorithms for digraphs with small integer edge weights from $\\{-M, -M+1, \\cdots, M-1, M\\}$.","As our secondary result, we obtained a runtime of $\\tilde{O}(Mn^{2.8716})$, which is faster than the previous bound of $\\tilde{O}(M^{2/3}n^{2.9144} + Mn^{2.8716})$ from [Vassilevska Williams, Woldeghebriela and Xu, 2022]."],"url":"http://arxiv.org/abs/2404.13907v1","category":"cs.DS"}
{"created":"2024-04-22 06:25:17","title":"Accelerating Image Generation with Sub-path Linear Approximation Model","abstract":"Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images.","sentences":["Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks.","However, their applications in practical scenarios are hindered by slow inference speed.","Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation.","SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path.","The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors.","An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models.","Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance.","Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images."],"url":"http://arxiv.org/abs/2404.13903v2","category":"cs.CV"}
{"created":"2024-04-22 04:31:56","title":"Stochastic waveform estimation at the fundamental quantum limit","abstract":"Although measuring the deterministic waveform of a weak classical force is a well-studied problem, estimating a random waveform, such as the spectral density of a stochastic signal field, is much less well-understood despite it being a widespread task at the frontier of experimental physics. State-of-the-art precision sensors of random forces must account for the underlying quantum nature of the measurement, but the optimal quantum protocol for interrogating such linear sensors is not known. We derive the fundamental precision limit, the extended channel quantum Cram\\'er-Rao bound, and the optimal protocol that attains it. In the experimentally relevant regime where losses dominate, we prove that non-Gaussian state preparation and measurements are required for optimality. We discuss how this non-Gaussian protocol could improve searches for signatures of quantum gravity, stochastic gravitational waves, and axionic dark matter.","sentences":["Although measuring the deterministic waveform of a weak classical force is a well-studied problem, estimating a random waveform, such as the spectral density of a stochastic signal field, is much less well-understood despite it being a widespread task at the frontier of experimental physics.","State-of-the-art precision sensors of random forces must account for the underlying quantum nature of the measurement, but the optimal quantum protocol for interrogating such linear sensors is not known.","We derive the fundamental precision limit, the extended channel quantum Cram\\'er-Rao bound, and the optimal protocol that attains it.","In the experimentally relevant regime where losses dominate, we prove that non-Gaussian state preparation and measurements are required for optimality.","We discuss how this non-Gaussian protocol could improve searches for signatures of quantum gravity, stochastic gravitational waves, and axionic dark matter."],"url":"http://arxiv.org/abs/2404.13867v1","category":"quant-ph"}
{"created":"2024-04-22 04:25:02","title":"PM-VIS: High-Performance Box-Supervised Video Instance Segmentation","abstract":"Labeling pixel-wise object masks in videos is a resource-intensive and laborious process. Box-supervised Video Instance Segmentation (VIS) methods have emerged as a viable solution to mitigate the labor-intensive annotation process. . In practical applications, the two-step approach is not only more flexible but also exhibits a higher recognition accuracy. Inspired by the recent success of Segment Anything Model (SAM), we introduce a novel approach that aims at harnessing instance box annotations from multiple perspectives to generate high-quality instance pseudo masks, thus enriching the information contained in instance annotations. We leverage ground-truth boxes to create three types of pseudo masks using the HQ-SAM model, the box-supervised VIS model (IDOL-BoxInst), and the VOS model (DeAOT) separately, along with three corresponding optimization mechanisms. Additionally, we introduce two ground-truth data filtering methods, assisted by high-quality pseudo masks, to further enhance the training dataset quality and improve the performance of fully supervised VIS methods. To fully capitalize on the obtained high-quality Pseudo Masks, we introduce a novel algorithm, PM-VIS, to integrate mask losses into IDOL-BoxInst. Our PM-VIS model, trained with high-quality pseudo mask annotations, demonstrates strong ability in instance mask prediction, achieving state-of-the-art performance on the YouTube-VIS 2019, YouTube-VIS 2021, and OVIS validation sets, notably narrowing the gap between box-supervised and fully supervised VIS methods.","sentences":["Labeling pixel-wise object masks in videos is a resource-intensive and laborious process.","Box-supervised Video Instance Segmentation (VIS) methods have emerged as a viable solution to mitigate the labor-intensive annotation process. .","In practical applications, the two-step approach is not only more flexible but also exhibits a higher recognition accuracy.","Inspired by the recent success of Segment Anything Model (SAM), we introduce a novel approach that aims at harnessing instance box annotations from multiple perspectives to generate high-quality instance pseudo masks, thus enriching the information contained in instance annotations.","We leverage ground-truth boxes to create three types of pseudo masks using the HQ-SAM model, the box-supervised VIS model (IDOL-BoxInst), and the VOS model (DeAOT) separately, along with three corresponding optimization mechanisms.","Additionally, we introduce two ground-truth data filtering methods, assisted by high-quality pseudo masks, to further enhance the training dataset quality and improve the performance of fully supervised VIS methods.","To fully capitalize on the obtained high-quality Pseudo Masks, we introduce a novel algorithm, PM-VIS, to integrate mask losses into IDOL-BoxInst.","Our PM-VIS model, trained with high-quality pseudo mask annotations, demonstrates strong ability in instance mask prediction, achieving state-of-the-art performance on the YouTube-VIS 2019, YouTube-VIS 2021, and OVIS validation sets, notably narrowing the gap between box-supervised and fully supervised VIS methods."],"url":"http://arxiv.org/abs/2404.13863v1","category":"cs.CV"}
{"created":"2024-04-22 03:19:17","title":"Reconstructing Intrinsic Stellar Noise with Stellar Atmospheric Parameters and Chromospheric Activity","abstract":"Accurately characterizing intrinsic stellar photometric noise induced by stellar astrophysics, such as stellar activity, granulation, and oscillations, is of crucial importance for detecting transiting exoplanets. In this study, we investigate the relation between the intrinsic stellar photometric noise, as quantified by the Kepler rrmsCDPP measurement, and the level of stellar chromospheric activity, as indicated by the S-index of Ca II HK lines derived from the LAMOST spectra. Our results reveal a clear positive correlation between S-index and rrmsCDPP, and the correlation becomes more significant at higher activity levels and on longer timescales. We have therefore built an empirical relation between rrmsCDPP and S-index as well as Teff, logg, [Fe/H], and apparent magnitude with the XGBoost regression algorithm, using the LAMOST-Kepler common star sample as the training set. This method achieves a precision of ~20 ppm for inferring the intrinsic noise from the S-index and other stellar labels on a 6-hour integration duration. We have applied this empirical relation to the full LAMOST DR7 spectra database, and obtained the intrinsic noise predictions for 1,358,275 stars. The resultant catalog is publicly available and expected to be valuable for optimizing target selection for future exoplanet-hunting space missions, such as the Earth 2.0 mission.","sentences":["Accurately characterizing intrinsic stellar photometric noise induced by stellar astrophysics, such as stellar activity, granulation, and oscillations, is of crucial importance for detecting transiting exoplanets.","In this study, we investigate the relation between the intrinsic stellar photometric noise, as quantified by the Kepler rrmsCDPP measurement, and the level of stellar chromospheric activity, as indicated by the S-index of Ca II HK lines derived from the LAMOST spectra.","Our results reveal a clear positive correlation between S-index and rrmsCDPP, and the correlation becomes more significant at higher activity levels and on longer timescales.","We have therefore built an empirical relation between rrmsCDPP and S-index as well as Teff, logg, [Fe/H], and apparent magnitude with the XGBoost regression algorithm, using the LAMOST-Kepler common star sample as the training set.","This method achieves a precision of ~20 ppm for inferring the intrinsic noise from the S-index and other stellar labels on a 6-hour integration duration.","We have applied this empirical relation to the full LAMOST DR7 spectra database, and obtained the intrinsic noise predictions for 1,358,275 stars.","The resultant catalog is publicly available and expected to be valuable for optimizing target selection for future exoplanet-hunting space missions, such as the Earth 2.0 mission."],"url":"http://arxiv.org/abs/2404.13850v1","category":"astro-ph.SR"}
{"created":"2024-04-22 02:57:57","title":"Stochastic thermodynamics of Brownian motion in a flowing fluid","abstract":"We study stochastic thermodynamics of over-damped Brownian motion in a flowing fluid. Unlike some previous works, we treat the effects of the flow field as a non-conservational driving force acting on the Brownian particle. This allows us to apply the theoretical formalism developed in a recent work for general non-conservative Langevin dynamics. We define heat and work both at the trajectory level and at the ensemble level, and prove the second law of thermodynamics explicitly. The entropy production (EP) is decomposed into a housekeeping part and an excess part, both of which are non-negative at the ensemble level. Fluctuation theorems are derived for the housekeeping work, the excess work, and the total work, which are further verified using numerical simulations. A comparison between our theory and an earlier theory by Speck et. al. is also carried out.","sentences":["We study stochastic thermodynamics of over-damped Brownian motion in a flowing fluid.","Unlike some previous works, we treat the effects of the flow field as a non-conservational driving force acting on the Brownian particle.","This allows us to apply the theoretical formalism developed in a recent work for general non-conservative Langevin dynamics.","We define heat and work both at the trajectory level and at the ensemble level, and prove the second law of thermodynamics explicitly.","The entropy production (EP) is decomposed into a housekeeping part and an excess part, both of which are non-negative at the ensemble level.","Fluctuation theorems are derived for the housekeeping work, the excess work, and the total work, which are further verified using numerical simulations.","A comparison between our theory and an earlier theory by Speck et. al. is also carried out."],"url":"http://arxiv.org/abs/2404.13845v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-22 02:42:32","title":"On Support Relations Inference and Scene Hierarchy Graph Construction from Point Cloud in Clustered Environments","abstract":"Over the years, scene understanding has attracted a growing interest in computer vision, providing the semantic and physical scene information necessary for robots to complete some particular tasks autonomously. In 3D scenes, rich spatial geometric and topological information are often ignored by RGB-based approaches for scene understanding. In this study, we develop a bottom-up approach for scene understanding that infers support relations between objects from a point cloud. Our approach utilizes the spatial topology information of the plane pairs in the scene, consisting of three major steps. 1) Detection of pairwise spatial configuration: dividing primitive pairs into local support connection and local inner connection; 2) primitive classification: a combinatorial optimization method applied to classify primitives; and 3) support relations inference and hierarchy graph construction: bottom-up support relations inference and scene hierarchy graph construction containing primitive level and object level. Through experiments, we demonstrate that the algorithm achieves excellent performance in primitive classification and support relations inference. Additionally, we show that the scene hierarchy graph contains rich geometric and topological information of objects, and it possesses great scalability for scene understanding.","sentences":["Over the years, scene understanding has attracted a growing interest in computer vision, providing the semantic and physical scene information necessary for robots to complete some particular tasks autonomously.","In 3D scenes, rich spatial geometric and topological information are often ignored by RGB-based approaches for scene understanding.","In this study, we develop a bottom-up approach for scene understanding that infers support relations between objects from a point cloud.","Our approach utilizes the spatial topology information of the plane pairs in the scene, consisting of three major steps.","1) Detection of pairwise spatial configuration: dividing primitive pairs into local support connection and local inner connection; 2) primitive classification: a combinatorial optimization method applied to classify primitives; and 3) support relations inference and hierarchy graph construction: bottom-up support relations inference and scene hierarchy graph construction containing primitive level and object level.","Through experiments, we demonstrate that the algorithm achieves excellent performance in primitive classification and support relations inference.","Additionally, we show that the scene hierarchy graph contains rich geometric and topological information of objects, and it possesses great scalability for scene understanding."],"url":"http://arxiv.org/abs/2404.13842v1","category":"cs.CV"}
{"created":"2024-04-22 02:13:58","title":"Characterization of Maximizers in A Non-Convex Geometric Optimization Problem With Application to Optical Wireless Power Transfer Systems","abstract":"This research studies a non-convex geometric optimization problem arising from the field of optical wireless power transfer. In the considered optimization problem, the cost function is a sum of negatively and fractionally powered distances from given points arbitrarily located in a plane to another point belonging to a different plane. Therefore, it is a strongly nonlinear and non-convex programming, hence posing a challenge on the characterization of its optimizer set, especially its set of global optimizers. To tackle this challenge, the bifurcation theory is employed to investigate the continuation and bifurcation structures of the Hessian matrix of the cost function. As such, two main results are derived. First, there is a critical distance between the two considered planes such that beyond which a unique global optimizer exists. Second, the exact number of maximizers is locally derived by the number of bifurcation branches determined via one-dimensional isotropic subgroups of a Lie group acting on $\\mathbb{R}^2$, when the inter-plane distance is smaller than the above-mentioned critical distance. Consequently, numerical simulations and computations of bifurcation points are carried out for various configurations of the given points, whose results confirm the derived theoretical outcomes.","sentences":["This research studies a non-convex geometric optimization problem arising from the field of optical wireless power transfer.","In the considered optimization problem, the cost function is a sum of negatively and fractionally powered distances from given points arbitrarily located in a plane to another point belonging to a different plane.","Therefore, it is a strongly nonlinear and non-convex programming, hence posing a challenge on the characterization of its optimizer set, especially its set of global optimizers.","To tackle this challenge, the bifurcation theory is employed to investigate the continuation and bifurcation structures of the Hessian matrix of the cost function.","As such, two main results are derived.","First, there is a critical distance between the two considered planes such that beyond which a unique global optimizer exists.","Second, the exact number of maximizers is locally derived by the number of bifurcation branches determined via one-dimensional isotropic subgroups of a Lie group acting on $\\mathbb{R}^2$, when the inter-plane distance is smaller than the above-mentioned critical distance.","Consequently, numerical simulations and computations of bifurcation points are carried out for various configurations of the given points, whose results confirm the derived theoretical outcomes."],"url":"http://arxiv.org/abs/2404.13832v1","category":"math.OC"}
{"created":"2024-04-22 01:58:22","title":"Formation of the four terrestrial planets in the Jupiter-Saturn chaotic excitation scenario: fundamental properties and water delivery","abstract":"The Jupiter-Saturn chaotic excitation (JSCE) scenario proposes that the protoplanetary disk was dynamically excited and depleted beyond ~1-1.5 au in a few Myr, offering a new and plausible explanation for several observed properties of the inner solar system. Here, we expanded our previous work by conducting a comprehensive analysis of 37 optimal terrestrial planet systems obtained in the context of the JSCE scenario. Each optimal system harbored exactly four terrestrial planets analogs to Mercury, Venus, Earth, and Mars. We further investigated water delivery, feeding zones, and accretion history for the planet analogs, which allowed us to better constrain the water distribution in the disk. The main findings of this work are as follows: 1) the formation of four terrestrial planets with orbits and masses similar to those observed in our solar system in most of our sample, as evidenced by the dynamically colder and hotter orbits of Venus-Earth and Mercury-Mars analogs, and the high success rates of similar mutual orbital separations (~40-85%) and mass ratios of the planets (~70-90%) among the 37 systems; and 2) water was delivered to all terrestrial planets during their formation through the accretion of water-bearing disk objects from beyond ~1-1.5 au. The achievement of Earth's estimated bulk water content required the disk to contain sufficient water mass distributed within those objects initially. This requirement implies that Mercury, Venus, and Mars acquired water similar to the amount on Earth during their formation. Several of our planet analogs also matched additional constraints, such as the timing of Moon formation by a giant impact, Earth's late accretion mass and composition, and Mars's formation timescale.","sentences":["The Jupiter-Saturn chaotic excitation (JSCE) scenario proposes that the protoplanetary disk was dynamically excited and depleted beyond ~1-1.5 au in a few Myr, offering a new and plausible explanation for several observed properties of the inner solar system.","Here, we expanded our previous work by conducting a comprehensive analysis of 37 optimal terrestrial planet systems obtained in the context of the JSCE scenario.","Each optimal system harbored exactly four terrestrial planets analogs to Mercury, Venus, Earth, and Mars.","We further investigated water delivery, feeding zones, and accretion history for the planet analogs, which allowed us to better constrain the water distribution in the disk.","The main findings of this work are as follows: 1) the formation of four terrestrial planets with orbits and masses similar to those observed in our solar system in most of our sample, as evidenced by the dynamically colder and hotter orbits of Venus-Earth and Mercury-Mars analogs, and the high success rates of similar mutual orbital separations (~40-85%) and mass ratios of the planets (~70-90%) among the 37 systems; and 2) water was delivered to all terrestrial planets during their formation through the accretion of water-bearing disk objects from beyond ~1-1.5 au.","The achievement of Earth's estimated bulk water content required the disk to contain sufficient water mass distributed within those objects initially.","This requirement implies that Mercury, Venus, and Mars acquired water similar to the amount on Earth during their formation.","Several of our planet analogs also matched additional constraints, such as the timing of Moon formation by a giant impact, Earth's late accretion mass and composition, and Mars's formation timescale."],"url":"http://arxiv.org/abs/2404.13826v1","category":"astro-ph.EP"}
{"created":"2024-04-21 23:49:53","title":"Quantum Transport Simulation of Sub-1-nm Gate Length Monolayer MoS2 Transistors","abstract":"Sub-1-nm gate length $MoS_2$ transistors have been experimentally fabricated, but their device performance limit remains elusive. Herein, we explore the performance limits of the sub-1-nm gate length monolayer (ML) $MoS_2$ transistors through ab initio quantum transport simulations. Our simulation results demonstrate that, through appropriate doping and dielectric engineering, the sub-1-nm devices can meet the requirement of extended 'ITRS'(International Technology Roadmap for Semiconductors) $L_g$=0.34 nm. Following device optimization, we achieve impressive maximum on-state current densities of 409 $\\mu A / \\mu m$ for n-type and 800 $\\mu A / \\mu m$ for p-type high-performance (HP) devices, while n-type and p-type low-power (LP) devices exhibit maximum on-state current densities of 75 $\\mu A / \\mu m$ and 187 $\\mu A / \\mu m$, respectively. We employed the Wentzel-Kramer-Brillouin (WKB) approximation to explain the physical mechanisms of underlap and spacer region optimization on transistor performance. The underlap and spacer regions primarily influence the transport properties of sub-1-nm transistors by respectively altering the width and body factor of the potential barriers. Compared to ML $MoS_2$ transistors with a 1 nm gate length, our sub-1-nm gate length HP and LP ML $MoS_2$ transistors exhibit lower energy-delay products. Hence the sub-1-nm gate length transistors have immense potential for driving the next generation of electronics.","sentences":["Sub-1-nm gate length $MoS_2$ transistors have been experimentally fabricated, but their device performance limit remains elusive.","Herein, we explore the performance limits of the sub-1-nm gate length monolayer (ML) $MoS_2$ transistors through ab initio quantum transport simulations.","Our simulation results demonstrate that, through appropriate doping and dielectric engineering, the sub-1-nm devices can meet the requirement of extended 'ITRS'(International Technology Roadmap for Semiconductors) $L_g$=0.34 nm.","Following device optimization, we achieve impressive maximum on-state current densities of 409 $\\mu A / \\mu m$ for n-type and 800 $\\mu A / \\mu m$ for p-type high-performance (HP) devices, while n-type and p-type low-power (LP) devices exhibit maximum on-state current densities of 75 $\\mu A / \\mu m$ and 187 $\\mu A / \\mu m$, respectively.","We employed the Wentzel-Kramer-Brillouin (WKB) approximation to explain the physical mechanisms of underlap and spacer region optimization on transistor performance.","The underlap and spacer regions primarily influence the transport properties of sub-1-nm transistors by respectively altering the width and body factor of the potential barriers.","Compared to ML $MoS_2$ transistors with a 1 nm gate length, our sub-1-nm gate length HP and LP ML $MoS_2$ transistors exhibit lower energy-delay products.","Hence the sub-1-nm gate length transistors have immense potential for driving the next generation of electronics."],"url":"http://arxiv.org/abs/2404.13801v1","category":"physics.comp-ph"}
{"created":"2024-04-21 21:21:41","title":"Mathematical and numerical analysis for PDE systems modeling intravascular drug release from arterial stents and transport in arterial tissue","abstract":"This paper is concerned with the PDE and numerical analysis of a modified one-dimensional intravascular stent model originally proposed in [4]. It is proved that the modified model has a unique weak solution using the Galerkin method combined with a compactness argument. A semi-discrete finite element method and a fully discrete scheme using the Euler time-stepping are formulated for the PDE model. Optimal order error estimates in the energy norm are proved for both schemes. Numerical results are presented along with comparisons between different decoupling strategies and time-stepping schemes. Lastly, extensions of the model and its PDE and numerical analysis results to the two-dimensional case are also briefly discussed.","sentences":["This paper is concerned with the PDE and numerical analysis of a modified one-dimensional intravascular stent model originally proposed in [4].","It is proved that the modified model has a unique weak solution using the Galerkin method combined with a compactness argument.","A semi-discrete finite element method and a fully discrete scheme using the Euler time-stepping are formulated for the PDE model.","Optimal order error estimates in the energy norm are proved for both schemes.","Numerical results are presented along with comparisons between different decoupling strategies and time-stepping schemes.","Lastly, extensions of the model and its PDE and numerical analysis results to the two-dimensional case are also briefly discussed."],"url":"http://arxiv.org/abs/2404.13780v1","category":"math.NA"}
{"created":"2024-04-21 21:08:36","title":"Waiting time statistics for a double quantum dot coupled with an optical cavity","abstract":"A double quantum dot coupled to an optical cavity is a prototypical example of a non-trivial open quantum system. Recent experimental and theoretical studies show that this system is a candidate for single-photon detection in the microwave domain. This motivates studies that go beyond just the average current, and also take into account the full counting statistics of photon and electron detections. With this in mind, here we provide a detailed analysis of the waiting time statistics of this system within the quantum jump unravelling, which allows us to extract analytical expressions for the success and failure probabilities, as well as for the inter detection times. Furthermore, by comparing single and multi-photon scenarios, we infer a hierarchy of occurrence probabilities for the different events, highlighting the role of photon interference events in the detection probabilities. Our results therefore provide a direct illustration of how waiting time statistics can be used to optimize a timely and relevant metrological task.","sentences":["A double quantum dot coupled to an optical cavity is a prototypical example of a non-trivial open quantum system.","Recent experimental and theoretical studies show that this system is a candidate for single-photon detection in the microwave domain.","This motivates studies that go beyond just the average current, and also take into account the full counting statistics of photon and electron detections.","With this in mind, here we provide a detailed analysis of the waiting time statistics of this system within the quantum jump unravelling, which allows us to extract analytical expressions for the success and failure probabilities, as well as for the inter detection times.","Furthermore, by comparing single and multi-photon scenarios, we infer a hierarchy of occurrence probabilities for the different events, highlighting the role of photon interference events in the detection probabilities.","Our results therefore provide a direct illustration of how waiting time statistics can be used to optimize a timely and relevant metrological task."],"url":"http://arxiv.org/abs/2404.13775v1","category":"quant-ph"}
{"created":"2024-04-21 19:45:13","title":"Sublinear Time Low-Rank Approximation of Toeplitz Matrices","abstract":"We present a sublinear time algorithm for computing a near optimal low-rank approximation to any positive semidefinite (PSD) Toeplitz matrix $T\\in \\mathbb{R}^{d\\times d}$, given noisy access to its entries. In particular, given entrywise query access to $T+E$ for an arbitrary noise matrix $E\\in \\mathbb{R}^{d\\times d}$, integer rank $k\\leq d$, and error parameter $\\delta>0$, our algorithm runs in time $\\text{poly}(k,\\log(d/\\delta))$ and outputs (in factored form) a Toeplitz matrix $\\widetilde{T} \\in \\mathbb{R}^{d \\times d}$ with rank $\\text{poly}(k,\\log(d/\\delta))$ satisfying, for some fixed constant $C$, \\begin{equation*}   \\|T-\\widetilde{T}\\|_F \\leq C \\cdot \\max\\{\\|E\\|_F,\\|T-T_k\\|_F\\} + \\delta \\cdot \\|T\\|_F. \\end{equation*} Here $\\|\\cdot \\|_F$ is the Frobenius norm and $T_k$ is the best (not necessarily Toeplitz) rank-$k$ approximation to $T$ in the Frobenius norm, given by projecting $T$ onto its top $k$ eigenvectors.   Our result has the following applications. When $E = 0$, we obtain the first sublinear time near-relative-error low-rank approximation algorithm for PSD Toeplitz matrices, resolving the main open problem of Kapralov et al. SODA `23, whose algorithm had sublinear query complexity but exponential runtime. Our algorithm can also be applied to approximate the unknown Toeplitz covariance matrix of a multivariate Gaussian distribution, given sample access to this distribution, resolving an open question of Eldar et al. SODA `20.   Our algorithm applies sparse Fourier transform techniques to recover a low-rank Toeplitz matrix using its Fourier structure. Our key technical contribution is the first polynomial time algorithm for \\emph{discrete time off-grid} sparse Fourier recovery, which may be of independent interest.","sentences":["We present a sublinear time algorithm for computing a near optimal low-rank approximation to any positive semidefinite (PSD)","Toeplitz matrix $T\\in \\mathbb{R}^{d\\times d}$, given noisy access to its entries.","In particular, given entrywise query access to $T+E$ for an arbitrary noise matrix $E\\in \\mathbb{R}^{d\\times d}$, integer rank $k\\leq d$, and error parameter $\\delta>0$, our algorithm runs in time $\\text{poly}(k,\\log(d/\\delta))$ and outputs (in factored form) a Toeplitz matrix $\\widetilde{T} \\in \\mathbb{R}^{d \\times d}$ with rank $\\text{poly}(k,\\log(d/\\delta))$ satisfying, for some fixed constant $C$, \\begin{equation*}   \\|T-\\widetilde{T}\\|_F \\leq C \\cdot \\max\\{\\|E\\|_F,\\|T-T_k\\|_F\\} + \\delta \\cdot \\|T\\|_F. \\end{equation*} Here $\\|\\cdot \\|_F$ is the Frobenius norm and $T_k$ is the best (not necessarily Toeplitz) rank-$k$ approximation to $T$ in the Frobenius norm, given by projecting $T$ onto its top $k$ eigenvectors.   ","Our result has the following applications.","When $E = 0$, we obtain the first sublinear time near-relative-error low-rank approximation algorithm for PSD Toeplitz matrices, resolving the main open problem of Kapralov et al. SODA `23, whose algorithm had sublinear query complexity but exponential runtime.","Our algorithm can also be applied to approximate the unknown Toeplitz covariance matrix of a multivariate Gaussian distribution, given sample access to this distribution, resolving an open question of Eldar et al. SODA `20.   ","Our algorithm applies sparse Fourier transform techniques to recover a low-rank Toeplitz matrix using its Fourier structure.","Our key technical contribution is the first polynomial time algorithm for \\emph{discrete time off-grid} sparse Fourier recovery, which may be of independent interest."],"url":"http://arxiv.org/abs/2404.13757v1","category":"cs.DS"}
{"created":"2024-04-21 19:31:21","title":"Dispensing with optimal control: a new approach for the pricing and management of share buyback contracts","abstract":"This paper introduces a novel methodology for the pricing and management of share buyback contracts, overcoming the limitations of traditional optimal control methods, which frequently encounter difficulties with high-dimensional state spaces and the intricacies of selecting appropriate risk penalty or risk aversion parameter. Our methodology applies optimized heuristic strategies to maximize the contract's value. The computation of this value utilizes classical methods typically used for pricing path-dependent Bermudan options. Additionally, our approach naturally leads to the formulation of a hedging strategy.","sentences":["This paper introduces a novel methodology for the pricing and management of share buyback contracts, overcoming the limitations of traditional optimal control methods, which frequently encounter difficulties with high-dimensional state spaces and the intricacies of selecting appropriate risk penalty or risk aversion parameter.","Our methodology applies optimized heuristic strategies to maximize the contract's value.","The computation of this value utilizes classical methods typically used for pricing path-dependent Bermudan options.","Additionally, our approach naturally leads to the formulation of a hedging strategy."],"url":"http://arxiv.org/abs/2404.13754v1","category":"q-fin.PR"}
{"created":"2024-04-21 19:30:47","title":"A nonstandard application of cross-validation to estimate density functionals","abstract":"Cross-validation is usually employed to evaluate the performance of a given statistical methodology. When such a methodology depends on a number of tuning parameters, cross-validation proves to be helpful to select the parameters that optimize the estimated performance. In this paper, however, a very different and nonstandard use of cross-validation is investigated. Instead of focusing on the cross-validated parameters, the main interest is switched to the estimated value of the error criterion at optimal performance. It is shown that this approach is able to provide consistent and efficient estimates of some density functionals, with the noteworthy feature that these estimates do not rely on the choice of any further tuning parameter, so that, in that sense, they can be considered to be purely empirical. Here, a base case of application of this new paradigm is developed in full detail, while many other possible extensions are hinted as well.","sentences":["Cross-validation is usually employed to evaluate the performance of a given statistical methodology.","When such a methodology depends on a number of tuning parameters, cross-validation proves to be helpful to select the parameters that optimize the estimated performance.","In this paper, however, a very different and nonstandard use of cross-validation is investigated.","Instead of focusing on the cross-validated parameters, the main interest is switched to the estimated value of the error criterion at optimal performance.","It is shown that this approach is able to provide consistent and efficient estimates of some density functionals, with the noteworthy feature that these estimates do not rely on the choice of any further tuning parameter, so that, in that sense, they can be considered to be purely empirical.","Here, a base case of application of this new paradigm is developed in full detail, while many other possible extensions are hinted as well."],"url":"http://arxiv.org/abs/2404.13753v1","category":"stat.ME"}
{"created":"2024-04-21 19:01:38","title":"A Splice Method for Local-to-Nonlocal Coupling of Weak Forms","abstract":"We propose a method to couple local and nonlocal diffusion models. By inheriting desirable properties such as patch tests, asymptotic compatibility and unintrusiveness from related splice and optimization-based coupling schemes, it enables the use of weak (or variational) formulations, is computationally efficient and straightforward to implement. We prove well-posedness of the coupling scheme and demonstrate its properties and effectiveness in a variety of numerical examples.","sentences":["We propose a method to couple local and nonlocal diffusion models.","By inheriting desirable properties such as patch tests, asymptotic compatibility and unintrusiveness from related splice and optimization-based coupling schemes, it enables the use of weak (or variational) formulations, is computationally efficient and straightforward to implement.","We prove well-posedness of the coupling scheme and demonstrate its properties and effectiveness in a variety of numerical examples."],"url":"http://arxiv.org/abs/2404.13744v1","category":"math.NA"}
{"created":"2024-04-21 15:31:33","title":"Stability of the Abstract Thermoelastic System with Singularity","abstract":"In this paper, we analyze an abstract thermoelastic system, where the heat conduction follows the Cattaneo law. Zero becomes a spectrum point of the system operator when the coupling and thermal damping parameters of the system satisfy specific conditions. We obtain the decay rates of solutions to the system with or without the inertial term. Furthermore, the decay rate of the system without inertial terms is shown to be optimal.","sentences":["In this paper, we analyze an abstract thermoelastic system, where the heat conduction follows the Cattaneo law.","Zero becomes a spectrum point of the system operator when the coupling and thermal damping parameters of the system satisfy specific conditions.","We obtain the decay rates of solutions to the system with or without the inertial term.","Furthermore, the decay rate of the system without inertial terms is shown to be optimal."],"url":"http://arxiv.org/abs/2404.13689v1","category":"math.AP"}
{"created":"2024-04-21 14:35:51","title":"Lowest-degree robust finite element schemes for inhomogeneous bi-Laplace problems","abstract":"In this paper, we study the numerical method for the bi-Laplace problems with inhomogeneous coefficients; particularly, we propose finite element schemes on rectangular grids respectively for an inhomogeneous fourth-order elliptic singular perturbation problem and for the Helmholtz transmission eigenvalue problem. The new methods use the reduced rectangle Morley (RRM for short) element space with piecewise quadratic polynomials, which are of the lowest degree possible. For the finite element space, a discrete analogue of an equality by Grisvard is proved for the stability issue and a locally-averaged interpolation operator is constructed for the approximation issue. Optimal convergence rates of the schemes are proved, and numerical experiments are given to verify the theoretical analysis.","sentences":["In this paper, we study the numerical method for the bi-Laplace problems with inhomogeneous coefficients; particularly, we propose finite element schemes on rectangular grids respectively for an inhomogeneous fourth-order elliptic singular perturbation problem and for the Helmholtz transmission eigenvalue problem.","The new methods use the reduced rectangle Morley (RRM for short) element space with piecewise quadratic polynomials, which are of the lowest degree possible.","For the finite element space, a discrete analogue of an equality by Grisvard is proved for the stability issue and a locally-averaged interpolation operator is constructed for the approximation issue.","Optimal convergence rates of the schemes are proved, and numerical experiments are given to verify the theoretical analysis."],"url":"http://arxiv.org/abs/2404.13676v1","category":"math.NA"}
{"created":"2024-04-21 13:34:52","title":"Optimal Interventions in Coupled-Activity Network Games: Application to Sustainable Forestry","abstract":"We consider the problem of promoting sustainability in production forests wherein a given number of strategic entities are authorized to own or manage concession regions. These entities harvest agricultural commodities and sell them in a market. We study optimal price-shaping in a coupled-activity network game model in which the concession owners (agents) engage in two activities: (a) the sustainable activity of producing a commodity that does not interfere with protected forest resources, and (b) the unsustainable activity of infringing into protected regions to expand their agricultural footprint. We characterize two types of policies in a budget-constrained setting: one that maximally suppresses the aggregate unsustainable activity and another that maximizes social welfare while constraining the aggregate unsustainable activity to remain below a predefined tolerance. Our analysis provides novel insights on the agents' equilibrium effort across the two activities and their influence on others due to intra- and cross-activity network effects. We also identify a measure of node centrality that resembles the Bonacich-Katz centrality and helps us determine pricing incentives that lead to welfare improvement while reducing the aggregate unsustainable activity.","sentences":["We consider the problem of promoting sustainability in production forests wherein a given number of strategic entities are authorized to own or manage concession regions.","These entities harvest agricultural commodities and sell them in a market.","We study optimal price-shaping in a coupled-activity network game model in which the concession owners (agents) engage in two activities: (a) the sustainable activity of producing a commodity that does not interfere with protected forest resources, and (b) the unsustainable activity of infringing into protected regions to expand their agricultural footprint.","We characterize two types of policies in a budget-constrained setting: one that maximally suppresses the aggregate unsustainable activity and another that maximizes social welfare while constraining the aggregate unsustainable activity to remain below a predefined tolerance.","Our analysis provides novel insights on the agents' equilibrium effort across the two activities and their influence on others due to intra- and cross-activity network effects.","We also identify a measure of node centrality that resembles the Bonacich-Katz centrality and helps us determine pricing incentives that lead to welfare improvement while reducing the aggregate unsustainable activity."],"url":"http://arxiv.org/abs/2404.13662v1","category":"math.OC"}
{"created":"2024-04-21 13:31:16","title":"Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, but their vulnerability to trojan or backdoor attacks poses significant security risks. This paper explores the challenges and insights gained from the Trojan Detection Competition 2023 (TDC2023), which focused on identifying and evaluating trojan attacks on LLMs. We investigate the difficulty of distinguishing between intended and unintended triggers, as well as the feasibility of reverse engineering trojans in real-world scenarios. Our comparative analysis of various trojan detection methods reveals that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores. The top-performing methods in the competition achieved Recall scores around 0.16, comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes. This finding raises questions about the detectability and recoverability of trojans inserted into the model, given only the harmful targets. Despite the inability to fully solve the problem, the competition has led to interesting observations about the viability of trojan detection and improved techniques for optimizing LLM input prompts. The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs. The TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs, laying the groundwork for future research in this area to ensure their safety and reliability in real-world applications.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, but their vulnerability to trojan or backdoor attacks poses significant security risks.","This paper explores the challenges and insights gained from the Trojan Detection Competition 2023 (TDC2023), which focused on identifying and evaluating trojan attacks on LLMs.","We investigate the difficulty of distinguishing between intended and unintended triggers, as well as the feasibility of reverse engineering trojans in real-world scenarios.","Our comparative analysis of various trojan detection methods reveals that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores.","The top-performing methods in the competition achieved Recall scores around 0.16, comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes.","This finding raises questions about the detectability and recoverability of trojans inserted into the model, given only the harmful targets.","Despite the inability to fully solve the problem, the competition has led to interesting observations about the viability of trojan detection and improved techniques for optimizing LLM input prompts.","The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs.","The TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs, laying the groundwork for future research in this area to ensure their safety and reliability in real-world applications."],"url":"http://arxiv.org/abs/2404.13660v1","category":"cs.CL"}
{"created":"2024-04-21 12:33:07","title":"Beyond Alignment: Blind Video Face Restoration via Parsing-Guided Temporal-Coherent Transformer","abstract":"Multiple complex degradations are coupled in low-quality video faces in the real world. Therefore, blind video face restoration is a highly challenging ill-posed problem, requiring not only hallucinating high-fidelity details but also enhancing temporal coherence across diverse pose variations. Restoring each frame independently in a naive manner inevitably introduces temporal incoherence and artifacts from pose changes and keypoint localization errors. To address this, we propose the first blind video face restoration approach with a novel parsing-guided temporal-coherent transformer (PGTFormer) without pre-alignment. PGTFormer leverages semantic parsing guidance to select optimal face priors for generating temporally coherent artifact-free results. Specifically, we pre-train a temporal-spatial vector quantized auto-encoder on high-quality video face datasets to extract expressive context-rich priors. Then, the temporal parse-guided codebook predictor (TPCP) restores faces in different poses based on face parsing context cues without performing face pre-alignment. This strategy reduces artifacts and mitigates jitter caused by cumulative errors from face pre-alignment. Finally, the temporal fidelity regulator (TFR) enhances fidelity through temporal feature interaction and improves video temporal consistency. Extensive experiments on face videos show that our method outperforms previous face restoration baselines. The code will be released on \\href{https://github.com/kepengxu/PGTFormer}{https://github.com/kepengxu/PGTFormer}.","sentences":["Multiple complex degradations are coupled in low-quality video faces in the real world.","Therefore, blind video face restoration is a highly challenging ill-posed problem, requiring not only hallucinating high-fidelity details but also enhancing temporal coherence across diverse pose variations.","Restoring each frame independently in a naive manner inevitably introduces temporal incoherence and artifacts from pose changes and keypoint localization errors.","To address this, we propose the first blind video face restoration approach with a novel parsing-guided temporal-coherent transformer (PGTFormer) without pre-alignment.","PGTFormer leverages semantic parsing guidance to select optimal face priors for generating temporally coherent artifact-free results.","Specifically, we pre-train a temporal-spatial vector quantized auto-encoder on high-quality video face datasets to extract expressive context-rich priors.","Then, the temporal parse-guided codebook predictor (TPCP) restores faces in different poses based on face parsing context cues without performing face pre-alignment.","This strategy reduces artifacts and mitigates jitter caused by cumulative errors from face pre-alignment.","Finally, the temporal fidelity regulator (TFR) enhances fidelity through temporal feature interaction and improves video temporal consistency.","Extensive experiments on face videos show that our method outperforms previous face restoration baselines.","The code will be released on \\href{https://github.com/kepengxu/PGTFormer}{https://github.com/kepengxu/PGTFormer}."],"url":"http://arxiv.org/abs/2404.13640v1","category":"cs.MM"}
{"created":"2024-04-21 10:54:12","title":"Parallel AIG Refactoring via Conflict Breaking","abstract":"Algorithm parallelization to leverage multi-core platforms for improving the efficiency of Electronic Design Automation~(EDA) tools plays a significant role in enhancing the scalability of Integrated Circuit (IC) designs. Logic optimization is a key process in the EDA design flow to reduce the area and depth of the circuit graph by finding logically equivalent graphs for substitution, which is typically time-consuming. To address these challenges, in this paper, we first analyze two types of conflicts that need to be handled in the parallelization framework of refactoring And-Inverter Graph~(AIG). We then present a fine-grained parallel AIG refactoring method, which strikes a balance between the degree of parallelism and the conflicts encountered during the refactoring operations. Experiment results show that our parallel refactor is 28x averagely faster than the sequential algorithm on large benchmark tests with 64 physical CPU cores, and has comparable optimization quality.","sentences":["Algorithm parallelization to leverage multi-core platforms for improving the efficiency of Electronic Design Automation~(EDA) tools plays a significant role in enhancing the scalability of Integrated Circuit (IC) designs.","Logic optimization is a key process in the EDA design flow to reduce the area and depth of the circuit graph by finding logically equivalent graphs for substitution, which is typically time-consuming.","To address these challenges, in this paper, we first analyze two types of conflicts that need to be handled in the parallelization framework of refactoring And-Inverter Graph~(AIG).","We then present a fine-grained parallel AIG refactoring method, which strikes a balance between the degree of parallelism and the conflicts encountered during the refactoring operations.","Experiment results show that our parallel refactor is 28x averagely faster than the sequential algorithm on large benchmark tests with 64 physical CPU cores, and has comparable optimization quality."],"url":"http://arxiv.org/abs/2404.13617v1","category":"cs.DC"}
{"created":"2024-04-21 10:52:06","title":"Stratified Monge-Kantorovich optimal transport problems","abstract":"In this paper, we investigate Monge-Kantorovich problems for which the absolute continuity of marginals is relaxed. For $X,Y\\subseteq\\mathbb{R}^{n+1}$ let $(X,\\mathcal{B}_X,\\mu)$ and $(Y,\\mathcal{B}_Y,\\nu)$ be two Borel probability spaces, $c:X\\times Y\\to\\mathbb{R}$ be a cost function, and consider the problem \\begin{align*}\\tag{MKP}\\label{MKPEQ} \\inf\\left\\{\\int_{X\\times Y} c(x,y)\\,d\\lambda\\ :\\ \\lambda \\in\\Pi(\\mu,\\nu) \\right\\}. \\end{align*} Inspired by the seminal paper \\cite{GANGBOMCCANN2} with applications in shape recognition problem, we first consider \\eqref{MKPEQ} for the cost $c(x,y)=h(x-y)$ with $h$ strictly convex defined on the multi-layers target space \\begin{align*} X=\\overline{X}\\times\\{\\overline{x}\\},\\quad\\text{and}\\quad Y=\\bigcup_{k=1}^K \\left(\\overline{Y}_{k}\\times \\{\\overline{y}_k\\}\\right), \\end{align*} where $\\overline{X}, \\overline{Y}_{k}\\subseteq \\mathbb{R}^{n}$ for $k\\in \\{1,\\ldots,K\\},$ $\\overline{x}\\in \\mathbb{R}$, and $\\{\\overline{y}_1,..., \\overline{y}_K\\}\\subseteq \\mathbb{R}$. Here, we assume that $\\mu|_\\overline{X}\\ll\\mathcal{L}^n$ (the Lebesgue measure on $\\mathbb{R}^n$), but $\\mu$ is singular w.r.t. $\\mathcal{L}^{n+1}$. When $K=1$, this translates to the standard \\eqref{MKPEQ} for which the unique solution is concentrated on a map. We show that for $K\\geq 2,$ the solution is still unique but it concentrates on the graph of several maps. Next, we study \\eqref{MKPEQ} for a closed subset $X\\subseteq \\mathbb{R}^{n+1}$ and its $n$-dimensional submanifold $X_0$ with the first marginal of the form \\begin{align*} \\int_X f(x)\\,d\\mu(x)=\\int_X f(x)\\alpha(x)\\,d\\mathcal{L}^{n+1}(x)+\\int_{X_0} f(x_0)\\,d S(x_0),\\ \\ \\forall f\\in C_b(X). \\end{align*} Here, $S$ is a measure on $X_0$ such that $S\\ll \\mathcal{L}^{n}$ on each coordinate chart of $X_0$. This can be seen as a two-layers problem as the measure $\\mu$ charges both $n$- and $n+1$-dimensional subsets.","sentences":["In this paper, we investigate Monge-Kantorovich problems for which the absolute continuity of marginals is relaxed.","For $X,Y\\subseteq\\mathbb{R}^{n+1}$ let $(X,\\mathcal{B}_X,\\mu)$ and $(Y,\\mathcal{B}_Y,\\nu)$ be two Borel probability spaces, $c:X\\times Y\\to\\mathbb{R}$ be a cost function, and consider the problem \\begin{align*}\\tag{MKP}\\label{MKPEQ} \\inf\\left\\{\\int_{X\\times Y} c(x,y)\\,d\\lambda\\ :\\ \\lambda \\in\\Pi(\\mu,\\nu) \\right\\}.","\\end{align*} Inspired by the seminal paper \\cite{GANGBOMCCANN2} with applications in shape recognition problem, we first consider \\eqref{MKPEQ} for the cost $c(x,y)=h(x-y)$ with $h$ strictly convex defined on the multi-layers target space \\begin{align*} X=\\overline{X}\\times\\{\\overline{x}\\},\\quad\\text{and}\\quad Y=\\bigcup_{k=1}^K \\left(\\overline{Y}_{k}\\times \\{\\overline{y}_k\\}\\right), \\end{align*} where $\\overline{X}, \\overline{Y}_{k}\\subseteq \\mathbb{R}^{n}$ for $k\\in \\{1,\\ldots,K\\},$ $\\overline{x}\\in \\mathbb{R}$, and $\\{\\overline{y}_1,..., \\overline{y}_K\\}\\subseteq \\mathbb{R}$. Here, we assume that $\\mu|_\\overline{X}\\ll\\mathcal{L}^n$ (the Lebesgue measure on $\\mathbb{R}^n$), but $\\mu$ is singular w.r.t.","$\\mathcal{L}^{n+1}$. When $K=1$, this translates to the standard \\eqref{MKPEQ} for which the unique solution is concentrated on a map.","We show that for $K\\geq 2,$ the solution is still unique but it concentrates on the graph of several maps.","Next, we study \\eqref{MKPEQ} for a closed subset $X\\subseteq \\mathbb{R}^{n+1}$ and its $n$-dimensional submanifold $X_0$ with the first marginal of the form \\begin{align*} \\int_X f(x)\\,d\\mu(x)=\\int_X f(x)\\alpha(x)\\,d\\mathcal{L}^{n+1}(x)+\\int_{X_0} f(x_0)\\,d S(x_0),\\ \\ \\forall f\\in C_b(X).","\\end{align*} Here, $S$ is a measure on $X_0$ such that $S\\ll \\mathcal{L}^{n}$ on each coordinate chart of $X_0$. This can be seen as a two-layers problem as the measure $\\mu$ charges both $n$- and $n+1$-dimensional subsets."],"url":"http://arxiv.org/abs/2404.13616v1","category":"math.OC"}
{"created":"2024-04-21 10:50:25","title":"Enhancing ASIC Technology Mapping via Parallel Supergate Computing","abstract":"With the development of large-scale integrated circuits, electronic design automation~(EDA) tools are increasingly emphasizing efficiency, with parallel algorithms becoming a trend. The optimization of delay reduction is a crucial factor for ASIC technology mapping, and supergate technology proves to be an effective method for achieving this in EDA tools flow. However, we have observed that increasing the number of generated supergates can reduce delay, but this comes at the cost of an exponential increase in computation time. In this paper, we propose a parallel supergate computing method that addresses the tradeoff between time-consuming and delay optimization. The proposed method utilizes the input-constrained supergate pattern to parallelly generate the supergate candidates, and then filter the valid supergates as the results. Experiment results show the efficiency of the proposed method, for example, it can attain the improvement of 4x speedup in computation time and 10.1 in delay reduction with 32 threads.","sentences":["With the development of large-scale integrated circuits, electronic design automation~(EDA) tools are increasingly emphasizing efficiency, with parallel algorithms becoming a trend.","The optimization of delay reduction is a crucial factor for ASIC technology mapping, and supergate technology proves to be an effective method for achieving this in EDA tools flow.","However, we have observed that increasing the number of generated supergates can reduce delay, but this comes at the cost of an exponential increase in computation time.","In this paper, we propose a parallel supergate computing method that addresses the tradeoff between time-consuming and delay optimization.","The proposed method utilizes the input-constrained supergate pattern to parallelly generate the supergate candidates, and then filter the valid supergates as the results.","Experiment results show the efficiency of the proposed method, for example, it can attain the improvement of 4x speedup in computation time and 10.1 in delay reduction with 32 threads."],"url":"http://arxiv.org/abs/2404.13614v1","category":"cs.DC"}
{"created":"2024-04-21 09:11:44","title":"The quantile-based classifier with variable-wise parameters","abstract":"Quantile-based classifiers can classify high-dimensional observations by minimising a discrepancy of an observation to a class based on suitable quantiles of the within-class distributions, corresponding to a unique percentage for all variables. The present work extends these classifiers by introducing a way to determine potentially different optimal percentages for different variables. Furthermore, a variable-wise scale parameter is introduced. A simple greedy algorithm to estimate the parameters is proposed. Their consistency in a nonparametric setting is proved. Experiments using artificially generated and real data confirm the potential of the quantile-based classifier with variable-wise parameters.","sentences":["Quantile-based classifiers can classify high-dimensional observations by minimising a discrepancy of an observation to a class based on suitable quantiles of the within-class distributions, corresponding to a unique percentage for all variables.","The present work extends these classifiers by introducing a way to determine potentially different optimal percentages for different variables.","Furthermore, a variable-wise scale parameter is introduced.","A simple greedy algorithm to estimate the parameters is proposed.","Their consistency in a nonparametric setting is proved.","Experiments using artificially generated and real data confirm the potential of the quantile-based classifier with variable-wise parameters."],"url":"http://arxiv.org/abs/2404.13589v1","category":"stat.ME"}
{"created":"2024-04-21 07:47:31","title":"Facility Location Problems with Capacity Constraints: Two Facilities and Beyond","abstract":"In this paper, we investigate the Mechanism Design aspects of the $m$-Capacitated Facility Location Problem ($m$-CFLP) on a line. We focus on two frameworks. In the first framework, the number of facilities is arbitrary, all facilities have the same capacity, and the number of agents is equal to the total capacity of all facilities. In the second framework, we aim to place two facilities, each with a capacity of at least half of the total agents. For both of these frameworks, we propose truthful mechanisms with bounded approximation ratios with respect to the Social Cost (SC) and the Maximum Cost (MC). When $m>2$, the result sharply contrasts with the impossibility results known for the classic $m$-Facility Location Problem \\cite{fotakis2014power}, where capacity constraints are not considered. Furthermore, all our mechanisms are (i) optimal with respect to the MC (ii) optimal or nearly optimal with respect to the SC among anonymous mechanisms. For both frameworks, we provide a lower bound on the approximation ratio that any truthful and deterministic mechanism can achieve with respect to the SC and MC.","sentences":["In this paper, we investigate the Mechanism Design aspects of the $m$-Capacitated Facility Location Problem ($m$-CFLP) on a line.","We focus on two frameworks.","In the first framework, the number of facilities is arbitrary, all facilities have the same capacity, and the number of agents is equal to the total capacity of all facilities.","In the second framework, we aim to place two facilities, each with a capacity of at least half of the total agents.","For both of these frameworks, we propose truthful mechanisms with bounded approximation ratios with respect to the Social Cost (SC) and the Maximum Cost (MC).","When $m>2$, the result sharply contrasts with the impossibility results known for the classic $m$-Facility Location Problem \\cite{fotakis2014power}, where capacity constraints are not considered.","Furthermore, all our mechanisms are (i) optimal with respect to the MC (ii) optimal or nearly optimal with respect to the SC among anonymous mechanisms.","For both frameworks, we provide a lower bound on the approximation ratio that any truthful and deterministic mechanism can achieve with respect to the SC and MC."],"url":"http://arxiv.org/abs/2404.13566v1","category":"cs.GT"}
{"created":"2024-04-21 07:22:09","title":"Mechanical quadrature squeezing beyond the 3dB limit via quantum learning control","abstract":"The preparation of mechanical quadrature-squeezed states holds significant importance in cavity optomechanics because the squeezed states have extensive applications in elucidating fundamental quantum mechanics and exploiting modern quantum technonogy. Here, we propose a reliable scheme for generating mechanical quadrature squeezing in a typical cavity optomechanical system via seeking for optimal cavity-field driving pulses using a quantum-learning-control method. We realize strong quadrature squeezing exceeded the 3 dB steady-state limit in the mechanical resonator subjected to thermal noise with one hundred thermal phonons. Furthermore, the mechanical squeezing can be ultrafastly created within one mechanical oscillation period. We also obtain the optimal pulsed drivings associated with the created mechanical squeezings and analyze the mechanism for mechanical squeezing generation. This work will motivate succeeding applications of optimal quantum control in quantum optics and quantum information science.","sentences":["The preparation of mechanical quadrature-squeezed states holds significant importance in cavity optomechanics because the squeezed states have extensive applications in elucidating fundamental quantum mechanics and exploiting modern quantum technonogy.","Here, we propose a reliable scheme for generating mechanical quadrature squeezing in a typical cavity optomechanical system via seeking for optimal cavity-field driving pulses using a quantum-learning-control method.","We realize strong quadrature squeezing exceeded the 3 dB steady-state limit in the mechanical resonator subjected to thermal noise with one hundred thermal phonons.","Furthermore, the mechanical squeezing can be ultrafastly created within one mechanical oscillation period.","We also obtain the optimal pulsed drivings associated with the created mechanical squeezings and analyze the mechanism for mechanical squeezing generation.","This work will motivate succeeding applications of optimal quantum control in quantum optics and quantum information science."],"url":"http://arxiv.org/abs/2404.13563v1","category":"quant-ph"}
{"created":"2024-04-21 06:03:43","title":"E-QGen: Educational Lecture Abstract-based Question Generation System","abstract":"To optimize the preparation process for educators in academic lectures and associated question-and-answer sessions, this paper presents E-QGen, a lecture abstract-based question generation system. Given a lecture abstract, E-QGen generates potential student inquiries. The questions suggested by our system are expected to not only facilitate teachers in preparing answers in advance but also enable them to supply additional resources when necessary.","sentences":["To optimize the preparation process for educators in academic lectures and associated question-and-answer sessions, this paper presents E-QGen, a lecture abstract-based question generation system.","Given a lecture abstract, E-QGen generates potential student inquiries.","The questions suggested by our system are expected to not only facilitate teachers in preparing answers in advance but also enable them to supply additional resources when necessary."],"url":"http://arxiv.org/abs/2404.13547v1","category":"cs.CL"}
{"created":"2024-04-21 05:59:04","title":"Faster Post-Quantum TLS 1.3 Based on ML-KEM: Implementation and Assessment","abstract":"TLS is extensively utilized for secure data transmission over networks. However, with the advent of quantum computers, the security of TLS based on traditional public-key cryptography is under threat. To counter quantum threats, it is imperative to integrate post-quantum algorithms into TLS. Most PQ-TLS research focuses on integration and evaluation, but few studies address the improvement of PQ-TLS performance by optimizing PQC implementation. For the TLS protocol, handshake performance is crucial, and for post-quantum TLS (PQ-TLS) the performance of post-quantum key encapsulation mechanisms (KEMs) directly impacts handshake performance. In this work, we explore the impact of post-quantum KEMs on PQ-TLS performance. We explore how to improve ML-KEM performance using the latest Intel's Advanced Vector Extensions instruction set AVX-512. We detail a spectrum of techniques devised to parallelize polynomial multiplication, modular reduction, and other computationally intensive modules within ML-KEM. Our optimized ML-KEM implementation achieves up to 1.64x speedup compared to the latest AVX2 implementation. Furthermore, we introduce a novel batch key generation method for ML-KEM that can seamlessly integrate into the TLS protocols. The batch method accelerates the key generation procedure by 3.5x to 4.9x. We integrate the optimized AVX-512 implementation of ML-KEM into TLS 1.3, and assess handshake performance under both PQ-only and hybrid modes. The assessment demonstrates that our faster ML-KEM implementation results in a higher number of TLS 1.3 handshakes per second under both modes. Additionally, we revisit two IND-1-CCA KEM constructions discussed in Eurocrypt22 and Asiacrypt23. Besides, we implement them based on ML-KEM and integrate the one of better performance into TLS 1.3 with benchmarks.","sentences":["TLS is extensively utilized for secure data transmission over networks.","However, with the advent of quantum computers, the security of TLS based on traditional public-key cryptography is under threat.","To counter quantum threats, it is imperative to integrate post-quantum algorithms into TLS.","Most PQ-TLS research focuses on integration and evaluation, but few studies address the improvement of PQ-TLS performance by optimizing PQC implementation.","For the TLS protocol, handshake performance is crucial, and for post-quantum TLS (PQ-TLS) the performance of post-quantum key encapsulation mechanisms (KEMs) directly impacts handshake performance.","In this work, we explore the impact of post-quantum KEMs on PQ-TLS performance.","We explore how to improve ML-KEM performance using the latest Intel's Advanced Vector Extensions instruction set AVX-512.","We detail a spectrum of techniques devised to parallelize polynomial multiplication, modular reduction, and other computationally intensive modules within ML-KEM.","Our optimized ML-KEM implementation achieves up to 1.64x speedup compared to the latest AVX2 implementation.","Furthermore, we introduce a novel batch key generation method for ML-KEM that can seamlessly integrate into the TLS protocols.","The batch method accelerates the key generation procedure by 3.5x to 4.9x.","We integrate the optimized AVX-512 implementation of ML-KEM into TLS 1.3, and assess handshake performance under both PQ-only and hybrid modes.","The assessment demonstrates that our faster ML-KEM implementation results in a higher number of TLS 1.3 handshakes per second under both modes.","Additionally, we revisit two IND-1-CCA KEM constructions discussed in Eurocrypt22 and Asiacrypt23.","Besides, we implement them based on ML-KEM and integrate the one of better performance into TLS 1.3 with benchmarks."],"url":"http://arxiv.org/abs/2404.13544v2","category":"cs.CR"}
{"created":"2024-04-21 04:53:42","title":"Towards Parameter-free Distributed Optimization: a Port-Hamiltonian Approach","abstract":"This paper introduces a novel distributed optimization technique for networked systems, which removes the dependency on specific parameter choices, notably the learning rate. Traditional parameter selection strategies in distributed optimization often lead to conservative performance, characterized by slow convergence or even divergence if parameters are not properly chosen. In this work, we propose a systems theory tool based on the port-Hamiltonian formalism to design algorithms for consensus optimization programs. Moreover, we propose the Mixed Implicit Discretization (MID), which transforms the continuous-time port-Hamiltonian system into a discrete time one, maintaining the same convergence properties regardless of the step size parameter. The consensus optimization algorithm enhances the convergence speed without worrying about the relationship between parameters and stability. Numerical experiments demonstrate the method's superior performance in convergence speed, outperforming other methods, especially in scenarios where conventional methods fail due to step size parameter limitations.","sentences":["This paper introduces a novel distributed optimization technique for networked systems, which removes the dependency on specific parameter choices, notably the learning rate.","Traditional parameter selection strategies in distributed optimization often lead to conservative performance, characterized by slow convergence or even divergence if parameters are not properly chosen.","In this work, we propose a systems theory tool based on the port-Hamiltonian formalism to design algorithms for consensus optimization programs.","Moreover, we propose the Mixed Implicit Discretization (MID), which transforms the continuous-time port-Hamiltonian system into a discrete time one, maintaining the same convergence properties regardless of the step size parameter.","The consensus optimization algorithm enhances the convergence speed without worrying about the relationship between parameters and stability.","Numerical experiments demonstrate the method's superior performance in convergence speed, outperforming other methods, especially in scenarios where conventional methods fail due to step size parameter limitations."],"url":"http://arxiv.org/abs/2404.13529v1","category":"math.OC"}
{"created":"2024-04-21 03:09:40","title":"Planning of Truck Platooning for Road-Network Capacitated Vehicle Routing Problem","abstract":"Truck platooning, a linking technology of trucks on the highway, has gained enormous attention in recent years due to its benefits in energy and operation cost savings. However, most existing studies on truck platooning limit their focus on scenarios in which each truck can serve only one customer demand and is thus with a specified origin-destination pair, so only routing and time schedules are considered. Nevertheless, in real-world logistics, each truck may need to serve multiple customers located at different places, and the operator has to determine not only the routing and time schedules of each truck but also the set of customers allocated to each truck and their sequence to visit. This is well known as a capacitated vehicle routing problem with time windows (CVRPTW), and considering the application of truck platooning in such a problem entails new modeling frameworks and tailored solution algorithms. In light of this, this study makes the first attempt to optimize the truck platooning plan for a road-network CVRPTW to minimize the total operation cost, including vehicles' fixed dispatch cost and energy cost, while fulfilling all delivery demands within their time window constraints. Specifically, the operation plan will dictate the number of trucks to be dispatched, the set of customers, and the routing and time schedules for each truck. In addition, the modeling framework is constructed based on a road network instead of a traditional customer node graph to better resemble and facilitate the platooning operation. A 3-stage algorithm embedded with a \"route-then-schedule\" scheme, dynamic programming, and modified insertion heuristic, is developed to solve the proposed model in a timely manner. Numerical experiments are conducted to validate the modeling framework, demonstrate the performance of the proposed solution algorithm, and quantify the benefit of truck platooning.","sentences":["Truck platooning, a linking technology of trucks on the highway, has gained enormous attention in recent years due to its benefits in energy and operation cost savings.","However, most existing studies on truck platooning limit their focus on scenarios in which each truck can serve only one customer demand and is thus with a specified origin-destination pair, so only routing and time schedules are considered.","Nevertheless, in real-world logistics, each truck may need to serve multiple customers located at different places, and the operator has to determine not only the routing and time schedules of each truck but also the set of customers allocated to each truck and their sequence to visit.","This is well known as a capacitated vehicle routing problem with time windows (CVRPTW), and considering the application of truck platooning in such a problem entails new modeling frameworks and tailored solution algorithms.","In light of this, this study makes the first attempt to optimize the truck platooning plan for a road-network CVRPTW to minimize the total operation cost, including vehicles' fixed dispatch cost and energy cost, while fulfilling all delivery demands within their time window constraints.","Specifically, the operation plan will dictate the number of trucks to be dispatched, the set of customers, and the routing and time schedules for each truck.","In addition, the modeling framework is constructed based on a road network instead of a traditional customer node graph to better resemble and facilitate the platooning operation.","A 3-stage algorithm embedded with a \"route-then-schedule\" scheme, dynamic programming, and modified insertion heuristic, is developed to solve the proposed model in a timely manner.","Numerical experiments are conducted to validate the modeling framework, demonstrate the performance of the proposed solution algorithm, and quantify the benefit of truck platooning."],"url":"http://arxiv.org/abs/2404.13512v1","category":"math.OC"}
