{"created":"2024-04-30 17:58:29","title":"KAN: Kolmogorov-Arnold Networks","abstract":"Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.","sentences":["Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).","While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\").","KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline.","We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability.","For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving.","Theoretically and empirically, KANs possess faster neural scaling laws than MLPs.","For interpretability, KANs can be intuitively visualized and can easily interact with human users.","Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws.","In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs."],"url":"http://arxiv.org/abs/2404.19756v1","category":"cs.LG"}
{"created":"2024-04-30 17:56:24","title":"DOCCI: Descriptions of Connected and Contrasting Images","abstract":"Vision-language datasets are vital for both text-to-image (T2I) and image-to-text (I2T) research. However, current datasets lack descriptions with fine-grained detail that would allow for richer associations to be learned by models. To fill the gap, we introduce Descriptions of Connected and Contrasting Images (DOCCI), a dataset with long, human-annotated English descriptions for 15k images that were taken, curated and donated by a single researcher intent on capturing key challenges such as spatial relations, counting, text rendering, world knowledge, and more. We instruct human annotators to create comprehensive descriptions for each image; these average 136 words in length and are crafted to clearly distinguish each image from those that are related or similar. Each description is highly compositional and typically encompasses multiple challenges. Through both quantitative and qualitative analyses, we demonstrate that DOCCI serves as an effective training resource for image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for text-to-image generation, highlighting the limitations of current text-to-image models in capturing long descriptions and fine details.","sentences":["Vision-language datasets are vital for both text-to-image (T2I) and image-to-text (I2T) research.","However, current datasets lack descriptions with fine-grained detail that would allow for richer associations to be learned by models.","To fill the gap, we introduce Descriptions of Connected and Contrasting Images (DOCCI), a dataset with long, human-annotated English descriptions for 15k images that were taken, curated and donated by a single researcher intent on capturing key challenges such as spatial relations, counting, text rendering, world knowledge, and more.","We instruct human annotators to create comprehensive descriptions for each image; these average 136 words in length and are crafted to clearly distinguish each image from those that are related or similar.","Each description is highly compositional and typically encompasses multiple challenges.","Through both quantitative and qualitative analyses, we demonstrate that DOCCI serves as an effective training resource for image-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or superior results compared to highly-performant larger models like LLaVA-1.5 7B and InstructBLIP 7B.","Furthermore, we show that DOCCI is a useful testbed for text-to-image generation, highlighting the limitations of current text-to-image models in capturing long descriptions and fine details."],"url":"http://arxiv.org/abs/2404.19753v1","category":"cs.CV"}
{"created":"2024-04-30 17:55:02","title":"A Joint Communication and Computation Design for Distributed RISs Assisted Probabilistic Semantic Communication in IIoT","abstract":"In this paper, the problem of spectral-efficient communication and computation resource allocation for distributed reconfigurable intelligent surfaces (RISs) assisted probabilistic semantic communication (PSC) in industrial Internet-of-Things (IIoT) is investigated. In the considered model, multiple RISs are deployed to serve multiple users, while PSC adopts compute-then-transmit protocol to reduce the transmission data size. To support high-rate transmission, the semantic compression ratio, transmit power allocation, and distributed RISs deployment must be jointly considered. This joint communication and computation problem is formulated as an optimization problem whose goal is to maximize the sum semantic-aware transmission rate of the system under total transmit power, phase shift, RIS-user association, and semantic compression ratio constraints. To solve this problem, a many-to-many matching scheme is proposed to solve the RIS-user association subproblem, the semantic compression ratio subproblem is addressed following greedy policy, while the phase shift of RIS can be optimized using the tensor based beamforming. Numerical results verify the superiority of the proposed algorithm.","sentences":["In this paper, the problem of spectral-efficient communication and computation resource allocation for distributed reconfigurable intelligent surfaces (RISs) assisted probabilistic semantic communication (PSC) in industrial Internet-of-Things (IIoT) is investigated.","In the considered model, multiple RISs are deployed to serve multiple users, while PSC adopts compute-then-transmit protocol to reduce the transmission data size.","To support high-rate transmission, the semantic compression ratio, transmit power allocation, and distributed RISs deployment must be jointly considered.","This joint communication and computation problem is formulated as an optimization problem whose goal is to maximize the sum semantic-aware transmission rate of the system under total transmit power, phase shift, RIS-user association, and semantic compression ratio constraints.","To solve this problem, a many-to-many matching scheme is proposed to solve the RIS-user association subproblem, the semantic compression ratio subproblem is addressed following greedy policy, while the phase shift of RIS can be optimized using the tensor based beamforming.","Numerical results verify the superiority of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.19750v1","category":"cs.IT"}
{"created":"2024-04-30 17:52:31","title":"Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning","abstract":"Every year, plant parasitic nematodes, one of the major groups of plant pathogens, cause a significant loss of crops worldwide. To mitigate crop yield losses caused by nematodes, an efficient nematode monitoring method is essential for plant and crop disease management. In other respects, efficient nematode detection contributes to medical research and drug discovery, as nematodes are model organisms. With the rapid development of computer technology, computer vision techniques provide a feasible solution for quantifying nematodes or nematode infections. In this paper, we survey and categorise the studies and available datasets on nematode detection through deep-learning models. To stimulate progress in related research, this survey presents the potential state-of-the-art object detection models, training techniques, optimisation techniques, and evaluation metrics for deep learning beginners. Moreover, seven state-of-the-art object detection models are validated on three public datasets and the AgriNema dataset for plant parasitic nematodes to construct a baseline for nematode detection.","sentences":["Every year, plant parasitic nematodes, one of the major groups of plant pathogens, cause a significant loss of crops worldwide.","To mitigate crop yield losses caused by nematodes, an efficient nematode monitoring method is essential for plant and crop disease management.","In other respects, efficient nematode detection contributes to medical research and drug discovery, as nematodes are model organisms.","With the rapid development of computer technology, computer vision techniques provide a feasible solution for quantifying nematodes or nematode infections.","In this paper, we survey and categorise the studies and available datasets on nematode detection through deep-learning models.","To stimulate progress in related research, this survey presents the potential state-of-the-art object detection models, training techniques, optimisation techniques, and evaluation metrics for deep learning beginners.","Moreover, seven state-of-the-art object detection models are validated on three public datasets and the AgriNema dataset for plant parasitic nematodes to construct a baseline for nematode detection."],"url":"http://arxiv.org/abs/2404.19748v1","category":"cs.CV"}
{"created":"2024-04-30 17:44:44","title":"PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for Privacy Policy Compliance Verification","abstract":"Data protection and privacy is becoming increasingly crucial in the digital era. Numerous companies depend on third-party vendors and service providers to carry out critical functions within their operations, encompassing tasks such as data handling and storage. However, this reliance introduces potential vulnerabilities, as these vendors' security measures and practices may not always align with the standards expected by regulatory bodies. Businesses are required, often under the penalty of law, to ensure compliance with the evolving regulatory rules. Interpreting and implementing these regulations pose challenges due to their complexity. Regulatory documents are extensive, demanding significant effort for interpretation, while vendor-drafted privacy policies often lack the detail required for full legal compliance, leading to ambiguity. To ensure a concise interpretation of the regulatory requirements and compliance of organizational privacy policy with said regulations, we propose a Large Language Model (LLM) and Semantic Web based approach for privacy compliance. In this paper, we develop the novel Privacy Policy Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to efficiently store and retrieve comprehensive information concerning privacy policies, regulatory frameworks, and domain-specific knowledge pertaining to the legal landscape of privacy. Using Retrieval Augmented Generation, we identify the relevant sections in a privacy policy with corresponding regulatory rules. This information about individual privacy policies is populated into the PrivComp-KG. Combining this with the domain context and rules, the PrivComp-KG can be queried to check for compliance with privacy policies by each vendor against relevant policy regulations. We demonstrate the relevance of the PrivComp-KG, by verifying compliance of privacy policy documents for various organizations.","sentences":["Data protection and privacy is becoming increasingly crucial in the digital era.","Numerous companies depend on third-party vendors and service providers to carry out critical functions within their operations, encompassing tasks such as data handling and storage.","However, this reliance introduces potential vulnerabilities, as these vendors' security measures and practices may not always align with the standards expected by regulatory bodies.","Businesses are required, often under the penalty of law, to ensure compliance with the evolving regulatory rules.","Interpreting and implementing these regulations pose challenges due to their complexity.","Regulatory documents are extensive, demanding significant effort for interpretation, while vendor-drafted privacy policies often lack the detail required for full legal compliance, leading to ambiguity.","To ensure a concise interpretation of the regulatory requirements and compliance of organizational privacy policy with said regulations, we propose a Large Language Model (LLM) and Semantic Web based approach for privacy compliance.","In this paper, we develop the novel Privacy Policy Compliance Verification Knowledge Graph, PrivComp-KG.","It is designed to efficiently store and retrieve comprehensive information concerning privacy policies, regulatory frameworks, and domain-specific knowledge pertaining to the legal landscape of privacy.","Using Retrieval Augmented Generation, we identify the relevant sections in a privacy policy with corresponding regulatory rules.","This information about individual privacy policies is populated into the PrivComp-KG.","Combining this with the domain context and rules, the PrivComp-KG can be queried to check for compliance with privacy policies by each vendor against relevant policy regulations.","We demonstrate the relevance of the PrivComp-KG, by verifying compliance of privacy policy documents for various organizations."],"url":"http://arxiv.org/abs/2404.19744v1","category":"cs.CR"}
{"created":"2024-04-30 17:28:05","title":"Iterative Reasoning Preference Optimization","abstract":"Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy for Llama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting out of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on ARC-Challenge, which outperforms other Llama-2-based models not relying on additionally sourced datasets.","sentences":["Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024).","In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer.","We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial.","We show reasoning improves across repeated iterations of this scheme.","While only relying on examples in the training set, our approach results in increasing accuracy for Llama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting out of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on ARC-Challenge, which outperforms other Llama-2-based models not relying on additionally sourced datasets."],"url":"http://arxiv.org/abs/2404.19733v1","category":"cs.CL"}
{"created":"2024-04-30 17:24:55","title":"A Framework for Leveraging Human Computation Gaming to Enhance Knowledge Graphs for Accuracy Critical Generative AI Applications","abstract":"External knowledge graphs (KGs) can be used to augment large language models (LLMs), while simultaneously providing an explainable knowledge base of facts that can be inspected by a human. This approach may be particularly valuable in domains where explainability is critical, like human trafficking data analysis. However, creating KGs can pose challenges. KGs parsed from documents may comprise explicit connections (those directly stated by a document) but miss implicit connections (those obvious to a human although not directly stated). To address these challenges, this preliminary research introduces the GAME-KG framework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge Graphs.\" GAME-KG is a federated approach to modifying explicit as well as implicit connections in KGs by using crowdsourced feedback collected through video games. GAME-KG is shown through two demonstrations: a Unity test scenario from Dark Shadows, a video game that collects feedback on KGs parsed from US Department of Justice (DOJ) Press Releases on human trafficking, and a following experiment where OpenAI's GPT-4 is prompted to answer questions based on a modified and unmodified KG. Initial results suggest that GAME-KG can be an effective framework for enhancing KGs, while simultaneously providing an explainable set of structured facts verified by humans.","sentences":["External knowledge graphs (KGs) can be used to augment large language models (LLMs), while simultaneously providing an explainable knowledge base of facts that can be inspected by a human.","This approach may be particularly valuable in domains where explainability is critical, like human trafficking data analysis.","However, creating KGs can pose challenges.","KGs parsed from documents may comprise explicit connections (those directly stated by a document) but miss implicit connections (those obvious to a human although not directly stated).","To address these challenges, this preliminary research introduces the GAME-KG framework, standing for \"Gaming for Augmenting Metadata and Enhancing Knowledge Graphs.\"","GAME-KG is a federated approach to modifying explicit as well as implicit connections in KGs by using crowdsourced feedback collected through video games.","GAME-KG is shown through two demonstrations: a Unity test scenario from Dark Shadows, a video game that collects feedback on KGs parsed from US Department of Justice (DOJ) Press Releases on human trafficking, and a following experiment where OpenAI's GPT-4 is prompted to answer questions based on a modified and unmodified KG.","Initial results suggest that GAME-KG can be an effective framework for enhancing KGs, while simultaneously providing an explainable set of structured facts verified by humans."],"url":"http://arxiv.org/abs/2404.19729v1","category":"cs.HC"}
{"created":"2024-04-30 17:19:52","title":"Fairness Without Demographics in Human-Centered Federated Learning","abstract":"Federated learning (FL) enables collaborative model training while preserving data privacy, making it suitable for decentralized human-centered AI applications. However, a significant research gap remains in ensuring fairness in these systems. Current fairness strategies in FL require knowledge of bias-creating/sensitive attributes, clashing with FL's privacy principles. Moreover, in human-centered datasets, sensitive attributes may remain latent. To tackle these challenges, we present a novel bias mitigation approach inspired by \"Fairness without Demographics\" in machine learning. The presented approach achieves fairness without needing knowledge of sensitive attributes by minimizing the top eigenvalue of the Hessian matrix during training, ensuring equitable loss landscapes across FL participants. Notably, we introduce a novel FL aggregation scheme that promotes participating models based on error rates and loss landscape curvature attributes, fostering fairness across the FL system. This work represents the first approach to attaining \"Fairness without Demographics\" in human-centered FL. Through comprehensive evaluation, our approach demonstrates effectiveness in balancing fairness and efficacy across various real-world applications, FL setups, and scenarios involving single and multiple bias-inducing factors, representing a significant advancement in human-centered FL.","sentences":["Federated learning (FL) enables collaborative model training while preserving data privacy, making it suitable for decentralized human-centered AI applications.","However, a significant research gap remains in ensuring fairness in these systems.","Current fairness strategies in FL require knowledge of bias-creating/sensitive attributes, clashing with FL's privacy principles.","Moreover, in human-centered datasets, sensitive attributes may remain latent.","To tackle these challenges, we present a novel bias mitigation approach inspired by \"Fairness without Demographics\" in machine learning.","The presented approach achieves fairness without needing knowledge of sensitive attributes by minimizing the top eigenvalue of the Hessian matrix during training, ensuring equitable loss landscapes across FL participants.","Notably, we introduce a novel FL aggregation scheme that promotes participating models based on error rates and loss landscape curvature attributes, fostering fairness across the FL system.","This work represents the first approach to attaining \"Fairness without Demographics\" in human-centered FL.","Through comprehensive evaluation, our approach demonstrates effectiveness in balancing fairness and efficacy across various real-world applications, FL setups, and scenarios involving single and multiple bias-inducing factors, representing a significant advancement in human-centered FL."],"url":"http://arxiv.org/abs/2404.19725v1","category":"cs.LG"}
{"created":"2024-04-30 17:11:54","title":"PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games","abstract":"This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer's high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs). Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative. The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses. PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative. A novel validation system that uses the LLM's intelligence evaluates text input and aligns generated responses with the unfolding narrative. Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative. For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs. PANGeA's ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game. These are, a custom, browser-based GPT and a Unity demo. As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input.","sentences":["This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer's high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs).","Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative.","The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses.","PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative.","A novel validation system that uses the LLM's intelligence evaluates text input and aligns generated responses with the unfolding narrative.","Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative.","For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs.","PANGeA's ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game.","These are, a custom, browser-based GPT and a Unity demo.","As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input."],"url":"http://arxiv.org/abs/2404.19721v1","category":"cs.AI"}
{"created":"2024-04-30 17:06:27","title":"Assessing LLMs in Malicious Code Deobfuscation of Real-world Malware Campaigns","abstract":"The integration of large language models (LLMs) into various pipelines is increasingly widespread, effectively automating many manual tasks and often surpassing human capabilities. Cybersecurity researchers and practitioners have recognised this potential. Thus, they are actively exploring its applications, given the vast volume of heterogeneous data that requires processing to identify anomalies, potential bypasses, attacks, and fraudulent incidents. On top of this, LLMs' advanced capabilities in generating functional code, comprehending code context, and summarising its operations can also be leveraged for reverse engineering and malware deobfuscation. To this end, we delve into the deobfuscation capabilities of state-of-the-art LLMs. Beyond merely discussing a hypothetical scenario, we evaluate four LLMs with real-world malicious scripts used in the notorious Emotet malware campaign. Our results indicate that while not absolutely accurate yet, some LLMs can efficiently deobfuscate such payloads. Thus, fine-tuning LLMs for this task can be a viable potential for future AI-powered threat intelligence pipelines in the fight against obfuscated malware.","sentences":["The integration of large language models (LLMs) into various pipelines is increasingly widespread, effectively automating many manual tasks and often surpassing human capabilities.","Cybersecurity researchers and practitioners have recognised this potential.","Thus, they are actively exploring its applications, given the vast volume of heterogeneous data that requires processing to identify anomalies, potential bypasses, attacks, and fraudulent incidents.","On top of this, LLMs' advanced capabilities in generating functional code, comprehending code context, and summarising its operations can also be leveraged for reverse engineering and malware deobfuscation.","To this end, we delve into the deobfuscation capabilities of state-of-the-art LLMs.","Beyond merely discussing a hypothetical scenario, we evaluate four LLMs with real-world malicious scripts used in the notorious Emotet malware campaign.","Our results indicate that while not absolutely accurate yet, some LLMs can efficiently deobfuscate such payloads.","Thus, fine-tuning LLMs for this task can be a viable potential for future AI-powered threat intelligence pipelines in the fight against obfuscated malware."],"url":"http://arxiv.org/abs/2404.19715v1","category":"cs.CR"}
{"created":"2024-04-30 17:00:32","title":"Harmonic LLMs are Trustworthy","abstract":"We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM in real-time, based upon the local deviation from harmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the first completely model-agnostic and unsupervised method of measuring the robustness of any given response from an LLM, based upon the model itself conforming to a purely mathematical standard. We conduct human annotation experiments to show the positive correlation of $\\gamma$ with false or misleading answers, and demonstrate that following the gradient of $\\gamma$ in stochastic gradient ascent efficiently exposes adversarial prompts. Measuring $\\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT, Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory answers automatically and quantitatively rank the reliability of these models in various objective domains (Web QA, TruthfulQA, and Programming QA). Across all models and domains tested, human ratings confirm that $\\gamma \\to 0$ indicates trustworthiness, and the low-$\\gamma$ leaders among these models are GPT-4, ChatGPT, and Smaug-72B.","sentences":["We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM in real-time, based upon the local deviation from harmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the first completely model-agnostic and unsupervised method of measuring the robustness of any given response from an LLM, based upon the model itself conforming to a purely mathematical standard.","We conduct human annotation experiments to show the positive correlation of $\\gamma$ with false or misleading answers, and demonstrate that following the gradient of $\\gamma$ in stochastic gradient ascent efficiently exposes adversarial prompts.","Measuring $\\gamma$ across thousands of queries in popular LLMs (GPT-4, ChatGPT, Claude-2.1, Mixtral-8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory answers automatically and quantitatively rank the reliability of these models in various objective domains (Web QA, TruthfulQA, and Programming QA).","Across all models and domains tested, human ratings confirm that $\\gamma \\to 0$ indicates trustworthiness, and the low-$\\gamma$ leaders among these models are GPT-4, ChatGPT, and Smaug-72B."],"url":"http://arxiv.org/abs/2404.19708v1","category":"cs.LG"}
{"created":"2024-04-30 16:45:54","title":"Generative AI Usage and Academic Performance","abstract":"This study evaluates the impact of students' usage of generative artificial intelligence (GenAI) tools such as ChatGPT on their academic performance. We analyze student essays using GenAI detection systems to identify GenAI users among the cohort. Employing multivariate regression analysis, we find that students using GenAI tools score on average 6.71 (out of 100) points lower than non-users. While GenAI tools may offer benefits for learning and engagement, the way students actually use it correlates with diminished academic outcomes. Exploring the underlying mechanism, additional analyses show that the effect is particularly detrimental to students with high learning potential, suggesting an effect whereby GenAI tool usage hinders learning. Our findings provide important empirical evidence for the ongoing debate on the integration of GenAI in higher education and underscores the necessity for educators, institutions, and policymakers to carefully consider its implications for student performance.","sentences":["This study evaluates the impact of students' usage of generative artificial intelligence (GenAI) tools such as ChatGPT on their academic performance.","We analyze student essays using GenAI detection systems to identify GenAI users among the cohort.","Employing multivariate regression analysis, we find that students using GenAI tools score on average 6.71 (out of 100) points lower than non-users.","While GenAI tools may offer benefits for learning and engagement, the way students actually use it correlates with diminished academic outcomes.","Exploring the underlying mechanism, additional analyses show that the effect is particularly detrimental to students with high learning potential, suggesting an effect whereby GenAI tool usage hinders learning.","Our findings provide important empirical evidence for the ongoing debate on the integration of GenAI in higher education and underscores the necessity for educators, institutions, and policymakers to carefully consider its implications for student performance."],"url":"http://arxiv.org/abs/2404.19699v1","category":"econ.GN"}
{"created":"2024-04-30 16:44:18","title":"Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners","abstract":"3D visual grounding is a challenging task that often requires direct and dense supervision, notably the semantic label for each object in the scene. In this paper, we instead study the naturally supervised setting that learns from only 3D scene and QA pairs, where prior works underperform. We propose the Language-Regularized Concept Learner (LARC), which uses constraints from language as regularization to significantly improve the accuracy of neuro-symbolic concept learners in the naturally supervised setting. Our approach is based on two core insights: the first is that language constraints (e.g., a word's relation to another) can serve as effective regularization for structured representations in neuro-symbolic models; the second is that we can query large language models to distill such constraints from language properties. We show that LARC improves performance of prior works in naturally supervised 3D visual grounding, and demonstrates a wide range of 3D visual reasoning capabilities-from zero-shot composition, to data efficiency and transferability. Our method represents a promising step towards regularizing structured visual reasoning frameworks with language-based priors, for learning in settings without dense supervision.","sentences":["3D visual grounding is a challenging task that often requires direct and dense supervision, notably the semantic label for each object in the scene.","In this paper, we instead study the naturally supervised setting that learns from only 3D scene and QA pairs, where prior works underperform.","We propose the Language-Regularized Concept Learner (LARC), which uses constraints from language as regularization to significantly improve the accuracy of neuro-symbolic concept learners in the naturally supervised setting.","Our approach is based on two core insights: the first is that language constraints (e.g., a word's relation to another) can serve as effective regularization for structured representations in neuro-symbolic models; the second is that we can query large language models to distill such constraints from language properties.","We show that LARC improves performance of prior works in naturally supervised 3D visual grounding, and demonstrates a wide range of 3D visual reasoning capabilities-from zero-shot composition, to data efficiency and transferability.","Our method represents a promising step towards regularizing structured visual reasoning frameworks with language-based priors, for learning in settings without dense supervision."],"url":"http://arxiv.org/abs/2404.19696v1","category":"cs.CV"}
{"created":"2024-04-30 16:18:01","title":"Collaborative Control Method of Transit Signal Priority Based on Cooperative Game and Reinforcement Learning","abstract":"To address the low efficiency in priority signal control within intelligent transportation systems, this study introduces a novel eight-phase priority signal control method, CBQL-TSP, leveraging a hybrid decision-making framework that integrates cooperative game theory and reinforcement learning. This approach conceptualizes the allocation of bus signal priorities as a multi-objective decision-making problem across an eight-phase signal sequence, differentiating between priority and non-priority phases. It employs a cooperative game model to facilitate this differentiation. The developed hybrid decision-making algorithm, CBQL, effectively tackles the multi-objective decision-making challenges inherent in the eight-phase signal sequence. By computing the Shapley value function, it quantifies the marginal contributions of each participant, which in turn inform the construction of a state transition probability equation based on Shapley value ratios. Compared to conventional control methods, the CBQL-TSP method not only upholds the fairness principles of cooperative game theory but also harnesses the adaptive learning capabilities of Q-Learning. This enables dynamic adjustments to signal timing in response to real-time traffic conditions, significantly enhancing the flexibility and efficiency of priority signal control.","sentences":["To address the low efficiency in priority signal control within intelligent transportation systems, this study introduces a novel eight-phase priority signal control method, CBQL-TSP, leveraging a hybrid decision-making framework that integrates cooperative game theory and reinforcement learning.","This approach conceptualizes the allocation of bus signal priorities as a multi-objective decision-making problem across an eight-phase signal sequence, differentiating between priority and non-priority phases.","It employs a cooperative game model to facilitate this differentiation.","The developed hybrid decision-making algorithm, CBQL, effectively tackles the multi-objective decision-making challenges inherent in the eight-phase signal sequence.","By computing the Shapley value function, it quantifies the marginal contributions of each participant, which in turn inform the construction of a state transition probability equation based on Shapley value ratios.","Compared to conventional control methods, the CBQL-TSP method not only upholds the fairness principles of cooperative game theory but also harnesses the adaptive learning capabilities of Q-Learning.","This enables dynamic adjustments to signal timing in response to real-time traffic conditions, significantly enhancing the flexibility and efficiency of priority signal control."],"url":"http://arxiv.org/abs/2404.19683v1","category":"cs.GT"}
{"created":"2024-04-30 16:13:01","title":"Tuning the coherent interaction of an electron qubit and a nuclear magnon","abstract":"A central spin qubit interacting coherently with an ensemble of proximal spins can be used to engineer entangled collective states or a multi-qubit register. Making full use of this many-body platform requires tuning the interaction between the central spin and its spin register. GaAs quantum dots offer a model realization of the central spin system where an electron qubit interacts with multiple ensembles of $\\sim 10^{4}$ nuclear spins. In this work, we demonstrate tuning of the interaction between the electron qubit and the nuclear many-body system in a GaAs quantum dot. The homogeneity of the GaAs system allows us to perform high-precision and isotopically selective nuclear sideband spectroscopy, which reveals the single-nucleus electronic Knight field. Together with time-resolved spectroscopy of the nuclear field, this fully characterizes the electron-nuclear interaction for a priori control. An algorithmic feedback sequence selects the nuclear polarization precisely, which adjusts the electron-nuclear exchange interaction in situ via the electronic g-factor anisotropy. This allows us to tune directly the activation rate of a collective nuclear excitation (magnon) and the coherence time of the electron qubit. Our method is applicable to similar central-spin systems and enables the programmable tuning of coherent interactions in the many-body regime.","sentences":["A central spin qubit interacting coherently with an ensemble of proximal spins can be used to engineer entangled collective states or a multi-qubit register.","Making full use of this many-body platform requires tuning the interaction between the central spin and its spin register.","GaAs quantum dots offer a model realization of the central spin system where an electron qubit interacts with multiple ensembles of $\\sim 10^{4}$ nuclear spins.","In this work, we demonstrate tuning of the interaction between the electron qubit and the nuclear many-body system in a GaAs quantum dot.","The homogeneity of the GaAs system allows us to perform high-precision and isotopically selective nuclear sideband spectroscopy, which reveals the single-nucleus electronic Knight field.","Together with time-resolved spectroscopy of the nuclear field, this fully characterizes the electron-nuclear interaction for a priori control.","An algorithmic feedback sequence selects the nuclear polarization precisely, which adjusts the electron-nuclear exchange interaction in situ via the electronic g-factor anisotropy.","This allows us to tune directly the activation rate of a collective nuclear excitation (magnon) and the coherence time of the electron qubit.","Our method is applicable to similar central-spin systems and enables the programmable tuning of coherent interactions in the many-body regime."],"url":"http://arxiv.org/abs/2404.19679v1","category":"quant-ph"}
{"created":"2024-04-30 16:10:21","title":"A Comprehensive Analysis of Pegasus Spyware and Its Implications for Digital Privacy and Security","abstract":"This paper comprehensively analyzes the Pegasus spyware and its implications for digital privacy and security. The Israeli cyber intelligence company NSO Group's Pegasus has gained recognition as a potent surveillance tool capable of hacking into smartphones and extracting data without the user's knowledge [49], [50]. The research emphasizes the technical aspects of this spyware, its deployment methods, and the controversies surrounding its use. The research also emphasizes the growing worries surrounding digital privacy and security as a result of the prevalent use of advanced spyware. By delving into legal, ethical, and policy issues, the objective of this study is to deliver a holistic understanding of the challenges posed by Pegasus and similar spyware tools. Through a comprehensive examination of the subject, the paper presents potential solutions to mitigate the threats and protect users from invasive surveillance techniques.","sentences":["This paper comprehensively analyzes the Pegasus spyware and its implications for digital privacy and security.","The Israeli cyber intelligence company NSO Group's Pegasus has gained recognition as a potent surveillance tool capable of hacking into smartphones and extracting data without the user's knowledge [49], [50].","The research emphasizes the technical aspects of this spyware, its deployment methods, and the controversies surrounding its use.","The research also emphasizes the growing worries surrounding digital privacy and security as a result of the prevalent use of advanced spyware.","By delving into legal, ethical, and policy issues, the objective of this study is to deliver a holistic understanding of the challenges posed by Pegasus and similar spyware tools.","Through a comprehensive examination of the subject, the paper presents potential solutions to mitigate the threats and protect users from invasive surveillance techniques."],"url":"http://arxiv.org/abs/2404.19677v1","category":"cs.CR"}
{"created":"2024-04-30 16:08:02","title":"The Nature of X-Rays from Young Stellar Objects in the Orion Nebula Cluster -- A Chandra HETGS Legacy Project","abstract":"The Orion Nebula Cluster (ONC) is the closest site of very young ($\\sim$ 1 Myrs) massive star formation. The ONC hosts more than 1600 young and X-ray bright stars with masses ranging from $\\sim$ 0.1 to 35 $M_\\odot$. The Chandra HETGS Orion Legacy Project observed the ONC with the Chandra high energy transmission grating spectrometer (HETGS) for $2.1\\,$Ms. We describe the spectral extraction and cleaning processes necessary to separate overlapping spectra. We obtained 36 high resolution spectra which includes a high brilliance X-ray spectrum of $\\theta^1$ Ori C with over 100 highly significant X-ray lines. The lines show Doppler broadening between 300 and $400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. Higher spectral diffraction orders allow us to resolve line components of high Z He-like triplets in $\\theta^1$ Ori C with unprecedented spectral resolution. Long term light curves spanning $\\sim$20 years show all stars to be highly variable, including the massive stars. Spectral fitting with thermal coronal emission line models reveals that most sources show column densities of up to a few times $10^{22}\\,$cm$^{-2}$ and high coronal temperatures of 10 to 90 MK. We observe a bifurcation of the high temperature component where some stars show a high component of 40 MK, while others show above 60 MK indicating heavy flaring activity. Some lines are resolved with Doppler broadening above our threshold of $\\sim200\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$, up to $500\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. This data set represents the largest collection of HETGS high resolution X-ray spectra from young pre-MS stars in a single star-forming region to date.","sentences":["The Orion Nebula Cluster (ONC) is the closest site of very young ($\\sim$ 1 Myrs) massive star formation.","The ONC hosts more than 1600 young and X-ray bright stars with masses ranging from $\\sim$ 0.1 to 35 $M_\\odot$. The Chandra HETGS Orion Legacy Project observed the ONC with the Chandra high energy transmission grating spectrometer (HETGS) for $2.1\\,$Ms.","We describe the spectral extraction and cleaning processes necessary to separate overlapping spectra.","We obtained 36 high resolution spectra which includes a high brilliance X-ray spectrum of $\\theta^1$ Ori C with over 100 highly significant X-ray lines.","The lines show Doppler broadening between 300 and $400\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. Higher spectral diffraction orders allow us to resolve line components of high Z He-like triplets in $\\theta^1$ Ori C with unprecedented spectral resolution.","Long term light curves spanning $\\sim$20 years show all stars to be highly variable, including the massive stars.","Spectral fitting with thermal coronal emission line models reveals that most sources show column densities of up to a few times $10^{22}\\,$cm$^{-2}$ and high coronal temperatures of 10 to 90 MK.","We observe a bifurcation of the high temperature component where some stars show a high component of 40 MK, while others show above 60 MK indicating heavy flaring activity.","Some lines are resolved with Doppler broadening above our threshold of $\\sim200\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$, up to $500\\;\\mathrm{km}\\;\\mathrm{s}^{-1}$. This data set represents the largest collection of HETGS high resolution X-ray spectra from young pre-MS stars in a single star-forming region to date."],"url":"http://arxiv.org/abs/2404.19676v1","category":"astro-ph.SR"}
{"created":"2024-04-30 16:00:21","title":"ATOMMIC: An Advanced Toolbox for Multitask Medical Imaging Consistency to facilitate Artificial Intelligence applications from acquisition to analysis in Magnetic Resonance Imaging","abstract":"AI is revolutionizing MRI along the acquisition and processing chain. Advanced AI frameworks have been developed to apply AI in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation. Existing frameworks are often designed to perform tasks independently or are focused on specific models or datasets, limiting generalization. We introduce ATOMMIC, an open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis. ATOMMIC implements several tasks using DL networks and enables MultiTask Learning (MTL) to perform related tasks integrated, targeting generalization in the MRI domain. We first review the current state of AI frameworks for MRI through a comprehensive literature search and by parsing 12,479 GitHub repositories. We benchmark 25 DL models on eight publicly available datasets to present distinct applications of ATOMMIC on accelerated MRI reconstruction, image segmentation, quantitative parameter map estimation, and joint accelerated MRI reconstruction and image segmentation utilizing MTL. Our findings demonstrate that ATOMMIC is the only MTL framework with harmonized complex-valued and real-valued data support. Evaluations on single tasks show that physics-based models, which enforce data consistency by leveraging the physical properties of MRI, outperform other models in reconstructing highly accelerated acquisitions. Physics-based models that produce high reconstruction quality can accurately estimate quantitative parameter maps. When high-performing reconstruction models are combined with robust segmentation networks utilizing MTL, performance is improved in both tasks. ATOMMIC facilitates MRI reconstruction and analysis by standardizing workflows, enhancing data interoperability, integrating unique features like MTL, and effectively benchmarking DL models.","sentences":["AI is revolutionizing MRI along the acquisition and processing chain.","Advanced AI frameworks have been developed to apply AI in various successive tasks, such as image reconstruction, quantitative parameter map estimation, and image segmentation.","Existing frameworks are often designed to perform tasks independently or are focused on specific models or datasets, limiting generalization.","We introduce ATOMMIC, an open-source toolbox that streamlines AI applications for accelerated MRI reconstruction and analysis.","ATOMMIC implements several tasks using DL networks and enables MultiTask Learning (MTL) to perform related tasks integrated, targeting generalization in the MRI domain.","We first review the current state of AI frameworks for MRI through a comprehensive literature search and by parsing 12,479 GitHub repositories.","We benchmark 25 DL models on eight publicly available datasets to present distinct applications of ATOMMIC on accelerated MRI reconstruction, image segmentation, quantitative parameter map estimation, and joint accelerated MRI reconstruction and image segmentation utilizing MTL.","Our findings demonstrate that ATOMMIC is the only MTL framework with harmonized complex-valued and real-valued data support.","Evaluations on single tasks show that physics-based models, which enforce data consistency by leveraging the physical properties of MRI, outperform other models in reconstructing highly accelerated acquisitions.","Physics-based models that produce high reconstruction quality can accurately estimate quantitative parameter maps.","When high-performing reconstruction models are combined with robust segmentation networks utilizing MTL, performance is improved in both tasks.","ATOMMIC facilitates MRI reconstruction and analysis by standardizing workflows, enhancing data interoperability, integrating unique features like MTL, and effectively benchmarking DL models."],"url":"http://arxiv.org/abs/2404.19665v1","category":"physics.med-ph"}
{"created":"2024-04-30 15:52:49","title":"Towards Scenario- and Capability-Driven Dataset Development and Evaluation: An Approach in the Context of Mapless Automated Driving","abstract":"The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation. At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems. Sensor-based, mapless automated driving is one of the contexts where this limitation is evident. While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.   To address these challenges, we propose a scenario- and capability-based approach to dataset development. Grounded in the principles of ISO 21448 (safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our approach facilitates the structured derivation of dataset requirements. This not only aids in the development of meaningful new datasets but also enables the effective comparison of existing ones. Applying this methodology to a broad range of existing lane detection datasets, we identify significant limitations in current datasets, particularly in terms of real-world applicability, a lack of labeling of critical features, and an absence of comprehensive information for complex driving maneuvers.","sentences":["The foundational role of datasets in defining the capabilities of deep learning models has led to their rapid proliferation.","At the same time, published research focusing on the process of dataset development for environment perception in automated driving has been scarce, thereby reducing the applicability of openly available datasets and impeding the development of effective environment perception systems.","Sensor-based, mapless automated driving is one of the contexts where this limitation is evident.","While leveraging real-time sensor data, instead of pre-defined HD maps promises enhanced adaptability and safety by effectively navigating unexpected environmental changes, it also increases the demands on the scope and complexity of the information provided by the perception system.   ","To address these challenges, we propose a scenario- and capability-based approach to dataset development.","Grounded in the principles of ISO 21448 (safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our approach facilitates the structured derivation of dataset requirements.","This not only aids in the development of meaningful new datasets but also enables the effective comparison of existing ones.","Applying this methodology to a broad range of existing lane detection datasets, we identify significant limitations in current datasets, particularly in terms of real-world applicability, a lack of labeling of critical features, and an absence of comprehensive information for complex driving maneuvers."],"url":"http://arxiv.org/abs/2404.19656v1","category":"cs.CV"}
{"created":"2024-04-30 15:49:03","title":"VimTS: A Unified Video and Image Text Spotter for Enhancing the Cross-domain Generalization","abstract":"Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization. In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks. Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters. The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task. Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm. Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500. For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data. We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data. The code and datasets will be made available at the https://VimTextSpotter.github.io.","sentences":["Text spotting, a task involving the extraction of textual information from image or video sequences, faces challenges in cross-domain adaption, such as image-to-image and image-to-video generalization.","In this paper, we introduce a new method, termed VimTS, which enhances the generalization ability of the model by achieving better synergy among different tasks.","Typically, we propose a Prompt Queries Generation Module and a Tasks-aware Adapter to effectively convert the original single-task model into a multi-task model suitable for both image and video scenarios with minimal additional parameters.","The Prompt Queries Generation Module facilitates explicit interaction between different tasks, while the Tasks-aware Adapter helps the model dynamically learn suitable features for each task.","Additionally, to further enable the model to learn temporal information at a lower cost, we propose a synthetic video text dataset (VTD-368k) by leveraging the Content Deformation Fields (CoDeF) algorithm.","Notably, our method outperforms the state-of-the-art method by an average of 2.6% in six cross-domain benchmarks such as TT-to-IC15, CTW1500-to-TT, and TT-to-CTW1500.","For video-level cross-domain adaption, our method even surpasses the previous end-to-end video spotting method in ICDAR2015 video and DSText v2 by an average of 5.5% on the MOTA metric, using only image-level data.","We further demonstrate that existing Large Multimodal Models exhibit limitations in generating cross-domain scene text spotting, in contrast to our VimTS model which requires significantly fewer parameters and data.","The code and datasets will be made available at the https://VimTextSpotter.github.io."],"url":"http://arxiv.org/abs/2404.19652v1","category":"cs.CV"}
{"created":"2024-04-30 15:49:01","title":"Provably Robust Conformal Prediction with Improved Efficiency","abstract":"Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d.. Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated. To address this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was first proposed to certify the robustness of conformal prediction methods to adversarial noise. However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets. To address these limitations, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method. Next, we propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the baseline method only yields trivial predictions including full label set, while our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and $16.9\\times$ respectively and provide practical robustness guarantee. Our codes are available at https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.","sentences":["Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d..","Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated.","To address this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was first proposed to certify the robustness of conformal prediction methods to adversarial noise.","However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets.","To address these limitations, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method.","Next, we propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead.","Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the baseline method only yields trivial predictions including full label set, while our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and $16.9\\times$ respectively and provide practical robustness guarantee.","Our codes are available at https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction."],"url":"http://arxiv.org/abs/2404.19651v1","category":"cs.LG"}
{"created":"2024-04-30 15:46:00","title":"A Fully Screen-Printed Vanadium-Dioxide Switches Based Wideband Reconfigurable Intelligent Surface for 5G Bands","abstract":"Reconfigurable Intelligent Surface (RIS) is attracting more and more research interest because of its ability to reprogram the radio environment. Designing and implementing the RIS, however, is challenging because of limitations of printed circuit board (PCB) technology related to manufacturing of large sizes as well as the cost of switches. Thus, a low-cost manufacturing process suitable for large size and volume of devices, such as screen-printing is necessary. In this paper, for the first time, a fully screen-printed reconfigurable intelligent surface (RIS) with vanadium dioxide (VO2) switches for 5G and beyond communications is proposed. A VO2 ink has been prepared and batches of switches have been printed and integrated with the resonator elements. These switches are a fraction of the cost of commercial switches. Furthermore, the printing of these switches directly on metal patterns negates the need of any minute soldering of the switches. To avoid the complications of multilayer printing and realizing the RIS without vias, the resonators and the biasing lines are realized on a single layer. However, this introduces the challenge of interference between the biasing lines and the resonators, which is tackled in this work by designing the bias lines as part of the resonator. By adjusting the unit cell periodicity and the dimension of the H-shaped resonator, we achieve a 220 to 170{\\deg} phase shift from 23.5 GHz to 29.5 GHz covering both n257 and n258 bands. Inside the wide bandwidth, the maximum ON reflection magnitude is 74%, and the maximum OFF magnitude is 94%. The RIS array comprises 20x20 unit cells (4.54x4.54{\\lambda}^2 at 29.5 GHz). Each column of unit cells is serially connected to a current biasing circuit. To validate the array's performance, we conduct full-wave simulations as well as near-field and far-field measurements.","sentences":["Reconfigurable Intelligent Surface (RIS) is attracting more and more research interest because of its ability to reprogram the radio environment.","Designing and implementing the RIS, however, is challenging because of limitations of printed circuit board (PCB) technology related to manufacturing of large sizes as well as the cost of switches.","Thus, a low-cost manufacturing process suitable for large size and volume of devices, such as screen-printing is necessary.","In this paper, for the first time, a fully screen-printed reconfigurable intelligent surface (RIS) with vanadium dioxide (VO2) switches for 5G and beyond communications is proposed.","A VO2 ink has been prepared and batches of switches have been printed and integrated with the resonator elements.","These switches are a fraction of the cost of commercial switches.","Furthermore, the printing of these switches directly on metal patterns negates the need of any minute soldering of the switches.","To avoid the complications of multilayer printing and realizing the RIS without vias, the resonators and the biasing lines are realized on a single layer.","However, this introduces the challenge of interference between the biasing lines and the resonators, which is tackled in this work by designing the bias lines as part of the resonator.","By adjusting the unit cell periodicity and the dimension of the H-shaped resonator, we achieve a 220 to 170{\\deg} phase shift from 23.5 GHz to 29.5 GHz covering both n257 and n258 bands.","Inside the wide bandwidth, the maximum ON reflection magnitude is 74%, and the maximum OFF magnitude is 94%.","The RIS array comprises 20x20 unit cells (4.54x4.54{\\lambda}^2 at 29.5 GHz).","Each column of unit cells is serially connected to a current biasing circuit.","To validate the array's performance, we conduct full-wave simulations as well as near-field and far-field measurements."],"url":"http://arxiv.org/abs/2404.19646v1","category":"eess.SP"}
{"created":"2024-04-30 15:45:30","title":"MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation","abstract":"Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from seen training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC. The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24.","sentences":["Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications.","Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC).","Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from seen training distributions but recognizes novel classes sampled from unseen testing distributions.","In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment.","Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks.","To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios.","Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model.","Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning.","The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts.","We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC.","The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24."],"url":"http://arxiv.org/abs/2404.19644v1","category":"cs.CV"}
{"created":"2024-04-30 15:34:51","title":"On Training a Neural Network to Explain Binaries","abstract":"In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding. Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign. Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction. However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models. Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries. A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs. Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space. We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated. We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value.","sentences":["In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding.","Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign.","Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction.","However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models.","Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries.","A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs.","Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space.","We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated.","We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value."],"url":"http://arxiv.org/abs/2404.19631v1","category":"cs.LG"}
{"created":"2024-04-30 15:30:14","title":"Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction","abstract":"The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP. However, among these leading DL models, there is a wide variance in both the training settings and architecture used. Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success. In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS. We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect. We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size.","sentences":["The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP.","However, among these leading DL models, there is a wide variance in both the training settings and architecture used.","Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success.","In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets.","Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS.","We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect.","We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size."],"url":"http://arxiv.org/abs/2404.19630v1","category":"cs.LG"}
{"created":"2024-04-30 15:29:01","title":"The Drawback of Insight: Detailed Explanations Can Reduce Agreement with XAI","abstract":"With the emergence of Artificial Intelligence (AI)-based decision-making, explanations help increase new technology adoption through enhanced trust and reliability. However, our experimental study challenges the notion that every user universally values explanations. We argue that the agreement with AI suggestions, whether accompanied by explanations or not, is influenced by individual differences in personality traits and the users' comfort with technology. We found that people with higher neuroticism and lower technological comfort showed more agreement with the recommendations without explanations. As more users become exposed to eXplainable AI (XAI) and AI-based systems, we argue that the XAI design should not provide explanations for users with high neuroticism and low technology comfort. Prioritizing user personalities in XAI systems will help users become better collaborators of AI systems.","sentences":["With the emergence of Artificial Intelligence (AI)-based decision-making, explanations help increase new technology adoption through enhanced trust and reliability.","However, our experimental study challenges the notion that every user universally values explanations.","We argue that the agreement with AI suggestions, whether accompanied by explanations or not, is influenced by individual differences in personality traits and the users' comfort with technology.","We found that people with higher neuroticism and lower technological comfort showed more agreement with the recommendations without explanations.","As more users become exposed to eXplainable AI (XAI) and AI-based systems, we argue that the XAI design should not provide explanations for users with high neuroticism and low technology comfort.","Prioritizing user personalities in XAI systems will help users become better collaborators of AI systems."],"url":"http://arxiv.org/abs/2404.19629v1","category":"cs.HC"}
{"created":"2024-04-30 15:06:20","title":"Radio Resource Management Design for RSMA: Optimization of Beamforming, User Admission, and Discrete/Continuous Rates with Imperfect SIC","abstract":"This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC). Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources. Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve. Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions. We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains. Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate. Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics. Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable.","sentences":["This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC).","Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources.","Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve.","Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions.","We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains.","Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate.","Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics.","Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable."],"url":"http://arxiv.org/abs/2404.19611v1","category":"eess.SP"}
{"created":"2024-04-30 14:49:03","title":"Artificial Intelligence in Bone Metastasis Analysis: Current Advancements, Opportunities and Challenges","abstract":"In recent years, Artificial Intelligence (AI) has been widely used in medicine, particularly in the analysis of medical imaging, which has been driven by advances in computer vision and deep learning methods. This is particularly important in overcoming the challenges posed by diseases such as Bone Metastases (BM), a common and complex malignancy of the bones. Indeed, there have been an increasing interest in developing Machine Learning (ML) techniques into oncologic imaging for BM analysis. In order to provide a comprehensive overview of the current state-of-the-art and advancements for BM analysis using artificial intelligence, this review is conducted with the accordance with PRISMA guidelines. Firstly, this review highlights the clinical and oncologic perspectives of BM and the used medical imaging modalities, with discussing their advantages and limitations. Then the review focuses on modern approaches with considering the main BM analysis tasks, which includes: classification, detection and segmentation. The results analysis show that ML technologies can achieve promising performance for BM analysis and have significant potential to improve clinician efficiency and cope with time and cost limitations. Furthermore, there are requirements for further research to validate the clinical performance of ML tools and facilitate their integration into routine clinical practice.","sentences":["In recent years, Artificial Intelligence (AI) has been widely used in medicine, particularly in the analysis of medical imaging, which has been driven by advances in computer vision and deep learning methods.","This is particularly important in overcoming the challenges posed by diseases such as Bone Metastases (BM), a common and complex malignancy of the bones.","Indeed, there have been an increasing interest in developing Machine Learning (ML) techniques into oncologic imaging for BM analysis.","In order to provide a comprehensive overview of the current state-of-the-art and advancements for BM analysis using artificial intelligence, this review is conducted with the accordance with PRISMA guidelines.","Firstly, this review highlights the clinical and oncologic perspectives of BM and the used medical imaging modalities, with discussing their advantages and limitations.","Then the review focuses on modern approaches with considering the main BM analysis tasks, which includes: classification, detection and segmentation.","The results analysis show that ML technologies can achieve promising performance for BM analysis and have significant potential to improve clinician efficiency and cope with time and cost limitations.","Furthermore, there are requirements for further research to validate the clinical performance of ML tools and facilitate their integration into routine clinical practice."],"url":"http://arxiv.org/abs/2404.19598v1","category":"eess.IV"}
{"created":"2024-04-30 14:25:32","title":"AI techniques for near real-time monitoring of contaminants in coastal waters on board future Phisat-2 mission","abstract":"Differently from conventional procedures, the proposed solution advocates for a groundbreaking paradigm in water quality monitoring through the integration of satellite Remote Sensing (RS) data, Artificial Intelligence (AI) techniques, and onboard processing. The objective is to offer nearly real-time detection of contaminants in coastal waters addressing a significant gap in the existing literature. Moreover, the expected outcomes include substantial advancements in environmental monitoring, public health protection, and resource conservation. The specific focus of our study is on the estimation of Turbidity and pH parameters, for their implications on human and aquatic health. Nevertheless, the designed framework can be extended to include other parameters of interest in the water environment and beyond. Originating from our participation in the European Space Agency (ESA) OrbitalAI Challenge, this article describes the distinctive opportunities and issues for the contaminants monitoring on the Phisat-2 mission. The specific characteristics of this mission, with the tools made available, will be presented, with the methodology proposed by the authors for the onboard monitoring of water contaminants in near real-time. Preliminary promising results are discussed and in progress and future work introduced.","sentences":["Differently from conventional procedures, the proposed solution advocates for a groundbreaking paradigm in water quality monitoring through the integration of satellite Remote Sensing (RS) data, Artificial Intelligence (AI) techniques, and onboard processing.","The objective is to offer nearly real-time detection of contaminants in coastal waters addressing a significant gap in the existing literature.","Moreover, the expected outcomes include substantial advancements in environmental monitoring, public health protection, and resource conservation.","The specific focus of our study is on the estimation of Turbidity and pH parameters, for their implications on human and aquatic health.","Nevertheless, the designed framework can be extended to include other parameters of interest in the water environment and beyond.","Originating from our participation in the European Space Agency (ESA) OrbitalAI","Challenge, this article describes the distinctive opportunities and issues for the contaminants monitoring on the Phisat-2 mission.","The specific characteristics of this mission, with the tools made available, will be presented, with the methodology proposed by the authors for the onboard monitoring of water contaminants in near real-time.","Preliminary promising results are discussed and in progress and future work introduced."],"url":"http://arxiv.org/abs/2404.19586v1","category":"cs.CV"}
{"created":"2024-04-30 14:16:45","title":"Automatic Cardiac Pathology Recognition in Echocardiography Images Using Higher Order Dynamic Mode Decomposition and a Vision Transformer for Small Datasets","abstract":"Heart diseases are the main international cause of human defunction. According to the WHO, nearly 18 million people decease each year because of heart diseases. Also considering the increase of medical data, much pressure is put on the health industry to develop systems for early and accurate heart disease recognition. In this work, an automatic cardiac pathology recognition system based on a novel deep learning framework is proposed, which analyses in real-time echocardiography video sequences. The system works in two stages. The first one transforms the data included in a database of echocardiography sequences into a machine-learning-compatible collection of annotated images which can be used in the training stage of any kind of machine learning-based framework, and more specifically with deep learning. This includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the first time to the authors' knowledge, for both data augmentation and feature extraction in the medical field. The second stage is focused on building and training a Vision Transformer (ViT), barely explored in the related literature. The ViT is adapted for an effective training from scratch, even with small datasets. The designed neural network analyses images from an echocardiography sequence to predict the heart state. The results obtained show the superiority of the proposed system and the efficacy of the HODMD algorithm, even outperforming pretrained Convolutional Neural Networks (CNNs), which are so far the method of choice in the literature.","sentences":["Heart diseases are the main international cause of human defunction.","According to the WHO, nearly 18 million people decease each year because of heart diseases.","Also considering the increase of medical data, much pressure is put on the health industry to develop systems for early and accurate heart disease recognition.","In this work, an automatic cardiac pathology recognition system based on a novel deep learning framework is proposed, which analyses in real-time echocardiography video sequences.","The system works in two stages.","The first one transforms the data included in a database of echocardiography sequences into a machine-learning-compatible collection of annotated images which can be used in the training stage of any kind of machine learning-based framework, and more specifically with deep learning.","This includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm, for the first time to the authors' knowledge, for both data augmentation and feature extraction in the medical field.","The second stage is focused on building and training a Vision Transformer (ViT), barely explored in the related literature.","The ViT is adapted for an effective training from scratch, even with small datasets.","The designed neural network analyses images from an echocardiography sequence to predict the heart state.","The results obtained show the superiority of the proposed system and the efficacy of the HODMD algorithm, even outperforming pretrained Convolutional Neural Networks (CNNs), which are so far the method of choice in the literature."],"url":"http://arxiv.org/abs/2404.19579v1","category":"eess.IV"}
{"created":"2024-04-30 14:07:57","title":"War Elephants: Rethinking Combat AI and Human Oversight","abstract":"This paper explores the changes that pervasive AI is having on the nature of combat. We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended. Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\" By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems. We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems. This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support.","sentences":["This paper explores the changes that pervasive AI is having on the nature of combat.","We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended.","Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\"","By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems.","We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems.","This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support."],"url":"http://arxiv.org/abs/2404.19573v1","category":"cs.CY"}
{"created":"2024-04-30 13:59:28","title":"Consensus + Innovations Approach for Online Distributed Multi-Area Inertia Estimation","abstract":"The reduction of overall system inertia in modern power systems due to the increasing deployment of distributed energy resources is generally recognized as a major issue for system stability. Consequently, real-time monitoring of system inertia is critical to ensure a reliable and cost-effective system operation. Large-scale power systems are typically managed by multiple transmission system operators, making it difficult to have a central entity with access to global measurement data, which is usually required for estimating the overall system inertia. We address this problem by proposing a fully distributed inertia estimation algorithm with rigorous analytical convergence guarantees. This method requires only peer-to-peer sharing of local parameter estimates between neighboring control areas, eliminating the need for a centralized collection of real-time measurements. We robustify the algorithm in the presence of typical power system disturbances and demonstrate its performance in simulations based on the well-known New England IEEE-39 bus system.","sentences":["The reduction of overall system inertia in modern power systems due to the increasing deployment of distributed energy resources is generally recognized as a major issue for system stability.","Consequently, real-time monitoring of system inertia is critical to ensure a reliable and cost-effective system operation.","Large-scale power systems are typically managed by multiple transmission system operators, making it difficult to have a central entity with access to global measurement data, which is usually required for estimating the overall system inertia.","We address this problem by proposing a fully distributed inertia estimation algorithm with rigorous analytical convergence guarantees.","This method requires only peer-to-peer sharing of local parameter estimates between neighboring control areas, eliminating the need for a centralized collection of real-time measurements.","We robustify the algorithm in the presence of typical power system disturbances and demonstrate its performance in simulations based on the well-known New England IEEE-39 bus system."],"url":"http://arxiv.org/abs/2404.19569v1","category":"eess.SY"}
{"created":"2024-04-30 13:14:51","title":"RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing","abstract":"Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.","sentences":["Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge.","To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks.","This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications.","The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications.","RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications.","The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment.","It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research.","In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP.","The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey."],"url":"http://arxiv.org/abs/2404.19543v1","category":"cs.CL"}
{"created":"2024-04-30 13:14:11","title":"Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse Inertial Sensors and Ultra-Wideband Ranging","abstract":"While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\\%$).","sentences":["While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity.","Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy.","In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances.","We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging$-$dynamically and without the need for stationary reference anchors.","Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation.","To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS.","For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB).","Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from $13.62$ to $10.65cm$ ($22\\%$ better) and lowering jitter from $1.56$ to $0.055km/s^3$ (a reduction of $97\\%$)."],"url":"http://arxiv.org/abs/2404.19541v1","category":"cs.CV"}
{"created":"2024-04-30 13:11:12","title":"MIPI 2024 Challenge on Nighttime Flare Removal: Methods and Results","abstract":"The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems. However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI). Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms. In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024. In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase. The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal. More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/.","sentences":["The increasing demand for computational photography and imaging on mobile platforms has led to the widespread development and integration of advanced image sensors with novel algorithms in camera systems.","However, the scarcity of high-quality data for research and the rare opportunity for in-depth exchange of views from industry and academia constrain the development of mobile intelligent photography and imaging (MIPI).","Building on the achievements of the previous MIPI Workshops held at ECCV 2022 and CVPR 2023, we introduce our third MIPI challenge including three tracks focusing on novel image sensors and imaging algorithms.","In this paper, we summarize and review the Nighttime Flare Removal track on MIPI 2024.","In total, 170 participants were successfully registered, and 14 teams submitted results in the final testing phase.","The developed solutions in this challenge achieved state-of-the-art performance on Nighttime Flare Removal.","More details of this challenge and the link to the dataset can be found at https://mipi-challenge.org/MIPI2024/."],"url":"http://arxiv.org/abs/2404.19534v1","category":"cs.CV"}
{"created":"2024-04-30 12:49:54","title":"MGCBS: An Optimal and Efficient Algorithm for Solving Multi-Goal Multi-Agent Path Finding Problem","abstract":"With the expansion of the scale of robotics applications, the multi-goal multi-agent pathfinding (MG-MAPF) problem began to gain widespread attention. This problem requires each agent to visit pre-assigned multiple goal points at least once without conflict. Some previous methods have been proposed to solve the MG-MAPF problem based on Decoupling the goal Vertex visiting order search and the Single-agent pathfinding (DVS). However, this paper demonstrates that the methods based on DVS cannot always obtain the optimal solution. To obtain the optimal result, we propose the Multi-Goal Conflict-Based Search (MGCBS), which is based on Decoupling the goal Safe interval visiting order search and the Single-agent pathfinding (DSS). Additionally, we present the Time-Interval-Space Forest (TIS Forest) to enhance the efficiency of MGCBS by maintaining the shortest paths from any start point at any start time step to each safe interval at the goal points. The experiment demonstrates that our method can consistently obtain optimal results and execute up to 7 times faster than the state-of-the-art method in our evaluation.","sentences":["With the expansion of the scale of robotics applications, the multi-goal multi-agent pathfinding (MG-MAPF) problem began to gain widespread attention.","This problem requires each agent to visit pre-assigned multiple goal points at least once without conflict.","Some previous methods have been proposed to solve the MG-MAPF problem based on Decoupling the goal Vertex visiting order search and the Single-agent pathfinding (DVS).","However, this paper demonstrates that the methods based on DVS cannot always obtain the optimal solution.","To obtain the optimal result, we propose the Multi-Goal Conflict-Based Search (MGCBS), which is based on Decoupling the goal Safe interval visiting order search and the Single-agent pathfinding (DSS).","Additionally, we present the Time-Interval-Space Forest (TIS Forest) to enhance the efficiency of MGCBS by maintaining the shortest paths from any start point at any start time step to each safe interval at the goal points.","The experiment demonstrates that our method can consistently obtain optimal results and execute up to 7 times faster than the state-of-the-art method in our evaluation."],"url":"http://arxiv.org/abs/2404.19518v1","category":"cs.MA"}
{"created":"2024-04-30 12:44:55","title":"First observation of $\u039b_{b}^{0} \\rightarrow \u03a3_c^{(*)++} D^{(*)-} K^{-}$ decays","abstract":"The four decays, $\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{(*)++} D^{(*)-} K^{-}$, are observed for the first time using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of $13\\,\\rm{TeV}$, corresponding to an integrated luminosity of $6\\,\\rm{fb}^{-1}$. By considering the $\\Lambda_b^0 \\rightarrow \\Lambda_c^{+} \\overline{D}^0 K^{-}$ decay as reference channel, the following branching fraction ratios are measured to be,   $$\\frac{\\cal{B} (\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm{D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Lambda_c^{+} \\rm \\overline{D}^0 {K}^{-})}   = {0.282}\\pm{0.016}\\pm{0.016}\\pm{0.005},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm {D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {0.460}\\pm{0.052}\\pm{0.028},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm {D}^{*-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {2.261}\\pm{0.202}\\pm{0.129}\\pm{0.046},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm D^{*-} K^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm D^{-} K^{-})}   = {0.896}\\pm{0.137}\\pm{0.066}\\pm{0.018},$$   where the first uncertainties are statistical, the second are systematic, and the third are due to uncertainties in the branching fractions of intermediate particle decays. These initial observations mark the beginning of pentaquark searches in these modes, with more data set to become available following the LHCb upgrade.","sentences":["The four decays, $\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{(*)++} D^{(*)-} K^{-}$, are observed for the first time using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of $13\\,\\rm{TeV}$, corresponding to an integrated luminosity of $6\\,\\rm{fb}^{-1}$. By considering the $\\Lambda_b^0 \\rightarrow \\Lambda_c^{+} \\overline{D}^0 K^{-}$ decay as reference channel, the following branching fraction ratios are measured to be,   $$\\frac{\\cal{B} (\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm{D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Lambda_c^{+} \\rm \\overline{D}^0 {K}^{-})}   = {0.282}\\pm{0.016}\\pm{0.016}\\pm{0.005},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm {D}^{-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow","\\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {0.460}\\pm{0.052}\\pm{0.028},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{++} \\rm {D}^{*-} {K}^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm {D}^{-} {K}^{-})}   = {2.261}\\pm{0.202}\\pm{0.129}\\pm{0.046},   \\frac{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_{c}^{*++} \\rm D^{*-} K^{-})}{\\cal{B}(\\Lambda_{b}^{0} \\rightarrow \\Sigma_c^{++} \\rm D^{-} K^{-})}   = {0.896}\\pm{0.137}\\pm{0.066}\\pm{0.018},$$   where the first uncertainties are statistical, the second are systematic, and the third are due to uncertainties in the branching fractions of intermediate particle decays.","These initial observations mark the beginning of pentaquark searches in these modes, with more data set to become available following the LHCb upgrade."],"url":"http://arxiv.org/abs/2404.19510v1","category":"hep-ex"}
{"created":"2024-04-30 12:40:52","title":"SPHERE RefPlanets: Search for epsilon Eridani b and warm dust","abstract":"We carried out very deep VLT/SPHERE imaging polarimetry of the nearby system Eps Eri based on 38.5 hours of integration time with a 600 - 900 nm broadband filter to search for polarized scattered light from a planet or from circumstellar dust using AO, coronagraphy, high precision differential polarimetry, and angular differential imaging. We have improved several data reduction and post-processing techniques and also developed new ones to further increase the sensitivity of SPHERE/ZIMPOL. The data provide unprecedented contrast limits, but no significant detection of a point source or an extended signal from circumstellar dust. For each observing epoch, we obtained a point source contrast for the polarized intensity between $2\\cdot 10^{-8}$ and $4\\cdot 10^{-8}$ at the expected separation of the planet Eps Eri b of 1'' near quadrature phase. The polarimetric contrast limits are about six to 50 times better than the intensity limits because polarimetric imaging is much more efficient in speckle suppression. Combining the entire 14-month data set to the search for a planet moving on a Keplerian orbit with the K-Stacker software further improves the contrast limits by a factor of about two, to about $8 \\cdot 10^{-9}$ at 1''. This would allow the detection of a planet with a radius of about 2.5 Jupiter radii. The surface brightness contrast limits achieved for the polarized intensity from an extended scattering region are about 15 mag arcsec$^{-2}$ at 1'', or up to 3 mag arcsec$^{-2}$ deeper than previous limits. For Eps Eri, these limits exclude the presence of a narrow dust ring and they constrain the dust properties. This study shows that the polarimetric contrast limits for reflecting planets with SPHERE/ZIMPOL can be improved to a level $<10^{-8}$ simply by collecting more data over many nights and using the K-Stacker software.","sentences":["We carried out very deep VLT/SPHERE imaging polarimetry of the nearby system Eps Eri based on 38.5 hours of integration time with a 600 - 900 nm broadband filter to search for polarized scattered light from a planet or from circumstellar dust using AO, coronagraphy, high precision differential polarimetry, and angular differential imaging.","We have improved several data reduction and post-processing techniques and also developed new ones to further increase the sensitivity of SPHERE/ZIMPOL.","The data provide unprecedented contrast limits, but no significant detection of a point source or an extended signal from circumstellar dust.","For each observing epoch, we obtained a point source contrast for the polarized intensity between $2\\cdot 10^{-8}$ and $4\\cdot 10^{-8}$ at the expected separation of the planet Eps Eri b of 1'' near quadrature phase.","The polarimetric contrast limits are about six to 50 times better than the intensity limits because polarimetric imaging is much more efficient in speckle suppression.","Combining the entire 14-month data set to the search for a planet moving on a Keplerian orbit with the K-Stacker software further improves the contrast limits by a factor of about two, to about $8 \\cdot 10^{-9}$ at 1''.","This would allow the detection of a planet with a radius of about 2.5 Jupiter radii.","The surface brightness contrast limits achieved for the polarized intensity from an extended scattering region are about 15 mag arcsec$^{-2}$ at 1'', or up to 3 mag arcsec$^{-2}$ deeper than previous limits.","For Eps Eri, these limits exclude the presence of a narrow dust ring and they constrain the dust properties.","This study shows that the polarimetric contrast limits for reflecting planets with SPHERE/ZIMPOL can be improved to a level $<10^{-8}$ simply by collecting more data over many nights and using the K-Stacker software."],"url":"http://arxiv.org/abs/2404.19504v1","category":"astro-ph.EP"}
{"created":"2024-04-30 12:37:01","title":"Towards Real-world Video Face Restoration: A New Benchmark","abstract":"Blind face restoration (BFR) on images has significantly progressed over the last several years, while real-world video face restoration (VFR), which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved, remains unsolved. Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images, which are limited in their coverage of real-world video frames. In this work, we introduced new real-world datasets named FOS with a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to study the applicability of current methods on videos. Compared with existing test datasets, FOS datasets cover more diverse degradations and involve face samples from more complex scenarios, which helps to revisit current face restoration approaches more comprehensively. Given the established datasets, we benchmarked both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches, identifying their potential and limitations in VFR tasks. In addition, we studied the effectiveness of the commonly used image quality assessment (IQA) metrics and face IQA (FIQA) metrics by leveraging a subjective user study. With extensive experimental results and detailed analysis provided, we gained insights from the successes and failures of both current BFR and VSR methods. These results also pose challenges to current face restoration approaches, which we hope stimulate future advances in VFR research.","sentences":["Blind face restoration (BFR) on images has significantly progressed over the last several years, while real-world video face restoration (VFR), which is more challenging for more complex face motions such as moving gaze directions and facial orientations involved, remains unsolved.","Typical BFR methods are evaluated on privately synthesized datasets or self-collected real-world low-quality face images, which are limited in their coverage of real-world video frames.","In this work, we introduced new real-world datasets named FOS with a taxonomy of \"Full, Occluded, and Side\" faces from mainly video frames to study the applicability of current methods on videos.","Compared with existing test datasets, FOS datasets cover more diverse degradations and involve face samples from more complex scenarios, which helps to revisit current face restoration approaches more comprehensively.","Given the established datasets, we benchmarked both the state-of-the-art BFR methods and the video super resolution (VSR) methods to comprehensively study current approaches, identifying their potential and limitations in VFR tasks.","In addition, we studied the effectiveness of the commonly used image quality assessment (IQA) metrics and face IQA (FIQA) metrics by leveraging a subjective user study.","With extensive experimental results and detailed analysis provided, we gained insights from the successes and failures of both current BFR and VSR methods.","These results also pose challenges to current face restoration approaches, which we hope stimulate future advances in VFR research."],"url":"http://arxiv.org/abs/2404.19500v1","category":"cs.CV"}
{"created":"2024-04-30 12:37:01","title":"A Unified Theory of Exact Inference and Learning in Exponential Family Latent Variable Models","abstract":"Bayes' rule describes how to infer posterior beliefs about latent variables given observations, and inference is a critical step in learning algorithms for latent variable models (LVMs). Although there are exact algorithms for inference and learning for certain LVMs such as linear Gaussian models and mixture models, researchers must typically develop approximate inference and learning algorithms when applying novel LVMs. In this paper we study the line that separates LVMs that rely on approximation schemes from those that do not, and develop a general theory of exponential family, latent variable models for which inference and learning may be implemented exactly. Firstly, under mild assumptions about the exponential family form of a given LVM, we derive necessary and sufficient conditions under which the LVM prior is in the same exponential family as its posterior, such that the prior is conjugate to the posterior. We show that all models that satisfy these conditions are constrained forms of a particular class of exponential family graphical model. We then derive general inference and learning algorithms, and demonstrate them on a variety of example models. Finally, we show how to compose our models into graphical models that retain tractable inference and learning. In addition to our theoretical work, we have implemented our algorithms in a collection of libraries with which we provide numerous demonstrations of our theory, and with which researchers may apply our theory in novel statistical settings.","sentences":["Bayes' rule describes how to infer posterior beliefs about latent variables given observations, and inference is a critical step in learning algorithms for latent variable models (LVMs).","Although there are exact algorithms for inference and learning for certain LVMs such as linear Gaussian models and mixture models, researchers must typically develop approximate inference and learning algorithms when applying novel LVMs.","In this paper we study the line that separates LVMs that rely on approximation schemes from those that do not, and develop a general theory of exponential family, latent variable models for which inference and learning may be implemented exactly.","Firstly, under mild assumptions about the exponential family form of a given LVM, we derive necessary and sufficient conditions under which the LVM prior is in the same exponential family as its posterior, such that the prior is conjugate to the posterior.","We show that all models that satisfy these conditions are constrained forms of a particular class of exponential family graphical model.","We then derive general inference and learning algorithms, and demonstrate them on a variety of example models.","Finally, we show how to compose our models into graphical models that retain tractable inference and learning.","In addition to our theoretical work, we have implemented our algorithms in a collection of libraries with which we provide numerous demonstrations of our theory, and with which researchers may apply our theory in novel statistical settings."],"url":"http://arxiv.org/abs/2404.19501v1","category":"cs.LG"}
{"created":"2024-04-30 12:27:49","title":"Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)","abstract":"Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta. Another fundamental premise of the functionalist theory is that \"effect\" is not a monolithic concept. Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call \"effect.\" A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV. Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV. It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect.","sentences":["Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis.","This paper retraces the theory behind the estimator.","It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation.","It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands.","Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison.","Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta.","Another fundamental premise of the functionalist theory is that \"effect\" is not a monolithic concept.","Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call \"effect.\"","A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV.","Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV.","It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect."],"url":"http://arxiv.org/abs/2404.19495v1","category":"stat.AP"}
{"created":"2024-04-30 12:09:53","title":"IID Relaxation by Logical Expressivity: A Research Agenda for Fitting Logics to Neurosymbolic Requirements","abstract":"Neurosymbolic background knowledge and the expressivity required of its logic can break Machine Learning assumptions about data Independence and Identical Distribution. In this position paper we propose to analyze IID relaxation in a hierarchy of logics that fit different use case requirements. We discuss the benefits of exploiting known data dependencies and distribution constraints for Neurosymbolic use cases and argue that the expressivity required for this knowledge has implications for the design of underlying ML routines. This opens a new research agenda with general questions about Neurosymbolic background knowledge and the expressivity required of its logic.","sentences":["Neurosymbolic background knowledge and the expressivity required of its logic can break Machine Learning assumptions about data Independence and Identical Distribution.","In this position paper we propose to analyze IID relaxation in a hierarchy of logics that fit different use case requirements.","We discuss the benefits of exploiting known data dependencies and distribution constraints for Neurosymbolic use cases and argue that the expressivity required for this knowledge has implications for the design of underlying ML routines.","This opens a new research agenda with general questions about Neurosymbolic background knowledge and the expressivity required of its logic."],"url":"http://arxiv.org/abs/2404.19485v1","category":"cs.AI"}
{"created":"2024-04-30 12:05:48","title":"More Compute Is What You Need","abstract":"Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal. In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size. Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance.","sentences":["Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal.","In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size.","Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance."],"url":"http://arxiv.org/abs/2404.19484v1","category":"cs.LG"}
{"created":"2024-04-30 11:48:13","title":"Mitigating and Analysis of Memory Usage Attack in IoE System","abstract":"Internet of Everything (IoE) is a newly emerging trend, especially in homes. Marketing forces toward smart homes are also accelerating the spread of IoE devices in households. An obvious risk of the rapid adoption of these smart devices is that many lack controls for protecting the privacy and security of end users from attacks designed to disrupt lives and incur financial losses. Today the smart home is a system for managing the basic life support processes of both small systems, e.g., commercial, office premises, apartments, cottages, and largely automated complexes, e.g., commercial and industrial complexes. One of the critical tasks to be solved by the concept of a modern smart home is the problem of preventing the usage of IoE resources. Recently, there has been a rapid increase in attacks on consumer IoE devices.   Memory corruption vulnerabilities constitute a significant class of vulnerabilities in software security through which attackers can gain control of an entire system. Numerous memory corruption vulnerabilities have been found in IoE firmware already deployed in the consumer market. This paper aims to analyze and explain the resource usage attack and create a low-cost simulation environment to aid in the dynamic analysis of the attack. Further, we perform controlled resource usage attacks while measuring resource consumption on resource-constrained victims' IoE devices, such as CPU and memory utilization. We also build a lightweight algorithm to detect memory usage attacks in the IoE environment. The result shows high efficiency in detecting and mitigating memory usage attacks by detecting when the intruder starts and stops the attack.","sentences":["Internet of Everything (IoE) is a newly emerging trend, especially in homes.","Marketing forces toward smart homes are also accelerating the spread of IoE devices in households.","An obvious risk of the rapid adoption of these smart devices is that many lack controls for protecting the privacy and security of end users from attacks designed to disrupt lives and incur financial losses.","Today the smart home is a system for managing the basic life support processes of both small systems, e.g., commercial, office premises, apartments, cottages, and largely automated complexes, e.g., commercial and industrial complexes.","One of the critical tasks to be solved by the concept of a modern smart home is the problem of preventing the usage of IoE resources.","Recently, there has been a rapid increase in attacks on consumer IoE devices.   ","Memory corruption vulnerabilities constitute a significant class of vulnerabilities in software security through which attackers can gain control of an entire system.","Numerous memory corruption vulnerabilities have been found in IoE firmware already deployed in the consumer market.","This paper aims to analyze and explain the resource usage attack and create a low-cost simulation environment to aid in the dynamic analysis of the attack.","Further, we perform controlled resource usage attacks while measuring resource consumption on resource-constrained victims' IoE devices, such as CPU and memory utilization.","We also build a lightweight algorithm to detect memory usage attacks in the IoE environment.","The result shows high efficiency in detecting and mitigating memory usage attacks by detecting when the intruder starts and stops the attack."],"url":"http://arxiv.org/abs/2404.19480v1","category":"cs.CR"}
{"created":"2024-04-30 11:13:23","title":"Imitation Learning: A Survey of Learning Methods, Environments and Metrics","abstract":"Imitation learning is an approach in which an agent learns how to execute a task by trying to mimic how one or more teachers perform it. This learning approach offers a compromise between the time it takes to learn a new task and the effort needed to collect teacher samples for the agent. It achieves this by balancing learning from the teacher, who has some information on how to perform the task, and deviating from their examples when necessary, such as states not present in the teacher samples. Consequently, the field of imitation learning has received much attention from researchers in recent years, resulting in many new methods and applications. However, with this increase in published work and past surveys focusing mainly on methodology, a lack of standardisation became more prominent in the field. This non-standardisation is evident in the use of environments, which appear in no more than two works, and evaluation processes, such as qualitative analysis, that have become rare in current literature. In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers.","sentences":["Imitation learning is an approach in which an agent learns how to execute a task by trying to mimic how one or more teachers perform it.","This learning approach offers a compromise between the time it takes to learn a new task and the effort needed to collect teacher samples for the agent.","It achieves this by balancing learning from the teacher, who has some information on how to perform the task, and deviating from their examples when necessary, such as states not present in the teacher samples.","Consequently, the field of imitation learning has received much attention from researchers in recent years, resulting in many new methods and applications.","However, with this increase in published work and past surveys focusing mainly on methodology, a lack of standardisation became more prominent in the field.","This non-standardisation is evident in the use of environments, which appear in no more than two works, and evaluation processes, such as qualitative analysis, that have become rare in current literature.","In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers."],"url":"http://arxiv.org/abs/2404.19456v1","category":"cs.LG"}
{"created":"2024-04-30 11:10:34","title":"Optimized neural forms for solving ordinary differential equations","abstract":"A critical issue in approximating solutions of ordinary differential equations using neural networks is the exact satisfaction of the boundary or initial conditions. For this purpose, neural forms have been introduced, i.e., functional expressions that depend on neural networks which, by design, satisfy the prescribed conditions exactly. Expanding upon prior progress, the present work contributes in three distinct aspects. First, it presents a novel formalism for crafting optimized neural forms. Second, it outlines a method for establishing an upper bound on the absolute deviation from the exact solution. Third, it introduces a technique for converting problems with Neumann or Robin conditions into equivalent problems with parametric Dirichlet conditions. The proposed optimized neural forms were numerically tested on a set of diverse problems, encompassing first-order and second-order ordinary differential equations, as well as first-order systems. Stiff and delay differential equations were also considered. The obtained solutions were compared against solutions obtained via Runge-Kutta methods and exact solutions wherever available. The reported results and analysis verify that in addition to the exact satisfaction of the boundary or initial conditions, optimized neural forms provide closed-form solutions of superior interpolation capability and controllable overall accuracy.","sentences":["A critical issue in approximating solutions of ordinary differential equations using neural networks is the exact satisfaction of the boundary or initial conditions.","For this purpose, neural forms have been introduced, i.e., functional expressions that depend on neural networks which, by design, satisfy the prescribed conditions exactly.","Expanding upon prior progress, the present work contributes in three distinct aspects.","First, it presents a novel formalism for crafting optimized neural forms.","Second, it outlines a method for establishing an upper bound on the absolute deviation from the exact solution.","Third, it introduces a technique for converting problems with Neumann or Robin conditions into equivalent problems with parametric Dirichlet conditions.","The proposed optimized neural forms were numerically tested on a set of diverse problems, encompassing first-order and second-order ordinary differential equations, as well as first-order systems.","Stiff and delay differential equations were also considered.","The obtained solutions were compared against solutions obtained via Runge-Kutta methods and exact solutions wherever available.","The reported results and analysis verify that in addition to the exact satisfaction of the boundary or initial conditions, optimized neural forms provide closed-form solutions of superior interpolation capability and controllable overall accuracy."],"url":"http://arxiv.org/abs/2404.19454v1","category":"cs.AI"}
{"created":"2024-04-30 10:57:37","title":"AoI-aware Sensing Scheduling and Trajectory Optimization for Multi-UAV-assisted Wireless Backscatter Networks","abstract":"This paper considers multiple unmanned aerial vehicles (UAVs) to assist sensing data transmissions from the ground users (GUs) to a remote base station (BS). Each UAV collects sensing data from the GUs and then forwards the sensing data to the remote BS. The GUs first backscatter their data to the UAVs and then all UAVs forward data to the BS by the nonorthogonal multiple access (NOMA) transmissions. We formulate a multi-stage stochastic optimization problem to minimize the long-term time-averaged age-of-information (AoI) by jointly optimizing the GUs' access control, the UAVs' beamforming, and trajectory planning strategies. To solve this problem, we first model the dynamics of the GUs' AoI statuses by virtual queueing systems, and then propose the AoI-aware sensing scheduling and trajectory optimization (AoI-STO) algorithm. This allows us to transform the multi-stage AoI minimization problem into a series of per-slot control problems by using the Lyapunov optimization framework. In each time slot, the GUs' access control, the UAVs' beamforming, and mobility control strategies are updated by using the block coordinate descent (BCD) method according to the instant GUs' AoI statuses. Simulation results reveal that the proposed AoI-STO algorithm can reduce the overall AoI by more than 50%. The GUs' scheduling fairness is also improved greatly by adapting the GUs' access control compared with typical baseline schemes.","sentences":["This paper considers multiple unmanned aerial vehicles (UAVs) to assist sensing data transmissions from the ground users (GUs) to a remote base station (BS).","Each UAV collects sensing data from the GUs and then forwards the sensing data to the remote BS.","The GUs first backscatter their data to the UAVs and then all UAVs forward data to the BS by the nonorthogonal multiple access (NOMA) transmissions.","We formulate a multi-stage stochastic optimization problem to minimize the long-term time-averaged age-of-information (AoI) by jointly optimizing the GUs' access control, the UAVs' beamforming, and trajectory planning strategies.","To solve this problem, we first model the dynamics of the GUs' AoI statuses by virtual queueing systems, and then propose the AoI-aware sensing scheduling and trajectory optimization (AoI-STO) algorithm.","This allows us to transform the multi-stage AoI minimization problem into a series of per-slot control problems by using the Lyapunov optimization framework.","In each time slot, the GUs' access control, the UAVs' beamforming, and mobility control strategies are updated by using the block coordinate descent (BCD) method according to the instant GUs' AoI statuses.","Simulation results reveal that the proposed AoI-STO algorithm can reduce the overall AoI by more than 50%.","The GUs' scheduling fairness is also improved greatly by adapting the GUs' access control compared with typical baseline schemes."],"url":"http://arxiv.org/abs/2404.19449v1","category":"cs.IT"}
{"created":"2024-04-30 10:54:10","title":"Exploring the potential of synthesizing unknown superheavy isotopes via cold-fusion reactions based on the dinuclear system model","abstract":"To assess the potential of cold-fusion for synthesizing superheavy nuclei (SHN) with proton numbers 104-113, we systematically calculated 145 naturally occurring projectile-target combinations within the DNS model. Reactions predominantly show maximum cross-sections in the 1n to 2n channels, peaking near the Coulomb barrier with a sum of barrier and Q-value within 30 MeV. The maximum cross-section occurs below the Bass barrier, suggesting either the Bass model's limitation or significant deformation reducing the effective Coulomb barrier. Our calculations align well with experimental data, revealing that more neutron-rich projectiles slightly enhance fusion, though the effect is minor. For fixed targets (Pb, Bi), evaporation residue cross-sections decrease linearly with increasing projectile proton number, attributed to reduced fusion probability and lower fission barriers in heavier SHN. The touching potential $V_{\\rm in}$ shows a linear trend with the product of projectile-target proton numbers, with neutron-rich systems exhibiting lower $V_{\\rm in}$. Some reactions with $V_{\\rm in} < V_{\\rm S}$ may involve nucleon transfer before capture. Based on the DNS model, we identified optimal combinations and collision energies for synthesizing SHN with significant cross-sections. Collectively, our findings indicate that cold fusion is a promising avenue for creating proton-rich SHN around the drip line in the Z=104-113 region, offering distinct advantages over alternative mechanisms.","sentences":["To assess the potential of cold-fusion for synthesizing superheavy nuclei (SHN) with proton numbers 104-113, we systematically calculated 145 naturally occurring projectile-target combinations within the DNS model.","Reactions predominantly show maximum cross-sections in the 1n to 2n channels, peaking near the Coulomb barrier with a sum of barrier and Q-value within 30 MeV.","The maximum cross-section occurs below the Bass barrier, suggesting either the Bass model's limitation or significant deformation reducing the effective Coulomb barrier.","Our calculations align well with experimental data, revealing that more neutron-rich projectiles slightly enhance fusion, though the effect is minor.","For fixed targets (Pb, Bi), evaporation residue cross-sections decrease linearly with increasing projectile proton number, attributed to reduced fusion probability and lower fission barriers in heavier SHN.","The touching potential $V_{\\rm in}$ shows a linear trend with the product of projectile-target proton numbers, with neutron-rich systems exhibiting lower $V_{\\rm in}$. Some reactions with $V_{\\rm in} < V_{\\rm S}$ may involve nucleon transfer before capture.","Based on the DNS model, we identified optimal combinations and collision energies for synthesizing SHN with significant cross-sections.","Collectively, our findings indicate that cold fusion is a promising avenue for creating proton-rich SHN around the drip line in the Z=104-113 region, offering distinct advantages over alternative mechanisms."],"url":"http://arxiv.org/abs/2404.19446v1","category":"nucl-th"}
{"created":"2024-04-30 10:29:25","title":"Detection of Energy Consumption Cyber Attacks on Smart Devices","abstract":"With the rapid development of Internet of Things (IoT) technology, intelligent systems are increasingly integrating into everyday life and people's homes. However, the proliferation of these technologies raises concerns about the security of smart home devices. These devices often face resource constraints and may connect to unreliable networks, posing risks to the data they handle. Securing IoT technology is crucial due to the sensitive data involved.   Preventing energy attacks and ensuring the security of IoT infrastructure are key challenges in modern smart homes. Monitoring energy consumption can be an effective approach to detecting abnormal behavior and IoT cyberattacks. Lightweight algorithms are necessary to accommodate the resource limitations of IoT devices.   This paper presents a lightweight technique for detecting energy consumption attacks on smart home devices by analyzing received packets. The proposed algorithm considers TCP, UDP, and MQTT protocols, as well as device statuses (Idle, active, under attack). It accounts for resource constraints and promptly alerts administrators upon detecting an attack. The proposed approach effectively identifies energy consumption attacks by measuring packet reception rates for different protocols.","sentences":["With the rapid development of Internet of Things (IoT) technology, intelligent systems are increasingly integrating into everyday life and people's homes.","However, the proliferation of these technologies raises concerns about the security of smart home devices.","These devices often face resource constraints and may connect to unreliable networks, posing risks to the data they handle.","Securing IoT technology is crucial due to the sensitive data involved.   ","Preventing energy attacks and ensuring the security of IoT infrastructure are key challenges in modern smart homes.","Monitoring energy consumption can be an effective approach to detecting abnormal behavior and IoT cyberattacks.","Lightweight algorithms are necessary to accommodate the resource limitations of IoT devices.   ","This paper presents a lightweight technique for detecting energy consumption attacks on smart home devices by analyzing received packets.","The proposed algorithm considers TCP, UDP, and MQTT protocols, as well as device statuses (Idle, active, under attack).","It accounts for resource constraints and promptly alerts administrators upon detecting an attack.","The proposed approach effectively identifies energy consumption attacks by measuring packet reception rates for different protocols."],"url":"http://arxiv.org/abs/2404.19434v1","category":"cs.CR"}
{"created":"2024-04-30 10:28:04","title":"Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships","abstract":"Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason (or rather, approximately reason). Since to date these lines of work progressed largely in parallel (with notable exceptions), we are interested in investigating the intersection: probing for reasoning about the implicitly-held knowledge. Suspecting the performance to be lacking in this area, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs a bird has versus the number of wheels on a tricycle). We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only. It is difficult to argue that pure statistical learning can cope with the combinatorial explosion inherent in many commonsense reasoning tasks, especially once arithmetical notions are involved. Further, we argue that bigger is not always better and chasing purely statistical improvements is flawed at the core, since it only exacerbates the dangerous conflation of the production of correct answers with genuine reasoning ability.","sentences":["Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason (or rather, approximately reason).","Since to date these lines of work progressed largely in parallel (with notable exceptions), we are interested in investigating the intersection: probing for reasoning about the implicitly-held knowledge.","Suspecting the performance to be lacking in this area, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs a bird has versus the number of wheels on a tricycle).","We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only.","It is difficult to argue that pure statistical learning can cope with the combinatorial explosion inherent in many commonsense reasoning tasks, especially once arithmetical notions are involved.","Further, we argue that bigger is not always better and chasing purely statistical improvements is flawed at the core, since it only exacerbates the dangerous conflation of the production of correct answers with genuine reasoning ability."],"url":"http://arxiv.org/abs/2404.19432v1","category":"cs.CL"}
{"created":"2024-04-30 09:51:21","title":"Rare Events in Extreme Value Statistics of L\u00e9vy Processes","abstract":"We study rare events in the extreme value statistics of stochastic symmetric jump processes with power tails in the distributions of the jumps, using the big jump principle. The principle states that in the presence of stochastic processes with power tails statistics, if at a certain time a physical quantity takes on a value much larger than its typical value, this large fluctuation is realised through a single macroscopic jump that exceeds the typical scale of the process by several orders of magnitude. In particular, our estimation focuses on the asymptotic behaviour of the tail of the probability distribution of maxima, a fundamental quantity in a wide class of stochastic models used in chemistry to estimate reaction thresholds, in climatology for earthquake risk assessment, in finance for portfolio management, and in ecology for the collective behaviour of species. We determine the analytical form of the probability distribution of rare events in the extreme value statistics of three L\\'evy processes; L\\'evy flights, L\\'evy walks and the L\\'evy-Lorentz gas. For the L\\'evy flights, we re-obtain through the big jump approach recent analytical results, extending their validity. For the L\\'evy-Lorentz gas we show that the topology of the disordered lattice along which the walker moves induces memory effects in its dynamics, which influences the extreme value statistics. Our results are confirmed by extensive numerical simulations.","sentences":["We study rare events in the extreme value statistics of stochastic symmetric jump processes with power tails in the distributions of the jumps, using the big jump principle.","The principle states that in the presence of stochastic processes with power tails statistics, if at a certain time a physical quantity takes on a value much larger than its typical value, this large fluctuation is realised through a single macroscopic jump that exceeds the typical scale of the process by several orders of magnitude.","In particular, our estimation focuses on the asymptotic behaviour of the tail of the probability distribution of maxima, a fundamental quantity in a wide class of stochastic models used in chemistry to estimate reaction thresholds, in climatology for earthquake risk assessment, in finance for portfolio management, and in ecology for the collective behaviour of species.","We determine the analytical form of the probability distribution of rare events in the extreme value statistics of three L\\'evy processes; L\\'evy flights, L\\'evy walks and the L\\'evy-Lorentz gas.","For the L\\'evy flights, we re-obtain through the big jump approach recent analytical results, extending their validity.","For the L\\'evy-Lorentz gas we show that the topology of the disordered lattice along which the walker moves induces memory effects in its dynamics, which influences the extreme value statistics.","Our results are confirmed by extensive numerical simulations."],"url":"http://arxiv.org/abs/2404.19406v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 09:48:11","title":"Transformer-Enhanced Motion Planner: Attention-Guided Sampling for State-Specific Decision Making","abstract":"Sampling-based motion planning (SBMP) algorithms are renowned for their robust global search capabilities. However, the inherent randomness in their sampling mechanisms often result in inconsistent path quality and limited search efficiency. In response to these challenges, this work proposes a novel deep learning-based motion planning framework, named Transformer-Enhanced Motion Planner (TEMP), which synergizes an Environmental Information Semantic Encoder (EISE) with a Motion Planning Transformer (MPT). EISE converts environmental data into semantic environmental information (SEI), providing MPT with an enriched environmental comprehension. MPT leverages an attention mechanism to dynamically recalibrate its focus on SEI, task objectives, and historical planning data, refining the sampling node generation. To demonstrate the capabilities of TEMP, we train our model using a dataset comprised of planning results produced by the RRT*. EISE and MPT are collaboratively trained, enabling EISE to autonomously learn and extract patterns from environmental data, thereby forming semantic representations that MPT could more effectively interpret and utilize for motion planning. Subsequently, we conducted a systematic evaluation of TEMP's efficacy across diverse task dimensions, which demonstrates that TEMP achieves exceptional performance metrics and a heightened degree of generalizability compared to state-of-the-art SBMPs.","sentences":["Sampling-based motion planning (SBMP) algorithms are renowned for their robust global search capabilities.","However, the inherent randomness in their sampling mechanisms often result in inconsistent path quality and limited search efficiency.","In response to these challenges, this work proposes a novel deep learning-based motion planning framework, named Transformer-Enhanced Motion Planner (TEMP), which synergizes an Environmental Information Semantic Encoder (EISE) with a Motion Planning Transformer (MPT).","EISE converts environmental data into semantic environmental information (SEI), providing MPT with an enriched environmental comprehension.","MPT leverages an attention mechanism to dynamically recalibrate its focus on SEI, task objectives, and historical planning data, refining the sampling node generation.","To demonstrate the capabilities of TEMP, we train our model using a dataset comprised of planning results produced by the RRT*.","EISE and MPT are collaboratively trained, enabling EISE to autonomously learn and extract patterns from environmental data, thereby forming semantic representations that MPT could more effectively interpret and utilize for motion planning.","Subsequently, we conducted a systematic evaluation of TEMP's efficacy across diverse task dimensions, which demonstrates that TEMP achieves exceptional performance metrics and a heightened degree of generalizability compared to state-of-the-art SBMPs."],"url":"http://arxiv.org/abs/2404.19403v1","category":"cs.RO"}
{"created":"2024-04-30 09:20:35","title":"Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection","abstract":"Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA). These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain. However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background. Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process. To resolve this problem, in this paper, we propose a novel pseudo label refinery framework. Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy. This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box. Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process. We alleviate this issue by generating additional proposals and aligning RoI features across different domains. Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks. Code will be available at https://github.com/Zhanwei-Z/PERE.","sentences":["Recent self-training techniques have shown notable improvements in unsupervised domain adaptation for 3D object detection (3D UDA).","These techniques typically select pseudo labels, i.e., 3D boxes, to supervise models for the target domain.","However, this selection process inevitably introduces unreliable 3D boxes, in which 3D points cannot be definitively assigned as foreground or background.","Previous techniques mitigate this by reweighting these boxes as pseudo labels, but these boxes can still poison the training process.","To resolve this problem, in this paper, we propose a novel pseudo label refinery framework.","Specifically, in the selection process, to improve the reliability of pseudo boxes, we propose a complementary augmentation strategy.","This strategy involves either removing all points within an unreliable box or replacing it with a high-confidence box.","Moreover, the point numbers of instances in high-beam datasets are considerably higher than those in low-beam datasets, also degrading the quality of pseudo labels during the training process.","We alleviate this issue by generating additional proposals and aligning RoI features across different domains.","Experimental results demonstrate that our method effectively enhances the quality of pseudo labels and consistently surpasses the state-of-the-art methods on six autonomous driving benchmarks.","Code will be available at https://github.com/Zhanwei-Z/PERE."],"url":"http://arxiv.org/abs/2404.19384v1","category":"cs.CV"}
{"created":"2024-04-30 09:16:30","title":"Cross-Block Fine-Grained Semantic Cascade for Skeleton-Based Sports Action Recognition","abstract":"Human action video recognition has recently attracted more attention in applications such as video security and sports posture correction. Popular solutions, including graph convolutional networks (GCNs) that model the human skeleton as a spatiotemporal graph, have proven very effective. GCNs-based methods with stacked blocks usually utilize top-layer semantics for classification/annotation purposes. Although the global features learned through the procedure are suitable for the general classification, they have difficulty capturing fine-grained action change across adjacent frames -- decisive factors in sports actions. In this paper, we propose a novel ``Cross-block Fine-grained Semantic Cascade (CFSC)'' module to overcome this challenge. In summary, the proposed CFSC progressively integrates shallow visual knowledge into high-level blocks to allow networks to focus on action details. In particular, the CFSC module utilizes the GCN feature maps produced at different levels, as well as aggregated features from proceeding levels to consolidate fine-grained features. In addition, a dedicated temporal convolution is applied at each level to learn short-term temporal features, which will be carried over from shallow to deep layers to maximize the leverage of low-level details. This cross-block feature aggregation methodology, capable of mitigating the loss of fine-grained information, has resulted in improved performance. Last, FD-7, a new action recognition dataset for fencing sports, was collected and will be made publicly available. Experimental results and empirical analysis on public benchmarks (FSD-10) and self-collected (FD-7) demonstrate the advantage of our CFSC module on learning discriminative patterns for action classification over others.","sentences":["Human action video recognition has recently attracted more attention in applications such as video security and sports posture correction.","Popular solutions, including graph convolutional networks (GCNs) that model the human skeleton as a spatiotemporal graph, have proven very effective.","GCNs-based methods with stacked blocks usually utilize top-layer semantics for classification/annotation purposes.","Although the global features learned through the procedure are suitable for the general classification, they have difficulty capturing fine-grained action change across adjacent frames -- decisive factors in sports actions.","In this paper, we propose a novel ``Cross-block Fine-grained Semantic Cascade (CFSC)''","module to overcome this challenge.","In summary, the proposed CFSC progressively integrates shallow visual knowledge into high-level blocks to allow networks to focus on action details.","In particular, the CFSC module utilizes the GCN feature maps produced at different levels, as well as aggregated features from proceeding levels to consolidate fine-grained features.","In addition, a dedicated temporal convolution is applied at each level to learn short-term temporal features, which will be carried over from shallow to deep layers to maximize the leverage of low-level details.","This cross-block feature aggregation methodology, capable of mitigating the loss of fine-grained information, has resulted in improved performance.","Last, FD-7, a new action recognition dataset for fencing sports, was collected and will be made publicly available.","Experimental results and empirical analysis on public benchmarks (FSD-10) and self-collected (FD-7) demonstrate the advantage of our CFSC module on learning discriminative patterns for action classification over others."],"url":"http://arxiv.org/abs/2404.19383v1","category":"cs.CV"}
{"created":"2024-04-30 09:06:32","title":"Deep low-latency joint speech transmission and enhancement over a gaussian channel","abstract":"Ensuring intelligible speech communication for hearing assistive devices in low-latency scenarios presents significant challenges in terms of speech enhancement, coding and transmission. In this paper, we propose novel solutions for low-latency joint speech transmission and enhancement, leveraging deep neural networks (DNNs). Our approach integrates two state-of-the-art DNN architectures for low-latency speech enhancement and low-latency analog joint source-channel-based transmission, creating a combined low-latency system and jointly training both systems in an end-to-end approach. Due to the computational demands of the enhancement system, this order is suitable when high computational power is unavailable in the decoder, like hearing assistive devices. The proposed system enables the configuration of total latency, achieving high performance even at latencies as low as 3 ms, which is typically challenging to attain. The simulation results provide compelling evidence that a joint enhancement and transmission system is superior to a simple concatenation system in diverse settings, encompassing various wireless channel conditions, latencies, and background noise scenarios.","sentences":["Ensuring intelligible speech communication for hearing assistive devices in low-latency scenarios presents significant challenges in terms of speech enhancement, coding and transmission.","In this paper, we propose novel solutions for low-latency joint speech transmission and enhancement, leveraging deep neural networks (DNNs).","Our approach integrates two state-of-the-art DNN architectures for low-latency speech enhancement and low-latency analog joint source-channel-based transmission, creating a combined low-latency system and jointly training both systems in an end-to-end approach.","Due to the computational demands of the enhancement system, this order is suitable when high computational power is unavailable in the decoder, like hearing assistive devices.","The proposed system enables the configuration of total latency, achieving high performance even at latencies as low as 3 ms, which is typically challenging to attain.","The simulation results provide compelling evidence that a joint enhancement and transmission system is superior to a simple concatenation system in diverse settings, encompassing various wireless channel conditions, latencies, and background noise scenarios."],"url":"http://arxiv.org/abs/2404.19375v1","category":"eess.AS"}
{"created":"2024-04-30 08:58:47","title":"Numeric Reward Machines","abstract":"Reward machines inform reinforcement learning agents about the reward structure of the environment and often drastically speed up the learning process. However, reward machines only accept Boolean features such as robot-reached-gold. Consequently, many inherently numeric tasks cannot profit from the guidance offered by reward machines. To address this gap, we aim to extend reward machines with numeric features such as distance-to-gold. For this, we present two types of reward machines: numeric-Boolean and numeric. In a numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold. In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold. We compare our new approaches to a baseline reward machine in the Craft domain, where the numeric feature is the agent-to-target distance. We use cross-product Q-learning, Q-learning with counter-factual experiences, and the options framework for learning. Our experimental results show that our new approaches significantly outperform the baseline approach. Extending reward machines with numeric features opens up new possibilities of using reward machines in inherently numeric tasks.","sentences":["Reward machines inform reinforcement learning agents about the reward structure of the environment and often drastically speed up the learning process.","However, reward machines only accept Boolean features such as robot-reached-gold.","Consequently, many inherently numeric tasks cannot profit from the guidance offered by reward machines.","To address this gap, we aim to extend reward machines with numeric features such as distance-to-gold.","For this, we present two types of reward machines: numeric-Boolean and numeric.","In a numeric-Boolean reward machine, distance-to-gold is emulated by two Boolean features distance-to-gold-decreased and robot-reached-gold.","In a numeric reward machine, distance-to-gold is used directly alongside the Boolean feature robot-reached-gold.","We compare our new approaches to a baseline reward machine in the Craft domain, where the numeric feature is the agent-to-target distance.","We use cross-product Q-learning, Q-learning with counter-factual experiences, and the options framework for learning.","Our experimental results show that our new approaches significantly outperform the baseline approach.","Extending reward machines with numeric features opens up new possibilities of using reward machines in inherently numeric tasks."],"url":"http://arxiv.org/abs/2404.19370v1","category":"cs.AI"}
{"created":"2024-04-30 08:47:24","title":"Expressivity and Speech Synthesis","abstract":"Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research. From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of. After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances. This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours. In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity. We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms.","sentences":["Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research.","From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of.","After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances.","This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours.","In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity.","We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms."],"url":"http://arxiv.org/abs/2404.19363v1","category":"cs.CL"}
{"created":"2024-04-30 08:45:18","title":"A Negotiator's Backup Plan: Optimal Concessions with a Reservation Value","abstract":"Automated negotiation is a well-known mechanism for autonomous agents to reach agreements. To realize beneficial agreements quickly, it is key to employ a good bidding strategy. When a negotiating agent has a good back-up plan, i.e., a high reservation value, failing to reach an agreement is not necessarily disadvantageous. Thus, the agent can adopt a risk-seeking strategy, aiming for outcomes with a higher utilities.   Accordingly, this paper develops an optimal bidding strategy called MIA-RVelous for bilateral negotiations with private reservation values. The proposed greedy algorithm finds the optimal bid sequence given the agent's beliefs about the opponent in $O(n^2D)$ time, with $D$ the maximum number of rounds and $n$ the number of outcomes. The results obtained here can pave the way to realizing effective concurrent negotiations, given that concurrent negotiations can serve as a (probabilistic) backup plan.","sentences":["Automated negotiation is a well-known mechanism for autonomous agents to reach agreements.","To realize beneficial agreements quickly, it is key to employ a good bidding strategy.","When a negotiating agent has a good back-up plan, i.e., a high reservation value, failing to reach an agreement is not necessarily disadvantageous.","Thus, the agent can adopt a risk-seeking strategy, aiming for outcomes with a higher utilities.   ","Accordingly, this paper develops an optimal bidding strategy called MIA-RVelous for bilateral negotiations with private reservation values.","The proposed greedy algorithm finds the optimal bid sequence given the agent's beliefs about the opponent in $O(n^2D)$ time, with $D$ the maximum number of rounds and $n$ the number of outcomes.","The results obtained here can pave the way to realizing effective concurrent negotiations, given that concurrent negotiations can serve as a (probabilistic) backup plan."],"url":"http://arxiv.org/abs/2404.19361v1","category":"cs.GT"}
{"created":"2024-04-30 08:41:06","title":"Evaluating Lexicon Incorporation for Depression Symptom Estimation","abstract":"This paper explores the impact of incorporating sentiment, emotion, and domain-specific lexicons into a transformer-based model for depression symptom estimation. Lexicon information is added by marking the words in the input transcripts of patient-therapist conversations as well as in social media posts. Overall results show that the introduction of external knowledge within pre-trained language models can be beneficial for prediction performance, while different lexicons show distinct behaviours depending on the targeted task. Additionally, new state-of-the-art results are obtained for the estimation of depression level over patient-therapist interviews.","sentences":["This paper explores the impact of incorporating sentiment, emotion, and domain-specific lexicons into a transformer-based model for depression symptom estimation.","Lexicon information is added by marking the words in the input transcripts of patient-therapist conversations as well as in social media posts.","Overall results show that the introduction of external knowledge within pre-trained language models can be beneficial for prediction performance, while different lexicons show distinct behaviours depending on the targeted task.","Additionally, new state-of-the-art results are obtained for the estimation of depression level over patient-therapist interviews."],"url":"http://arxiv.org/abs/2404.19359v1","category":"cs.CL"}
{"created":"2024-04-30 08:39:43","title":"QML-IB: Quantized Collaborative Intelligence between Multiple Devices and the Mobile Network","abstract":"The integration of artificial intelligence (AI) and mobile networks is regarded as one of the most important scenarios for 6G. In 6G, a major objective is to realize the efficient transmission of task-relevant data. Then a key problem arises, how to design collaborative AI models for the device side and the network side, so that the transmitted data between the device and the network is efficient enough, which means the transmission overhead is low but the AI task result is accurate. In this paper, we propose the multi-link information bottleneck (ML-IB) scheme for such collaborative models design. We formulate our problem based on a novel performance metric, which can evaluate both task accuracy and transmission overhead. Then we introduce a quantizer that is adjustable in the quantization bit depth, amplitudes, and breakpoints. Given the infeasibility of calculating our proposed metric on high-dimensional data, we establish a variational upper bound for this metric. However, due to the incorporation of quantization, the closed form of the variational upper bound remains uncomputable. Hence, we employ the Log-Sum Inequality to derive an approximation and provide a theoretical guarantee. Based on this, we devise the quantized multi-link information bottleneck (QML-IB) algorithm for collaborative AI models generation. Finally, numerical experiments demonstrate the superior performance of our QML-IB algorithm compared to the state-of-the-art algorithm.","sentences":["The integration of artificial intelligence (AI) and mobile networks is regarded as one of the most important scenarios for 6G.","In 6G, a major objective is to realize the efficient transmission of task-relevant data.","Then a key problem arises, how to design collaborative AI models for the device side and the network side, so that the transmitted data between the device and the network is efficient enough, which means the transmission overhead is low but the AI task result is accurate.","In this paper, we propose the multi-link information bottleneck (ML-IB) scheme for such collaborative models design.","We formulate our problem based on a novel performance metric, which can evaluate both task accuracy and transmission overhead.","Then we introduce a quantizer that is adjustable in the quantization bit depth, amplitudes, and breakpoints.","Given the infeasibility of calculating our proposed metric on high-dimensional data, we establish a variational upper bound for this metric.","However, due to the incorporation of quantization, the closed form of the variational upper bound remains uncomputable.","Hence, we employ the Log-Sum Inequality to derive an approximation and provide a theoretical guarantee.","Based on this, we devise the quantized multi-link information bottleneck (QML-IB) algorithm for collaborative AI models generation.","Finally, numerical experiments demonstrate the superior performance of our QML-IB algorithm compared to the state-of-the-art algorithm."],"url":"http://arxiv.org/abs/2404.19358v1","category":"cs.IT"}
{"created":"2024-04-30 08:20:31","title":"Human-AI Interaction in Industrial Robotics: Design and Empirical Evaluation of a User Interface for Explainable AI-Based Robot Program Optimization","abstract":"While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study.","sentences":["While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited.","We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications.","To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study."],"url":"http://arxiv.org/abs/2404.19349v1","category":"cs.RO"}
{"created":"2024-04-30 08:16:52","title":"Pessimistic Value Iteration for Multi-Task Data Sharing in Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) has shown promising results in learning a task-specific policy from a fixed dataset. However, successful offline RL often relies heavily on the coverage and quality of the given dataset. In scenarios where the dataset for a specific task is limited, a natural approach is to improve offline RL with datasets from other tasks, namely, to conduct Multi-Task Data Sharing (MTDS). Nevertheless, directly sharing datasets from other tasks exacerbates the distribution shift in offline RL. In this paper, we propose an uncertainty-based MTDS approach that shares the entire dataset without data selection. Given ensemble-based uncertainty quantification, we perform pessimistic value iteration on the shared offline dataset, which provides a unified framework for single- and multi-task offline RL. We further provide theoretical analysis, which shows that the optimality gap of our method is only related to the expected data coverage of the shared dataset, thus resolving the distribution shift issue in data sharing. Empirically, we release an MTDS benchmark and collect datasets from three challenging domains. The experimental results show our algorithm outperforms the previous state-of-the-art methods in challenging MTDS problems. See https://github.com/Baichenjia/UTDS for the datasets and code.","sentences":["Offline Reinforcement Learning (RL) has shown promising results in learning a task-specific policy from a fixed dataset.","However, successful offline RL often relies heavily on the coverage and quality of the given dataset.","In scenarios where the dataset for a specific task is limited, a natural approach is to improve offline RL with datasets from other tasks, namely, to conduct Multi-Task Data Sharing (MTDS).","Nevertheless, directly sharing datasets from other tasks exacerbates the distribution shift in offline RL.","In this paper, we propose an uncertainty-based MTDS approach that shares the entire dataset without data selection.","Given ensemble-based uncertainty quantification, we perform pessimistic value iteration on the shared offline dataset, which provides a unified framework for single- and multi-task offline RL.","We further provide theoretical analysis, which shows that the optimality gap of our method is only related to the expected data coverage of the shared dataset, thus resolving the distribution shift issue in data sharing.","Empirically, we release an MTDS benchmark and collect datasets from three challenging domains.","The experimental results show our algorithm outperforms the previous state-of-the-art methods in challenging MTDS problems.","See https://github.com/Baichenjia/UTDS for the datasets and code."],"url":"http://arxiv.org/abs/2404.19346v1","category":"cs.LG"}
{"created":"2024-04-30 08:06:04","title":"Reliable or Deceptive? Investigating Gated Features for Smooth Visual Explanations in CNNs","abstract":"Deep learning models have achieved remarkable success across diverse domains. However, the intricate nature of these models often impedes a clear understanding of their decision-making processes. This is where Explainable AI (XAI) becomes indispensable, offering intuitive explanations for model decisions. In this work, we propose a simple yet highly effective approach, ScoreCAM++, which introduces modifications to enhance the promising ScoreCAM method for visual explainability. Our proposed approach involves altering the normalization function within the activation layer utilized in ScoreCAM, resulting in significantly improved results compared to previous efforts. Additionally, we apply an activation function to the upsampled activation layers to enhance interpretability. This improvement is achieved by selectively gating lower-priority values within the activation layer. Through extensive experiments and qualitative comparisons, we demonstrate that ScoreCAM++ consistently achieves notably superior performance and fairness in interpreting the decision-making process compared to both ScoreCAM and previous methods.","sentences":["Deep learning models have achieved remarkable success across diverse domains.","However, the intricate nature of these models often impedes a clear understanding of their decision-making processes.","This is where Explainable AI (XAI) becomes indispensable, offering intuitive explanations for model decisions.","In this work, we propose a simple yet highly effective approach, ScoreCAM++, which introduces modifications to enhance the promising ScoreCAM method for visual explainability.","Our proposed approach involves altering the normalization function within the activation layer utilized in ScoreCAM, resulting in significantly improved results compared to previous efforts.","Additionally, we apply an activation function to the upsampled activation layers to enhance interpretability.","This improvement is achieved by selectively gating lower-priority values within the activation layer.","Through extensive experiments and qualitative comparisons, we demonstrate that ScoreCAM++ consistently achieves notably superior performance and fairness in interpreting the decision-making process compared to both ScoreCAM and previous methods."],"url":"http://arxiv.org/abs/2404.19341v1","category":"cs.CV"}
{"created":"2024-04-30 08:04:06","title":"Design of a Representation Information Repository for the Long-Term Usability of Digital Building Documents","abstract":"The long-term usability of digital building documents is essential for the maintenance and optimization of infrastructure portfolios. It supports the preservation of building-specific knowledge and the cultural heritage hidden within. However, having to do this throughout the lifecycle of a building - or even indefinitely - remains a major challenge. This is especially true for organizations responsible for large collections of digital building documents, such as public administrations or archives. In this article, we first describe the challenges and requirements associated with preservation tasks, and then introduce the concept of so-called representation information within BIM (Building Information Modeling). This type of information is important to give meaning to the stored bit sequences for a particular community. Then, we design a repository for representation information and introduce some so-called 23 BIMcore content elements. Finally, we focus on BIM and the construction sector and explain how the proposed repository can be used to implement the two concepts introduced in the ISO reference model OAIS (Open Archival Information System), namely the representation information and the context information, as well as the concept of significant properties, which has not yet been explicitly modeled in OAIS.","sentences":["The long-term usability of digital building documents is essential for the maintenance and optimization of infrastructure portfolios.","It supports the preservation of building-specific knowledge and the cultural heritage hidden within.","However, having to do this throughout the lifecycle of a building - or even indefinitely - remains a major challenge.","This is especially true for organizations responsible for large collections of digital building documents, such as public administrations or archives.","In this article, we first describe the challenges and requirements associated with preservation tasks, and then introduce the concept of so-called representation information within BIM (Building Information Modeling).","This type of information is important to give meaning to the stored bit sequences for a particular community.","Then, we design a repository for representation information and introduce some so-called 23 BIMcore content elements.","Finally, we focus on BIM and the construction sector and explain how the proposed repository can be used to implement the two concepts introduced in the ISO reference model OAIS (Open Archival Information System), namely the representation information and the context information, as well as the concept of significant properties, which has not yet been explicitly modeled in OAIS."],"url":"http://arxiv.org/abs/2404.19337v1","category":"cs.DL"}
{"created":"2024-04-30 08:03:22","title":"Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts","abstract":"LLMs trained in the understanding of programming syntax are now providing effective assistance to developers and are being used in programming education such as in generation of coding problem examples or providing code explanations. A key aspect of programming education is understanding and dealing with error message. However, 'logical errors' in which the program operates against the programmer's intentions do not receive error messages from the compiler. In this study, building on existing research on programming errors, we first define the types of logical errors that can occur in programming in general. Based on the definition, we propose an effective approach for detecting logical errors with LLMs that makes use of relations among error types in the Chain-of-Thought and Tree-of-Thought prompts. The experimental results indicate that when such logical error descriptions in the prompt are used, the average classifition performance is about 21% higher than the ones without them. We also conducted an experiment for exploiting the relations among errors in generating a new logical error dataset using LLMs. As there is very limited dataset for logical errors such benchmark dataset can be very useful for various programming related applications. We expect that our work can assist novice programmers in identifying the causes of code errors and correct them more effectively.","sentences":["LLMs trained in the understanding of programming syntax are now providing effective assistance to developers and are being used in programming education such as in generation of coding problem examples or providing code explanations.","A key aspect of programming education is understanding and dealing with error message.","However, 'logical errors' in which the program operates against the programmer's intentions do not receive error messages from the compiler.","In this study, building on existing research on programming errors, we first define the types of logical errors that can occur in programming in general.","Based on the definition, we propose an effective approach for detecting logical errors with LLMs that makes use of relations among error types in the Chain-of-Thought and Tree-of-Thought prompts.","The experimental results indicate that when such logical error descriptions in the prompt are used, the average classifition performance is about 21% higher than the ones without them.","We also conducted an experiment for exploiting the relations among errors in generating a new logical error dataset using LLMs.","As there is very limited dataset for logical errors such benchmark dataset can be very useful for various programming related applications.","We expect that our work can assist novice programmers in identifying the causes of code errors and correct them more effectively."],"url":"http://arxiv.org/abs/2404.19336v2","category":"cs.AI"}
{"created":"2024-04-30 07:53:34","title":"G2LTraj: A Global-to-Local Generation Approach for Trajectory Prediction","abstract":"Predicting future trajectories of traffic agents accurately holds substantial importance in various applications such as autonomous driving. Previous methods commonly infer all future steps of an agent either recursively or simultaneously. However, the recursive strategy suffers from the accumulated error, while the simultaneous strategy overlooks the constraints among future steps, resulting in kinematically infeasible predictions. To address these issues, in this paper, we propose G2LTraj, a plug-and-play global-to-local generation approach for trajectory prediction. Specifically, we generate a series of global key steps that uniformly cover the entire future time range. Subsequently, the local intermediate steps between the adjacent key steps are recursively filled in. In this way, we prevent the accumulated error from propagating beyond the adjacent key steps. Moreover, to boost the kinematical feasibility, we not only introduce the spatial constraints among key steps but also strengthen the temporal constraints among the intermediate steps. Finally, to ensure the optimal granularity of key steps, we design a selectable granularity strategy that caters to each predicted trajectory. Our G2LTraj significantly improves the performance of seven existing trajectory predictors across the ETH, UCY and nuScenes datasets. Experimental results demonstrate its effectiveness. Code will be available at https://github.com/Zhanwei-Z/G2LTraj.","sentences":["Predicting future trajectories of traffic agents accurately holds substantial importance in various applications such as autonomous driving.","Previous methods commonly infer all future steps of an agent either recursively or simultaneously.","However, the recursive strategy suffers from the accumulated error, while the simultaneous strategy overlooks the constraints among future steps, resulting in kinematically infeasible predictions.","To address these issues, in this paper, we propose G2LTraj, a plug-and-play global-to-local generation approach for trajectory prediction.","Specifically, we generate a series of global key steps that uniformly cover the entire future time range.","Subsequently, the local intermediate steps between the adjacent key steps are recursively filled in.","In this way, we prevent the accumulated error from propagating beyond the adjacent key steps.","Moreover, to boost the kinematical feasibility, we not only introduce the spatial constraints among key steps but also strengthen the temporal constraints among the intermediate steps.","Finally, to ensure the optimal granularity of key steps, we design a selectable granularity strategy that caters to each predicted trajectory.","Our G2LTraj significantly improves the performance of seven existing trajectory predictors across the ETH, UCY and nuScenes datasets.","Experimental results demonstrate its effectiveness.","Code will be available at https://github.com/Zhanwei-Z/G2LTraj."],"url":"http://arxiv.org/abs/2404.19330v1","category":"cs.CV"}
{"created":"2024-04-30 07:18:10","title":"Comprehensive Forecasting-Based Analysis of Hybrid and Stacked Stateful/ Stateless Models","abstract":"Wind speed is a powerful source of renewable energy, which can be used as an alternative to the non-renewable resources for production of electricity. Renewable sources are clean, infinite and do not impact the environment negatively during production of electrical energy. However, while eliciting electrical energy from renewable resources viz. solar irradiance, wind speed, hydro should require special planning failing which may result in huge loss of labour and money for setting up the system. In this paper, we discuss four deep recurrent neural networks viz. Stacked Stateless LSTM, Stacked Stateless GRU, Stacked Stateful LSTM and Statcked Stateful GRU which will be used to predict wind speed on a short-term basis for the airport sites beside two campuses of Mississippi State University. The paper does a comprehensive analysis of the performance of the models used describing their architectures and how efficiently they elicit the results with the help of RMSE values. A detailed description of the time and space complexities of the above models has also been discussed.","sentences":["Wind speed is a powerful source of renewable energy, which can be used as an alternative to the non-renewable resources for production of electricity.","Renewable sources are clean, infinite and do not impact the environment negatively during production of electrical energy.","However, while eliciting electrical energy from renewable resources viz.","solar irradiance, wind speed, hydro should require special planning failing which may result in huge loss of labour and money for setting up the system.","In this paper, we discuss four deep recurrent neural networks viz.","Stacked Stateless LSTM, Stacked Stateless GRU, Stacked Stateful LSTM and Statcked Stateful GRU which will be used to predict wind speed on a short-term basis for the airport sites beside two campuses of Mississippi State University.","The paper does a comprehensive analysis of the performance of the models used describing their architectures and how efficiently they elicit the results with the help of RMSE values.","A detailed description of the time and space complexities of the above models has also been discussed."],"url":"http://arxiv.org/abs/2404.19306v1","category":"cs.LG"}
{"created":"2024-04-30 07:07:45","title":"Data Set Terminology of Artificial Intelligence in Medicine: A Historical Review and Recommendation","abstract":"Medicine and artificial intelligence (AI) engineering represent two distinct fields each with decades of published history. With such history comes a set of terminology that has a specific way in which it is applied. However, when two distinct fields with overlapping terminology start to collaborate, miscommunication and misunderstandings can occur. This narrative review aims to give historical context for these terms, accentuate the importance of clarity when these terms are used in medical AI contexts, and offer solutions to mitigate misunderstandings by readers from either field. Through an examination of historical documents, including articles, writing guidelines, and textbooks, this review traces the divergent evolution of terms for data sets and their impact. Initially, the discordant interpretations of the word 'validation' in medical and AI contexts are explored. Then the data sets used for AI evaluation are classified, namely random splitting, cross-validation, temporal, geographic, internal, and external sets. The accurate and standardized description of these data sets is crucial for demonstrating the robustness and generalizability of AI applications in medicine. This review clarifies existing literature to provide a comprehensive understanding of these classifications and their implications in AI evaluation. This review then identifies often misunderstood terms and proposes pragmatic solutions to mitigate terminological confusion. Among these solutions are the use of standardized terminology such as 'training set,' 'validation (or tuning) set,' and 'test set,' and explicit definition of data set splitting terminologies in each medical AI research publication. This review aspires to enhance the precision of communication in medical AI, thereby fostering more effective and transparent research methodologies in this interdisciplinary field.","sentences":["Medicine and artificial intelligence (AI) engineering represent two distinct fields each with decades of published history.","With such history comes a set of terminology that has a specific way in which it is applied.","However, when two distinct fields with overlapping terminology start to collaborate, miscommunication and misunderstandings can occur.","This narrative review aims to give historical context for these terms, accentuate the importance of clarity when these terms are used in medical AI contexts, and offer solutions to mitigate misunderstandings by readers from either field.","Through an examination of historical documents, including articles, writing guidelines, and textbooks, this review traces the divergent evolution of terms for data sets and their impact.","Initially, the discordant interpretations of the word 'validation' in medical and AI contexts are explored.","Then the data sets used for AI evaluation are classified, namely random splitting, cross-validation, temporal, geographic, internal, and external sets.","The accurate and standardized description of these data sets is crucial for demonstrating the robustness and generalizability of AI applications in medicine.","This review clarifies existing literature to provide a comprehensive understanding of these classifications and their implications in AI evaluation.","This review then identifies often misunderstood terms and proposes pragmatic solutions to mitigate terminological confusion.","Among these solutions are the use of standardized terminology such as 'training set,' 'validation (or tuning) set,' and 'test set,' and explicit definition of data set splitting terminologies in each medical AI research publication.","This review aspires to enhance the precision of communication in medical AI, thereby fostering more effective and transparent research methodologies in this interdisciplinary field."],"url":"http://arxiv.org/abs/2404.19303v1","category":"cs.AI"}
{"created":"2024-04-30 06:59:12","title":"Channel Performance Metrics and Evaluation for XR Head-Mounted Displays with mmWave Arrays","abstract":"Millimeter-wave (mmWave) technology holds the potential to revolutionize head-mounted displays (HMDs) by enabling high-speed wireless communication with nearby processing nodes, where complex video rendering can take place. However, the sparse angular profile of mmWave channels, coupled with the narrow field of view (FoV) of patch-antenna arrays and frequent HMD rotation, can lead to poor performance.   We introduce six channel performance metrics to evaluate the performance of an HMD equipped with mmWave arrays. We analyze the metrics using analytical models, discuss their impact for the application, and apply them to 28 GHz channel sounding data, collected in a conference room using eight HMD patch-antenna arrays, offset by 45 degrees from each other in azimuth.   Our findings confirm that a single array performs poorly due to the narrow FoV, and featuring multiple arrays along the HMD's azimuth is required. Namely, the broader FoV stabilizes channel gain during HMD rotation, lessens the attenuation caused by line of sight (LoS) obstruction, and increases the channel's spatial multiplexing capability. In light of our findings, we conclude that it is imperative to either equip the HMD with multiple arrays or, as an alternative approach, incorporate macroscopic diversity by leveraging distributed access point (AP) infrastructure.","sentences":["Millimeter-wave (mmWave) technology holds the potential to revolutionize head-mounted displays (HMDs) by enabling high-speed wireless communication with nearby processing nodes, where complex video rendering can take place.","However, the sparse angular profile of mmWave channels, coupled with the narrow field of view (FoV) of patch-antenna arrays and frequent HMD rotation, can lead to poor performance.   ","We introduce six channel performance metrics to evaluate the performance of an HMD equipped with mmWave arrays.","We analyze the metrics using analytical models, discuss their impact for the application, and apply them to 28 GHz channel sounding data, collected in a conference room using eight HMD patch-antenna arrays, offset by 45 degrees from each other in azimuth.   ","Our findings confirm that a single array performs poorly due to the narrow FoV, and featuring multiple arrays along the HMD's azimuth is required.","Namely, the broader FoV stabilizes channel gain during HMD rotation, lessens the attenuation caused by line of sight (LoS) obstruction, and increases the channel's spatial multiplexing capability.","In light of our findings, we conclude that it is imperative to either equip the HMD with multiple arrays or, as an alternative approach, incorporate macroscopic diversity by leveraging distributed access point (AP) infrastructure."],"url":"http://arxiv.org/abs/2404.19297v1","category":"eess.SP"}
{"created":"2024-04-30 06:55:45","title":"Octopus v4: Graph of language models","abstract":"Language models have been effective in a wide range of applications, yet the most sophisticated models are often proprietary. For example, GPT-4 by OpenAI and various models by Anthropic are expensive and consume substantial energy. In contrast, the open-source community has produced competitive models, like Llama3. Furthermore, niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts. This paper introduces a novel approach that employs \\textit{functional tokens} to integrate \\textbf{multiple open-source models}, each optimized for particular tasks. Our newly developed Octopus v4 model leverages \\textit{functional tokens} to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance. Octopus v4, an evolution of the Octopus v1, v2, and v3 models, excels in selection and parameter understanding and reformatting. Additionally, we explore the use of graph as a versatile data structure that effectively coordinates multiple open-source models by harnessing the capabilities of the Octopus model and \\textit{functional tokens}. Use our open-sourced GitHub (\\url{https://www.nexa4ai.com/}) to try Octopus v4 models (\\url{https://huggingface.co/NexaAIDev/Octopus-v4}), and contrite to a larger graph of language models. By activating models less than 10B parameters, we achieved SOTA MMLU score of 74.8 among the same level models.","sentences":["Language models have been effective in a wide range of applications, yet the most sophisticated models are often proprietary.","For example, GPT-4 by OpenAI and various models by Anthropic are expensive and consume substantial energy.","In contrast, the open-source community has produced competitive models, like Llama3.","Furthermore, niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts.","This paper introduces a novel approach that employs \\textit{functional tokens} to integrate \\textbf{multiple open-source models}, each optimized for particular tasks.","Our newly developed Octopus v4 model leverages \\textit{functional tokens} to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance.","Octopus v4, an evolution of the Octopus v1, v2, and v3 models, excels in selection and parameter understanding and reformatting.","Additionally, we explore the use of graph as a versatile data structure that effectively coordinates multiple open-source models by harnessing the capabilities of the Octopus model and \\textit{functional tokens}.","Use our open-sourced GitHub (\\url{https://www.nexa4ai.com/}) to try Octopus v4 models (\\url{https://huggingface.co/NexaAIDev/Octopus-v4}), and contrite to a larger graph of language models.","By activating models less than 10B parameters, we achieved SOTA MMLU score of 74.8 among the same level models."],"url":"http://arxiv.org/abs/2404.19296v1","category":"cs.CL"}
{"created":"2024-04-30 06:36:43","title":"Training-free Graph Neural Networks and the Power of Labels as Features","abstract":"We propose training-free graph neural networks (TFGNNs), which can be used without training and can also be improved with optional training, for transductive node classification. We first advocate labels as features (LaF), which is an admissible but not explored technique. We show that LaF provably enhances the expressive power of graph neural networks. We design TFGNNs based on this analysis. In the experiments, we confirm that TFGNNs outperform existing GNNs in the training-free setting and converge with much fewer training iterations than traditional GNNs.","sentences":["We propose training-free graph neural networks (TFGNNs), which can be used without training and can also be improved with optional training, for transductive node classification.","We first advocate labels as features (LaF), which is an admissible but not explored technique.","We show that LaF provably enhances the expressive power of graph neural networks.","We design TFGNNs based on this analysis.","In the experiments, we confirm that TFGNNs outperform existing GNNs in the training-free setting and converge with much fewer training iterations than traditional GNNs."],"url":"http://arxiv.org/abs/2404.19288v1","category":"cs.LG"}
{"created":"2024-04-30 06:21:44","title":"Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation","abstract":"Approximate k-Nearest Neighbour (ANN) methods are often used for mining information and aiding machine learning on large scale high-dimensional datasets. ANN methods typically differ in the index structure used for accelerating searches, resulting in various recall/runtime trade-off points. For applications with static datasets, runtime constraints and dataset properties can be used to empirically select an ANN method with suitable operating characteristics. However, for applications with dynamic datasets, which are subject to frequent online changes (like addition of new samples), there is currently no consensus as to which ANN methods are most suitable. Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the frequency and size of index updates. To address this, we empirically evaluate 5 popular ANN methods on two main applications (online data collection and online feature learning) while taking into account these considerations. Two dynamic datasets are used, derived from the SIFT1M dataset with 1 million samples and the DEEP1B dataset with 1 billion samples. The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method. For online data collection, the Hierarchical Navigable Small World Graphs method achieves a consistent speedup over baseline across a wide range of recall rates. For online feature learning, the Scalable Nearest Neighbours method is faster than baseline for recall rates below 75%.","sentences":["Approximate k-Nearest Neighbour (ANN) methods are often used for mining information and aiding machine learning on large scale high-dimensional datasets.","ANN methods typically differ in the index structure used for accelerating searches, resulting in various recall/runtime trade-off points.","For applications with static datasets, runtime constraints and dataset properties can be used to empirically select an ANN method with suitable operating characteristics.","However, for applications with dynamic datasets, which are subject to frequent online changes (like addition of new samples), there is currently no consensus as to which ANN methods are most suitable.","Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the frequency and size of index updates.","To address this, we empirically evaluate 5 popular ANN methods on two main applications (online data collection and online feature learning) while taking into account these considerations.","Two dynamic datasets are used, derived from the SIFT1M dataset with 1 million samples and the DEEP1B dataset with 1 billion samples.","The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method.","For online data collection, the Hierarchical Navigable Small World Graphs method achieves a consistent speedup over baseline across a wide range of recall rates.","For online feature learning, the Scalable Nearest Neighbours method is faster than baseline for recall rates below 75%."],"url":"http://arxiv.org/abs/2404.19284v1","category":"cs.LG"}
{"created":"2024-04-30 06:21:42","title":"MAP-Former: Multi-Agent-Pair Gaussian Joint Prediction","abstract":"There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed. Closing this gap necessitates advancements in prediction beyond current practices. Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents. Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents. Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories. This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene. We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions. Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs.","sentences":["There is a gap in risk assessment of trajectories between the trajectory information coming from a traffic motion prediction module and what is actually needed.","Closing this gap necessitates advancements in prediction beyond current practices.","Existing prediction models yield joint predictions of agents' future trajectories with uncertainty weights or marginal Gaussian probability density functions (PDFs) for single agents.","Although, these methods achieve high accurate trajectory predictions, they only provide little or no information about the dependencies of interacting agents.","Since traffic is a process of highly interdependent agents, whose actions directly influence their mutual behavior, the existing methods are not sufficient to reliably assess the risk of future trajectories.","This paper addresses that gap by introducing a novel approach to motion prediction, focusing on predicting agent-pair covariance matrices in a ``scene-centric'' manner, which can then be used to model Gaussian joint PDFs for all agent-pairs in a scene.","We propose a model capable of predicting those agent-pair covariance matrices, leveraging an enhanced awareness of interactions.","Utilizing the prediction results of our model, this work forms the foundation for comprehensive risk assessment with statistically based methods for analyzing agents' relations by their joint PDFs."],"url":"http://arxiv.org/abs/2404.19283v1","category":"cs.LG"}
{"created":"2024-04-30 06:12:05","title":"Audio-Visual Traffic Light State Detection for Urban Robots","abstract":"We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings. This is a challenging problem because of the visual occlusions and noise from robot locomotion. Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors. The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability. Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion. This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics.","sentences":["We present a multimodal traffic light state detection using vision and sound, from the viewpoint of a quadruped robot navigating in urban settings.","This is a challenging problem because of the visual occlusions and noise from robot locomotion.","Our method combines features from raw audio with the ratios of red and green pixels within bounding boxes, identified by established vision-based detectors.","The fusion method aggregates features across multiple frames in a given timeframe, increasing robustness and adaptability.","Results show that our approach effectively addresses the challenge of visual occlusion and surpasses the performance of single-modality solutions when the robot is in motion.","This study serves as a proof of concept, highlighting the significant, yet often overlooked, potential of multi-modal perception in robotics."],"url":"http://arxiv.org/abs/2404.19281v1","category":"cs.RO"}
{"created":"2024-04-30 04:44:19","title":"Persistent Homology generalizations for Social Media Network Analysis","abstract":"This study details an approach for the analysis of social media collected political data through the lens of Topological Data Analysis, with a specific focus on Persistent Homology and the political processes they represent by proposing a set of mathematical generalizations using Gaussian functions to define and analyze these Persistent Homology categories. Three distinct types of Persistent Homologies were recurrent across datasets that had been plotted through retweeting patterns and analyzed through the k-Nearest-Neighbor filtrations. As these Persistent Homologies continued to appear, they were then categorized and dubbed Nuclear, Bipolar, and Multipolar Constellations. Upon investigating the content of these plotted tweets, specific patterns of interaction and political information dissemination were identified, namely Political Personalism and Political Polarization. Through clustering and application of Gaussian density functions, I have mathematically characterized each category, encapsulating their distinctive topological features. The mathematical generalizations of Bipolar, Nuclear, and Multipolar Constellations developed in this study are designed to inspire other political science digital media researchers to utilize these categories as to identify Persistent Homology in datasets derived from various social media platforms, suggesting the broader hypothesis that such structures are bound to be present on political scraped data regardless of the social media it's derived from. This method aims to offer a new perspective in Network Analysis as it allows for an exploration of the underlying shape of the networks formed by retweeting patterns, enhancing the understanding of digital interactions within the sphere of Computational Social Sciences.","sentences":["This study details an approach for the analysis of social media collected political data through the lens of Topological Data Analysis, with a specific focus on Persistent Homology and the political processes they represent by proposing a set of mathematical generalizations using Gaussian functions to define and analyze these Persistent Homology categories.","Three distinct types of Persistent Homologies were recurrent across datasets that had been plotted through retweeting patterns and analyzed through the k-Nearest-Neighbor filtrations.","As these Persistent Homologies continued to appear, they were then categorized and dubbed Nuclear, Bipolar, and Multipolar Constellations.","Upon investigating the content of these plotted tweets, specific patterns of interaction and political information dissemination were identified, namely Political Personalism and Political Polarization.","Through clustering and application of Gaussian density functions, I have mathematically characterized each category, encapsulating their distinctive topological features.","The mathematical generalizations of Bipolar, Nuclear, and Multipolar Constellations developed in this study are designed to inspire other political science digital media researchers to utilize these categories as to identify Persistent Homology in datasets derived from various social media platforms, suggesting the broader hypothesis that such structures are bound to be present on political scraped data regardless of the social media it's derived from.","This method aims to offer a new perspective in Network Analysis as it allows for an exploration of the underlying shape of the networks formed by retweeting patterns, enhancing the understanding of digital interactions within the sphere of Computational Social Sciences."],"url":"http://arxiv.org/abs/2404.19257v1","category":"cs.CY"}
{"created":"2024-04-30 04:41:47","title":"Bias Mitigation via Compensation: A Reinforcement Learning Perspective","abstract":"As AI increasingly integrates with human decision-making, we must carefully consider interactions between the two. In particular, current approaches focus on optimizing individual agent actions but often overlook the nuances of collective intelligence. Group dynamics might require that one agent (e.g., the AI system) compensate for biases and errors in another agent (e.g., the human), but this compensation should be carefully developed. We provide a theoretical framework for algorithmic compensation that synthesizes game theory and reinforcement learning principles to demonstrate the natural emergence of deceptive outcomes from the continuous learning dynamics of agents. We provide simulation results involving Markov Decision Processes (MDP) learning to interact. This work then underpins our ethical analysis of the conditions in which AI agents should adapt to biases and behaviors of other agents in dynamic and complex decision-making environments. Overall, our approach addresses the nuanced role of strategic deception of humans, challenging previous assumptions about its detrimental effects. We assert that compensation for others' biases can enhance coordination and ethical alignment: strategic deception, when ethically managed, can positively shape human-AI interactions.","sentences":["As AI increasingly integrates with human decision-making, we must carefully consider interactions between the two.","In particular, current approaches focus on optimizing individual agent actions but often overlook the nuances of collective intelligence.","Group dynamics might require that one agent (e.g., the AI system) compensate for biases and errors in another agent (e.g., the human), but this compensation should be carefully developed.","We provide a theoretical framework for algorithmic compensation that synthesizes game theory and reinforcement learning principles to demonstrate the natural emergence of deceptive outcomes from the continuous learning dynamics of agents.","We provide simulation results involving Markov Decision Processes (MDP) learning to interact.","This work then underpins our ethical analysis of the conditions in which AI agents should adapt to biases and behaviors of other agents in dynamic and complex decision-making environments.","Overall, our approach addresses the nuanced role of strategic deception of humans, challenging previous assumptions about its detrimental effects.","We assert that compensation for others' biases can enhance coordination and ethical alignment: strategic deception, when ethically managed, can positively shape human-AI interactions."],"url":"http://arxiv.org/abs/2404.19256v1","category":"cs.AI"}
{"created":"2024-04-30 04:19:17","title":"Suvach -- Generated Hindi QA benchmark","abstract":"Current evaluation benchmarks for question answering (QA) in Indic languages often rely on machine translation of existing English datasets. This approach suffers from bias and inaccuracies inherent in machine translation, leading to datasets that may not reflect the true capabilities of EQA models for Indic languages. This paper proposes a new benchmark specifically designed for evaluating Hindi EQA models and discusses the methodology to do the same for any task. This method leverages large language models (LLMs) to generate a high-quality dataset in an extractive setting, ensuring its relevance for the target language. We believe this new resource will foster advancements in Hindi NLP research by providing a more accurate and reliable evaluation tool.","sentences":["Current evaluation benchmarks for question answering (QA) in Indic languages often rely on machine translation of existing English datasets.","This approach suffers from bias and inaccuracies inherent in machine translation, leading to datasets that may not reflect the true capabilities of EQA models for Indic languages.","This paper proposes a new benchmark specifically designed for evaluating Hindi EQA models and discusses the methodology to do the same for any task.","This method leverages large language models (LLMs) to generate a high-quality dataset in an extractive setting, ensuring its relevance for the target language.","We believe this new resource will foster advancements in Hindi NLP research by providing a more accurate and reliable evaluation tool."],"url":"http://arxiv.org/abs/2404.19254v1","category":"cs.CL"}
{"created":"2024-04-30 04:18:21","title":"Learning to Communicate Functional States with Nonverbal Expressions for Improved Human-Robot Collaboration","abstract":"Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction. Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors. In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck). We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions. The proposed approach was evaluated through a user study with 24 participants. The results demonstrate that: 1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot. 2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process. 3. The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state. 4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states.","sentences":["Collaborative robots must effectively communicate their internal state to humans to enable a smooth interaction.","Nonverbal communication is widely used to communicate information during human-robot interaction, however, such methods may also be misunderstood, leading to communication errors.","In this work, we explore modulating the acoustic parameter values (pitch bend, beats per minute, beats per loop) of nonverbal auditory expressions to convey functional robot states (accomplished, progressing, stuck).","We propose a reinforcement learning (RL) algorithm based on noisy human feedback to produce accurately interpreted nonverbal auditory expressions.","The proposed approach was evaluated through a user study with 24 participants.","The results demonstrate that:","1. Our proposed RL-based approach is able to learn suitable acoustic parameter values which improve the users' ability to correctly identify the state of the robot.","2. Algorithm initialization informed by previous user data can be used to significantly speed up the learning process.","3.","The method used for algorithm initialization strongly influences whether participants converge to similar sounds for each robot state.","4. Modulation of pitch bend has the largest influence on user association between sounds and robotic states."],"url":"http://arxiv.org/abs/2404.19253v1","category":"cs.RO"}
{"created":"2024-04-30 04:01:09","title":"HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning","abstract":"Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. \\href{https://github.com/Clin0212/HydraLoRA}{Code}.","sentences":["Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA.","However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets.","This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance.","Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA.","Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise.","Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases.","\\href{https://github.com/Clin0212/HydraLoRA}{Code}."],"url":"http://arxiv.org/abs/2404.19245v1","category":"cs.CL"}
{"created":"2024-04-30 04:00:15","title":"A University Framework for the Responsible use of Generative AI in Research","abstract":"Generative Artificial Intelligence (generative AI) poses both opportunities and risks for the integrity of research. Universities must guide researchers in using generative AI responsibly, and in navigating a complex regulatory landscape subject to rapid change. By drawing on the experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of generative AI. We provide guidance to help distil the diverse regulatory environment into a principles-based position statement. Further, we explain how a position statement can then serve as a foundation for initiatives in training, communications, infrastructure, and process change. Despite the growing body of literature about AI's impact on academic integrity for undergraduate students, there has been comparatively little attention on the impacts of generative AI for research integrity, and the vital role of institutions in helping to address those challenges. This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework for so doing.","sentences":["Generative Artificial Intelligence (generative AI) poses both opportunities and risks for the integrity of research.","Universities must guide researchers in using generative AI responsibly, and in navigating a complex regulatory landscape subject to rapid change.","By drawing on the experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of generative AI.","We provide guidance to help distil the diverse regulatory environment into a principles-based position statement.","Further, we explain how a position statement can then serve as a foundation for initiatives in training, communications, infrastructure, and process change.","Despite the growing body of literature about AI's impact on academic integrity for undergraduate students, there has been comparatively little attention on the impacts of generative AI for research integrity, and the vital role of institutions in helping to address those challenges.","This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework for so doing."],"url":"http://arxiv.org/abs/2404.19244v1","category":"cs.CY"}
{"created":"2024-04-30 03:31:03","title":"Multi-hop Question Answering over Knowledge Graphs using Large Language Models","abstract":"Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges. Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes. Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially. In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops. We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window. We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance.","sentences":["Knowledge graphs (KGs) are large datasets with specific structures representing large knowledge bases (KB) where each node represents a key entity and relations amongst them are typed edges.","Natural language queries formed to extract information from a KB entail starting from specific nodes and reasoning over multiple edges of the corresponding KG to arrive at the correct set of answer nodes.","Traditional approaches of question answering on KG are based on (a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL query, etc.) is generated using node and edge embeddings and then reasoning over these representations or tuning language models to generate the final answer directly, or (b) information-retrieval based that works by extracting entities and relations sequentially.","In this work, we evaluate the capability of (LLMs) to answer questions over KG that involve multiple hops.","We show that depending upon the size and nature of the KG we need different approaches to extract and feed the relevant information to an LLM since every LLM comes with a fixed context window.","We evaluate our approach on six KGs with and without the availability of example-specific sub-graphs and show that both the IR and SP-based methods can be adopted by LLMs resulting in an extremely competitive performance."],"url":"http://arxiv.org/abs/2404.19234v1","category":"cs.AI"}
{"created":"2024-04-30 03:29:30","title":"GRAMMAR: Grounded and Modular Evaluation of Domain-Specific Retrieval-Augmented Language Models","abstract":"Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base. However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure cases -- whether they stem from knowledge deficits or issues related to system robustness. To address these challenges, we introduce GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs. This method facilitates the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an evaluation framework that differentiates knowledge gaps from robustness and enables the identification of defective modules. Our empirical results underscore the limitations of current reference-free evaluation approaches and the reliability of GRAMMAR to accurately identify model vulnerabilities.","sentences":["Retrieval-augmented Generation (RAG) systems have been actively studied and deployed across various industries to query on domain-specific knowledge base.","However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure cases -- whether they stem from knowledge deficits or issues related to system robustness.","To address these challenges, we introduce GRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs.","This method facilitates the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an evaluation framework that differentiates knowledge gaps from robustness and enables the identification of defective modules.","Our empirical results underscore the limitations of current reference-free evaluation approaches and the reliability of GRAMMAR to accurately identify model vulnerabilities."],"url":"http://arxiv.org/abs/2404.19232v1","category":"cs.CL"}
{"created":"2024-04-30 17:59:40","title":"Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting","abstract":"3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models. Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry. These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation. These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt. In this work, we make two fundamental contributions to the field of 3D scene generation. First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene. We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene. Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene.","sentences":["3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models.","Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry.","These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation.","These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt.","In this work, we make two fundamental contributions to the field of 3D scene generation.","First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene.","We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene.","Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene."],"url":"http://arxiv.org/abs/2404.19758v1","category":"cs.CV"}
{"created":"2024-04-30 17:47:30","title":"Analyzing Transport Policies in Developing Countries with ABM","abstract":"Deciphering travel behavior and mode choices is a critical aspect of effective urban transportation system management, particularly in developing countries where unique socio-economic and cultural conditions complicate decision-making. Agent-based simulations offer a valuable tool for modeling transportation systems, enabling a nuanced understanding and policy impact evaluation. This work aims to shed light on the effects of transport policies and analyzes travel behavior by simulating agents making mode choices for their daily commutes. Agents gather information from the environment and their social network to assess the optimal transport option based on personal satisfaction criteria. Our findings, stemming from simulating a free-fare policy for public transit in a developing-country city, reveal a significant influence on decision-making, fostering public service use while positively influencing pollution levels, accident rates, and travel speed.","sentences":["Deciphering travel behavior and mode choices is a critical aspect of effective urban transportation system management, particularly in developing countries where unique socio-economic and cultural conditions complicate decision-making.","Agent-based simulations offer a valuable tool for modeling transportation systems, enabling a nuanced understanding and policy impact evaluation.","This work aims to shed light on the effects of transport policies and analyzes travel behavior by simulating agents making mode choices for their daily commutes.","Agents gather information from the environment and their social network to assess the optimal transport option based on personal satisfaction criteria.","Our findings, stemming from simulating a free-fare policy for public transit in a developing-country city, reveal a significant influence on decision-making, fostering public service use while positively influencing pollution levels, accident rates, and travel speed."],"url":"http://arxiv.org/abs/2404.19745v1","category":"cs.MA"}
{"created":"2024-04-30 17:38:15","title":"Almost Envy-Freeness under Weakly Lexicographic Preferences","abstract":"In fair division of indivisible items, domain restriction has played a key role in escaping from negative results and providing structural insights into the computational and axiomatic boundaries of fairness. One notable subdomain of additive preferences, the lexicographic domain, has yielded several positive results in dealing with goods, chores, and mixtures thereof. However, the majority of work within this domain primarily consider strict linear orders over items, which do not allow the modeling of more expressive preferences that contain indifferences (ties). We investigate the most prominent fairness notions of envy-freeness up to any (EFX) or some (EF1) item under weakly lexicographic preferences. For the goods-only setting, we develop an algorithm that can be customized to guarantee EF1, EFX, maximin share (MMS), or a combination thereof, along the efficiency notion of Pareto optimality (PO). From the conceptual perspective, we propose techniques such as preference graphs and potential envy that are independently of interest when dealing with ties. Finally, we demonstrate challenges in dealing with chores and highlight key algorithmic and axiomatic differences of finding EFX solutions with the goods-only setting. Nevertheless, we show that there is an algorithm that always returns an EF1 and PO allocation for the chores-only instances.","sentences":["In fair division of indivisible items, domain restriction has played a key role in escaping from negative results and providing structural insights into the computational and axiomatic boundaries of fairness.","One notable subdomain of additive preferences, the lexicographic domain, has yielded several positive results in dealing with goods, chores, and mixtures thereof.","However, the majority of work within this domain primarily consider strict linear orders over items, which do not allow the modeling of more expressive preferences that contain indifferences (ties).","We investigate the most prominent fairness notions of envy-freeness up to any (EFX) or some (EF1) item under weakly lexicographic preferences.","For the goods-only setting, we develop an algorithm that can be customized to guarantee EF1, EFX, maximin share (MMS), or a combination thereof, along the efficiency notion of Pareto optimality (PO).","From the conceptual perspective, we propose techniques such as preference graphs and potential envy that are independently of interest when dealing with ties.","Finally, we demonstrate challenges in dealing with chores and highlight key algorithmic and axiomatic differences of finding EFX solutions with the goods-only setting.","Nevertheless, we show that there is an algorithm that always returns an EF1 and PO allocation for the chores-only instances."],"url":"http://arxiv.org/abs/2404.19740v1","category":"cs.GT"}
{"created":"2024-04-30 17:37:21","title":"Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule Generation","abstract":"Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery. Diffusion models currently achieve state of the art performance for 3D molecule generation. In this work, we explore the use of flow matching, a recently proposed generative modeling framework that generalizes diffusion models, for the task of de novo molecule generation. Flow matching provides flexibility in model design; however, the framework is predicated on the assumption of continuously-valued data. 3D de novo molecule generation requires jointly sampling continuous and categorical variables such as atom position and atom type. We extend the flow matching framework to categorical data by constructing flows that are constrained to exist on a continuous representation of categorical data known as the probability simplex. We call this extension SimplexFlow. We explore the use of SimplexFlow for de novo molecule generation. However, we find that, in practice, a simpler approach that makes no accommodations for the categorical nature of the data yields equivalent or superior performance. As a result of these experiments, we present FlowMol, a flow matching model for 3D de novo generative model that achieves improved performance over prior flow matching methods, and we raise important questions about the design of prior distributions for achieving strong performance in flow matching models. Code and trained models for reproducing this work are available at https://github.com/dunni3/FlowMol","sentences":["Deep generative models that produce novel molecular structures have the potential to facilitate chemical discovery.","Diffusion models currently achieve state of the art performance for 3D molecule generation.","In this work, we explore the use of flow matching, a recently proposed generative modeling framework that generalizes diffusion models, for the task of de novo molecule generation.","Flow matching provides flexibility in model design; however, the framework is predicated on the assumption of continuously-valued data.","3D de novo molecule generation requires jointly sampling continuous and categorical variables such as atom position and atom type.","We extend the flow matching framework to categorical data by constructing flows that are constrained to exist on a continuous representation of categorical data known as the probability simplex.","We call this extension SimplexFlow.","We explore the use of SimplexFlow for de novo molecule generation.","However, we find that, in practice, a simpler approach that makes no accommodations for the categorical nature of the data yields equivalent or superior performance.","As a result of these experiments, we present FlowMol, a flow matching model for 3D de novo generative model that achieves improved performance over prior flow matching methods, and we raise important questions about the design of prior distributions for achieving strong performance in flow matching models.","Code and trained models for reproducing this work are available at https://github.com/dunni3/FlowMol"],"url":"http://arxiv.org/abs/2404.19739v1","category":"q-bio.BM"}
{"created":"2024-04-30 17:27:21","title":"An evaluation of the BALROG and RoboBA algorithms for determining the position of Fermi/GBM GRBs","abstract":"The Fermi/GBM instrument is a vital source of detections of gamma-ray bursts and has an increasingly important role to play in understanding gravitational-wave transients. In both cases, its impact is increased by accurate positions with reliable uncertainties. We evaluate the RoboBA and BALROG algorithms for determining the position of gamma-ray bursts detected by the Fermi/GBM instrument. We construct a sample of 54 bursts with detections both by Swift/BAT and by Fermi/GBM. We then compare the positions predicted by RoboBA and BALROG with the positions measured by BAT, which we can assume to be the true position. We find that RoboBA and BALROG are similarly precise for bright bursts whose uncertainties are dominated by systematic errors, but RoboBA performs better for faint bursts whose uncertainties are dominated by statistical noise. We further find that the uncertainties in the positions predicted by RoboBA are consistent with the distribution of position errors, whereas BALROG seems to be underestimating the uncertainties by a factor of about two. Additionally, we consider the implications of these results for the follow-up of the optical afterglows of Fermi/GBM bursts. In particular, for the DDOTI wide-field imager we conclude that a single pointing is best. Our sample would allow a similar study to be carried out for other telescopes.","sentences":["The Fermi/GBM instrument is a vital source of detections of gamma-ray bursts and has an increasingly important role to play in understanding gravitational-wave transients.","In both cases, its impact is increased by accurate positions with reliable uncertainties.","We evaluate the RoboBA and BALROG algorithms for determining the position of gamma-ray bursts detected by the Fermi/GBM instrument.","We construct a sample of 54 bursts with detections both by Swift/BAT and by Fermi/GBM.","We then compare the positions predicted by RoboBA and BALROG with the positions measured by BAT, which we can assume to be the true position.","We find that RoboBA and BALROG are similarly precise for bright bursts whose uncertainties are dominated by systematic errors, but RoboBA performs better for faint bursts whose uncertainties are dominated by statistical noise.","We further find that the uncertainties in the positions predicted by RoboBA are consistent with the distribution of position errors, whereas BALROG seems to be underestimating the uncertainties by a factor of about two.","Additionally, we consider the implications of these results for the follow-up of the optical afterglows of Fermi/GBM bursts.","In particular, for the DDOTI wide-field imager we conclude that a single pointing is best.","Our sample would allow a similar study to be carried out for other telescopes."],"url":"http://arxiv.org/abs/2404.19732v1","category":"astro-ph.HE"}
{"created":"2024-04-30 17:22:28","title":"On the inefficiency of fermion level-crossing under the parity-violating spin-2 gravitational field","abstract":"Gravitational chiral anomaly connects the topological charge of spacetime and the chirality of fermions. It has been known that the chirality is carried by the particles (or the excited states) and also by vacuum. While the gravitational anomaly equation has been applied to cosmology, distinction between these two contributions has been rarely discussed. In the study of gravitational leptogenesis, for example, lepton asymmetry associated with the chiral gravitational waves sourced during inflation is evaluated only by integrating the anomaly equation. How these two contributions are distributed has not been seriously investigated. Meanwhile, a dominance of vacuum contribution is observed in some specific types of Bianchi spacetime with parity-violating gravitational fields, whose application to cosmology is not straightforward. One may wonder whether such a vacuum dominance takes place also in the system with chiral gravitational waves around the flat background, which is more suitable for application to realistic cosmology. In this work, we apply an analogy between U(1) electromagnetism and the weak gravity to the spacetime that resembles the one considered in the gravitational leptogenesis scenario. This approach allows us to obtain intuitive understanding of the fermion chirality generation under the parity-violating spin-2 gravitational field. By assuming the emergence of Landau level-like dispersion relation in our setup, we conjecture that level-crossing does not seem to be efficient while the charge accumulation in the vacuum likely takes place. Phenomenological implication is also discussed in the context of gravitational leptogenesis.","sentences":["Gravitational chiral anomaly connects the topological charge of spacetime and the chirality of fermions.","It has been known that the chirality is carried by the particles (or the excited states) and also by vacuum.","While the gravitational anomaly equation has been applied to cosmology, distinction between these two contributions has been rarely discussed.","In the study of gravitational leptogenesis, for example, lepton asymmetry associated with the chiral gravitational waves sourced during inflation is evaluated only by integrating the anomaly equation.","How these two contributions are distributed has not been seriously investigated.","Meanwhile, a dominance of vacuum contribution is observed in some specific types of Bianchi spacetime with parity-violating gravitational fields, whose application to cosmology is not straightforward.","One may wonder whether such a vacuum dominance takes place also in the system with chiral gravitational waves around the flat background, which is more suitable for application to realistic cosmology.","In this work, we apply an analogy between U(1) electromagnetism and the weak gravity to the spacetime that resembles the one considered in the gravitational leptogenesis scenario.","This approach allows us to obtain intuitive understanding of the fermion chirality generation under the parity-violating spin-2 gravitational field.","By assuming the emergence of Landau level-like dispersion relation in our setup, we conjecture that level-crossing does not seem to be efficient while the charge accumulation in the vacuum likely takes place.","Phenomenological implication is also discussed in the context of gravitational leptogenesis."],"url":"http://arxiv.org/abs/2404.19726v1","category":"hep-ph"}
{"created":"2024-04-30 17:11:39","title":"Efficient Multiparty Quantum Key Distribution over Quantum Networks","abstract":"Multiparty quantum key distribution (QKD) is useful for many applications that involve secure communication or collaboration among multiple parties. While it can be achieved using pairwise QKD, a more efficient approach is to achieve it using multipartite entanglement distributed over quantum networks that connect the multiple parties. Existing studies on multipartite entanglement distribution, however, are not designed for multiparty QKD, and hence do not aim to maximize secret key generation rate. In this paper, we design efficient strategies for multiparty QKD over quantum networks. For 3-party QKD, we derive closed-form expressions for analyzing key distribution over quantum networks. We then use it to develop an efficient strategy for 3-party QKD by packing multiple stars that connect the 3 parties. For the general form of N-party QKD, we develop an approach that packs multiple trees to connect the N parties, while directly incorporating the estimated key rates on network paths. Extensive evaluation of our strategies, in both grid and random graphs, under a wide range of settings, demonstrates that our schemes achieve high key rate, which degrades gracefully when increasing the number of parties.","sentences":["Multiparty quantum key distribution (QKD) is useful for many applications that involve secure communication or collaboration among multiple parties.","While it can be achieved using pairwise QKD, a more efficient approach is to achieve it using multipartite entanglement distributed over quantum networks that connect the multiple parties.","Existing studies on multipartite entanglement distribution, however, are not designed for multiparty QKD, and hence do not aim to maximize secret key generation rate.","In this paper, we design efficient strategies for multiparty QKD over quantum networks.","For 3-party QKD, we derive closed-form expressions for analyzing key distribution over quantum networks.","We then use it to develop an efficient strategy for 3-party QKD by packing multiple stars that connect the 3 parties.","For the general form of N-party QKD, we develop an approach that packs multiple trees to connect the N parties, while directly incorporating the estimated key rates on network paths.","Extensive evaluation of our strategies, in both grid and random graphs, under a wide range of settings, demonstrates that our schemes achieve high key rate, which degrades gracefully when increasing the number of parties."],"url":"http://arxiv.org/abs/2404.19720v1","category":"quant-ph"}
{"created":"2024-04-30 17:10:25","title":"Automated, Reliable, and Efficient Continental-Scale Replication of 7.3 Petabytes of Climate Simulation Data: A Case Study","abstract":"We report on our experiences replicating 7.3 petabytes (PB) of Earth System Grid Federation (ESGF) climate simulation data from Lawrence Livermore National Laboratory (LLNL) in California to Argonne National Laboratory (ANL) in Illinois and Oak Ridge National Laboratory (ORNL) in Tennessee. This movement of some 29 million files, twice, undertaken in order to establish new ESGF nodes at ANL and ORNL, was performed largely automatically by a simple replication tool, a script that invoked Globus to transfer large bundles of files while tracking progress in a database. Under the covers, Globus organized transfers to make efficient use of the high-speed Energy Sciences network (ESnet) and the data transfer nodes deployed at participating sites, and also addressed security, integrity checking, and recovery from a variety of transient failures. This success demonstrates the considerable benefits that can accrue from the adoption of performant data replication infrastructure.","sentences":["We report on our experiences replicating 7.3 petabytes (PB) of Earth System Grid Federation (ESGF) climate simulation data from Lawrence Livermore National Laboratory (LLNL) in California to Argonne National Laboratory (ANL) in Illinois and Oak Ridge National Laboratory (ORNL) in Tennessee.","This movement of some 29 million files, twice, undertaken in order to establish new ESGF nodes at ANL and ORNL, was performed largely automatically by a simple replication tool, a script that invoked Globus to transfer large bundles of files while tracking progress in a database.","Under the covers, Globus organized transfers to make efficient use of the high-speed Energy Sciences network (ESnet) and the data transfer nodes deployed at participating sites, and also addressed security, integrity checking, and recovery from a variety of transient failures.","This success demonstrates the considerable benefits that can accrue from the adoption of performant data replication infrastructure."],"url":"http://arxiv.org/abs/2404.19717v1","category":"cs.DC"}
{"created":"2024-04-30 17:06:42","title":"Controlled Spalling of Single Crystal 4H-SiC Bulk Substrates","abstract":"We detail several scientific and engineering innovations which enable the controlled spalling of 10 - 50 micron thick films of single crystal 4H silicon carbide (4H-SiC) from bulk substrates. 4H-SiC's properties, including high thermal conductivity and a wide bandgap, make it an ideal candidate for high-temperature, high-voltage power electronic devices. Moreover, 4H-SiC has been shown to be an excellent host of solid-state atomic defect qubits for quantum computing and quantum networking. Because 4H-SiC single crystal substrates are expensive (due to long growth times and limited yield), techniques for removal and transfer of layers in the tens-of-microns thickness range are highly desirable for substrate reuse and heterogenous integration of separated layers. In this work we utilize novel approaches for stressor layer thickness control and spalling crack initiation to demonstrate controlled spalling of 4H-SiC, the highest fracture toughness material spalled to date. Additionally, we demonstrate substrate re-use, bonding of the spalled films to carrier substrates, and explore the spin coherence of the spalled films. In preliminary studies we are able to achieve coherent spin control of neutral divacancy ($VV^{0}$) qubit ensembles and measure a spin T2* of 0.581 $\\mu$s in such spalled films.","sentences":["We detail several scientific and engineering innovations which enable the controlled spalling of 10 - 50 micron thick films of single crystal 4H silicon carbide (4H-SiC) from bulk substrates.","4H-SiC's properties, including high thermal conductivity and a wide bandgap, make it an ideal candidate for high-temperature, high-voltage power electronic devices.","Moreover, 4H-SiC has been shown to be an excellent host of solid-state atomic defect qubits for quantum computing and quantum networking.","Because 4H-SiC single crystal substrates are expensive (due to long growth times and limited yield), techniques for removal and transfer of layers in the tens-of-microns thickness range are highly desirable for substrate reuse and heterogenous integration of separated layers.","In this work we utilize novel approaches for stressor layer thickness control and spalling crack initiation to demonstrate controlled spalling of 4H-SiC, the highest fracture toughness material spalled to date.","Additionally, we demonstrate substrate re-use, bonding of the spalled films to carrier substrates, and explore the spin coherence of the spalled films.","In preliminary studies we are able to achieve coherent spin control of neutral divacancy ($VV^{0}$) qubit ensembles and measure a spin T2* of 0.581 $\\mu$s in such spalled films."],"url":"http://arxiv.org/abs/2404.19716v1","category":"physics.app-ph"}
{"created":"2024-04-30 17:01:20","title":"A rank decomposition for the topological classification of neural representations","abstract":"Neural networks can be thought of as applying a transformation to an input dataset. The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems. In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset. Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces.   As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight. We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change. As the width increases, the homology groups of the input manifold become more likely to be preserved. We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.   Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on.","sentences":["Neural networks can be thought of as applying a transformation to an input dataset.","The way in which they change the topology of such a dataset often holds practical significance for many tasks, particularly those demanding non-homeomorphic mappings for optimal solutions, such as classification problems.","In this work, we leverage the fact that neural networks are equivalent to continuous piecewise-affine maps, whose rank can be used to pinpoint regions in the input space that undergo non-homeomorphic transformations, leading to alterations in the topological structure of the input dataset.","Our approach enables us to make use of the relative homology sequence, with which one can study the homology groups of the quotient of a manifold $\\mathcal{M}$ and a subset $A$, assuming some minimal properties on these spaces.   ","As a proof of principle, we empirically investigate the presence of low-rank (topology-changing) affine maps as a function of network width and mean weight.","We show that in randomly initialized narrow networks, there will be regions in which the (co)homology groups of a data manifold can change.","As the width increases, the homology groups of the input manifold become more likely to be preserved.","We end this part of our work by constructing highly non-random wide networks that do not have this property and relating this non-random regime to Dale's principle, which is a defining characteristic of biological neural networks.   ","Finally, we study simple feedforward networks trained on MNIST, as well as on toy classification and regression tasks, and show that networks manipulate the topology of data differently depending on the continuity of the task they are trained on."],"url":"http://arxiv.org/abs/2404.19710v1","category":"cs.LG"}
{"created":"2024-04-30 16:59:38","title":"Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models","abstract":"Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty. The introduced methods are implemented to the accompanying R package sstvars.","sentences":["Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian.","We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes.","Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy.","In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty.","The introduced methods are implemented to the accompanying R package sstvars."],"url":"http://arxiv.org/abs/2404.19707v1","category":"econ.EM"}
{"created":"2024-04-30 16:52:55","title":"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively","abstract":"In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question. Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, <RET>, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that Adapt-LLM is able to generate the <RET> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.","sentences":["In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question.","Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself.","Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage.","Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets.","Here, LLMs are trained to generate a special token, <RET>, when they do not know the answer to a question.","Our evaluation of the Adaptive Retrieval LLM (Adapt-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever.","Through our analysis, we demonstrate that Adapt-LLM is able to generate the <RET> token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory."],"url":"http://arxiv.org/abs/2404.19705v1","category":"cs.CL"}
{"created":"2024-04-30 16:46:20","title":"Comparing Multivariate Distributions: A Novel Approach Using Optimal Transport-based Plots","abstract":"Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets. Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data. In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data. Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets. Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets. Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour. We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously.","sentences":["Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets.","Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data.","In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data.","Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets.","Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets.","Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour.","We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously."],"url":"http://arxiv.org/abs/2404.19700v1","category":"stat.ME"}
{"created":"2024-04-30 16:44:56","title":"Structural properties of Krylov subspaces and applications to unbounded self-adjoint operators","abstract":"This paper presents a study of the inherent structural properties of Krylov subspaces, in particular for the self-adjoint class of operators, and how they relate with the important phenomenon of `Krylov solvability' of linear inverse problems. Owing to the complexity of the problem in the unbounded setting, recently developed perturbative techniques are used that exploit the use of the weak topology on $\\mathcal{H}$. We also make a strong connection between the approximative properties of the Krylov subspace and the famous Hamburger problem of moments, in particular the determinacy condition.","sentences":["This paper presents a study of the inherent structural properties of Krylov subspaces, in particular for the self-adjoint class of operators, and how they relate with the important phenomenon of `Krylov solvability' of linear inverse problems.","Owing to the complexity of the problem in the unbounded setting, recently developed perturbative techniques are used that exploit the use of the weak topology on $\\mathcal{H}$. We also make a strong connection between the approximative properties of the Krylov subspace and the famous Hamburger problem of moments, in particular the determinacy condition."],"url":"http://arxiv.org/abs/2404.19698v1","category":"math.FA"}
{"created":"2024-04-30 16:21:47","title":"Exponential localization for eigensections of the Bochner-Schr\u00f6dinger operator","abstract":"We study asymptotic spectral properties of the Bochner-Schr\\\"odinger operator $H_{p}=\\frac 1p\\Delta^{L^p\\otimes E}+V$ on high tensor powers of a Hermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a Riemannian manifold $X$ of bounded geometry under assumption that the curvature form of $L$ is non-degenerate. At an arbitrary point $x_0$ of $X$ the operator $H_p$ can be approximated by a model operator $\\mathcal H^{(x_0)}$, which is a Schr\\\"odinger operator with constant magnetic field. For large $p$, the spectrum of $H_p$ asymptotically coincides, up to order $p^{-1/4}$, with the union of the spectra of the model operators $\\mathcal H^{(x_0)}$ over $X$. We show that, if the union of the spectra of $\\mathcal H^{(x_0)}$ over the complement of a compact subset of $X$ has a gap, then the spectrum of $H_{p}$ in the gap is discrete and the corresponding eigensections decay exponentially away the compact subset.","sentences":["We study asymptotic spectral properties of the Bochner-Schr\\\"odinger operator $H_{p}=\\frac 1p\\Delta^{L^p\\otimes E}+V$ on high tensor powers of a Hermitian line bundle $L$ twisted by a Hermitian vector bundle $E$ on a Riemannian manifold $X$ of bounded geometry under assumption that the curvature form of $L$ is non-degenerate.","At an arbitrary point $x_0$ of $X$ the operator $H_p$ can be approximated by a model operator $\\mathcal H^{(x_0)}$, which is a Schr\\\"odinger operator with constant magnetic field.","For large $p$, the spectrum of $H_p$ asymptotically coincides, up to order $p^{-1/4}$, with the union of the spectra of the model operators $\\mathcal H^{(x_0)}$ over $X$. We show that, if the union of the spectra of $\\mathcal H^{(x_0)}$ over the complement of a compact subset of $X$ has a gap, then the spectrum of $H_{p}$ in the gap is discrete and the corresponding eigensections decay exponentially away the compact subset."],"url":"http://arxiv.org/abs/2404.19684v1","category":"math.SP"}
{"created":"2024-04-30 16:11:39","title":"Density-wave-like gap evolution in La$_3$Ni$_2$O$_7$ under high pressure revealed by ultrafast optical spectroscopy","abstract":"We explore the quasiparticle dynamics in bilayer nickelate La$_3$Ni$_2$O$_7$ crystal using ultrafast optical pump-probe spectroscopy at high pressure up to 34.2 GPa. At ambient pressure, the temperature dependence of relaxation indicates appearance of phonon bottleneck effect due to the opening of density-wave-like gap at 151 K. By analyzing the data with RT model, we identified the energy scale of the gap to be 70 meV at ambient pressure. The relaxation bottleneck effect is suppressed gradually by the pressure and disappears around 26 GPa. At high pressure above 29.4 GPa, we discover a new density-wave like order with transition temperature of $\\sim$130 K. Our results not only provide the first experimental evidence of the density-wave like gap evolution under high pressure, but also offering insight into the underline interplay between the density wave order and superconductivity in pressured La$_3$Ni$_2$O$_7$.","sentences":["We explore the quasiparticle dynamics in bilayer nickelate La$_3$Ni$_2$O$_7$ crystal using ultrafast optical pump-probe spectroscopy at high pressure up to 34.2 GPa.","At ambient pressure, the temperature dependence of relaxation indicates appearance of phonon bottleneck effect due to the opening of density-wave-like gap at 151 K. By analyzing the data with RT model, we identified the energy scale of the gap to be 70 meV at ambient pressure.","The relaxation bottleneck effect is suppressed gradually by the pressure and disappears around 26 GPa.","At high pressure above 29.4 GPa, we discover a new density-wave like order with transition temperature of $\\sim$130 K. Our results not only provide the first experimental evidence of the density-wave like gap evolution under high pressure, but also offering insight into the underline interplay between the density wave order and superconductivity in pressured La$_3$Ni$_2$O$_7$."],"url":"http://arxiv.org/abs/2404.19678v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 16:05:59","title":"Specific Wasserstein divergence between continuous martingales","abstract":"Defining a divergence between the laws of continuous martingales is a delicate task, owing to the fact that these laws tend to be singular to each other. An important idea, put forward by N. Gantert, is to instead consider a scaling limit of the relative entropy between such continuous martingales sampled over a finite time grid. This gives rise to the concept of specific relative entropy. In order to develop a general theory of divergences between continuous martingales, it is only natural to replace the role of the relative entropy in this construction by a different notion of discrepancy between finite dimensional probability distributions. In the present work we take a first step in this direction, taking a power $p$ of the Wasserstein distance instead of the relative entropy. We call the newly obtained scaling limit the specific $p$-Wasserstein divergence.   In our first main result we prove that the specific $p$-Wasserstein divergence is well-defined, and exhibit an explicit expression for it in terms of the quadratic variations of the martingales involved. This is obtained under vastly weaker assumptions than the corresponding results for the specific relative entropy. Next we illustrate the usefulness of the concept, by considering the problem of optimizing the specific $p$-Wasserstein divergence over the set of win-martingales. In our second main result we characterize the solution of this optimization problem for all $p>0$ and, somewhat surprisingly, we single out the case $p=1/2$ as the one with the best probabilistic properties. For instance, the optimal martingale in this case is very explicit and can be connected, through a space transformation, to the solution of a variant of the Schr\\\"odinger problem.","sentences":["Defining a divergence between the laws of continuous martingales is a delicate task, owing to the fact that these laws tend to be singular to each other.","An important idea, put forward by N. Gantert, is to instead consider a scaling limit of the relative entropy between such continuous martingales sampled over a finite time grid.","This gives rise to the concept of specific relative entropy.","In order to develop a general theory of divergences between continuous martingales, it is only natural to replace the role of the relative entropy in this construction by a different notion of discrepancy between finite dimensional probability distributions.","In the present work we take a first step in this direction, taking a power $p$ of the Wasserstein distance instead of the relative entropy.","We call the newly obtained scaling limit the specific $p$-Wasserstein divergence.   ","In our first main result we prove that the specific $p$-Wasserstein divergence is well-defined, and exhibit an explicit expression for it in terms of the quadratic variations of the martingales involved.","This is obtained under vastly weaker assumptions than the corresponding results for the specific relative entropy.","Next we illustrate the usefulness of the concept, by considering the problem of optimizing the specific $p$-Wasserstein divergence over the set of win-martingales.","In our second main result we characterize the solution of this optimization problem for all $p>0$ and, somewhat surprisingly, we single out the case $p=1/2$ as the one with the best probabilistic properties.","For instance, the optimal martingale in this case is very explicit and can be connected, through a space transformation, to the solution of a variant of the Schr\\\"odinger problem."],"url":"http://arxiv.org/abs/2404.19672v1","category":"math.PR"}
{"created":"2024-04-30 15:57:41","title":"Constrained maximization of conformal capacity","abstract":"We consider constellations of disks which are unions of disjoint hyperbolic disks in the unit disk with fixed radii and unfixed centers. We study the problem of maximizing the conformal capacity of a constellation under constraints on the centers in two cases. In the first case the constraint is that the centers are at most at distance $R \\in(0,1)$ from the origin and in the second case it is required that the centers are on the subsegment $[-R,R]$ of a diameter of the unit disk. We study also similar types of constellations with hyperbolic segments instead of the hyperbolic disks. Our computational experiments suggest that a dispersion phenomenon occurs: the disks/segments go as close to the unit circle as possible under these constraints and stay as far as possible from each other. The computation of capacity reduces to the Dirichlet problem for the Laplace equation which we solve with a fast boundary integral equation method. The results are double-checked with the $hp$-FEM method.","sentences":["We consider constellations of disks which are unions of disjoint hyperbolic disks in the unit disk with fixed radii and unfixed centers.","We study the problem of maximizing the conformal capacity of a constellation under constraints on the centers in two cases.","In the first case the constraint is that the centers are at most at distance $R \\in(0,1)$ from the origin and in the second case it is required that the centers are on the subsegment $[-R,R]$ of a diameter of the unit disk.","We study also similar types of constellations with hyperbolic segments instead of the hyperbolic disks.","Our computational experiments suggest that a dispersion phenomenon occurs: the disks/segments go as close to the unit circle as possible under these constraints and stay as far as possible from each other.","The computation of capacity reduces to the Dirichlet problem for the Laplace equation which we solve with a fast boundary integral equation method.","The results are double-checked with the $hp$-FEM method."],"url":"http://arxiv.org/abs/2404.19663v1","category":"math.CV"}
{"created":"2024-04-30 15:47:29","title":"Exploring the hierarchy of quantum correlations under thermal effects in two gravitational cat states","abstract":"In this article, we investigate the hierarchy of quantum correlations between two gravitational cats states (modeled by two qubits). We use concurrence to quantify the entanglement between the two gravitational cat states. Quantum steering is employed to measure the steerabilities. We consider geometric quantum discord to quantify quantum correlations beyond entanglement. We show that the concurrence persists even when steerability is lost under thermal effects. We also show that the temperature influences the degree of quantum correlations between the two gravitational cat states. Besides, when the energy difference between the ground state and the first excited level becomes significant, the states become separable.","sentences":["In this article, we investigate the hierarchy of quantum correlations between two gravitational cats states (modeled by two qubits).","We use concurrence to quantify the entanglement between the two gravitational cat states.","Quantum steering is employed to measure the steerabilities.","We consider geometric quantum discord to quantify quantum correlations beyond entanglement.","We show that the concurrence persists even when steerability is lost under thermal effects.","We also show that the temperature influences the degree of quantum correlations between the two gravitational cat states.","Besides, when the energy difference between the ground state and the first excited level becomes significant, the states become separable."],"url":"http://arxiv.org/abs/2404.19648v1","category":"quant-ph"}
{"created":"2024-04-30 15:24:53","title":"Machine learning of continuous and discrete variational ODEs with convergence guarantee and uncertainty quantification","abstract":"The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data. The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design. A rigorous proof of convergence as the distance between observation data points converges to zero is provided. Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques. We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification. The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces.","sentences":["The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data.","The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design.","A rigorous proof of convergence as the distance between observation data points converges to zero is provided.","Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques.","We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification.","The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces."],"url":"http://arxiv.org/abs/2404.19626v1","category":"math.NA"}
{"created":"2024-04-30 15:23:36","title":"Dual-Port Grid-Forming Interconnecting Power Converters in Hybrid AC/DC Grids","abstract":"Interconnecting power converters (IPC) are the main elements enabling the interconnection of multiple high-voltage alternating current (HVAC) and high-voltage direct current (HVDC) subgrids. These converters can be classified either as grid-forming or grid-following. These roles can be assigned to both ac and dc terminals. This work compares state-of-the-art single-port grid-forming and grid-following control schemes with a dual-port grid-forming control scheme, which can simultaneously form a stable voltage on the ac and the dc sides. The dual-port grid-forming small-signal stability and dynamic behavior under fluctuations in the power flow are studied and compared against state-of-the-art control architectures. Moreover, the dual-port control scheme is validated and tested on a down-scaled laboratory platform with several transient events.","sentences":["Interconnecting power converters (IPC) are the main elements enabling the interconnection of multiple high-voltage alternating current (HVAC) and high-voltage direct current (HVDC) subgrids.","These converters can be classified either as grid-forming or grid-following.","These roles can be assigned to both ac and dc terminals.","This work compares state-of-the-art single-port grid-forming and grid-following control schemes with a dual-port grid-forming control scheme, which can simultaneously form a stable voltage on the ac and the dc sides.","The dual-port grid-forming small-signal stability and dynamic behavior under fluctuations in the power flow are studied and compared against state-of-the-art control architectures.","Moreover, the dual-port control scheme is validated and tested on a down-scaled laboratory platform with several transient events."],"url":"http://arxiv.org/abs/2404.19625v1","category":"eess.SY"}
{"created":"2024-04-30 15:23:16","title":"Level-$k$ Reasoning, Cognitive Hierarchy, and Rationalizability","abstract":"We use a uniform framework to give Camerer, Ho, and Chong's (2004) cognitive hierarchy (CH) solution and its dynamic extension a decision-theoretical foundation by the epistemic game theoretical solution concept $\\Delta$-rationalizability (Battigalli and Siniscalchi, 2003). We interpret level-$k$ strategic sophistication as an information type and define restriction $\\Delta^\\kappa$ on information types; based on it, we show that in the behavioral consequence of rationality, common belief in rationality and transparency, called $\\Delta^\\kappa$-rationalizability, the levels of reasoning is endogenously determined. We show that in static games, CH solution generically coincides with $\\Delta^\\kappa$-rationalizability; based on this, we connect CH with Bayesian equilibrium. By adapting $\\Delta^\\kappa$ into dynamic games, we show that Lin and Palfrey's (2024) DCH solution generically coincides with the behavioral consequence of rationality, common strong belief in rationality, and transparency of (dynamic) $\\Delta^\\kappa$. The same framework could also be used to analyze many variations of CH in the literature.","sentences":["We use a uniform framework to give Camerer, Ho, and Chong's (2004) cognitive hierarchy (CH) solution and its dynamic extension a decision-theoretical foundation by the epistemic game theoretical solution concept $\\Delta$-rationalizability (Battigalli and Siniscalchi, 2003).","We interpret level-$k$ strategic sophistication as an information type and define restriction $\\Delta^\\kappa$ on information types; based on it, we show that in the behavioral consequence of rationality, common belief in rationality and transparency, called $\\Delta^\\kappa$-rationalizability, the levels of reasoning is endogenously determined.","We show that in static games, CH solution generically coincides with $\\Delta^\\kappa$-rationalizability; based on this, we connect CH with Bayesian equilibrium.","By adapting $\\Delta^\\kappa$ into dynamic games, we show that Lin and Palfrey's (2024) DCH solution generically coincides with the behavioral consequence of rationality, common strong belief in rationality, and transparency of (dynamic) $\\Delta^\\kappa$. The same framework could also be used to analyze many variations of CH in the literature."],"url":"http://arxiv.org/abs/2404.19623v2","category":"econ.TH"}
{"created":"2024-04-30 15:22:19","title":"Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis","abstract":"Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a new and emerging field. These technologies hold great promise for more human-like, efficient, expressive, and robust synthetic communication, but are currently held back by the lack of suitably large datasets, as existing methods are trained on parallel data from all constituent modalities. Inspired by student-teacher methods, we propose a straightforward solution to the data shortage, by simply synthesising additional training material. Specifically, we use unimodal synthesis models trained on large datasets to create multimodal (but synthetic) parallel training data, and then pre-train a joint synthesis model on that material. In addition, we propose a new synthesis architecture that adds better and more controllable prosody modelling to the state-of-the-art method in the field. Our results confirm that pre-training on large amounts of synthetic data improves the quality of both the speech and the motion synthesised by the multimodal model, with the proposed architecture yielding further benefits when pre-trained on the synthetic data. See https://shivammehta25.github.io/MAGI/ for example output.","sentences":["Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a new and emerging field.","These technologies hold great promise for more human-like, efficient, expressive, and robust synthetic communication, but are currently held back by the lack of suitably large datasets, as existing methods are trained on parallel data from all constituent modalities.","Inspired by student-teacher methods, we propose a straightforward solution to the data shortage, by simply synthesising additional training material.","Specifically, we use unimodal synthesis models trained on large datasets to create multimodal (but synthetic) parallel training data, and then pre-train a joint synthesis model on that material.","In addition, we propose a new synthesis architecture that adds better and more controllable prosody modelling to the state-of-the-art method in the field.","Our results confirm that pre-training on large amounts of synthetic data improves the quality of both the speech and the motion synthesised by the multimodal model, with the proposed architecture yielding further benefits when pre-trained on the synthetic data.","See https://shivammehta25.github.io/MAGI/ for example output."],"url":"http://arxiv.org/abs/2404.19622v1","category":"cs.HC"}
{"created":"2024-04-30 15:20:41","title":"Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference","abstract":"Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We further develop two new estimators for estimating the proposed ideal loss. We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effect are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.","sentences":["Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection.","Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect.","To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect.","On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect.","We further develop two new estimators for estimating the proposed ideal loss.","We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effect are present, while the existing methods are biased.","Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.19620v1","category":"cs.LG"}
{"created":"2024-04-30 15:18:46","title":"Novel Round Trip Time Estimation in 5G NR","abstract":"The fifth generation new radio (5G NR) technology is expected to fulfill reliable and accurate positioning requirements of industry use cases, such as autonomous robots, connected vehicles, and future factories. Starting from Third Generation Partnership Project (3GPP) Release-16, several enhanced positioning solutions are featured in the 5G standards, including the multi-cell round trip time (multi-RTT) method. This work presents a novel framework to estimate the round-trip time (RTT) between a user equipment (UE) and a base station (gNB) in 5G NR. Unlike the existing scheme in the standards, RTT can be estimated without the need to send timing measurements from both the gNB and UE to a central node. The proposed method relies on obtaining multiple coherent uplink wide-band channel measurements at the gNB by circumventing the timing advance control loops and the clock drift. The performance is evaluated through experiments leveraging a real world 5G testbed based on OpenAirInterface (OAI). Under a moderate system bandwidth of 40MHz, the experimental results show meter level range accuracy even in low signal-to-noise ratio (SNR) conditions.","sentences":["The fifth generation new radio (5G NR) technology is expected to fulfill reliable and accurate positioning requirements of industry use cases, such as autonomous robots, connected vehicles, and future factories.","Starting from Third Generation Partnership Project (3GPP) Release-16, several enhanced positioning solutions are featured in the 5G standards, including the multi-cell round trip time (multi-RTT) method.","This work presents a novel framework to estimate the round-trip time (RTT) between a user equipment (UE) and a base station (gNB) in 5G NR.","Unlike the existing scheme in the standards, RTT can be estimated without the need to send timing measurements from both the gNB and UE to a central node.","The proposed method relies on obtaining multiple coherent uplink wide-band channel measurements at the gNB by circumventing the timing advance control loops and the clock drift.","The performance is evaluated through experiments leveraging a real world 5G testbed based on OpenAirInterface (OAI).","Under a moderate system bandwidth of 40MHz, the experimental results show meter level range accuracy even in low signal-to-noise ratio (SNR) conditions."],"url":"http://arxiv.org/abs/2404.19618v1","category":"cs.IT"}
{"created":"2024-04-30 15:11:34","title":"Quantum Cloud Computing: Trends and Challenges","abstract":"Quantum computing (QC) is a new paradigm that will revolutionize various areas of computing, especially cloud computing. QC, still in its infancy, is a costly technology capable of operating in highly isolated environments due to its rapid response to environmental factors. For this reason, it is still a challenging technology for researchers to reach. Integrating QC into an isolated remote server, like a cloud, and making it available to users can overcome these problems. Furthermore, experts predict that QC, with its ability to swiftly resolve complex and computationally intensive operations, will offer significant benefits in systems that process large amounts of data, like cloud computing. This article presents the vision and challenges for the quantum cloud computing (QCC) paradigm that will emerge with the integration of quantum and cloud computing. Next, we present the advantages of QC over classical computing applications. We analyze the effects of QC on cloud systems, such as cost, security, and scalability. Besides all of these advantages, we highlight research gaps in QCC, such as qubit stability and efficient resource allocation. This article identifies QCC's advantages and challenges for future research, highlighting research gaps.","sentences":["Quantum computing (QC) is a new paradigm that will revolutionize various areas of computing, especially cloud computing.","QC, still in its infancy, is a costly technology capable of operating in highly isolated environments due to its rapid response to environmental factors.","For this reason, it is still a challenging technology for researchers to reach.","Integrating QC into an isolated remote server, like a cloud, and making it available to users can overcome these problems.","Furthermore, experts predict that QC, with its ability to swiftly resolve complex and computationally intensive operations, will offer significant benefits in systems that process large amounts of data, like cloud computing.","This article presents the vision and challenges for the quantum cloud computing (QCC) paradigm that will emerge with the integration of quantum and cloud computing.","Next, we present the advantages of QC over classical computing applications.","We analyze the effects of QC on cloud systems, such as cost, security, and scalability.","Besides all of these advantages, we highlight research gaps in QCC, such as qubit stability and efficient resource allocation.","This article identifies QCC's advantages and challenges for future research, highlighting research gaps."],"url":"http://arxiv.org/abs/2404.19612v1","category":"cs.DC"}
{"created":"2024-04-30 15:00:58","title":"Three-dimensional Moir\u00e9 Crystal","abstract":"The work intends to extend the moir\\'e physics to three dimensions. Three-dimensional moir\\'e patterns can be realized in ultracold atomic gases by coupling two spin states in spin-dependent optical lattices with a relative twist, a structure currently unachievable in solid-state materials. We give the commensurate conditions under which the three-dimensional moir\\'e pattern features a periodic structure, termed a three-dimensional moir\\'e crystal. We emphasize a key distinction of three-dimensional moir\\'e physics: in three dimensions, the twist operation generically does not commute with the rotational symmetry of the original lattice, unlike in two dimensions, where these two always commute. Consequently, the moir\\'e crystal can exhibit a crystalline structure that differs from the original underlying lattice. We demonstrate that twisting a simple cubic lattice can generate various crystal structures. This capability of altering crystal structures by twisting offers a broad range of tunability for three-dimensional band structures.","sentences":["The work intends to extend the moir\\'e physics to three dimensions.","Three-dimensional moir\\'e patterns can be realized in ultracold atomic gases by coupling two spin states in spin-dependent optical lattices with a relative twist, a structure currently unachievable in solid-state materials.","We give the commensurate conditions under which the three-dimensional moir\\'e pattern features a periodic structure, termed a three-dimensional moir\\'e crystal.","We emphasize a key distinction of three-dimensional moir\\'e physics: in three dimensions, the twist operation generically does not commute with the rotational symmetry of the original lattice, unlike in two dimensions, where these two always commute.","Consequently, the moir\\'e crystal can exhibit a crystalline structure that differs from the original underlying lattice.","We demonstrate that twisting a simple cubic lattice can generate various crystal structures.","This capability of altering crystal structures by twisting offers a broad range of tunability for three-dimensional band structures."],"url":"http://arxiv.org/abs/2404.19608v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 14:59:42","title":"Strong minimal model theorem and Massey products","abstract":"Kadeishvili's minimal model theorem establishes the existence of an $A_\\infty$-structure, unique up to isomorphism, on the cohomology of a dg associative algebra, which captures its homotopy type. In this note we prove the existence of minimal models that are unique up to isotopy, a stronger result obviously known to T. Kadeishvili and certainly to others, yet seemingly overlooked by mankind. We will explore how this stronger result can help in the study of Massey products. First, we show that the attempts to extract a local information from the ternary operation $\\mu_3$ of our minimal model leads directly to the rediscovery of the triple Massey product. The motto is: \"The triple Massey product is an invariant manifestation of $\\mu_3$.\" We then prove that, under reasonable assumptions, the higher Massey product $\\langle x_1,\\ldots,x_n\\rangle$ equals the set of all values $\\mu_n(x_1,\\ldots,x_n)$, where $\\mu_n$ runs over the $n$-ary products of our minimal models. We believe that this note will help to elucidate the still somewhat enigmatic relationship between minimal models and Massey products.","sentences":["Kadeishvili's minimal model theorem establishes the existence of an $A_\\infty$-structure, unique up to isomorphism, on the cohomology of a dg associative algebra, which captures its homotopy type.","In this note we prove the existence of minimal models that are unique up to isotopy, a stronger result obviously known to T. Kadeishvili and certainly to others, yet seemingly overlooked by mankind.","We will explore how this stronger result can help in the study of Massey products.","First, we show that the attempts to extract a local information from the ternary operation $\\mu_3$ of our minimal model leads directly to the rediscovery of the triple Massey product.","The motto is: \"The triple Massey product is an invariant manifestation of $\\mu_3$.\" We then prove that, under reasonable assumptions, the higher Massey product $\\langle x_1,\\ldots,x_n\\rangle$ equals the set of all values $\\mu_n(x_1,\\ldots,x_n)$, where $\\mu_n$ runs over the $n$-ary products of our minimal models.","We believe that this note will help to elucidate the still somewhat enigmatic relationship between minimal models and Massey products."],"url":"http://arxiv.org/abs/2404.19607v1","category":"math.AT"}
{"created":"2024-04-30 14:57:10","title":"Electronic structure of the surface superconducting Weyl semimetal PtBi$_2$","abstract":"Trigonal PtBi$_2$ is a layered semimetal without inversion symmetry, featuring 12 Weyl points in the vicinity of the Fermi energy. Its topological Fermi arcs were recently shown to superconduct at low temperatures where bulk superconductivity is absent. Here, we perform first-principles calculations to investigate in detail the bulk and surface electronic structure of PtBi$_2$, and obtain the spin texture as well as the momentum-dependent localization of the arcs. Motivated by the experimentally observed recovery of inversion symmetry under pressure or upon doping, we interpolate between the two structures and determine the energy and momentum dependence of the Weyl nodes. For deeper insights into the surface superconductivity of PtBi$_2$, we construct a symmetry-adapted effective four-band model that accurately reproduces the Weyl points of PtBi$_2$. We supplement this model with an analysis of the symmetry-allowed pairings between the Fermi arcs, which naturally mix spin-singlet and spin-triplet channels. Moreover, the presence of surface-only superconductivity facilitates an intrinsic superconductor-semimetal-superconductor Josephson junction, with the semimetallic phase sandwiched between the two superconducting surfaces. For a phase difference of $\\pi$, zero-energy Andreev bound states develop between the two terminations.","sentences":["Trigonal PtBi$_2$ is a layered semimetal without inversion symmetry, featuring 12 Weyl points in the vicinity of the Fermi energy.","Its topological Fermi arcs were recently shown to superconduct at low temperatures where bulk superconductivity is absent.","Here, we perform first-principles calculations to investigate in detail the bulk and surface electronic structure of PtBi$_2$, and obtain the spin texture as well as the momentum-dependent localization of the arcs.","Motivated by the experimentally observed recovery of inversion symmetry under pressure or upon doping, we interpolate between the two structures and determine the energy and momentum dependence of the Weyl nodes.","For deeper insights into the surface superconductivity of PtBi$_2$, we construct a symmetry-adapted effective four-band model that accurately reproduces the Weyl points of PtBi$_2$. We supplement this model with an analysis of the symmetry-allowed pairings between the Fermi arcs, which naturally mix spin-singlet and spin-triplet channels.","Moreover, the presence of surface-only superconductivity facilitates an intrinsic superconductor-semimetal-superconductor Josephson junction, with the semimetallic phase sandwiched between the two superconducting surfaces.","For a phase difference of $\\pi$, zero-energy Andreev bound states develop between the two terminations."],"url":"http://arxiv.org/abs/2404.19606v1","category":"cond-mat.supr-con"}
{"created":"2024-04-30 14:52:15","title":"Uncertainty quantification for charge transport in GNRs through particle Galerkin methods for the semiclassical Boltzmann equation","abstract":"In this article, we investigate some issues related to the quantification of uncertainties associated with the electrical properties of graphene nanoribbons. The approach is suited to understand the effects of missing information linked to the difficulty of fixing some material parameters, such as the band gap, and the strength of the applied electric field. In particular, we focus on the extension of particle Galerkin methods for kinetic equations in the case of the semiclassical Boltzmann equation for charge transport in graphene nanoribbons with uncertainties. To this end, we develop an efficient particle scheme which allows us to parallelize the computation and then, after a suitable generalization of the scheme to the case of random inputs, we present a Galerkin reformulation of the particle dynamics, obtained by means of a generalized polynomial chaos approach, which allows the reconstruction of the kinetic distribution. As a consequence, the proposed particle-based scheme preserves the physical properties and the positivity of the distribution function also in the presence of a complex scattering in the transport equation of electrons. The impact of the uncertainty of the band gap and applied field on the electrical current is analyzed.","sentences":["In this article, we investigate some issues related to the quantification of uncertainties associated with the electrical properties of graphene nanoribbons.","The approach is suited to understand the effects of missing information linked to the difficulty of fixing some material parameters, such as the band gap, and the strength of the applied electric field.","In particular, we focus on the extension of particle Galerkin methods for kinetic equations in the case of the semiclassical Boltzmann equation for charge transport in graphene nanoribbons with uncertainties.","To this end, we develop an efficient particle scheme which allows us to parallelize the computation and then, after a suitable generalization of the scheme to the case of random inputs, we present a Galerkin reformulation of the particle dynamics, obtained by means of a generalized polynomial chaos approach, which allows the reconstruction of the kinetic distribution.","As a consequence, the proposed particle-based scheme preserves the physical properties and the positivity of the distribution function also in the presence of a complex scattering in the transport equation of electrons.","The impact of the uncertainty of the band gap and applied field on the electrical current is analyzed."],"url":"http://arxiv.org/abs/2404.19602v1","category":"physics.comp-ph"}
{"created":"2024-04-30 14:51:41","title":"Recovery of the X-ray polarization of Swift J1727.8$-$1613 after the soft to hard spectral transition","abstract":"We report on the detection of X-ray polarization in the black-hole X-ray binary Swift J1727.8$-$1613 during its dim hard spectral state by the Imaging X-ray Polarimetry Explorer (IXPE). This is the first detection of the X-ray polarization at the transition from the soft to the hard state in an X-ray binary. We find a 2$-$8 keV averaged polarization degree of (3.3 ${\\pm}$ 0.4) % and the corresponding polarization angle of 3{\\deg} ${\\pm}$ 4{\\deg}, which matches with the polarization detected during the rising stage of the outburst, in September$-$October 2023, within 1${\\sigma}$ uncertainty. The observational campaign complements previous studies of this source and enables comparison of the X-ray polarization properties of a single transient across the X-ray hardness-intensity diagram. The complete recovery of X-ray polarization properties, including energy dependence, follows a dramatic drop of the X-ray polarization during the soft state. The new IXPE observations in the dim hard state at the reverse transition indicate that the accretion properties, including the geometry of the corona, appear to be strikingly similar to the bright hard state during the outburst rise even though the X-ray luminosities differ by two orders of magnitude.","sentences":["We report on the detection of X-ray polarization in the black-hole X-ray binary Swift J1727.8$-$1613 during its dim hard spectral state by the Imaging X-ray Polarimetry Explorer (IXPE).","This is the first detection of the X-ray polarization at the transition from the soft to the hard state in an X-ray binary.","We find a 2$-$8 keV averaged polarization degree of (3.3 ${\\pm}$ 0.4) % and the corresponding polarization angle of 3{\\deg} ${\\pm}$ 4{\\deg}, which matches with the polarization detected during the rising stage of the outburst, in September$-$October 2023, within 1${\\sigma}$ uncertainty.","The observational campaign complements previous studies of this source and enables comparison of the X-ray polarization properties of a single transient across the X-ray hardness-intensity diagram.","The complete recovery of X-ray polarization properties, including energy dependence, follows a dramatic drop of the X-ray polarization during the soft state.","The new IXPE observations in the dim hard state at the reverse transition indicate that the accretion properties, including the geometry of the corona, appear to be strikingly similar to the bright hard state during the outburst rise even though the X-ray luminosities differ by two orders of magnitude."],"url":"http://arxiv.org/abs/2404.19601v1","category":"astro-ph.HE"}
{"created":"2024-04-30 14:49:26","title":"Parity and time-reversal symmetry violation in diatomic molecules: LaO, LaS and LuO","abstract":"The violation of parity (P) and time-reversal (T) symmetry is enhanced in the LaS, LaO and LuO molecules due to the existence of states of opposite parity with small energy differences and the presence of heavy nuclei. We calculate the molecular enhancement for the P, T-violating electron electric dipole moment ($W_{\\mathrm{d}}$), scalar-pseudoscalar nucleon-electron interaction ($W_{\\mathrm{s}}$), nuclear magnetic quadrupole moment ($W_{\\mathrm{M}}$), and for the nuclear spin-dependent P-violating anapole moment ($W_{\\mathrm{A}}$). We use the relativistic 4-components coupled cluster method and perform a systematic study to estimate the associated uncertainties in our approach. We find that the individual contribution of each computational parameter to the total uncertainty in a system is approximately the same for all the calculated enhancement factors, summing up to a total uncertainty of $\\sim7$\\%. We discuss the energy shifts and matrix elements associated with the calculated molecular enhancement factors and relate them to higher-energy P- and P, T- violating interactions.","sentences":["The violation of parity (P) and time-reversal (T) symmetry is enhanced in the LaS, LaO and LuO molecules due to the existence of states of opposite parity with small energy differences and the presence of heavy nuclei.","We calculate the molecular enhancement for the P, T-violating electron electric dipole moment ($W_{\\mathrm{d}}$), scalar-pseudoscalar nucleon-electron interaction ($W_{\\mathrm{s}}$), nuclear magnetic quadrupole moment ($W_{\\mathrm{M}}$), and for the nuclear spin-dependent P-violating anapole moment ($W_{\\mathrm{A}}$).","We use the relativistic 4-components coupled cluster method and perform a systematic study to estimate the associated uncertainties in our approach.","We find that the individual contribution of each computational parameter to the total uncertainty in a system is approximately the same for all the calculated enhancement factors, summing up to a total uncertainty of $\\sim7$\\%.","We discuss the energy shifts and matrix elements associated with the calculated molecular enhancement factors and relate them to higher-energy P- and P, T- violating interactions."],"url":"http://arxiv.org/abs/2404.19599v1","category":"physics.atom-ph"}
{"created":"2024-04-30 14:43:57","title":"Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning","abstract":"The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. However, the impact of backdoor attacks on multilingual models remains under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data in one or two languages can affect the outputs in languages whose instruction-tuning data was not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5, BLOOM, and GPT-3.5-turbo, with high attack success rates, surpassing 95% in several languages across various scenarios. Alarmingly, our findings also indicate that larger models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments show that triggers can still work even after paraphrasing, and the backdoor mechanism proves highly effective in cross-lingual response settings across 25 languages, achieving an average attack success rate of 50%. Our study aims to highlight the vulnerabilities and significant security risks present in current multilingual LLMs, underscoring the emergent need for targeted security measures.","sentences":["The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs.","However, the impact of backdoor attacks on multilingual models remains under-explored.","Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data in one or two languages can affect the outputs in languages whose instruction-tuning data was not poisoned.","Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5, BLOOM, and GPT-3.5-turbo, with high attack success rates, surpassing 95% in several languages across various scenarios.","Alarmingly, our findings also indicate that larger models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma.","Moreover, our experiments show that triggers can still work even after paraphrasing, and the backdoor mechanism proves highly effective in cross-lingual response settings across 25 languages, achieving an average attack success rate of 50%.","Our study aims to highlight the vulnerabilities and significant security risks present in current multilingual LLMs, underscoring the emergent need for targeted security measures."],"url":"http://arxiv.org/abs/2404.19597v1","category":"cs.CL"}
{"created":"2024-04-30 14:31:23","title":"Internal migration after a uniform minimum wage introduction","abstract":"Internal migration is an essential aspect to study labor mobility. I exploit the German statutory minimum wage introduction in 2015 to estimate its push and pull effects on internal migration using a 2% sample of administrative data. In a conditional fixed effects Poisson difference-in-differences framework with a continuous treatment, I find that the minimum wage introduction leads to an increase in the out-migration of low-skilled workers with migrant background by 25% with an increasing tendency over time from districts where a high share of workers are subject to the minimum wage (high-bite districts). In contrast the migration decision of native-born low-skilled workers is not affected by the policy. However, both native-born low-skilled workers and those with a migrant background do relocate across establishments, leaving high-bite districts as their workplace. In addition, I find an increase for unemployed individuals with a migrant background in out-migrating from high-bite districts. These results emphasize the importance of considering the effects on geographical labor mobility when implementing and analyzing policies that affect the determinants of internal migration.","sentences":["Internal migration is an essential aspect to study labor mobility.","I exploit the German statutory minimum wage introduction in 2015 to estimate its push and pull effects on internal migration using a 2% sample of administrative data.","In a conditional fixed effects Poisson difference-in-differences framework with a continuous treatment, I find that the minimum wage introduction leads to an increase in the out-migration of low-skilled workers with migrant background by 25% with an increasing tendency over time from districts where a high share of workers are subject to the minimum wage (high-bite districts).","In contrast the migration decision of native-born low-skilled workers is not affected by the policy.","However, both native-born low-skilled workers and those with a migrant background do relocate across establishments, leaving high-bite districts as their workplace.","In addition, I find an increase for unemployed individuals with a migrant background in out-migrating from high-bite districts.","These results emphasize the importance of considering the effects on geographical labor mobility when implementing and analyzing policies that affect the determinants of internal migration."],"url":"http://arxiv.org/abs/2404.19590v1","category":"econ.GN"}
{"created":"2024-04-30 14:19:06","title":"Leveraging Label Information for Stealthy Data Stealing in Vertical Federated Learning","abstract":"We develop DMAVFL, a novel attack strategy that evades current detection mechanisms. The key idea is to integrate a discriminator with auxiliary classifier that takes a full advantage of the label information (which was completely ignored in previous attacks): on one hand, label information helps to better characterize embeddings of samples from distinct classes, yielding an improved reconstruction performance; on the other hand, computing malicious gradients with label information better mimics the honest training, making the malicious gradients indistinguishable from the honest ones, and the attack much more stealthy. Our comprehensive experiments demonstrate that DMAVFL significantly outperforms existing attacks, and successfully circumvents SOTA defenses for malicious attacks. Additional ablation studies and evaluations on other defenses further underscore the robustness and effectiveness of DMAVFL.","sentences":["We develop DMAVFL, a novel attack strategy that evades current detection mechanisms.","The key idea is to integrate a discriminator with auxiliary classifier that takes a full advantage of the label information (which was completely ignored in previous attacks): on one hand, label information helps to better characterize embeddings of samples from distinct classes, yielding an improved reconstruction performance; on the other hand, computing malicious gradients with label information better mimics the honest training, making the malicious gradients indistinguishable from the honest ones, and the attack much more stealthy.","Our comprehensive experiments demonstrate that DMAVFL significantly outperforms existing attacks, and successfully circumvents SOTA defenses for malicious attacks.","Additional ablation studies and evaluations on other defenses further underscore the robustness and effectiveness of DMAVFL."],"url":"http://arxiv.org/abs/2404.19582v1","category":"cs.LG"}
{"created":"2024-04-30 14:07:11","title":"Self-assembling of multilayered polymorphs with ion beams","abstract":"Polymorphism contributes to the diversity of nature, so that even materials having identical chemical compositions exhibit variations in properties because of different lattice symmetries. Thus, if stacked together into multilayers, polymorphs may work as an alternative approach to the sequential deposition of layers with different chemical compositions. However, selective polymorph crystallization during conventional thin film synthesis is not trivial; e.g. opting for step-like changes of temperature and/or pressure correlated with switching from one polymorph to another during synthesis is tricky, since it may cause degradation of the structural quality. In the present work, applying the disorder-induced ordering approach we fabricated such multilayered polymorph structures using ion beams. We show that during ion irradiation of gallium oxide, the dynamic annealing of disorder may be tuned towards self-assembling of several polymorph interfaces, consistently with theoretical modelling. Specifically, we demonstrated multilayers with two polymorph interface repetitions obtained in one ion beam assisted fabrication step. Importantly, single crystal structure of the polymorphs was maintained in between interfaces exhibiting repeatable crystallographic relationships, correlating with optical cross-sectional maps. This data paves the way for enhancing functionalities in materials with not previously thought capabilities of ion beam technology.","sentences":["Polymorphism contributes to the diversity of nature, so that even materials having identical chemical compositions exhibit variations in properties because of different lattice symmetries.","Thus, if stacked together into multilayers, polymorphs may work as an alternative approach to the sequential deposition of layers with different chemical compositions.","However, selective polymorph crystallization during conventional thin film synthesis is not trivial; e.g. opting for step-like changes of temperature and/or pressure correlated with switching from one polymorph to another during synthesis is tricky, since it may cause degradation of the structural quality.","In the present work, applying the disorder-induced ordering approach we fabricated such multilayered polymorph structures using ion beams.","We show that during ion irradiation of gallium oxide, the dynamic annealing of disorder may be tuned towards self-assembling of several polymorph interfaces, consistently with theoretical modelling.","Specifically, we demonstrated multilayers with two polymorph interface repetitions obtained in one ion beam assisted fabrication step.","Importantly, single crystal structure of the polymorphs was maintained in between interfaces exhibiting repeatable crystallographic relationships, correlating with optical cross-sectional maps.","This data paves the way for enhancing functionalities in materials with not previously thought capabilities of ion beam technology."],"url":"http://arxiv.org/abs/2404.19572v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 14:05:17","title":"A note on background independence in $\\text{AdS}_3$ string theory","abstract":"In this note, we comment on the path integral formulation of string theory on $\\mathcal{M}\\times\\text{S}^3\\times\\mathbb{T}^4$ where $\\mathcal{M}$ is any hyperbolic 3-manifold. In the special case of $k=1$ NS-NS flux, we provide a covariant description of the worldsheet theory and argue that the path integral depends only on the details of the conformal boundary $\\partial\\mathcal{M}$, making the background independence of this theory manifest. We provide a simple path integral argument that the path integral localizes onto holomorphic covering maps from the worldsheet to the boundary. For closed manifolds $\\mathcal{M}$, the gravitational path integral is argued to be trivial. This implies that the bulk gravitational theory has precisely one state in its Hilbert space. Finally, we comment on the effect of continuous deformations of the worldsheet theory which introduce non-minimal string tension.","sentences":["In this note, we comment on the path integral formulation of string theory on $\\mathcal{M}\\times\\text{S}^3\\times\\mathbb{T}^4$ where $\\mathcal{M}$ is any hyperbolic 3-manifold.","In the special case of $k=1$ NS-NS flux, we provide a covariant description of the worldsheet theory and argue that the path integral depends only on the details of the conformal boundary $\\partial\\mathcal{M}$, making the background independence of this theory manifest.","We provide a simple path integral argument that the path integral localizes onto holomorphic covering maps from the worldsheet to the boundary.","For closed manifolds $\\mathcal{M}$, the gravitational path integral is argued to be trivial.","This implies that the bulk gravitational theory has precisely one state in its Hilbert space.","Finally, we comment on the effect of continuous deformations of the worldsheet theory which introduce non-minimal string tension."],"url":"http://arxiv.org/abs/2404.19571v1","category":"hep-th"}
{"created":"2024-04-30 13:59:13","title":"Enhancing Deep Learning Model Explainability in Brain Tumor Datasets using Post-Heuristic Approaches","abstract":"The application of deep learning models in medical diagnosis has showcased considerable efficacy in recent years. Nevertheless, a notable limitation involves the inherent lack of explainability during decision-making processes. This study addresses such a constraint, by enhancing the interpretability robustness. The primary focus is directed towards refining the explanations generated by the LIME Library and LIME image explainer. This is achieved throuhg post-processing mechanisms, based on scenario-specific rules. Multiple experiments have been conducted using publicly accessible datasets related to brain tumor detection. Our proposed post-heuristic approach demonstrates significant advancements, yielding more robust and concrete results, in the context of medical diagnosis.","sentences":["The application of deep learning models in medical diagnosis has showcased considerable efficacy in recent years.","Nevertheless, a notable limitation involves the inherent lack of explainability during decision-making processes.","This study addresses such a constraint, by enhancing the interpretability robustness.","The primary focus is directed towards refining the explanations generated by the LIME Library and LIME image explainer.","This is achieved throuhg post-processing mechanisms, based on scenario-specific rules.","Multiple experiments have been conducted using publicly accessible datasets related to brain tumor detection.","Our proposed post-heuristic approach demonstrates significant advancements, yielding more robust and concrete results, in the context of medical diagnosis."],"url":"http://arxiv.org/abs/2404.19568v1","category":"eess.IV"}
{"created":"2024-04-30 13:50:55","title":"RepEval: Effective Text Evaluation with LLM Representation","abstract":"Automatic evaluation metrics for generated texts play an important role in the NLG field, especially with the rapid growth of LLMs. However, existing metrics are often limited to specific scenarios, making it challenging to meet the evaluation requirements of expanding LLM applications. Therefore, there is a demand for new, flexible, and effective metrics. In this study, we introduce RepEval, the first metric leveraging the projection of LLM representations for evaluation. RepEval requires minimal sample pairs for training, and through simple prompt modifications, it can easily transition to various tasks. Results on ten datasets from three tasks demonstrate the high effectiveness of our method, which exhibits stronger correlations with human judgments compared to previous metrics, even outperforming GPT-4. Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics.","sentences":["Automatic evaluation metrics for generated texts play an important role in the NLG field, especially with the rapid growth of LLMs.","However, existing metrics are often limited to specific scenarios, making it challenging to meet the evaluation requirements of expanding LLM applications.","Therefore, there is a demand for new, flexible, and effective metrics.","In this study, we introduce RepEval, the first metric leveraging the projection of LLM representations for evaluation.","RepEval requires minimal sample pairs for training, and through simple prompt modifications, it can easily transition to various tasks.","Results on ten datasets from three tasks demonstrate the high effectiveness of our method, which exhibits stronger correlations with human judgments compared to previous metrics, even outperforming GPT-4.","Our work underscores the richness of information regarding text quality embedded within LLM representations, offering insights for the development of new metrics."],"url":"http://arxiv.org/abs/2404.19563v1","category":"cs.CL"}
{"created":"2024-04-30 13:42:01","title":"Computational study of numerical flux schemes for mesoscale atmospheric flows in a Finite Volume framework","abstract":"We develop, and implement in a Finite Volume environment, a density-based approach for the Euler equations written in conservative form using density, momentum, and total energy as variables. Under simplifying assumptions, these equations are used to describe non-hydrostatic atmospheric flow. The well-balancing of the approach is ensured by a local hydrostatic reconstruction updated in runtime during the simulation to keep the numerical error under control. To approximate the solution of the Riemann problem, we consider four methods: Roe-Pike, HLLC, AUSM+-up and HLLC-AUSM. We assess our density-based approach and compare the accuracy of these four approximated Riemann solvers using two two classical benchmarks, namely the smooth rising thermal bubble and the density current.","sentences":["We develop, and implement in a Finite Volume environment, a density-based approach for the Euler equations written in conservative form using density, momentum, and total energy as variables.","Under simplifying assumptions, these equations are used to describe non-hydrostatic atmospheric flow.","The well-balancing of the approach is ensured by a local hydrostatic reconstruction updated in runtime during the simulation to keep the numerical error under control.","To approximate the solution of the Riemann problem, we consider four methods: Roe-Pike, HLLC, AUSM+-up and HLLC-AUSM.","We assess our density-based approach and compare the accuracy of these four approximated Riemann solvers using two two classical benchmarks, namely the smooth rising thermal bubble and the density current."],"url":"http://arxiv.org/abs/2404.19559v1","category":"math.NA"}
{"created":"2024-04-30 17:15:42","title":"PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios","abstract":"We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios. Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios. This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond. In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory. The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy. This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control. Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios. More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html .","sentences":["We address the challenge of content diversity and controllability in pedestrian simulation for driving scenarios.","Recent pedestrian animation frameworks have a significant limitation wherein they primarily focus on either following trajectory [46] or the content of the reference video [57], consequently overlooking the potential diversity of human motion within such scenarios.","This limitation restricts the ability to generate pedestrian behaviors that exhibit a wider range of variations and realistic motions and therefore restricts its usage to provide rich motion content for other components in the driving simulation system, e.g., suddenly changed motion to which the autonomous vehicle should respond.","In our approach, we strive to surpass the limitation by showcasing diverse human motions obtained from various sources, such as generated human motions, in addition to following the given trajectory.","The fundamental contribution of our framework lies in combining the motion tracking task with trajectory following, which enables the tracking of specific motion parts (e.g., upper body) while simultaneously following the given trajectory by a single policy.","This way, we significantly enhance both the diversity of simulated human motion within the given scenario and the controllability of the content, including language-based control.","Our framework facilitates the generation of a wide range of human motions, contributing to greater realism and adaptability in pedestrian simulations for driving scenarios.","More information is on our project page https://wangjingbo1219.github.io/papers/CVPR2024_PACER_PLUS/PACERPLUSPage.html ."],"url":"http://arxiv.org/abs/2404.19722v1","category":"cs.CV"}
{"created":"2024-04-30 16:06:07","title":"Derivative learning of tensorial quantities -- Predicting finite temperature infrared spectra from first principles","abstract":"We develop a strategy that integrates machine learning and first-principles calculations to achieve technical accurate predictions of infrared spectra. Specifically, the methodology allows to predict infrared spectra for complex systems at finite temperatures. The method's effectiveness is demonstrated in challenging scenarios, such as the analysis of water and the organic-inorganic halide perovskite MAPbI$_{3}$, where our results consistently align with experimental data. A distinctive feature of the methodology is the incorporation of derivative learning, which proves indispensable for obtaining accurate polarization data in bulk materials and facilitates the training of a machine learning surrogate model of the polarization adapted to rotational and translational symmetries. We achieve polarisation prediction accuracies of about 1 % by training only on the predicted Born effective charges.","sentences":["We develop a strategy that integrates machine learning and first-principles calculations to achieve technical accurate predictions of infrared spectra.","Specifically, the methodology allows to predict infrared spectra for complex systems at finite temperatures.","The method's effectiveness is demonstrated in challenging scenarios, such as the analysis of water and the organic-inorganic halide perovskite MAPbI$_{3}$, where our results consistently align with experimental data.","A distinctive feature of the methodology is the incorporation of derivative learning, which proves indispensable for obtaining accurate polarization data in bulk materials and facilitates the training of a machine learning surrogate model of the polarization adapted to rotational and translational symmetries.","We achieve polarisation prediction accuracies of about 1 % by training only on the predicted Born effective charges."],"url":"http://arxiv.org/abs/2404.19674v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 15:51:05","title":"Masked Multi-Query Slot Attention for Unsupervised Object Discovery","abstract":"Unsupervised object discovery is becoming an essential line of research for tackling recognition problems that require decomposing an image into entities, such as semantic segmentation and object detection. Recently, object-centric methods that leverage self-supervision have gained popularity, due to their simplicity and adaptability to different settings and conditions. However, those methods do not exploit effective techniques already employed in modern self-supervised approaches. In this work, we consider an object-centric approach in which DINO ViT features are reconstructed via a set of queried representations called slots. Based on that, we propose a masking scheme on input features that selectively disregards the background regions, inducing our model to focus more on salient objects during the reconstruction phase. Moreover, we extend the slot attention to a multi-query approach, allowing the model to learn multiple sets of slots, producing more stable masks. During training, these multiple sets of slots are learned independently while, at test time, these sets are merged through Hungarian matching to obtain the final slots. Our experimental results and ablations on the PASCAL-VOC 2012 dataset show the importance of each component and highlight how their combination consistently improves object localization. Our source code is available at: https://github.com/rishavpramanik/maskedmultiqueryslot","sentences":["Unsupervised object discovery is becoming an essential line of research for tackling recognition problems that require decomposing an image into entities, such as semantic segmentation and object detection.","Recently, object-centric methods that leverage self-supervision have gained popularity, due to their simplicity and adaptability to different settings and conditions.","However, those methods do not exploit effective techniques already employed in modern self-supervised approaches.","In this work, we consider an object-centric approach in which DINO ViT features are reconstructed via a set of queried representations called slots.","Based on that, we propose a masking scheme on input features that selectively disregards the background regions, inducing our model to focus more on salient objects during the reconstruction phase.","Moreover, we extend the slot attention to a multi-query approach, allowing the model to learn multiple sets of slots, producing more stable masks.","During training, these multiple sets of slots are learned independently while, at test time, these sets are merged through Hungarian matching to obtain the final slots.","Our experimental results and ablations on the PASCAL-VOC 2012 dataset show the importance of each component and highlight how their combination consistently improves object localization.","Our source code is available at: https://github.com/rishavpramanik/maskedmultiqueryslot"],"url":"http://arxiv.org/abs/2404.19654v1","category":"cs.CV"}
{"created":"2024-04-30 15:44:57","title":"Cybersecurity Pathways Towards CE-Certified Autonomous Forestry Machines","abstract":"The increased importance of cybersecurity in autonomous machinery is becoming evident in the forestry domain. Forestry worksites are becoming more complex with the involvement of multiple systems and system of systems. Hence, there is a need to investigate how to address cybersecurity challenges for autonomous systems of systems in the forestry domain. Using a literature review and adapting standards from similar domains, as well as collaborative sessions with domain experts, we identify challenges towards CE-certified autonomous forestry machines focusing on cybersecurity and safety. Furthermore, we discuss the relationship between safety and cybersecurity risk assessment and their relation to AI, highlighting the need for a holistic methodology for their assurance.","sentences":["The increased importance of cybersecurity in autonomous machinery is becoming evident in the forestry domain.","Forestry worksites are becoming more complex with the involvement of multiple systems and system of systems.","Hence, there is a need to investigate how to address cybersecurity challenges for autonomous systems of systems in the forestry domain.","Using a literature review and adapting standards from similar domains, as well as collaborative sessions with domain experts, we identify challenges towards CE-certified autonomous forestry machines focusing on cybersecurity and safety.","Furthermore, we discuss the relationship between safety and cybersecurity risk assessment and their relation to AI, highlighting the need for a holistic methodology for their assurance."],"url":"http://arxiv.org/abs/2404.19643v1","category":"cs.SE"}
{"created":"2024-04-30 15:42:45","title":"ESP-Zero: Unsupervised enhancement of zero-shot classification for Extremely Sparse Point cloud","abstract":"In recent years, zero-shot learning has attracted the focus of many researchers, due to its flexibility and generality. Many approaches have been proposed to achieve the zero-shot classification of the point clouds for 3D object understanding, following the schema of CLIP. However, in the real world, the point clouds could be extremely sparse, dramatically limiting the effectiveness of the 3D point cloud encoders, and resulting in the misalignment of point cloud features and text embeddings. To the point cloud encoders to fit the extremely sparse point clouds without re-running the pre-training procedure which could be time-consuming and expensive, in this work, we propose an unsupervised model adaptation approach to enhance the point cloud encoder for the extremely sparse point clouds. We propose a novel fused-cross attention layer that expands the pre-trained self-attention layer with additional learnable tokens and attention blocks, which effectively modifies the point cloud features while maintaining the alignment between point cloud features and text embeddings. We also propose a complementary learning-based self-distillation schema that encourages the modified features to be pulled apart from the irrelevant text embeddings without overfitting the feature space to the observed text embeddings. Extensive experiments demonstrate that the proposed approach effectively increases the zero-shot capability on extremely sparse point clouds, and overwhelms other state-of-the-art model adaptation approaches.","sentences":["In recent years, zero-shot learning has attracted the focus of many researchers, due to its flexibility and generality.","Many approaches have been proposed to achieve the zero-shot classification of the point clouds for 3D object understanding, following the schema of CLIP.","However, in the real world, the point clouds could be extremely sparse, dramatically limiting the effectiveness of the 3D point cloud encoders, and resulting in the misalignment of point cloud features and text embeddings.","To the point cloud encoders to fit the extremely sparse point clouds without re-running the pre-training procedure which could be time-consuming and expensive, in this work, we propose an unsupervised model adaptation approach to enhance the point cloud encoder for the extremely sparse point clouds.","We propose a novel fused-cross attention layer that expands the pre-trained self-attention layer with additional learnable tokens and attention blocks, which effectively modifies the point cloud features while maintaining the alignment between point cloud features and text embeddings.","We also propose a complementary learning-based self-distillation schema that encourages the modified features to be pulled apart from the irrelevant text embeddings without overfitting the feature space to the observed text embeddings.","Extensive experiments demonstrate that the proposed approach effectively increases the zero-shot capability on extremely sparse point clouds, and overwhelms other state-of-the-art model adaptation approaches."],"url":"http://arxiv.org/abs/2404.19639v1","category":"cs.CV"}
{"created":"2024-04-30 14:43:51","title":"Debiased Collaborative Filtering with Kernel-Based Causal Balancing","abstract":"Debiased collaborative filtering aims to learn an unbiased prediction model by removing different biases in observational datasets. To solve this problem, one of the simple and effective methods is based on the propensity score, which adjusts the observational sample distribution to the target one by reweighting observed instances. Ideally, propensity scores should be learned with causal balancing constraints. However, existing methods usually ignore such constraints or implement them with unreasonable approximations, which may affect the accuracy of the learned propensity scores. To bridge this gap, in this paper, we first analyze the gaps between the causal balancing requirements and existing methods such as learning the propensity with cross-entropy loss or manually selecting functions to balance. Inspired by these gaps, we propose to approximate the balancing functions in reproducing kernel Hilbert space and demonstrate that, based on the universal property and representer theorem of kernel functions, the causal balancing constraints can be better satisfied. Meanwhile, we propose an algorithm that adaptively balances the kernel function and theoretically analyze the generalization error bound of our methods. We conduct extensive experiments to demonstrate the effectiveness of our methods, and to promote this research direction, we have released our project at https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing.","sentences":["Debiased collaborative filtering aims to learn an unbiased prediction model by removing different biases in observational datasets.","To solve this problem, one of the simple and effective methods is based on the propensity score, which adjusts the observational sample distribution to the target one by reweighting observed instances.","Ideally, propensity scores should be learned with causal balancing constraints.","However, existing methods usually ignore such constraints or implement them with unreasonable approximations, which may affect the accuracy of the learned propensity scores.","To bridge this gap, in this paper, we first analyze the gaps between the causal balancing requirements and existing methods such as learning the propensity with cross-entropy loss or manually selecting functions to balance.","Inspired by these gaps, we propose to approximate the balancing functions in reproducing kernel Hilbert space and demonstrate that, based on the universal property and representer theorem of kernel functions, the causal balancing constraints can be better satisfied.","Meanwhile, we propose an algorithm that adaptively balances the kernel function and theoretically analyze the generalization error bound of our methods.","We conduct extensive experiments to demonstrate the effectiveness of our methods, and to promote this research direction, we have released our project at https://github.com/haoxuanli-pku/ICLR24-Kernel-Balancing."],"url":"http://arxiv.org/abs/2404.19596v1","category":"cs.IR"}
{"created":"2024-04-30 14:41:06","title":"Reactive Temporal Logic-based Planning and Control for Interactive Robotic Tasks","abstract":"Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes. Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt. To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans. We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes. At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL). Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes.","sentences":["Robots interacting with humans must be safe, reactive and adapt online to unforeseen environmental and task changes.","Achieving these requirements concurrently is a challenge as interactive planners lack formal safety guarantees, while safe motion planners lack flexibility to adapt.","To tackle this, we propose a modular control architecture that generates both safe and reactive motion plans for human-robot interaction by integrating temporal logic-based discrete task level plans with continuous Dynamical System (DS)-based motion plans.","We formulate a reactive temporal logic formula that enables users to define task specifications through structured language, and propose a planning algorithm at the task level that generates a sequence of desired robot behaviors while being adaptive to environmental changes.","At the motion level, we incorporate control Lyapunov functions and control barrier functions to compute stable and safe continuous motion plans for two types of robot behaviors: (i) complex, possibly periodic motions given by autonomous DS and (ii) time-critical tasks specified by Signal Temporal Logic~(STL).","Our methodology is demonstrated on the Franka robot arm performing wiping tasks on a whiteboard and a mannequin that is compliant to human interactions and adaptive to environmental changes."],"url":"http://arxiv.org/abs/2404.19594v1","category":"cs.RO"}
{"created":"2024-04-30 14:28:08","title":"Acceptance Tests of more than 10 000 Photomultiplier Tubes for the multi-PMT Digital Optical Modules of the IceCube Upgrade","abstract":"More than 10,000 photomultiplier tubes (PMTs) with a diameter of 80 mm will be installed in multi-PMT Digital Optical Modules (mDOMs) of the IceCube Upgrade. These have been tested and pre-calibrated at two sites. A throughput of more than 1000 PMTs per week with both sites was achieved with a modular design of the testing facilities and highly automated testing procedures. The testing facilities can easily be adapted to other PMTs, such that they can, e.g., be re-used for testing the PMTs for IceCube-Gen2. Single photoelectron response, high voltage dependence, time resolution, prepulse, late pulse, afterpulse probabilities, and dark rates were measured for each PMT. We describe the design of the testing facilities, the testing procedures, and the results of the acceptance tests.","sentences":["More than 10,000 photomultiplier tubes (PMTs) with a diameter of 80 mm will be installed in multi-PMT Digital Optical Modules (mDOMs) of the IceCube Upgrade.","These have been tested and pre-calibrated at two sites.","A throughput of more than 1000 PMTs per week with both sites was achieved with a modular design of the testing facilities and highly automated testing procedures.","The testing facilities can easily be adapted to other PMTs, such that they can, e.g., be re-used for testing the PMTs for IceCube-Gen2.","Single photoelectron response, high voltage dependence, time resolution, prepulse, late pulse, afterpulse probabilities, and dark rates were measured for each PMT.","We describe the design of the testing facilities, the testing procedures, and the results of the acceptance tests."],"url":"http://arxiv.org/abs/2404.19589v1","category":"astro-ph.IM"}
{"created":"2024-04-30 14:22:12","title":"Broadband microwave-rate dark pulse microcombs in dissipation-engineered LiNbO$_3$ microresonators","abstract":"Kerr microcombs generated in optical microresonators provide broadband light sources bridging optical and microwave signals. Their translation to thin-film lithium niobate unlocks second-order nonlinear optical interfaces such as electro-optic modulation and frequency doubling for completing comb functionalities. However, the strong Raman response of LiNbO$_3$ has complicated the formation of Kerr microcombs. Until now, dark pulse microcombs, requiring a double balance between Kerr nonlinearity and normal group velocity dispersion as well as gain and loss, have remained elusive in LiNbO$_3$ microresonators. Here, by incorporating dissipation engineering, we demonstrate dark pulse microcombs with 25 GHz repetition frequency and 200 nm span in a high-$Q$ LiNbO$_3$ microresonator. Resonances near the Raman-active wavelengths are strongly damped by controlling phase-matching conditions of a specially designed pulley coupler. The coherence and tunability of the dark pulse microcombs are also investigated. Our work provides a solution to realize high-power microcombs operating at microwave rates on LiNbO$_3$ chips, promising new opportunities for the monolithic integration of applications spanning communication to microwave photonics.","sentences":["Kerr microcombs generated in optical microresonators provide broadband light sources bridging optical and microwave signals.","Their translation to thin-film lithium niobate unlocks second-order nonlinear optical interfaces such as electro-optic modulation and frequency doubling for completing comb functionalities.","However, the strong Raman response of LiNbO$_3$ has complicated the formation of Kerr microcombs.","Until now, dark pulse microcombs, requiring a double balance between Kerr nonlinearity and normal group velocity dispersion as well as gain and loss, have remained elusive in LiNbO$_3$ microresonators.","Here, by incorporating dissipation engineering, we demonstrate dark pulse microcombs with 25 GHz repetition frequency and 200 nm span in a high-$Q$ LiNbO$_3$ microresonator.","Resonances near the Raman-active wavelengths are strongly damped by controlling phase-matching conditions of a specially designed pulley coupler.","The coherence and tunability of the dark pulse microcombs are also investigated.","Our work provides a solution to realize high-power microcombs operating at microwave rates on LiNbO$_3$ chips, promising new opportunities for the monolithic integration of applications spanning communication to microwave photonics."],"url":"http://arxiv.org/abs/2404.19584v1","category":"physics.optics"}
{"created":"2024-04-30 14:03:07","title":"Morphodynamics of chloroplast network control light-avoidance response in the non-motile dinoflagellate Pyrocystis lunula","abstract":"Photosynthetic algae play a significant role in oceanic carbon capture. Their performance, however, is constantly challenged by fluctuations in environmental light conditions. Here, we show that the non-motile single-celled marine dinoflagellate Pyrocystis lunula can internally contract its chloroplast network in response to light. By exposing the cell to various physiological light conditions and applying temporal illumination sequences, we find that network morphodynamics follows simple rules, as established in a mathematical model. Our analysis of the chloroplast structure reveals that its unusual reticulated morphology constitutes properties similar to auxetic metamaterials, facilitating drastic deformations for light-avoidance, while confined by the cell wall. Our study shows how the topologically complex network of chloroplasts is crucial in supporting the dinoflagellate's adaptation to varying light conditions, thereby facilitating essential life-sustaining processes.","sentences":["Photosynthetic algae play a significant role in oceanic carbon capture.","Their performance, however, is constantly challenged by fluctuations in environmental light conditions.","Here, we show that the non-motile single-celled marine dinoflagellate Pyrocystis lunula can internally contract its chloroplast network in response to light.","By exposing the cell to various physiological light conditions and applying temporal illumination sequences, we find that network morphodynamics follows simple rules, as established in a mathematical model.","Our analysis of the chloroplast structure reveals that its unusual reticulated morphology constitutes properties similar to auxetic metamaterials, facilitating drastic deformations for light-avoidance, while confined by the cell wall.","Our study shows how the topologically complex network of chloroplasts is crucial in supporting the dinoflagellate's adaptation to varying light conditions, thereby facilitating essential life-sustaining processes."],"url":"http://arxiv.org/abs/2404.19570v1","category":"physics.bio-ph"}
{"created":"2024-04-30 13:50:45","title":"Starshaped compact hypersurfaces in warped product manifolds I: prescribed curvature equations","abstract":"We derive global curvature estimates for closed strictly star-shaped (n-2)-convex hypersurfaces in warped product spaces, satisfying the prescribed (n-2)-curvature equation with a general right-hand side. The proof is inspired by the recent breakthrough of [Guan-Ren-Wang CPAM 2015] and it can be readily adapted to establish curvature estimates for semi-convex and (k+1)-convex solutions to the general k-curvature equation. Furthermore, it can also be used to prove the same estimates for prescribed curvature measure type equations.","sentences":["We derive global curvature estimates for closed strictly star-shaped (n-2)-convex hypersurfaces in warped product spaces, satisfying the prescribed (n-2)-curvature equation with a general right-hand side.","The proof is inspired by the recent breakthrough of [Guan-Ren-Wang CPAM 2015] and it can be readily adapted to establish curvature estimates for semi-convex and (k+1)-convex solutions to the general k-curvature equation.","Furthermore, it can also be used to prove the same estimates for prescribed curvature measure type equations."],"url":"http://arxiv.org/abs/2404.19562v1","category":"math.AP"}
{"created":"2024-04-30 13:16:05","title":"Distributed Traffic Signal Control via Coordinated Maximum Pressure-plus-Penalty","abstract":"This paper develops an adaptive traffic control policy inspired by Maximum Pressure (MP) while imposing coordination across intersections. The proposed Coordinated Maximum Pressure-plus-Penalty (CMPP) control policy features a local objective for each intersection that consists of the total pressure within the neighborhood and a penalty accounting for the queue capacities and continuous green time for certain movements. The corresponding control task is reformulated as a distributed optimization problem and solved via two customized algorithms: one based on the alternating direction method of multipliers (ADMM) and the other follows a greedy heuristic augmented with a majority vote. CMPP not only provides a theoretical guarantee of queuing network stability but also outperforms several benchmark controllers in simulations on a large-scale real traffic network with lower average travel and waiting time per vehicle, as well as less network congestion. Furthermore, CPMM with the greedy algorithm enjoys comparable computational efficiency as fully decentralized controllers without significantly compromising the control performance, which highlights its great potential for real-world deployment.","sentences":["This paper develops an adaptive traffic control policy inspired by Maximum Pressure (MP) while imposing coordination across intersections.","The proposed Coordinated Maximum Pressure-plus-Penalty (CMPP) control policy features a local objective for each intersection that consists of the total pressure within the neighborhood and a penalty accounting for the queue capacities and continuous green time for certain movements.","The corresponding control task is reformulated as a distributed optimization problem and solved via two customized algorithms: one based on the alternating direction method of multipliers (ADMM) and the other follows a greedy heuristic augmented with a majority vote.","CMPP not only provides a theoretical guarantee of queuing network stability but also outperforms several benchmark controllers in simulations on a large-scale real traffic network with lower average travel and waiting time per vehicle, as well as less network congestion.","Furthermore, CPMM with the greedy algorithm enjoys comparable computational efficiency as fully decentralized controllers without significantly compromising the control performance, which highlights its great potential for real-world deployment."],"url":"http://arxiv.org/abs/2404.19547v1","category":"eess.SY"}
{"created":"2024-04-30 13:03:22","title":"Tree P{\u00f3}lya Splitting distributions for multivariate count data","abstract":"In this article, we develop a new class of multivariate distributions adapted for count data, called Tree P{\\'o}lya Splitting. This class results from the combination of a univariate distribution and singular multivariate distributions along a fixed partition tree. As we will demonstrate, these distributions are flexible, allowing for the modeling of complex dependencies (positive, negative, or null) at the observation level. Specifically, we present the theoretical properties of Tree P{\\'o}lya Splitting distributions by focusing primarily on marginal distributions, factorial moments, and dependency structures (covariance and correlations). The abundance of 17 species of Trichoptera recorded at 49 sites is used, on one hand, to illustrate the theoretical properties developed in this article on a concrete case, and on the other hand, to demonstrate the interest of this type of models, notably by comparing them to classical approaches in ecology or microbiome.","sentences":["In this article, we develop a new class of multivariate distributions adapted for count data, called Tree P{\\'o}lya Splitting.","This class results from the combination of a univariate distribution and singular multivariate distributions along a fixed partition tree.","As we will demonstrate, these distributions are flexible, allowing for the modeling of complex dependencies (positive, negative, or null) at the observation level.","Specifically, we present the theoretical properties of Tree P{\\'o}lya Splitting distributions by focusing primarily on marginal distributions, factorial moments, and dependency structures (covariance and correlations).","The abundance of 17 species of Trichoptera recorded at 49 sites is used, on one hand, to illustrate the theoretical properties developed in this article on a concrete case, and on the other hand, to demonstrate the interest of this type of models, notably by comparing them to classical approaches in ecology or microbiome."],"url":"http://arxiv.org/abs/2404.19528v1","category":"math.ST"}
{"created":"2024-04-30 12:58:38","title":"Scale and Conformal Invariance in 2d Sigma Models, with an Application to N=4 Supersymmetry","abstract":"By adapting previously known arguments concerning Ricci flow and the c-theorem, we give a direct proof that in a two-dimensional sigma-model with compact target space, scale invariance implies conformal invariance in perturbation theory. This argument, which applies to a general sigma-model constructed with a target space metric and B-field, is in accord with a more general proof in the literature that applies to arbitrary two-dimensional quantum field theories. Models with extended supersymmetry and a B-field are known to provide interesting test cases for the relation between scale invariance and conformal invariance in sigma-model perturbation theory. We give examples showing that in such models, the obstructions to conformal invariance suggested by general arguments can actually occur in models with target spaces that are not compact or complete. Thus compactness of the target space, or at least a suitable condition of completeness, is necessary as well as sufficient to ensure that scale invariance implies conformal invariance in models of this type.","sentences":["By adapting previously known arguments concerning Ricci flow and the c-theorem, we give a direct proof that in a two-dimensional sigma-model with compact target space, scale invariance implies conformal invariance in perturbation theory.","This argument, which applies to a general sigma-model constructed with a target space metric and B-field, is in accord with a more general proof in the literature that applies to arbitrary two-dimensional quantum field theories.","Models with extended supersymmetry and a B-field are known to provide interesting test cases for the relation between scale invariance and conformal invariance in sigma-model perturbation theory.","We give examples showing that in such models, the obstructions to conformal invariance suggested by general arguments can actually occur in models with target spaces that are not compact or complete.","Thus compactness of the target space, or at least a suitable condition of completeness, is necessary as well as sufficient to ensure that scale invariance implies conformal invariance in models of this type."],"url":"http://arxiv.org/abs/2404.19526v1","category":"hep-th"}
{"created":"2024-04-30 11:49:29","title":"SpecstatOR: Speckle statistics-based iOCT Segmentation Network for Ophthalmic Surgery","abstract":"This paper presents an innovative approach to intraoperative Optical Coherence Tomography (iOCT) image segmentation in ophthalmic surgery, leveraging statistical analysis of speckle patterns to incorporate statistical pathology-specific prior knowledge. Our findings indicate statistically different speckle patterns within the retina and between retinal layers and surgical tools, facilitating the segmentation of previously unseen data without the necessity for manual labeling. The research involves fitting various statistical distributions to iOCT data, enabling the differentiation of different ocular structures and surgical tools. The proposed segmentation model aims to refine the statistical findings based on prior tissue understanding to leverage statistical and biological knowledge. Incorporating statistical parameters, physical analysis of light-tissue interaction, and deep learning informed by biological structures enhance segmentation accuracy, offering potential benefits to real-time applications in ophthalmic surgical procedures. The study demonstrates the adaptability and precision of using Gamma distribution parameters and the derived binary maps as sole inputs for segmentation, notably enhancing the model's inference performance on unseen data.","sentences":["This paper presents an innovative approach to intraoperative Optical Coherence Tomography (iOCT) image segmentation in ophthalmic surgery, leveraging statistical analysis of speckle patterns to incorporate statistical pathology-specific prior knowledge.","Our findings indicate statistically different speckle patterns within the retina and between retinal layers and surgical tools, facilitating the segmentation of previously unseen data without the necessity for manual labeling.","The research involves fitting various statistical distributions to iOCT data, enabling the differentiation of different ocular structures and surgical tools.","The proposed segmentation model aims to refine the statistical findings based on prior tissue understanding to leverage statistical and biological knowledge.","Incorporating statistical parameters, physical analysis of light-tissue interaction, and deep learning informed by biological structures enhance segmentation accuracy, offering potential benefits to real-time applications in ophthalmic surgical procedures.","The study demonstrates the adaptability and precision of using Gamma distribution parameters and the derived binary maps as sole inputs for segmentation, notably enhancing the model's inference performance on unseen data."],"url":"http://arxiv.org/abs/2404.19481v1","category":"eess.IV"}
{"created":"2024-04-30 11:27:02","title":"Effect of detachment on Magnum-PSI ELM-like pulses: I. Direct observations and qualitative results","abstract":"Conditions similar to those at the end of the divertor leg in a tokamak were replicated in the linear plasma machine Magnum-PSI. The neutral pressure in the target chamber is then increased to cause the target to transition from an attached to a detached state. Superimposed to this steady state regime, ELM-like pulses are reproduced, resulting in a sudden increase in plasma temperature and density, such that the heat flux increases transiently by half an order of magnitude. Visible light emission, target thermography, and Thomson scattering are used to demonstrate that the higher the neutral pressure the more energy is removed from the ELM-like pulse in the volume. If the neutral pressure is sufficiently high, the ELM-like pulse can be prevented from affecting the target and the plasma energy is fully dissipated in the volume instead (ID 4 in Table 1). The visible light images allow the division of the ELM-plasma interaction process of ELM energy dissipation into 3 \"stages\" ranging from no dissipation to full dissipation (the target plasma is detached). In the second publication related to this study, spectroscopic data is analysed with a Bayesian approach, to acquire insights into the significance of molecular processes in dissipating the plasma energy and particles.","sentences":["Conditions similar to those at the end of the divertor leg in a tokamak were replicated in the linear plasma machine Magnum-PSI.","The neutral pressure in the target chamber is then increased to cause the target to transition from an attached to a detached state.","Superimposed to this steady state regime, ELM-like pulses are reproduced, resulting in a sudden increase in plasma temperature and density, such that the heat flux increases transiently by half an order of magnitude.","Visible light emission, target thermography, and Thomson scattering are used to demonstrate that the higher the neutral pressure the more energy is removed from the ELM-like pulse in the volume.","If the neutral pressure is sufficiently high, the ELM-like pulse can be prevented from affecting the target and the plasma energy is fully dissipated in the volume instead (ID 4 in Table 1).","The visible light images allow the division of the ELM-plasma interaction process of ELM energy dissipation into 3 \"stages\" ranging from no dissipation to full dissipation (the target plasma is detached).","In the second publication related to this study, spectroscopic data is analysed with a Bayesian approach, to acquire insights into the significance of molecular processes in dissipating the plasma energy and particles."],"url":"http://arxiv.org/abs/2404.19464v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 11:18:18","title":"Adaptive Gaussian Process Regression for Bayesian inverse problems","abstract":"We introduce a novel adaptive Gaussian Process Regression (GPR) methodology for efficient construction of surrogate models for Bayesian inverse problems with expensive forward model evaluations. An adaptive design strategy focuses on optimizing both the positioning and simulation accuracy of training data in order to reduce the computational cost of simulating training data without compromising the fidelity of the posterior distributions of parameters. The method interleaves a goal-oriented active learning algorithm selecting evaluation points and tolerances based on the expected impact on the Kullback-Leibler divergence of surrogated and true posterior with a Markov Chain Monte Carlo sampling of the posterior. The performance benefit of the adaptive approach is demonstrated for two simple test problems.","sentences":["We introduce a novel adaptive Gaussian Process Regression (GPR) methodology for efficient construction of surrogate models for Bayesian inverse problems with expensive forward model evaluations.","An adaptive design strategy focuses on optimizing both the positioning and simulation accuracy of training data in order to reduce the computational cost of simulating training data without compromising the fidelity of the posterior distributions of parameters.","The method interleaves a goal-oriented active learning algorithm selecting evaluation points and tolerances based on the expected impact on the Kullback-Leibler divergence of surrogated and true posterior with a Markov Chain Monte Carlo sampling of the posterior.","The performance benefit of the adaptive approach is demonstrated for two simple test problems."],"url":"http://arxiv.org/abs/2404.19459v1","category":"math.NA"}
{"created":"2024-04-30 10:48:43","title":"AnomalyXFusion: Multi-modal Anomaly Synthesis with Diffusion","abstract":"Anomaly synthesis is one of the effective methods to augment abnormal samples for training. However, current anomaly synthesis methods predominantly rely on texture information as input, which limits the fidelity of synthesized abnormal samples. Because texture information is insufficient to correctly depict the pattern of anomalies, especially for logical anomalies. To surmount this obstacle, we present the AnomalyXFusion framework, designed to harness multi-modality information to enhance the quality of synthesized abnormal samples. The AnomalyXFusion framework comprises two distinct yet synergistic modules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion (DDF) module. The MIF module refines modality alignment by aggregating and integrating various modality features into a unified embedding space, termed X-embedding, which includes image, text, and mask features. Concurrently, the DDF module facilitates controlled generation through an adaptive adjustment of X-embedding conditioned on the diffusion steps. In addition, to reveal the multi-modality representational power of AnomalyXFusion, we propose a new dataset, called MVTec Caption. More precisely, MVTec Caption extends 2.2k accurate image-mask-text annotations for the MVTec AD and LOCO datasets. Comprehensive evaluations demonstrate the effectiveness of AnomalyXFusion, especially regarding the fidelity and diversity for logical anomalies. Project page: http:github.com/hujiecpp/MVTec-Caption","sentences":["Anomaly synthesis is one of the effective methods to augment abnormal samples for training.","However, current anomaly synthesis methods predominantly rely on texture information as input, which limits the fidelity of synthesized abnormal samples.","Because texture information is insufficient to correctly depict the pattern of anomalies, especially for logical anomalies.","To surmount this obstacle, we present the AnomalyXFusion framework, designed to harness multi-modality information to enhance the quality of synthesized abnormal samples.","The AnomalyXFusion framework comprises two distinct yet synergistic modules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion (DDF) module.","The MIF module refines modality alignment by aggregating and integrating various modality features into a unified embedding space, termed X-embedding, which includes image, text, and mask features.","Concurrently, the DDF module facilitates controlled generation through an adaptive adjustment of X-embedding conditioned on the diffusion steps.","In addition, to reveal the multi-modality representational power of AnomalyXFusion, we propose a new dataset, called MVTec Caption.","More precisely, MVTec Caption extends 2.2k accurate image-mask-text annotations for the MVTec AD and LOCO datasets.","Comprehensive evaluations demonstrate the effectiveness of AnomalyXFusion, especially regarding the fidelity and diversity for logical anomalies.","Project page: http:github.com/hujiecpp/MVTec-Caption"],"url":"http://arxiv.org/abs/2404.19444v1","category":"cs.CV"}
{"created":"2024-04-30 10:15:57","title":"Heat capacity of periodically driven two-level systems","abstract":"We define the heat capacity for steady periodically driven systems and as an example we compute it for dissipative two-level systems where the energy gap is time-modulated. There, as a function of ambient temperature, the Schottky peak remains the dominant feature. Yet, in contrast with equilibrium, the quasistatic thermal response of a nonequilibrium system also reveals kinetic information present in the transition rates; e.g., the heat capacity depends on the time-symmetric reactivities and changes by the presence of a kinetic barrier. It still vanishes though at absolute zero, in accord with an extended Nernst heat postulate, but at a different rate from the equilibrium case. More generally, we discuss the dependence on driving frequency and amplitude.","sentences":["We define the heat capacity for steady periodically driven systems and as an example we compute it for dissipative two-level systems where the energy gap is time-modulated.","There, as a function of ambient temperature, the Schottky peak remains the dominant feature.","Yet, in contrast with equilibrium, the quasistatic thermal response of a nonequilibrium system also reveals kinetic information present in the transition rates; e.g., the heat capacity depends on the time-symmetric reactivities and changes by the presence of a kinetic barrier.","It still vanishes though at absolute zero, in accord with an extended Nernst heat postulate, but at a different rate from the equilibrium case.","More generally, we discuss the dependence on driving frequency and amplitude."],"url":"http://arxiv.org/abs/2404.19426v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 10:11:03","title":"Active Dendrites Enable Efficient Continual Learning in Time-To-First-Spike Neural Networks","abstract":"While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks. This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner. Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally. By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites. Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset. Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices. Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy.","sentences":["While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks.","This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner.","Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally.","By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites.","Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset.","Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices.","Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy."],"url":"http://arxiv.org/abs/2404.19419v1","category":"cs.NE"}
{"created":"2024-04-30 10:00:39","title":"Enhancing Robotic Adaptability: Integrating Unsupervised Trajectory Segmentation and Conditional ProMPs for Dynamic Learning Environments","abstract":"We propose a novel framework for enhancing robotic adaptability and learning efficiency, which integrates unsupervised trajectory segmentation with adaptive probabilistic movement primitives (ProMPs). By employing a cutting-edge deep learning architecture that combines autoencoders and Recurrent Neural Networks (RNNs), our approach autonomously pinpoints critical transitional points in continuous, unlabeled motion data, thus significantly reducing dependence on extensively labeled datasets. This innovative method dynamically adjusts motion trajectories using conditional variables, significantly enhancing the flexibility and accuracy of robotic actions under dynamic conditions while also reducing the computational overhead associated with traditional robotic programming methods. Our experimental validation demonstrates superior learning efficiency and adaptability compared to existing techniques, paving the way for advanced applications in industrial and service robotics.","sentences":["We propose a novel framework for enhancing robotic adaptability and learning efficiency, which integrates unsupervised trajectory segmentation with adaptive probabilistic movement primitives (ProMPs).","By employing a cutting-edge deep learning architecture that combines autoencoders and Recurrent Neural Networks (RNNs), our approach autonomously pinpoints critical transitional points in continuous, unlabeled motion data, thus significantly reducing dependence on extensively labeled datasets.","This innovative method dynamically adjusts motion trajectories using conditional variables, significantly enhancing the flexibility and accuracy of robotic actions under dynamic conditions while also reducing the computational overhead associated with traditional robotic programming methods.","Our experimental validation demonstrates superior learning efficiency and adaptability compared to existing techniques, paving the way for advanced applications in industrial and service robotics."],"url":"http://arxiv.org/abs/2404.19412v1","category":"cs.RO"}
{"created":"2024-04-30 08:55:01","title":"Evaluating Telugu Proficiency in Large Language Models_ A Comparative Analysis of ChatGPT and Gemini","abstract":"The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English. This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs. Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu. The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning. By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction. Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations. This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities.","sentences":["The growing prominence of large language models (LLMs) necessitates the exploration of their capabilities beyond English.","This research investigates the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.","Through a designed set of 20 questions encompassing greetings, grammar, vocabulary, common phrases, task completion, and situational reasoning, the study delves into their strengths and weaknesses in handling Telugu.","The analysis aims to identify the LLM that demonstrates a deeper understanding of Telugu grammatical structures, possesses a broader vocabulary, and exhibits superior performance in tasks like writing and reasoning.","By comparing their ability to comprehend and use everyday Telugu expressions, the research sheds light on their suitability for real-world language interaction.","Furthermore, the evaluation of adaptability and reasoning capabilities provides insights into how each LLM leverages Telugu to respond to dynamic situations.","This comparative analysis contributes to the ongoing discussion on multilingual capabilities in AI and paves the way for future research in developing LLMs that can seamlessly integrate with Telugu-speaking communities."],"url":"http://arxiv.org/abs/2404.19369v1","category":"cs.CL"}
{"created":"2024-04-30 08:33:52","title":"PEFSL: A deployment Pipeline for Embedded Few-Shot Learning on a FPGA SoC","abstract":"This paper tackles the challenges of implementing few-shot learning on embedded systems, specifically FPGA SoCs, a vital approach for adapting to diverse classification tasks, especially when the costs of data acquisition or labeling prove to be prohibitively high. Our contributions encompass the development of an end-to-end open-source pipeline for a few-shot learning platform for object classification on a FPGA SoCs. The pipeline is built on top of the Tensil open-source framework, facilitating the design, training, evaluation, and deployment of DNN backbones tailored for few-shot learning. Additionally, we showcase our work's potential by building and deploying a low-power, low-latency demonstrator trained on the MiniImageNet dataset with a dataflow architecture. The proposed system has a latency of 30 ms while consuming 6.2 W on the PYNQ-Z1 board.","sentences":["This paper tackles the challenges of implementing few-shot learning on embedded systems, specifically FPGA SoCs, a vital approach for adapting to diverse classification tasks, especially when the costs of data acquisition or labeling prove to be prohibitively high.","Our contributions encompass the development of an end-to-end open-source pipeline for a few-shot learning platform for object classification on a FPGA SoCs.","The pipeline is built on top of the Tensil open-source framework, facilitating the design, training, evaluation, and deployment of DNN backbones tailored for few-shot learning.","Additionally, we showcase our work's potential by building and deploying a low-power, low-latency demonstrator trained on the MiniImageNet dataset with a dataflow architecture.","The proposed system has a latency of 30 ms while consuming 6.2 W on the PYNQ-Z1 board."],"url":"http://arxiv.org/abs/2404.19354v1","category":"cs.AR"}
{"created":"2024-04-30 08:09:24","title":"Data-adaptive structural change-point detection via isolation","abstract":"In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals. The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors.","sentences":["In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence.","The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals.","We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals.","The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability.","Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval.","The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy.","The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors."],"url":"http://arxiv.org/abs/2404.19344v1","category":"stat.ME"}
{"created":"2024-04-30 07:52:36","title":"End-to-end information extraction in handwritten documents: Understanding Paris marriage records from 1880 to 1940","abstract":"The EXO-POPP project aims to establish a comprehensive database comprising 300,000 marriage records from Paris and its suburbs, spanning the years 1880 to 1940, which are preserved in over 130,000 scans of double pages. Each marriage record may encompass up to 118 distinct types of information that require extraction from plain text. In this paper, we introduce the M-POPP dataset, a subset of the M-POPP database with annotations for full-page text recognition and information extraction in both handwritten and printed documents, and which is now publicly available. We present a fully end-to-end architecture adapted from the DAN, designed to perform both handwritten text recognition and information extraction directly from page images without the need for explicit segmentation. We showcase the information extraction capabilities of this architecture by achieving a new state of the art for full-page Information Extraction on Esposalles and we use this architecture as a baseline for the M-POPP dataset. We also assess and compare how different encoding strategies for named entities in the text affect the performance of jointly recognizing handwritten text and extracting information, from full pages.","sentences":["The EXO-POPP project aims to establish a comprehensive database comprising 300,000 marriage records from Paris and its suburbs, spanning the years 1880 to 1940, which are preserved in over 130,000 scans of double pages.","Each marriage record may encompass up to 118 distinct types of information that require extraction from plain text.","In this paper, we introduce the M-POPP dataset, a subset of the M-POPP database with annotations for full-page text recognition and information extraction in both handwritten and printed documents, and which is now publicly available.","We present a fully end-to-end architecture adapted from the DAN, designed to perform both handwritten text recognition and information extraction directly from page images without the need for explicit segmentation.","We showcase the information extraction capabilities of this architecture by achieving a new state of the art for full-page Information Extraction on Esposalles and we use this architecture as a baseline for the M-POPP dataset.","We also assess and compare how different encoding strategies for named entities in the text affect the performance of jointly recognizing handwritten text and extracting information, from full pages."],"url":"http://arxiv.org/abs/2404.19329v1","category":"cs.CV"}
{"created":"2024-04-30 07:34:42","title":"QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering","abstract":"Extractive Question Answering (EQA) in Machine Reading Comprehension (MRC) often faces the challenge of dealing with semantically identical but format-variant inputs. Our work introduces a novel approach, called the ``Query Latent Semantic Calibrator (QLSC)'', designed as an auxiliary module for existing MRC models. We propose a unique scaling strategy to capture latent semantic center features of queries. These features are then seamlessly integrated into traditional query and passage embeddings using an attention mechanism. By deepening the comprehension of the semantic queries-passage relationship, our approach diminishes sensitivity to variations in text format and boosts the model's capability in pinpointing accurate answers. Experimental results on robust Question-Answer datasets confirm that our approach effectively handles format-variant but semantically identical queries, highlighting the effectiveness and adaptability of our proposed method.","sentences":["Extractive Question Answering (EQA) in Machine Reading Comprehension (MRC) often faces the challenge of dealing with semantically identical but format-variant inputs.","Our work introduces a novel approach, called the ``Query Latent Semantic Calibrator (QLSC)'', designed as an auxiliary module for existing MRC models.","We propose a unique scaling strategy to capture latent semantic center features of queries.","These features are then seamlessly integrated into traditional query and passage embeddings using an attention mechanism.","By deepening the comprehension of the semantic queries-passage relationship, our approach diminishes sensitivity to variations in text format and boosts the model's capability in pinpointing accurate answers.","Experimental results on robust Question-Answer datasets confirm that our approach effectively handles format-variant but semantically identical queries, highlighting the effectiveness and adaptability of our proposed method."],"url":"http://arxiv.org/abs/2404.19316v1","category":"cs.CL"}
{"created":"2024-04-30 06:51:30","title":"Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement","abstract":"The main function of depth completion is to compensate for an insufficient and unpredictable number of sparse depth measurements of hardware sensors. However, existing research on depth completion assumes that the sparsity -- the number of points or LiDAR lines -- is fixed for training and testing. Hence, the completion performance drops severely when the number of sparse depths changes significantly. To address this issue, we propose the sparsity-adaptive depth refinement (SDR) framework, which refines monocular depth estimates using sparse depth points. For SDR, we propose the masked spatial propagation network (MSPN) to perform SDR with a varying number of sparse depths effectively by gradually propagating sparse depth information throughout the entire depth map. Experimental results demonstrate that MPSN achieves state-of-the-art performance on both SDR and conventional depth completion scenarios.","sentences":["The main function of depth completion is to compensate for an insufficient and unpredictable number of sparse depth measurements of hardware sensors.","However, existing research on depth completion assumes that the sparsity -- the number of points or LiDAR lines -- is fixed for training and testing.","Hence, the completion performance drops severely when the number of sparse depths changes significantly.","To address this issue, we propose the sparsity-adaptive depth refinement (SDR) framework, which refines monocular depth estimates using sparse depth points.","For SDR, we propose the masked spatial propagation network (MSPN) to perform SDR with a varying number of sparse depths effectively by gradually propagating sparse depth information throughout the entire depth map.","Experimental results demonstrate that MPSN achieves state-of-the-art performance on both SDR and conventional depth completion scenarios."],"url":"http://arxiv.org/abs/2404.19294v1","category":"cs.CV"}
{"created":"2024-04-30 06:34:21","title":"Revisiting the Adversarial Robustness of Vision Language Models: a Multimodal Perspective","abstract":"Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks. While prior research has primarily concentrated on improving the adversarial robustness of image encoders to guard against attacks on images, the exploration of text-based and multimodal attacks has largely been overlooked. In this work, we initiate the first known and comprehensive effort to study adapting vision-language models for adversarial robustness under the multimodal attack. Firstly, we introduce a multimodal attack strategy and investigate the impact of different attacks. We then propose a multimodal contrastive adversarial training loss, aligning the clean and adversarial text embeddings with the adversarial and clean visual features, to enhance the adversarial robustness of both image and text encoders of CLIP. Extensive experiments on 15 datasets across two tasks demonstrate that our method significantly improves the adversarial robustness of CLIP. Interestingly, we find that the model fine-tuned against multimodal adversarial attacks exhibits greater robustness than its counterpart fine-tuned solely against image-based attacks, even in the context of image attacks, which may open up new possibilities for enhancing the security of VLMs.","sentences":["Pretrained vision-language models (VLMs) like CLIP have shown impressive generalization performance across various downstream tasks, yet they remain vulnerable to adversarial attacks.","While prior research has primarily concentrated on improving the adversarial robustness of image encoders to guard against attacks on images, the exploration of text-based and multimodal attacks has largely been overlooked.","In this work, we initiate the first known and comprehensive effort to study adapting vision-language models for adversarial robustness under the multimodal attack.","Firstly, we introduce a multimodal attack strategy and investigate the impact of different attacks.","We then propose a multimodal contrastive adversarial training loss, aligning the clean and adversarial text embeddings with the adversarial and clean visual features, to enhance the adversarial robustness of both image and text encoders of CLIP.","Extensive experiments on 15 datasets across two tasks demonstrate that our method significantly improves the adversarial robustness of CLIP.","Interestingly, we find that the model fine-tuned against multimodal adversarial attacks exhibits greater robustness than its counterpart fine-tuned solely against image-based attacks, even in the context of image attacks, which may open up new possibilities for enhancing the security of VLMs."],"url":"http://arxiv.org/abs/2404.19287v1","category":"cs.CV"}
{"created":"2024-04-30 06:33:07","title":"Soft Prompt Generation for Domain Generalization","abstract":"Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt, which are not optimal for specific domains. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which acts as a learning vector that undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt and residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains, potentially compromising the transferability of the prompts. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely \\textbf{S}oft \\textbf{P}rompt \\textbf{G}eneration (SPG). To the best of our knowledge, we are the first to introduce the generative model into prompt learning in VLMs and explore its potential for producing soft prompts by relying solely on the generative model, ensuring the diversity of prompts. Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt labels for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that our proposed SPG achieves state-of-the-art performance. The code will be available soon.","sentences":["Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt, which are not optimal for specific domains.","To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which acts as a learning vector that undergoes fine-tuning based on specific domain data.","Prior prompt learning methods primarily learn a fixed prompt and residuled prompt from training samples.","However, the learned prompts lack diversity and ignore information about unseen domains, potentially compromising the transferability of the prompts.","In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely \\textbf{S}oft \\textbf{P}rompt \\textbf{G}eneration (SPG).","To the best of our knowledge, we are the first to introduce the generative model into prompt learning in VLMs and explore its potential for producing soft prompts by relying solely on the generative model, ensuring the diversity of prompts.","Specifically, SPG consists of a two-stage training phase and an inference phase.","During the training phase, we introduce soft prompt labels for each domain, aiming to incorporate the generative model domain knowledge.","During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain.","Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that our proposed SPG achieves state-of-the-art performance.","The code will be available soon."],"url":"http://arxiv.org/abs/2404.19286v1","category":"cs.CV"}
{"created":"2024-04-30 06:14:51","title":"Dual Dynamic Threshold Adjustment Strategy for Deep Metric Learning","abstract":"Loss functions and sample mining strategies are essential components in deep metric learning algorithms. However, the existing loss function or mining strategy often necessitate the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative. The threshold provides a stable numerical standard for determining whether to retain the pairs. It is a vital parameter to reduce the redundant sample pairs participating in training. Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches. Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold. Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy. We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples. AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs. This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds. We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS). Experimental results show that our algorithm achieves competitive performance on CUB200, Cars196, and SOP datasets.","sentences":["Loss functions and sample mining strategies are essential components in deep metric learning algorithms.","However, the existing loss function or mining strategy often necessitate the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative.","The threshold provides a stable numerical standard for determining whether to retain the pairs.","It is a vital parameter to reduce the redundant sample pairs participating in training.","Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches.","Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold.","Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy.","We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods.","ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples.","AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs.","This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds.","We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS).","Experimental results show that our algorithm achieves competitive performance on CUB200, Cars196, and SOP datasets."],"url":"http://arxiv.org/abs/2404.19282v1","category":"cs.MM"}
{"created":"2024-04-30 05:43:30","title":"AdapTics: A Toolkit for Creative Design and Integration of Real-Time Adaptive Mid-Air Ultrasound Tactons","abstract":"Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback. Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime. These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable. This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons-dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs. A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications. We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers.","sentences":["Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback.","Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime.","These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable.","This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons-dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs.","A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications.","We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers."],"url":"http://arxiv.org/abs/2404.19275v1","category":"cs.HC"}
{"created":"2024-04-30 04:12:59","title":"A Nonnested Augmented Subspace Method for Kohn-Sham Equation","abstract":"In this paper, a novel adaptive finite element method is proposed to solve the Kohn-Sham equation based on the moving mesh (nonnested mesh) adaptive technique and the augmented subspace method. Different from the classical self-consistent field iterative algorithm which requires to solve the Kohn-Sham equation directly in each adaptive finite element space, our algorithm transforms the Kohn-Sham equation into some linear boundary value problems of the same scale in each adaptive finite element space, and then the wavefunctions derived from the linear boundary value problems are corrected by solving a small-scale Kohn-Sham equation defined in a low-dimensional augmented subspace. Since the new algorithm avoids solving large-scale Kohn-Sham equation directly, a significant improvement for the solving efficiency can be obtained. In addition, the adaptive moving mesh technique is used to generate the nonnested adaptive mesh for the nonnested augmented subspace method according to the singularity of the approximate wavefunctions. The modified Hessian matrix of the approximate wavefunctions is used as the metric matrix to redistribute the mesh. Through the moving mesh adaptive technique, the redistributed mesh is almost optimal. A number of numerical experiments are carried out to verify the efficiency and the accuracy of the proposed algorithm.","sentences":["In this paper, a novel adaptive finite element method is proposed to solve the Kohn-Sham equation based on the moving mesh (nonnested mesh) adaptive technique and the augmented subspace method.","Different from the classical self-consistent field iterative algorithm which requires to solve the Kohn-Sham equation directly in each adaptive finite element space, our algorithm transforms the Kohn-Sham equation into some linear boundary value problems of the same scale in each adaptive finite element space, and then the wavefunctions derived from the linear boundary value problems are corrected by solving a small-scale Kohn-Sham equation defined in a low-dimensional augmented subspace.","Since the new algorithm avoids solving large-scale Kohn-Sham equation directly, a significant improvement for the solving efficiency can be obtained.","In addition, the adaptive moving mesh technique is used to generate the nonnested adaptive mesh for the nonnested augmented subspace method according to the singularity of the approximate wavefunctions.","The modified Hessian matrix of the approximate wavefunctions is used as the metric matrix to redistribute the mesh.","Through the moving mesh adaptive technique, the redistributed mesh is almost optimal.","A number of numerical experiments are carried out to verify the efficiency and the accuracy of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.19249v1","category":"math.NA"}
{"created":"2024-04-30 04:12:36","title":"Transition Rate Scheduling for Quantization-Aware Training","abstract":"Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations. It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers. We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT. Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states. This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions. It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually. We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels. Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly. Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT. Experimental results demonstrate the effectiveness of our approach on standard benchmarks.","sentences":["Quantization-aware training (QAT) simulates a quantization process during training to lower bit-precision of weights/activations.","It learns quantized weights indirectly by updating latent weights, i.e., full-precision inputs to a quantizer, using gradient-based optimizers.","We claim that coupling a user-defined learning rate (LR) with these optimizers is sub-optimal for QAT.","Quantized weights transit discrete levels of a quantizer, only if corresponding latent weights pass transition points, where the quantizer changes discrete states.","This suggests that the changes of quantized weights are affected by both the LR for latent weights and their distributions.","It is thus difficult to control the degree of changes for quantized weights by scheduling the LR manually.","We conjecture that the degree of parameter changes in QAT is related to the number of quantized weights transiting discrete levels.","Based on this, we introduce a transition rate (TR) scheduling technique that controls the number of transitions of quantized weights explicitly.","Instead of scheduling a LR for latent weights, we schedule a target TR of quantized weights, and update the latent weights with a novel transition-adaptive LR (TALR), enabling considering the degree of changes for the quantized weights during QAT.","Experimental results demonstrate the effectiveness of our approach on standard benchmarks."],"url":"http://arxiv.org/abs/2404.19248v1","category":"cs.CV"}
{"created":"2024-04-30 02:51:29","title":"Temporal Logic Resilience for Dynamical Systems","abstract":"We consider the notion of resilience for cyber-physical systems, that is, the ability of the system to withstand adverse events while maintaining acceptable functionality. We use finite temporal logic to express the requirements on the acceptable functionality and define the resilience metric as the maximum disturbance under which the system satisfies the temporal requirements. We fix a parameterized template for the set of disturbances and form a robust optimization problem under the system dynamics and the temporal specifications to find the maximum value of the parameter. Additionally, we introduce two novel classes of specifications: closed and convex finite temporal logics specifications, offering a comprehensive analysis of the resilience metric within these specific frameworks. From a computational standpoint, we present an exact solution for linear systems and exact-time reachability and finite-horizon safety, complemented by an approximate solution for finite-horizon reachability. Extending our findings to nonlinear systems, we leverage linear approximations and SMT-based approaches to offer viable computational methodologies. The theoretical results are demonstrated on the temperature regulation of buildings, adaptive cruise control and DC motors.","sentences":["We consider the notion of resilience for cyber-physical systems, that is, the ability of the system to withstand adverse events while maintaining acceptable functionality.","We use finite temporal logic to express the requirements on the acceptable functionality and define the resilience metric as the maximum disturbance under which the system satisfies the temporal requirements.","We fix a parameterized template for the set of disturbances and form a robust optimization problem under the system dynamics and the temporal specifications to find the maximum value of the parameter.","Additionally, we introduce two novel classes of specifications: closed and convex finite temporal logics specifications, offering a comprehensive analysis of the resilience metric within these specific frameworks.","From a computational standpoint, we present an exact solution for linear systems and exact-time reachability and finite-horizon safety, complemented by an approximate solution for finite-horizon reachability.","Extending our findings to nonlinear systems, we leverage linear approximations and SMT-based approaches to offer viable computational methodologies.","The theoretical results are demonstrated on the temperature regulation of buildings, adaptive cruise control and DC motors."],"url":"http://arxiv.org/abs/2404.19223v1","category":"eess.SY"}
{"created":"2024-04-30 01:35:27","title":"Assessing the safety benefits of CACC+ based coordination of connected and autonomous vehicle platoons in emergency braking scenarios","abstract":"Ensuring safety is the most important factor in connected and autonomous vehicles, especially in emergency braking situations. As such, assessing the safety benefits of one information topology over other is a necessary step towards evaluating and ensuring safety. In this paper, we compare the safety benefits of a cooperative adaptive cruise control which utilizes information from one predecessor vehicle (CACC) with the one that utilizes information from multiple predecessors (CACC+) for the maintenance of spacing under an emergency braking scenario. A constant time headway policy is employed for maintenance of spacing (that includes a desired standstill spacing distance and a velocity dependent spacing distance) between the vehicles in the platoon. The considered emergency braking scenario consists of braking of the leader vehicle of the platoon at its maximum deceleration and that of the following vehicles to maintain the spacing as per CACC or CACC+. By focusing on the standstill spacing distance and utilizing Monte Carlo simulations, we assess the safety benefits of CACC+ over CACC by utilizing the following safety metrics: (1) probability of collision, (2) expected number of collisions, and (3) severity of collision (defined as the relative velocity of the two vehicles at impact). We present and provide discussion of these results.","sentences":["Ensuring safety is the most important factor in connected and autonomous vehicles, especially in emergency braking situations.","As such, assessing the safety benefits of one information topology over other is a necessary step towards evaluating and ensuring safety.","In this paper, we compare the safety benefits of a cooperative adaptive cruise control which utilizes information from one predecessor vehicle (CACC) with the one that utilizes information from multiple predecessors (CACC+) for the maintenance of spacing under an emergency braking scenario.","A constant time headway policy is employed for maintenance of spacing (that includes a desired standstill spacing distance and a velocity dependent spacing distance) between the vehicles in the platoon.","The considered emergency braking scenario consists of braking of the leader vehicle of the platoon at its maximum deceleration and that of the following vehicles to maintain the spacing as per CACC or CACC+.","By focusing on the standstill spacing distance and utilizing Monte Carlo simulations, we assess the safety benefits of CACC+ over CACC by utilizing the following safety metrics: (1) probability of collision, (2) expected number of collisions, and (3) severity of collision (defined as the relative velocity of the two vehicles at impact).","We present and provide discussion of these results."],"url":"http://arxiv.org/abs/2404.19189v1","category":"eess.SY"}
{"created":"2024-04-30 01:35:14","title":"Maximum bound principle and original energy dissipation of arbitrarily high-order rescaled exponential time differencing Runge-Kutta schemes for Allen--Cahn equations","abstract":"The energy dissipation law and the maximum bound principle are two critical physical properties of the Allen--Cahn equations. While many existing time-stepping methods are known to preserve the energy dissipation law, most apply to a modified form of energy. In this work, we demonstrate that, when the nonlinear term of the Allen--Cahn equation is Lipschitz continuous, a class of arbitrarily high-order exponential time differencing Runge--Kutta (ETDRK) schemes preserve the original energy dissipation property, under a mild step-size constraint. Additionally, we guarantee the Lipschitz condition on the nonlinear term by applying a rescaling post-processing technique, which ensures that the numerical solution unconditionally satisfies the maximum bound principle. Consequently, our proposed schemes maintain both the original energy dissipation law and the maximum bound principle and can achieve arbitrarily high-order accuracy. We also establish an optimal error estimate for the proposed schemes. Some numerical experiments are carried out to verify our theoretical results.","sentences":["The energy dissipation law and the maximum bound principle are two critical physical properties of the Allen--Cahn equations.","While many existing time-stepping methods are known to preserve the energy dissipation law, most apply to a modified form of energy.","In this work, we demonstrate that, when the nonlinear term of the Allen--Cahn equation is Lipschitz continuous, a class of arbitrarily high-order exponential time differencing Runge--Kutta (ETDRK) schemes preserve the original energy dissipation property, under a mild step-size constraint.","Additionally, we guarantee the Lipschitz condition on the nonlinear term by applying a rescaling post-processing technique, which ensures that the numerical solution unconditionally satisfies the maximum bound principle.","Consequently, our proposed schemes maintain both the original energy dissipation law and the maximum bound principle and can achieve arbitrarily high-order accuracy.","We also establish an optimal error estimate for the proposed schemes.","Some numerical experiments are carried out to verify our theoretical results."],"url":"http://arxiv.org/abs/2404.19188v1","category":"math.NA"}
{"created":"2024-04-30 01:16:53","title":"MACO: Exploring GEMM Acceleration on a Loosely-Coupled Multi-core Processor","abstract":"General-purpose processor vendors have integrated customized accelerator in their products due to the widespread use of General Matrix-Matrix Multiplication (GEMM) kernels. However, it remains a challenge to further improve the flexibilityand scalability of these GEMM-enhanced processors to cater to the emerging large-scale GEMM workloads. In this paper we propose MACO, a novel loosely-coupled multi-core general-purpose architecture optimized for GEMM-related applications. To enhance the programmability and flexibility of MACO, the paper introduces a tile-based instruction set architecture. Additionally, the paper presents techniques such as hardware-assisted data prefetching and locking, and predictive address translation to further enhance the computational efficiency of MACO for GEMM workloads. The experimental results demonstrate that MACO exhibits good scalability, achieving an average computational efficiency of 90% across multiple cores. Furthermore, evaluations on state-of-the-art deep neural networks show that MACO can achieve up to 1.1 TFLOPS with 88% computational efficiency, indicating its adaptivity to deep learning workloads.","sentences":["General-purpose processor vendors have integrated customized accelerator in their products due to the widespread use of General Matrix-Matrix Multiplication (GEMM) kernels.","However, it remains a challenge to further improve the flexibilityand scalability of these GEMM-enhanced processors to cater to the emerging large-scale GEMM workloads.","In this paper we propose MACO, a novel loosely-coupled multi-core general-purpose architecture optimized for GEMM-related applications.","To enhance the programmability and flexibility of MACO, the paper introduces a tile-based instruction set architecture.","Additionally, the paper presents techniques such as hardware-assisted data prefetching and locking, and predictive address translation to further enhance the computational efficiency of MACO for GEMM workloads.","The experimental results demonstrate that MACO exhibits good scalability, achieving an average computational efficiency of 90% across multiple cores.","Furthermore, evaluations on state-of-the-art deep neural networks show that MACO can achieve up to 1.1 TFLOPS with 88% computational efficiency, indicating its adaptivity to deep learning workloads."],"url":"http://arxiv.org/abs/2404.19180v1","category":"cs.AR"}
{"created":"2024-04-30 01:13:24","title":"Align-Free Multi-Plane Phase Retrieval","abstract":"The multi-plane phase retrieval method provides a budget-friendly and effective way to perform phase imaging, yet it often encounters alignment challenges due to shifts along the optical axis in experiments. Traditional methods, such as employing beamsplitters instead of mechanical stage movements or adjusting focus using tunable light sources, add complexity to the setup required for multi-plane phase retrieval. Attempts to address these issues computationally face difficulties due to the variable impact of diffraction, which renders conventional homography techniques inadequate. In our research, we introduce a novel Adaptive Cascade Calibrated (ACC) strategy for multi-plane phase retrieval that overcomes misalignment issues. This technique detects feature points within the refocused sample space and calculates the transformation matrix for neighboring planes on-the-fly to digitally adjust measurements, facilitating alignment-free multi-plane phase retrieval. This approach not only avoids the need for complex and expensive optical hardware but also simplifies the imaging setup, reducing overall costs. The effectiveness of our method is validated through simulations and real-world optical experiments.","sentences":["The multi-plane phase retrieval method provides a budget-friendly and effective way to perform phase imaging, yet it often encounters alignment challenges due to shifts along the optical axis in experiments.","Traditional methods, such as employing beamsplitters instead of mechanical stage movements or adjusting focus using tunable light sources, add complexity to the setup required for multi-plane phase retrieval.","Attempts to address these issues computationally face difficulties due to the variable impact of diffraction, which renders conventional homography techniques inadequate.","In our research, we introduce a novel Adaptive Cascade Calibrated (ACC) strategy for multi-plane phase retrieval that overcomes misalignment issues.","This technique detects feature points within the refocused sample space and calculates the transformation matrix for neighboring planes on-the-fly to digitally adjust measurements, facilitating alignment-free multi-plane phase retrieval.","This approach not only avoids the need for complex and expensive optical hardware but also simplifies the imaging setup, reducing overall costs.","The effectiveness of our method is validated through simulations and real-world optical experiments."],"url":"http://arxiv.org/abs/2404.18946v1","category":"physics.optics"}
{"created":"2024-04-29 23:01:03","title":"Workload Intelligence: Punching Holes Through the Cloud Abstraction","abstract":"Today, cloud workloads are essentially opaque to the cloud platform. Typically, the only information the platform receives is the virtual machine (VM) type and possibly a decoration to the type (e.g., the VM is evictable). Similarly, workloads receive little to no information from the platform; generally, workloads might receive telemetry from their VMs or exceptional signals (e.g., shortly before a VM is evicted). The narrow interface between workloads and platforms has several drawbacks: (1) a surge in VM types and decorations in public cloud platforms complicates customer selection; (2) essential workload characteristics (e.g., low availability requirements, high latency tolerance) are often unspecified, hindering platform customization for optimized resource usage and cost savings; and (3) workloads may be unaware of potential optimizations or lack sufficient time to react to platform events.   In this paper, we propose a framework, called Workload Intelligence (WI), for dynamic bi-directional communication between cloud workloads and cloud platform. Via WI, workloads can programmatically adjust their key characteristics, requirements, and even dynamically adapt behaviors like VM priorities. In the other direction, WI allows the platform to programmatically inform workloads about upcoming events, opportunities for optimization, among other scenarios. Because of WI, the cloud platform can drastically simplify its offerings, reduce its costs without fear of violating any workload requirements, and reduce prices to its customers on average by 48.8%.","sentences":["Today, cloud workloads are essentially opaque to the cloud platform.","Typically, the only information the platform receives is the virtual machine (VM) type and possibly a decoration to the type (e.g., the VM is evictable).","Similarly, workloads receive little to no information from the platform; generally, workloads might receive telemetry from their VMs or exceptional signals (e.g., shortly before a VM is evicted).","The narrow interface between workloads and platforms has several drawbacks: (1) a surge in VM types and decorations in public cloud platforms complicates customer selection; (2) essential workload characteristics (e.g., low availability requirements, high latency tolerance) are often unspecified, hindering platform customization for optimized resource usage and cost savings; and (3) workloads may be unaware of potential optimizations or lack sufficient time to react to platform events.   ","In this paper, we propose a framework, called Workload Intelligence (WI), for dynamic bi-directional communication between cloud workloads and cloud platform.","Via WI, workloads can programmatically adjust their key characteristics, requirements, and even dynamically adapt behaviors like VM priorities.","In the other direction, WI allows the platform to programmatically inform workloads about upcoming events, opportunities for optimization, among other scenarios.","Because of WI, the cloud platform can drastically simplify its offerings, reduce its costs without fear of violating any workload requirements, and reduce prices to its customers on average by 48.8%."],"url":"http://arxiv.org/abs/2404.19143v1","category":"cs.DC"}
{"created":"2024-04-29 21:25:59","title":"Source-Free Domain Adaptation of Weakly-Supervised Object Localization Models for Histology","abstract":"Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images. Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations. A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type. In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons. SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks. In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy. They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment. Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification.","sentences":["Given the emergence of deep learning, digital pathology has gained popularity for cancer diagnosis based on histology images.","Deep weakly supervised object localization (WSOL) models can be trained to classify histology images according to cancer grade and identify regions of interest (ROIs) for interpretation, using inexpensive global image-class annotations.","A WSOL model initially trained on some labeled source image data can be adapted using unlabeled target data in cases of significant domain shifts caused by variations in staining, scanners, and cancer type.","In this paper, we focus on source-free (unsupervised) domain adaptation (SFDA), a challenging problem where a pre-trained source model is adapted to a new target domain without using any source domain data for privacy and efficiency reasons.","SFDA of WSOL models raises several challenges in histology, most notably because they are not intended to adapt for both classification and localization tasks.","In this paper, 4 state-of-the-art SFDA methods, each one representative of a main SFDA family, are compared for WSOL in terms of classification and localization accuracy.","They are the SFDA-Distribution Estimation, Source HypOthesis Transfer, Cross-Domain Contrastive Learning, and Adaptively Domain Statistics Alignment.","Experimental results on the challenging Glas (smaller, breast cancer) and Camelyon16 (larger, colon cancer) histology datasets indicate that these SFDA methods typically perform poorly for localization after adaptation when optimized for classification."],"url":"http://arxiv.org/abs/2404.19113v1","category":"cs.CV"}
{"created":"2024-04-29 20:22:33","title":"Data-Driven Min-Max MPC for Linear Systems: Robustness and Adaptation","abstract":"Data-driven controllers design is an important research problem, in particular when data is corrupted by the noise. In this paper, we propose a data-driven min-max model predictive control (MPC) scheme using noisy input-state data for unknown linear time-invariant (LTI) system. The unknown system matrices are characterized by a set-membership representation using the noisy input-state data. Leveraging this representation, we derive an upper bound on the worst-case cost and determine the corresponding optimal state-feedback control law through a semidefinite program (SDP). We prove that the resulting closed-loop system is robustly stabilized and satisfies the input and state constraints. Further, we propose an adaptive data-driven min-max MPC scheme which exploits additional online input-state data to improve closed-loop performance. Numerical examples show the effectiveness of the proposed methods.","sentences":["Data-driven controllers design is an important research problem, in particular when data is corrupted by the noise.","In this paper, we propose a data-driven min-max model predictive control (MPC) scheme using noisy input-state data for unknown linear time-invariant (LTI) system.","The unknown system matrices are characterized by a set-membership representation using the noisy input-state data.","Leveraging this representation, we derive an upper bound on the worst-case cost and determine the corresponding optimal state-feedback control law through a semidefinite program (SDP).","We prove that the resulting closed-loop system is robustly stabilized and satisfies the input and state constraints.","Further, we propose an adaptive data-driven min-max MPC scheme which exploits additional online input-state data to improve closed-loop performance.","Numerical examples show the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2404.19096v1","category":"eess.SY"}
{"created":"2024-04-29 19:26:25","title":"Reinforcement Learning Driven Cooperative Ball Balance in Rigidly Coupled Drones","abstract":"Multi-drone cooperative transport (CT) problem has been widely studied in the literature. However, limited work exists on control of such systems in the presence of time-varying uncertainties, such as the time-varying center of gravity (CG). This paper presents a leader-follower approach for the control of a multi-drone CT system with time-varying CG. The leader uses a traditional Proportional-Integral-Derivative (PID) controller, and in contrast, the follower uses a deep reinforcement learning (RL) controller using only local information and minimal leader information. Extensive simulation results are presented, showing the effectiveness of the proposed method over a previously developed adaptive controller and for variations in the mass of the objects being transported and CG speeds. Preliminary experimental work also demonstrates ball balance (depicting moving CG) on a stick/rod lifted by two Crazyflie drones cooperatively.","sentences":["Multi-drone cooperative transport (CT) problem has been widely studied in the literature.","However, limited work exists on control of such systems in the presence of time-varying uncertainties, such as the time-varying center of gravity (CG).","This paper presents a leader-follower approach for the control of a multi-drone CT system with time-varying CG.","The leader uses a traditional Proportional-Integral-Derivative (PID) controller, and in contrast, the follower uses a deep reinforcement learning (RL) controller using only local information and minimal leader information.","Extensive simulation results are presented, showing the effectiveness of the proposed method over a previously developed adaptive controller and for variations in the mass of the objects being transported and CG speeds.","Preliminary experimental work also demonstrates ball balance (depicting moving CG) on a stick/rod lifted by two Crazyflie drones cooperatively."],"url":"http://arxiv.org/abs/2404.19070v1","category":"cs.RO"}
{"created":"2024-04-29 18:49:06","title":"Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes","abstract":"The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \\textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.","sentences":["The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice.","Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density.","In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \\textit{any} continuous, integrable spectral density.","In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence.","As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop.","We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer."],"url":"http://arxiv.org/abs/2404.19053v1","category":"stat.CO"}
{"created":"2024-04-29 18:13:23","title":"Adaptive Regulated Sparsity Promoting Approach for Data-Driven Modeling and Control of Grid-Connected Solar Photovoltaic Generation","abstract":"This paper aims to introduce a new statistical learning technique based on sparsity promoting for data-driven modeling and control of solar photovoltaic (PV) systems. Compared with conventional sparse regression techniques that might introduce computational complexities when the number of candidate functions increases, an innovative algorithm, named adaptive regulated sparse regression (ARSR) is proposed that adaptively regulates the hyperparameter weights of candidate functions to best represent the dynamics of PV systems. Utilizing this algorithm, open-loop and closed-loop models of single-stage and two-stage PV systems are obtained from measurements and are utilized for control design purposes. Moreover, it is demonstrated that the proposed data-driven approach can successfully be employed for fault analysis studies, which distinguishes its capabilities compared with other data-driven techniques. Finally, the proposed approach is validated through real-time simulations.","sentences":["This paper aims to introduce a new statistical learning technique based on sparsity promoting for data-driven modeling and control of solar photovoltaic (PV) systems.","Compared with conventional sparse regression techniques that might introduce computational complexities when the number of candidate functions increases, an innovative algorithm, named adaptive regulated sparse regression (ARSR) is proposed that adaptively regulates the hyperparameter weights of candidate functions to best represent the dynamics of PV systems.","Utilizing this algorithm, open-loop and closed-loop models of single-stage and two-stage PV systems are obtained from measurements and are utilized for control design purposes.","Moreover, it is demonstrated that the proposed data-driven approach can successfully be employed for fault analysis studies, which distinguishes its capabilities compared with other data-driven techniques.","Finally, the proposed approach is validated through real-time simulations."],"url":"http://arxiv.org/abs/2404.19028v1","category":"eess.SY"}
{"created":"2024-04-29 18:07:47","title":"Multi-Page Document Visual Question Answering using Self-Attention Scoring Mechanism","abstract":"Documents are 2-dimensional carriers of written communication, and as such their interpretation requires a multi-modal approach where textual and visual information are efficiently combined. Document Visual Question Answering (Document VQA), due to this multi-modal nature, has garnered significant interest from both the document understanding and natural language processing communities. The state-of-the-art single-page Document VQA methods show impressive performance, yet in multi-page scenarios, these methods struggle. They have to concatenate all pages into one large page for processing, demanding substantial GPU resources, even for evaluation. In this work, we propose a novel method and efficient training strategy for multi-page Document VQA tasks. In particular, we employ a visual-only document representation, leveraging the encoder from a document understanding model, Pix2Struct. Our approach utilizes a self-attention scoring mechanism to generate relevance scores for each document page, enabling the retrieval of pertinent pages. This adaptation allows us to extend single-page Document VQA models to multi-page scenarios without constraints on the number of pages during evaluation, all with minimal demand for GPU resources. Our extensive experiments demonstrate not only achieving state-of-the-art performance without the need for Optical Character Recognition (OCR), but also sustained performance in scenarios extending to documents of nearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA dataset. Our code is publicly available at \\url{https://github.com/leitro/SelfAttnScoring-MPDocVQA}.","sentences":["Documents are 2-dimensional carriers of written communication, and as such their interpretation requires a multi-modal approach where textual and visual information are efficiently combined.","Document Visual Question Answering (Document VQA), due to this multi-modal nature, has garnered significant interest from both the document understanding and natural language processing communities.","The state-of-the-art single-page Document VQA methods show impressive performance, yet in multi-page scenarios, these methods struggle.","They have to concatenate all pages into one large page for processing, demanding substantial GPU resources, even for evaluation.","In this work, we propose a novel method and efficient training strategy for multi-page Document VQA tasks.","In particular, we employ a visual-only document representation, leveraging the encoder from a document understanding model, Pix2Struct.","Our approach utilizes a self-attention scoring mechanism to generate relevance scores for each document page, enabling the retrieval of pertinent pages.","This adaptation allows us to extend single-page Document VQA models to multi-page scenarios without constraints on the number of pages during evaluation, all with minimal demand for GPU resources.","Our extensive experiments demonstrate not only achieving state-of-the-art performance without the need for Optical Character Recognition (OCR), but also sustained performance in scenarios extending to documents of nearly 800 pages compared to a maximum of 20 pages in the MP-DocVQA dataset.","Our code is publicly available at \\url{https://github.com/leitro/SelfAttnScoring-MPDocVQA}."],"url":"http://arxiv.org/abs/2404.19024v1","category":"cs.CV"}
{"created":"2024-04-29 18:04:30","title":"Enhancing Autonomous Vehicle Design and Testing: A Comprehensive Review of AR and VR Integration","abstract":"This comprehensive literature review explores the potential of Augmented Reality and Virtual Reality technologies to enhance the design and testing of autonomous vehicles. By analyzing existing research, the review aims to identify how AR and VR can be leveraged to improve various aspects of autonomous vehicle development, including: creating more realistic and comprehensive testing environments, facilitating the design of user centered interfaces, and safely evaluating driver behavior in complex scenarios. Ultimately, the review highlights AR and VR utilization as a key driver in the development of adaptable testing environments, fostering more dependable autonomous vehicle technology, and ultimately propelling significant advancements within the field.","sentences":["This comprehensive literature review explores the potential of Augmented Reality and Virtual Reality technologies to enhance the design and testing of autonomous vehicles.","By analyzing existing research, the review aims to identify how AR and VR can be leveraged to improve various aspects of autonomous vehicle development, including: creating more realistic and comprehensive testing environments, facilitating the design of user centered interfaces, and safely evaluating driver behavior in complex scenarios.","Ultimately, the review highlights AR and VR utilization as a key driver in the development of adaptable testing environments, fostering more dependable autonomous vehicle technology, and ultimately propelling significant advancements within the field."],"url":"http://arxiv.org/abs/2404.19021v1","category":"cs.HC"}
{"created":"2024-04-29 17:54:19","title":"Timely Status Updates in Slotted ALOHA Network With Energy Harvesting","abstract":"We investigate the age of information (AoI) in a scenario where energy-harvesting devices send status updates to a gateway following the slotted ALOHA protocol and receive no feedback. We let the devices adjust the transmission probabilities based on their current battery level. Using a Markovian analysis, we derive analytically the average AoI. We further provide an approximate analysis for accurate and easy-to-compute approximations of both the average AoI and the age-violation probability (AVP), i.e., the probability that the AoI exceeds a given threshold. We also analyze the average throughput. Via numerical results, we investigate two baseline strategies: transmit a new update whenever possible to exploit every opportunity to reduce the AoI, and transmit only when sufficient energy is available to increase the chance of successful decoding. The two strategies are beneficial for low and high update-generation rates, respectively. We show that an optimized policy that balances the two strategies outperforms them significantly in terms of both AoI metrics and throughput. Finally, we show the benefit of decoding multiple packets in a slot using successive interference cancellation and adapting the transmission probability based on both the current battery level and the time elapsed since the last transmission.","sentences":["We investigate the age of information (AoI) in a scenario where energy-harvesting devices send status updates to a gateway following the slotted ALOHA protocol and receive no feedback.","We let the devices adjust the transmission probabilities based on their current battery level.","Using a Markovian analysis, we derive analytically the average AoI. We further provide an approximate analysis for accurate and easy-to-compute approximations of both the average AoI and the age-violation probability (AVP), i.e., the probability that the AoI exceeds a given threshold.","We also analyze the average throughput.","Via numerical results, we investigate two baseline strategies: transmit a new update whenever possible to exploit every opportunity to reduce the AoI, and transmit only when sufficient energy is available to increase the chance of successful decoding.","The two strategies are beneficial for low and high update-generation rates, respectively.","We show that an optimized policy that balances the two strategies outperforms them significantly in terms of both AoI metrics and throughput.","Finally, we show the benefit of decoding multiple packets in a slot using successive interference cancellation and adapting the transmission probability based on both the current battery level and the time elapsed since the last transmission."],"url":"http://arxiv.org/abs/2404.18990v1","category":"cs.IT"}
{"created":"2024-04-29 17:49:49","title":"Cyberbully and Online Harassment: Issues Associated with Digital Wellbeing","abstract":"As digital technology becomes increasingly embedded in daily life, its impact on social interactions has become a critical area of study, particularly concerning cyberbullying. This meta-analysis investigates the dual role of technology in cyberbullying both as a catalyst that can exacerbate the issue and as a potential solution. Cyberbullying, characterized by the use of digital platforms to harass, threaten, or humiliate individuals, poses significant challenges to mental and social wellbeing. This research synthesizes empirical findings from diverse studies to evaluate how innovative technological interventions, such as content monitoring algorithms, anonymous reporting systems, and educational initiatives integrated within digital platforms, contribute to reducing the prevalence of cyberbullying. The study focuses on the effectiveness of these interventions in various settings, highlighting the need for adaptive strategies that respond to the dynamic digital landscape. By offering a comprehensive overview of the current state of cyberbullying and the efficacy of technology based solutions, this analysis provides valuable insights for stakeholders, including educators, policymakers, and technology developers, aiming to enhance digital wellbeing and create safer online environments. The findings underscore the importance of leveraging technology not only as a medium of communication but also as a strategic tool to combat the negative impacts of cyberbullying, thus promoting a more inclusive and respectful digital world.","sentences":["As digital technology becomes increasingly embedded in daily life, its impact on social interactions has become a critical area of study, particularly concerning cyberbullying.","This meta-analysis investigates the dual role of technology in cyberbullying both as a catalyst that can exacerbate the issue and as a potential solution.","Cyberbullying, characterized by the use of digital platforms to harass, threaten, or humiliate individuals, poses significant challenges to mental and social wellbeing.","This research synthesizes empirical findings from diverse studies to evaluate how innovative technological interventions, such as content monitoring algorithms, anonymous reporting systems, and educational initiatives integrated within digital platforms, contribute to reducing the prevalence of cyberbullying.","The study focuses on the effectiveness of these interventions in various settings, highlighting the need for adaptive strategies that respond to the dynamic digital landscape.","By offering a comprehensive overview of the current state of cyberbullying and the efficacy of technology based solutions, this analysis provides valuable insights for stakeholders, including educators, policymakers, and technology developers, aiming to enhance digital wellbeing and create safer online environments.","The findings underscore the importance of leveraging technology not only as a medium of communication but also as a strategic tool to combat the negative impacts of cyberbullying, thus promoting a more inclusive and respectful digital world."],"url":"http://arxiv.org/abs/2404.18989v1","category":"cs.CY"}
{"created":"2024-04-29 16:30:59","title":"Evidence for Plasmoid-mediated Magnetic Reconnection during a Small-scale Flare in the Partially Ionized Low Solar Atmosphere","abstract":"Magnetic reconnection plays a crucial role in the energy release process for different kinds of solar eruptions and activities. The rapid solar eruption requires a fast reconnection model. Plasmoid instability in the reconnecting current sheets is one of the most acceptable fast reconnection mechanisms for explaining the explosive events in the magnetohydrodynamics (MHD) scale, which is also a potential bridge between the macroscopic MHD reconnection process and microscale dissipations. Plenty of high resolution observations indicate that the plasmoid-like structures exist in the high temperature solar corona, but such evidences are very rare in the lower solar atmosphere with partially ionized plasmas. Utilizing joint observations from the Goode Solar Telescope (GST) and the Solar Dynamics Observatory (SDO), we discovered a small-scale eruptive phenomenon in NOAA AR 13085, characterized by clear reconnection cusp structures, supported by Nonlinear Force-Free Field (NLFFF) extrapolation results. The plasmoid-like structures with a size about 150 km were observed to be ejected downward from the current sheet at a maximum velocity of 24 km$\\cdot$s$^{-1}$ in the H$\\alpha$ line wing images, followed by enhanced emissions at around the post flare loop region in multiple wave lengths. Our 2.5D high-resolution MHD simulations further reproduced such a phenomenon and revealed reconnection fine structures. These results provide comprehensive evidences for the plasmoid mediated reconnection in partially ionized plasmas, and suggest an unified reconnection model for solar flares with different length scales from the lower chromosphere to corona.","sentences":["Magnetic reconnection plays a crucial role in the energy release process for different kinds of solar eruptions and activities.","The rapid solar eruption requires a fast reconnection model.","Plasmoid instability in the reconnecting current sheets is one of the most acceptable fast reconnection mechanisms for explaining the explosive events in the magnetohydrodynamics (MHD) scale, which is also a potential bridge between the macroscopic MHD reconnection process and microscale dissipations.","Plenty of high resolution observations indicate that the plasmoid-like structures exist in the high temperature solar corona, but such evidences are very rare in the lower solar atmosphere with partially ionized plasmas.","Utilizing joint observations from the Goode Solar Telescope (GST) and the Solar Dynamics Observatory (SDO), we discovered a small-scale eruptive phenomenon in NOAA AR 13085, characterized by clear reconnection cusp structures, supported by Nonlinear Force-Free Field (NLFFF) extrapolation results.","The plasmoid-like structures with a size about 150 km were observed to be ejected downward from the current sheet at a maximum velocity of 24 km$\\cdot$s$^{-1}$ in the H$\\alpha$ line wing images, followed by enhanced emissions at around the post flare loop region in multiple wave lengths.","Our 2.5D high-resolution MHD simulations further reproduced such a phenomenon and revealed reconnection fine structures.","These results provide comprehensive evidences for the plasmoid mediated reconnection in partially ionized plasmas, and suggest an unified reconnection model for solar flares with different length scales from the lower chromosphere to corona."],"url":"http://arxiv.org/abs/2404.18983v1","category":"astro-ph.SR"}
{"created":"2024-04-30 17:59:51","title":"Lightplane: Highly-Scalable Components for Neural 3D Fields","abstract":"Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision. However, current designs for these 2D-3D mapping are memory-intensive, posing a significant bottleneck for existing methods and hindering new applications. In response, we propose a pair of highly scalable components for 3D neural fields: Lightplane Render and Splatter, which significantly reduce memory usage in 2D-3D mapping. These innovations enable the processing of vastly more and higher resolution images with small memory and computational costs. We demonstrate their utility in various applications, from benefiting single-scene optimization with image-level losses to realizing a versatile pipeline for dramatically scaling 3D reconstruction and generation. Code: \\url{https://github.com/facebookresearch/lightplane}.","sentences":["Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision.","However, current designs for these 2D-3D mapping are memory-intensive, posing a significant bottleneck for existing methods and hindering new applications.","In response, we propose a pair of highly scalable components for 3D neural fields:","Lightplane Render and Splatter, which significantly reduce memory usage in 2D-3D mapping.","These innovations enable the processing of vastly more and higher resolution images with small memory and computational costs.","We demonstrate their utility in various applications, from benefiting single-scene optimization with image-level losses to realizing a versatile pipeline for dramatically scaling 3D reconstruction and generation.","Code: \\url{https://github.com/facebookresearch/lightplane}."],"url":"http://arxiv.org/abs/2404.19760v1","category":"cs.CV"}
{"created":"2024-04-30 17:55:11","title":"curvedSpaceSim: A framework for simulating particles interacting along geodesics","abstract":"A large number of powerful, high-quality, and open-source simulation packages exist to efficiently perform molecular dynamics simulations, and their prevalence has greatly accelerated discoveries across a wide range of scientific domains. These packages typically simulate particles in free (Euclidean) space, with options to specify a variety of boundary conditions. While more exotic, many physical systems are constrained to and interact across curved surfaces, such as organisms moving across the landscape, colloids pinned at curved fluid-fluid interfaces, and layers of epithelial cells forming highly curved tissues. The calculation of distances and the updating of equations of motion in idealized geometries (namely, on surfaces of constant curvature) can be done analytically, but it is much more challenging to efficiently perform molecular-dynamics-like simulations on arbitrarily curved surfaces. This article discusses a simulation framework which combines tools from particle-based simulations with recent work in discrete differential geometry to model particles that interact via geodesic distances and move on an arbitrarily curved surface. We present computational cost estimates for a variety of surface complexities with and without various algorithmic specializations (e.g., restrictions to short-range interaction potentials, or multi-threaded parallelization). Our flexible and extensible framework is set up to easily handle both equilibrium and non-equilibrium dynamics, and will enable researchers to access time- and particle-number-scales previously inaccessible.","sentences":["A large number of powerful, high-quality, and open-source simulation packages exist to efficiently perform molecular dynamics simulations, and their prevalence has greatly accelerated discoveries across a wide range of scientific domains.","These packages typically simulate particles in free (Euclidean) space, with options to specify a variety of boundary conditions.","While more exotic, many physical systems are constrained to and interact across curved surfaces, such as organisms moving across the landscape, colloids pinned at curved fluid-fluid interfaces, and layers of epithelial cells forming highly curved tissues.","The calculation of distances and the updating of equations of motion in idealized geometries (namely, on surfaces of constant curvature) can be done analytically, but it is much more challenging to efficiently perform molecular-dynamics-like simulations on arbitrarily curved surfaces.","This article discusses a simulation framework which combines tools from particle-based simulations with recent work in discrete differential geometry to model particles that interact via geodesic distances and move on an arbitrarily curved surface.","We present computational cost estimates for a variety of surface complexities with and without various algorithmic specializations (e.g., restrictions to short-range interaction potentials, or multi-threaded parallelization).","Our flexible and extensible framework is set up to easily handle both equilibrium and non-equilibrium dynamics, and will enable researchers to access time- and particle-number-scales previously inaccessible."],"url":"http://arxiv.org/abs/2404.19751v1","category":"cond-mat.soft"}
{"created":"2024-04-30 17:11:12","title":"The lazy (NTK) and rich ($\u03bc$P) regimes: a gentle tutorial","abstract":"A central theme of the modern machine learning paradigm is that larger neural networks achieve better performance on a variety of metrics. Theoretical analyses of these overparameterized models have recently centered around studying very wide neural networks. In this tutorial, we provide a nonrigorous but illustrative derivation of the following fact: in order to train wide networks effectively, there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights. This degree of freedom controls the richness of training behavior: at minimum, the wide network trains lazily like a kernel machine, and at maximum, it exhibits feature learning in the so-called $\\mu$P regime. In this paper, we explain this richness scale, synthesize recent research results into a coherent whole, offer new perspectives and intuitions, and provide empirical evidence supporting our claims. In doing so, we hope to encourage further study of the richness scale, as it may be key to developing a scientific theory of feature learning in practical deep neural networks.","sentences":["A central theme of the modern machine learning paradigm is that larger neural networks achieve better performance on a variety of metrics.","Theoretical analyses of these overparameterized models have recently centered around studying very wide neural networks.","In this tutorial, we provide a nonrigorous but illustrative derivation of the following fact: in order to train wide networks effectively, there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights.","This degree of freedom controls the richness of training behavior: at minimum, the wide network trains lazily like a kernel machine, and at maximum, it exhibits feature learning in the so-called $\\mu$P regime.","In this paper, we explain this richness scale, synthesize recent research results into a coherent whole, offer new perspectives and intuitions, and provide empirical evidence supporting our claims.","In doing so, we hope to encourage further study of the richness scale, as it may be key to developing a scientific theory of feature learning in practical deep neural networks."],"url":"http://arxiv.org/abs/2404.19719v1","category":"cs.LG"}
{"created":"2024-04-30 16:47:46","title":"GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting","abstract":"We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ .","sentences":["We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU.","Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering.","In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity.","We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively.","In both scenarios, the models outperform state-of-the-art baselines by a wide margin.","We also demonstrate applications of our model in downstream 3D generation tasks.","Our project webpage is available at: https://sai-bi.github.io/project/gs-lrm/ ."],"url":"http://arxiv.org/abs/2404.19702v1","category":"cs.CV"}
{"created":"2024-04-30 16:46:36","title":"Dynamics of particle aggregation in dewetting films of complex liquids","abstract":"We consider the dynamic wetting and dewetting processes of films and droplets of complex liquids on planar surfaces, focusing on the case of colloidal suspensions, where the particle interactions can be sufficiently attractive to cause agglomeration of the colloids within the film. This leads to an interesting array of dynamic behaviours within the liquid and of the liquid-air interface. Incorporating concepts from thermodynamics and using the thin-film approximation, we construct a model consisting of a pair of coupled partial differential equations that represent the evolution of the liquid film and the effective colloidal height profiles. We determine the relevant phase behaviour of the uniform system, including finding associated binodal and spinodal curves, helping to uncover how the emerging behaviour depends on the particle interactions. Performing a linear stability analysis of our system enables us to identify parameter regimes where agglomerates form, which we independently confirm through numerical simulations and continuation of steady states, to construct bifurcation diagrams. We obtain various dynamics such as uniform colloidal profiles in an unstable situation evolving into agglomerates and thus elucidate the interplay between dewetting and particle aggregation in complex liquids on surfaces.","sentences":["We consider the dynamic wetting and dewetting processes of films and droplets of complex liquids on planar surfaces, focusing on the case of colloidal suspensions, where the particle interactions can be sufficiently attractive to cause agglomeration of the colloids within the film.","This leads to an interesting array of dynamic behaviours within the liquid and of the liquid-air interface.","Incorporating concepts from thermodynamics and using the thin-film approximation, we construct a model consisting of a pair of coupled partial differential equations that represent the evolution of the liquid film and the effective colloidal height profiles.","We determine the relevant phase behaviour of the uniform system, including finding associated binodal and spinodal curves, helping to uncover how the emerging behaviour depends on the particle interactions.","Performing a linear stability analysis of our system enables us to identify parameter regimes where agglomerates form, which we independently confirm through numerical simulations and continuation of steady states, to construct bifurcation diagrams.","We obtain various dynamics such as uniform colloidal profiles in an unstable situation evolving into agglomerates and thus elucidate the interplay between dewetting and particle aggregation in complex liquids on surfaces."],"url":"http://arxiv.org/abs/2404.19701v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 16:42:52","title":"Scalar perturbations from inflation in the presence of gauge fields","abstract":"We study how Abelian-gauge-field production during inflation affects scalar perturbations in the case when the gauge field interacts with the inflaton directly (by means of generic kinetic and axial couplings) and via gravity. The homogeneous background solution is defined by self-consistently taking into account the backreaction of the gauge field on the evolution of the inflaton and the scale factor. For the perturbations on top of this background, all possible scalar contributions coming from the inflaton, the metric, and the gauge field are considered. We derive a second-order differential equation for the curvature perturbation, $\\zeta$, capturing the impact of the gauge field, both on the background dynamics and on the evolution of scalar perturbations. The latter is described by a source term in the $\\zeta$-equation, which is quadratic in the gauge-field operators and leads to non-Gaussianities in the curvature perturbations. We derive general expressions for the induced scalar power spectrum and bispectrum. Finally, we apply our formalism to the well-known case of axion inflation without backreaction. Numerical results show that, in this example, the effect of including metric perturbations is small for values of the gauge-field production parameter $\\xi> 3$. This is in agreement with the results of previous studies in the literature. However, in the region of smaller values, $\\xi\\lesssim 2$, our new results exhibit order-of-unity deviations when compared to previous results.","sentences":["We study how Abelian-gauge-field production during inflation affects scalar perturbations in the case when the gauge field interacts with the inflaton directly (by means of generic kinetic and axial couplings) and via gravity.","The homogeneous background solution is defined by self-consistently taking into account the backreaction of the gauge field on the evolution of the inflaton and the scale factor.","For the perturbations on top of this background, all possible scalar contributions coming from the inflaton, the metric, and the gauge field are considered.","We derive a second-order differential equation for the curvature perturbation, $\\zeta$, capturing the impact of the gauge field, both on the background dynamics and on the evolution of scalar perturbations.","The latter is described by a source term in the $\\zeta$-equation, which is quadratic in the gauge-field operators and leads to non-Gaussianities in the curvature perturbations.","We derive general expressions for the induced scalar power spectrum and bispectrum.","Finally, we apply our formalism to the well-known case of axion inflation without backreaction.","Numerical results show that, in this example, the effect of including metric perturbations is small for values of the gauge-field production parameter $\\xi> 3$.","This is in agreement with the results of previous studies in the literature.","However, in the region of smaller values, $\\xi\\lesssim 2$, our new results exhibit order-of-unity deviations when compared to previous results."],"url":"http://arxiv.org/abs/2404.19694v1","category":"astro-ph.CO"}
{"created":"2024-04-30 16:29:44","title":"Continuum limit of $p$-biharmonic equations on graphs","abstract":"This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph. The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity. We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions. The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations. The $L^\\infty$ estimates of solutions are also obtained as a byproduct.","sentences":["This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph.","The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity.","We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions.","The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations.","The $L^\\infty$ estimates of solutions are also obtained as a byproduct."],"url":"http://arxiv.org/abs/2404.19689v1","category":"math.AP"}
{"created":"2024-04-30 16:26:23","title":"On the lack of selection for the transport equation over a dense set of vector fields","abstract":"The purpose of this work is to demonstrate that the lack of selection by smooth regularisation for the continuity equation with a bounded, divergence-free vector field as demonstrated in \\cite{DeLellis_Giri22} by De Lellis and Giri takes place over a dense set of vector fields. More precisely, we construct a set of bounded vector fields $D$ dense in $L^p_{loc}([0,2]\\times \\R^2;\\R^2)$ such that for each vector field $\\bb\\in D$, there are two smooth regularisations of $\\bb$, for which the unique solution of the Cauchy problem for the continuity equation along each regularisation converges to two distinct solutions of the Cauchy problem along $\\bb$.","sentences":["The purpose of this work is to demonstrate that the lack of selection by smooth regularisation for the continuity equation with a bounded, divergence-free vector field as demonstrated in \\cite{DeLellis_Giri22} by De Lellis and Giri takes place over a dense set of vector fields.","More precisely, we construct a set of bounded vector fields $D$ dense in $L^p_{loc}([0,2]\\times \\R^2;\\R^2)$ such that for each vector field $\\bb\\in D$, there are two smooth regularisations of $\\bb$, for which the unique solution of the Cauchy problem for the continuity equation along each regularisation converges to two distinct solutions of the Cauchy problem along $\\bb$."],"url":"http://arxiv.org/abs/2404.19687v1","category":"math.AP"}
{"created":"2024-04-30 16:06:04","title":"Neural Controlled Differential Equations with Quantum Hidden Evolutions","abstract":"We introduce a class of neural controlled differential equation inspired by quantum mechanics. Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schr\\\"{o}dinger equation. Specifically, the hidden state represents the wave function, and its collapse leads to an interpretation of the classification probability. We implement and compare the results of four variants of NQDEs on a toy spiral classification problem.","sentences":["We introduce a class of neural controlled differential equation inspired by quantum mechanics.","Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schr\\\"{o}dinger equation.","Specifically, the hidden state represents the wave function, and its collapse leads to an interpretation of the classification probability.","We implement and compare the results of four variants of NQDEs on a toy spiral classification problem."],"url":"http://arxiv.org/abs/2404.19673v1","category":"cs.LG"}
{"created":"2024-04-30 15:35:27","title":"Thermal Fluid Closures and Pressure Anisotropies in Numerical Simulations of Plasma Wakefield Acceleration","abstract":"We investigate the dynamics of plasma-based acceleration processes with collisionless particle dynamics and non negligible thermal effects. We aim at assessing the applicability of fluid-like models, obtained by suitable closure assumptions applied to the relativistic kinetic equations, thus not suffering of statistical noise, even in presence of a finite temperature. The work here presented focuses on the characterization of pressure anisotropies, which crucially depend on the adopted closure scheme, and hence are useful to discern the appropriate thermal fluid model. To this aim, simulation results of spatially resolved fluid models with different thermal closure assumptions are compared with the results of particle-in-cell (PIC) simulations at changing temperature and amplitude of plasma oscillations.","sentences":["We investigate the dynamics of plasma-based acceleration processes with collisionless particle dynamics and non negligible thermal effects.","We aim at assessing the applicability of fluid-like models, obtained by suitable closure assumptions applied to the relativistic kinetic equations, thus not suffering of statistical noise, even in presence of a finite temperature.","The work here presented focuses on the characterization of pressure anisotropies, which crucially depend on the adopted closure scheme, and hence are useful to discern the appropriate thermal fluid model.","To this aim, simulation results of spatially resolved fluid models with different thermal closure assumptions are compared with the results of particle-in-cell (PIC) simulations at changing temperature and amplitude of plasma oscillations."],"url":"http://arxiv.org/abs/2404.19635v1","category":"physics.plasm-ph"}
{"created":"2024-04-30 15:19:51","title":"Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture","abstract":"Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.","sentences":["Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default.","We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically.","In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics.","With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton's laws of motion are satisfied.","In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture.","Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware.","This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system.","Code is available at https://xinyu-yi.github.io/PNP/."],"url":"http://arxiv.org/abs/2404.19619v1","category":"cs.GR"}
{"created":"2024-04-30 14:55:57","title":"Data-Driven Invertible Neural Surrogates of Atmospheric Transmission","abstract":"We present a framework for inferring an atmospheric transmission profile from a spectral scene. This framework leverages a lightweight, physics-based simulator that is automatically tuned - by virtue of autodifferentiation and differentiable programming - to construct a surrogate atmospheric profile to model the observed data. We demonstrate utility of the methodology by (i) performing atmospheric correction, (ii) recasting spectral data between various modalities (e.g. radiance and reflectance at the surface and at the sensor), and (iii) inferring atmospheric transmission profiles, such as absorbing bands and their relative magnitudes.","sentences":["We present a framework for inferring an atmospheric transmission profile from a spectral scene.","This framework leverages a lightweight, physics-based simulator that is automatically tuned - by virtue of autodifferentiation and differentiable programming - to construct a surrogate atmospheric profile to model the observed data.","We demonstrate utility of the methodology by (i) performing atmospheric correction, (ii) recasting spectral data between various modalities (e.g. radiance and reflectance at the surface and at the sensor), and (iii) inferring atmospheric transmission profiles, such as absorbing bands and their relative magnitudes."],"url":"http://arxiv.org/abs/2404.19605v1","category":"cs.LG"}
{"created":"2024-04-30 14:51:39","title":"Stabilized POD Reduced Order Models for convection-dominated incompressible flows","abstract":"We present a comparative computational study of two stabilized Reduced Order Models (ROMs) for the simulation of convection-dominated incompressible flow (Reynolds number of the order of a few thousands). Representative solutions in the parameter space, which includes either time only or time and Reynolds number, are computed with a Finite Volume method and used to generate a reduced basis via Proper Orthogonal Decomposition (POD). Galerkin projection of the Navier-Stokes equations onto the reduced space is used to compute the ROM solution. To ensure computational efficiency, the number of POD modes is truncated and ROM solution accuracy is recovered through two stabilization methods: i) adding a global constant artificial viscosity to the reduced dimensional model, and ii) adding a different value of artificial viscosity for the different POD modes. We test the stabilized ROMs for fluid flow in an idealized medical device consisting of a conical convergent, a narrow throat, and a sudden expansion. Both stabilization methods significantly improve the ROM solution accuracy over a standard (non-stabilized) POD-Galerkin model.","sentences":["We present a comparative computational study of two stabilized Reduced Order Models (ROMs) for the simulation of convection-dominated incompressible flow (Reynolds number of the order of a few thousands).","Representative solutions in the parameter space, which includes either time only or time and Reynolds number, are computed with a Finite Volume method and used to generate a reduced basis via Proper Orthogonal Decomposition (POD).","Galerkin projection of the Navier-Stokes equations onto the reduced space is used to compute the ROM solution.","To ensure computational efficiency, the number of POD modes is truncated and ROM solution accuracy is recovered through two stabilization methods: i) adding a global constant artificial viscosity to the reduced dimensional model, and ii) adding a different value of artificial viscosity for the different POD modes.","We test the stabilized ROMs for fluid flow in an idealized medical device consisting of a conical convergent, a narrow throat, and a sudden expansion.","Both stabilization methods significantly improve the ROM solution accuracy over a standard (non-stabilized) POD-Galerkin model."],"url":"http://arxiv.org/abs/2404.19600v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 14:21:27","title":"Catalan percolation","abstract":"In Catalan percolation, all nearest-neighbor edges $\\{i,i+1\\}$ along $\\mathbb Z$ are initially occupied, and all other edges are open independently with probability $p$. Open edges $\\{i,j\\}$ are occupied if some pair of edges $\\{i,k\\}$ and $\\{k,j\\}$, with $i<k<j$, become occupied. This model was introduced by Gravner and the third author, in the context of polluted graph bootstrap percolation.   We prove that the critical $p_{\\mathrm c}$ is strictly between that of oriented site percolation on $\\mathbb Z^2$ and the Catalan growth rate $1/4$. Our main result shows that an enhanced oriented percolation model, with non-decaying infinite-range dependency, has a strictly smaller critical parameter than the classical model. This is reminiscent of the work of Duminil-Copin, Hil\\'ario, Kozma and Sidoravicius on brochette percolation. Our proof differs, however, in that we do not use Aizenman--Grimmett enhancements or differential inequalities. Two key ingredients are the work of Hil\\'ario, S\\'a, Sanchis and Teixeira on stretched lattices, and the Russo--Seymour--Welsh result for oriented percolation by Duminil-Copin, Tassion and Teixeira.","sentences":["In Catalan percolation, all nearest-neighbor edges $\\{i,i+1\\}$ along $\\mathbb Z$ are initially occupied, and all other edges are open independently with probability $p$. Open edges $\\{i,j\\}$ are occupied if some pair of edges $\\{i,k\\}$ and $\\{k,j\\}$, with $i<k<j$, become occupied.","This model was introduced by Gravner and the third author, in the context of polluted graph bootstrap percolation.   ","We prove that the critical $p_{\\mathrm c}$ is strictly between that of oriented site percolation on $\\mathbb Z^2$ and the Catalan growth rate $1/4$. Our main result shows that an enhanced oriented percolation model, with non-decaying infinite-range dependency, has a strictly smaller critical parameter than the classical model.","This is reminiscent of the work of Duminil-Copin, Hil\\'ario, Kozma and Sidoravicius on brochette percolation.","Our proof differs, however, in that we do not use Aizenman--Grimmett enhancements or differential inequalities.","Two key ingredients are the work of Hil\\'ario, S\\'a, Sanchis and Teixeira on stretched lattices, and the Russo--Seymour--Welsh result for oriented percolation by Duminil-Copin, Tassion and Teixeira."],"url":"http://arxiv.org/abs/2404.19583v1","category":"math.PR"}
{"created":"2024-04-30 14:11:37","title":"Test function approach to fully nonlinear equations in thin domains","abstract":"In this note we extend to fully nonlinear operators the well known result on thin domains of Hale and Raugel. The result is more general even in the case of the Laplacian.","sentences":["In this note we extend to fully nonlinear operators the well known result on thin domains of Hale and Raugel.","The result is more general even in the case of the Laplacian."],"url":"http://arxiv.org/abs/2404.19577v1","category":"math.AP"}
{"created":"2024-04-30 14:11:10","title":"Topology in a Su--Schrieffer--Heeger plasmonic crystal","abstract":"In this paper we study the topology of the bands of a polaritonic crystal composed of graphene and of a metallic grating. We first derive a Kronig--Penney type of equation for the polaritonic bands as function of the Bloch wavevector and discuss the propagation of the surface plasmon polaritons on the polaritonic crystal using a transfer-matrix approach. Secondly, we reformulate the problem as a tight-binding model that resembles the Su--Schrieffer-Heeger (SSH) Hamiltonian, one difference being that the hopping amplitudes are, in this case, energy dependent. In possession of the tight-binding equations it is a simple task to determine the topology of the bands. Similarly to the SSH model, we show that there is a tunable parameter that induces topological phase transitions from trivial to non-trivial. In our case, it is the distance $d$ between the graphene sheet and the metallic grating. We note that $d$ is a parameter that can be easily tuned experimentally simply by controlling the thickness of the spacer between the grating and the graphene sheet. It is then experimentally feasible to engineer devices with the required topological properties.","sentences":["In this paper we study the topology of the bands of a polaritonic crystal composed of graphene and of a metallic grating.","We first derive a Kronig--Penney type of equation for the polaritonic bands as function of the Bloch wavevector and discuss the propagation of the surface plasmon polaritons on the polaritonic crystal using a transfer-matrix approach.","Secondly, we reformulate the problem as a tight-binding model that resembles the Su--Schrieffer-Heeger (SSH) Hamiltonian, one difference being that the hopping amplitudes are, in this case, energy dependent.","In possession of the tight-binding equations it is a simple task to determine the topology of the bands.","Similarly to the SSH model, we show that there is a tunable parameter that induces topological phase transitions from trivial to non-trivial.","In our case, it is the distance $d$ between the graphene sheet and the metallic grating.","We note that $d$ is a parameter that can be easily tuned experimentally simply by controlling the thickness of the spacer between the grating and the graphene sheet.","It is then experimentally feasible to engineer devices with the required topological properties."],"url":"http://arxiv.org/abs/2404.19576v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 13:55:30","title":"Causal Perception Inspired Representation Learning for Trustworthy Image Quality Assessment","abstract":"Despite great success in modeling visual perception, deep neural network based image quality assessment (IQA) still remains unreliable in real-world applications due to its vulnerability to adversarial perturbations and the inexplicit black-box structure. In this paper, we propose to build a trustworthy IQA model via Causal Perception inspired Representation Learning (CPRL), and a score reflection attack method for IQA model. More specifically, we assume that each image is composed of Causal Perception Representation (CPR) and non-causal perception representation (N-CPR). CPR serves as the causation of the subjective quality label, which is invariant to the imperceptible adversarial perturbations. Inversely, N-CPR presents spurious associations with the subjective quality label, which may significantly change with the adversarial perturbations. To extract the CPR from each input image, we develop a soft ranking based channel-wise activation function to mediate the causally sufficient (beneficial for high prediction accuracy) and necessary (beneficial for high robustness) deep features, and based on intervention employ minimax game to optimize. Experiments on four benchmark databases show that the proposed CPRL method outperforms many state-of-the-art adversarial defense methods and provides explicit model interpretation.","sentences":["Despite great success in modeling visual perception, deep neural network based image quality assessment (IQA) still remains unreliable in real-world applications due to its vulnerability to adversarial perturbations and the inexplicit black-box structure.","In this paper, we propose to build a trustworthy IQA model via Causal Perception inspired Representation Learning (CPRL), and a score reflection attack method for IQA model.","More specifically, we assume that each image is composed of Causal Perception Representation (CPR) and non-causal perception representation (N-CPR).","CPR serves as the causation of the subjective quality label, which is invariant to the imperceptible adversarial perturbations.","Inversely, N-CPR presents spurious associations with the subjective quality label, which may significantly change with the adversarial perturbations.","To extract the CPR from each input image, we develop a soft ranking based channel-wise activation function to mediate the causally sufficient (beneficial for high prediction accuracy) and necessary (beneficial for high robustness) deep features, and based on intervention employ minimax game to optimize.","Experiments on four benchmark databases show that the proposed CPRL method outperforms many state-of-the-art adversarial defense methods and provides explicit model interpretation."],"url":"http://arxiv.org/abs/2404.19567v1","category":"cs.CV"}
{"created":"2024-04-30 13:39:26","title":"Neural Dynamic Data Valuation","abstract":"Data constitute the foundational component of the data economy and its marketplaces. Efficient and fair data valuation has emerged as a topic of significant interest.\\ Many approaches based on marginal contribution have shown promising results in various downstream tasks. However, they are well known to be computationally expensive as they require training a large number of utility functions, which are used to evaluate the usefulness or value of a given dataset for a specific purpose. As a result, it has been recognized as infeasible to apply these methods to a data marketplace involving large-scale datasets. Consequently, a critical issue arises: how can the re-training of the utility function be avoided? To address this issue, we propose a novel data valuation method from the perspective of optimal control, named the neural dynamic data valuation (NDDV). Our method has solid theoretical interpretations to accurately identify the data valuation via the sensitivity of the data optimal control state. In addition, we implement a data re-weighting strategy to capture the unique features of data points, ensuring fairness through the interaction between data points and the mean-field states. Notably, our method requires only training once to estimate the value of all data points, significantly improving the computational efficiency. We conduct comprehensive experiments using different datasets and tasks. The results demonstrate that the proposed NDDV method outperforms the existing state-of-the-art data valuation methods in accurately identifying data points with either high or low values and is more computationally efficient.","sentences":["Data constitute the foundational component of the data economy and its marketplaces.","Efficient and fair data valuation has emerged as a topic of significant interest.\\ Many approaches based on marginal contribution have shown promising results in various downstream tasks.","However, they are well known to be computationally expensive as they require training a large number of utility functions, which are used to evaluate the usefulness or value of a given dataset for a specific purpose.","As a result, it has been recognized as infeasible to apply these methods to a data marketplace involving large-scale datasets.","Consequently, a critical issue arises: how can the re-training of the utility function be avoided?","To address this issue, we propose a novel data valuation method from the perspective of optimal control, named the neural dynamic data valuation (NDDV).","Our method has solid theoretical interpretations to accurately identify the data valuation via the sensitivity of the data optimal control state.","In addition, we implement a data re-weighting strategy to capture the unique features of data points, ensuring fairness through the interaction between data points and the mean-field states.","Notably, our method requires only training once to estimate the value of all data points, significantly improving the computational efficiency.","We conduct comprehensive experiments using different datasets and tasks.","The results demonstrate that the proposed NDDV method outperforms the existing state-of-the-art data valuation methods in accurately identifying data points with either high or low values and is more computationally efficient."],"url":"http://arxiv.org/abs/2404.19557v1","category":"stat.ML"}
{"created":"2024-04-30 13:12:36","title":"Physics-Informed Machine Learning On Polar Ice: A Survey","abstract":"The mass loss of the polar ice sheets contributes considerably to ongoing sea-level rise and changing ocean circulation, leading to coastal flooding and risking the homes and livelihoods of tens of millions of people globally. To address the complex problem of ice behavior, physical models and data-driven models have been proposed in the literature. Although traditional physical models can guarantee physically meaningful results, they have limitations in producing high-resolution results. On the other hand, data-driven approaches require large amounts of high-quality and labeled data, which is rarely available in the polar regions. Hence, as a promising framework that leverages the advantages of physical models and data-driven methods, physics-informed machine learning (PIML) has been widely studied in recent years. In this paper, we review the existing algorithms of PIML, provide our own taxonomy based on the methods of combining physics and data-driven approaches, and analyze the advantages of PIML in the aspects of accuracy and efficiency. Further, our survey discusses some current challenges and highlights future opportunities, including PIML on sea ice studies, PIML with different combination methods and backbone networks, and neural operator methods.","sentences":["The mass loss of the polar ice sheets contributes considerably to ongoing sea-level rise and changing ocean circulation, leading to coastal flooding and risking the homes and livelihoods of tens of millions of people globally.","To address the complex problem of ice behavior, physical models and data-driven models have been proposed in the literature.","Although traditional physical models can guarantee physically meaningful results, they have limitations in producing high-resolution results.","On the other hand, data-driven approaches require large amounts of high-quality and labeled data, which is rarely available in the polar regions.","Hence, as a promising framework that leverages the advantages of physical models and data-driven methods, physics-informed machine learning (PIML) has been widely studied in recent years.","In this paper, we review the existing algorithms of PIML, provide our own taxonomy based on the methods of combining physics and data-driven approaches, and analyze the advantages of PIML in the aspects of accuracy and efficiency.","Further, our survey discusses some current challenges and highlights future opportunities, including PIML on sea ice studies, PIML with different combination methods and backbone networks, and neural operator methods."],"url":"http://arxiv.org/abs/2404.19536v1","category":"cs.LG"}
{"created":"2024-04-30 13:11:32","title":"Ferroelectrically-enhanced Schottky barrier transistors for Logic-in-Memory applications","abstract":"Artificial neural networks (ANNs) have had an enormous impact on a multitude of sectors, from research to industry, generating an unprecedented demand for tailor-suited hardware platforms. Their training and execution is highly memory-intensive, clearly evidencing the limitations affecting the currently available hardware based on the von Neumann architecture, which requires frequent data shuttling due to the physical separation of logic and memory units. This does not only limit the achievable performances but also greatly increases the energy consumption, hindering the integration of ANNs into low-power platforms. New Logic in Memory (LiM) architectures, able to unify memory and logic functionalities into a single component, are highly promising for overcoming these limitations, by drastically reducing the need of data transfers. Recently, it has been shown that a very flexible platform for logic applications can be realized recurring to a multi-gated Schottky-Barrier Field Effect Transistor (SBFET). If equipped with memory capabilities, this architecture could represent an ideal building block for versatile LiM hardware. To reach this goal, here we investigate the integration of a ferroelectric Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) layer onto Dual Top Gated SBFETs. We demonstrate that HZO polarization charges can be successfully employed to tune the height of the two Schottky barriers, influencing the injection behavior, thus defining the transistor mode, switching it between n and p-type transport. The modulation strength is strongly dependent on the polarization pulse height, allowing for the selection of multiple current levels. All these achievable states can be well retained over time, thanks to the HZO stability. The presented result show how ferroelectric-enhanced SBFETs are promising for the realization of novel LiM hardware, enabling low-power circuits for ANNs execution.","sentences":["Artificial neural networks (ANNs) have had an enormous impact on a multitude of sectors, from research to industry, generating an unprecedented demand for tailor-suited hardware platforms.","Their training and execution is highly memory-intensive, clearly evidencing the limitations affecting the currently available hardware based on the von Neumann architecture, which requires frequent data shuttling due to the physical separation of logic and memory units.","This does not only limit the achievable performances but also greatly increases the energy consumption, hindering the integration of ANNs into low-power platforms.","New Logic in Memory (LiM) architectures, able to unify memory and logic functionalities into a single component, are highly promising for overcoming these limitations, by drastically reducing the need of data transfers.","Recently, it has been shown that a very flexible platform for logic applications can be realized recurring to a multi-gated Schottky-Barrier Field Effect Transistor (SBFET).","If equipped with memory capabilities, this architecture could represent an ideal building block for versatile LiM hardware.","To reach this goal, here we investigate the integration of a ferroelectric Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) layer onto Dual Top Gated SBFETs.","We demonstrate that HZO polarization charges can be successfully employed to tune the height of the two Schottky barriers, influencing the injection behavior, thus defining the transistor mode, switching it between n and p-type transport.","The modulation strength is strongly dependent on the polarization pulse height, allowing for the selection of multiple current levels.","All these achievable states can be well retained over time, thanks to the HZO stability.","The presented result show how ferroelectric-enhanced SBFETs are promising for the realization of novel LiM hardware, enabling low-power circuits for ANNs execution."],"url":"http://arxiv.org/abs/2404.19535v1","category":"physics.app-ph"}
{"created":"2024-04-30 13:09:41","title":"MoST: Multi-modality Scene Tokenization for Motion Prediction","abstract":"Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.","sentences":["Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights.","This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions).","An alternative paradigm is end-to-end learning from raw sensors.","However, this approach suffers from the lack of interpretability and requires significantly more training resources.","In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner.","The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information.","Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures.","To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings.","Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art."],"url":"http://arxiv.org/abs/2404.19531v1","category":"cs.CV"}
{"created":"2024-04-30 12:56:14","title":"MicroDreamer: Zero-shot 3D Generation in $\\sim$20 Seconds by Score-based Iterative Reconstruction","abstract":"Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample. In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model. Given the images produced by the diffusion model, SIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single optimization in SDS, mimicking the 3D reconstruction process. With other improvements including optimization in the pixel space, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks. In particular, retaining a comparable performance, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field and takes about 20 seconds to generate meshes from 3D Gaussian splitting on a single A100 GPU, halving the time of the fastest zero-shot baseline, DreamGaussian. Our code is available at https://github.com/ML-GSAI/MicroDreamer.","sentences":["Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample.","In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model.","Given the images produced by the diffusion model, SIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single optimization in SDS, mimicking the 3D reconstruction process.","With other improvements including optimization in the pixel space, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks.","In particular, retaining a comparable performance, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field and takes about 20 seconds to generate meshes from 3D Gaussian splitting on a single A100 GPU, halving the time of the fastest zero-shot baseline, DreamGaussian.","Our code is available at https://github.com/ML-GSAI/MicroDreamer."],"url":"http://arxiv.org/abs/2404.19525v1","category":"cs.CV"}
{"created":"2024-04-30 12:49:54","title":"Generating Robust Counterfactual Witnesses for Graph Neural Networks","abstract":"This paper introduces a new class of explanation structures, called robust counterfactual witnesses (RCWs), to provide robust, both counterfactual and factual explanations for graph neural networks. Given a graph neural network M, a robust counterfactual witness refers to the fraction of a graph G that are counterfactual and factual explanation of the results of M over G, but also remains so for any \"disturbed\" G by flipping up to k of its node pairs. We establish the hardness results, from tractable results to co-NP-hardness, for verifying and generating robust counterfactual witnesses. We study such structures for GNN-based node classification, and present efficient algorithms to verify and generate RCWs. We also provide a parallel algorithm to verify and generate RCWs for large graphs with scalability guarantees. We experimentally verify our explanation generation process for benchmark datasets, and showcase their applications.","sentences":["This paper introduces a new class of explanation structures, called robust counterfactual witnesses (RCWs), to provide robust, both counterfactual and factual explanations for graph neural networks.","Given a graph neural network M, a robust counterfactual witness refers to the fraction of a graph G that are counterfactual and factual explanation of the results of M over G, but also remains so for any \"disturbed\" G by flipping up to k of its node pairs.","We establish the hardness results, from tractable results to co-NP-hardness, for verifying and generating robust counterfactual witnesses.","We study such structures for GNN-based node classification, and present efficient algorithms to verify and generate RCWs.","We also provide a parallel algorithm to verify and generate RCWs for large graphs with scalability guarantees.","We experimentally verify our explanation generation process for benchmark datasets, and showcase their applications."],"url":"http://arxiv.org/abs/2404.19519v1","category":"cs.LG"}
{"created":"2024-04-30 12:47:42","title":"Inexact subgradient methods for semialgebraic functions","abstract":"Motivated by the widespread use of approximate derivatives in machine learning and optimization, we study inexact subgradient methods with non-vanishing additive errors and step sizes. In the nonconvex semialgebraic setting, under boundedness assumptions, we prove that the method provides points that eventually fluctuate close to the critical set at a distance proportional to $\\epsilon^\\rho$ where $\\epsilon$ is the error in subgradient evaluation and $\\rho$ relates to the geometry of the problem. In the convex setting, we provide complexity results for the averaged values. We also obtain byproducts of independent interest, such as descent-like lemmas for nonsmooth nonconvex problems and some results on the limit of affine interpolants of differential inclusions.","sentences":["Motivated by the widespread use of approximate derivatives in machine learning and optimization, we study inexact subgradient methods with non-vanishing additive errors and step sizes.","In the nonconvex semialgebraic setting, under boundedness assumptions, we prove that the method provides points that eventually fluctuate close to the critical set at a distance proportional to $\\epsilon^\\rho$ where $\\epsilon$ is the error in subgradient evaluation and $\\rho$ relates to the geometry of the problem.","In the convex setting, we provide complexity results for the averaged values.","We also obtain byproducts of independent interest, such as descent-like lemmas for nonsmooth nonconvex problems and some results on the limit of affine interpolants of differential inclusions."],"url":"http://arxiv.org/abs/2404.19517v1","category":"math.OC"}
{"created":"2024-04-30 12:45:47","title":"Chirality and odd mechanics in active columnar phases","abstract":"Chiral active materials display odd dynamical effects in both their elastic and viscous responses. We show that the most symmetric mesophase with two-dimensional odd elasticity in three dimensions is chiral, polar and columnar, with two-dimensional translational order in the plane perpendicular to the columns and no elastic restoring force for their relative sliding. We derive its hydrodynamic equations from those of a chiral active variant of model H. The most striking prediction of the odd dynamics is two distinct types of column oscillation whose frequencies do not vanish at zero wavenumber. In addition, activity leads to a buckling instability coming from the generic force-dipole active stress analogous to the mechanical Helfrich-Hurault instability in passive materials, while the chiral torque-dipole active stress fundamentally modifies the instability by the selection of helical column undulations.","sentences":["Chiral active materials display odd dynamical effects in both their elastic and viscous responses.","We show that the most symmetric mesophase with two-dimensional odd elasticity in three dimensions is chiral, polar and columnar, with two-dimensional translational order in the plane perpendicular to the columns and no elastic restoring force for their relative sliding.","We derive its hydrodynamic equations from those of a chiral active variant of model H. The most striking prediction of the odd dynamics is two distinct types of column oscillation whose frequencies do not vanish at zero wavenumber.","In addition, activity leads to a buckling instability coming from the generic force-dipole active stress analogous to the mechanical Helfrich-Hurault instability in passive materials, while the chiral torque-dipole active stress fundamentally modifies the instability by the selection of helical column undulations."],"url":"http://arxiv.org/abs/2404.19514v1","category":"cond-mat.soft"}
{"created":"2024-04-30 12:43:11","title":"Temporal Graph ODEs for Irregularly-Sampled Time Series","abstract":"Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations. To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced. We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks.","sentences":["Modern graph representation learning works mostly under the assumption of dealing with regularly sampled temporal graph snapshots, which is far from realistic, e.g., social networks and physical systems are characterized by continuous dynamics and sporadic observations.","To address this limitation, we introduce the Temporal Graph Ordinary Differential Equation (TG-ODE) framework, which learns both the temporal and spatial dynamics from graph streams where the intervals between observations are not regularly spaced.","We empirically validate the proposed approach on several graph benchmarks, showing that TG-ODE can achieve state-of-the-art performance in irregular graph stream tasks."],"url":"http://arxiv.org/abs/2404.19508v1","category":"cs.LG"}
{"created":"2024-04-30 12:36:28","title":"Well-posedness of McKean-Vlasov SDEs with density-dependent drift","abstract":"In this paper, we study the well-posedness of McKean-Vlasov stochastic differential equations (SDE) whose drift depends pointwisely on marginal density and satisfies a condition about local integrability in time-space variables. The drift is assumed to be Lipschitz continuous in distribution variable with respect to Wasserstein metric $W_p$. Our approach is by approximation with mollified SDEs. We establish a new estimate about H{\\\"o}lder continuity in time of marginal density. Then we deduce that the marginal distributions (resp. marginal densities) of the mollified SDEs converge in $W_p$ (resp. topology of compact convergence) to the solution of the Fokker-Planck equation associated with the density-dependent SDE. We prove strong existence of a solution. Weak and strong uniqueness are obtained when $p=1$, the drift coefficient is bounded, and the diffusion coefficient is distribution free.","sentences":["In this paper, we study the well-posedness of McKean-Vlasov stochastic differential equations (SDE) whose drift depends pointwisely on marginal density and satisfies a condition about local integrability in time-space variables.","The drift is assumed to be Lipschitz continuous in distribution variable with respect to Wasserstein metric $W_p$. Our approach is by approximation with mollified SDEs.","We establish a new estimate about H{\\\"o}lder continuity in time of marginal density.","Then we deduce that the marginal distributions (resp.","marginal densities) of the mollified SDEs converge in $W_p$ (resp.","topology of compact convergence) to the solution of the Fokker-Planck equation associated with the density-dependent SDE.","We prove strong existence of a solution.","Weak and strong uniqueness are obtained when $p=1$, the drift coefficient is bounded, and the diffusion coefficient is distribution free."],"url":"http://arxiv.org/abs/2404.19499v1","category":"math.PR"}
{"created":"2024-04-30 12:33:41","title":"Into the MAG-verse or: Cosmology of the Complete Quadratic Metric-Affine Gravity","abstract":"We study the cosmology of the complete quadratic (in torsion and nonmetricity) metric-affine gravity. Namely, we add to the scalar-curvature gravitational Lagrangian, the 17 independent quadratic (parity-even and parity-odd) torsion and nonmetricity invariants. Sticking to a homogeneous and isotropic Friedmann-Robertson-Walker spacetime and assuming a perfect hyperfluid source, we explore the new effects that torsion and nonmetricity bring into play. It is shown that the inclusion of these invariants offers rich phenomenology. In particular, some well-known examples of exotic matter like cosmic strings, domain walls, stiff matter, etc., emerge quite naturally as manifestations of the fluid's intrinsic structure (hypermomentum). By studying the extended Friedmann equations in the complete quadratic theory and isolating the various parts of the hypermomentum, we find a plethora of solutions with interesting features.","sentences":["We study the cosmology of the complete quadratic (in torsion and nonmetricity) metric-affine gravity.","Namely, we add to the scalar-curvature gravitational Lagrangian, the 17 independent quadratic (parity-even and parity-odd) torsion and nonmetricity invariants.","Sticking to a homogeneous and isotropic Friedmann-Robertson-Walker spacetime and assuming a perfect hyperfluid source, we explore the new effects that torsion and nonmetricity bring into play.","It is shown that the inclusion of these invariants offers rich phenomenology.","In particular, some well-known examples of exotic matter like cosmic strings, domain walls, stiff matter, etc., emerge quite naturally as manifestations of the fluid's intrinsic structure (hypermomentum).","By studying the extended Friedmann equations in the complete quadratic theory and isolating the various parts of the hypermomentum, we find a plethora of solutions with interesting features."],"url":"http://arxiv.org/abs/2404.19498v1","category":"gr-qc"}
{"created":"2024-04-30 12:24:07","title":"Spherically symmetric Einstein-scalar-field equations for slowly particle-like decaying null infinity","abstract":"We show that the spherically symmetric Einstein-scalar-field equations for small slowly particle-like decaying initial data at null infinity have unique global solutions.","sentences":["We show that the spherically symmetric Einstein-scalar-field equations for small slowly particle-like decaying initial data at null infinity have unique global solutions."],"url":"http://arxiv.org/abs/2404.19493v1","category":"gr-qc"}
{"created":"2024-04-30 12:22:50","title":"Fokker-Planck equation for McKean-Vlasov SPDEs driven by time-space Brownian sheet","abstract":"In this paper, we consider a McKean-Vlasov (mean-field) stochastic partial differential equations (SPDEs) driven by a Brownian sheet. We study the propagation of chaos for a space-time Ornstein-Uhlenbeck SPDE type. Subsequently, we prove the existence and uniqueness of a nonlinear McKean-Vlasov SPDE. Finally, we establish a Fokker-Planck equation for the law of the solution of the McKean-Vlasov type SPDE driven by a time-space Brownian sheet, and we provide some examples to illustrate the results obtained.","sentences":["In this paper, we consider a McKean-Vlasov (mean-field) stochastic partial differential equations (SPDEs) driven by a Brownian sheet.","We study the propagation of chaos for a space-time Ornstein-Uhlenbeck SPDE type.","Subsequently, we prove the existence and uniqueness of a nonlinear McKean-Vlasov SPDE.","Finally, we establish a Fokker-Planck equation for the law of the solution of the McKean-Vlasov type SPDE driven by a time-space Brownian sheet, and we provide some examples to illustrate the results obtained."],"url":"http://arxiv.org/abs/2404.19490v1","category":"math.PR"}
{"created":"2024-04-30 12:18:47","title":"EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision","abstract":"Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud. As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems. However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications. While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators. In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras. It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme enabling the low-latency execution of multi-layer GNNs. We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge.","sentences":["Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud.","As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems.","However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications.","While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators.","In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras.","It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme enabling the low-latency execution of multi-layer GNNs.","We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge."],"url":"http://arxiv.org/abs/2404.19489v1","category":"cs.CV"}
{"created":"2024-04-30 11:46:43","title":"Non-Gaussian statistics in static and dynamic Galton boards","abstract":"Perturbing the arrangements of pegs on a static Galton board can result in non-trivial stationary distributions, which in the continuum limit correspond to departure from regular gaussian behavior. Two such distributions are obtained. Further, the distributions generated for a dynamic galton board under external forcing in a general direction are obtained by solution of the corresponding stochastic differential equations. Exact cumulant generating functions for the distribution are presented for forcing in one dimension. An approximate expression, correct to first order in the forcing amplitude, is presented for the case of two dimensions. Both cases show nontrivial departures from the static gaussian solution.","sentences":["Perturbing the arrangements of pegs on a static Galton board can result in non-trivial stationary distributions, which in the continuum limit correspond to departure from regular gaussian behavior.","Two such distributions are obtained.","Further, the distributions generated for a dynamic galton board under external forcing in a general direction are obtained by solution of the corresponding stochastic differential equations.","Exact cumulant generating functions for the distribution are presented for forcing in one dimension.","An approximate expression, correct to first order in the forcing amplitude, is presented for the case of two dimensions.","Both cases show nontrivial departures from the static gaussian solution."],"url":"http://arxiv.org/abs/2404.19478v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 11:37:10","title":"New phenomenology in the first-order thermodynamics of scalar-tensor gravity for Bianchi universes","abstract":"The phase space of Bianchi I universes in vacuum Brans-Dicke gravity is analyzed in terms of physical variables. The behaviour of the solutions of the field equations near the fixed points (which are solutions of Einstein gravity) is compared with basic ideas of the recent first-order thermodynamics of scalar-tensor gravity, elucidating new phenomenology.","sentences":["The phase space of Bianchi I universes in vacuum Brans-Dicke gravity is analyzed in terms of physical variables.","The behaviour of the solutions of the field equations near the fixed points (which are solutions of Einstein gravity) is compared with basic ideas of the recent first-order thermodynamics of scalar-tensor gravity, elucidating new phenomenology."],"url":"http://arxiv.org/abs/2404.19470v1","category":"gr-qc"}
{"created":"2024-04-30 11:14:46","title":"Computing Borel complexity of some geometrical properties in Banach spaces","abstract":"We compute the Borel complexity of some classes of Banach spaces such as different versions of diameter two properties, spaces satisfying the Daugavet equation or spaces with an octahedral norm. In most of the above cases our computation is even optimal, which completes the research done during the last years around this topic for isomorphism classes of Banach spaces.","sentences":["We compute the Borel complexity of some classes of Banach spaces such as different versions of diameter two properties, spaces satisfying the Daugavet equation or spaces with an octahedral norm.","In most of the above cases our computation is even optimal, which completes the research done during the last years around this topic for isomorphism classes of Banach spaces."],"url":"http://arxiv.org/abs/2404.19457v1","category":"math.FA"}
{"created":"2024-04-30 11:11:50","title":"Bifurcations and explicit unfoldings of grazing loops connecting one high multiplicity tangent point","abstract":"For piecewise-smooth differential systems, in this paper we focus on crossing limit cycles and sliding loops bifurcating from a grazing loop connecting one high multiplicity tangent point. For the low multiplicity cases considered in previous publications, the method is to define and analyze return maps following the classic idea of Poincar\\'e. However, high multiplicity leads to that either domains or properties of return maps are unclear under perturbations. To overcome these difficulties, we unfold grazing loops by functional parameters and functional functions, and analyze this unfolding along some specific parameter curve. Relationships between multiplicity and the numbers of crossing limit cycles and sliding loops are given, and our results not only generalize the results obtained in [J. Differential Equations 255(2013), 4403-4436; 269(2020), 11396-11434], but also are new for some specific grazing loops.","sentences":["For piecewise-smooth differential systems, in this paper we focus on crossing limit cycles and sliding loops bifurcating from a grazing loop connecting one high multiplicity tangent point.","For the low multiplicity cases considered in previous publications, the method is to define and analyze return maps following the classic idea of Poincar\\'e.","However, high multiplicity leads to that either domains or properties of return maps are unclear under perturbations.","To overcome these difficulties, we unfold grazing loops by functional parameters and functional functions, and analyze this unfolding along some specific parameter curve.","Relationships between multiplicity and the numbers of crossing limit cycles and sliding loops are given, and our results not only generalize the results obtained in [J. Differential Equations 255(2013), 4403-4436; 269(2020), 11396-11434], but also are new for some specific grazing loops."],"url":"http://arxiv.org/abs/2404.19455v1","category":"math.DS"}
{"created":"2024-04-30 10:46:26","title":"Rediscussion of eclipsing binaries. Paper XIX. The long-period solar-type system V454 Aurigae","abstract":"V454 Aur is an eclipsing binary system containing two solar-type stars on an orbit of relatively long period (P = 27.02 d) and large eccentricity (e = 0.381). Eclipses were detected using data from the Hipparcos satellite, and a high-quality double-lined spectroscopic orbit has been presented by Griffin (2001). The NASA Transiting Exoplanet Survey Satellite (TESS) has observed the system during eight sectors, capturing ten eclipses in their entirety. V454 Aur is unusual in that the primary star - the star eclipsed at the deeper minimum - is less massive, smaller \\emph{and} cooler than its companion. This phenomenon can occur in certain configurations of eccentric orbits when the stars are closer together at the primary eclipse, causing a larger area to be eclipsed than at the secondary. We use the radial velocity measurements from Griffin and the light curves from TESS to determine the masses and radii of the component stars for the first time, finding masses of 1.034 +/- 0.006 Msun and 1.161 +/- 0.008 Msun, and radii of 0.979 +/- 0.003 Rsun and 1.211 +/- 0.003 Rsun. Our measurement of the distance to the system is consistent with that from the Gaia DR3 parallax. A detailed spectroscopic study to determine chemical abundances and more precise temperatures is encouraged. Finally, we present equations to derive the effective temperatures of the stars from the inferred temperature of the system as a whole, plus the ratio of the radii and either the surface brightness or light ratio of the stars.","sentences":["V454 Aur is an eclipsing binary system containing two solar-type stars on an orbit of relatively long period (P = 27.02 d) and large eccentricity (e = 0.381).","Eclipses were detected using data from the Hipparcos satellite, and a high-quality double-lined spectroscopic orbit has been presented by Griffin (2001).","The NASA Transiting Exoplanet Survey Satellite (TESS) has observed the system during eight sectors, capturing ten eclipses in their entirety.","V454 Aur is unusual in that the primary star - the star eclipsed at the deeper minimum - is less massive, smaller \\emph{and} cooler than its companion.","This phenomenon can occur in certain configurations of eccentric orbits when the stars are closer together at the primary eclipse, causing a larger area to be eclipsed than at the secondary.","We use the radial velocity measurements from Griffin and the light curves from TESS to determine the masses and radii of the component stars for the first time, finding masses of 1.034 +/- 0.006 Msun and 1.161 +/- 0.008 Msun, and radii of 0.979 +/- 0.003 Rsun and 1.211 +/- 0.003 Rsun.","Our measurement of the distance to the system is consistent with that from the Gaia DR3 parallax.","A detailed spectroscopic study to determine chemical abundances and more precise temperatures is encouraged.","Finally, we present equations to derive the effective temperatures of the stars from the inferred temperature of the system as a whole, plus the ratio of the radii and either the surface brightness or light ratio of the stars."],"url":"http://arxiv.org/abs/2404.19443v1","category":"astro-ph.SR"}
{"created":"2024-04-30 10:44:33","title":"ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers","abstract":"Existing neural audio codecs usually sacrifice computational complexity for audio quality. They build the feature transformation layers mainly on convolutional blocks, which are not inherently appropriate for capturing local redundancies of audio signals. As compensation, either adversarial losses from a discriminator or a large number of model parameters are required to improve the codec. To that end, we propose Efficient Speech Codec (ESC), a lightweight parameter-efficient codec laid on cross-scale residual vector quantization and transformers. Our model leverages mirrored hierarchical window-attention transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance codebook utilization, we design a learning paradigm that involves a pre-training stage to assist with codec training. Extensive results show that ESC can achieve high audio quality with much lower complexity, which is a prospective alternative in place of existing codecs.","sentences":["Existing neural audio codecs usually sacrifice computational complexity for audio quality.","They build the feature transformation layers mainly on convolutional blocks, which are not inherently appropriate for capturing local redundancies of audio signals.","As compensation, either adversarial losses from a discriminator or a large number of model parameters are required to improve the codec.","To that end, we propose Efficient Speech Codec (ESC), a lightweight parameter-efficient codec laid on cross-scale residual vector quantization and transformers.","Our model leverages mirrored hierarchical window-attention transformer blocks and performs step-wise decoding from coarse-to-fine feature representations.","To enhance codebook utilization, we design a learning paradigm that involves a pre-training stage to assist with codec training.","Extensive results show that ESC can achieve high audio quality with much lower complexity, which is a prospective alternative in place of existing codecs."],"url":"http://arxiv.org/abs/2404.19441v1","category":"cs.SD"}
{"created":"2024-04-30 10:41:27","title":"Invariant divisors and equivariant line bundles","abstract":"Scalar relative invariants play an important role in the theory of group actions on a manifold as their zero sets are invariant hypersurfaces. Relative invariants are central in many applications, where they often are treated locally since an invariant hypersurface may not be a locus of a single function. Our aim is to establish a global theory of relative invariants.   For a Lie algebra $\\mathfrak{g}$ of holomorphic vector fields on a complex manifold $M$, any holomorphic $\\mathfrak{g}$-invariant hypersurface is given in terms of a $\\mathfrak{g}$-invariant divisor. This generalizes the classical notion of scalar relative $\\mathfrak{g}$-invariant. Any $\\mathfrak{g}$-invariant divisor gives rise to a $\\mathfrak{g}$-equivariant line bundle, and a large part of this paper is therefore devoted to the investigation of the group $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ of $\\mathfrak{g}$-equivariant line bundles. We give a cohomological description of $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ in terms of a double complex interpolating the Chevalley-Eilenberg complex for $\\mathfrak{g}$ with the \\v{C}ech complex of the sheaf of holomorphic functions on $M$.   We also obtain results about polynomial divisors on affine bundles and jet bundles. This has applications to the theory of differential invariants. Those were actively studied in relation to invariant differential equations, but the description of multipliers (or weights) of relative differential invariants was an open problem. We derive a characterization of them with our general theory. Examples, including projective geometry of curves and second-order ODEs, not only illustrate the developed machinery, but also give another approach and rigorously justify some classical computations. At the end, we briefly discuss generalizations of this theory.","sentences":["Scalar relative invariants play an important role in the theory of group actions on a manifold as their zero sets are invariant hypersurfaces.","Relative invariants are central in many applications, where they often are treated locally since an invariant hypersurface may not be a locus of a single function.","Our aim is to establish a global theory of relative invariants.   ","For a Lie algebra $\\mathfrak{g}$ of holomorphic vector fields on a complex manifold $M$, any holomorphic $\\mathfrak{g}$-invariant hypersurface is given in terms of a $\\mathfrak{g}$-invariant divisor.","This generalizes the classical notion of scalar relative $\\mathfrak{g}$-invariant.","Any $\\mathfrak{g}$-invariant divisor gives rise to a $\\mathfrak{g}$-equivariant line bundle, and a large part of this paper is therefore devoted to the investigation of the group $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ of $\\mathfrak{g}$-equivariant line bundles.","We give a cohomological description of $\\mathrm{Pic}_{\\mathfrak{g}}(M)$ in terms of a double complex interpolating the Chevalley-Eilenberg complex for $\\mathfrak{g}$ with the \\v{C}ech complex of the sheaf of holomorphic functions on $M$.   We also obtain results about polynomial divisors on affine bundles and jet bundles.","This has applications to the theory of differential invariants.","Those were actively studied in relation to invariant differential equations, but the description of multipliers (or weights) of relative differential invariants was an open problem.","We derive a characterization of them with our general theory.","Examples, including projective geometry of curves and second-order ODEs, not only illustrate the developed machinery, but also give another approach and rigorously justify some classical computations.","At the end, we briefly discuss generalizations of this theory."],"url":"http://arxiv.org/abs/2404.19439v1","category":"math.DG"}
{"created":"2024-04-30 10:41:23","title":"Neuro-Vision to Language: Image Reconstruction and Language enabled Interaction via Brain Recordings","abstract":"Decoding non-invasive brain recordings is crucial for advancing our understanding of human cognition, yet faces challenges from individual differences and complex neural signal representations. Traditional methods require custom models and extensive trials, and lack interpretability in visual reconstruction tasks. Our framework integrating integrates 3D brain structures with visual semantics by Vision Transformer 3D. The unified feature extractor aligns fMRI features with multiple levels of visual embeddings efficiently, removing the need for individual-specific models and allowing extraction from single-trial data. This extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with various fMRI-image related textual data to support multimodal large model development. The integration with LLMs enhances decoding capabilities, enabling tasks like brain captioning, question-answering, detailed descriptions, complex reasoning, and visual reconstruction. Our approach not only shows superior performance across these tasks but also precisely identifies and manipulates language-based concepts within brain signals, enhancing interpretability and providing deeper neural process insights. These advances significantly broaden non-invasive brain decoding applicability in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.","sentences":["Decoding non-invasive brain recordings is crucial for advancing our understanding of human cognition, yet faces challenges from individual differences and complex neural signal representations.","Traditional methods require custom models and extensive trials, and lack interpretability in visual reconstruction tasks.","Our framework integrating integrates 3D brain structures with visual semantics by Vision Transformer","3D.","The unified feature extractor aligns fMRI features with multiple levels of visual embeddings efficiently, removing the need for individual-specific models and allowing extraction from single-trial data.","This extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs).","Additionally, we have enhanced the fMRI dataset with various fMRI-image related textual data to support multimodal large model development.","The integration with LLMs enhances decoding capabilities, enabling tasks like brain captioning, question-answering, detailed descriptions, complex reasoning, and visual reconstruction.","Our approach not only shows superior performance across these tasks but also precisely identifies and manipulates language-based concepts within brain signals, enhancing interpretability and providing deeper neural process insights.","These advances significantly broaden non-invasive brain decoding applicability in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models."],"url":"http://arxiv.org/abs/2404.19438v2","category":"cs.NE"}
{"created":"2024-04-30 10:35:04","title":"Quintom cosmology and modified gravity after DESI 2024","abstract":"We reconstruct the cosmological background evolution under the scenario of dynamical dark energy through the Gaussian process approach, using the latest Dark Energy Spectroscopic Instrument (DESI) baryon acoustic oscillations (BAO) \\cite{DESI:2024mwx} combined with other observations. Our results reveal that the reconstructed dark-energy equation-of-state (EoS) parameter $w(z)$ exhibits the so-called quintom-B behavior, crossing $-1$ from phantom to quintessence regime as the universe expands. We investigate under what situation this type of evolution could be achieved from the perspectives of field theories and modified gravity. In particular, we reconstruct the corresponding actions for $f(R)$, $f(T)$, and $f(Q)$ gravity, respectively. We explicitly show that, certain modified gravity can exhibit the quintom dynamics and fit the recent DESI data efficiently, and for all cases the quadratic deviation from the $\\Lambda$CDM scenario is mildly favored.","sentences":["We reconstruct the cosmological background evolution under the scenario of dynamical dark energy through the Gaussian process approach, using the latest Dark Energy Spectroscopic Instrument (DESI) baryon acoustic oscillations (BAO) \\cite{DESI:2024mwx} combined with other observations.","Our results reveal that the reconstructed dark-energy equation-of-state (EoS) parameter $w(z)$ exhibits the so-called quintom-B behavior, crossing $-1$ from phantom to quintessence regime as the universe expands.","We investigate under what situation this type of evolution could be achieved from the perspectives of field theories and modified gravity.","In particular, we reconstruct the corresponding actions for $f(R)$, $f(T)$, and $f(Q)$ gravity, respectively.","We explicitly show that, certain modified gravity can exhibit the quintom dynamics and fit the recent DESI data efficiently, and for all cases the quadratic deviation from the $\\Lambda$CDM scenario is mildly favored."],"url":"http://arxiv.org/abs/2404.19437v1","category":"astro-ph.CO"}
{"created":"2024-04-30 10:12:21","title":"On wave systems with antisymmetric potential in dimension d >= 4 and well-posedness for (half-)wave maps","abstract":"We prove a priori estimates for wave systems of the type \\[ \\partial_{tt} u - \\Delta u = \\Omega \\cdot du + F(u) \\quad \\text{in $\\mathbb{R}^d \\times \\mathbb{R}$} \\]   where $d \\geq 4$ and $\\Omega$ is a suitable antisymmetric potential. We show that the assumptions on $\\Omega$ are applicable to wave- and half-wave maps, the latter by means of the Krieger-Sire reduction. We thus obtain well-posedness of those equations for small initial data in $\\dot{H}^{\\frac{d}{2}}(\\mathbb{R}^d)$.","sentences":["We prove a priori estimates for wave systems of the type \\[ \\partial_{tt} u - \\Delta u = \\Omega \\cdot du + F(u) \\quad \\text{in $\\mathbb{R}^d \\times \\mathbb{R}$} \\]   where $d \\geq 4$ and $\\Omega$ is a suitable antisymmetric potential.","We show that the assumptions on $\\Omega$ are applicable to wave- and half-wave maps, the latter by means of the Krieger-Sire reduction.","We thus obtain well-posedness of those equations for small initial data in $\\dot{H}^{\\frac{d}{2}}(\\mathbb{R}^d)$."],"url":"http://arxiv.org/abs/2404.19421v1","category":"math.AP"}
{"created":"2024-04-30 10:02:58","title":"Fat equator effect and Minimality in immersions and submersions of the Sphere","abstract":"Inspired by the equatorial concentration of measure phenomenon in the sphere, a result which is deduced from the general, (and intrinsic), concentration of measure in $\\mathbb{S}^n(1)$, we describe in this paper an equatorial concentration of measure satisfied by the closed, (compact without boundary), isometric and minimal immersions $x:\\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\leq n$), and by the minimal Riemannian submersions $\\pi: \\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\geq n$).","sentences":["Inspired by the equatorial concentration of measure phenomenon in the sphere, a result which is deduced from the general, (and intrinsic), concentration of measure in $\\mathbb{S}^n(1)$, we describe in this paper an equatorial concentration of measure satisfied by the closed, (compact without boundary), isometric and minimal immersions $x:\\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\leq n$), and by the minimal Riemannian submersions $\\pi: \\Sigma^m \\rightarrow \\mathbb{S}^n(1)$, ($m \\geq n$)."],"url":"http://arxiv.org/abs/2404.19416v1","category":"math.DG"}
{"created":"2024-04-30 09:12:55","title":"Strength in numbers: A multiphase wind model with multiple cloud populations","abstract":"Galactic outflows have a multiphase nature making them challenging to model analytically. Many previous studies have tried to produce models that come closer to reality. In this work, we continue these efforts and describe the interaction of the hot wind fluid with multiple cold cloud populations, with their number density determined by different probability density functions. To do so, we introduced realistic cloud-wind interaction source terms and a time-varying cooling area. We find that the model reproduces well results from small-scale hydrodynamic simulations, but exhibits a general destructive behavior both for a single cloud population as well as multiple ones. We show that including multiple cloud populations can alter the evolution of the wind drastically. We also compare our model to observations and show that the differential acceleration of multiple clouds can lead to a non-negligible velocity `dispersion' relevant for down-the-barrel studies. Furthermore, we compute the emitted cooling surface brightness and find it generally too faint to explain observed Lyman-$\\alpha$ halos.","sentences":["Galactic outflows have a multiphase nature making them challenging to model analytically.","Many previous studies have tried to produce models that come closer to reality.","In this work, we continue these efforts and describe the interaction of the hot wind fluid with multiple cold cloud populations, with their number density determined by different probability density functions.","To do so, we introduced realistic cloud-wind interaction source terms and a time-varying cooling area.","We find that the model reproduces well results from small-scale hydrodynamic simulations, but exhibits a general destructive behavior both for a single cloud population as well as multiple ones.","We show that including multiple cloud populations can alter the evolution of the wind drastically.","We also compare our model to observations and show that the differential acceleration of multiple clouds can lead to a non-negligible velocity `dispersion' relevant for down-the-barrel studies.","Furthermore, we compute the emitted cooling surface brightness and find it generally too faint to explain observed Lyman-$\\alpha$ halos."],"url":"http://arxiv.org/abs/2404.19380v1","category":"astro-ph.GA"}
{"created":"2024-04-30 08:51:13","title":"Toward thermoelectric characterization of (nano)materials by in situ transmission electron microscopy","abstract":"We explore the possibility to perform an in situ transmission electron microscopy (TEM) thermoelectric characterization of materials. A differential heating element on a custom in situ TEM microchip allows to generate a temperature gradient across the studied materials, which are simultaneously measured electrically. A thermovoltage was induced in all studied devices, whose sign corresponds to the sign of the Seebeck coefficient of the tested materials. The results indicate that in situ thermoelectric TEM studies can help to profoundly understand the interplay between the thermoelectric properties on one side and the structure/composition of the materials down to the atomic level on the other side, including grain boundaries, dopants or crystal defects. We propose an improved in situ TEM microchip design, which should facilitate a full quantitative measurement of the induced temperature gradient, the electrical and thermal conductivities, as well as the Seebeck coefficient.","sentences":["We explore the possibility to perform an in situ transmission electron microscopy (TEM) thermoelectric characterization of materials.","A differential heating element on a custom in situ TEM microchip allows to generate a temperature gradient across the studied materials, which are simultaneously measured electrically.","A thermovoltage was induced in all studied devices, whose sign corresponds to the sign of the Seebeck coefficient of the tested materials.","The results indicate that in situ thermoelectric TEM studies can help to profoundly understand the interplay between the thermoelectric properties on one side and the structure/composition of the materials down to the atomic level on the other side, including grain boundaries, dopants or crystal defects.","We propose an improved in situ TEM microchip design, which should facilitate a full quantitative measurement of the induced temperature gradient, the electrical and thermal conductivities, as well as the Seebeck coefficient."],"url":"http://arxiv.org/abs/2404.19366v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 08:48:07","title":"Navigating Brain Language Representations: A Comparative Analysis of Neural Language Models and Psychologically Plausible Models","abstract":"Neural language models, particularly large-scale ones, have been consistently proven to be most effective in predicting brain neural activity across a range of studies. However, previous research overlooked the comparison of these models with psychologically plausible ones. Moreover, evaluations were reliant on limited, single-modality, and English cognitive datasets. To address these questions, we conducted an analysis comparing encoding performance of various neural language models and psychologically plausible models. Our study utilized extensive multi-modal cognitive datasets, examining bilingual word and discourse levels. Surprisingly, our findings revealed that psychologically plausible models outperformed neural language models across diverse contexts, encompassing different modalities such as fMRI and eye-tracking, and spanning languages from English to Chinese. Among psychologically plausible models, the one incorporating embodied information emerged as particularly exceptional. This model demonstrated superior performance at both word and discourse levels, exhibiting robust prediction of brain activation across numerous regions in both English and Chinese.","sentences":["Neural language models, particularly large-scale ones, have been consistently proven to be most effective in predicting brain neural activity across a range of studies.","However, previous research overlooked the comparison of these models with psychologically plausible ones.","Moreover, evaluations were reliant on limited, single-modality, and English cognitive datasets.","To address these questions, we conducted an analysis comparing encoding performance of various neural language models and psychologically plausible models.","Our study utilized extensive multi-modal cognitive datasets, examining bilingual word and discourse levels.","Surprisingly, our findings revealed that psychologically plausible models outperformed neural language models across diverse contexts, encompassing different modalities such as fMRI and eye-tracking, and spanning languages from English to Chinese.","Among psychologically plausible models, the one incorporating embodied information emerged as particularly exceptional.","This model demonstrated superior performance at both word and discourse levels, exhibiting robust prediction of brain activation across numerous regions in both English and Chinese."],"url":"http://arxiv.org/abs/2404.19364v1","category":"cs.CL"}
{"created":"2024-04-30 08:45:55","title":"Global solution for the stochastic nonlinear Schr\u00f6dinger system with quadratic interaction in four dimensions","abstract":"We discuss the global existence of solutions to a system of stochastic Schr\\\"odinger equations with multiplicative noise. Our setting of the quadratic nonlinear terms in dimension 4 is $L^2$-critical. We treat the solutions under the ground state. We estimate the time derivative of the quantity of energy by using the cancellation of the cubic terms in the spatial derivative of the solution.","sentences":["We discuss the global existence of solutions to a system of stochastic Schr\\\"odinger equations with multiplicative noise.","Our setting of the quadratic nonlinear terms in dimension 4 is $L^2$-critical.","We treat the solutions under the ground state.","We estimate the time derivative of the quantity of energy by using the cancellation of the cubic terms in the spatial derivative of the solution."],"url":"http://arxiv.org/abs/2404.19362v1","category":"math.AP"}
{"created":"2024-04-30 08:28:03","title":"Deep Learning Forecasts Caldera Collapse Events at K\u012blauea Volcano","abstract":"During the three month long eruption of K\\=ilauea volcano, Hawaii in 2018, the pre-existing summit caldera collapsed in over 60 quasi-periodic failure events. The last 40 of these events, which generated Mw >5 very long period (VLP) earthquakes, had inter-event times between 0.8 - 2.2 days. These failure events offer a unique dataset for testing methods for predicting earthquake recurrence based on locally recorded GPS, tilt, and seismicity data. In this work, we train a deep learning graph neural network (GNN) to predict the time-to-failure of the caldera collapse events using only a fraction of the data recorded at the start of each cycle. We find that the GNN generalizes to unseen data and can predict the time-to-failure to within a few hours using only 0.5 days of data, substantially improving upon a null model based only on inter-event statistics. Predictions improve with increasing input data length, and are most accurate when using high-SNR tilt-meter data. Applying the trained GNN to synthetic data with different magma pressure decay times predicts failure at a nearly constant stress threshold, revealing that the GNN is sensing the underling physics of caldera collapse. These findings demonstrate the predictability of caldera collapse sequences under well monitored conditions, and highlight the potential of machine learning methods for forecasting real world catastrophic events with limited training data.","sentences":["During the three month long eruption of K\\=ilauea volcano, Hawaii in 2018, the pre-existing summit caldera collapsed in over 60 quasi-periodic failure events.","The last 40 of these events, which generated Mw >5 very long period (VLP) earthquakes, had inter-event times between 0.8 - 2.2 days.","These failure events offer a unique dataset for testing methods for predicting earthquake recurrence based on locally recorded GPS, tilt, and seismicity data.","In this work, we train a deep learning graph neural network (GNN) to predict the time-to-failure of the caldera collapse events using only a fraction of the data recorded at the start of each cycle.","We find that the GNN generalizes to unseen data and can predict the time-to-failure to within a few hours using only 0.5 days of data, substantially improving upon a null model based only on inter-event statistics.","Predictions improve with increasing input data length, and are most accurate when using high-SNR tilt-meter data.","Applying the trained GNN to synthetic data with different magma pressure decay times predicts failure at a nearly constant stress threshold, revealing that the GNN is sensing the underling physics of caldera collapse.","These findings demonstrate the predictability of caldera collapse sequences under well monitored conditions, and highlight the potential of machine learning methods for forecasting real world catastrophic events with limited training data."],"url":"http://arxiv.org/abs/2404.19351v1","category":"physics.geo-ph"}
{"created":"2024-04-30 08:11:14","title":"Connecting physics to systems with modular spin-circuits","abstract":"An emerging paradigm in modern electronics is that of CMOS + $\\sf X$ requiring the integration of standard CMOS technology with novel materials and technologies denoted by $\\sf X$. In this context, a crucial challenge is to develop accurate circuit models for $\\sf X$ that are compatible with standard models for CMOS-based circuits and systems. In this perspective we present physics-based, experimentally benchmarked modular circuit models that can be used to evaluate a class of CMOS + $\\sf X$ systems, where $\\sf X$ denotes magnetic and spintronic materials and phenomena. This class of materials is particularly challenging because they go beyond conventional charge-based phenomena and involve the spin degree of freedom which involves non-trivial quantum effects. Starting from density matrices $-$ the central quantity in quantum transport $-$ using well-defined approximations, it is possible to obtain spin-circuits that generalize ordinary circuit theory to 4-component currents and voltages (1 for charge and 3 for spin). With step-by-step examples that progressively go higher in the computing stack, we illustrate how the spin-circuit approach can be used to start from the physics of magnetism and spintronics to enable accurate system-level evaluations. We believe the core approach can be extended to include other quantum degrees of freedom like valley and pseudospins starting from corresponding density matrices.","sentences":["An emerging paradigm in modern electronics is that of CMOS + $\\sf X$ requiring the integration of standard CMOS technology with novel materials and technologies denoted by $\\sf X$. In this context, a crucial challenge is to develop accurate circuit models for $\\sf X$ that are compatible with standard models for CMOS-based circuits and systems.","In this perspective we present physics-based, experimentally benchmarked modular circuit models that can be used to evaluate a class of CMOS + $\\sf X$ systems, where $\\sf X$ denotes magnetic and spintronic materials and phenomena.","This class of materials is particularly challenging because they go beyond conventional charge-based phenomena and involve the spin degree of freedom which involves non-trivial quantum effects.","Starting from density matrices $-$ the central quantity in quantum transport $-$ using well-defined approximations, it is possible to obtain spin-circuits that generalize ordinary circuit theory to 4-component currents and voltages (1 for charge and 3 for spin).","With step-by-step examples that progressively go higher in the computing stack, we illustrate how the spin-circuit approach can be used to start from the physics of magnetism and spintronics to enable accurate system-level evaluations.","We believe the core approach can be extended to include other quantum degrees of freedom like valley and pseudospins starting from corresponding density matrices."],"url":"http://arxiv.org/abs/2404.19345v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 08:06:02","title":"Revisiting the Constraint on Equation of State of Neutron Star based on the Binary Neutron Star Mergers","abstract":"The merger of neutron star (NS)-NS binary can form different production of the compact remnant, among which the supramassive NS (SMNS) could create an internal plateau and the followed steep decay marks the collapse of the SMNS. The proportion of SMNS and the corresponding collapse-time are often used to constrain the NS equation of state (EoS). This paper revisits this topic by considering the effect of an accretion disk on the compact remnant, which is not considered in previous works. Compared with previous works, the collapse-time distribution (peaks $\\sim$100 s) of the SMNSs formed from NS-NS merger is almost unaffected by the initial surface magnetic ($B_{{\\rm s},i}$) of NS, but the total energy output of the magnetic dipole radiation from the SMNSs depends on $B_{{\\rm s},i}$ significantly. Coupling the constraints from the SMNS fraction, we exclude some EoSs and obtain three candidate EoSs, i.e., DD2, ENG, and MPA1. By comparing the distributions of the collapse-time and the luminosity of the internal plateau (in the short gamma-ray bursts) for those from observations with those obtained based on the three candidate EoSs, it is shown that only the EoS of ENG is favored. Our sample based on the ENG EOS and a mass distribution motivated by Galactic systems suggests that approximately $99\\%$ of NS-NS mergers collapse to form a black hole within $10^7$s. This includes scenarios forming a BH promptly ($36.5\\%$), a SMNS ($60.7\\%$), or a stable NS that transitions into a BH or a SMNS following accretion ($2.1\\%$). It also indicates that the remnants for GW170817 and GW190425, and the second object of GW190814 are more likely to be BHs.","sentences":["The merger of neutron star (NS)-NS binary can form different production of the compact remnant, among which the supramassive NS (SMNS) could create an internal plateau and the followed steep decay marks the collapse of the SMNS.","The proportion of SMNS and the corresponding collapse-time are often used to constrain the NS equation of state (EoS).","This paper revisits this topic by considering the effect of an accretion disk on the compact remnant, which is not considered in previous works.","Compared with previous works, the collapse-time distribution (peaks $\\sim$100 s) of the SMNSs formed from NS-NS merger is almost unaffected by the initial surface magnetic ($B_{{\\rm s},i}$) of NS, but the total energy output of the magnetic dipole radiation from the SMNSs depends on $B_{{\\rm s},i}$ significantly.","Coupling the constraints from the SMNS fraction, we exclude some EoSs and obtain three candidate EoSs, i.e., DD2, ENG, and MPA1.","By comparing the distributions of the collapse-time and the luminosity of the internal plateau (in the short gamma-ray bursts) for those from observations with those obtained based on the three candidate EoSs, it is shown that only the EoS of ENG is favored.","Our sample based on the ENG EOS and a mass distribution motivated by Galactic systems suggests that approximately $99\\%$ of NS-NS mergers collapse to form a black hole within $10^7$s.","This includes scenarios forming a BH promptly ($36.5\\%$), a SMNS ($60.7\\%$), or a stable NS that transitions into a BH or a SMNS following accretion ($2.1\\%$).","It also indicates that the remnants for GW170817 and GW190425, and the second object of GW190814 are more likely to be BHs."],"url":"http://arxiv.org/abs/2404.19340v1","category":"astro-ph.HE"}
{"created":"2024-04-30 08:00:17","title":"Multi-Scale Heterogeneity-Aware Hypergraph Representation for Histopathology Whole Slide Images","abstract":"Survival prediction is a complex ordinal regression task that aims to predict the survival coefficient ranking among a cohort of patients, typically achieved by analyzing patients' whole slide images. Existing deep learning approaches mainly adopt multiple instance learning or graph neural networks under weak supervision. Most of them are unable to uncover the diverse interactions between different types of biological entities(\\textit{e.g.}, cell cluster and tissue block) across multiple scales, while such interactions are crucial for patient survival prediction. In light of this, we propose a novel multi-scale heterogeneity-aware hypergraph representation framework. Specifically, our framework first constructs a multi-scale heterogeneity-aware hypergraph and assigns each node with its biological entity type. It then mines diverse interactions between nodes on the graph structure to obtain a global representation. Experimental results demonstrate that our method outperforms state-of-the-art approaches on three benchmark datasets. Code is publicly available at \\href{https://github.com/Hanminghao/H2GT}{https://github.com/Hanminghao/H2GT}.","sentences":["Survival prediction is a complex ordinal regression task that aims to predict the survival coefficient ranking among a cohort of patients, typically achieved by analyzing patients' whole slide images.","Existing deep learning approaches mainly adopt multiple instance learning or graph neural networks under weak supervision.","Most of them are unable to uncover the diverse interactions between different types of biological entities(\\textit{e.g.}, cell cluster and tissue block) across multiple scales, while such interactions are crucial for patient survival prediction.","In light of this, we propose a novel multi-scale heterogeneity-aware hypergraph representation framework.","Specifically, our framework first constructs a multi-scale heterogeneity-aware hypergraph and assigns each node with its biological entity type.","It then mines diverse interactions between nodes on the graph structure to obtain a global representation.","Experimental results demonstrate that our method outperforms state-of-the-art approaches on three benchmark datasets.","Code is publicly available at \\href{https://github.com/Hanminghao/H2GT}{https://github.com/Hanminghao/H2GT}."],"url":"http://arxiv.org/abs/2404.19334v1","category":"cs.CV"}
{"created":"2024-04-30 07:59:47","title":"Super-resolution by converting evanescent waves in microsphere to propagating and transfer function from its surface to nano-jet","abstract":"The EM waves transmitted through a thin object with fine structures is observed, by microsphere located above the object. While the waves include both evanescent and propagating waves, the high resolution is obtained by the evanescent ones, including the information on the fine structures of the object. Description of this process is divided into two parts: a) The super resolution is analyzed by using Helmholtz equation for the evanescent waves transmitted from the object to the microsphere surface. b) Using boundary condition, the electric fields on the inner surface of the microsphere includes both the evanescent and propagating waves. The transmission of these waves to a nano jet is produced by a transfer function, including convolution between the spatial modes of the evanescent waves with those of the microsphere, which increases the conversion of the evanescent waves to propagating waves, and thus increase the resolution.","sentences":["The EM waves transmitted through a thin object with fine structures is observed, by microsphere located above the object.","While the waves include both evanescent and propagating waves, the high resolution is obtained by the evanescent ones, including the information on the fine structures of the object.","Description of this process is divided into two parts: a)","The super resolution is analyzed by using Helmholtz equation for the evanescent waves transmitted from the object to the microsphere surface.","b) Using boundary condition, the electric fields on the inner surface of the microsphere includes both the evanescent and propagating waves.","The transmission of these waves to a nano jet is produced by a transfer function, including convolution between the spatial modes of the evanescent waves with those of the microsphere, which increases the conversion of the evanescent waves to propagating waves, and thus increase the resolution."],"url":"http://arxiv.org/abs/2404.19333v1","category":"physics.optics"}
{"created":"2024-04-30 07:55:45","title":"Fusing Depthwise and Pointwise Convolutions for Efficient Inference on GPUs","abstract":"Depthwise and pointwise convolutions have fewer parameters and perform fewer operations than standard convolutions. As a result, they have become increasingly used in various compact DNNs, including convolutional neural networks (CNNs) and vision transformers (ViTs). However, they have a lower compute-to-memory-access ratio than standard convolutions, making their memory accesses often the performance bottleneck. This paper explores fusing depthwise and pointwise convolutions to overcome the memory access bottleneck. The focus is on fusing these operators on GPUs. The prior art on GPU-based fusion suffers from one or more of the following: (1) fusing either a convolution with an element-wise or multiple non-convolutional operators, (2) not explicitly optimizing for memory accesses, (3) not supporting depthwise convolutions. This paper proposes Fused Convolutional Modules (FCMs), a set of novel fused depthwise and pointwise GPU kernels. FCMs significantly reduce pointwise and depthwise convolutions memory accesses, improving execution time and energy efficiency. To evaluate the trade-offs associated with fusion and determine which convolutions are beneficial to fuse and the optimal FCM parameters, we propose FusePlanner. FusePlanner consists of cost models to estimate the memory accesses of depthwise, pointwise, and FCM kernels given GPU characteristics. Our experiments on three GPUs using representative CNNs and ViTs demonstrate that FCMs save up to 83% of the memory accesses and achieve speedups of up to 3.7x compared to cuDNN. Complete model implementations of various CNNs using our modules outperform TVMs' achieving speedups of up to 1.8x and saving up to two-thirds of the energy.","sentences":["Depthwise and pointwise convolutions have fewer parameters and perform fewer operations than standard convolutions.","As a result, they have become increasingly used in various compact DNNs, including convolutional neural networks (CNNs) and vision transformers (ViTs).","However, they have a lower compute-to-memory-access ratio than standard convolutions, making their memory accesses often the performance bottleneck.","This paper explores fusing depthwise and pointwise convolutions to overcome the memory access bottleneck.","The focus is on fusing these operators on GPUs.","The prior art on GPU-based fusion suffers from one or more of the following: (1) fusing either a convolution with an element-wise or multiple non-convolutional operators, (2) not explicitly optimizing for memory accesses, (3) not supporting depthwise convolutions.","This paper proposes Fused Convolutional Modules (FCMs), a set of novel fused depthwise and pointwise GPU kernels.","FCMs significantly reduce pointwise and depthwise convolutions memory accesses, improving execution time and energy efficiency.","To evaluate the trade-offs associated with fusion and determine which convolutions are beneficial to fuse and the optimal FCM parameters, we propose FusePlanner.","FusePlanner consists of cost models to estimate the memory accesses of depthwise, pointwise, and FCM kernels given GPU characteristics.","Our experiments on three GPUs using representative CNNs and ViTs demonstrate that FCMs save up to 83% of the memory accesses and achieve speedups of up to 3.7x compared to cuDNN.","Complete model implementations of various CNNs using our modules outperform TVMs' achieving speedups of up to 1.8x and saving up to two-thirds of the energy."],"url":"http://arxiv.org/abs/2404.19331v1","category":"cs.PF"}
{"created":"2024-04-30 07:47:26","title":"The Effect of Data Types' on the Performance of Machine Learning Algorithms for Financial Prediction","abstract":"Forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits. A small improvement in forecasting performance can lead to increased profitability; therefore, obtaining a realistic forecast is very important for investors. Successful forecasting provides traders with effective buy-or-hold strategies, allowing them to make more profits. The most important thing in this process is to produce accurate forecasts suitable for real-life applications. Bitcoin, frequently mentioned recently due to its volatility and chaotic behavior, has begun to pay great attention and has become an investment tool, especially during and after the COVID-19 pandemic. This study provided a comprehensive methodology, including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning (ML) algorithms to forecast Bitcoin price movement. A binarization procedure was applied using continuous data to construct the trend data representing each input feature trend. Following the related literature, the input features are determined as technical indicators, google trends, and the number of tweets. Random forest (RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB), Support vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks (ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected features for prediction purposes. This work investigates two main research questions: i. How does the sample size affect the prediction performance of ML algorithms? ii. How does the data type affect the prediction performance of ML algorithms? Accuracy and area under the ROC curve (AUC) values were used to compare the model performance. A t-test was performed to test the statistical significance of the prediction results.","sentences":["Forecasting cryptocurrencies as a financial issue is crucial as it provides investors with possible financial benefits.","A small improvement in forecasting performance can lead to increased profitability; therefore, obtaining a realistic forecast is very important for investors.","Successful forecasting provides traders with effective buy-or-hold strategies, allowing them to make more profits.","The most important thing in this process is to produce accurate forecasts suitable for real-life applications.","Bitcoin, frequently mentioned recently due to its volatility and chaotic behavior, has begun to pay great attention and has become an investment tool, especially during and after the COVID-19 pandemic.","This study provided a comprehensive methodology, including constructing continuous and trend data using one and seven years periods of data as inputs and applying machine learning (ML) algorithms to forecast Bitcoin price movement.","A binarization procedure was applied using continuous data to construct the trend data representing each input feature trend.","Following the related literature, the input features are determined as technical indicators, google trends, and the number of tweets.","Random forest (RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB), Support vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks (ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected features for prediction purposes.","This work investigates two main research questions:","i. How does the sample size affect the prediction performance of ML algorithms?","ii.","How does the data type affect the prediction performance of ML algorithms?","Accuracy and area under the ROC curve (AUC) values were used to compare the model performance.","A t-test was performed to test the statistical significance of the prediction results."],"url":"http://arxiv.org/abs/2404.19324v1","category":"q-fin.CP"}
{"created":"2024-04-30 07:40:35","title":"Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget","abstract":"Compared to standard language model (LM) pretraining (i.e., from scratch), Knowledge Distillation (KD) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model. As such, KD in LM pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch. Scaling laws of LM pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)-and under a fixed computation budget, smaller models are able be process more data than larger models. We thus hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget. To test this, we compare pretraining from scratch against several KD strategies for masked language modeling (MLM) in a fair experimental setup, with respect to amount of computation as well as pretraining data. Downstream results on GLUE, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary KD under a fixed computation budget, more sophisticated KD strategies, namely TinyBERT (Jiao et al., 2020) and MiniLM (Wang et al., 2023), outperform it by a notable margin. We further find that KD yields larger gains over pretraining from scratch when the data must be repeated under the fixed computation budget.","sentences":["Compared to standard language model (LM) pretraining (i.e., from scratch), Knowledge Distillation (KD) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model.","As such, KD in LM pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch.","Scaling laws of LM pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)-and under a fixed computation budget, smaller models are able be process more data than larger models.","We thus hypothesize that KD might, in fact, be suboptimal to pretraining from scratch for obtaining smaller LMs, when appropriately accounting for the compute budget.","To test this, we compare pretraining from scratch against several KD strategies for masked language modeling (MLM) in a fair experimental setup, with respect to amount of computation as well as pretraining data.","Downstream results on GLUE, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary KD under a fixed computation budget, more sophisticated KD strategies, namely TinyBERT (Jiao et al., 2020) and MiniLM (Wang et al., 2023), outperform it by a notable margin.","We further find that KD yields larger gains over pretraining from scratch when the data must be repeated under the fixed computation budget."],"url":"http://arxiv.org/abs/2404.19319v1","category":"cs.CL"}
{"created":"2024-04-30 07:37:48","title":"Revisiting N-Gram Models: Their Impact in Modern Neural Networks for Handwritten Text Recognition","abstract":"In recent advances in automatic text recognition (ATR), deep neural networks have demonstrated the ability to implicitly capture language statistics, potentially reducing the need for traditional language models. This study directly addresses whether explicit language models, specifically n-gram models, still contribute to the performance of state-of-the-art deep learning architectures in the field of handwriting recognition. We evaluate two prominent neural network architectures, PyLaia and DAN, with and without the integration of explicit n-gram language models. Our experiments on three datasets - IAM, RIMES, and NorHand v2 - at both line and page level, investigate optimal parameters for n-gram models, including their order, weight, smoothing methods and tokenization level. The results show that incorporating character or subword n-gram models significantly improves the performance of ATR models on all datasets, challenging the notion that deep learning models alone are sufficient for optimal performance. In particular, the combination of DAN with a character language model outperforms current benchmarks, confirming the value of hybrid approaches in modern document analysis systems.","sentences":["In recent advances in automatic text recognition (ATR), deep neural networks have demonstrated the ability to implicitly capture language statistics, potentially reducing the need for traditional language models.","This study directly addresses whether explicit language models, specifically n-gram models, still contribute to the performance of state-of-the-art deep learning architectures in the field of handwriting recognition.","We evaluate two prominent neural network architectures, PyLaia and DAN, with and without the integration of explicit n-gram language models.","Our experiments on three datasets - IAM, RIMES, and NorHand v2 - at both line and page level, investigate optimal parameters for n-gram models, including their order, weight, smoothing methods and tokenization level.","The results show that incorporating character or subword n-gram models significantly improves the performance of ATR models on all datasets, challenging the notion that deep learning models alone are sufficient for optimal performance.","In particular, the combination of DAN with a character language model outperforms current benchmarks, confirming the value of hybrid approaches in modern document analysis systems."],"url":"http://arxiv.org/abs/2404.19317v1","category":"cs.CV"}
{"created":"2024-04-30 07:32:17","title":"Revealing the working mechanism of quantum neural networks by mutual information","abstract":"Quantum neural networks (QNNs) is a parameterized quantum circuit model, which can be trained by gradient-based optimizer, can be used for supervised learning, regression tasks, combinatorial optimization, etc. Although many works have demonstrated that QNNs have better learnability, generalizability, etc. compared to classical neural networks. However, as with classical neural networks, we still can't explain their working mechanism well. In this paper, we reveal the training mechanism of QNNs by mutual information. Unlike traditional mutual information in neural networks, due to quantum computing remains information conserved, the mutual information is trivial of the input and output of U operator. In our work, in order to observe the change of mutual information during training, we divide the quantum circuit (U operator) into two subsystems, discard subsystem (D) and measurement subsystem (M) respectively. We calculate two mutual information, I(Di : Mo) and I(Mi : Mo) (i and o means input or output of the corresponding subsystem), and observe their behavior during training. As the epochs increases, I(Di : Mo) gradually increases, this may means some information of discard subsystem is continuously pushed into the measurement subsystem, the information should be label-related. What's more, I(Mi : Mo) exist two-phase behavior in training process, this consistent with the information bottleneck anticipation. The first phase, I(Mi : Mo) is increasing, this means the measurement subsystem perform feature fitting. The second phase, I(Mi : Mo) is decreasing, this may means the system is generalizing, the measurement subsystem discard label-irrelevant information into the discard subsystem as many as possible. Our work discussed the working mechanism of QNNs by mutual information, further, it can be used to analyze the accuracy and generalization of QNNs.","sentences":["Quantum neural networks (QNNs) is a parameterized quantum circuit model, which can be trained by gradient-based optimizer, can be used for supervised learning, regression tasks, combinatorial optimization, etc.","Although many works have demonstrated that QNNs have better learnability, generalizability, etc. compared to classical neural networks.","However, as with classical neural networks, we still can't explain their working mechanism well.","In this paper, we reveal the training mechanism of QNNs by mutual information.","Unlike traditional mutual information in neural networks, due to quantum computing remains information conserved, the mutual information is trivial of the input and output of U operator.","In our work, in order to observe the change of mutual information during training, we divide the quantum circuit (U operator) into two subsystems, discard subsystem (D) and measurement subsystem (M) respectively.","We calculate two mutual information, I(Di : Mo) and I(Mi : Mo) (i and o means input or output of the corresponding subsystem), and observe their behavior during training.","As the epochs increases, I(Di : Mo) gradually increases, this may means some information of discard subsystem is continuously pushed into the measurement subsystem, the information should be label-related.","What's more, I(Mi : Mo) exist two-phase behavior in training process, this consistent with the information bottleneck anticipation.","The first phase, I(Mi : Mo) is increasing, this means the measurement subsystem perform feature fitting.","The second phase, I(Mi : Mo) is decreasing, this may means the system is generalizing, the measurement subsystem discard label-irrelevant information into the discard subsystem as many as possible.","Our work discussed the working mechanism of QNNs by mutual information, further, it can be used to analyze the accuracy and generalization of QNNs."],"url":"http://arxiv.org/abs/2404.19312v1","category":"quant-ph"}
{"created":"2024-04-30 06:55:12","title":"Collisional dynamics of symmetric two-dimensional quantum droplets","abstract":"The collisional dynamics of two symmetric droplets with equal intraspecies scattering lengths and particle number density for each component is studied by solving the corresponding extended Gross-Pitaevskii equation in two dimensions by including a logarithmic correction term in the usual contact interaction. We find the merging droplet after collision experiences a quadrupole oscillation in its shape and the oscillation period is found to be independent of the incidental momentum for small droplets. With increasing collision momentum the colliding droplets may separate into two, or even more, and finally into small pieces of droplets. For these dynamical phases, we manage to present boundaries determined by the remnant particle number in the central area and the damped oscillation of the quadrupole mode. A stability peak for the existence of droplets emerges at the critical particle number $N_c \\simeq 48$ for the quasi-Gaussian and flat-top shapes of the droplets.","sentences":["The collisional dynamics of two symmetric droplets with equal intraspecies scattering lengths and particle number density for each component is studied by solving the corresponding extended Gross-Pitaevskii equation in two dimensions by including a logarithmic correction term in the usual contact interaction.","We find the merging droplet after collision experiences a quadrupole oscillation in its shape and the oscillation period is found to be independent of the incidental momentum for small droplets.","With increasing collision momentum the colliding droplets may separate into two, or even more, and finally into small pieces of droplets.","For these dynamical phases, we manage to present boundaries determined by the remnant particle number in the central area and the damped oscillation of the quadrupole mode.","A stability peak for the existence of droplets emerges at the critical particle number $N_c \\simeq 48$ for the quasi-Gaussian and flat-top shapes of the droplets."],"url":"http://arxiv.org/abs/2404.19295v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-30 06:47:04","title":"Dynamic Human Trust Modeling of Autonomous Agents With Varying Capability and Strategy","abstract":"Objective We model the dynamic trust of human subjects in a human-autonomy-teaming screen-based task.   Background Trust is an emerging area of study in human-robot collaboration. Many studies have looked at the issue of robot performance as a sole predictor of human trust, but this could underestimate the complexity of the interaction.   Method Subjects were paired with autonomous agents to search an on-screen grid to determine the number of outlier objects. In each trial, a different autonomous agent with a preassigned capability used one of three search strategies and then reported the number of outliers it found as a fraction of its capability. Then, the subject reported their total outlier estimate. Human subjects then evaluated statements about the agent's behavior, reliability, and their trust in the agent.   Results 80 subjects were recruited. Self-reported trust was modeled using Ordinary Least Squares, but the group that interacted with varying capability agents on a short time order produced a better performing ARIMAX model. Models were cross-validated between groups and found a moderate improvement in the next trial trust prediction.   Conclusion A time series modeling approach reveals the effects of temporal ordering of agent performance on estimated trust. Recency bias may affect how subjects weigh the contribution of strategy or capability to trust. Understanding the connections between agent behavior, agent performance, and human trust is crucial to improving human-robot collaborative tasks.   Application The modeling approach in this study demonstrates the need to represent autonomous agent characteristics over time to capture changes in human trust.","sentences":["Objective We model the dynamic trust of human subjects in a human-autonomy-teaming screen-based task.   ","Background Trust is an emerging area of study in human-robot collaboration.","Many studies have looked at the issue of robot performance as a sole predictor of human trust, but this could underestimate the complexity of the interaction.   ","Method Subjects were paired with autonomous agents to search an on-screen grid to determine the number of outlier objects.","In each trial, a different autonomous agent with a preassigned capability used one of three search strategies and then reported the number of outliers it found as a fraction of its capability.","Then, the subject reported their total outlier estimate.","Human subjects then evaluated statements about the agent's behavior, reliability, and their trust in the agent.   ","Results 80 subjects were recruited.","Self-reported trust was modeled using Ordinary Least Squares, but the group that interacted with varying capability agents on a short time order produced a better performing ARIMAX model.","Models were cross-validated between groups and found a moderate improvement in the next trial trust prediction.   ","Conclusion A time series modeling approach reveals the effects of temporal ordering of agent performance on estimated trust.","Recency bias may affect how subjects weigh the contribution of strategy or capability to trust.","Understanding the connections between agent behavior, agent performance, and human trust is crucial to improving human-robot collaborative tasks.   ","Application","The modeling approach in this study demonstrates the need to represent autonomous agent characteristics over time to capture changes in human trust."],"url":"http://arxiv.org/abs/2404.19291v1","category":"cs.HC"}
{"created":"2024-04-30 05:41:49","title":"Statistical Mechanics Calculations Using Variational Autoregressive Networks and Quantum Annealing","abstract":"In statistical mechanics, computing the partition function is generally difficult. An approximation method using a variational autoregressive network (VAN) has been proposed recently. This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples. The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution. When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field.","sentences":["In statistical mechanics, computing the partition function is generally difficult.","An approximation method using a variational autoregressive network (VAN) has been proposed recently.","This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples.","The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution.","When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field."],"url":"http://arxiv.org/abs/2404.19274v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-30 05:33:37","title":"Regularity and long-time behavior of global weak solutions to a coupled Cahn-Hilliard system: the off-critical case","abstract":"We consider a diffuse interface model that describes the macro- and micro-phase separation processes of a polymer mixture. The resulting system consists of a Cahn-Hilliard equation and a Cahn-Hilliard-Oono type equation endowed with the singular Flory-Huggins potential. For the initial boundary value problem in a bounded smooth domain of $\\mathbb{R}^d$ ($d\\in\\{2,3\\}$) with homogeneous Neumann boundary conditions for the phase functions as well as chemical potentials, we study the regularity and long-time behavior of global weak solutions in the off-critical case, i.e., the mass is not conserved during the micro-phase separation of diblock copolymers. By investigating an auxiliary system with viscous regularizations, we show that every global weak solution regularizes instantaneously for $t>0$. In two dimensions, we obtain the instantaneous strict separation property under a mild growth condition on the first derivative of potential functions near pure phases $\\pm 1$, while in three dimensions, we establish the eventual strict separation property for sufficiently large time. Finally, we prove that every global weak solution converges to a single equilibrium as $t\\to +\\infty$.","sentences":["We consider a diffuse interface model that describes the macro- and micro-phase separation processes of a polymer mixture.","The resulting system consists of a Cahn-Hilliard equation and a Cahn-Hilliard-Oono type equation endowed with the singular Flory-Huggins potential.","For the initial boundary value problem in a bounded smooth domain of $\\mathbb{R}^d$ ($d\\in\\{2,3\\}$) with homogeneous Neumann boundary conditions for the phase functions as well as chemical potentials, we study the regularity and long-time behavior of global weak solutions in the off-critical case, i.e., the mass is not conserved during the micro-phase separation of diblock copolymers.","By investigating an auxiliary system with viscous regularizations, we show that every global weak solution regularizes instantaneously for $t>0$. In two dimensions, we obtain the instantaneous strict separation property under a mild growth condition on the first derivative of potential functions near pure phases $\\pm 1$, while in three dimensions, we establish the eventual strict separation property for sufficiently large time.","Finally, we prove that every global weak solution converges to a single equilibrium as $t\\to +\\infty$."],"url":"http://arxiv.org/abs/2404.19271v1","category":"math.AP"}
{"created":"2024-04-30 05:18:06","title":"Flow by Gauss Curvature to the Orlicz Minkowski Problem for q-torsional rigidity","abstract":"The Minkowski problem for torsional rigidity ($2$-torsional rigidity) was firstly studied by Colesanti and Fimiani \\cite{CA} using variational method. Moreover, Hu, Liu and Ma \\cite{HJ00} also studied this problem by method of curvature flow and obtain the existence of smooth solution. In addition, the Minkowski problem for $2$-torsional rigidity was also extended to $L_p$ version and Orlicz version.   Recently, Hu and Zhang \\cite{HJ2} introduced the concept of Orlicz mixed $q$-torsional rigidity and obtained Orlicz $q$-torsional measure through the variational method for $q>1$. Specially, they established the functional Orlicz Brunn-Minkowski inequality and the functional Orlicz Minkowski inequality.   Motivated by the remarkable work by Hu and Zhang in \\cite{HJ2}, we can propose the Orlicz Minkowksi problem for $q$-torsional rigidity, and then confirm the existence of smooth even solutions to the Orlicz Minkowski problem for $q$-torsional rigidity with $q>1$ by method of a Gauss curvature flow.","sentences":["The Minkowski problem for torsional rigidity ($2$-torsional rigidity) was firstly studied by Colesanti and Fimiani \\cite{CA} using variational method.","Moreover, Hu, Liu and Ma \\cite{HJ00} also studied this problem by method of curvature flow and obtain the existence of smooth solution.","In addition, the Minkowski problem for $2$-torsional rigidity was also extended to $L_p$ version and Orlicz version.   ","Recently, Hu and Zhang \\cite{HJ2} introduced the concept of Orlicz mixed $q$-torsional rigidity and obtained Orlicz $q$-torsional measure through the variational method for $q>1$. Specially, they established the functional Orlicz Brunn-Minkowski inequality and the functional Orlicz Minkowski inequality.   ","Motivated by the remarkable work by Hu and Zhang in \\cite{HJ2}, we can propose the Orlicz Minkowksi problem for $q$-torsional rigidity, and then confirm the existence of smooth even solutions to the Orlicz Minkowski problem for $q$-torsional rigidity with $q>1$ by method of a Gauss curvature flow."],"url":"http://arxiv.org/abs/2404.19266v1","category":"math.DG"}
{"created":"2024-04-30 05:05:07","title":"Length and torsion dependence of thermal conductivity in twisted graphene nanoribbons","abstract":"Research on the physical properties of materials at the nanoscale is crucial for the development of breakthrough nanotechnologies. One of the key properties to consider is the ability to conduct heat, i.e., its thermal conductivity. Graphene is a remarkable nanostructure with exceptional physical properties, including one of the highest thermal conductivities (TC) ever measured. Graphene nanoribbons (GNRs) share most fundamental properties with graphene, with the added benefit of having a controllable electronic bandgap. One method to achieve such control is by twisting the GNR, which can tailor its electronic properties, as well as change their TC. Here, we revisit the dependence of the TC of twisted GNRs (TGNRs) on the number of applied turns to the GNR by calculating more precise and mathematically well defined geometric parameters related to the TGNR shape, namely, its twist and writhe. We show that the dependence of the TC on twist is not a simple function of the number of turns initially applied to a straight GNR. In fact, we show that the TC of TGNRs requires at least two parameters to be properly described. Our conclusions are supported by atomistic molecular dynamics simulations to obtain the TC of suspended TGNRs prepared under different values of initially applied turns and different sizes of their suspended part. Among possible choices of parameter pairs, we show that TC can be appropriately described by the initial number of turns and the initial twist density of the TGNRs.","sentences":["Research on the physical properties of materials at the nanoscale is crucial for the development of breakthrough nanotechnologies.","One of the key properties to consider is the ability to conduct heat, i.e., its thermal conductivity.","Graphene is a remarkable nanostructure with exceptional physical properties, including one of the highest thermal conductivities (TC) ever measured.","Graphene nanoribbons (GNRs) share most fundamental properties with graphene, with the added benefit of having a controllable electronic bandgap.","One method to achieve such control is by twisting the GNR, which can tailor its electronic properties, as well as change their TC.","Here, we revisit the dependence of the TC of twisted GNRs (TGNRs) on the number of applied turns to the GNR by calculating more precise and mathematically well defined geometric parameters related to the TGNR shape, namely, its twist and writhe.","We show that the dependence of the TC on twist is not a simple function of the number of turns initially applied to a straight GNR.","In fact, we show that the TC of TGNRs requires at least two parameters to be properly described.","Our conclusions are supported by atomistic molecular dynamics simulations to obtain the TC of suspended TGNRs prepared under different values of initially applied turns and different sizes of their suspended part.","Among possible choices of parameter pairs, we show that TC can be appropriately described by the initial number of turns and the initial twist density of the TGNRs."],"url":"http://arxiv.org/abs/2404.19262v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-30 04:54:15","title":"High dimensional analysis reveals conservative sharpening and a stochastic edge of stability","abstract":"Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization.","sentences":["Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime.","There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability.","Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening.","We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown.","We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues.","We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization."],"url":"http://arxiv.org/abs/2404.19261v1","category":"cs.LG"}
{"created":"2024-04-30 04:13:14","title":"Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair","abstract":"In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes. The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes. While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features. To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features. We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair). Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample. To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model. The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity.","sentences":["In the image classification task, deep neural networks frequently rely on bias attributes that are spuriously correlated with a target class in the presence of dataset bias, resulting in degraded performance when applied to data without bias attributes.","The task of debiasing aims to compel classifiers to learn intrinsic attributes that inherently define a target class rather than focusing on bias attributes.","While recent approaches mainly focus on emphasizing the learning of data samples without bias attributes (i.e., bias-conflicting samples) compared to samples with bias attributes (i.e., bias-aligned samples), they fall short of directly guiding models where to focus for learning intrinsic features.","To address this limitation, this paper proposes a method that provides the model with explicit spatial guidance that indicates the region of intrinsic features.","We first identify the intrinsic features by investigating the class-discerning common features between a bias-aligned (BA) sample and a bias-conflicting (BC) sample (i.e., bias-contrastive pair).","Next, we enhance the intrinsic features in the BA sample that are relatively under-exploited for prediction compared to the BC sample.","To construct the bias-contrastive pair without using bias information, we introduce a bias-negative score that distinguishes BC samples from BA samples employing a biased model.","The experiments demonstrate that our method achieves state-of-the-art performance on synthetic and real-world datasets with various levels of bias severity."],"url":"http://arxiv.org/abs/2404.19250v1","category":"cs.CV"}
{"created":"2024-04-30 03:55:19","title":"Central elements of the degenerate quantum general linear group","abstract":"We construct central elements of the degenerate quantum general linear group introduced by Cheng, Wang and Zhang. In particular, we give an explicit formula for the quantum Casimir element. Our method is based on the explicit $L$ operators. Moreover, we construct a universal $L$ operator, which is a spectral parameter-dependent solution of the quantum Yang-Baxter equation in the tensor product of the degenerate quantum general linear group and the endomorphism ring of its natural representation. This construction leads to the FRT approach to the degenerate quantum general linear group.","sentences":["We construct central elements of the degenerate quantum general linear group introduced by Cheng, Wang and Zhang.","In particular, we give an explicit formula for the quantum Casimir element.","Our method is based on the explicit $L$ operators.","Moreover, we construct a universal $L$ operator, which is a spectral parameter-dependent solution of the quantum Yang-Baxter equation in the tensor product of the degenerate quantum general linear group and the endomorphism ring of its natural representation.","This construction leads to the FRT approach to the degenerate quantum general linear group."],"url":"http://arxiv.org/abs/2404.19239v1","category":"math.QA"}
{"created":"2024-04-30 17:54:16","title":"Scale-Robust Timely Asynchronous Decentralized Learning","abstract":"We consider an asynchronous decentralized learning system, which consists of a network of connected devices trying to learn a machine learning model without any centralized parameter server. The users in the network have their own local training data, which is used for learning across all the nodes in the network. The learning method consists of two processes, evolving simultaneously without any necessary synchronization. The first process is the model update, where the users update their local model via a fixed number of stochastic gradient descent steps. The second process is model mixing, where the users communicate with each other via randomized gossiping to exchange their models and average them to reach consensus. In this work, we investigate the staleness criteria for such a system, which is a sufficient condition for convergence of individual user models. We show that for network scaling, i.e., when the number of user devices $n$ is very large, if the gossip capacity of individual users scales as $\\Omega(\\log n)$, we can guarantee the convergence of user models in finite time. Furthermore, we show that the bounded staleness can only be guaranteed by any distributed opportunistic scheme by $\\Omega(n)$ scaling.","sentences":["We consider an asynchronous decentralized learning system, which consists of a network of connected devices trying to learn a machine learning model without any centralized parameter server.","The users in the network have their own local training data, which is used for learning across all the nodes in the network.","The learning method consists of two processes, evolving simultaneously without any necessary synchronization.","The first process is the model update, where the users update their local model via a fixed number of stochastic gradient descent steps.","The second process is model mixing, where the users communicate with each other via randomized gossiping to exchange their models and average them to reach consensus.","In this work, we investigate the staleness criteria for such a system, which is a sufficient condition for convergence of individual user models.","We show that for network scaling, i.e., when the number of user devices $n$ is very large, if the gossip capacity of individual users scales as $\\Omega(\\log n)$, we can guarantee the convergence of user models in finite time.","Furthermore, we show that the bounded staleness can only be guaranteed by any distributed opportunistic scheme by $\\Omega(n)$ scaling."],"url":"http://arxiv.org/abs/2404.19749v1","category":"cs.IT"}
{"created":"2024-04-30 17:06:20","title":"ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text data on social disorders in children and adolescents","abstract":"This paper describes our participation in Task 3 and Task 5 of the #SMM4H (Social Media Mining for Health) 2024 Workshop, explicitly targeting the classification challenges within tweet data. Task 3 is a multi-class classification task centered on tweets discussing the impact of outdoor environments on symptoms of social anxiety. Task 5 involves a binary classification task focusing on tweets reporting medical disorders in children. We applied transfer learning from pre-trained encoder-decoder models such as BART-base and T5-small to identify the labels of a set of given tweets. We also presented some data augmentation methods to see their impact on the model performance. Finally, the systems obtained the best F1 score of 0.627 in Task 3 and the best F1 score of 0.841 in Task 5.","sentences":["This paper describes our participation in Task 3 and Task 5 of the #SMM4H (Social Media Mining for Health) 2024 Workshop, explicitly targeting the classification challenges within tweet data.","Task 3 is a multi-class classification task centered on tweets discussing the impact of outdoor environments on symptoms of social anxiety.","Task 5 involves a binary classification task focusing on tweets reporting medical disorders in children.","We applied transfer learning from pre-trained encoder-decoder models such as BART-base and T5-small to identify the labels of a set of given tweets.","We also presented some data augmentation methods to see their impact on the model performance.","Finally, the systems obtained the best F1 score of 0.627 in Task 3 and the best F1 score of 0.841 in Task 5."],"url":"http://arxiv.org/abs/2404.19714v1","category":"cs.CL"}
{"created":"2024-04-30 17:06:11","title":"Automated Generation of High-Quality Medical Simulation Scenarios Through Integration of Semi-Structured Data and Large Language Models","abstract":"This study introduces a transformative framework for medical education by integrating semi-structured data with Large Language Models (LLMs), primarily OpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios. Traditionally, developing these scenarios was a time-intensive process with limited flexibility to meet diverse educational needs. The proposed approach utilizes AI to efficiently generate detailed, clinically relevant scenarios that are tailored to specific educational objectives. This innovation has significantly reduced the time and resources required for scenario development, allowing for a broader variety of simulations. Preliminary feedback from educators and learners has shown enhanced engagement and improved knowledge acquisition, confirming the effectiveness of this AI-enhanced methodology in simulation-based learning. The integration of structured data with LLMs not only streamlines the creation process but also offers a scalable, dynamic solution that could revolutionize medical training, highlighting the critical role of AI in advancing educational outcomes and patient care standards.","sentences":["This study introduces a transformative framework for medical education by integrating semi-structured data with Large Language Models (LLMs), primarily OpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios.","Traditionally, developing these scenarios was a time-intensive process with limited flexibility to meet diverse educational needs.","The proposed approach utilizes AI to efficiently generate detailed, clinically relevant scenarios that are tailored to specific educational objectives.","This innovation has significantly reduced the time and resources required for scenario development, allowing for a broader variety of simulations.","Preliminary feedback from educators and learners has shown enhanced engagement and improved knowledge acquisition, confirming the effectiveness of this AI-enhanced methodology in simulation-based learning.","The integration of structured data with LLMs not only streamlines the creation process but also offers a scalable, dynamic solution that could revolutionize medical training, highlighting the critical role of AI in advancing educational outcomes and patient care standards."],"url":"http://arxiv.org/abs/2404.19713v1","category":"cs.CL"}
{"created":"2024-04-30 15:57:41","title":"Towards Generalist Robot Learning from Internet Video: A Survey","abstract":"This survey presents an overview of methods for learning from video (LfV) in the context of reinforcement learning (RL) and robotics. We focus on methods capable of scaling to large internet video datasets and, in the process, extracting foundational knowledge about the world's dynamics and physical human behaviour. Such methods hold great promise for developing general-purpose robots.   We open with an overview of fundamental concepts relevant to the LfV-for-robotics setting. This includes a discussion of the exciting benefits LfV methods can offer (e.g., improved generalization beyond the available robot data) and commentary on key LfV challenges (e.g., challenges related to missing information in video and LfV distribution shifts). Our literature review begins with an analysis of video foundation model techniques that can extract knowledge from large, heterogeneous video datasets. Next, we review methods that specifically leverage video data for robot learning. Here, we categorise work according to which RL knowledge modality benefits from the use of video data. We additionally highlight techniques for mitigating LfV challenges, including reviewing action representations that address the issue of missing action labels in video.   Finally, we examine LfV datasets and benchmarks, before concluding the survey by discussing challenges and opportunities in LfV. Here, we advocate for scalable approaches that can leverage the full range of available data and that target the key benefits of LfV. Overall, we hope this survey will serve as a comprehensive reference for the emerging field of LfV, catalysing further research in the area, and ultimately facilitating progress towards obtaining general-purpose robots.","sentences":["This survey presents an overview of methods for learning from video (LfV) in the context of reinforcement learning (RL) and robotics.","We focus on methods capable of scaling to large internet video datasets and, in the process, extracting foundational knowledge about the world's dynamics and physical human behaviour.","Such methods hold great promise for developing general-purpose robots.   ","We open with an overview of fundamental concepts relevant to the LfV-for-robotics setting.","This includes a discussion of the exciting benefits LfV methods can offer (e.g., improved generalization beyond the available robot data) and commentary on key LfV challenges (e.g., challenges related to missing information in video and LfV distribution shifts).","Our literature review begins with an analysis of video foundation model techniques that can extract knowledge from large, heterogeneous video datasets.","Next, we review methods that specifically leverage video data for robot learning.","Here, we categorise work according to which RL knowledge modality benefits from the use of video data.","We additionally highlight techniques for mitigating LfV challenges, including reviewing action representations that address the issue of missing action labels in video.   ","Finally, we examine LfV datasets and benchmarks, before concluding the survey by discussing challenges and opportunities in LfV. Here, we advocate for scalable approaches that can leverage the full range of available data and that target the key benefits of LfV. Overall, we hope this survey will serve as a comprehensive reference for the emerging field of LfV, catalysing further research in the area, and ultimately facilitating progress towards obtaining general-purpose robots."],"url":"http://arxiv.org/abs/2404.19664v1","category":"cs.RO"}
{"created":"2024-04-30 15:57:18","title":"PCA for Point Processes","abstract":"We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo\\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.","sentences":["We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level.","By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes.","The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis.","Key theoretical contributions include establishing a Karhunen-Lo\\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures.","We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns.","We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved.","We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness.","Our method is implemented in the pppca R-package."],"url":"http://arxiv.org/abs/2404.19661v1","category":"stat.ME"}
{"created":"2024-04-30 15:56:16","title":"Regularization of Riemannian optimization: Application to process tomography and quantum machine learning","abstract":"Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels. In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches. Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible. We apply the method to quantum process tomography and a quantum machine learning problem. Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography. Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data.","sentences":["Gradient descent algorithms on Riemannian manifolds have been used recently for the optimization of quantum channels.","In this contribution, we investigate the influence of various regularization terms added to the cost function of these gradient descent approaches.","Motivated by Lasso regularization, we apply penalties for large ranks of the quantum channel, favoring solutions that can be represented by as few Kraus operators as possible.","We apply the method to quantum process tomography and a quantum machine learning problem.","Suitably regularized models show faster convergence of the optimization as well as better fidelities in the case of process tomography.","Applied to quantum classification scenarios, the regularization terms can simplify the classifying quantum channel without degrading the accuracy of the classification, thereby revealing the minimum channel rank needed for the given input data."],"url":"http://arxiv.org/abs/2404.19659v1","category":"quant-ph"}
{"created":"2024-04-30 15:13:57","title":"SemiPL: A Semi-supervised Method for Event Sound Source Localization","abstract":"In recent years, Event Sound Source Localization has been widely applied in various fields. Recent works typically relying on the contrastive learning framework show impressive performance. However, all work is based on large relatively simple datasets. It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services. In this paper, we apply the existing model to a more complex dataset, explore the influence of parameters on the model, and propose a semi-supervised improvement method SemiPL. With the increase in data quantity and the influence of label quality, self-supervised learning will be an unstoppable trend. The experiment shows that the parameter adjustment will positively affect the existing model. In particular, SSPL achieved an improvement of 12.2% cIoU and 0.56% AUC in Chaotic World compared to the results provided. The code is available at: https://github.com/ly245422/SSPL","sentences":["In recent years, Event Sound Source Localization has been widely applied in various fields.","Recent works typically relying on the contrastive learning framework show impressive performance.","However, all work is based on large relatively simple datasets.","It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services.","In this paper, we apply the existing model to a more complex dataset, explore the influence of parameters on the model, and propose a semi-supervised improvement method SemiPL.","With the increase in data quantity and the influence of label quality, self-supervised learning will be an unstoppable trend.","The experiment shows that the parameter adjustment will positively affect the existing model.","In particular, SSPL achieved an improvement of 12.2% cIoU and 0.56% AUC in Chaotic World compared to the results provided.","The code is available at: https://github.com/ly245422/SSPL"],"url":"http://arxiv.org/abs/2404.19615v1","category":"cs.CV"}
{"created":"2024-04-30 15:12:26","title":"High-throughput discovery of metal oxides with high thermoelectric performance via interpretable feature engineering on small data","abstract":"In this work, we have proposed a data-driven screening framework combining the interpretable machine learning with high-throughput calculations to identify a series of metal oxides that exhibit both high-temperature tolerance and high power factors. Aiming at the problem of weak generalization ability of small data with power factors at high temperatures, we employ symbolic regression for feature creation which enhances the robustness of the model while preserving the physical meaning of features. 33 candidate metal oxides are finally targeted for high-temperature thermoelectric applications from a pool of 48,694 compounds in the Materials Project database. The Boltzmann transport theory is utilized to perform electrical transport properties calculations at 1,000 K. The relaxation time is approximated by employing constant electron-phonon coupling based on the deformation potential theory. Considering band degeneracy, the electron group velocity is obtained using the momentum matrix element method, yielding 28 materials with power factors greater than 50 ${\\mu}W cm^{-1} K^{-2} $. The high-throughput framework we proposed is instrumental in the selection of metal oxides for high-temperature thermoelectric applications. Furthermore, our data-driven analysis and transport calculation suggest that metal oxides rich in elements such as cerium (Ce), tin (Sn), and lead (Pb) tend to exhibit high power factors at high temperatures.","sentences":["In this work, we have proposed a data-driven screening framework combining the interpretable machine learning with high-throughput calculations to identify a series of metal oxides that exhibit both high-temperature tolerance and high power factors.","Aiming at the problem of weak generalization ability of small data with power factors at high temperatures, we employ symbolic regression for feature creation which enhances the robustness of the model while preserving the physical meaning of features.","33 candidate metal oxides are finally targeted for high-temperature thermoelectric applications from a pool of 48,694 compounds in the Materials Project database.","The Boltzmann transport theory is utilized to perform electrical transport properties calculations at 1,000 K. The relaxation time is approximated by employing constant electron-phonon coupling based on the deformation potential theory.","Considering band degeneracy, the electron group velocity is obtained using the momentum matrix element method, yielding 28 materials with power factors greater than 50 ${\\mu}W cm^{-1} K^{-2} $.","The high-throughput framework we proposed is instrumental in the selection of metal oxides for high-temperature thermoelectric applications.","Furthermore, our data-driven analysis and transport calculation suggest that metal oxides rich in elements such as cerium (Ce), tin (Sn), and lead (Pb) tend to exhibit high power factors at high temperatures."],"url":"http://arxiv.org/abs/2404.19613v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-30 14:53:07","title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models","abstract":"In this work, we present X-Diffusion, a cross-sectional diffusion model tailored for Magnetic Resonance Imaging (MRI) data. X-Diffusion is capable of generating the entire MRI volume from just a single MRI slice or optionally from few multiple slices, setting new benchmarks in the precision of synthesized MRIs from extremely sparse observations. The uniqueness lies in the novel view-conditional training and inference of X-Diffusion on MRI volumes, allowing for generalized MRI learning. Our evaluations span both brain tumour MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) and MRI modalities in the UK Biobank dataset, X-Diffusion is able to generate detailed 3D MRI volume from a single full-body DXA. Remarkably, the resultant MRIs not only stand out in precision on unseen examples (surpassing state-of-the-art results by large margins) but also flawlessly retain essential features of the original MRI, including tumour profiles, spine curvature, brain volume, and beyond. Furthermore, the trained X-Diffusion model on the MRI datasets attains a generalization capacity out-of-domain (e.g. generating knee MRIs even though it is trained on brains). The code is available on the project website https://emmanuelleb985.github.io/XDiffusion/ .","sentences":["In this work, we present X-Diffusion, a cross-sectional diffusion model tailored for Magnetic Resonance Imaging (MRI) data.","X-Diffusion is capable of generating the entire MRI volume from just a single MRI slice or optionally from few multiple slices, setting new benchmarks in the precision of synthesized MRIs from extremely sparse observations.","The uniqueness lies in the novel view-conditional training and inference of X-Diffusion on MRI volumes, allowing for generalized MRI learning.","Our evaluations span both brain tumour MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset.","Utilizing the paired pre-registered Dual-energy X-ray Absorptiometry (DXA) and MRI modalities in the UK Biobank dataset, X-Diffusion is able to generate detailed 3D MRI volume from a single full-body DXA.","Remarkably, the resultant MRIs not only stand out in precision on unseen examples (surpassing state-of-the-art results by large margins) but also flawlessly retain essential features of the original MRI, including tumour profiles, spine curvature, brain volume, and beyond.","Furthermore, the trained X-Diffusion model on the MRI datasets attains a generalization capacity out-of-domain (e.g. generating knee MRIs even though it is trained on brains).","The code is available on the project website https://emmanuelleb985.github.io/XDiffusion/ ."],"url":"http://arxiv.org/abs/2404.19604v1","category":"eess.IV"}
{"created":"2024-04-30 14:42:55","title":"Perceptual Constancy Constrained Single Opinion Score Calibration for Image Quality Assessment","abstract":"In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS). Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS. More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images. Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality. Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3). Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively. Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available.","sentences":["In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS).","Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS.","More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images.","Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality.","Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3).","Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively.","Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available."],"url":"http://arxiv.org/abs/2404.19595v1","category":"cs.CV"}
{"created":"2024-04-30 14:36:04","title":"Towards Interactively Improving ML Data Preparation Code via \"Shadow Pipelines\"","abstract":"Data scientists develop ML pipelines in an iterative manner: they repeatedly screen a pipeline for potential issues, debug it, and then revise and improve its code according to their findings. However, this manual process is tedious and error-prone. Therefore, we propose to support data scientists during this development cycle with automatically derived interactive suggestions for pipeline improvements. We discuss our vision to generate these suggestions with so-called shadow pipelines, hidden variants of the original pipeline that modify it to auto-detect potential issues, try out modifications for improvements, and suggest and explain these modifications to the user. We envision to apply incremental view maintenance-based optimisations to ensure low-latency computation and maintenance of the shadow pipelines. We conduct preliminary experiments to showcase the feasibility of our envisioned approach and the potential benefits of our proposed optimisations.","sentences":["Data scientists develop ML pipelines in an iterative manner: they repeatedly screen a pipeline for potential issues, debug it, and then revise and improve its code according to their findings.","However, this manual process is tedious and error-prone.","Therefore, we propose to support data scientists during this development cycle with automatically derived interactive suggestions for pipeline improvements.","We discuss our vision to generate these suggestions with so-called shadow pipelines, hidden variants of the original pipeline that modify it to auto-detect potential issues, try out modifications for improvements, and suggest and explain these modifications to the user.","We envision to apply incremental view maintenance-based optimisations to ensure low-latency computation and maintenance of the shadow pipelines.","We conduct preliminary experiments to showcase the feasibility of our envisioned approach and the potential benefits of our proposed optimisations."],"url":"http://arxiv.org/abs/2404.19591v1","category":"cs.DB"}
{"created":"2024-04-30 14:22:33","title":"Integrating Visuo-tactile Sensing with Haptic Feedback for Teleoperated Robot Manipulation","abstract":"Telerobotics enables humans to overcome spatial constraints and allows them to physically interact with the environment in remote locations. However, the sensory feedback provided by the system to the operator is often purely visual, limiting the operator's dexterity in manipulation tasks. In this work, we address this issue by equipping the robot's end-effector with high-resolution visuotactile GelSight sensors. Using low-cost MANUS-Gloves, we provide the operator with haptic feedback about forces acting at the points of contact in the form of vibration signals. We propose two different methods for estimating these forces; one based on estimating the movement of markers on the sensor surface and one deep-learning approach. Additionally, we integrate our system into a virtual-reality teleoperation pipeline in which a human operator controls both arms of a Tiago robot while receiving visual and haptic feedback. We believe that integrating haptic feedback is a crucial step for dexterous manipulation in teleoperated robotic systems.","sentences":["Telerobotics enables humans to overcome spatial constraints and allows them to physically interact with the environment in remote locations.","However, the sensory feedback provided by the system to the operator is often purely visual, limiting the operator's dexterity in manipulation tasks.","In this work, we address this issue by equipping the robot's end-effector with high-resolution visuotactile GelSight sensors.","Using low-cost MANUS-Gloves, we provide the operator with haptic feedback about forces acting at the points of contact in the form of vibration signals.","We propose two different methods for estimating these forces; one based on estimating the movement of markers on the sensor surface and one deep-learning approach.","Additionally, we integrate our system into a virtual-reality teleoperation pipeline in which a human operator controls both arms of a Tiago robot while receiving visual and haptic feedback.","We believe that integrating haptic feedback is a crucial step for dexterous manipulation in teleoperated robotic systems."],"url":"http://arxiv.org/abs/2404.19585v1","category":"cs.RO"}
{"created":"2024-04-30 13:50:20","title":"Short term vs. long term: optimization of microswimmer navigation on different time horizons","abstract":"We use reinforcement learning to find strategies that allow microswimmers in turbulence to avoid regions of large strain. This question is motivated by the hypothesis that swimming microorganisms tend to avoid such regions to minimise the risk of predation. We ask which local cues a microswimmer must measure to efficiently avoid such straining regions. We find that it can succeed without directional information, merely by measuring the magnitude of the local strain. However, the swimmer avoids straining regions more efficiently if it can measure the sign of local strain gradients. We compare our results with those of an earlier study [Mousavi et al. arxiv:2309.09641] where a short-time expansion was used to find optimal strategies. We find that the short-time strategies work well in some cases but not in others. We derive a new theory that explains when the time-horizon matters for our optimisation problem, and when it does not. We find the strategy with best performance when the time-horizon coincides with the correlation time of the turbulent fluctuations. We also explain how the update frequency (the frequency at which the swimmer updates its state) affects the found strategies. We find that higher update frequencies yield better performance, as long as the time between updates is smaller than the correlation time of the flow.","sentences":["We use reinforcement learning to find strategies that allow microswimmers in turbulence to avoid regions of large strain.","This question is motivated by the hypothesis that swimming microorganisms tend to avoid such regions to minimise the risk of predation.","We ask which local cues a microswimmer must measure to efficiently avoid such straining regions.","We find that it can succeed without directional information, merely by measuring the magnitude of the local strain.","However, the swimmer avoids straining regions more efficiently if it can measure the sign of local strain gradients.","We compare our results with those of an earlier study [Mousavi et al. arxiv:2309.09641] where a short-time expansion was used to find optimal strategies.","We find that the short-time strategies work well in some cases but not in others.","We derive a new theory that explains when the time-horizon matters for our optimisation problem, and when it does not.","We find the strategy with best performance when the time-horizon coincides with the correlation time of the turbulent fluctuations.","We also explain how the update frequency (the frequency at which the swimmer updates its state) affects the found strategies.","We find that higher update frequencies yield better performance, as long as the time between updates is smaller than the correlation time of the flow."],"url":"http://arxiv.org/abs/2404.19561v1","category":"physics.flu-dyn"}
{"created":"2024-04-30 17:59:47","title":"MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model","abstract":"This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD). By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.","sentences":["This work introduces MotionLCM, extending controllable motion generation to a real-time level.","Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency.","To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD).","By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation.","To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation.","By employing these techniques, our approach can generate human motions with text and control signals in real-time.","Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency."],"url":"http://arxiv.org/abs/2404.19759v1","category":"cs.CV"}
{"created":"2024-04-30 16:54:59","title":"RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting","abstract":"We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.","sentences":["We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting.","The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme.","We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors.","By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost.","For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors.","We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable.","We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians.","In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time.","We show real-time reconstructions of a variety of large scenes.","Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy."],"url":"http://arxiv.org/abs/2404.19706v2","category":"cs.CV"}
{"created":"2024-04-30 15:49:09","title":"BAD-NEUS: Rapidly converging trajectory stratification","abstract":"An issue for molecular dynamics simulations is that events of interest often involve timescales that are much longer than the simulation time step, which is set by the fastest timescales of the model. Because of this timescale separation, direct simulation of many events is prohibitively computationally costly. This issue can be overcome by aggregating information from many relatively short simulations that sample segments of trajectories involving events of interest. This is the strategy of Markov state models (MSMs) and related approaches, but such methods suffer from approximation error because the variables defining the states generally do not capture the dynamics fully. By contrast, once converged, the weighted ensemble (WE) method aggregates information from trajectory segments so as to yield unbiased estimates of both thermodynamic and kinetic statistics. Unfortunately, errors decay no faster than unbiased simulation in WE. Here we introduce a theoretical framework for describing WE that shows that introduction of an element of stratification, as in nonequilibrium umbrella sampling (NEUS), accelerates convergence. Then, building on ideas from MSMs and related methods, we propose an improved stratification that allows approximation error to be reduced systematically. We show that the improved stratification can decrease simulation times required to achieve a desired precision by orders of magnitude.","sentences":["An issue for molecular dynamics simulations is that events of interest often involve timescales that are much longer than the simulation time step, which is set by the fastest timescales of the model.","Because of this timescale separation, direct simulation of many events is prohibitively computationally costly.","This issue can be overcome by aggregating information from many relatively short simulations that sample segments of trajectories involving events of interest.","This is the strategy of Markov state models (MSMs) and related approaches, but such methods suffer from approximation error because the variables defining the states generally do not capture the dynamics fully.","By contrast, once converged, the weighted ensemble (WE) method aggregates information from trajectory segments so as to yield unbiased estimates of both thermodynamic and kinetic statistics.","Unfortunately, errors decay no faster than unbiased simulation in WE.","Here we introduce a theoretical framework for describing WE that shows that introduction of an element of stratification, as in nonequilibrium umbrella sampling (NEUS), accelerates convergence.","Then, building on ideas from MSMs and related methods, we propose an improved stratification that allows approximation error to be reduced systematically.","We show that the improved stratification can decrease simulation times required to achieve a desired precision by orders of magnitude."],"url":"http://arxiv.org/abs/2404.19653v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-30 15:45:53","title":"Best polynomial approximation for non-autonomous linear ODEs in the $\\star$-product framework","abstract":"We present the first formulation of the optimal polynomial approximation of the solution of linear non-autonomous systems of ODEs in the framework of the so-called $\\star$-product. This product is the basis of new approaches for the solution of such ODEs, both in the analytical and the numerical sense. The paper shows how to formally state the problem and derives upper bounds for its error.","sentences":["We present the first formulation of the optimal polynomial approximation of the solution of linear non-autonomous systems of ODEs in the framework of the so-called $\\star$-product.","This product is the basis of new approaches for the solution of such ODEs, both in the analytical and the numerical sense.","The paper shows how to formally state the problem and derives upper bounds for its error."],"url":"http://arxiv.org/abs/2404.19645v1","category":"math.CA"}
{"created":"2024-04-30 14:15:16","title":"New EVENODD+ Codes with More Flexible Parameters and Lower Complexity","abstract":"EVENODD+ codes are binary maximum distance separable (MDS) array codes for correcting double disk failures in RAID-6 with asymptotically optimal encoding/decoding/update complexities. However, the number of bits stored in each disk of EVENODD+ codes should be an odd number minus one. In this paper, we present a new construction of EVENODD+ codes that have more flexible parameters. The number of bits stored in each disk of our codes is an odd minus one times any positive integer. Moreover, our codes not only have asymptotically optimal encoding/decoding/update complexities but also have lower encoding/decoding/update complexities than the existing EVENODD+ codes.","sentences":["EVENODD+ codes are binary maximum distance separable (MDS) array codes for correcting double disk failures in RAID-6 with asymptotically optimal encoding/decoding/update complexities.","However, the number of bits stored in each disk of EVENODD+ codes should be an odd number minus one.","In this paper, we present a new construction of EVENODD+ codes that have more flexible parameters.","The number of bits stored in each disk of our codes is an odd minus one times any positive integer.","Moreover, our codes not only have asymptotically optimal encoding/decoding/update complexities but also have lower encoding/decoding/update complexities than the existing EVENODD+ codes."],"url":"http://arxiv.org/abs/2404.19578v1","category":"cs.IT"}
{"created":"2024-04-30 13:31:37","title":"Entanglement-assisted phase estimation algorithm for calculating dynamical response functions","abstract":"Dynamical response functions are fundamental quantities to describe the excited-state properties in quantum many-body systems. Quantum algorithms have been proposed to evaluate these quantities by means of quantum phase estimation (QPE), where the energy spectra are directly extracted from the QPE measurement outcomes in the frequency domain. Accurate estimation of excitation energies and transition probabilities with these QPE-based approaches is, however, challenging because of the problem of spectral leakage (or peak broadening) which is inherent in the QPE algorithm. To overcome this issue, in this work we consider an extension of the QPE-based approach adopting the optimal entangled input states, which is known to achieve the Heisenberg-limited scaling for the estimation precision. We demonstrate that with this method the peaks in the calculated energy spectra are more localized than those calculated by the original QPE-based approaches, suggesting the mitigation of the spectral leakage problem. By analyzing the probability distribution with the entangled phase estimation, we propose a simple scheme to better estimate both the transition energies and the corresponding transition probabilities of the peaks of interest in the spectra. The validity of our prescription is demonstrated by numerical simulations in various quantum many-body problems: the spectral function of a simple electron-plasmon model in condensed-matter physics, the dipole transitions of the H$_2$O molecule in quantum chemistry, and the electromagnetic transitions of the $^6$Li nucleus in nuclear physics.","sentences":["Dynamical response functions are fundamental quantities to describe the excited-state properties in quantum many-body systems.","Quantum algorithms have been proposed to evaluate these quantities by means of quantum phase estimation (QPE), where the energy spectra are directly extracted from the QPE measurement outcomes in the frequency domain.","Accurate estimation of excitation energies and transition probabilities with these QPE-based approaches is, however, challenging because of the problem of spectral leakage (or peak broadening) which is inherent in the QPE algorithm.","To overcome this issue, in this work we consider an extension of the QPE-based approach adopting the optimal entangled input states, which is known to achieve the Heisenberg-limited scaling for the estimation precision.","We demonstrate that with this method the peaks in the calculated energy spectra are more localized than those calculated by the original QPE-based approaches, suggesting the mitigation of the spectral leakage problem.","By analyzing the probability distribution with the entangled phase estimation, we propose a simple scheme to better estimate both the transition energies and the corresponding transition probabilities of the peaks of interest in the spectra.","The validity of our prescription is demonstrated by numerical simulations in various quantum many-body problems: the spectral function of a simple electron-plasmon model in condensed-matter physics, the dipole transitions of the H$_2$O molecule in quantum chemistry, and the electromagnetic transitions of the $^6$Li nucleus in nuclear physics."],"url":"http://arxiv.org/abs/2404.19554v1","category":"quant-ph"}
{"created":"2024-04-30 13:11:11","title":"Shocks in the Warm Neutral Medium I -- Theoretical model","abstract":"Context. Atomic and molecular line emissions from shocks may provide valuable information on the injection of mechanical energy in the interstellar medium (ISM), the generation of turbulence, and the processes of phase transition between the Warm Neutral Medium (WNM) and the Cold Neutral Medium (CNM).Aims. In this series of papers, we investigate the properties of shocks propagating in the WNM. Our objective is to identify the tracers of these shocks, use them to interpret ancillary observations of the local diffuse matter, and provide predictions for future observations.   Methods. Shocks propagating in the WNM are studied using the Paris-Durham shock code, a multi-fluid model built to follow the thermodynamical and chemical structures of shock waves, at steady-state, in a plane-parallel geometry. The code, already designed to take into account the impact of an external radiation field, is updated to treat self-irradiated shocks at intermediate ($30<V_S <100$ km s$^{-1}$) and high velocity ($V_S \\ge 100$ km s$^{-1}$) which emit ultraviolet (UV), extreme-ultraviolet (EUV), and X-ray photons. The couplings between the photons generated by the shock, the radiative precursor, and the shock structure are computed self-consistently using an exact radiative transfer algorithm for line emission. The resulting code is explored over a wide range of parameters ($0.1 \\le n_H \\le 2$ cm$^{-3}$, $10 \\le V_S \\le 500$ km s$^{-1}$, and $0.1 \\le B \\le 10$ $\\mu$G) that covers the typical conditions of the WNM in the solar neighborhood.   Results. The explored physical conditions are prompt to the existence of a diversity of stationary magnetohydrodynamic solutions, including J-type, CJ-type, and C-type shocks. These shocks are found to naturally induce phase transition between the WNM and the CNM, provided that the postshock thermal pressure is larger than the maximum pressure of the WNM and that the maximum density allowed by magnetic compression is larger than the minimum density of the CNM. The input flux of mechanical energy is primarily reprocessed into line emissions from the X-ray to the submillimeter domain. Intermediate and high velocity shocks are found to generate a UV radiation field that scales as $V_S^3$ for $V_S < 100$ km s$^{-1}$ and as $V_S^2$ at higher velocities, and an X-ray radiation field that scales as $V_S^3$ for $V_S \\ge 100$ km s$^{-1}$. Both radiation fields may extend over large distances in the preshock depending of the density of the surrounding medium and the hardness of the X-ray field which is solely driven by the shock velocity.   Conclusions. This first paper presents the thermochemical trajectories of shocks in the WNM and their associated spectra. It corresponds to a new milestone in the development of the Paris-Durham shock code and a stepping stone for the analysis of observations that will be carried out in forthcoming works.","sentences":["Context.","Atomic and molecular line emissions from shocks may provide valuable information on the injection of mechanical energy in the interstellar medium (ISM), the generation of turbulence, and the processes of phase transition between the Warm Neutral Medium (WNM) and the Cold Neutral Medium (CNM).Aims.","In this series of papers, we investigate the properties of shocks propagating in the WNM.","Our objective is to identify the tracers of these shocks, use them to interpret ancillary observations of the local diffuse matter, and provide predictions for future observations.   Methods.","Shocks propagating in the WNM are studied using the Paris-Durham shock code, a multi-fluid model built to follow the thermodynamical and chemical structures of shock waves, at steady-state, in a plane-parallel geometry.","The code, already designed to take into account the impact of an external radiation field, is updated to treat self-irradiated shocks at intermediate ($30<V_S <100$ km s$^{-1}$) and high velocity ($V_S \\ge 100$ km s$^{-1}$) which emit ultraviolet (UV), extreme-ultraviolet (EUV), and X-ray photons.","The couplings between the photons generated by the shock, the radiative precursor, and the shock structure are computed self-consistently using an exact radiative transfer algorithm for line emission.","The resulting code is explored over a wide range of parameters ($0.1 \\le n_H \\le 2$ cm$^{-3}$, $10 \\le V_S \\le 500$ km s$^{-1}$, and $0.1 \\le B \\le 10$ $\\mu$G) that covers the typical conditions of the WNM in the solar neighborhood.   ","Results.","The explored physical conditions are prompt to the existence of a diversity of stationary magnetohydrodynamic solutions, including J-type, CJ-type, and C-type shocks.","These shocks are found to naturally induce phase transition between the WNM and the CNM, provided that the postshock thermal pressure is larger than the maximum pressure of the WNM and that the maximum density allowed by magnetic compression is larger than the minimum density of the CNM.","The input flux of mechanical energy is primarily reprocessed into line emissions from the X-ray to the submillimeter domain.","Intermediate and high velocity shocks are found to generate a UV radiation field that scales as $V_S^3$ for $V_S < 100$ km s$^{-1}$ and as $V_S^2$ at higher velocities, and an X-ray radiation field that scales as $V_S^3$ for $V_S \\ge 100$ km s$^{-1}$. Both radiation fields may extend over large distances in the preshock depending of the density of the surrounding medium and the hardness of the X-ray field which is solely driven by the shock velocity.   ","Conclusions.","This first paper presents the thermochemical trajectories of shocks in the WNM and their associated spectra.","It corresponds to a new milestone in the development of the Paris-Durham shock code and a stepping stone for the analysis of observations that will be carried out in forthcoming works."],"url":"http://arxiv.org/abs/2404.19533v1","category":"astro-ph.GA"}
{"created":"2024-04-30 13:10:19","title":"Optimized Soft-Aided Decoding of OFEC and Staircase Codes","abstract":"We propose a novel soft-aided hard-decision decoding algorithm for general product-like codes. It achieves error correcting performance similar to that of a soft-decision turbo decoder for staircase and OFEC codes, while maintaining a low complexity.","sentences":["We propose a novel soft-aided hard-decision decoding algorithm for general product-like codes.","It achieves error correcting performance similar to that of a soft-decision turbo decoder for staircase and OFEC codes, while maintaining a low complexity."],"url":"http://arxiv.org/abs/2404.19532v1","category":"cs.IT"}
{"created":"2024-04-30 12:42:05","title":"Choosing a consultant in a dynamic investment problem","abstract":"Consider a dynamic decision-making scenario where at every stage the investor has to choose between investing in one of two projects or gathering more information. At each stage, the investor may seek counsel from one of several consultants, who, for a fixed cost, provide partial information about the realized state. We explore the optimal strategy and its dependence on the belief and the consultation cost. Our analysis reveals that if one of the consultants discloses the state with a nonzero probability, this consultant will be used in any optimal strategy, provided the consultation cost is sufficiently small.","sentences":["Consider a dynamic decision-making scenario where at every stage the investor has to choose between investing in one of two projects or gathering more information.","At each stage, the investor may seek counsel from one of several consultants, who, for a fixed cost, provide partial information about the realized state.","We explore the optimal strategy and its dependence on the belief and the consultation cost.","Our analysis reveals that if one of the consultants discloses the state with a nonzero probability, this consultant will be used in any optimal strategy, provided the consultation cost is sufficiently small."],"url":"http://arxiv.org/abs/2404.19507v1","category":"cs.IT"}
{"created":"2024-04-30 12:31:03","title":"Light Cone Cancellation for Variational Quantum Eigensolver Ansatz","abstract":"Variational Quantum Algorithms (VQAs) represent a class of algorithms that utilize a hybrid approach, combining classical and quantum computing techniques. In this approach, classical computers serve as optimizers that update circuit parameters to find approximate solutions to complex problems. In this study, we apply a method known as Light Cone Cancellation (LCC) to optimize variational circuits, effectively reducing the required number of qubits and gates for circuit simulation. We then evaluate the performance of LCC one of the VQAs -- the Variational Quantum Eigensolver (VQE) -- to address the Max-Cut problem. Compared with the Quantum Approximate Optimization Algorithm (QAOA), VQE offers greater degrees of freedom at lower circuit depths. By applying LCC to VQE, we can shift the complexity of circuit simulation from the number of qubits to the number of edges in the graph, i.e., from exponential time to polynomial time. This enables us to solve large problems up to 50 vertices, without actually simulating the entire circuit. From our simulation in a 7-qubit and a 27-qubit noisy devices, we show that LCC yields higher approximation ratios than those cases without LCC, implying that the effect of noise is reduced when LCC is applied.","sentences":["Variational Quantum Algorithms (VQAs) represent a class of algorithms that utilize a hybrid approach, combining classical and quantum computing techniques.","In this approach, classical computers serve as optimizers that update circuit parameters to find approximate solutions to complex problems.","In this study, we apply a method known as Light Cone Cancellation (LCC) to optimize variational circuits, effectively reducing the required number of qubits and gates for circuit simulation.","We then evaluate the performance of LCC one of the VQAs -- the Variational Quantum Eigensolver (VQE) -- to address the Max-Cut problem.","Compared with the Quantum Approximate Optimization Algorithm (QAOA), VQE offers greater degrees of freedom at lower circuit depths.","By applying LCC to VQE, we can shift the complexity of circuit simulation from the number of qubits to the number of edges in the graph, i.e., from exponential time to polynomial time.","This enables us to solve large problems up to 50 vertices, without actually simulating the entire circuit.","From our simulation in a 7-qubit and a 27-qubit noisy devices, we show that LCC yields higher approximation ratios than those cases without LCC, implying that the effect of noise is reduced when LCC is applied."],"url":"http://arxiv.org/abs/2404.19497v1","category":"quant-ph"}
{"created":"2024-04-30 12:23:37","title":"Reducing Communication Overhead in the IoT-Edge-Cloud Continuum: A Survey on Protocols and Data Reduction Strategies","abstract":"The adoption of the Internet of Things (IoT) deployments has led to a sharp increase in network traffic as a vast number of IoT devices communicate with each other and IoT services through the IoT-edge-cloud continuum. This network traffic increase poses a major challenge to the global communications infrastructure since it hinders communication performance and also puts significant strain on the energy consumption of IoT devices. To address these issues, efficient and collaborative IoT solutions which enable information exchange while reducing the transmitted data and associated network traffic are crucial. This survey provides a comprehensive overview of the communication technologies and protocols as well as data reduction strategies that contribute to this goal. First, we present a comparative analysis of prevalent communication technologies in the IoT domain, highlighting their unique characteristics and exploring the potential for protocol composition and joint usage to enhance overall communication efficiency within the IoT-edge-cloud continuum. Next, we investigate various data traffic reduction techniques tailored to the IoT-edge-cloud context and evaluate their applicability and effectiveness on resource-constrained and devices. Finally, we investigate the emerging concepts that have the potential to further reduce the communication overhead in the IoT-edge-cloud continuum, including cross-layer optimization strategies and Edge AI techniques for IoT data reduction. The paper offers a comprehensive roadmap for developing efficient and scalable solutions across the layers of the IoT-edge-cloud continuum that are beneficial for real-time processing to alleviate network congestion in complex IoT environments.","sentences":["The adoption of the Internet of Things (IoT) deployments has led to a sharp increase in network traffic as a vast number of IoT devices communicate with each other and IoT services through the IoT-edge-cloud continuum.","This network traffic increase poses a major challenge to the global communications infrastructure since it hinders communication performance and also puts significant strain on the energy consumption of IoT devices.","To address these issues, efficient and collaborative IoT solutions which enable information exchange while reducing the transmitted data and associated network traffic are crucial.","This survey provides a comprehensive overview of the communication technologies and protocols as well as data reduction strategies that contribute to this goal.","First, we present a comparative analysis of prevalent communication technologies in the IoT domain, highlighting their unique characteristics and exploring the potential for protocol composition and joint usage to enhance overall communication efficiency within the IoT-edge-cloud continuum.","Next, we investigate various data traffic reduction techniques tailored to the IoT-edge-cloud context and evaluate their applicability and effectiveness on resource-constrained and devices.","Finally, we investigate the emerging concepts that have the potential to further reduce the communication overhead in the IoT-edge-cloud continuum, including cross-layer optimization strategies and Edge AI techniques for IoT data reduction.","The paper offers a comprehensive roadmap for developing efficient and scalable solutions across the layers of the IoT-edge-cloud continuum that are beneficial for real-time processing to alleviate network congestion in complex IoT environments."],"url":"http://arxiv.org/abs/2404.19492v1","category":"cs.NI"}
{"created":"2024-04-30 11:45:49","title":"Global Phase Helps in Quantum Search: Yet Another Look at the Welded Tree Problem","abstract":"Up to now, relatively few exponential quantum speed-ups have been achieved. Out of them, the welded tree problem (Childs, Cleve, Deotto, Farhi, Gutmann, and Spielman'2003) is one of the unusual examples, as the exponential speed-up is attained by a quantum walk. In this paper, we give a very short proof of the optimal linear hitting time for this problem by a discrete-time quantum walk, which is based on a simple modification of the electric quantum walk framework. The same technique can be applied to other 1-dimensional hierarchical graphs, yielding results similar to (Balasubramanian, Li, and Harrow'2023).","sentences":["Up to now, relatively few exponential quantum speed-ups have been achieved.","Out of them, the welded tree problem (Childs, Cleve, Deotto, Farhi, Gutmann, and Spielman'2003) is one of the unusual examples, as the exponential speed-up is attained by a quantum walk.","In this paper, we give a very short proof of the optimal linear hitting time for this problem by a discrete-time quantum walk, which is based on a simple modification of the electric quantum walk framework.","The same technique can be applied to other 1-dimensional hierarchical graphs, yielding results similar to (Balasubramanian, Li, and Harrow'2023)."],"url":"http://arxiv.org/abs/2404.19476v1","category":"quant-ph"}
{"created":"2024-04-30 11:43:37","title":"TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models","abstract":"Diffusion models have emerged as effective tools for generating diverse and high-quality content. However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions. In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: Crop Fusion for quality enhancement and Cross Sampling for efficiency optimization. We introduce a training-free optimizing stage to refine the similarity of the adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process. A comprehensive evaluation is conducted to compare TwinDiffusion with the existing methods, considering factors including coherence, fidelity, compatibility, and efficiency. The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation.","sentences":["Diffusion models have emerged as effective tools for generating diverse and high-quality content.","However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions.","In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: Crop Fusion for quality enhancement and Cross Sampling for efficiency optimization.","We introduce a training-free optimizing stage to refine the similarity of the adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process.","A comprehensive evaluation is conducted to compare TwinDiffusion with the existing methods, considering factors including coherence, fidelity, compatibility, and efficiency.","The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation."],"url":"http://arxiv.org/abs/2404.19475v1","category":"cs.CV"}
{"created":"2024-04-30 11:40:07","title":"Quantum Relaxation for Solving Multiple Knapsack Problems","abstract":"Combinatorial problems are a common challenge in business, requiring finding optimal solutions under specified constraints. While significant progress has been made with variational approaches such as QAOA, most problems addressed are unconstrained (such as Max-Cut). In this study, we investigate a hybrid quantum-classical method for constrained optimization problems, particularly those with knapsack constraints that occur frequently in financial and supply chain applications. Our proposed method relies firstly on relaxations to local quantum Hamiltonians, defined through commutative maps. Drawing inspiration from quantum random access code (QRAC) concepts, particularly Quantum Random Access Optimizer (QRAO), we explore QRAO's potential in solving large constrained optimization problems. We employ classical techniques like Linear Relaxation as a presolve mechanism to handle constraints and cope further with scalability. We compare our approach with QAOA and present the final results for a real-world procurement optimization problem: a significant sized multi-knapsack-constrained problem.","sentences":["Combinatorial problems are a common challenge in business, requiring finding optimal solutions under specified constraints.","While significant progress has been made with variational approaches such as QAOA, most problems addressed are unconstrained (such as Max-Cut).","In this study, we investigate a hybrid quantum-classical method for constrained optimization problems, particularly those with knapsack constraints that occur frequently in financial and supply chain applications.","Our proposed method relies firstly on relaxations to local quantum Hamiltonians, defined through commutative maps.","Drawing inspiration from quantum random access code (QRAC) concepts, particularly Quantum Random Access Optimizer (QRAO), we explore QRAO's potential in solving large constrained optimization problems.","We employ classical techniques like Linear Relaxation as a presolve mechanism to handle constraints and cope further with scalability.","We compare our approach with QAOA and present the final results for a real-world procurement optimization problem: a significant sized multi-knapsack-constrained problem."],"url":"http://arxiv.org/abs/2404.19474v1","category":"quant-ph"}
{"created":"2024-04-30 11:28:28","title":"Optimal E-Values for Exponential Families: the Simple Case","abstract":"We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\\cal Q}$ and the null are in a certain relation. Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.","sentences":["We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family.","Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time.","Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families.","We start with a simple alternative $Q$ and a regular exponential family null.","Together these induce a second exponential family ${\\cal Q}$ containing $Q$, with the same sufficient statistic as the null.","Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\\cal Q}$ and the null are in a certain relation.","Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families."],"url":"http://arxiv.org/abs/2404.19465v1","category":"stat.ME"}
{"created":"2024-04-30 11:24:50","title":"Enhancing Physical Layer Security with Deep SIMO Auto-Encoder and RF Impairments Modeling","abstract":"This paper presents a novel approach to achieving secure wireless communication by leveraging the inherent characteristics of wireless channels through end-to-end learning using a single-input-multiple-output (SIMO) autoencoder (AE). To ensure a more realistic signal transmission, we derive the signal model that captures all radio frequency (RF) hardware impairments to provide reliable and secure communication. Performance evaluations against traditional linear decoders, such as zero-forcing (ZR) and linear minimum mean square error (LMMSE), and the optimal nonlinear decoder, maximum likelihood (ML), demonstrate that the AE-based SIMO model exhibits superior bit error rate (BER) performance, but with a substantial gap even in the presence of RF hardware impairments. Additionally, the proposed model offers enhanced security features, preventing potential eavesdroppers from intercepting transmitted information and leveraging RF impairments for augmented physical layer security and device identification. These findings underscore the efficacy of the proposed end-to-end learning approach in achieving secure and robust wireless communication.","sentences":["This paper presents a novel approach to achieving secure wireless communication by leveraging the inherent characteristics of wireless channels through end-to-end learning using a single-input-multiple-output (SIMO) autoencoder (AE).","To ensure a more realistic signal transmission, we derive the signal model that captures all radio frequency (RF) hardware impairments to provide reliable and secure communication.","Performance evaluations against traditional linear decoders, such as zero-forcing (ZR) and linear minimum mean square error (LMMSE), and the optimal nonlinear decoder, maximum likelihood (ML), demonstrate that the AE-based SIMO model exhibits superior bit error rate (BER) performance, but with a substantial gap even in the presence of RF hardware impairments.","Additionally, the proposed model offers enhanced security features, preventing potential eavesdroppers from intercepting transmitted information and leveraging RF impairments for augmented physical layer security and device identification.","These findings underscore the efficacy of the proposed end-to-end learning approach in achieving secure and robust wireless communication."],"url":"http://arxiv.org/abs/2404.19463v1","category":"eess.SP"}
{"created":"2024-04-30 11:19:05","title":"AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples","abstract":"Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than 100 attack implementations with a total of over 800 different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.","sentences":["Adversarial examples are typically optimized with gradient-based attacks.","While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models.","This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others.","In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks.","To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences.","We then introduce our framework, which evaluates their effectiveness and efficiency.","We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget.","Our extensive experimental analysis compares more than 100 attack implementations with a total of over 800 different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches.","Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all.","We release AttackBench as a publicly available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples."],"url":"http://arxiv.org/abs/2404.19460v1","category":"cs.LG"}
{"created":"2024-04-30 10:29:54","title":"Fine-tuning the Microstructure and Photophysical Characteristics of Fluorescent Conjugated Copolymers Using Photoalignment and Liquid-crystalline Ordering","abstract":"Replicating the microstructure and near-unity excitation energy transfer efficiency in natural light-harvesting complexes (LHCs) remains a major challenge for synthetic energy-harvesting devices. Biological photosynthesis can spontaneously regulate the active ensembles of involved energy absorbing and funnelling chlorophyll-containing proteins in response to fluctuating sunlight. Here we utilize liquid crystalline (LC) ordering to fine-tune the polymer packing and photophysical properties in liquid crystalline conjugated polymer (LCCP) films for LHC biomimicry and optimizing photoluminescence quantum efficiency (PLQE). We show that the long-range orientational ordering present in a LC phase of poly(9,9-dioctylfluorene-co-benzothiadiazole) (F8BT) stabilizes a small fraction of randomly-oriented F8BT nanocrystals dispersed in an amorphous matrix of disordered F8BT chains, hence resembling a self-doped host-guest system whereby excitation energy funnelling and PLQE are reinforced significantly by three-dimensional donor-to-acceptor Forster resonance energy transfer (FRET) and dominant intrachain emission in the nano-crystalline acceptor. Furthermore, the photoalignment of nematic F8BT layers is combined to fabricate long-sought large-area-extended monodomains which exhibit >60% crystallinity and ~20 nm-long interchain packing order, whilst also promoting linearly polarized emission, a new band-edge absorption species, and an extra emissive interchain excited state. Our micro-PL spectral results support the feasibility of making use of self-doped F8BT nematic films for bio-mimicry of certain structural basis and light-harvesting properties of naturally occurring LHCs.","sentences":["Replicating the microstructure and near-unity excitation energy transfer efficiency in natural light-harvesting complexes (LHCs) remains a major challenge for synthetic energy-harvesting devices.","Biological photosynthesis can spontaneously regulate the active ensembles of involved energy absorbing and funnelling chlorophyll-containing proteins in response to fluctuating sunlight.","Here we utilize liquid crystalline (LC) ordering to fine-tune the polymer packing and photophysical properties in liquid crystalline conjugated polymer (LCCP) films for LHC biomimicry and optimizing photoluminescence quantum efficiency (PLQE).","We show that the long-range orientational ordering present in a LC phase of poly(9,9-dioctylfluorene-co-benzothiadiazole) (F8BT) stabilizes a small fraction of randomly-oriented F8BT nanocrystals dispersed in an amorphous matrix of disordered F8BT chains, hence resembling a self-doped host-guest system whereby excitation energy funnelling and PLQE are reinforced significantly by three-dimensional donor-to-acceptor Forster resonance energy transfer (FRET) and dominant intrachain emission in the nano-crystalline acceptor.","Furthermore, the photoalignment of nematic F8BT layers is combined to fabricate long-sought large-area-extended monodomains which exhibit >60% crystallinity and ~20 nm-long interchain packing order, whilst also promoting linearly polarized emission, a new band-edge absorption species, and an extra emissive interchain excited state.","Our micro-PL spectral results support the feasibility of making use of self-doped F8BT nematic films for bio-mimicry of certain structural basis and light-harvesting properties of naturally occurring LHCs."],"url":"http://arxiv.org/abs/2404.19435v1","category":"cond-mat.soft"}
{"created":"2024-04-30 10:17:21","title":"Lancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication Overlapping","abstract":"The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters. However, it faces the challenge of extended all-to-all communication latency during the training process. Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation. Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements. In our study, we extend the scope of this challenge by considering overlap at the broader training graph level. During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining. In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations. We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training. Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%. Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions.","sentences":["The Mixture-of-Expert (MoE) technique plays a crucial role in expanding the size of DNN model parameters.","However, it faces the challenge of extended all-to-all communication latency during the training process.","Existing methods attempt to mitigate this issue by overlapping all-to-all with expert computation.","Yet, these methods frequently fall short of achieving sufficient overlap, consequently restricting the potential for performance enhancements.","In our study, we extend the scope of this challenge by considering overlap at the broader training graph level.","During the forward pass, we enable non-MoE computations to overlap with all-to-all through careful partitioning and pipelining.","In the backward pass, we achieve overlap with all-to-all by scheduling gradient weight computations.","We implement these techniques in Lancet, a system using compiler-based optimization to automatically enhance MoE model training.","Our extensive evaluation reveals that Lancet significantly reduces the time devoted to non-overlapping communication, by as much as 77%.","Moreover, it achieves a notable end-to-end speedup of up to 1.3 times when compared to the state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.19429v1","category":"cs.DC"}
{"created":"2024-04-30 10:17:01","title":"From Quantum Mechanics to Quantum Software Engineering","abstract":"Victor Hugo's timeless observation, \"Nothing is more powerful than an idea whose time has come\", resonates today as Quantum Computing, once only a dream of a physicist, stands at the threshold of reality with the potential to revolutionise the world. To comprehend the surge of attention it commands today, one must delve into the motivations that birthed and nurtured Quantum Computing. While the past of Quantum Computing provides insights into the present, the future could unfold through the lens of Quantum Software Engineering. Quantum Software Engineering, guided by its principles and methodologies investigates the most effective ways to interact with Quantum Computers to unlock their true potential and usher in a new era of possibilities. To gain insight into the present landscape and anticipate the trajectory of Quantum Computing and Quantum Software Engineering, this paper embarks on a journey through their evolution and outlines potential directions for future research.","sentences":["Victor Hugo's timeless observation, \"Nothing is more powerful than an idea whose time has come\", resonates today as Quantum Computing, once only a dream of a physicist, stands at the threshold of reality with the potential to revolutionise the world.","To comprehend the surge of attention it commands today, one must delve into the motivations that birthed and nurtured Quantum Computing.","While the past of Quantum Computing provides insights into the present, the future could unfold through the lens of Quantum Software Engineering.","Quantum Software Engineering, guided by its principles and methodologies investigates the most effective ways to interact with Quantum Computers to unlock their true potential and usher in a new era of possibilities.","To gain insight into the present landscape and anticipate the trajectory of Quantum Computing and Quantum Software Engineering, this paper embarks on a journey through their evolution and outlines potential directions for future research."],"url":"http://arxiv.org/abs/2404.19428v1","category":"quant-ph"}
{"created":"2024-04-30 10:11:44","title":"Let's Focus: Focused Backdoor Attack against Federated Transfer Learning","abstract":"Federated Transfer Learning (FTL) is the most general variation of Federated Learning. According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data. After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor. Each involved client contributes by locally training only the classification layers on a private training set. The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor. State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients. Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step. Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation. In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class. Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario. With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning.","sentences":["Federated Transfer Learning (FTL) is the most general variation of Federated Learning.","According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data.","After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor.","Each involved client contributes by locally training only the classification layers on a private training set.","The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor.","State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients.","Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step.","Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation.","In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class.","Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario.","With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning."],"url":"http://arxiv.org/abs/2404.19420v1","category":"cs.LG"}
{"created":"2024-04-30 10:01:45","title":"Two-Stage Robust Planning Model for Park-Level Integrated Energy System Considering Uncertain Equipment Contingency","abstract":"In this paper, we propose a two-stage robust planning model for an Integrated Energy System (IES) that serves an industrial park. The term 'Park-level IES' is used to refers to IES of a smaller scale but have high demands for various forms of energy. The proposed planning model considers uncertainties like load demand fluctuations and equipment contingencies, and provides a reliable scheme of equipment selection and sizing for IES investors. Inspired by the unit commitment problem, we formulate an equipment contingency uncertainty set to accurately describe the potential equipment contingencies which happen and can be repaired within a day. Then, a novel and modified nested column-and-constraint generation algorithm is applied to solve this two-stage robust planning model with integer recourse efficiently. In the case study, the role of energy storage system for IES reliability enhancement is analyzed in detail. Computational results demonstrate the advantage of the proposed models over the deterministic planning model in terms of improving reliability.","sentences":["In this paper, we propose a two-stage robust planning model for an Integrated Energy System (IES) that serves an industrial park.","The term 'Park-level IES' is used to refers to IES of a smaller scale but have high demands for various forms of energy.","The proposed planning model considers uncertainties like load demand fluctuations and equipment contingencies, and provides a reliable scheme of equipment selection and sizing for IES investors.","Inspired by the unit commitment problem, we formulate an equipment contingency uncertainty set to accurately describe the potential equipment contingencies which happen and can be repaired within a day.","Then, a novel and modified nested column-and-constraint generation algorithm is applied to solve this two-stage robust planning model with integer recourse efficiently.","In the case study, the role of energy storage system for IES reliability enhancement is analyzed in detail.","Computational results demonstrate the advantage of the proposed models over the deterministic planning model in terms of improving reliability."],"url":"http://arxiv.org/abs/2404.19415v1","category":"eess.SY"}
{"created":"2024-04-30 09:57:21","title":"Countering Reward Over-optimization in LLM with Demonstration-Guided Reinforcement Learning","abstract":"While Reinforcement Learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO). Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning. Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself. Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective. Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function. This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation. We show the effectiveness of RCfD on three language tasks, which achieves comparable performance to carefully tuned baselines while mitigating ROO.","sentences":["While Reinforcement Learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO).","Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning.","Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself.","Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective.","Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations' and LLM's rewards rather than directly maximizing the reward function.","This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation.","We show the effectiveness of RCfD on three language tasks, which achieves comparable performance to carefully tuned baselines while mitigating ROO."],"url":"http://arxiv.org/abs/2404.19409v1","category":"cs.CL"}
{"created":"2024-04-30 09:56:03","title":"Comparison of two numerical methods for Riemannian cubic polynomials on Stiefel manifolds","abstract":"In this paper we compare two numerical methods to integrate Riemannian cubic polynomials on the Stiefel manifold $\\textbf{St}_{n,k}$. The first one is the adjusted de Casteljau algorithm, and the second one is a symplectic integrator constructed through discretization maps. In particular, we choose the cases of $n=3$ together with $k=1$ and $k=2$. The first case is diffeomorphic to the sphere and the quasi-geodesics appearing in the adjusted de Casteljau algorithm are actually geodesics. The second case is an example where we have a pure quasi-geodesic different from a geodesic. We provide a numerical comparison of both methods and discuss the obtained results to highlight the benefits of each method.","sentences":["In this paper we compare two numerical methods to integrate Riemannian cubic polynomials on the Stiefel manifold $\\textbf{St}_{n,k}$.","The first one is the adjusted de Casteljau algorithm, and the second one is a symplectic integrator constructed through discretization maps.","In particular, we choose the cases of $n=3$ together with $k=1$ and $k=2$. The first case is diffeomorphic to the sphere and the quasi-geodesics appearing in the adjusted de Casteljau algorithm are actually geodesics.","The second case is an example where we have a pure quasi-geodesic different from a geodesic.","We provide a numerical comparison of both methods and discuss the obtained results to highlight the benefits of each method."],"url":"http://arxiv.org/abs/2404.19407v1","category":"math.OC"}
{"created":"2024-04-30 09:51:02","title":"Exploring the role of mean-field potentials and short-range wave function behavior in the adiabatic connection","abstract":"In this article, we explore the construction of Hamiltonians with long-range interactions and their corrections using the short-range behavior of the wave function. A key aspect of our investigation is the examination of the one-particle potential, kept constant in our previous work, and the effects of its optimization on the adiabatic connection.   Our methodology involves the use of a parameter-dependent potential dependent on a single parameter to facilitate practical computations. We analyze the energy errors and densities in a two-electron system (harmonium) under various conditions, employing different confinement potentials and interaction parameters. The study reveals that while the mean-field potential improves the expectation value of the physical Hamiltonian, it does not necessarily improve the energy of the system within the bounds of chemical accuracy.   We also delve into the impact of density variations in adiabatic connections, challenging the common assumption that a mean field improves results. Our findings indicate that as long as energy errors remain within chemical accuracy, the mean field does not significantly outperform a bare potential. This observation is attributed to the effectiveness of corrections based on the short-range behavior of the wave function, a universal characteristic that diminishes the distinction between using a mean field or not.","sentences":["In this article, we explore the construction of Hamiltonians with long-range interactions and their corrections using the short-range behavior of the wave function.","A key aspect of our investigation is the examination of the one-particle potential, kept constant in our previous work, and the effects of its optimization on the adiabatic connection.   ","Our methodology involves the use of a parameter-dependent potential dependent on a single parameter to facilitate practical computations.","We analyze the energy errors and densities in a two-electron system (harmonium) under various conditions, employing different confinement potentials and interaction parameters.","The study reveals that while the mean-field potential improves the expectation value of the physical Hamiltonian, it does not necessarily improve the energy of the system within the bounds of chemical accuracy.   ","We also delve into the impact of density variations in adiabatic connections, challenging the common assumption that a mean field improves results.","Our findings indicate that as long as energy errors remain within chemical accuracy, the mean field does not significantly outperform a bare potential.","This observation is attributed to the effectiveness of corrections based on the short-range behavior of the wave function, a universal characteristic that diminishes the distinction between using a mean field or not."],"url":"http://arxiv.org/abs/2404.19405v1","category":"physics.chem-ph"}
{"created":"2024-04-30 09:47:44","title":"UniFS: Universal Few-shot Instance Perception with Point Representations","abstract":"Instance perception tasks (object detection, instance segmentation, pose estimation, counting) play a key role in industrial applications of visual models. As supervised learning methods suffer from high labeling cost, few-shot learning methods which effectively learn from a limited number of labeled examples are desired. Existing few-shot learning methods primarily focus on a restricted set of tasks, presumably due to the challenges involved in designing a generic model capable of representing diverse tasks in a unified manner. In this paper, we propose UniFS, a universal few-shot instance perception model that unifies a wide range of instance perception tasks by reformulating them into a dynamic point representation learning framework. Additionally, we propose Structure-Aware Point Learning (SAPL) to exploit the higher-order structural relationship among points to further enhance representation learning. Our approach makes minimal assumptions about the tasks, yet it achieves competitive results compared to highly specialized and well optimized specialist models. Codes will be released soon.","sentences":["Instance perception tasks (object detection, instance segmentation, pose estimation, counting) play a key role in industrial applications of visual models.","As supervised learning methods suffer from high labeling cost, few-shot learning methods which effectively learn from a limited number of labeled examples are desired.","Existing few-shot learning methods primarily focus on a restricted set of tasks, presumably due to the challenges involved in designing a generic model capable of representing diverse tasks in a unified manner.","In this paper, we propose UniFS, a universal few-shot instance perception model that unifies a wide range of instance perception tasks by reformulating them into a dynamic point representation learning framework.","Additionally, we propose Structure-Aware Point Learning (SAPL) to exploit the higher-order structural relationship among points to further enhance representation learning.","Our approach makes minimal assumptions about the tasks, yet it achieves competitive results compared to highly specialized and well optimized specialist models.","Codes will be released soon."],"url":"http://arxiv.org/abs/2404.19401v1","category":"cs.CV"}
{"created":"2024-04-30 09:38:00","title":"Convergence analysis of the transformed gradient projection algorithms on compact matrix manifolds","abstract":"In this paper, to address the optimization problem on a compact matrix manifold, we introduce a novel algorithmic framework called the Transformed Gradient Projection (TGP) algorithm, using the projection onto this compact matrix manifold. Compared with the existing algorithms, the key innovation in our approach lies in the utilization of a new class of search directions and various stepsizes, including the Armijo, nonmonotone Armijo, and fixed stepsizes, to guide the selection of the next iterate. Our framework offers flexibility by encompassing the classical gradient projection algorithms as special cases, and intersecting the retraction-based line-search algorithms. Notably, our focus is on the Stiefel or Grassmann manifold, revealing that many existing algorithms in the literature can be seen as specific instances within our proposed framework, and this algorithmic framework also induces several new special cases. Then, we conduct a thorough exploration of the convergence properties of these algorithms, considering various search directions and stepsizes. To achieve this, we extensively analyze the geometric properties of the projection onto compact matrix manifolds, allowing us to extend classical inequalities related to retractions from the literature. Building upon these insights, we establish the weak convergence, convergence rate, and global convergence of TGP algorithms under three distinct stepsizes. In cases where the compact matrix manifold is the Stiefel or Grassmann manifold, our convergence results either encompass or surpass those found in the literature. Finally, through a series of numerical experiments, we observe that the TGP algorithms, owing to their increased flexibility in choosing search directions, outperform classical gradient projection and retraction-based line-search algorithms in several scenarios.","sentences":["In this paper, to address the optimization problem on a compact matrix manifold, we introduce a novel algorithmic framework called the Transformed Gradient Projection (TGP) algorithm, using the projection onto this compact matrix manifold.","Compared with the existing algorithms, the key innovation in our approach lies in the utilization of a new class of search directions and various stepsizes, including the Armijo, nonmonotone Armijo, and fixed stepsizes, to guide the selection of the next iterate.","Our framework offers flexibility by encompassing the classical gradient projection algorithms as special cases, and intersecting the retraction-based line-search algorithms.","Notably, our focus is on the Stiefel or Grassmann manifold, revealing that many existing algorithms in the literature can be seen as specific instances within our proposed framework, and this algorithmic framework also induces several new special cases.","Then, we conduct a thorough exploration of the convergence properties of these algorithms, considering various search directions and stepsizes.","To achieve this, we extensively analyze the geometric properties of the projection onto compact matrix manifolds, allowing us to extend classical inequalities related to retractions from the literature.","Building upon these insights, we establish the weak convergence, convergence rate, and global convergence of TGP algorithms under three distinct stepsizes.","In cases where the compact matrix manifold is the Stiefel or Grassmann manifold, our convergence results either encompass or surpass those found in the literature.","Finally, through a series of numerical experiments, we observe that the TGP algorithms, owing to their increased flexibility in choosing search directions, outperform classical gradient projection and retraction-based line-search algorithms in several scenarios."],"url":"http://arxiv.org/abs/2404.19392v1","category":"math.OC"}
{"created":"2024-04-30 09:23:42","title":"Online Electricity Purchase for Data Center with Dynamic Virtual Battery from Flexibility Aggregation","abstract":"As a critical component of modern infrastructure, data centers account for a huge amount of power consumption and greenhouse gas emission. This paper studies the electricity purchase strategy for a data center to lower its energy cost while integrating local renewable generation under uncertainty. To facilitate efficient and scalable decision-making, we propose a two-layer hierarchy where the lower layer consists of the operation of all electrical equipment in the data center and the upper layer determines the procurement and dispatch of electricity. At the lower layer, instead of device-level scheduling in real time, we propose to exploit the inherent flexibility in demand, such as thermostatically controlled loads and flexible computing tasks, and aggregate them into virtual batteries. By this means, the upper-layer decision only needs to take into account these virtual batteries, the size of which is generally small and independent of the data center scale. We further propose an online algorithm based on Lyapunov optimization to purchase electricity from the grid with a manageable energy cost, even though the prices, renewable availability, and battery specifications are uncertain and dynamic. In particular, we show that, under mild conditions, our algorithm can achieve bounded loss compared with the offline optimal cost, while strictly respecting battery operational constraints. Extensive simulation studies validate the theoretical analysis and illustrate the tradeoff between optimality and conservativeness.","sentences":["As a critical component of modern infrastructure, data centers account for a huge amount of power consumption and greenhouse gas emission.","This paper studies the electricity purchase strategy for a data center to lower its energy cost while integrating local renewable generation under uncertainty.","To facilitate efficient and scalable decision-making, we propose a two-layer hierarchy where the lower layer consists of the operation of all electrical equipment in the data center and the upper layer determines the procurement and dispatch of electricity.","At the lower layer, instead of device-level scheduling in real time, we propose to exploit the inherent flexibility in demand, such as thermostatically controlled loads and flexible computing tasks, and aggregate them into virtual batteries.","By this means, the upper-layer decision only needs to take into account these virtual batteries, the size of which is generally small and independent of the data center scale.","We further propose an online algorithm based on Lyapunov optimization to purchase electricity from the grid with a manageable energy cost, even though the prices, renewable availability, and battery specifications are uncertain and dynamic.","In particular, we show that, under mild conditions, our algorithm can achieve bounded loss compared with the offline optimal cost, while strictly respecting battery operational constraints.","Extensive simulation studies validate the theoretical analysis and illustrate the tradeoff between optimality and conservativeness."],"url":"http://arxiv.org/abs/2404.19387v1","category":"eess.SY"}
{"created":"2024-04-30 09:14:54","title":"Probing Unlearned Diffusion Models: A Transferable Adversarial Attack Perspective","abstract":"Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.","sentences":["Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation.","Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models.","However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts.","This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model.","And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity.","Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting.","This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models.","Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models.","This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods.","Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts."],"url":"http://arxiv.org/abs/2404.19382v1","category":"cs.CV"}
{"created":"2024-04-30 09:14:12","title":"Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders","abstract":"To overcome the memory capacity wall of large-scale AI and big data applications, Compute Express Link (CXL) enables cost-efficient memory expansion beyond the local DRAM of processors. While its CXL.mem protocol stack minimizes interconnect latency, CXL memory accesses can still result in significant slowdowns for memory-bound applications. While near-data processing (NDP) in CXL memory can overcome such limitations, prior works propose application-specific HW units that are not suitable for practical CXL memory-based systems that should support various applications. On the other hand, existing CPU or GPU cores are not cost-effective for NDP because they are not optimized for memory-bound applications. In addition, the communication between the host processor and CXL controller for NDP offloading should achieve low latency, but the CXL$.$io (or PCIe) protocol incurs $\\mu$s-scale latency and is not suitable for fine-grain NDP.   To achieve high-performance NDP end-to-end, we propose a low-overhead general-purpose NDP architecture for CXL memory referred to as Memory-Mapped NDP (M$^2$NDP), which comprises memory-mapped functions (M$^2$func) and memory-mapped $\\mu$threading (M$^2\\mu$thr). The M$^2$func is a CXL.mem-compatible low-overhead communication mechanism between the host processor and NDP controller in the CXL memory. The M$^2\\mu$thr enables low-cost, general-purpose NDP unit design by introducing lightweight $\\mu$threads that support highly concurrent execution of NDP kernels with minimal resource wastage. By combining them, our M$^2$NDP achieves significant speedups for various applications, including in-memory OLAP, key-value store, large language model, recommendation model, and graph analytics by up to 128$\\times$ (11.5$\\times$ overall) and reduces energy by up to 87.9\\% (80.1\\% overall) compared to a baseline CPU or GPU host with passive CXL memory.","sentences":["To overcome the memory capacity wall of large-scale AI and big data applications, Compute Express Link (CXL) enables cost-efficient memory expansion beyond the local DRAM of processors.","While its CXL.mem protocol stack minimizes interconnect latency, CXL memory accesses can still result in significant slowdowns for memory-bound applications.","While near-data processing (NDP) in CXL memory can overcome such limitations, prior works propose application-specific HW units that are not suitable for practical CXL memory-based systems that should support various applications.","On the other hand, existing CPU or GPU cores are not cost-effective for NDP because they are not optimized for memory-bound applications.","In addition, the communication between the host processor and CXL controller for NDP offloading should achieve low latency, but the CXL$.$io (or PCIe) protocol incurs $\\mu$s-scale latency and is not suitable for fine-grain NDP.   ","To achieve high-performance NDP end-to-end, we propose a low-overhead general-purpose NDP architecture for CXL memory referred to as Memory-Mapped NDP (M$^2$NDP), which comprises memory-mapped functions (M$^2$func) and memory-mapped $\\mu$threading (M$^2\\mu$thr).","The M$^2$func is a CXL.mem-compatible low-overhead communication mechanism between the host processor and NDP controller in the CXL memory.","The M$^2\\mu$thr enables low-cost, general-purpose NDP unit design by introducing lightweight $\\mu$threads that support highly concurrent execution of NDP kernels with minimal resource wastage.","By combining them, our M$^2$NDP achieves significant speedups for various applications, including in-memory OLAP, key-value store, large language model, recommendation model, and graph analytics by up to 128$\\times$ (11.5$\\times$ overall) and reduces energy by up to 87.9\\% (80.1\\% overall) compared to a baseline CPU or GPU host with passive CXL memory."],"url":"http://arxiv.org/abs/2404.19381v1","category":"cs.AR"}
{"created":"2024-04-30 09:11:04","title":"SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction using Knowledge Graphs","abstract":"Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene including traffic participants, road topology, traffic signs as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. This paper describes a method SemanticFormer to predict multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. We extract high-level information in the form of semantic meta-paths from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. The proposed architecture comprises a hierarchical heterogeneous graph encoder, which can capture spatio-temporal and relational information across agents and between agents and road elements, and a predictor that fuses the different encodings and decodes trajectories with probabilities. Finally, a refinement module evaluates permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to the state-of-the-art methods.","sentences":["Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene including traffic participants, road topology, traffic signs as well as their semantic relations to each other.","Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently.","This paper describes a method SemanticFormer to predict multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach.","We extract high-level information in the form of semantic meta-paths from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories.","The proposed architecture comprises a hierarchical heterogeneous graph encoder, which can capture spatio-temporal and relational information across agents and between agents and road elements, and a predictor that fuses the different encodings and decodes trajectories with probabilities.","Finally, a refinement module evaluates permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories.","Evaluation of the nuScenes benchmark demonstrates improved performance compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.19379v1","category":"cs.CV"}
{"created":"2024-04-30 09:09:14","title":"Gaussian mixtures closest to a given measure via optimal transport","abstract":"Given a determinate (multivariate) probability measure $\\mu$, we characterize Gaussian mixtures $\\nu\\_\\phi$ which minimize the Wasserstein distance $W\\_2(\\mu,\\nu\\_\\phi)$ to $\\mu$ when the mixing probability measure $\\phi$ on the parameters $(m,\\Sigma)$ of the Gaussians is supported on a compact set $S$.(i) We first show that such mixtures are optimal solutions of a particular optimal transport (OT) problem where the marginal $\\nu\\_{\\phi}$ of the OT problem is also unknown via the mixing measure variable $\\phi$. Next (ii) by using a well-known specific property of Gaussian measures, this optimal transport is then viewed as a Generalized Moment Problem (GMP) and if the set $S$ of mixture parameters $(m,\\Sigma)$ is a basic compact semi-algebraic set, we provide a \"mesh-free\" numerical scheme to approximate as closely as desired the optimal distance by solving a hierarchy of semidefinite relaxations of increasing size. In particular, we neither assume that the mixing measure is finitely supported nor that the variance is the same for all components. If the original measure $\\mu$ is not a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then a strictly positive distance is detected at a finite step of the hierarchy. If the original measure $\\mu$ is a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then all semidefinite relaxations of the hierarchy have same zero optimal value. Moreover if the mixing measure is atomic with finite support, its components can sometimes be extracted from an optimal solution at some semidefinite relaxation of the hierarchy when Curto & Fialkow's flatness condition holds for some moment matrix.","sentences":["Given a determinate (multivariate) probability measure $\\mu$, we characterize Gaussian mixtures $\\nu\\_\\phi$ which minimize the Wasserstein distance $W\\_2(\\mu,\\nu\\_\\phi)$ to $\\mu$ when the mixing probability measure $\\phi$ on the parameters $(m,\\Sigma)$ of the Gaussians is supported on a compact set $S$.(i)","We first show that such mixtures are optimal solutions of a particular optimal transport (OT) problem where the marginal $\\nu\\_{\\phi}$ of the OT problem is also unknown via the mixing measure variable $\\phi$. Next (ii) by using a well-known specific property of Gaussian measures, this optimal transport is then viewed as a Generalized Moment Problem (GMP) and if the set $S$ of mixture parameters $(m,\\Sigma)$ is a basic compact semi-algebraic set, we provide a \"mesh-free\" numerical scheme to approximate as closely as desired the optimal distance by solving a hierarchy of semidefinite relaxations of increasing size.","In particular, we neither assume that the mixing measure is finitely supported nor that the variance is the same for all components.","If the original measure $\\mu$ is not a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then a strictly positive distance is detected at a finite step of the hierarchy.","If the original measure $\\mu$ is a Gaussian mixture with parameters $(m,\\Sigma)\\in S$, then all semidefinite relaxations of the hierarchy have same zero optimal value.","Moreover if the mixing measure is atomic with finite support, its components can sometimes be extracted from an optimal solution at some semidefinite relaxation of the hierarchy when Curto & Fialkow's flatness condition holds for some moment matrix."],"url":"http://arxiv.org/abs/2404.19378v1","category":"math.OC"}
{"created":"2024-04-30 09:05:56","title":"Perspectives of a single-anode cylindrical chamber operating in ionization mode and high gas pressure","abstract":"As part of the R2D2 (Rare Decays with Radial Detector) R&D, the use of a gas detector with a spherical or cylindrical cathode, equipped with a single anode and operating at high pressure, was studied for the search of rare phenomena such as neutrinoless double-beta decay. The presented measurements were obtained with a cylindrical detector, covering gas pressures ranging from 1 to 10 bar in argon and 1 to 6 bar in xenon, using both a point-like source of $^{210}$Po (5.3 MeV $\\alpha$ ) and a diffuse source of $^{222}$Rn (5.5 MeV $\\alpha$). Analysis and interpretation of the data were developed using the anodic current waveform. Similar detection performances were achieved with both gases, and comparable energy resolutions were measured with both sources. As long as the purity of the gas was sufficient, no significant degradation of the measured energy was observed by increasing the pressure. At the highest operating pressure, an energy resolution better than 1.5% full-width at half-maximum (FWHM) was obtained for both gaseous media, although optimal noise conditions were not reached.","sentences":["As part of the R2D2 (Rare Decays with Radial Detector) R&D, the use of a gas detector with a spherical or cylindrical cathode, equipped with a single anode and operating at high pressure, was studied for the search of rare phenomena such as neutrinoless double-beta decay.","The presented measurements were obtained with a cylindrical detector, covering gas pressures ranging from 1 to 10 bar in argon and 1 to 6 bar in xenon, using both a point-like source of $^{210}$Po (5.3 MeV $\\alpha$ ) and a diffuse source of $^{222}$Rn (5.5 MeV $\\alpha$).","Analysis and interpretation of the data were developed using the anodic current waveform.","Similar detection performances were achieved with both gases, and comparable energy resolutions were measured with both sources.","As long as the purity of the gas was sufficient, no significant degradation of the measured energy was observed by increasing the pressure.","At the highest operating pressure, an energy resolution better than 1.5% full-width at half-maximum (FWHM) was obtained for both gaseous media, although optimal noise conditions were not reached."],"url":"http://arxiv.org/abs/2404.19374v1","category":"physics.ins-det"}
{"created":"2024-04-30 08:59:53","title":"AutoNet: Automatic Reachability Policy Management in Public Cloud Networks","abstract":"Virtual Private Cloud (VPC) is the main network abstraction technology used in public cloud systems. VPCs are composed of a set of network services that permit the definition of complex network reachability properties among internal and external cloud entities such as tenants' VMs or some generic internet nodes. Although hiding the underlying complexity through a comprehensible abstraction layer, manually enforcing particular reachability intents in VPC networks is still notably error-prone and complex. In this paper, we propose AutoNet, a new model for assisting cloud tenants in managing reachability-based policies in VPC networks. AutoNet is capable of safely generating incremental VPC configurations while satisfying some metric-based high-level intent defined by the tenants. To achieve this goal, we leverage a MaxSAT-based encoding of the network configuration combined with several optimizations to scale to topologies with thousands of nodes. Our results show that the developed system is capable of achieving a sub-second response time for production VPC deployments while still providing fine-grained control over the generated configurations.","sentences":["Virtual Private Cloud (VPC) is the main network abstraction technology used in public cloud systems.","VPCs are composed of a set of network services that permit the definition of complex network reachability properties among internal and external cloud entities such as tenants' VMs or some generic internet nodes.","Although hiding the underlying complexity through a comprehensible abstraction layer, manually enforcing particular reachability intents in VPC networks is still notably error-prone and complex.","In this paper, we propose AutoNet, a new model for assisting cloud tenants in managing reachability-based policies in VPC networks.","AutoNet is capable of safely generating incremental VPC configurations while satisfying some metric-based high-level intent defined by the tenants.","To achieve this goal, we leverage a MaxSAT-based encoding of the network configuration combined with several optimizations to scale to topologies with thousands of nodes.","Our results show that the developed system is capable of achieving a sub-second response time for production VPC deployments while still providing fine-grained control over the generated configurations."],"url":"http://arxiv.org/abs/2404.19372v1","category":"cs.NI"}
{"created":"2024-04-30 08:01:49","title":"StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation","abstract":"Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity. However, this approach largely depends on the quality of prompt initialization, and always exhibits large variability among different runs. Such property makes prompt tuning highly unreliable and vulnerable to poorly constructed prompts, which limits its extension to more real-world applications. To tackle this issue, we propose to treat the hard prompt and soft prompt as separate inputs to mitigate noise brought by the prompt initialization. Furthermore, we optimize soft prompts with contrastive learning for utilizing class-aware information in the training process to maintain model performance. Experimental results demonstrate that \\sysname outperforms state-of-the-art methods by 7.20% in accuracy and reduces the standard deviation by 2.02 on average. Furthermore, extensive experiments underscore its robustness and stability across 7 datasets covering various tasks.","sentences":["Large language models have shown their ability to become effective few-shot learners with prompting, revoluting the paradigm of learning with data scarcity.","However, this approach largely depends on the quality of prompt initialization, and always exhibits large variability among different runs.","Such property makes prompt tuning highly unreliable and vulnerable to poorly constructed prompts, which limits its extension to more real-world applications.","To tackle this issue, we propose to treat the hard prompt and soft prompt as separate inputs to mitigate noise brought by the prompt initialization.","Furthermore, we optimize soft prompts with contrastive learning for utilizing class-aware information in the training process to maintain model performance.","Experimental results demonstrate that \\sysname outperforms state-of-the-art methods by 7.20% in accuracy and reduces the standard deviation by 2.02 on average.","Furthermore, extensive experiments underscore its robustness and stability across 7 datasets covering various tasks."],"url":"http://arxiv.org/abs/2404.19335v1","category":"cs.CL"}
{"created":"2024-04-30 07:51:43","title":"Assessment of physical schemes for WRF model in convection-permitting mode over southern Iberian Peninsula","abstract":"Convection-permitting models (CPMs) enable the representation of meteorological variables at horizontal high resolution spatial scales (higher than 4 km), where convection plays a significant role. Physical schemes need to be evaluated considering factors in the studied region such as orography and climate variability. This study investigates the sensitivity of the WRF model as CPM to the use of different physics schemes on Andalusia, a complex orography region in southern Iberian Peninsula (IP). A set of 1-year WRF simulations was completed based on two one-way nested domains: the parent domain (d01) spanning the entire IP with 5 km spatial resolution and the nested domain (d02) for the region of Andalusia at 1 km of spatial resolution. 12 physic schemes were examined from combinations of microphysics (MP) schemes including THOMPSON, WRF single moment 6-class (WSM6), and WRF single moment 7-class (WSM7), and different options for the convection in d01, the Grell 3D (G3), Grell-Freitas (GF), Kain-Fritsch (KF), and deactivated cumulus parameterization (OFF). The simulated precipitation and 2-m temperature for the year 2018, a very wet year, were compared with observational datasets to determine the optimal WRF configuration, including point-to-point and station-point comparisons at different time aggregations. In general, greater differences were shown when comparing the results of convection schemes in d01. Simulations completed with GF or OFF presented better performance compared to the reference datasets. Concerning the MP, although THOMPSON showed a better fit in high mountain areas, it generally presents a worse agreement with the reference datasets. In terms of temperature, the results were very similar and, therefore, the selection of the best configuration was based mainly on the precipitation results with the WSM7-GF scheme being suitable for Andalusia region.","sentences":["Convection-permitting models (CPMs) enable the representation of meteorological variables at horizontal high resolution spatial scales (higher than 4 km), where convection plays a significant role.","Physical schemes need to be evaluated considering factors in the studied region such as orography and climate variability.","This study investigates the sensitivity of the WRF model as CPM to the use of different physics schemes on Andalusia, a complex orography region in southern Iberian Peninsula (IP).","A set of 1-year WRF simulations was completed based on two one-way nested domains: the parent domain (d01) spanning the entire IP with 5 km spatial resolution and the nested domain (d02) for the region of Andalusia at 1 km of spatial resolution.","12 physic schemes were examined from combinations of microphysics (MP) schemes including THOMPSON, WRF single moment 6-class (WSM6), and WRF single moment 7-class (WSM7), and different options for the convection in d01, the Grell 3D (G3), Grell-Freitas (GF), Kain-Fritsch (KF), and deactivated cumulus parameterization (OFF).","The simulated precipitation and 2-m temperature for the year 2018, a very wet year, were compared with observational datasets to determine the optimal WRF configuration, including point-to-point and station-point comparisons at different time aggregations.","In general, greater differences were shown when comparing the results of convection schemes in d01.","Simulations completed with GF or OFF presented better performance compared to the reference datasets.","Concerning the MP, although THOMPSON showed a better fit in high mountain areas, it generally presents a worse agreement with the reference datasets.","In terms of temperature, the results were very similar and, therefore, the selection of the best configuration was based mainly on the precipitation results with the WSM7-GF scheme being suitable for Andalusia region."],"url":"http://arxiv.org/abs/2404.19327v1","category":"physics.ao-ph"}
{"created":"2024-04-30 07:28:09","title":"A characterization of entangled two-qubit states via partial-transpose-moments","abstract":"Although quantum entanglement is an important resource, its characterization is quite challenging. The partial transposition is a common method to detect bipartite entanglement. In this paper, the authors study the partial-transpose(PT)-moments of two-qubit states,and completely describe the whole region, composed of the second and third PT-moments, for all two-qubit states. Furthermore, they determine the accurate region corresponding to all entangled two-qubit states. The states corresponding to those boundary points of the whole region, and to the border lines between separable and entangled states are analyzed. As an application, they characterize the entangled region of PT-moments for the two families of Werner states and Bell-diagonal states. The relations between entanglement and the pairs of PT-moments are revealed from these typical examples. They also numerically plot the whole region of possible PT-moments for all two-qubit X-states, and find that this region is almost the same as the whole region of PT-moments for all two-qubit states. Moreover, they extend their results to detect the entanglement of multiqubit states. By utilizing the PT-moment-based method to characterize the entanglement of the multiqubit states mixed by the GHZ and W states, they propose an operational way of verifying the genuine entanglement in such states.","sentences":["Although quantum entanglement is an important resource, its characterization is quite challenging.","The partial transposition is a common method to detect bipartite entanglement.","In this paper, the authors study the partial-transpose(PT)-moments of two-qubit states,and completely describe the whole region, composed of the second and third PT-moments, for all two-qubit states.","Furthermore, they determine the accurate region corresponding to all entangled two-qubit states.","The states corresponding to those boundary points of the whole region, and to the border lines between separable and entangled states are analyzed.","As an application, they characterize the entangled region of PT-moments for the two families of Werner states and Bell-diagonal states.","The relations between entanglement and the pairs of PT-moments are revealed from these typical examples.","They also numerically plot the whole region of possible PT-moments for all two-qubit X-states, and find that this region is almost the same as the whole region of PT-moments for all two-qubit states.","Moreover, they extend their results to detect the entanglement of multiqubit states.","By utilizing the PT-moment-based method to characterize the entanglement of the multiqubit states mixed by the GHZ and W states, they propose an operational way of verifying the genuine entanglement in such states."],"url":"http://arxiv.org/abs/2404.19308v1","category":"quant-ph"}
{"created":"2024-04-30 06:48:56","title":"Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning","abstract":"This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS). These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs. For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium. The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed. Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes. To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity. Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment. Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner.","sentences":["This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS).","These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs.","For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium.","The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed.","Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes.","To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity.","Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment.","Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner."],"url":"http://arxiv.org/abs/2404.19292v1","category":"cs.IT"}
{"created":"2024-04-30 17:59:58","title":"Generalized Symmetries in 2D from String Theory: SymTFTs, Intrinsic Relativeness, and Anomalies of Non-invertible Symmetries","abstract":"Generalized global symmetries, in particular non-invertible and categorical symmetries, have become a focal point in the recent study of quantum field theory (QFT). In this paper, we investigate aspects of symmetry topological field theories (SymTFTs) and anomalies of non-invertible symmetries for 2D QFTs from a string theory perspective. Our primary focus is on an infinite class of 2D QFTs engineered on D1-branes probing toric Calabi-Yau 4-fold singularities. We derive 3D SymTFTs from the topological sector of IIB supergravity and discuss the resulting 2D QFTs, which can be intrinsically relative or absolute. For intrinsically relative QFTs, we propose a sufficient condition for them to exist. For absolute QFTs, we show that they exhibit non-invertible symmetries with an elegant brane origin. Furthermore, we find that these non-invertible symmetries can suffer from anomalies, which we discuss from a top-down perspective. Explicit examples are provided, including theories for $Y^{(p,k)}(\\mathbb{P}^2)$, $Y^{(2,0)}(\\mathbb{P}^1\\times \\mathbb{P}^1)$, and $\\mathbb{C}^4/\\mathbb{Z}_4$ geometries.","sentences":["Generalized global symmetries, in particular non-invertible and categorical symmetries, have become a focal point in the recent study of quantum field theory (QFT).","In this paper, we investigate aspects of symmetry topological field theories (SymTFTs) and anomalies of non-invertible symmetries for 2D QFTs from a string theory perspective.","Our primary focus is on an infinite class of 2D QFTs engineered on D1-branes probing toric Calabi-Yau 4-fold singularities.","We derive 3D SymTFTs from the topological sector of IIB supergravity and discuss the resulting 2D QFTs, which can be intrinsically relative or absolute.","For intrinsically relative QFTs, we propose a sufficient condition for them to exist.","For absolute QFTs, we show that they exhibit non-invertible symmetries with an elegant brane origin.","Furthermore, we find that these non-invertible symmetries can suffer from anomalies, which we discuss from a top-down perspective.","Explicit examples are provided, including theories for $Y^{(p,k)}(\\mathbb{P}^2)$, $Y^{(2,0)}(\\mathbb{P}^1\\times \\mathbb{P}^1)$, and $\\mathbb{C}^4/\\mathbb{Z}_4$ geometries."],"url":"http://arxiv.org/abs/2404.19761v1","category":"hep-th"}
