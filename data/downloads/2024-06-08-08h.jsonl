{"created":"2024-06-05 17:59:40","title":"Wings: Learning Multimodal LLMs without Text-only Forgetting","abstract":"Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM. In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension. Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text. From that, we construct extra modules that act as the boosted learner to compensate for the attention shift. The complementary visual and textual learners, like \"wings\" on either side, are connected in parallel within each layer's attention block. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners. We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks.","sentences":["Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs.","However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM.","In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension.","Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text.","From that, we construct extra modules that act as the boosted learner to compensate for the attention shift.","The complementary visual and textual learners, like \"wings\" on either side, are connected in parallel within each layer's attention block.","Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements.","Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners.","We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners.","Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks.","On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks."],"url":"http://arxiv.org/abs/2406.03496v1","category":"cs.CL"}
{"created":"2024-06-05 17:57:58","title":"The Logarithmic Memristor-Based Bayesian Machine","abstract":"The demand for explainable and energy-efficient artificial intelligence (AI) systems for edge computing has led to significant interest in electronic systems dedicated to Bayesian inference. Traditional designs of such systems often rely on stochastic computing, which offers high energy efficiency but suffers from latency issues and struggles with low-probability values. In this paper, we introduce the logarithmic memristor-based Bayesian machine, an innovative design that leverages the unique properties of memristors and logarithmic computing as an alternative to stochastic computing. We present a prototype machine fabricated in a hybrid CMOS/hafnium-oxide memristor process. We validate the versatility and robustness of our system through experimental validation and extensive simulations in two distinct applications: gesture recognition and sleep stage classification. The logarithmic approach simplifies the computational model by converting multiplications into additions and enhances the handling of low-probability events, which are crucial in time-dependent tasks. Our results demonstrate that the logarithmic Bayesian machine achieves superior performance in terms of accuracy and energy efficiency compared to its stochastic counterpart, particularly in scenarios involving complex probabilistic models. This work paves the way for the deployment of advanced AI capabilities in edge devices, where power efficiency and reliability are paramount.","sentences":["The demand for explainable and energy-efficient artificial intelligence (AI) systems for edge computing has led to significant interest in electronic systems dedicated to Bayesian inference.","Traditional designs of such systems often rely on stochastic computing, which offers high energy efficiency but suffers from latency issues and struggles with low-probability values.","In this paper, we introduce the logarithmic memristor-based Bayesian machine, an innovative design that leverages the unique properties of memristors and logarithmic computing as an alternative to stochastic computing.","We present a prototype machine fabricated in a hybrid CMOS/hafnium-oxide memristor process.","We validate the versatility and robustness of our system through experimental validation and extensive simulations in two distinct applications: gesture recognition and sleep stage classification.","The logarithmic approach simplifies the computational model by converting multiplications into additions and enhances the handling of low-probability events, which are crucial in time-dependent tasks.","Our results demonstrate that the logarithmic Bayesian machine achieves superior performance in terms of accuracy and energy efficiency compared to its stochastic counterpart, particularly in scenarios involving complex probabilistic models.","This work paves the way for the deployment of advanced AI capabilities in edge devices, where power efficiency and reliability are paramount."],"url":"http://arxiv.org/abs/2406.03492v1","category":"cs.ET"}
{"created":"2024-06-05 17:49:47","title":"Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends","abstract":"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of \"Circumstantial Inference\" to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying \"Circumstantial Inference.\"","sentences":["Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems.","However, they continue to face concerns about hallucinations.","While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness.","Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies.","Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models.","We propose a refined taxonomy of errors, coining the category of \"Circumstantial Inference\" to bucket these LLM behaviors and release the dataset.","Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models.","Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors.","To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying \"Circumstantial Inference.\""],"url":"http://arxiv.org/abs/2406.03487v1","category":"cs.CL"}
{"created":"2024-06-05 17:49:24","title":"BIPED: Pedagogically Informed Tutoring System for ESL Education","abstract":"Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.","sentences":["Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English.","Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies.","To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions.","Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset.","Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively.","We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies."],"url":"http://arxiv.org/abs/2406.03486v1","category":"cs.CL"}
{"created":"2024-06-05 17:46:26","title":"Highway Value Iteration Networks","abstract":"Value iteration networks (VINs) enable end-to-end learning for planning tasks by employing a differentiable \"planning module\" that approximates the value iteration algorithm. However, long-term planning remains a challenge because training very deep VINs is difficult. To address this problem, we embed highway value iteration -- a recent algorithm designed to facilitate long-term credit assignment -- into the structure of VINs. This improvement augments the \"planning module\" of the VIN with three additional components: 1) an \"aggregate gate,\" which constructs skip connections to improve information flow across many layers; 2) an \"exploration module,\" crafted to increase the diversity of information and gradient flow in spatial dimensions; 3) a \"filter gate\" designed to ensure safe exploration. The resulting novel highway VIN can be trained effectively with hundreds of layers using standard backpropagation. In long-term planning tasks requiring hundreds of planning steps, deep highway VINs outperform both traditional VINs and several advanced, very deep NNs.","sentences":["Value iteration networks (VINs) enable end-to-end learning for planning tasks by employing a differentiable \"planning module\" that approximates the value iteration algorithm.","However, long-term planning remains a challenge because training very deep VINs is difficult.","To address this problem, we embed highway value iteration -- a recent algorithm designed to facilitate long-term credit assignment -- into the structure of VINs.","This improvement augments the \"planning module\" of the VIN with three additional components: 1) an \"aggregate gate,\" which constructs skip connections to improve information flow across many layers; 2) an \"exploration module,\" crafted to increase the diversity of information and gradient flow in spatial dimensions; 3) a \"filter gate\" designed to ensure safe exploration.","The resulting novel highway VIN can be trained effectively with hundreds of layers using standard backpropagation.","In long-term planning tasks requiring hundreds of planning steps, deep highway VINs outperform both traditional VINs and several advanced, very deep NNs."],"url":"http://arxiv.org/abs/2406.03485v1","category":"cs.LG"}
{"created":"2024-06-05 17:42:05","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead","abstract":"Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.","sentences":["Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length.","An effective approach to compress KV cache is quantization.","However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block.","Depending on the block size, this overhead can add 1 or 2 bits per quantized number.","We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization.","In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants.","We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion.","We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation.","When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime.","Codes are available at \\url{https://github.com/amirzandieh/QJL}."],"url":"http://arxiv.org/abs/2406.03482v1","category":"cs.LG"}
{"created":"2024-06-05 17:37:21","title":"Unpacking Approaches to Learning and Teaching Machine Learning in K-12 Education: Transparency, Ethics, and Design Activities","abstract":"In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized. One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models. A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work. In addition, we identify efforts within a third approach that integrates the previous two. In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice. In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities.","sentences":["In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized.","One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models.","A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work.","In addition, we identify efforts within a third approach that integrates the previous two.","In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice.","In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities."],"url":"http://arxiv.org/abs/2406.03480v1","category":"cs.CY"}
{"created":"2024-06-05 17:24:07","title":"SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN","abstract":"Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer","sentences":["Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy.","Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks.","However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts.","In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation.","SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs.","The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer"],"url":"http://arxiv.org/abs/2406.03470v1","category":"cs.NE"}
{"created":"2024-06-05 17:16:07","title":"Parallel Quantum Computing Simulations via Quantum Accelerator Platform Virtualization","abstract":"Quantum circuit execution is the central task in quantum computation. Due to inherent quantum-mechanical constraints, quantum computing workflows often involve a considerable number of independent measurements over a large set of slightly different quantum circuits. Here we discuss a simple model for parallelizing simulation of such quantum circuit executions that is based on introducing a large array of virtual quantum processing units, mapped to classical HPC nodes, as a parallel quantum computing platform. Implemented within the XACC framework, the model can readily take advantage of its backend-agnostic features, enabling parallel quantum circuit execution over any target backend supported by XACC. We illustrate the performance of this approach by demonstrating strong scaling in two pertinent domain science problems, namely in computing the gradients for the multi-contracted variational quantum eigensolver and in data-driven quantum circuit learning, where we vary the number of qubits and the number of circuit layers. The latter (classical) simulation leverages the cuQuantum SDK library to run efficiently on GPU-accelerated HPC platforms.","sentences":["Quantum circuit execution is the central task in quantum computation.","Due to inherent quantum-mechanical constraints, quantum computing workflows often involve a considerable number of independent measurements over a large set of slightly different quantum circuits.","Here we discuss a simple model for parallelizing simulation of such quantum circuit executions that is based on introducing a large array of virtual quantum processing units, mapped to classical HPC nodes, as a parallel quantum computing platform.","Implemented within the XACC framework, the model can readily take advantage of its backend-agnostic features, enabling parallel quantum circuit execution over any target backend supported by XACC.","We illustrate the performance of this approach by demonstrating strong scaling in two pertinent domain science problems, namely in computing the gradients for the multi-contracted variational quantum eigensolver and in data-driven quantum circuit learning, where we vary the number of qubits and the number of circuit layers.","The latter (classical) simulation leverages the cuQuantum SDK library to run efficiently on GPU-accelerated HPC platforms."],"url":"http://arxiv.org/abs/2406.03466v1","category":"quant-ph"}
{"created":"2024-06-05 16:48:26","title":"What is the Best Way for ChatGPT to Translate Poetry?","abstract":"Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.","sentences":["Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation.","The advent of Large Language Models such as ChatGPT holds potential for innovation in this field.","This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance.","Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention.","To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process.","Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation.","We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4.","The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems.","This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation."],"url":"http://arxiv.org/abs/2406.03450v1","category":"cs.CL"}
{"created":"2024-06-05 16:44:06","title":"FILS: Self-Supervised Video Feature Prediction In Semantic Language Space","abstract":"This paper demonstrates a self-supervised approach for learning semantic video representations. Recent vision studies show that a masking strategy for vision and natural language supervision has contributed to developing transferable visual pretraining. Our goal is to achieve a more semantic video representation by leveraging the text related to the video content during the pretraining in a fully self-supervised manner. To this end, we present FILS, a novel self-supervised video Feature prediction In semantic Language Space (FILS). The vision model can capture valuable structured information by correctly predicting masked feature semantics in language space. It is learned using a patch-wise video-text contrastive strategy, in which the text representations act as prototypes for transforming vision features into a language space, which are then used as targets for semantically meaningful feature prediction using our masked encoder-decoder structure. FILS demonstrates remarkable transferability on downstream action recognition tasks, achieving state-of-the-art on challenging egocentric datasets, like Epic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base. Our efficient method requires less computation and smaller batches compared to previous works.","sentences":["This paper demonstrates a self-supervised approach for learning semantic video representations.","Recent vision studies show that a masking strategy for vision and natural language supervision has contributed to developing transferable visual pretraining.","Our goal is to achieve a more semantic video representation by leveraging the text related to the video content during the pretraining in a fully self-supervised manner.","To this end, we present FILS, a novel self-supervised video Feature prediction In semantic Language Space (FILS).","The vision model can capture valuable structured information by correctly predicting masked feature semantics in language space.","It is learned using a patch-wise video-text contrastive strategy, in which the text representations act as prototypes for transforming vision features into a language space, which are then used as targets for semantically meaningful feature prediction using our masked encoder-decoder structure.","FILS demonstrates remarkable transferability on downstream action recognition tasks, achieving state-of-the-art on challenging egocentric datasets, like Epic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base.","Our efficient method requires less computation and smaller batches compared to previous works."],"url":"http://arxiv.org/abs/2406.03447v1","category":"cs.CV"}
{"created":"2024-06-05 16:36:21","title":"Are language models rational? The case of coherence norms and belief revision","abstract":"Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.","sentences":["Do norms of rationality apply to machine learning models, in particular language models?","In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms.","We consider both logical coherence norms as well as coherence norms tied to the strength of belief.","To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models.","This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities.","We argue that rational norms tied to coherence do apply to some language models, but not to others.","This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally."],"url":"http://arxiv.org/abs/2406.03442v1","category":"cs.CL"}
{"created":"2024-06-05 16:34:12","title":"Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input","abstract":"Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.","sentences":["Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses.","However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training.","This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events.","Our proposed text-to-events model produces synthetic event frames directly from text prompts.","It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs.","By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects.","The autoencoder was first trained on an event camera dataset of diverse scenes.","In the combined training with the diffusion model, the DVS gesture dataset was used.","We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements.","The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group.","The results demonstrate the capability of this method in synthesizing event datasets."],"url":"http://arxiv.org/abs/2406.03439v1","category":"cs.CV"}
{"created":"2024-06-05 16:32:14","title":"Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling","abstract":"Off-policy learning (OPL) often involves minimizing a risk estimator based on importance weighting to correct bias from the logging policy used to collect data. However, this method can produce an estimator with a high variance. A common solution is to regularize the importance weights and learn the policy by minimizing an estimator with penalties derived from generalization bounds specific to the estimator. This approach, known as pessimism, has gained recent attention but lacks a unified framework for analysis. To address this gap, we introduce a comprehensive PAC-Bayesian framework to examine pessimism with regularized importance weighting. We derive a tractable PAC-Bayesian generalization bound that universally applies to common importance weight regularizations, enabling their comparison within a single framework. Our empirical results challenge common understanding, demonstrating the effectiveness of standard IW regularization techniques.","sentences":["Off-policy learning (OPL) often involves minimizing a risk estimator based on importance weighting to correct bias from the logging policy used to collect data.","However, this method can produce an estimator with a high variance.","A common solution is to regularize the importance weights and learn the policy by minimizing an estimator with penalties derived from generalization bounds specific to the estimator.","This approach, known as pessimism, has gained recent attention but lacks a unified framework for analysis.","To address this gap, we introduce a comprehensive PAC-Bayesian framework to examine pessimism with regularized importance weighting.","We derive a tractable PAC-Bayesian generalization bound that universally applies to common importance weight regularizations, enabling their comparison within a single framework.","Our empirical results challenge common understanding, demonstrating the effectiveness of standard IW regularization techniques."],"url":"http://arxiv.org/abs/2406.03434v1","category":"cs.LG"}
{"created":"2024-06-05 16:25:57","title":"HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits","abstract":"Benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users. Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.","sentences":["Benchmarks have been essential for driving progress in machine learning.","A better understanding of LLM capabilities on real world tasks is vital for safe development.","Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results.","We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers.","It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting.","Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post.","Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users.","Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web.","We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking.","To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM."],"url":"http://arxiv.org/abs/2406.03428v1","category":"cs.LG"}
{"created":"2024-06-05 16:24:32","title":"Field Theory Approach to Classical $N$-Particle Systems In and Out of Equilibrium","abstract":"We present an approach to solving the evolution of a classical $N$-particle ensemble based on the path integral approach to classical mechanics. This formulation provides a perturbative solution to the Liouville equation in terms of a propagator which can be expanded in a Dyson series. We show that this perturbative expansion exactly corresponds to an iterative solution of the BBGKY-hierarchy in orders of the interaction potential. Using the path integral formulation, we perform a Hubbard-Stratonovich transformation (HST) to obtain an effective field theoretic description in terms of macroscopic fields, which contains the full microscopic dynamics of the system in its vertices. Naturally, the HST leads to a new perturbative expansion scheme which contains an infinite order of microscopic interactions already at the lowest order of the perturbative expansion. Our approach can be applied to in and out of equilibrium systems with arbitrary interaction potentials and initial conditions. We show how (unequal-time) cumulants of the Klimontovich phase space densities can be computed within this framework and derive results for density and momentum correlations for a spatially homogeneous system. Under the explicit assumptions for the interaction potential and initial conditions, we show that well-known results related to plasma oscillations and the Jeans instability criterion for gravitational collapse can be recovered in the lowest order perturbative expansion and that both are the effect of the same collective behaviour of the many-body system.","sentences":["We present an approach to solving the evolution of a classical $N$-particle ensemble based on the path integral approach to classical mechanics.","This formulation provides a perturbative solution to the Liouville equation in terms of a propagator which can be expanded in a Dyson series.","We show that this perturbative expansion exactly corresponds to an iterative solution of the BBGKY-hierarchy in orders of the interaction potential.","Using the path integral formulation, we perform a Hubbard-Stratonovich transformation (HST) to obtain an effective field theoretic description in terms of macroscopic fields, which contains the full microscopic dynamics of the system in its vertices.","Naturally, the HST leads to a new perturbative expansion scheme which contains an infinite order of microscopic interactions already at the lowest order of the perturbative expansion.","Our approach can be applied to in and out of equilibrium systems with arbitrary interaction potentials and initial conditions.","We show how (unequal-time) cumulants of the Klimontovich phase space densities can be computed within this framework and derive results for density and momentum correlations for a spatially homogeneous system.","Under the explicit assumptions for the interaction potential and initial conditions, we show that well-known results related to plasma oscillations and the Jeans instability criterion for gravitational collapse can be recovered in the lowest order perturbative expansion and that both are the effect of the same collective behaviour of the many-body system."],"url":"http://arxiv.org/abs/2406.03425v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 16:11:15","title":"RemixTape: Enriching Narratives about Metrics with Semantic Alignment and Contextual Recommendation","abstract":"The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision making within enterprise organizations. Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values. However, these values are often presented in isolation and appropriate context is seldom externalized. In this design study, we present RemixTape, an application for constructing structured narratives around metrics. With design imperatives grounded in an formative interview study, RemixTape provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them. RemixTape incorporates affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas. We evaluated RemixTape in a user study in which six enterprise data professionals reproduced and extended partial narratives, with participants appreciating RemixTape as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics. We conclude with a reflection on how aspects of RemixTape could generalize beyond metrics, with a call to define a conceptual foundation for remixing in the context of visualization.","sentences":["The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision making within enterprise organizations.","Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values.","However, these values are often presented in isolation and appropriate context is seldom externalized.","In this design study, we present RemixTape, an application for constructing structured narratives around metrics.","With design imperatives grounded in an formative interview study, RemixTape provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them.","RemixTape incorporates affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas.","We evaluated RemixTape in a user study in which six enterprise data professionals reproduced and extended partial narratives, with participants appreciating RemixTape as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics.","We conclude with a reflection on how aspects of RemixTape could generalize beyond metrics, with a call to define a conceptual foundation for remixing in the context of visualization."],"url":"http://arxiv.org/abs/2406.03415v1","category":"cs.HC"}
{"created":"2024-06-05 15:54:50","title":"Automating Turkish Educational Quiz Generation Using Large Language Models","abstract":"Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding. In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context. We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes. This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content. Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation. The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English. The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish. By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes.","sentences":["Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding.","In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context.","We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes.","This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content.","Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation.","The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English.","The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish.","By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes."],"url":"http://arxiv.org/abs/2406.03397v1","category":"cs.CL"}
{"created":"2024-06-05 15:42:38","title":"Joint Association, Beamforming, and Resource Allocation for Multi-IRS Enabled MU-MISO Systems With RSMA","abstract":"Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems. This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes. The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively. Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems. The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms.","sentences":["Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems.","This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes.","The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively.","Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems.","The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms."],"url":"http://arxiv.org/abs/2406.03391v1","category":"eess.SP"}
{"created":"2024-06-05 15:38:02","title":"SelfReDepth: Self-Supervised Real-Time Depth Restoration for Consumer-Grade Sensors","abstract":"Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources. Data-driven denoising algorithms can mitigate such problems. However, they require vast amounts of ground truth depth data. Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors. Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest, highlighting a need for methods to effectively denoise depth maps in real-time dynamic environments. This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting full-depth maps captured with RGB-D sensors. The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence. Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms. Our results demonstrate our approach's real-time performance on real-world datasets. They show that it outperforms state-of-the-art denoising and restoration performance at over 30fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications.","sentences":["Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources.","Data-driven denoising algorithms can mitigate such problems.","However, they require vast amounts of ground truth depth data.","Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors.","Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest, highlighting a need for methods to effectively denoise depth maps in real-time dynamic environments.","This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting full-depth maps captured with RGB-D sensors.","The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence.","Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms.","Our results demonstrate our approach's real-time performance on real-world datasets.","They show that it outperforms state-of-the-art denoising and restoration performance at over 30fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications."],"url":"http://arxiv.org/abs/2406.03388v1","category":"cs.CV"}
{"created":"2024-06-05 15:37:30","title":"Measurement of the branching fraction ratios $R(D^{+})$ and $R(D^{*+})$ using muonic $\u03c4$ decays","abstract":"The branching fraction ratios of $\\overline{B}^0\\to D^+\\tau^-\\overline{\\nu}_{\\tau}$ and $\\overline{B}^0\\to D^{*+}\\tau^-\\overline{\\nu}_{\\tau}$ decays are measured with respect to their muonic counterparts, using a data sample corresponding to an integrated luminosity of 2.0 fb$^{-1}$ collected by the LHCb experiment in proton-proton collisions at $\\sqrt{s} = 13$ TeV. The reconstructed final states are formed by combining $D^+$ mesons with $\\tau^-\\to\\mu^-\\overline{\\nu}_{\\mu}\\nu_{\\tau}$ candidates, where the $D^+$ is reconstructed via the $D^+\\to K^-\\pi^+\\pi^+$ decay. The results are   \\begin{align*}   R(D^{+}) &= 0.249 \\pm 0.043 \\pm 0.047,   R(D^{*+}) &= 0.402 \\pm 0.081\\pm 0.085,   \\end{align*} where the first uncertainties are statistical and the second systematic. The two measurements have a correlation coefficient of $-0.39$ and are compatible with the Standard Model.","sentences":["The branching fraction ratios of $\\overline{B}^0\\to D^+\\tau^-\\overline{\\nu}_{\\tau}$ and $\\overline{B}^0\\to D^{*+}\\tau^-\\overline{\\nu}_{\\tau}$ decays are measured with respect to their muonic counterparts, using a data sample corresponding to an integrated luminosity of 2.0 fb$^{-1}$ collected by the LHCb experiment in proton-proton collisions at $\\sqrt{s} = 13$ TeV.","The reconstructed final states are formed by combining $D^+$ mesons with $\\tau^-\\to\\mu^-\\overline{\\nu}_{\\mu}\\nu_{\\tau}$ candidates, where the $D^+$ is reconstructed via the $D^+\\to K^-\\pi^+\\pi^+$ decay.","The results are   \\begin{align*}   R(D^{+}) &= 0.249 \\pm 0.043 \\pm 0.047,   R(D^{*+}) &= 0.402 \\pm 0.081\\pm 0.085,   \\end{align*} where the first uncertainties are statistical and the second systematic.","The two measurements have a correlation coefficient of $-0.39$ and are compatible with the Standard Model."],"url":"http://arxiv.org/abs/2406.03387v1","category":"hep-ex"}
{"created":"2024-06-05 15:36:55","title":"Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data","abstract":"We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning","sentences":["We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series.","We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process.","We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors.","We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data.","For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space.","We thoroughly investigate performance of our proposed methodology through several simulation studies.","We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning"],"url":"http://arxiv.org/abs/2406.03385v1","category":"stat.ME"}
{"created":"2024-06-05 15:23:08","title":"IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models","abstract":"Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\\% of the best-performing proprietary model GPT-4o performance. Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.","sentences":["Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages.","Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages.","In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU).","We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs.","Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages.","We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\\% of the best-performing proprietary model GPT-4o performance.","Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages."],"url":"http://arxiv.org/abs/2406.03368v1","category":"cs.CL"}
{"created":"2024-06-05 15:21:44","title":"CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning","abstract":"Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios. However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions. This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge. CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database. This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts. Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy. Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%.","sentences":["Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios.","However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions.","This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge.","CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database.","This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts.","Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy.","Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%."],"url":"http://arxiv.org/abs/2406.03367v1","category":"cs.AI"}
{"created":"2024-06-05 15:14:58","title":"What Matters in Hierarchical Search for Combinatorial Reasoning Problems?","abstract":"Efficiently tackling combinatorial reasoning problems, particularly the notorious NP-hard tasks, remains a significant challenge for AI research. Recent efforts have sought to enhance planning by incorporating hierarchical high-level search strategies, known as subgoal methods. While promising, their performance against traditional low-level planners is inconsistent, raising questions about their application contexts. In this study, we conduct an in-depth exploration of subgoal-planning methods for combinatorial reasoning. We identify the attributes pivotal for leveraging the advantages of high-level search: hard-to-learn value functions, complex action spaces, presence of dead ends in the environment, or using data collected from diverse experts. We propose a consistent evaluation methodology to achieve meaningful comparisons between methods and reevaluate the state-of-the-art algorithms.","sentences":["Efficiently tackling combinatorial reasoning problems, particularly the notorious NP-hard tasks, remains a significant challenge for AI research.","Recent efforts have sought to enhance planning by incorporating hierarchical high-level search strategies, known as subgoal methods.","While promising, their performance against traditional low-level planners is inconsistent, raising questions about their application contexts.","In this study, we conduct an in-depth exploration of subgoal-planning methods for combinatorial reasoning.","We identify the attributes pivotal for leveraging the advantages of high-level search: hard-to-learn value functions, complex action spaces, presence of dead ends in the environment, or using data collected from diverse experts.","We propose a consistent evaluation methodology to achieve meaningful comparisons between methods and reevaluate the state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2406.03361v1","category":"cs.LG"}
{"created":"2024-06-05 15:12:29","title":"Cooperative learning of Pl@ntNet's Artificial Intelligence algorithm: how does it work and how can we improve it?","abstract":"Deep learning models for plant species identification rely on large annotated datasets. The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills. Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging. Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information. Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user. Our proposed label aggregation strategy aims to cooperatively train plant identification AI models. This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data. The trust score is recursively estimated from correctly identified species given the current estimated labels. This interpretable score exploits botanical experts' knowledge and the heterogeneity of users. Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches. We evaluate PlantNet's strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users. We demonstrate that estimating users' skills based on the diversity of their expertise enhances labeling performance. Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset. We explore incorporating AI-based votes alongside human input. This can further enhance human-AI interactions to detect unreliable observations.","sentences":["Deep learning models for plant species identification rely on large annotated datasets.","The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills.","Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging.","Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information.","Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user.","Our proposed label aggregation strategy aims to cooperatively train plant identification AI models.","This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data.","The trust score is recursively estimated from correctly identified species given the current estimated labels.","This interpretable score exploits botanical experts' knowledge and the heterogeneity of users.","Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches.","We evaluate PlantNet's strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users.","We demonstrate that estimating users' skills based on the diversity of their expertise enhances labeling performance.","Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset.","We explore incorporating AI-based votes alongside human input.","This can further enhance human-AI interactions to detect unreliable observations."],"url":"http://arxiv.org/abs/2406.03356v1","category":"cs.LG"}
{"created":"2024-06-05 15:04:28","title":"Normalizing Flows for Conformal Regression","abstract":"Conformal Prediction (CP) algorithms estimate the uncertainty of a prediction model by calibrating its outputs on labeled data. The same calibration scheme usually applies to any model and data without modifications. The obtained prediction intervals are valid by construction but could be inefficient, i.e. unnecessarily big, if the prediction errors are not uniformly distributed over the input space.   We present a general scheme to localize the intervals by training the calibration process. The standard prediction error is replaced by an optimized distance metric that depends explicitly on the object attributes. Learning the optimal metric is equivalent to training a Normalizing Flow that acts on the joint distribution of the errors and the inputs. Unlike the Error Re-weighting CP algorithm of Papadopoulos et al. (2008), the framework allows estimating the gap between nominal and empirical conditional validity. The approach is compatible with existing locally-adaptive CP strategies based on re-weighting the calibration samples and applies to any point-prediction model without retraining.","sentences":["Conformal Prediction (CP) algorithms estimate the uncertainty of a prediction model by calibrating its outputs on labeled data.","The same calibration scheme usually applies to any model and data without modifications.","The obtained prediction intervals are valid by construction but could be inefficient, i.e. unnecessarily big, if the prediction errors are not uniformly distributed over the input space.   ","We present a general scheme to localize the intervals by training the calibration process.","The standard prediction error is replaced by an optimized distance metric that depends explicitly on the object attributes.","Learning the optimal metric is equivalent to training a Normalizing Flow that acts on the joint distribution of the errors and the inputs.","Unlike the Error Re-weighting CP algorithm of Papadopoulos et al. (2008), the framework allows estimating the gap between nominal and empirical conditional validity.","The approach is compatible with existing locally-adaptive CP strategies based on re-weighting the calibration samples and applies to any point-prediction model without retraining."],"url":"http://arxiv.org/abs/2406.03346v1","category":"cs.LG"}
{"created":"2024-06-05 15:04:27","title":"Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize","abstract":"Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks. We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network. Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts. Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations. Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization.","sentences":["Learning representations that generalize under distribution shifts is critical for building robust machine learning models.","However, despite significant efforts in recent years, algorithmic advances in this direction have been limited.","In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks.","We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network.","Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts.","Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations.","Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization."],"url":"http://arxiv.org/abs/2406.03345v2","category":"cs.LG"}
{"created":"2024-06-05 15:00:59","title":"Audio Mamba: Bidirectional State Space Model for Audio Representation Learning","abstract":"Transformers have rapidly become the preferred choice for audio classification, surpassing methods based on CNNs. However, Audio Spectrogram Transformers (ASTs) exhibit quadratic scaling due to self-attention. The removal of this quadratic self-attention cost presents an appealing direction. Recently, state space models (SSMs), such as Mamba, have demonstrated potential in language and vision tasks in this regard. In this study, we explore whether reliance on self-attention is necessary for audio classification tasks. By introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based model for audio classification, we aim to address this question. We evaluate AuM on various audio datasets - comprising six different benchmarks - where it achieves comparable or better performance compared to well-established AST model.","sentences":["Transformers have rapidly become the preferred choice for audio classification, surpassing methods based on CNNs.","However, Audio Spectrogram Transformers (ASTs) exhibit quadratic scaling due to self-attention.","The removal of this quadratic self-attention cost presents an appealing direction.","Recently, state space models (SSMs), such as Mamba, have demonstrated potential in language and vision tasks in this regard.","In this study, we explore whether reliance on self-attention is necessary for audio classification tasks.","By introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based model for audio classification, we aim to address this question.","We evaluate AuM on various audio datasets - comprising six different benchmarks - where it achieves comparable or better performance compared to well-established AST model."],"url":"http://arxiv.org/abs/2406.03344v1","category":"cs.SD"}
{"created":"2024-06-05 14:55:10","title":"The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches","abstract":"Chatbots have been an interesting application of natural language generation since its inception. With novel transformer based Generative AI methods, building chatbots have become trivial. Chatbots which are targeted at specific domains such as medicine, psychology, and general information retrieval are implemented rapidly. This, however, should not distract from the need to evaluate the chatbot responses. Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications. With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations. Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations.   We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation. Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval.","sentences":["Chatbots have been an interesting application of natural language generation since its inception.","With novel transformer based Generative AI methods, building chatbots have become trivial.","Chatbots which are targeted at specific domains such as medicine, psychology, and general information retrieval are implemented rapidly.","This, however, should not distract from the need to evaluate the chatbot responses.","Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications.","With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations.","Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations.   ","We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation.","Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval."],"url":"http://arxiv.org/abs/2406.03339v1","category":"cs.CL"}
{"created":"2024-06-05 14:36:33","title":"Comparative Benchmarking of Failure Detection Methods in Medical Image Segmentation: Unveiling the Role of Confidence Aggregation","abstract":"Semantic segmentation is an essential component of medical image analysis research, with recent deep learning algorithms offering out-of-the-box applicability across diverse datasets. Despite these advancements, segmentation failures remain a significant concern for real-world clinical applications, necessitating reliable detection mechanisms. This paper introduces a comprehensive benchmarking framework aimed at evaluating failure detection methodologies within medical image segmentation. Through our analysis, we identify the strengths and limitations of current failure detection metrics, advocating for the risk-coverage analysis as a holistic evaluation approach. Utilizing a collective dataset comprising five public 3D medical image collections, we assess the efficacy of various failure detection strategies under realistic test-time distribution shifts. Our findings highlight the importance of pixel confidence aggregation and we observe superior performance of the pairwise Dice score (Roy et al., 2019) between ensemble predictions, positioning it as a simple and robust baseline for failure detection in medical image segmentation. To promote ongoing research, we make the benchmarking framework available to the community.","sentences":["Semantic segmentation is an essential component of medical image analysis research, with recent deep learning algorithms offering out-of-the-box applicability across diverse datasets.","Despite these advancements, segmentation failures remain a significant concern for real-world clinical applications, necessitating reliable detection mechanisms.","This paper introduces a comprehensive benchmarking framework aimed at evaluating failure detection methodologies within medical image segmentation.","Through our analysis, we identify the strengths and limitations of current failure detection metrics, advocating for the risk-coverage analysis as a holistic evaluation approach.","Utilizing a collective dataset comprising five public 3D medical image collections, we assess the efficacy of various failure detection strategies under realistic test-time distribution shifts.","Our findings highlight the importance of pixel confidence aggregation and we observe superior performance of the pairwise Dice score (Roy et al., 2019) between ensemble predictions, positioning it as a simple and robust baseline for failure detection in medical image segmentation.","To promote ongoing research, we make the benchmarking framework available to the community."],"url":"http://arxiv.org/abs/2406.03323v1","category":"cs.CV"}
{"created":"2024-06-05 14:26:45","title":"Reproducibility study of FairAC","abstract":"This work aims to reproduce the findings of the paper \"Fair Attribute Completion on Graph with Missing Attributes\" written by Guo, Chu, and Li arXiv:2302.12977 by investigating the claims made in the paper. This paper suggests that the results of the original paper are reproducible and thus, the claims hold. However, the claim that FairAC is a generic framework for many downstream tasks is very broad and could therefore only be partially tested. Moreover, we show that FairAC is generalizable to various datasets and sensitive attributes and show evidence that the improvement in group fairness of the FairAC framework does not come at the expense of individual fairness. Lastly, the codebase of FairAC has been refactored and is now easily applicable for various datasets and models.","sentences":["This work aims to reproduce the findings of the paper \"Fair Attribute Completion on Graph with Missing Attributes\" written by Guo, Chu, and Li arXiv:2302.12977 by investigating the claims made in the paper.","This paper suggests that the results of the original paper are reproducible and thus, the claims hold.","However, the claim that FairAC is a generic framework for many downstream tasks is very broad and could therefore only be partially tested.","Moreover, we show that FairAC is generalizable to various datasets and sensitive attributes and show evidence that the improvement in group fairness of the FairAC framework does not come at the expense of individual fairness.","Lastly, the codebase of FairAC has been refactored and is now easily applicable for various datasets and models."],"url":"http://arxiv.org/abs/2406.03314v1","category":"cs.LG"}
{"created":"2024-06-05 14:08:54","title":"The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games","abstract":"Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.   In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human emotional responses.","sentences":["Behavior study experiments are an important part of society modeling and understanding human interactions.","In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies.","Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior.","However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.   ","In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states.","Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies.","While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions.","Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human emotional responses."],"url":"http://arxiv.org/abs/2406.03299v1","category":"cs.AI"}
{"created":"2024-06-05 14:00:46","title":"Evaluating AI fairness in credit scoring with the BRIO tool","abstract":"We present a method for quantitative, in-depth analyses of fairness issues in AI systems with an application to credit scoring. To this aim we use BRIO, a tool for the evaluation of AI systems with respect to social unfairness and, more in general, ethically undesirable behaviours. It features a model-agnostic bias detection module, presented in \\cite{DBLP:conf/beware/CoragliaDGGPPQ23}, to which a full-fledged unfairness risk evaluation module is added. As a case study, we focus on the context of credit scoring, analysing the UCI German Credit Dataset \\cite{misc_statlog_(german_credit_data)_144}. We apply the BRIO fairness metrics to several, socially sensitive attributes featured in the German Credit Dataset, quantifying fairness across various demographic segments, with the aim of identifying potential sources of bias and discrimination in a credit scoring model. We conclude by combining our results with a revenue analysis.","sentences":["We present a method for quantitative, in-depth analyses of fairness issues in AI systems with an application to credit scoring.","To this aim we use BRIO, a tool for the evaluation of AI systems with respect to social unfairness and, more in general, ethically undesirable behaviours.","It features a model-agnostic bias detection module, presented in \\cite{DBLP:conf/beware/CoragliaDGGPPQ23}, to which a full-fledged unfairness risk evaluation module is added.","As a case study, we focus on the context of credit scoring, analysing the UCI German Credit Dataset \\cite{misc_statlog_(german_credit_data)_144}.","We apply the BRIO fairness metrics to several, socially sensitive attributes featured in the German Credit Dataset, quantifying fairness across various demographic segments, with the aim of identifying potential sources of bias and discrimination in a credit scoring model.","We conclude by combining our results with a revenue analysis."],"url":"http://arxiv.org/abs/2406.03292v1","category":"cs.AI"}
{"created":"2024-06-05 13:59:03","title":"SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms","abstract":"Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation. Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models. However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization. This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones. Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs. In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed. We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM. It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible. SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling. Our code is available at https://github.com/Xingrun-Xing/SpikeLM.","sentences":["Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation.","Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models.","However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization.","This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones.","Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs.","In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed.","We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM.","It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible.","SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling.","Our code is available at https://github.com/Xingrun-Xing/SpikeLM."],"url":"http://arxiv.org/abs/2406.03287v1","category":"cs.NE"}
{"created":"2024-06-05 13:56:42","title":"Enhancing Repository-Level Code Generation with Integrated Contextual Information","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 17.35%, in terms of pass@k score. Furthermore, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks.","However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository.","Existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context.","In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages.","CatCoder enhances repository-level code generation by integrating relevant code and type context.","Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs.","To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks.","The results show that CatCoder outperforms the RepoCoder baseline by up to 17.35%, in terms of pass@k score.","Furthermore, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models.","Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder."],"url":"http://arxiv.org/abs/2406.03283v1","category":"cs.SE"}
{"created":"2024-06-05 13:54:28","title":"FusionBench: A Comprehensive Benchmark of Deep Model Fusion","abstract":"Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner. This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts. To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion. FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation. Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies. We implement and evaluate a broad spectrum of deep model fusion techniques. These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models. FusionBench now contains 26 distinct tasks, 74 fine-tuned models, and 16 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques. In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results. Homepage https://tanganke.github.io/fusion_bench/","sentences":["Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner.","This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance.","Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts.","To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion.","FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation.","Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies.","We implement and evaluate a broad spectrum of deep model fusion techniques.","These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models.","FusionBench now contains 26 distinct tasks, 74 fine-tuned models, and 16 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques.","In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results.","Homepage https://tanganke.github.io/fusion_bench/"],"url":"http://arxiv.org/abs/2406.03280v1","category":"cs.LG"}
{"created":"2024-06-05 13:53:20","title":"Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning","abstract":"Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future.","sentences":["Second-order information is valuable for many applications but challenging to compute.","Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient.","In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community.","We introduce HesScale, an improvement over BL89, which adds negligible extra computation.","On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute.","We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter.","In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling.","These findings are promising for scaling second-order methods in larger models in the future."],"url":"http://arxiv.org/abs/2406.03276v1","category":"cs.LG"}
{"created":"2024-06-05 13:53:05","title":"Improved stability for the size and structure of sumsets","abstract":"Let $A \\subset \\mathbb{Z}^d$ be a finite set. It is known that the sumset $NA$ has predictable size ($\\vert NA\\vert = P_A(N)$ for some $P_A(X) \\in \\mathbb{Q}[X]$) and structure (all of the lattice points in some finite cone other than all of the lattice points in a finite collection of exceptional subcones), once $N$ is larger than some threshold. In previous work, joint with Shakan, the first and third named authors established the first effective bounds for both of these thresholds for an arbitrary set $A$. In this article we substantially improve each of these bounds, coming much closer to the corresponding lower bounds known.","sentences":["Let $A \\subset \\mathbb{Z}^d$ be a finite set.","It is known that the sumset $NA$ has predictable size ($\\vert NA\\vert","= P_A(N)$ for some $P_A(X) \\in \\mathbb{Q}[X]$) and structure (all of the lattice points in some finite cone other than all of the lattice points in a finite collection of exceptional subcones), once $N$ is larger than some threshold.","In previous work, joint with Shakan, the first and third named authors established the first effective bounds for both of these thresholds for an arbitrary set $A$.","In this article we substantially improve each of these bounds, coming much closer to the corresponding lower bounds known."],"url":"http://arxiv.org/abs/2406.03275v1","category":"math.CO"}
{"created":"2024-06-05 13:52:55","title":"Enhancing CTC-based speech recognition with diverse modeling units","abstract":"In recent years, the evolution of end-to-end (E2E) automatic speech recognition (ASR) models has been remarkable, largely due to advances in deep learning architectures like transformer. On top of E2E systems, researchers have achieved substantial accuracy improvement by rescoring E2E model's N-best hypotheses with a phoneme-based model. This raises an interesting question about where the improvements come from other than the system combination effect. We examine the underlying mechanisms driving these gains and propose an efficient joint training approach, where E2E models are trained jointly with diverse modeling units. This methodology does not only align the strengths of both phoneme and grapheme-based models but also reveals that using these diverse modeling units in a synergistic way can significantly enhance model accuracy. Our findings offer new insights into the optimal integration of heterogeneous modeling units in the development of more robust and accurate ASR systems.","sentences":["In recent years, the evolution of end-to-end (E2E) automatic speech recognition (ASR) models has been remarkable, largely due to advances in deep learning architectures like transformer.","On top of E2E systems, researchers have achieved substantial accuracy improvement by rescoring E2E model's N-best hypotheses with a phoneme-based model.","This raises an interesting question about where the improvements come from other than the system combination effect.","We examine the underlying mechanisms driving these gains and propose an efficient joint training approach, where E2E models are trained jointly with diverse modeling units.","This methodology does not only align the strengths of both phoneme and grapheme-based models but also reveals that using these diverse modeling units in a synergistic way can significantly enhance model accuracy.","Our findings offer new insights into the optimal integration of heterogeneous modeling units in the development of more robust and accurate ASR systems."],"url":"http://arxiv.org/abs/2406.03274v1","category":"eess.AS"}
{"created":"2024-06-05 13:52:45","title":"VWise: A novel benchmark for evaluating scene classification for vehicular applications","abstract":"Current datasets for vehicular applications are mostly collected in North America or Europe. Models trained or evaluated on these datasets might suffer from geographical bias when deployed in other regions. Specifically, for scene classification, a highway in a Latin American country differs drastically from an Autobahn, for example, both in design and maintenance levels. We propose VWise, a novel benchmark for road-type classification and scene classification tasks, in addition to tasks focused on external contexts related to vehicular applications in LatAm. We collected over 520 video clips covering diverse urban and rural environments across Latin American countries, annotated with six classes of road types. We also evaluated several state-of-the-art classification models in baseline experiments, obtaining over 84% accuracy. With this dataset, we aim to enhance research on vehicular tasks in Latin America.","sentences":["Current datasets for vehicular applications are mostly collected in North America or Europe.","Models trained or evaluated on these datasets might suffer from geographical bias when deployed in other regions.","Specifically, for scene classification, a highway in a Latin American country differs drastically from an Autobahn, for example, both in design and maintenance levels.","We propose VWise, a novel benchmark for road-type classification and scene classification tasks, in addition to tasks focused on external contexts related to vehicular applications in LatAm.","We collected over 520 video clips covering diverse urban and rural environments across Latin American countries, annotated with six classes of road types.","We also evaluated several state-of-the-art classification models in baseline experiments, obtaining over 84% accuracy.","With this dataset, we aim to enhance research on vehicular tasks in Latin America."],"url":"http://arxiv.org/abs/2406.03273v1","category":"cs.CV"}
{"created":"2024-06-05 13:50:59","title":"Multi-Microphone Speech Emotion Recognition using the Hierarchical Token-semantic Audio Transformer Architecture","abstract":"Most emotion recognition systems fail in real-life situations (in the wild scenarios) where the audio is contaminated by reverberation. Our study explores new methods to alleviate the performance degradation of Speech Emotion Recognition (SER) algorithms and develop a more robust system for adverse conditions. We propose processing multi-microphone signals to address these challenges and improve emotion classification accuracy. We adopt a state-of-the-art transformer model, the Hierarchical Token-semantic Audio Transformer (HTS-AT), to handle multi-channel audio inputs. We evaluate two strategies: averaging mel-spectrograms across channels and summing patch-embedded representations. Our multimicrophone model achieves superior performance compared to single-channel baselines when tested on real-world reverberant environments.","sentences":["Most emotion recognition systems fail in real-life situations (in the wild scenarios) where the audio is contaminated by reverberation.","Our study explores new methods to alleviate the performance degradation of Speech Emotion Recognition (SER) algorithms and develop a more robust system for adverse conditions.","We propose processing multi-microphone signals to address these challenges and improve emotion classification accuracy.","We adopt a state-of-the-art transformer model, the Hierarchical Token-semantic Audio Transformer (HTS-AT), to handle multi-channel audio inputs.","We evaluate two strategies: averaging mel-spectrograms across channels and summing patch-embedded representations.","Our multimicrophone model achieves superior performance compared to single-channel baselines when tested on real-world reverberant environments."],"url":"http://arxiv.org/abs/2406.03272v1","category":"eess.AS"}
{"created":"2024-06-05 13:41:26","title":"No-Regret Algorithms for Safe Bayesian Optimization with Monotonicity Constraints","abstract":"We consider the problem of sequentially maximizing an unknown function $f$ over a set of actions of the form $(s,\\mathbf{x})$, where the selected actions must satisfy a safety constraint with respect to an unknown safety function $g$. We model $f$ and $g$ as lying in a reproducing kernel Hilbert space (RKHS), which facilitates the use of Gaussian process methods. While existing works for this setting have provided algorithms that are guaranteed to identify a near-optimal safe action, the problem of attaining low cumulative regret has remained largely unexplored, with a key challenge being that expanding the safe region can incur high regret. To address this challenge, we show that if $g$ is monotone with respect to just the single variable $s$ (with no such constraint on $f$), sublinear regret becomes achievable with our proposed algorithm. In addition, we show that a modified version of our algorithm is able to attain sublinear regret (for suitably defined notions of regret) for the task of finding a near-optimal $s$ corresponding to every $\\mathbf{x}$, as opposed to only finding the global safe optimum. Our findings are supported with empirical evaluations on various objective and safety functions.","sentences":["We consider the problem of sequentially maximizing an unknown function $f$ over a set of actions of the form $(s,\\mathbf{x})$, where the selected actions must satisfy a safety constraint with respect to an unknown safety function $g$. We model $f$ and $g$ as lying in a reproducing kernel Hilbert space (RKHS), which facilitates the use of Gaussian process methods.","While existing works for this setting have provided algorithms that are guaranteed to identify a near-optimal safe action, the problem of attaining low cumulative regret has remained largely unexplored, with a key challenge being that expanding the safe region can incur high regret.","To address this challenge, we show that if $g$ is monotone with respect to just the single variable $s$ (with no such constraint on $f$), sublinear regret becomes achievable with our proposed algorithm.","In addition, we show that a modified version of our algorithm is able to attain sublinear regret (for suitably defined notions of regret) for the task of finding a near-optimal $s$ corresponding to every $\\mathbf{x}$, as opposed to only finding the global safe optimum.","Our findings are supported with empirical evaluations on various objective and safety functions."],"url":"http://arxiv.org/abs/2406.03264v1","category":"stat.ML"}
{"created":"2024-06-05 13:41:09","title":"Deep Generative Models for Proton Zero Degree Calorimeter Simulations in ALICE, CERN","abstract":"Simulating detector responses is a crucial part of understanding the inner-workings of particle collisions in the Large Hadron Collider at CERN. The current reliance on statistical Monte-Carlo simulations strains CERN's computational grid, underscoring the urgency for more efficient alternatives. Addressing these challenges, recent proposals advocate for generative machine learning methods. In this study, we present an innovative deep learning simulation approach tailored for the proton Zero Degree Calorimeter in the ALICE experiment. Leveraging a Generative Adversarial Network model with Selective Diversity Increase loss, we directly simulate calorimeter responses. To enhance its capabilities in modeling a broad range of calorimeter response intensities, we expand the SDI-GAN architecture with additional regularization. Moreover, to improve the spatial fidelity of the generated data, we introduce an auxiliary regressor network. Our method offers a significant speedup when comparing to the traditional Monte-Carlo based approaches.","sentences":["Simulating detector responses is a crucial part of understanding the inner-workings of particle collisions in the Large Hadron Collider at CERN.","The current reliance on statistical Monte-Carlo simulations strains CERN's computational grid, underscoring the urgency for more efficient alternatives.","Addressing these challenges, recent proposals advocate for generative machine learning methods.","In this study, we present an innovative deep learning simulation approach tailored for the proton Zero Degree Calorimeter in the ALICE experiment.","Leveraging a Generative Adversarial Network model with Selective Diversity Increase loss, we directly simulate calorimeter responses.","To enhance its capabilities in modeling a broad range of calorimeter response intensities, we expand the SDI-GAN architecture with additional regularization.","Moreover, to improve the spatial fidelity of the generated data, we introduce an auxiliary regressor network.","Our method offers a significant speedup when comparing to the traditional Monte-Carlo based approaches."],"url":"http://arxiv.org/abs/2406.03263v1","category":"cs.LG"}
{"created":"2024-06-05 13:28:28","title":"ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in Meetings","abstract":"Speaker Diarization (SD) aims at grouping speech segments that belong to the same speaker. This task is required in many speech-processing applications, such as rich meeting transcription. In this context, distant microphone arrays usually capture the audio signal. Beamforming, i.e., spatial filtering, is a common practice to process multi-microphone audio data. However, it often requires an explicit localization of the active source to steer the filter. This paper proposes a self-attention-based algorithm to select the output of a bank of fixed spatial filters. This method serves as a feature extractor for joint Voice Activity (VAD) and Overlapped Speech Detection (OSD). The speaker diarization is then inferred from the detected segments. The approach shows convincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the AISHELL-4 dataset. The analysis of the self-attention weights demonstrates their explainability, as they correlate with the speaker's angular locations.","sentences":["Speaker Diarization (SD) aims at grouping speech segments that belong to the same speaker.","This task is required in many speech-processing applications, such as rich meeting transcription.","In this context, distant microphone arrays usually capture the audio signal.","Beamforming, i.e., spatial filtering, is a common practice to process multi-microphone audio data.","However, it often requires an explicit localization of the active source to steer the filter.","This paper proposes a self-attention-based algorithm to select the output of a bank of fixed spatial filters.","This method serves as a feature extractor for joint Voice Activity (VAD) and Overlapped Speech Detection (OSD).","The speaker diarization is then inferred from the detected segments.","The approach shows convincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the AISHELL-4 dataset.","The analysis of the self-attention weights demonstrates their explainability, as they correlate with the speaker's angular locations."],"url":"http://arxiv.org/abs/2406.03251v1","category":"cs.SD"}
{"created":"2024-06-05 13:26:30","title":"Prompt-based Visual Alignment for Zero-shot Policy Transfer","abstract":"Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data.","sentences":["Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL).","Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains.","Besides, abundant data from multiple domains are needed.","To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer.","Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner.","Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance.","To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens.","With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains.","We verify PVA on a vision-based autonomous driving task with CARLA simulator.","Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data."],"url":"http://arxiv.org/abs/2406.03250v1","category":"cs.CV"}
{"created":"2024-06-05 13:23:23","title":"Large Language Models as Evaluators for Recommendation Explanations","abstract":"The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning. However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.","sentences":["The explainability of recommender systems has attracted significant attention in academia and industry.","Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue.","In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning.","However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective.","In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations.","To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations.","We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users.","Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings.","We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations.","Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts.","Our code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator."],"url":"http://arxiv.org/abs/2406.03248v2","category":"cs.IR"}
{"created":"2024-06-05 13:22:04","title":"Intrinsic permeability of heterogeneous porous media","abstract":"Providing a sound appraisal of the nature of the relationship between flow $(Q)$ and pressure drop $(\\Delta P)$ for porous media is a long-standing fundamental research challenge. A wide variety of environmental, societal and industrial issues, ranging, e.g., from water-soil system remediation to subsurface energy optimization, is affected by this critical issue. While such dependence is well represented by the Kozeny-Carman formulation for homogeneous media, the fundamental nature of such a relationship ($Q$ vs $\\Delta P$) within heterogeneous porous systems characterized by a broad range of pore sizes is still not fully understood. We design a set of controlled and complex porous structures and quantify their intrinsic permeability through detailed high quality microfluidics experiments. We synthesize the results upon deriving an original analytical formulation relating the overall intrinsic permeability of the porous structure and their key features. Our formulation explicitly embeds the spatial variability of pore sizes into the medium permeability through a conceptualization of the system as a collection of smaller scale porous media arranged in series. The resulting analytical formulation yields permeability values matching their experimentally-based counterparts without the need of additional tunable parameters. Our study then documents and supports the strong role played by the micro-structure on the overall medium permeability.","sentences":["Providing a sound appraisal of the nature of the relationship between flow $(Q)$ and pressure drop $(\\Delta P)$ for porous media is a long-standing fundamental research challenge.","A wide variety of environmental, societal and industrial issues, ranging, e.g., from water-soil system remediation to subsurface energy optimization, is affected by this critical issue.","While such dependence is well represented by the Kozeny-Carman formulation for homogeneous media, the fundamental nature of such a relationship ($Q$ vs $\\Delta P$) within heterogeneous porous systems characterized by a broad range of pore sizes is still not fully understood.","We design a set of controlled and complex porous structures and quantify their intrinsic permeability through detailed high quality microfluidics experiments.","We synthesize the results upon deriving an original analytical formulation relating the overall intrinsic permeability of the porous structure and their key features.","Our formulation explicitly embeds the spatial variability of pore sizes into the medium permeability through a conceptualization of the system as a collection of smaller scale porous media arranged in series.","The resulting analytical formulation yields permeability values matching their experimentally-based counterparts without the need of additional tunable parameters.","Our study then documents and supports the strong role played by the micro-structure on the overall medium permeability."],"url":"http://arxiv.org/abs/2406.03246v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 13:21:46","title":"Reconfiguring Participatory Design to Resist AI Realism","abstract":"The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order. In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism. I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system. I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible. I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values.","sentences":["The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order.","In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism.","I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system.","I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible.","I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values."],"url":"http://arxiv.org/abs/2406.03245v1","category":"cs.HC"}
{"created":"2024-06-05 13:16:55","title":"Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion strategy","abstract":"With the proliferation of deepfake audio, there is an urgent need to investigate their attribution. Current source tracing methods can effectively distinguish in-distribution (ID) categories. However, the rapid evolution of deepfake algorithms poses a critical challenge in the accurate identification of out-of-distribution (OOD) novel deepfake algorithms. In this paper, we propose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake algorithm recognition, demonstrating its effectiveness in discriminating ID samples while identifying OOD samples. For effective OOD detection, we first explore current post-hoc OOD methods and propose NSD, a novel OOD approach in identifying novel deepfake algorithms through the similarity consideration of both feature and logits scores. REFD achieves 86.83% F1-score as a single system in Audio Deepfake Detection Challenge 2023 Track3, showcasing its state-of-the-art performance.","sentences":["With the proliferation of deepfake audio, there is an urgent need to investigate their attribution.","Current source tracing methods can effectively distinguish in-distribution (ID) categories.","However, the rapid evolution of deepfake algorithms poses a critical challenge in the accurate identification of out-of-distribution (OOD) novel deepfake algorithms.","In this paper, we propose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake algorithm recognition, demonstrating its effectiveness in discriminating ID samples while identifying OOD samples.","For effective OOD detection, we first explore current post-hoc OOD methods and propose NSD, a novel OOD approach in identifying novel deepfake algorithms through the similarity consideration of both feature and logits scores.","REFD achieves 86.83% F1-score as a single system in Audio Deepfake Detection Challenge 2023 Track3, showcasing its state-of-the-art performance."],"url":"http://arxiv.org/abs/2406.03240v1","category":"cs.SD"}
{"created":"2024-06-05 13:15:37","title":"Error-preserving Automatic Speech Recognition of Young English Learners' Language","abstract":"One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. Recent advances in speech technology and natural language processing allow for the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR), which faces a number of challenges: first, state-of-the-art ASR models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech. Second, most ASR systems contain a powerful language model, which smooths out errors made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the errors made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their errors. For this, we collected a corpus containing around 85 hours of English audio spoken by learners in Switzerland from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning on children's voices and has a much higher error preservation rate than other models.","sentences":["One of the central skills that language learners need to practice is speaking the language.","Currently, students in school do not get enough speaking opportunities and lack conversational practice.","Recent advances in speech technology and natural language processing allow for the creation of novel tools to practice their speaking skills.","In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR), which faces a number of challenges: first, state-of-the-art ASR models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech.","Second, most ASR systems contain a powerful language model, which smooths out errors made by the speakers.","To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the errors made by the language learners.","In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their errors.","For this, we collected a corpus containing around 85 hours of English audio spoken by learners in Switzerland from grades 4 to 6 on different language learning tasks, which we used to train an ASR model.","Our experiments show that our model benefits from direct fine-tuning on children's voices and has a much higher error preservation rate than other models."],"url":"http://arxiv.org/abs/2406.03235v1","category":"cs.CL"}
{"created":"2024-06-05 13:13:58","title":"Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning","abstract":"Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods.","sentences":["Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL).","Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities.","Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics.","In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL.","The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups.","This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training.","Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial.","We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods."],"url":"http://arxiv.org/abs/2406.03234v1","category":"cs.LG"}
{"created":"2024-06-05 17:32:22","title":"Convolutional Neural Networks and Vision Transformers for Fashion MNIST Classification: A Literature Review","abstract":"Our review explores the comparative analysis between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the domain of image classification, with a particular focus on clothing classification within the e-commerce sector. Utilizing the Fashion MNIST dataset, we delve into the unique attributes of CNNs and ViTs. While CNNs have long been the cornerstone of image classification, ViTs introduce an innovative self-attention mechanism enabling nuanced weighting of different input data components. Historically, transformers have primarily been associated with Natural Language Processing (NLP) tasks. Through a comprehensive examination of existing literature, our aim is to unveil the distinctions between ViTs and CNNs in the context of image classification. Our analysis meticulously scrutinizes state-of-the-art methodologies employing both architectures, striving to identify the factors influencing their performance. These factors encompass dataset characteristics, image dimensions, the number of target classes, hardware infrastructure, and the specific architectures along with their respective top results. Our key goal is to determine the most appropriate architecture between ViT and CNN for classifying images in the Fashion MNIST dataset within the e-commerce industry, while taking into account specific conditions and needs. We highlight the importance of combining these two architectures with different forms to enhance overall performance. By uniting these architectures, we can take advantage of their unique strengths, which may lead to more precise and reliable models for e-commerce applications. CNNs are skilled at recognizing local patterns, while ViTs are effective at grasping overall context, making their combination a promising strategy for boosting image classification performance.","sentences":["Our review explores the comparative analysis between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the domain of image classification, with a particular focus on clothing classification within the e-commerce sector.","Utilizing the Fashion MNIST dataset, we delve into the unique attributes of CNNs and ViTs.","While CNNs have long been the cornerstone of image classification, ViTs introduce an innovative self-attention mechanism enabling nuanced weighting of different input data components.","Historically, transformers have primarily been associated with Natural Language Processing (NLP) tasks.","Through a comprehensive examination of existing literature, our aim is to unveil the distinctions between ViTs and CNNs in the context of image classification.","Our analysis meticulously scrutinizes state-of-the-art methodologies employing both architectures, striving to identify the factors influencing their performance.","These factors encompass dataset characteristics, image dimensions, the number of target classes, hardware infrastructure, and the specific architectures along with their respective top results.","Our key goal is to determine the most appropriate architecture between ViT and CNN for classifying images in the Fashion MNIST dataset within the e-commerce industry, while taking into account specific conditions and needs.","We highlight the importance of combining these two architectures with different forms to enhance overall performance.","By uniting these architectures, we can take advantage of their unique strengths, which may lead to more precise and reliable models for e-commerce applications.","CNNs are skilled at recognizing local patterns, while ViTs are effective at grasping overall context, making their combination a promising strategy for boosting image classification performance."],"url":"http://arxiv.org/abs/2406.03478v1","category":"cs.CV"}
{"created":"2024-06-05 17:29:22","title":"Lagrangian filtering for wave-mean flow decomposition","abstract":"Geophysical flows are typically composed of wave and mean motions with a wide range of overlapping temporal scales, making separation between the two types of motion in wave-resolving numerical simulations challenging. Lagrangian filtering - whereby a temporal filter is applied in the frame of the flow - is an effective way to overcome this challenge, allowing clean separation of waves from mean flow based on frequency separation in a Lagrangian frame. Previous implementations of Lagrangian filtering have used particle tracking approaches, which are subject to large memory requirements or difficulties with particle clustering. Kafiabad and Vanneste (2023, KV23) recently proposed a novel method for finding Lagrangian means without particle tracking by solving a set of partial differential equations alongside the governing equations of the flow. In this work, we adapt the approach of KV23 to develop a flexible, on-the-fly, PDE-based method for Lagrangian filtering using arbitrary convolutional filters. We present several different wave-mean decompositions, demonstrating that our Lagrangian methods are capable of recovering a clean wave-field from a nonlinear simulation of geostrophic turbulence interacting with Poincar\\'e waves.","sentences":["Geophysical flows are typically composed of wave and mean motions with a wide range of overlapping temporal scales, making separation between the two types of motion in wave-resolving numerical simulations challenging.","Lagrangian filtering - whereby a temporal filter is applied in the frame of the flow - is an effective way to overcome this challenge, allowing clean separation of waves from mean flow based on frequency separation in a Lagrangian frame.","Previous implementations of Lagrangian filtering have used particle tracking approaches, which are subject to large memory requirements or difficulties with particle clustering.","Kafiabad and Vanneste (2023, KV23) recently proposed a novel method for finding Lagrangian means without particle tracking by solving a set of partial differential equations alongside the governing equations of the flow.","In this work, we adapt the approach of KV23 to develop a flexible, on-the-fly, PDE-based method for Lagrangian filtering using arbitrary convolutional filters.","We present several different wave-mean decompositions, demonstrating that our Lagrangian methods are capable of recovering a clean wave-field from a nonlinear simulation of geostrophic turbulence interacting with Poincar\\'e waves."],"url":"http://arxiv.org/abs/2406.03477v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 17:25:46","title":"AD-H: Autonomous Driving with Hierarchical Agents","abstract":"Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments. However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers. As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning. To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between. We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution. The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning. We build a new dataset with action hierarchy annotations. Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system. First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset. Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods. We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H","sentences":["Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments.","However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers.","As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning.","To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between.","We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution.","The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning.","We build a new dataset with action hierarchy annotations.","Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system.","First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset.","Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods.","We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H"],"url":"http://arxiv.org/abs/2406.03474v1","category":"cs.CV"}
{"created":"2024-06-05 17:25:33","title":"Mapping dynamical systems into chemical reactions","abstract":"Dynamical systems with polynomials on the right-hand side can model a wide range of physical processes. A subset of such dynamical systems that can model chemical reactions under mass-action kinetics are called chemical systems. A central problem in synthetic biology is to map general polynomial dynamical systems into dynamically similar chemical ones. In this paper, we present a novel map, called the quasi-chemical map, that can systematically solve this problem. The quasi-chemical map introduces suitable state-dependent perturbations into any given polynomial dynamical system which then becomes chemical under suitably large translation of variables. We prove that this map preserves robust dynamical features, such as generic equilibria and limit cycles, as well as temporal properties, such as periods of oscillations. Furthermore, the resulting chemical systems are of only at most one degree higher than the original dynamical systems. We demonstrate the quasi-chemical map by designing relatively simple chemical systems with exotic dynamics and pre-defined bifurcation structures.","sentences":["Dynamical systems with polynomials on the right-hand side can model a wide range of physical processes.","A subset of such dynamical systems that can model chemical reactions under mass-action kinetics are called chemical systems.","A central problem in synthetic biology is to map general polynomial dynamical systems into dynamically similar chemical ones.","In this paper, we present a novel map, called the quasi-chemical map, that can systematically solve this problem.","The quasi-chemical map introduces suitable state-dependent perturbations into any given polynomial dynamical system which then becomes chemical under suitably large translation of variables.","We prove that this map preserves robust dynamical features, such as generic equilibria and limit cycles, as well as temporal properties, such as periods of oscillations.","Furthermore, the resulting chemical systems are of only at most one degree higher than the original dynamical systems.","We demonstrate the quasi-chemical map by designing relatively simple chemical systems with exotic dynamics and pre-defined bifurcation structures."],"url":"http://arxiv.org/abs/2406.03473v1","category":"q-bio.MN"}
{"created":"2024-06-05 17:11:59","title":"Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles","abstract":"We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.","sentences":["We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas.","The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion.","When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data.","We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences.","We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina.","These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured.","We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention.","Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes."],"url":"http://arxiv.org/abs/2406.03463v1","category":"stat.ME"}
{"created":"2024-06-05 17:00:16","title":"Recurrent neural chemical reaction networks that approximate arbitrary dynamics","abstract":"Many important phenomena in chemistry and biology are realized via dynamical features such as multi-stability, oscillations, and chaos. Construction of novel chemical systems with such finely-tuned dynamics is a challenging problem central to the growing field of synthetic biology. In this paper, we address this problem by putting forward a molecular version of a recurrent artificial neural network, which we call a recurrent neural chemical reaction network (RNCRN). We prove that the RNCRN, with sufficiently many auxiliary chemical species and suitable fast reactions, can be systematically trained to achieve any dynamics. This approximation ability is shown to hold independent of the initial conditions for the auxiliary species, making the RNCRN more experimentally feasible. To demonstrate the results, we present a number of relatively simple RNCRNs trained to display a variety of biologically-important dynamical features.","sentences":["Many important phenomena in chemistry and biology are realized via dynamical features such as multi-stability, oscillations, and chaos.","Construction of novel chemical systems with such finely-tuned dynamics is a challenging problem central to the growing field of synthetic biology.","In this paper, we address this problem by putting forward a molecular version of a recurrent artificial neural network, which we call a recurrent neural chemical reaction network (RNCRN).","We prove that the RNCRN, with sufficiently many auxiliary chemical species and suitable fast reactions, can be systematically trained to achieve any dynamics.","This approximation ability is shown to hold independent of the initial conditions for the auxiliary species, making the RNCRN more experimentally feasible.","To demonstrate the results, we present a number of relatively simple RNCRNs trained to display a variety of biologically-important dynamical features."],"url":"http://arxiv.org/abs/2406.03456v1","category":"q-bio.MN"}
{"created":"2024-06-05 16:57:57","title":"Mission Design for Unmanned Aerial Vehicles using Hybrid Probabilistic Logic Program","abstract":"Advanced Air Mobility (AAM) is a growing field that demands a deep understanding of legal, spatial and temporal concepts in navigation. Hence, any implementation of AAM is forced to deal with the inherent uncertainties of human-inhabited spaces. Enabling growth and innovation requires the creation of a system for safe and robust mission design, i.e., the way we formalize intentions and decide their execution as trajectories for the Unmanned Aerial Vehicle (UAV). Although legal frameworks have emerged to govern urban air spaces, their full integration into the decision process of autonomous agents and operators remains an open task. In this work we present ProMis, a system architecture for probabilistic mission design. It links the data available from various static and dynamic data sources with legal text and operator requirements by following principles of formal verification and probabilistic modeling. Hereby, ProMis enables the combination of low-level perception and high-level rules in AAM to infer validity over the UAV's state-space. To this end, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying, intermediate representation between perception and action-taking. Furthermore, we present methods to connect ProMis with crowd-sourced map data by generating HPLP atoms that represent spatial relations in a probabilistic fashion. Our claims of the utility and generality of ProMis are supported by experiments on a diverse set of scenarios and a discussion of the computational demands associated with probabilistic missions.","sentences":["Advanced Air Mobility (AAM) is a growing field that demands a deep understanding of legal, spatial and temporal concepts in navigation.","Hence, any implementation of AAM is forced to deal with the inherent uncertainties of human-inhabited spaces.","Enabling growth and innovation requires the creation of a system for safe and robust mission design, i.e., the way we formalize intentions and decide their execution as trajectories for the Unmanned Aerial Vehicle (UAV).","Although legal frameworks have emerged to govern urban air spaces, their full integration into the decision process of autonomous agents and operators remains an open task.","In this work we present ProMis, a system architecture for probabilistic mission design.","It links the data available from various static and dynamic data sources with legal text and operator requirements by following principles of formal verification and probabilistic modeling.","Hereby, ProMis enables the combination of low-level perception and high-level rules in AAM to infer validity over the UAV's state-space.","To this end, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying, intermediate representation between perception and action-taking.","Furthermore, we present methods to connect ProMis with crowd-sourced map data by generating HPLP atoms that represent spatial relations in a probabilistic fashion.","Our claims of the utility and generality of ProMis are supported by experiments on a diverse set of scenarios and a discussion of the computational demands associated with probabilistic missions."],"url":"http://arxiv.org/abs/2406.03454v1","category":"cs.RO"}
{"created":"2024-06-05 16:45:49","title":"Even Integer Quantum Hall Effect in Materials with Hidden Spin Texture","abstract":"Because spin-orbit coupling (SOC) is invisible in the band structure when inversion symmetry exists, whether spins are trivially degenerate or strongly coupled to momentum due to SOC is presumed to make little difference in transport measurements, such as magnetoresistance and quantum oscillations. In this work, however, we show that hidden Rashba SOC in a centrosymmetric two-dimensional material can lead to the quantum Hall effect with only even-integer plateaus, unlike a spinless electron gas. Here, two Rashba layers that are degenerate but with opposite SOC due to inversion symmetry, hybridize with each other and create two doubly-degenerate bands with hidden spin texture. Correspondingly, two branches of Landau levels interact, resulting in significant suppression of spin splitting due to the balancing of intralayer SOC and interlayer hybridization. Furthermore, we show that breaking inversion symmetry restores the ordinary quantum Hall fluid by introducing spin-split Fermi surfaces. Our theory can apply to centrosymmetric materials with strong SOC, as demonstrated in a recent experiment on the two-dimensional semiconductor Bi$_2$O$_2$Se.","sentences":["Because spin-orbit coupling (SOC) is invisible in the band structure when inversion symmetry exists, whether spins are trivially degenerate or strongly coupled to momentum due to SOC is presumed to make little difference in transport measurements, such as magnetoresistance and quantum oscillations.","In this work, however, we show that hidden Rashba SOC in a centrosymmetric two-dimensional material can lead to the quantum Hall effect with only even-integer plateaus, unlike a spinless electron gas.","Here, two Rashba layers that are degenerate but with opposite SOC due to inversion symmetry, hybridize with each other and create two doubly-degenerate bands with hidden spin texture.","Correspondingly, two branches of Landau levels interact, resulting in significant suppression of spin splitting due to the balancing of intralayer SOC and interlayer hybridization.","Furthermore, we show that breaking inversion symmetry restores the ordinary quantum Hall fluid by introducing spin-split Fermi surfaces.","Our theory can apply to centrosymmetric materials with strong SOC, as demonstrated in a recent experiment on the two-dimensional semiconductor Bi$_2$O$_2$Se."],"url":"http://arxiv.org/abs/2406.03448v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 16:35:30","title":"Cycles of Thought: Measuring LLM Confidence through Stable Explanations","abstract":"In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs.","sentences":["In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction.","While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode.","Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models.","A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence.","We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer.","While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers.","We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets.","We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs."],"url":"http://arxiv.org/abs/2406.03441v1","category":"cs.CL"}
{"created":"2024-06-05 16:29:43","title":"Bayesian inference for scale mixtures of skew-normal linear models under the centered parameterization","abstract":"In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails. In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions. The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate. In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed. We explore a hierarchical representation and set up a MCMC scheme for parameter estimation. Furthermore, we developed residuals and influence analysis tools. A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution. The methodology is illustrated with the analysis of a real data set.","sentences":["In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails.","In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions.","The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate.","In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed.","We explore a hierarchical representation and set up a MCMC scheme for parameter estimation.","Furthermore, we developed residuals and influence analysis tools.","A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution.","The methodology is illustrated with the analysis of a real data set."],"url":"http://arxiv.org/abs/2406.03432v1","category":"stat.ME"}
{"created":"2024-06-05 16:29:03","title":"Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis","abstract":"Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.","sentences":["Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks.","However, the emergence of transformers has altered this paradigm due to their superior performance.","Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations.","However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation.","State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence.","Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models.","Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging.","Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context.","Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs.","Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field.","In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository."],"url":"http://arxiv.org/abs/2406.03430v1","category":"eess.IV"}
{"created":"2024-06-05 17:32:28","title":"MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization","abstract":"The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out. Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text. We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task. The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries. Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality.","sentences":["The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out.","Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text.","We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task.","The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries.","Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality."],"url":"http://arxiv.org/abs/2406.03479v1","category":"cs.CL"}
{"created":"2024-06-05 17:12:38","title":"Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach","abstract":"Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.","sentences":["Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns.","Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs.","However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal.","In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns.","To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes.","Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs."],"url":"http://arxiv.org/abs/2406.03464v1","category":"cs.LG"}
{"created":"2024-06-05 15:53:25","title":"Noisy Data Visualization using Functional Data Analysis","abstract":"Data visualization via dimensionality reduction is an important tool in exploratory data analysis. However, when the data are noisy, many existing methods fail to capture the underlying structure of the data. The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise. However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality. Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality. We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed. We then use our method to visualize EEG brain measurements of sleep activity.","sentences":["Data visualization via dimensionality reduction is an important tool in exploratory data analysis.","However, when the data are noisy, many existing methods fail to capture the underlying structure of the data.","The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise.","However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality.","Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality.","We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed.","We then use our method to visualize EEG brain measurements of sleep activity."],"url":"http://arxiv.org/abs/2406.03396v1","category":"cs.LG"}
{"created":"2024-06-05 15:44:54","title":"Gaussian Representation for Deformable Image Registration","abstract":"Deformable image registration (DIR) is a fundamental task in radiotherapy, with existing methods often struggling to balance computational efficiency, registration accuracy, and speed effectively. We introduce a novel DIR approach employing parametric 3D Gaussian control points achieving a better tradeoff. It provides an explicit and flexible representation for spatial deformation fields between 3D volumetric medical images, producing a displacement vector field (DVF) across all volumetric positions. The movement of individual voxels is derived using linear blend skinning (LBS) through localized interpolation of transformations associated with neighboring Gaussians. This interpolation strategy not only simplifies the determination of voxel motions but also acts as an effective regularization technique. Our approach incorporates a unified optimization process through backpropagation, enabling iterative learning of both the parameters of the 3D Gaussians and their transformations. Additionally, the density of Gaussians is adjusted adaptively during the learning phase to accommodate varying degrees of motion complexity. We validated our approach on the 4D-CT lung DIR-Lab and cardiac ACDC datasets, achieving an average target registration error (TRE) of 1.06 mm within a much-improved processing time of 2.43 seconds for the DIR-Lab dataset over existing methods, demonstrating significant advancements in both accuracy and efficiency.","sentences":["Deformable image registration (DIR) is a fundamental task in radiotherapy, with existing methods often struggling to balance computational efficiency, registration accuracy, and speed effectively.","We introduce a novel DIR approach employing parametric 3D Gaussian control points achieving a better tradeoff.","It provides an explicit and flexible representation for spatial deformation fields between 3D volumetric medical images, producing a displacement vector field (DVF) across all volumetric positions.","The movement of individual voxels is derived using linear blend skinning (LBS) through localized interpolation of transformations associated with neighboring Gaussians.","This interpolation strategy not only simplifies the determination of voxel motions but also acts as an effective regularization technique.","Our approach incorporates a unified optimization process through backpropagation, enabling iterative learning of both the parameters of the 3D Gaussians and their transformations.","Additionally, the density of Gaussians is adjusted adaptively during the learning phase to accommodate varying degrees of motion complexity.","We validated our approach on the 4D-CT lung DIR-Lab and cardiac ACDC datasets, achieving an average target registration error (TRE) of 1.06 mm within a much-improved processing time of 2.43 seconds for the DIR-Lab dataset over existing methods, demonstrating significant advancements in both accuracy and efficiency."],"url":"http://arxiv.org/abs/2406.03394v1","category":"cs.CV"}
{"created":"2024-06-05 15:31:43","title":"Log Parsing with Self-Generated In-Context Learning and Self-Correction","abstract":"Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin.","sentences":["Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis.","Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data.","The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing.","Consequently, several studies have proposed LLM-based log parsers.","However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing.","Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data.","To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction.","To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates.","In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data.","Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios.","Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin."],"url":"http://arxiv.org/abs/2406.03376v1","category":"cs.SE"}
{"created":"2024-06-05 15:24:20","title":"Posterior and variational inference for deep neural networks with heavy-tailed weights","abstract":"We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.","sentences":["We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random.","Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation.","We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces.","While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network.","We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support."],"url":"http://arxiv.org/abs/2406.03369v1","category":"stat.ML"}
{"created":"2024-06-05 15:14:40","title":"On determinantal point processes with nonsymmetric kernels","abstract":"Determinantal point processes (DPPs for short) are a class of repulsive point processes. They have found some statistical applications to model spatial point pattern datasets with repulsion between close points. In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric. While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties. In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels. We also generalize various common results on DPPs. We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks.","sentences":["Determinantal point processes (DPPs for short) are a class of repulsive point processes.","They have found some statistical applications to model spatial point pattern datasets with repulsion between close points.","In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric.","While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties.","In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels.","We also generalize various common results on DPPs.","We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks."],"url":"http://arxiv.org/abs/2406.03360v1","category":"math.ST"}
{"created":"2024-06-05 14:52:50","title":"Strength of Kitaev Interaction in Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$","abstract":"Kitaev spin liquid is proposed to be promisingly realized in low spin-orbit coupling $3d$ systems, represented by Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$. However, the existence of Kitaev interaction is still debatable among experiments, and obtaining the strength of Kitaev interaction from first-principles calculations is also challenging. Here, we report the state-dependent anisotropy of Kitaev interaction, based on which a convenient method is developed to rapidly determine the strength of Kitaev interaction. Applying such method and density functional theory calculations, it is found that Na$_3$Co$_2$SbO$_6$ with $3d^7$ configuration exhibits considerable ferromagnetic Kitaev interaction. Moreover, by further applying the symmetry-adapted cluster expansion method, a realistic spin model is determined for Na$_3$Ni$_2$BiO$_6$ with $3d^8$ configuration. Such model indicates negligible small Kitaev interaction, but it predicts many properties, such as ground states and field effects, which are well consistent with measurements. Furthermore, we demonstrate that the heavy elements, Sb or Bi, located at the hollow sites of honeycomb lattice, do not contribute to emergence of Kitaev interaction through proximity, contradictory to common belief. The presently developed anisotropy method will be beneficial not only for computations but also for measurements.","sentences":["Kitaev spin liquid is proposed to be promisingly realized in low spin-orbit coupling $3d$ systems, represented by Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$.","However, the existence of Kitaev interaction is still debatable among experiments, and obtaining the strength of Kitaev interaction from first-principles calculations is also challenging.","Here, we report the state-dependent anisotropy of Kitaev interaction, based on which a convenient method is developed to rapidly determine the strength of Kitaev interaction.","Applying such method and density functional theory calculations, it is found that Na$_3$Co$_2$SbO$_6$ with $3d^7$ configuration exhibits considerable ferromagnetic Kitaev interaction.","Moreover, by further applying the symmetry-adapted cluster expansion method, a realistic spin model is determined for Na$_3$Ni$_2$BiO$_6$ with $3d^8$ configuration.","Such model indicates negligible small Kitaev interaction, but it predicts many properties, such as ground states and field effects, which are well consistent with measurements.","Furthermore, we demonstrate that the heavy elements, Sb or Bi, located at the hollow sites of honeycomb lattice, do not contribute to emergence of Kitaev interaction through proximity, contradictory to common belief.","The presently developed anisotropy method will be beneficial not only for computations but also for measurements."],"url":"http://arxiv.org/abs/2406.03338v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 14:52:43","title":"Identifying latent state transition in non-linear dynamical systems","abstract":"This work aims to improve generalization and interpretability of dynamical systems by recovering the underlying lower-dimensional latent states and their time evolutions. Previous work on disentangled representation learning within the realm of dynamical systems focused on the latent states, possibly with linear transition approximations. As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior. Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present. We introduce a practical algorithm based on variational auto-encoders and empirically demonstrate in realistic synthetic settings that we can (i) recover latent state dynamics with high accuracy, (ii) correspondingly achieve high future prediction accuracy, and (iii) adapt fast to new environments.","sentences":["This work aims to improve generalization and interpretability of dynamical systems by recovering the underlying lower-dimensional latent states and their time evolutions.","Previous work on disentangled representation learning within the realm of dynamical systems focused on the latent states, possibly with linear transition approximations.","As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior.","Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present.","We introduce a practical algorithm based on variational auto-encoders and empirically demonstrate in realistic synthetic settings that we can (i) recover latent state dynamics with high accuracy, (ii) correspondingly achieve high future prediction accuracy, and (iii) adapt fast to new environments."],"url":"http://arxiv.org/abs/2406.03337v2","category":"cs.LG"}
{"created":"2024-06-05 14:16:47","title":"Heisenberg-limited adaptive gradient estimation for multiple observables","abstract":"In quantum mechanics, measuring the expectation value of a general observable has an inherent statistical uncertainty that is quantified by variance or mean squared error of measurement outcome. While the uncertainty can be reduced by averaging several samples, the number of samples should be minimized when each sample is very costly. This is especially the case for fault-tolerant quantum computing that involves measurement of multiple observables of non-trivial states in large quantum systems that exceed the capabilities of classical computers. In this work, we provide an adaptive quantum algorithm for estimating the expectation values of $M$ general observables within root mean squared error $\\varepsilon$ simultaneously, using $\\mathcal{O}(\\varepsilon^{-1}\\sqrt{M}\\log M)$ queries to a state preparation oracle of a target state. This remarkably achieves the scaling of Heisenberg limit $1/\\varepsilon$, a fundamental bound on the estimation precision in terms of mean squared error, together with the sublinear scaling of the number of observables $M$. The proposed method is an adaptive version of the quantum gradient estimation algorithm and has a resource-efficient implementation due to its adaptiveness. Specifically, the space overhead in the proposed method is $\\mathcal{O}(M)$ which is independent from the estimation precision $\\varepsilon$ unlike non-iterative algorithms. In addition, our method can avoid the numerical instability problem for constructing quantum circuits in a large-scale task (e.g., $\\varepsilon\\ll 1$ in our case), which appears in the actual implementation of many algorithms relying on quantum signal processing techniques. Our method paves a new way to precisely understand and predict various physical properties in complicated quantum systems using quantum computers.","sentences":["In quantum mechanics, measuring the expectation value of a general observable has an inherent statistical uncertainty that is quantified by variance or mean squared error of measurement outcome.","While the uncertainty can be reduced by averaging several samples, the number of samples should be minimized when each sample is very costly.","This is especially the case for fault-tolerant quantum computing that involves measurement of multiple observables of non-trivial states in large quantum systems that exceed the capabilities of classical computers.","In this work, we provide an adaptive quantum algorithm for estimating the expectation values of $M$ general observables within root mean squared error $\\varepsilon$ simultaneously, using $\\mathcal{O}(\\varepsilon^{-1}\\sqrt{M}\\log M)$ queries to a state preparation oracle of a target state.","This remarkably achieves the scaling of Heisenberg limit $1/\\varepsilon$, a fundamental bound on the estimation precision in terms of mean squared error, together with the sublinear scaling of the number of observables $M$. The proposed method is an adaptive version of the quantum gradient estimation algorithm and has a resource-efficient implementation due to its adaptiveness.","Specifically, the space overhead in the proposed method is $\\mathcal{O}(M)$ which is independent from the estimation precision $\\varepsilon$ unlike non-iterative algorithms.","In addition, our method can avoid the numerical instability problem for constructing quantum circuits in a large-scale task (e.g., $\\varepsilon\\ll 1$ in our case), which appears in the actual implementation of many algorithms relying on quantum signal processing techniques.","Our method paves a new way to precisely understand and predict various physical properties in complicated quantum systems using quantum computers."],"url":"http://arxiv.org/abs/2406.03306v1","category":"quant-ph"}
{"created":"2024-06-05 14:13:38","title":"Learning Visual Prompts for Guiding the Attention of Vision Transformers","abstract":"Visual prompting infuses visual information into the input image to adapt models toward specific predictions and tasks. Recently, manually crafted markers such as red circles are shown to guide the model to attend to a target region on the image. However, these markers only work on models trained with data containing those markers. Moreover, finding these prompts requires guesswork or prior knowledge of the domain on which the model is trained. This work circumvents manual design constraints by proposing to learn the visual prompts for guiding the attention of vision transformers. The learned visual prompt, added to any input image would redirect the attention of the pre-trained vision transformer to its spatial location on the image. Specifically, the prompt is learned in a self-supervised manner without requiring annotations and without fine-tuning the vision transformer. Our experiments demonstrate the effectiveness of the proposed optimization-based visual prompting strategy across various pre-trained vision encoders.","sentences":["Visual prompting infuses visual information into the input image to adapt models toward specific predictions and tasks.","Recently, manually crafted markers such as red circles are shown to guide the model to attend to a target region on the image.","However, these markers only work on models trained with data containing those markers.","Moreover, finding these prompts requires guesswork or prior knowledge of the domain on which the model is trained.","This work circumvents manual design constraints by proposing to learn the visual prompts for guiding the attention of vision transformers.","The learned visual prompt, added to any input image would redirect the attention of the pre-trained vision transformer to its spatial location on the image.","Specifically, the prompt is learned in a self-supervised manner without requiring annotations and without fine-tuning the vision transformer.","Our experiments demonstrate the effectiveness of the proposed optimization-based visual prompting strategy across various pre-trained vision encoders."],"url":"http://arxiv.org/abs/2406.03303v1","category":"cs.CV"}
{"created":"2024-06-05 14:08:13","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration","abstract":"Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework named L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it. The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set. The second-level graph is constructed as a factor graph. By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem. We conduct qualitative and quantitative experiments to demonstrate that the proposed method exhibits superiority over competitors in four aspects: registration accuracy, instance reconstruction quality, localization accuracy, and robustness to the degraded scene. To benefit the community, we open-source our method and dataset at https://github.com/yorklyb/LiDAR-SFM.","sentences":["Point cloud registration is a prerequisite for many applications in computer vision and robotics.","Most existing methods focus on pairwise registration of two point clouds with high overlap.","Although there have been some methods for low overlap cases, they struggle in degraded scenarios.","This paper introduces a novel framework named L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers.","We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment.","We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically.","Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it.","The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set.","The second-level graph is constructed as a factor graph.","By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem.","We conduct qualitative and quantitative experiments to demonstrate that the proposed method exhibits superiority over competitors in four aspects: registration accuracy, instance reconstruction quality, localization accuracy, and robustness to the degraded scene.","To benefit the community, we open-source our method and dataset at https://github.com/yorklyb/LiDAR-SFM."],"url":"http://arxiv.org/abs/2406.03298v1","category":"cs.CV"}
{"created":"2024-06-05 13:46:50","title":"Relative Entropy for the Numerical Diffusive Limit of the Linear Jin-Xin System","abstract":"This paper deals with the diffusive limit of the Jin and Xin model and its approximation by an asymptotic preserving finite volume scheme. At the continuous level, we determine a convergence rate to the diffusive limit by means of a relative entropy method. Considering a semi-discrete approximation (discrete in space and continuous in time), we adapt the method to this semi-discrete framework and establish that the approximated solutions converge towards the discrete convection-diffusion limit with the same convergence rate.","sentences":["This paper deals with the diffusive limit of the Jin and Xin model and its approximation by an asymptotic preserving finite volume scheme.","At the continuous level, we determine a convergence rate to the diffusive limit by means of a relative entropy method.","Considering a semi-discrete approximation (discrete in space and continuous in time), we adapt the method to this semi-discrete framework and establish that the approximated solutions converge towards the discrete convection-diffusion limit with the same convergence rate."],"url":"http://arxiv.org/abs/2406.03268v1","category":"math.NA"}
{"created":"2024-06-05 13:22:09","title":"Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake Audio Detection","abstract":"The generalization of Fake Audio Detection (FAD) is critical due to the emergence of new spoofing techniques. Traditional FAD methods often focus solely on distinguishing between genuine and known spoofed audio. We propose a Genuine-Focused Learning (GFL) framework guided, aiming for highly generalized FAD, called GFL-FAD. This method incorporates a Counterfactual Reasoning Enhanced Representation (CRER) based on audio reconstruction using the Mask AutoEncoder (MAE) architecture to accurately model genuine audio features. To reduce the influence of spoofed audio during training, we introduce a genuine audio reconstruction loss, maintaining the focus on learning genuine data features. In addition, content-related bottleneck (BN) features are extracted from the MAE to supplement the knowledge of the original audio. These BN features are adaptively fused with CRER to further improve robustness. Our method achieves state-of-the-art performance with an EER of 0.25% on ASVspoof2019 LA.","sentences":["The generalization of Fake Audio Detection (FAD) is critical due to the emergence of new spoofing techniques.","Traditional FAD methods often focus solely on distinguishing between genuine and known spoofed audio.","We propose a Genuine-Focused Learning (GFL) framework guided, aiming for highly generalized FAD, called GFL-FAD.","This method incorporates a Counterfactual Reasoning Enhanced Representation (CRER) based on audio reconstruction using the Mask AutoEncoder (MAE) architecture to accurately model genuine audio features.","To reduce the influence of spoofed audio during training, we introduce a genuine audio reconstruction loss, maintaining the focus on learning genuine data features.","In addition, content-related bottleneck (BN) features are extracted from the MAE to supplement the knowledge of the original audio.","These BN features are adaptively fused with CRER to further improve robustness.","Our method achieves state-of-the-art performance with an EER of 0.25% on ASVspoof2019 LA."],"url":"http://arxiv.org/abs/2406.03247v1","category":"cs.SD"}
{"created":"2024-06-05 13:05:42","title":"Reference Channel Selection by Multi-Channel Masking for End-to-End Multi-Channel Speech Enhancement","abstract":"In end-to-end multi-channel speech enhancement, the traditional approach of designating one microphone signal as the reference for processing may not always yield optimal results. The limitation is particularly in scenarios with large distributed microphone arrays with varying speaker-to-microphone distances or compact, highly directional microphone arrays where speaker or microphone positions change over time. Current mask-based methods often fix the reference channel during training, which makes it not possible to adaptively select the reference channel for optimal performance. To address this problem, we introduce an adaptive approach for selecting the optimal reference channel. Our method leverages a multi-channel masking-based scheme, where multiple masked signals are combined to generate a single-channel output signal. This enhanced signal is then used for loss calculation, while the reference clean speech is adjusted based on the highest scale-invariant signal-to-distortion ratio (SI-SDR). The experimental results on the Spear challenge simulated dataset D4 demonstrate the superiority of our proposed method over the conventional approach of using a fixed reference channel with single-channel masking","sentences":["In end-to-end multi-channel speech enhancement, the traditional approach of designating one microphone signal as the reference for processing may not always yield optimal results.","The limitation is particularly in scenarios with large distributed microphone arrays with varying speaker-to-microphone distances or compact, highly directional microphone arrays where speaker or microphone positions change over time.","Current mask-based methods often fix the reference channel during training, which makes it not possible to adaptively select the reference channel for optimal performance.","To address this problem, we introduce an adaptive approach for selecting the optimal reference channel.","Our method leverages a multi-channel masking-based scheme, where multiple masked signals are combined to generate a single-channel output signal.","This enhanced signal is then used for loss calculation, while the reference clean speech is adjusted based on the highest scale-invariant signal-to-distortion ratio (SI-SDR).","The experimental results on the Spear challenge simulated dataset D4 demonstrate the superiority of our proposed method over the conventional approach of using a fixed reference channel with single-channel masking"],"url":"http://arxiv.org/abs/2406.03228v1","category":"eess.AS"}
{"created":"2024-06-05 13:01:55","title":"Exponentially Stable Projector-based Control of Lagrangian Systems with Gaussian Processes","abstract":"Designing accurate yet robust tracking controllers with tight performance guarantees for Lagrangian systems is challenging due to nonlinear modeling uncertainties and conservative stability criteria. This article proposes a structure-preserving projector-based tracking control law for uncertain Euler-Lagrange (EL) systems using physically consistent Lagrangian-Gaussian Processes (L-GPs). We leverage the uncertainty quantification of the L-GP for adaptive feedforward-feedback balancing. In particular, an accurate probabilistic guarantee for exponential stability is derived by leveraging matrix analysis results and contraction theory, where the benefit of the proposed controller is proven and shown in the closed-form expressions for convergence rate and radius. Extensive numerical simulations not only demonstrate the controller's efficacy based on a two-link and a soft robotic manipulator but also all theoretical results are explicitly analyzed and validated.","sentences":["Designing accurate yet robust tracking controllers with tight performance guarantees for Lagrangian systems is challenging due to nonlinear modeling uncertainties and conservative stability criteria.","This article proposes a structure-preserving projector-based tracking control law for uncertain Euler-Lagrange (EL) systems using physically consistent Lagrangian-Gaussian Processes (L-GPs).","We leverage the uncertainty quantification of the L-GP for adaptive feedforward-feedback balancing.","In particular, an accurate probabilistic guarantee for exponential stability is derived by leveraging matrix analysis results and contraction theory, where the benefit of the proposed controller is proven and shown in the closed-form expressions for convergence rate and radius.","Extensive numerical simulations not only demonstrate the controller's efficacy based on a two-link and a soft robotic manipulator but also all theoretical results are explicitly analyzed and validated."],"url":"http://arxiv.org/abs/2406.03224v1","category":"eess.SY"}
{"created":"2024-06-05 12:45:25","title":"Text-like Encoding of Collaborative Information in Large Language Models for Recommendation","abstract":"When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs' latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences -- a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.","sentences":["When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information.","Existing methods achieve this by learning collaborative embeddings in LLMs' latent space from scratch or by mapping from external models.","However, they fail to represent the information in a text-like format, which may not align optimally with LLMs.","To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding.","BinLLM converts collaborative embeddings from external models into binary sequences -- a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs.","Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths.","Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance.","We release our code at https://github.com/zyang1580/BinLLM."],"url":"http://arxiv.org/abs/2406.03210v1","category":"cs.IR"}
{"created":"2024-06-05 12:33:11","title":"Adaptive Distance Functions via Kelvin Transformation","abstract":"The term safety in robotics is often understood as a synonym for avoidance. Although this perspective has led to progress in path planning and reactive control, a generalization of this perspective is necessary to include task semantics relevant to contact-rich manipulation tasks, especially during teleoperation and to ensure the safety of learned policies.   We introduce the semantics-aware distance function and a corresponding computational method based on the Kelvin Transformation. The semantics-aware distance generalizes signed distance functions by allowing the zero level set to lie inside of the object in regions where contact is allowed, effectively incorporating task semantics -- such as object affordances and user intent -- in an adaptive implicit representation of safe sets. In validation experiments we show the capability of our method to adapt to time-varying semantic information, and to perform queries in sub-microsecond, enabling applications in reinforcement learning, trajectory optimization, and motion planning.","sentences":["The term safety in robotics is often understood as a synonym for avoidance.","Although this perspective has led to progress in path planning and reactive control, a generalization of this perspective is necessary to include task semantics relevant to contact-rich manipulation tasks, especially during teleoperation and to ensure the safety of learned policies.   ","We introduce the semantics-aware distance function and a corresponding computational method based on the Kelvin Transformation.","The semantics-aware distance generalizes signed distance functions by allowing the zero level set to lie inside of the object in regions where contact is allowed, effectively incorporating task semantics -- such as object affordances and user intent -- in an adaptive implicit representation of safe sets.","In validation experiments we show the capability of our method to adapt to time-varying semantic information, and to perform queries in sub-microsecond, enabling applications in reinforcement learning, trajectory optimization, and motion planning."],"url":"http://arxiv.org/abs/2406.03200v1","category":"cs.RO"}
{"created":"2024-06-05 12:15:22","title":"Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion","abstract":"Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. Project page: https://costwen.github.io/Ouroboros3D/","sentences":["Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction.","However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results.","We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process.","In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference.","During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions.","The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.","Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase.","Project page: https://costwen.github.io/Ouroboros3D/"],"url":"http://arxiv.org/abs/2406.03184v1","category":"cs.CV"}
{"created":"2024-06-05 12:08:01","title":"FAPNet: An Effective Frequency Adaptive Point-based Eye Tracker","abstract":"Eye tracking is crucial for human-computer interaction in different domains. Conventional cameras encounter challenges such as power consumption and image quality during different eye movements, prompting the need for advanced solutions with ultra-fast, low-power, and accurate eye trackers. Event cameras, fundamentally designed to capture information about moving objects, exhibit low power consumption and high temporal resolution. This positions them as an alternative to traditional cameras in the realm of eye tracking. Nevertheless, existing event-based eye tracking networks neglect the pivotal sparse and fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we utilize Point Cloud as the event representation to harness the high temporal resolution and sparse characteristics of events in eye tracking tasks. We rethink the point-based architecture PEPNet with preprocessing the long-term relationships between samples, leading to the innovative design of FAPNet. A frequency adaptive mechanism is designed to realize adaptive tracking according to the speed of the pupil movement and the Inter Sample LSTM module is introduced to utilize the temporal correlation between samples. In the Event-based Eye Tracking Challenge, we utilize vanilla PEPNet, which is the former work to achieve the $p_{10}$ accuracy of 97.95\\%. On the SEET synthetic dataset, FAPNet can achieve state-of-the-art while consuming merely 10\\% of the PEPNet's computational resources. Notably, the computational demand of FAPNet is independent of the sensor's spatial resolution, enhancing its applicability on resource-limited edge devices.","sentences":["Eye tracking is crucial for human-computer interaction in different domains.","Conventional cameras encounter challenges such as power consumption and image quality during different eye movements, prompting the need for advanced solutions with ultra-fast, low-power, and accurate eye trackers.","Event cameras, fundamentally designed to capture information about moving objects, exhibit low power consumption and high temporal resolution.","This positions them as an alternative to traditional cameras in the realm of eye tracking.","Nevertheless, existing event-based eye tracking networks neglect the pivotal sparse and fine-grained temporal information in events, resulting in unsatisfactory performance.","Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices.","In this paper, we utilize Point Cloud as the event representation to harness the high temporal resolution and sparse characteristics of events in eye tracking tasks.","We rethink the point-based architecture PEPNet with preprocessing the long-term relationships between samples, leading to the innovative design of FAPNet.","A frequency adaptive mechanism is designed to realize adaptive tracking according to the speed of the pupil movement and the Inter Sample LSTM module is introduced to utilize the temporal correlation between samples.","In the Event-based Eye Tracking Challenge, we utilize vanilla PEPNet, which is the former work to achieve the $p_{10}$ accuracy of 97.95\\%.","On the SEET synthetic dataset, FAPNet can achieve state-of-the-art while consuming merely 10\\% of the PEPNet's computational resources.","Notably, the computational demand of FAPNet is independent of the sensor's spatial resolution, enhancing its applicability on resource-limited edge devices."],"url":"http://arxiv.org/abs/2406.03177v1","category":"cs.CV"}
{"created":"2024-06-05 11:56:54","title":"Topological Neural Networks go Persistent, Equivariant, and Continuous","abstract":"Topological Neural Networks (TNNs) incorporate higher-order relational information beyond pairwise interactions, enabling richer representations than Graph Neural Networks (GNNs). Concurrently, topological descriptors based on persistent homology (PH) are being increasingly employed to augment the GNNs. We investigate the benefits of integrating these two paradigms. Specifically, we introduce TopNets as a broad framework that subsumes and unifies various methods in the intersection of GNNs/TNNs and PH such as (generalizations of) RePHINE and TOGL. TopNets can also be readily adapted to handle (symmetries in) geometric complexes, extending the scope of TNNs and PH to spatial settings. Theoretically, we show that PH descriptors can provably enhance the expressivity of simplicial message-passing networks. Empirically, (continuous and E(n)-equivariant extensions of) TopNets achieve strong performance across diverse tasks, including antibody design, molecular dynamics simulation, and drug property prediction.","sentences":["Topological Neural Networks (TNNs) incorporate higher-order relational information beyond pairwise interactions, enabling richer representations than Graph Neural Networks (GNNs).","Concurrently, topological descriptors based on persistent homology (PH) are being increasingly employed to augment the GNNs.","We investigate the benefits of integrating these two paradigms.","Specifically, we introduce TopNets as a broad framework that subsumes and unifies various methods in the intersection of GNNs/TNNs and PH such as (generalizations of) RePHINE and TOGL.","TopNets can also be readily adapted to handle (symmetries in) geometric complexes, extending the scope of TNNs and PH to spatial settings.","Theoretically, we show that PH descriptors can provably enhance the expressivity of simplicial message-passing networks.","Empirically, (continuous and E(n)-equivariant extensions of) TopNets achieve strong performance across diverse tasks, including antibody design, molecular dynamics simulation, and drug property prediction."],"url":"http://arxiv.org/abs/2406.03164v1","category":"cs.LG"}
{"created":"2024-06-05 11:53:28","title":"Foundation Models for Geophysics: Reviews and Perspectives","abstract":"Recently, large models, or foundation models have demonstrated outstanding performance and have been applied in a variety of disciplines, such as chemistry, biology, economics, etc. Foundation models, trained on vast amounts of data, can be adapted to a wide range of use cases. The emergence of foundation models has a significant impact on the research paradigms in these fields. Geophysics is a scientific field dedicated to exploring and understanding the Earth's structures and states through the application of physical principles and the analysis of multimodal geophysical data. In the field of geophysics, the processing and interpretation of geophysical data are characterized by three primary features: extensive data volume, multimodality, and dependence on experience. These characteristics provide a suitable environment as well as challenges for the development and breakthrough of foundation models in the field of geophysics. In this perspective, we discuss the potential applications and research directions of geophysical foundation models (GeoFMs), exploring new research paradigms in geophysics in the era of foundation models. Exploration geophysics is the main focus, while the development of foundation models in remote sensing, seismology, and other related sub-disciplines in geophysics is also discussed. In the meantime, we also propose two strategies for constructing GeoFMs and discuss challenges that may arise during the process of development.","sentences":["Recently, large models, or foundation models have demonstrated outstanding performance and have been applied in a variety of disciplines, such as chemistry, biology, economics, etc.","Foundation models, trained on vast amounts of data, can be adapted to a wide range of use cases.","The emergence of foundation models has a significant impact on the research paradigms in these fields.","Geophysics is a scientific field dedicated to exploring and understanding the Earth's structures and states through the application of physical principles and the analysis of multimodal geophysical data.","In the field of geophysics, the processing and interpretation of geophysical data are characterized by three primary features: extensive data volume, multimodality, and dependence on experience.","These characteristics provide a suitable environment as well as challenges for the development and breakthrough of foundation models in the field of geophysics.","In this perspective, we discuss the potential applications and research directions of geophysical foundation models (GeoFMs), exploring new research paradigms in geophysics in the era of foundation models.","Exploration geophysics is the main focus, while the development of foundation models in remote sensing, seismology, and other related sub-disciplines in geophysics is also discussed.","In the meantime, we also propose two strategies for constructing GeoFMs and discuss challenges that may arise during the process of development."],"url":"http://arxiv.org/abs/2406.03163v1","category":"physics.geo-ph"}
{"created":"2024-06-05 11:15:43","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","abstract":"Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model. The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples. In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation. Motivated by this finding, we design a new framework for VR called sample-specific multi-channel masks (SMM). Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask. Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods. We also empirically demonstrate its performance gain on both ResNet and ViT. The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks. Our code is available at https://github.com/tmlr-group/SMM.","sentences":["Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model.","The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples.","In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation.","Motivated by this finding, we design a new framework for VR called sample-specific multi-channel masks (SMM).","Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask.","Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods.","We also empirically demonstrate its performance gain on both ResNet and ViT.","The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks.","Our code is available at https://github.com/tmlr-group/SMM."],"url":"http://arxiv.org/abs/2406.03150v1","category":"cs.LG"}
{"created":"2024-06-05 10:58:15","title":"ZeroPur: Succinct Training-Free Adversarial Purification","abstract":"Adversarial purification is a kind of defense technique that can defend various unseen adversarial attacks without modifying the victim classifier. Existing methods often depend on external generative models or cooperation between auxiliary functions and victim classifiers. However, retraining generative models, auxiliary functions, or victim classifiers relies on the domain of the fine-tuned dataset and is computation-consuming. In this work, we suppose that adversarial images are outliers of the natural image manifold and the purification process can be considered as returning them to this manifold. Following this assumption, we present a simple adversarial purification method without further training to purify adversarial images, called ZeroPur. ZeroPur contains two steps: given an adversarial example, Guided Shift obtains the shifted embedding of the adversarial example by the guidance of its blurred counterparts; after that, Adaptive Projection constructs a directional vector by this shifted embedding to provide momentum, projecting adversarial images onto the manifold adaptively. ZeroPur is independent of external models and requires no retraining of victim classifiers or auxiliary functions, relying solely on victim classifiers themselves to achieve purification. Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet, WideResNet) demonstrate that our method achieves state-of-the-art robust performance. The code will be publicly available.","sentences":["Adversarial purification is a kind of defense technique that can defend various unseen adversarial attacks without modifying the victim classifier.","Existing methods often depend on external generative models or cooperation between auxiliary functions and victim classifiers.","However, retraining generative models, auxiliary functions, or victim classifiers relies on the domain of the fine-tuned dataset and is computation-consuming.","In this work, we suppose that adversarial images are outliers of the natural image manifold and the purification process can be considered as returning them to this manifold.","Following this assumption, we present a simple adversarial purification method without further training to purify adversarial images, called ZeroPur.","ZeroPur contains two steps: given an adversarial example, Guided Shift obtains the shifted embedding of the adversarial example by the guidance of its blurred counterparts; after that, Adaptive Projection constructs a directional vector by this shifted embedding to provide momentum, projecting adversarial images onto the manifold adaptively.","ZeroPur is independent of external models and requires no retraining of victim classifiers or auxiliary functions, relying solely on victim classifiers themselves to achieve purification.","Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet, WideResNet) demonstrate that our method achieves state-of-the-art robust performance.","The code will be publicly available."],"url":"http://arxiv.org/abs/2406.03143v1","category":"cs.CV"}
{"created":"2024-06-05 10:51:17","title":"Continual Traffic Forecasting via Mixture of Experts","abstract":"The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time. Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient. To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (TFMoE) for traffic forecasting under evolving networks. The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group. This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting. Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of TFMoE. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks.","sentences":["The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time.","Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient.","To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (TFMoE) for traffic forecasting under evolving networks.","The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group.","This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting.","Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of TFMoE. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks."],"url":"http://arxiv.org/abs/2406.03140v1","category":"cs.LG"}
{"created":"2024-06-05 10:44:08","title":"Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models","abstract":"We study the computational limits of Low-Rank Adaptation (LoRA) update for finetuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term, assuming the Strong Exponential Time Hypothesis (SETH). For the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $\\mathbf{X}$, pretrained weights $\\mathbf{W^\\star}$, and adapter matrices $\\alpha \\mathbf{B} \\mathbf{A} / r$. Specifically, we derive a shared upper bound threshold for such norms and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold. For the latter, we prove the existence of nearly linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations. To showcase our theory, we consider two practical scenarios: partial (e.g., only $\\mathbf{W}_V$ and $\\mathbf{W}_Q$) and full adaptations (e.g., $\\mathbf{W}_Q$, $\\mathbf{W}_V$, and $\\mathbf{W}_K$) of weights in attention heads.","sentences":["We study the computational limits of Low-Rank Adaptation (LoRA) update for finetuning transformer-based models using fine-grained complexity theory.","Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup.","This allows us to (i) identify a phase transition behavior and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term, assuming the Strong Exponential Time Hypothesis (SETH).","For the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $\\mathbf{X}$, pretrained weights $\\mathbf{W^\\star}$, and adapter matrices $\\alpha \\mathbf{B} \\mathbf{A} / r$.","Specifically, we derive a shared upper bound threshold for such norms and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold.","For the latter, we prove the existence of nearly linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations.","To showcase our theory, we consider two practical scenarios: partial (e.g., only $\\mathbf{W}_V$ and $\\mathbf{W}_Q$) and full adaptations (e.g., $\\mathbf{W}_Q$, $\\mathbf{W}_V$, and $\\mathbf{W}_K$) of weights in attention heads."],"url":"http://arxiv.org/abs/2406.03136v1","category":"cs.LG"}
{"created":"2024-06-05 10:02:56","title":"Singing Voice Graph Modeling for SingFake Detection","abstract":"Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice. Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization. To bridge the gap, we present a groundbreaking SingGraph model. The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance. Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%.","sentences":["Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice.","Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization.","To bridge the gap, we present a groundbreaking SingGraph model.","The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics.","Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance.","Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%."],"url":"http://arxiv.org/abs/2406.03111v1","category":"eess.AS"}
{"created":"2024-06-05 09:54:53","title":"Heavy Particle Clustering in Inertial Subrange of High--Reynolds Number Turbulence","abstract":"Direct numerical simulation of homogeneous isotropic turbulence shows pronounced clustering of inertial particles in the inertial subrange at high Reynolds number, in addition to the clustering typically observed in the near dissipation range. The clustering in the inertial subrange is characterized by the bump in the particle number density spectra and is due to modulation of preferential concentration. The number density spectrum can be modeled by a rational function of the scale-dependent Stokes number.","sentences":["Direct numerical simulation of homogeneous isotropic turbulence shows pronounced clustering of inertial particles in the inertial subrange at high Reynolds number, in addition to the clustering typically observed in the near dissipation range.","The clustering in the inertial subrange is characterized by the bump in the particle number density spectra and is due to modulation of preferential concentration.","The number density spectrum can be modeled by a rational function of the scale-dependent Stokes number."],"url":"http://arxiv.org/abs/2406.03107v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 09:45:26","title":"DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays","abstract":"Classic reinforcement learning (RL) frequently confronts challenges in tasks involving delays, which cause a mismatch between received observations and subsequent actions, thereby deviating from the Markov assumption. Existing methods usually tackle this issue with end-to-end solutions using state augmentation. However, these black-box approaches often involve incomprehensible processes and redundant information in the information states, causing instability and potentially undermining the overall performance. To alleviate the delay challenges in RL, we propose $\\textbf{DEER (Delay-resilient Encoder-Enhanced RL)}$, a framework designed to effectively enhance the interpretability and address the random delay issues. DEER employs a pretrained encoder to map delayed states, along with their variable-length past action sequences resulting from different delays, into hidden states, which is trained on delay-free environment datasets. In a variety of delayed scenarios, the trained encoder can seamlessly integrate with standard RL algorithms without requiring additional modifications and enhance the delay-solving capability by simply adapting the input dimension of the original algorithms. We evaluate DEER through extensive experiments on Gym and Mujoco environments. The results confirm that DEER is superior to state-of-the-art RL algorithms in both constant and random delay settings.","sentences":["Classic reinforcement learning (RL) frequently confronts challenges in tasks involving delays, which cause a mismatch between received observations and subsequent actions, thereby deviating from the Markov assumption.","Existing methods usually tackle this issue with end-to-end solutions using state augmentation.","However, these black-box approaches often involve incomprehensible processes and redundant information in the information states, causing instability and potentially undermining the overall performance.","To alleviate the delay challenges in RL, we propose $\\textbf{DEER (Delay-resilient Encoder-Enhanced RL)}$, a framework designed to effectively enhance the interpretability and address the random delay issues.","DEER employs a pretrained encoder to map delayed states, along with their variable-length past action sequences resulting from different delays, into hidden states, which is trained on delay-free environment datasets.","In a variety of delayed scenarios, the trained encoder can seamlessly integrate with standard RL algorithms without requiring additional modifications and enhance the delay-solving capability by simply adapting the input dimension of the original algorithms.","We evaluate DEER through extensive experiments on Gym and Mujoco environments.","The results confirm that DEER is superior to state-of-the-art RL algorithms in both constant and random delay settings."],"url":"http://arxiv.org/abs/2406.03102v1","category":"cs.LG"}
{"created":"2024-06-05 09:22:19","title":"Task-Oriented Wireless Communications for Collaborative Perception in Intelligent Unmanned Systems","abstract":"Collaborative Perception (CP) has shown great potential to achieve more holistic and reliable environmental perception in intelligent unmanned systems (IUSs). However, implementing CP still faces key challenges due to the characteristics of the CP task and the dynamics of wireless channels. In this article, a task-oriented wireless communication framework is proposed to jointly optimize the communication scheme and the CP procedure. We first propose channel-adaptive compression and robust fusion approaches to extract and exploit the most valuable semantic information under wireless communication constraints. We then propose a task-oriented distributed scheduling algorithm to identify the best collaborators for CP under dynamic environments. The main idea is learning while scheduling, where the collaboration utility is effectively learned with low computation and communication overhead. Case studies are carried out in connected autonomous driving scenarios to verify the proposed framework. Finally, we identify several future research directions.","sentences":["Collaborative Perception (CP) has shown great potential to achieve more holistic and reliable environmental perception in intelligent unmanned systems (IUSs).","However, implementing CP still faces key challenges due to the characteristics of the CP task and the dynamics of wireless channels.","In this article, a task-oriented wireless communication framework is proposed to jointly optimize the communication scheme and the CP procedure.","We first propose channel-adaptive compression and robust fusion approaches to extract and exploit the most valuable semantic information under wireless communication constraints.","We then propose a task-oriented distributed scheduling algorithm to identify the best collaborators for CP under dynamic environments.","The main idea is learning while scheduling, where the collaboration utility is effectively learned with low computation and communication overhead.","Case studies are carried out in connected autonomous driving scenarios to verify the proposed framework.","Finally, we identify several future research directions."],"url":"http://arxiv.org/abs/2406.03086v1","category":"cs.MA"}
{"created":"2024-06-05 09:09:32","title":"Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud?","abstract":"Recent advances in the field of large language models (LLMs), particularly the ChatGPT family, have given rise to a powerful and versatile machine interlocutor, packed with knowledge and challenging our understanding of learning. This interlocutor is a double-edged sword: it can be harnessed for a wide variety of beneficial tasks, but it can also be used to cause harm. This study explores the complicated interaction between ChatGPT and the growing problem of cryptocurrency fraud. Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem. Based on our categorization of cryptocurrency frauds, we show how to influence outputs, bypass ethical terms, and achieve specific fraud goals by manipulating ChatGPT prompts. Furthermore, our findings emphasize the importance of realizing that ChatGPT could be a valuable instructor even for novice fraudsters, as well as understanding and safely deploying complex language models, particularly in the context of cryptocurrency frauds. Finally, our study underlines the importance of using LLMs responsibly and ethically in the digital currency sector, identifying potential risks and resolving ethical issues. It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT.","sentences":["Recent advances in the field of large language models (LLMs), particularly the ChatGPT family, have given rise to a powerful and versatile machine interlocutor, packed with knowledge and challenging our understanding of learning.","This interlocutor is a double-edged sword: it can be harnessed for a wide variety of beneficial tasks, but it can also be used to cause harm.","This study explores the complicated interaction between ChatGPT and the growing problem of cryptocurrency fraud.","Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem.","Based on our categorization of cryptocurrency frauds, we show how to influence outputs, bypass ethical terms, and achieve specific fraud goals by manipulating ChatGPT prompts.","Furthermore, our findings emphasize the importance of realizing that ChatGPT could be a valuable instructor even for novice fraudsters, as well as understanding and safely deploying complex language models, particularly in the context of cryptocurrency frauds.","Finally, our study underlines the importance of using LLMs responsibly and ethically in the digital currency sector, identifying potential risks and resolving ethical issues.","It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT."],"url":"http://arxiv.org/abs/2406.03079v1","category":"cs.CL"}
{"created":"2024-06-05 08:43:11","title":"RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence Models for Abstractive Radiology Report Summarization","abstract":"Radiology report summarization is a crucial task that can help doctors quickly identify clinically significant findings without the need to review detailed sections of reports. This study proposes RadBARTsum, a domain-specific and ontology facilitated adaptation of the BART model for abstractive radiology report summarization. The approach involves two main steps: 1) re-training the BART model on a large corpus of radiology reports using a novel entity masking strategy to improving biomedical domain knowledge learning, and 2) fine-tuning the model for the summarization task using the Findings and Background sections to predict the Impression section. Experiments are conducted using different masking strategies. Results show that the re-training process with domain knowledge facilitated masking improves performances consistently across various settings. This work contributes a domain-specific generative language model for radiology report summarization and a method for utilising medical knowledge to realise entity masking language model. The proposed approach demonstrates a promising direction of enhancing the efficiency of language models by deepening its understanding of clinical knowledge in radiology reports.","sentences":["Radiology report summarization is a crucial task that can help doctors quickly identify clinically significant findings without the need to review detailed sections of reports.","This study proposes RadBARTsum, a domain-specific and ontology facilitated adaptation of the BART model for abstractive radiology report summarization.","The approach involves two main steps: 1) re-training the BART model on a large corpus of radiology reports using a novel entity masking strategy to improving biomedical domain knowledge learning, and 2) fine-tuning the model for the summarization task using the Findings and Background sections to predict the Impression section.","Experiments are conducted using different masking strategies.","Results show that the re-training process with domain knowledge facilitated masking improves performances consistently across various settings.","This work contributes a domain-specific generative language model for radiology report summarization and a method for utilising medical knowledge to realise entity masking language model.","The proposed approach demonstrates a promising direction of enhancing the efficiency of language models by deepening its understanding of clinical knowledge in radiology reports."],"url":"http://arxiv.org/abs/2406.03062v1","category":"cs.CL"}
{"created":"2024-06-05 08:38:41","title":"Whistler waves in the quasi-parallel and quasi-perpendicular magnetosheath","abstract":"In the Earth's magnetosheath (MSH), several processes contribute to energy dissipation and plasma heating, one of which is wave-particle interactions between whistler waves and electrons. However, the overall impact of whistlers on electron dynamics in the MSH remains to be quantified. We analyze 18 hours of burst-mode measurements from the Magnetospheric Multiscale (MMS) mission, including data from the unbiased magnetosheath campaign during February-March 2023. We present a statistical study of 34,409 whistler waves found using automatic detection. We compare wave occurrence in the different MSH geometries and find three times higher occurrence in the quasi-perpendicular MSH compared to the quasi-parallel case. We also study the wave properties and find that the waves propagate quasi-parallel to the background magnetic field, have a median frequency of 0.2 times the electron cyclotron frequency, median amplitude of 0.03-0.06 nT (30-60 pT), and median duration of a few tens of wave periods. The whistler waves are preferentially observed in local magnetic dips and density peaks and are not associated with an increased temperature anisotropy. Also, almost no whistlers are observed in regions with parallel electron plasma beta lower than 0.1. Importantly, when estimating pitch-angle diffusion times we find that the whistler waves cause significant pitch-angle scattering of electrons in the MSH.","sentences":["In the Earth's magnetosheath (MSH), several processes contribute to energy dissipation and plasma heating, one of which is wave-particle interactions between whistler waves and electrons.","However, the overall impact of whistlers on electron dynamics in the MSH remains to be quantified.","We analyze 18 hours of burst-mode measurements from the Magnetospheric Multiscale (MMS) mission, including data from the unbiased magnetosheath campaign during February-March 2023.","We present a statistical study of 34,409 whistler waves found using automatic detection.","We compare wave occurrence in the different MSH geometries and find three times higher occurrence in the quasi-perpendicular MSH compared to the quasi-parallel case.","We also study the wave properties and find that the waves propagate quasi-parallel to the background magnetic field, have a median frequency of 0.2 times the electron cyclotron frequency, median amplitude of 0.03-0.06","nT","(30-60 pT), and median duration of a few tens of wave periods.","The whistler waves are preferentially observed in local magnetic dips and density peaks and are not associated with an increased temperature anisotropy.","Also, almost no whistlers are observed in regions with parallel electron plasma beta lower than 0.1.","Importantly, when estimating pitch-angle diffusion times we find that the whistler waves cause significant pitch-angle scattering of electrons in the MSH."],"url":"http://arxiv.org/abs/2406.03060v1","category":"physics.space-ph"}
{"created":"2024-06-05 08:26:44","title":"Adapter-X: A Novel General Parameter-Efficient Fine-Tuning Framework for Vision","abstract":"Parameter-efficient fine-tuning (PEFT) has become increasingly important as foundation models continue to grow in both popularity and size. Adapter has been particularly well-received due to their potential for parameter reduction and adaptability across diverse tasks. However, striking a balance between high efficiency and robust generalization across tasks remains a challenge for adapter-based methods. We analyze existing methods and find that: 1) parameter sharing is the key to reducing redundancy; 2) more tunable parameters, dynamic allocation, and block-specific design are keys to improving performance. Unfortunately, no previous work considers all these factors. Inspired by this insight, we introduce a novel framework named Adapter-X. First, a Sharing Mixture of Adapters (SMoA) module is proposed to fulfill token-level dynamic allocation, increased tunable parameters, and inter-block sharing at the same time. Second, some block-specific designs like Prompt Generator (PG) are introduced to further enhance the ability of adaptation. Extensive experiments across 2D image and 3D point cloud modalities demonstrate that Adapter-X represents a significant milestone as it is the first to outperform full fine-tuning in both 2D image and 3D point cloud modalities with significantly fewer parameters, i.e., only 0.20% and 1.88% of original trainable parameters for 2D and 3D classification tasks. Our code will be publicly available.","sentences":["Parameter-efficient fine-tuning (PEFT) has become increasingly important as foundation models continue to grow in both popularity and size.","Adapter has been particularly well-received due to their potential for parameter reduction and adaptability across diverse tasks.","However, striking a balance between high efficiency and robust generalization across tasks remains a challenge for adapter-based methods.","We analyze existing methods and find that: 1) parameter sharing is the key to reducing redundancy; 2) more tunable parameters, dynamic allocation, and block-specific design are keys to improving performance.","Unfortunately, no previous work considers all these factors.","Inspired by this insight, we introduce a novel framework named Adapter-X. First, a Sharing Mixture of Adapters (SMoA) module is proposed to fulfill token-level dynamic allocation, increased tunable parameters, and inter-block sharing at the same time.","Second, some block-specific designs like Prompt Generator (PG) are introduced to further enhance the ability of adaptation.","Extensive experiments across 2D image and 3D point cloud modalities demonstrate that Adapter-X represents a significant milestone as it is the first to outperform full fine-tuning in both 2D image and 3D point cloud modalities with significantly fewer parameters, i.e., only 0.20% and 1.88% of original trainable parameters for 2D and 3D classification tasks.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2406.03051v2","category":"cs.CV"}
{"created":"2024-06-05 08:21:55","title":"When Spiking neural networks meet temporal attention image decoding and adaptive spiking neuron","abstract":"Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\'echet Inception Distance, and Fr\\'echet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\%) and CIFAR-10 (93.89\\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons. The code is available at https://github.com/bollossom/ICLR_TINY_SNN.","sentences":["Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way.","However, most existing SNN-based methods for image tasks do not fully exploit this feature.","Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability.","To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model.","Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\'echet Inception Distance, and Fr\\'echet Autoencoder Distance.","Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\%) and CIFAR-10 (93.89\\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons.","The code is available at https://github.com/bollossom/ICLR_TINY_SNN."],"url":"http://arxiv.org/abs/2406.03046v1","category":"cs.NE"}
{"created":"2024-06-05 08:21:49","title":"High-order Discontinuous Galerkin Methods for the Monodomain and Bidomain Models","abstract":"This work aims at presenting a Discontinuous Galerkin (DG) formulation employing a spectral basis for two important models employed in cardiac electrophysiology, namely the monodomain and bidomain models. The use of DG methods is motivated by the characteristic of the mathematical solution of such equations which often corresponds to a highly steep wavefront. Hence, the built-in flexibility of discontinuous methods in developing adaptive approaches, combined with the high-order accuracy, can well represent the underlying physics. The choice of a semi-implicit time integration allows for a fast solution at each time step. The article includes some numerical tests to verify the convergence properties and the physiological behaviour of the numerical solution. Also, a pseudo-realistic simulation turns out to fully reconstruct the propagation of the electric potential, comprising the phases of depolarization and repolarization, by overcoming the typical issues related to the steepness of the wave front.","sentences":["This work aims at presenting a Discontinuous Galerkin (DG) formulation employing a spectral basis for two important models employed in cardiac electrophysiology, namely the monodomain and bidomain models.","The use of DG methods is motivated by the characteristic of the mathematical solution of such equations which often corresponds to a highly steep wavefront.","Hence, the built-in flexibility of discontinuous methods in developing adaptive approaches, combined with the high-order accuracy, can well represent the underlying physics.","The choice of a semi-implicit time integration allows for a fast solution at each time step.","The article includes some numerical tests to verify the convergence properties and the physiological behaviour of the numerical solution.","Also, a pseudo-realistic simulation turns out to fully reconstruct the propagation of the electric potential, comprising the phases of depolarization and repolarization, by overcoming the typical issues related to the steepness of the wave front."],"url":"http://arxiv.org/abs/2406.03045v1","category":"math.NA"}
{"created":"2024-06-05 17:59:35","title":"Grokking Modular Polynomials","abstract":"Neural networks readily learn a subset of the modular arithmetic tasks, while failing to generalize on the rest. This limitation remains unmoved by the choice of architecture and training strategies. On the other hand, an analytical solution for the weights of Multi-layer Perceptron (MLP) networks that generalize on the modular addition task is known in the literature. In this work, we (i) extend the class of analytical solutions to include modular multiplication as well as modular addition with many terms. Additionally, we show that real networks trained on these datasets learn similar solutions upon generalization (grokking). (ii) We combine these \"expert\" solutions to construct networks that generalize on arbitrary modular polynomials. (iii) We hypothesize a classification of modular polynomials into learnable and non-learnable via neural networks training; and provide experimental evidence supporting our claims.","sentences":["Neural networks readily learn a subset of the modular arithmetic tasks, while failing to generalize on the rest.","This limitation remains unmoved by the choice of architecture and training strategies.","On the other hand, an analytical solution for the weights of Multi-layer Perceptron (MLP) networks that generalize on the modular addition task is known in the literature.","In this work, we (i) extend the class of analytical solutions to include modular multiplication as well as modular addition with many terms.","Additionally, we show that real networks trained on these datasets learn similar solutions upon generalization (grokking).","(ii) We combine these \"expert\" solutions to construct networks that generalize on arbitrary modular polynomials.","(iii) We hypothesize a classification of modular polynomials into learnable and non-learnable via neural networks training; and provide experimental evidence supporting our claims."],"url":"http://arxiv.org/abs/2406.03495v1","category":"cs.LG"}
{"created":"2024-06-05 17:59:22","title":"Solving Poisson Equations using Neural Walk-on-Spheres","abstract":"We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the efficient solution of high-dimensional Poisson equations. Leveraging stochastic representations and Walk-on-Spheres methods, we develop novel losses for neural networks based on the recursive solution of Poisson equations on spheres inside the domain. The resulting method is highly parallelizable and does not require spatial gradients for the loss. We provide a comprehensive comparison against competing methods based on PINNs, the Deep Ritz method, and (backward) stochastic differential equations. In several challenging, high-dimensional numerical examples, we demonstrate the superiority of NWoS in accuracy, speed, and computational costs. Compared to commonly used PINNs, our approach can reduce memory usage and errors by orders of magnitude. Furthermore, we apply NWoS to problems in PDE-constrained optimization and molecular dynamics to show its efficiency in practical applications.","sentences":["We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the efficient solution of high-dimensional Poisson equations.","Leveraging stochastic representations and Walk-on-Spheres methods, we develop novel losses for neural networks based on the recursive solution of Poisson equations on spheres inside the domain.","The resulting method is highly parallelizable and does not require spatial gradients for the loss.","We provide a comprehensive comparison against competing methods based on PINNs, the Deep Ritz method, and (backward) stochastic differential equations.","In several challenging, high-dimensional numerical examples, we demonstrate the superiority of NWoS in accuracy, speed, and computational costs.","Compared to commonly used PINNs, our approach can reduce memory usage and errors by orders of magnitude.","Furthermore, we apply NWoS to problems in PDE-constrained optimization and molecular dynamics to show its efficiency in practical applications."],"url":"http://arxiv.org/abs/2406.03494v1","category":"cs.LG"}
{"created":"2024-06-05 17:25:29","title":"Solving Differential Equations using Physics-Informed Deep Equilibrium Models","abstract":"This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for solving initial value problems (IVPs) of ordinary differential equations (ODEs). Leveraging recent advancements in deep equilibrium models (DEQs) and physics-informed neural networks (PINNs), PIDEQs combine the implicit output representation of DEQs with physics-informed training techniques. We validate PIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating their efficiency and effectiveness in solving IVPs. Our analysis includes key hyperparameter considerations for optimizing PIDEQ performance. By bridging deep learning and physics-based modeling, this work advances computational techniques for solving IVPs, with implications for scientific computing and engineering applications.","sentences":["This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for solving initial value problems (IVPs) of ordinary differential equations (ODEs).","Leveraging recent advancements in deep equilibrium models (DEQs) and physics-informed neural networks (PINNs), PIDEQs combine the implicit output representation of DEQs with physics-informed training techniques.","We validate PIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating their efficiency and effectiveness in solving IVPs.","Our analysis includes key hyperparameter considerations for optimizing PIDEQ performance.","By bridging deep learning and physics-based modeling, this work advances computational techniques for solving IVPs, with implications for scientific computing and engineering applications."],"url":"http://arxiv.org/abs/2406.03472v1","category":"cs.LG"}
{"created":"2024-06-05 17:21:25","title":"Fast randomized least-squares solvers can be just as accurate and stable as classical direct solvers","abstract":"One of the greatest success stories of randomized algorithms for linear algebra has been the development of fast, randomized algorithms for highly overdetermined linear least-squares problems. However, none of the existing algorithms is backward stable, preventing them from being deployed as drop-in replacements for existing QR-based solvers. This paper introduces FOSSILS, a fast, provably backward stable randomized least-squares solver. FOSSILS combines iterative refinement with a preconditioned iterative method applied to the normal equations, and it converges at the same rate as existing randomized least-squares solvers. This work offers the promise of incorporating randomized least-squares solvers into existing software libraries while maintaining the same level of accuracy and stability as classical solvers.","sentences":["One of the greatest success stories of randomized algorithms for linear algebra has been the development of fast, randomized algorithms for highly overdetermined linear least-squares problems.","However, none of the existing algorithms is backward stable, preventing them from being deployed as drop-in replacements for existing QR-based solvers.","This paper introduces FOSSILS, a fast, provably backward stable randomized least-squares solver.","FOSSILS combines iterative refinement with a preconditioned iterative method applied to the normal equations, and it converges at the same rate as existing randomized least-squares solvers.","This work offers the promise of incorporating randomized least-squares solvers into existing software libraries while maintaining the same level of accuracy and stability as classical solvers."],"url":"http://arxiv.org/abs/2406.03468v1","category":"math.NA"}
{"created":"2024-06-05 16:33:30","title":"Analytical Survival Analysis of the Non-autonomous Ornstein-Uhlenbeck Process","abstract":"The survival probability for a periodic non-autonomous Ornstein-Uhlenbeck process is calculated analytically using two different methods. The first uses an asymptotic approach. We treat the associated Kolmogorov Backward Equation with an absorbing boundary by dividing the domain into an interior region, centered around the origin, and a \"boundary layer\" near the absorbing boundary. In each region we determine the leading-order analytical solutions, and construct a uniformly valid solution over the entire domain using asymptotic matching. In the second method we examine the integral relationship between the probability density function and the mean first passage time probability density function. These allow us to determine approximate analytical forms for the exit rate. The validity of the solutions derived from both methods is assessed numerically, and we find the asymptotic method to be superior.","sentences":["The survival probability for a periodic non-autonomous Ornstein-Uhlenbeck process is calculated analytically using two different methods.","The first uses an asymptotic approach.","We treat the associated Kolmogorov Backward Equation with an absorbing boundary by dividing the domain into an interior region, centered around the origin, and a \"boundary layer\" near the absorbing boundary.","In each region we determine the leading-order analytical solutions, and construct a uniformly valid solution over the entire domain using asymptotic matching.","In the second method we examine the integral relationship between the probability density function and the mean first passage time probability density function.","These allow us to determine approximate analytical forms for the exit rate.","The validity of the solutions derived from both methods is assessed numerically, and we find the asymptotic method to be superior."],"url":"http://arxiv.org/abs/2406.03436v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 16:25:45","title":"The strong data processing inequality under the heat flow","abstract":"Let $\\nu$ and $\\mu$ be probability distributions on $\\mathbb{R}^n$, and $\\nu_s,\\mu_s$ be their evolution under the heat flow, that is, the probability distributions resulting from convolving their density with the density of an isotropic Gaussian random vector with variance $s$ in each entry. This paper studies the rate of decay of $s\\mapsto D(\\nu_s\\|\\mu_s)$ for various divergences, including the $\\chi^2$ and Kullback-Leibler (KL) divergences. We prove upper and lower bounds on the strong data-processing inequality (SDPI) coefficients corresponding to the source $\\mu$ and the Gaussian channel. We also prove generalizations of de Brujin's identity, and Costa's result on the concavity in $s$ of the differential entropy of $\\nu_s$. As a byproduct of our analysis, we obtain new lower bounds on the mutual information between $X$ and $Y=X+\\sqrt{s} Z$, where $Z$ is a standard Gaussian vector in $\\mathbb{R}^n$, independent of $X$, and on the minimum mean-square error (MMSE) in estimating $X$ from $Y$, in terms of the Poincar\\'e constant of $X$.","sentences":["Let $\\nu$ and $\\mu$ be probability distributions on $\\mathbb{R}^n$, and $\\nu_s,\\mu_s$ be their evolution under the heat flow, that is, the probability distributions resulting from convolving their density with the density of an isotropic Gaussian random vector with variance $s$ in each entry.","This paper studies the rate of decay of $s\\mapsto D(\\nu_s\\|\\mu_s)$ for various divergences, including the $\\chi^2$ and Kullback-Leibler (KL) divergences.","We prove upper and lower bounds on the strong data-processing inequality (SDPI) coefficients corresponding to the source $\\mu$ and the Gaussian channel.","We also prove generalizations of de Brujin's identity, and Costa's result on the concavity in $s$ of the differential entropy of $\\nu_s$. As a byproduct of our analysis, we obtain new lower bounds on the mutual information between $X$ and $Y=X+\\sqrt{s} Z$, where $Z$ is a standard Gaussian vector in $\\mathbb{R}^n$, independent of $X$, and on the minimum mean-square error (MMSE) in estimating $X$ from $Y$, in terms of the Poincar\\'e constant of $X$."],"url":"http://arxiv.org/abs/2406.03427v1","category":"cs.IT"}
{"created":"2024-06-05 16:12:19","title":"CoFie: Learning Compact Neural Surface Representations with Coordinate Fields","abstract":"This paper introduces CoFie, a novel local geometry-aware neural surface representation. CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation. We find that local shapes are highly compressive in an aligned coordinate frame defined by the normal and tangent directions of local shapes. Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes. The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame. It largely reduces the complexity of local shapes and benefits the learning of MLP-based implicit representations. Moreover, we introduce quadratic layers into the MLP to enhance expressiveness concerning local shape geometry. CoFie is a generalizable surface representation. It is trained on a curated set of 3D shapes and works on novel shape instances during testing. When using the same amount of parameters with prior works, CoFie reduces the shape error by 48% and 56% on novel instances of both training and unseen shape categories. Moreover, CoFie demonstrates comparable performance to prior works when using only 70% fewer parameters.","sentences":["This paper introduces CoFie, a novel local geometry-aware neural surface representation.","CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation.","We find that local shapes are highly compressive in an aligned coordinate frame defined by the normal and tangent directions of local shapes.","Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes.","The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame.","It largely reduces the complexity of local shapes and benefits the learning of MLP-based implicit representations.","Moreover, we introduce quadratic layers into the MLP to enhance expressiveness concerning local shape geometry.","CoFie is a generalizable surface representation.","It is trained on a curated set of 3D shapes and works on novel shape instances during testing.","When using the same amount of parameters with prior works, CoFie reduces the shape error by 48% and 56% on novel instances of both training and unseen shape categories.","Moreover, CoFie demonstrates comparable performance to prior works when using only 70% fewer parameters."],"url":"http://arxiv.org/abs/2406.03417v1","category":"cs.CV"}
{"created":"2024-06-05 16:03:17","title":"Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation","abstract":"We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.","sentences":["We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying.","Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method.","The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores.","A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection.","Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles.","The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations."],"url":"http://arxiv.org/abs/2406.03400v1","category":"stat.ME"}
{"created":"2024-06-05 15:59:30","title":"Elliptic curves over Hasse pairs","abstract":"We call a pair of distinct prime powers $(q_1,q_2) = (p_1^{a_1},p_2^{a_2})$ a Hasse pair if $|\\sqrt{q_1}-\\sqrt{q_2}| \\leq 1$. For such pairs, we study the relation between the set $\\mathcal{E}_1$ of isomorphism classes of elliptic curves defined over $\\mathbb{F}_{q_1}$ with $q_2$ points, and the set $\\mathcal{E}_2$ of isomorphism classes of elliptic curves over $\\mathbb{F}_{q_2}$ with $q_1$ points. When both families $\\mathcal{E}_i$ contain only ordinary elliptic curves, we prove that their isogeny graphs are isomorphic. When supersingular curves are involved, we describe which curves might belong to these sets. We also show that if both the $q_i$'s are odd and $\\mathcal{E}_1 \\cup \\mathcal{E}_2 \\neq \\emptyset$, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ always contains an ordinary elliptic curve. Conversely, if $q_1$ is even, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ may contain only supersingular curves precisely when $q_2$ is a given power of a Fermat or a Mersenne prime. In the case of odd Hasse pairs, we could not rule out the possibility of an empty union $\\mathcal{E}_1 \\cup \\mathcal{E}_2$, but we give necessary conditions for such a case to exist. In an appendix, Moree and Sofos consider how frequently Hasse pairs occur using analytic number theory, making a connection with Andrica's conjecture on the difference between consecutive primes.","sentences":["We call a pair of distinct prime powers $(q_1,q_2) = (p_1^{a_1},p_2^{a_2})$ a Hasse pair if $|\\sqrt{q_1}-\\sqrt{q_2}| \\leq 1$. For such pairs, we study the relation between the set $\\mathcal{E}_1$ of isomorphism classes of elliptic curves defined over $\\mathbb{F}_{q_1}$ with $q_2$ points, and the set $\\mathcal{E}_2$ of isomorphism classes of elliptic curves over $\\mathbb{F}_{q_2}$ with $q_1$ points.","When both families $\\mathcal{E}_i$ contain only ordinary elliptic curves, we prove that their isogeny graphs are isomorphic.","When supersingular curves are involved, we describe which curves might belong to these sets.","We also show that if both the $q_i$'s are odd and $\\mathcal{E}_1 \\cup \\mathcal{E}_2 \\neq \\emptyset$, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ always contains an ordinary elliptic curve.","Conversely, if $q_1$ is even, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ may contain only supersingular curves precisely when $q_2$ is a given power of a Fermat or a Mersenne prime.","In the case of odd Hasse pairs, we could not rule out the possibility of an empty union $\\mathcal{E}_1 \\cup \\mathcal{E}_2$, but we give necessary conditions for such a case to exist.","In an appendix, Moree and Sofos consider how frequently Hasse pairs occur using analytic number theory, making a connection with Andrica's conjecture on the difference between consecutive primes."],"url":"http://arxiv.org/abs/2406.03399v1","category":"math.NT"}
{"created":"2024-06-05 15:43:14","title":"On the embedding between the variable Lebesgue space $L^{p(\\cdot)}(\u03a9)$ and the Orlicz space $L(\\log L)^\u03b1(\u03a9)$","abstract":"We give a sharp sufficient condition on the distribution function, $|\\{x\\in \\Omega :\\,p(x)\\leq 1+\\lambda\\}|$, $\\lambda>0$, of the exponent function $p(\\cdot): \\Omega \\to [1,\\infty)$ that implies the embedding of the variable Lebesgue space $L^{p(\\cdot)}(\\Omega)$ into the Orlicz space $L(\\log L)^{\\alpha}(\\Omega)$, $\\alpha>0$, where $\\Omega$ is an open set with finite Lebesgue measure. As applications of our results, we first give conditions that imply the strong differentiation of integrals of functions in $L^{p(\\cdot)}((0,1)^{n})$, $n>1$. We then consider the integrability of the maximal function on variable Lebesgue spaces, where the exponent function $p(\\cdot)$ approaches $1$ in value on some part of the domain. This result is an improvement of the result in~\\cite{CUF2}.","sentences":["We give a sharp sufficient condition on the distribution function, $|\\{x\\in \\Omega :\\,p(x)\\leq 1+\\lambda\\}|$, $\\lambda>0$, of the exponent function $p(\\cdot): \\Omega","\\to","[1,\\infty)$ that implies the embedding of the variable Lebesgue space $L^{p(\\cdot)}(\\Omega)$ into the Orlicz space $L(\\log L)^{\\alpha}(\\Omega)$, $\\alpha>0$, where $\\Omega$ is an open set with finite Lebesgue measure.","As applications of our results, we first give conditions that imply the strong differentiation of integrals of functions in $L^{p(\\cdot)}((0,1)^{n})$, $n>1$. We then consider the integrability of the maximal function on variable Lebesgue spaces, where the exponent function $p(\\cdot)$ approaches $1$ in value on some part of the domain.","This result is an improvement of the result in~\\cite{CUF2}."],"url":"http://arxiv.org/abs/2406.03392v1","category":"math.CA"}
{"created":"2024-06-05 15:36:57","title":"Learning Long Range Dependencies on Graphs via Random Walks","abstract":"Message-passing graph neural networks (GNNs), while excelling at capturing local relationships, often struggle with long-range dependencies on graphs. Conversely, graph transformers (GTs) enable information exchange between all nodes but oversimplify the graph structure by treating them as a set of fixed-length vectors. This work proposes a novel architecture, NeuralWalker, that overcomes the limitations of both methods by combining random walks with message passing. NeuralWalker achieves this by treating random walks as sequences, allowing for the application of recent advances in sequence models in order to capture long-range dependencies within these walks. Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures. Our experimental evaluations demonstrate that NeuralWalker achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13% on the PascalVoc-SP and COCO-SP datasets. Code is available at https://github.com/BorgwardtLab/NeuralWalker.","sentences":["Message-passing graph neural networks (GNNs), while excelling at capturing local relationships, often struggle with long-range dependencies on graphs.","Conversely, graph transformers (GTs) enable information exchange between all nodes but oversimplify the graph structure by treating them as a set of fixed-length vectors.","This work proposes a novel architecture, NeuralWalker, that overcomes the limitations of both methods by combining random walks with message passing.","NeuralWalker achieves this by treating random walks as sequences, allowing for the application of recent advances in sequence models in order to capture long-range dependencies within these walks.","Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures.","Our experimental evaluations demonstrate that NeuralWalker achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13% on the PascalVoc-SP and COCO-SP datasets.","Code is available at https://github.com/BorgwardtLab/NeuralWalker."],"url":"http://arxiv.org/abs/2406.03386v1","category":"cs.LG"}
{"created":"2024-06-05 15:36:39","title":"A mathematical analysis of IPT-DMFT","abstract":"We provide a mathematical analysis of the Dynamical Mean-Field Theory, a celebrated representative of a class of approximations in quantum mechanics known as embedding methods. We start by a pedagogical and self-contained mathematical formulation of the Dynamical Mean-Field Theory equations for the finite Hubbard model. After recalling the definition and properties of one-body time-ordered Green's functions and self-energies, and the mathematical structure of the Hubbard and Anderson impurity models, we describe a specific impurity solver, namely the Iterated Perturbation Theory solver, which can be conveniently formulated using Matsubara's Green's functions. Within this framework, we prove under certain assumptions that the Dynamical Mean-Field Theory equations admit a solution for any set of physical parameters. Moreover, we establish some properties of the solution(s).","sentences":["We provide a mathematical analysis of the Dynamical Mean-Field Theory, a celebrated representative of a class of approximations in quantum mechanics known as embedding methods.","We start by a pedagogical and self-contained mathematical formulation of the Dynamical Mean-Field Theory equations for the finite Hubbard model.","After recalling the definition and properties of one-body time-ordered Green's functions and self-energies, and the mathematical structure of the Hubbard and Anderson impurity models, we describe a specific impurity solver, namely the Iterated Perturbation Theory solver, which can be conveniently formulated using Matsubara's Green's functions.","Within this framework, we prove under certain assumptions that the Dynamical Mean-Field Theory equations admit a solution for any set of physical parameters.","Moreover, we establish some properties of the solution(s)."],"url":"http://arxiv.org/abs/2406.03384v1","category":"math-ph"}
{"created":"2024-06-05 15:32:38","title":"Paths towards time evolution with larger neural-network quantum states","abstract":"In recent years, the neural-network quantum states method has been investigated to study the ground state and the time evolution of many-body quantum systems. Here we expand on the investigation and consider a quantum quench from the paramagnetic to the anti-ferromagnetic phase in the tilted Ising model. We use two types of neural networks, a restricted Boltzmann machine and a feed-forward neural network. We show that for both types of networks, the projected time-dependent variational Monte Carlo (p-tVMC) method performs better than the non-projected approach. We further demonstrate that one can use K-FAC or minSR in conjunction with p-tVMC to reduce the computational complexity of the stochastic reconfiguration approach, thus allowing the use of these techniques for neural networks with more parameters.","sentences":["In recent years, the neural-network quantum states method has been investigated to study the ground state and the time evolution of many-body quantum systems.","Here we expand on the investigation and consider a quantum quench from the paramagnetic to the anti-ferromagnetic phase in the tilted Ising model.","We use two types of neural networks, a restricted Boltzmann machine and a feed-forward neural network.","We show that for both types of networks, the projected time-dependent variational Monte Carlo (p-tVMC) method performs better than the non-projected approach.","We further demonstrate that one can use K-FAC or minSR in conjunction with p-tVMC to reduce the computational complexity of the stochastic reconfiguration approach, thus allowing the use of these techniques for neural networks with more parameters."],"url":"http://arxiv.org/abs/2406.03381v1","category":"quant-ph"}
{"created":"2024-06-05 15:32:35","title":"Maximal information at the edge of stability in excitatory-inhibitory neural populations","abstract":"Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question. Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals. We show that information is maximized at the edge of stability, where excitation is balanced by inhibition. When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics. By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy. In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities.","sentences":["Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question.","Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals.","We show that information is maximized at the edge of stability, where excitation is balanced by inhibition.","When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics.","By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy.","In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities."],"url":"http://arxiv.org/abs/2406.03380v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 15:30:10","title":"A field-level emulator for modified gravity","abstract":"Stage IV dark energy surveys such as the Vera C. Rubin Observatory and Euclid present a unique opportunity to shed light on the nature of dark energy. However, the full constraining power of the new data cannot be unlocked unless accurate predictions are available at all observable scales. Currently, only the linear regime is well understood in models beyond $\\Lambda$CDM: on the nonlinear scales, expensive numerical simulations become necessary, making their direct use impractical in the analyses of large datasets. Recently, machine learning techniques have shown potential to break this impasse: by training emulators, we can reproduce complex data fields in a fraction of the time it takes to produce them.   In this work, we present a field-level emulator capable of turning a $\\Lambda$CDM N-body simulation into one evolved under $f(R)$ gravity. To achieve this, we build on the map2map neural network, using the strength of modified gravity $|f_{R_0}|$ as style parameter. We find that our emulator correctly estimates the changes it needs to apply to the positions and velocities of the input N-body particles to produce the target simulation.   We test the performance of our network against several summary statistics, finding $1\\%$ agreement in the power spectrum up to $k \\sim 1$ $h/$Mpc, and $1.5\\%$ agreement against the independent boost emulator eMantis. Although the algorithm is trained on fixed cosmological parameters, we find it can extrapolate to models it was not trained on. Coupled with available field-level emulators and simulations suites for $\\Lambda$CDM, our algorithm can be used to constrain modified gravity in the large-scale structure using full information available at the field-level.","sentences":["Stage IV dark energy surveys such as the Vera C. Rubin Observatory and Euclid present a unique opportunity to shed light on the nature of dark energy.","However, the full constraining power of the new data cannot be unlocked unless accurate predictions are available at all observable scales.","Currently, only the linear regime is well understood in models beyond $\\Lambda$CDM: on the nonlinear scales, expensive numerical simulations become necessary, making their direct use impractical in the analyses of large datasets.","Recently, machine learning techniques have shown potential to break this impasse: by training emulators, we can reproduce complex data fields in a fraction of the time it takes to produce them.   ","In this work, we present a field-level emulator capable of turning a $\\Lambda$CDM N-body simulation into one evolved under $f(R)$ gravity.","To achieve this, we build on the map2map neural network, using the strength of modified gravity $|f_{R_0}|$ as style parameter.","We find that our emulator correctly estimates the changes it needs to apply to the positions and velocities of the input N-body particles to produce the target simulation.   ","We test the performance of our network against several summary statistics, finding $1\\%$ agreement in the power spectrum up to $k \\sim 1$ $h/$Mpc, and $1.5\\%$ agreement against the independent boost emulator eMantis.","Although the algorithm is trained on fixed cosmological parameters, we find it can extrapolate to models it was not trained on.","Coupled with available field-level emulators and simulations suites for $\\Lambda$CDM, our algorithm can be used to constrain modified gravity in the large-scale structure using full information available at the field-level."],"url":"http://arxiv.org/abs/2406.03374v1","category":"astro-ph.CO"}
{"created":"2024-06-05 15:28:04","title":"Training of Physical Neural Networks","abstract":"Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely \"yes, with enough research\": PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.","sentences":["Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation.","While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI.","Could we train AI models 1000x larger than current ones?","Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors?","Research over the past few years has shown that the answer to all these questions is likely \"yes, with enough research\": PNNs could one day radically change what is possible and practical for AI systems.","To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics.","To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored.","These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today.","However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models."],"url":"http://arxiv.org/abs/2406.03372v1","category":"physics.app-ph"}
{"created":"2024-06-05 15:25:16","title":"Orbits of particles with magnetic dipole moment around magnetized Schwarzschild black holes: Applications to S2 star orbit","abstract":"This study provides a comprehensive analytical investigation of the bound and unbound motion of magnetized particles orbiting a Schwarzschild black hole immersed in an external asymptotically uniform magnetic field, which includes all conceivable types of bounded and unbounded orbits. In particular, for planetary orbits, we perform a comparative analysis of our findings with the observed position of the S2 star carrying magnetic dipole moment around Sagittarius A* (Sgr A*). We found maximum and minimum values for the parameter of magnetic interaction between the magnetic dipole of the star and the external magnetic field, as well as the energy and angular momentum of the S2 star. As a result, we obtain estimations of the magnetic dipole of the star in order of $10^6 \\rm \\ G\\cdot cm^{3}$. Additionally, we explore deflecting trajectories akin to gravitational Rutherford scattering. In obtaining the solutions for the orbital equations, we articulate the elliptic integrals and Jacobi elliptic functions, and our study is augmented by illustrative figures and simulations.","sentences":["This study provides a comprehensive analytical investigation of the bound and unbound motion of magnetized particles orbiting a Schwarzschild black hole immersed in an external asymptotically uniform magnetic field, which includes all conceivable types of bounded and unbounded orbits.","In particular, for planetary orbits, we perform a comparative analysis of our findings with the observed position of the S2 star carrying magnetic dipole moment around Sagittarius A* (Sgr A*).","We found maximum and minimum values for the parameter of magnetic interaction between the magnetic dipole of the star and the external magnetic field, as well as the energy and angular momentum of the S2 star.","As a result, we obtain estimations of the magnetic dipole of the star in order of $10^6 \\rm \\ G\\cdot","cm^{3}$. Additionally, we explore deflecting trajectories akin to gravitational Rutherford scattering.","In obtaining the solutions for the orbital equations, we articulate the elliptic integrals and Jacobi elliptic functions, and our study is augmented by illustrative figures and simulations."],"url":"http://arxiv.org/abs/2406.03371v1","category":"gr-qc"}
{"created":"2024-06-05 15:07:06","title":"Resonant phenomena in finite motions of test particles in oscillating dark matter configurations","abstract":"Nonlinear differential equations are derived that describe the time evolution of the test particle coordinates during finite motions in the gravitational field of oscillating dark matter. It is shown that in the weak field approximation, the radial oscillations of a test particle and oscillations in orbital motion are described by the Hill equation and the nonhomogeneous Hill equation, respectively. In the case of scalar dark matter with a logarithmic self-interactions, these equations are integrated numerically, and the solutions are compared with the corresponding solutions of the original nonlinear system to identify possible resonance effects.","sentences":["Nonlinear differential equations are derived that describe the time evolution of the test particle coordinates during finite motions in the gravitational field of oscillating dark matter.","It is shown that in the weak field approximation, the radial oscillations of a test particle and oscillations in orbital motion are described by the Hill equation and the nonhomogeneous Hill equation, respectively.","In the case of scalar dark matter with a logarithmic self-interactions, these equations are integrated numerically, and the solutions are compared with the corresponding solutions of the original nonlinear system to identify possible resonance effects."],"url":"http://arxiv.org/abs/2406.03351v1","category":"gr-qc"}
{"created":"2024-06-05 15:06:31","title":"Input of the Coulomb law modification to the Lamb shift of the hydrogen atom","abstract":"Radiative corrections which remove accidental degeneracy in the spectrum of the relativistic hydrogen atom and lead to the modification of the Coulomb law, are calculated within the novel approach, based on the exact solution of the Dirac equation with the Coulomb potential. The energy spectrum of the hydrogen atom is obtained with account of these corrections and the Lamb shift is calculated for the lowest energy states.","sentences":["Radiative corrections which remove accidental degeneracy in the spectrum of the relativistic hydrogen atom and lead to the modification of the Coulomb law, are calculated within the novel approach, based on the exact solution of the Dirac equation with the Coulomb potential.","The energy spectrum of the hydrogen atom is obtained with account of these corrections and the Lamb shift is calculated for the lowest energy states."],"url":"http://arxiv.org/abs/2406.03350v1","category":"quant-ph"}
{"created":"2024-06-05 17:29:15","title":"Does your data spark joy? Performance gains from domain upsampling at the end of training","abstract":"Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.","sentences":["Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets.","It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks.","Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data?","In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks.","This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\\unicode{x2014}$a model trained for twice as long.","We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks.","We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training.","This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs."],"url":"http://arxiv.org/abs/2406.03476v1","category":"cs.LG"}
{"created":"2024-06-05 17:07:39","title":"The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement","abstract":"To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.","sentences":["To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics.","However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see.","The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation.","For this, we introduce enhancement models that exploit the widely used PESQ measure.","Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment.","While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t.","a metric, an isolated evaluation on the same metric may be misleading.","Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening."],"url":"http://arxiv.org/abs/2406.03460v1","category":"eess.AS"}
{"created":"2024-06-05 16:47:37","title":"Global fermionic mode optimization via swap gates","abstract":"We propose a general approach to find an optimal representation of a quantum many body wave function for a given error margin via global fermionic mode optimization. The stationary point on a fixed rank matrix product state manifold is obtained via a joint optimization on the Grassman manifold [Phys. Rev. Lett. 117, 210402] together with swap gates controlled permutations. The minimization of the global quantity, the block entropy area, guarantees that the method fulfills all criteria with respect to partial derivatives. Numerical results via large scale density matrix renormalization group simulations on strongly correlated molecular systems and two-dimensional fermionic lattice models are discussed.","sentences":["We propose a general approach to find an optimal representation of a quantum many body wave function for a given error margin via global fermionic mode optimization.","The stationary point on a fixed rank matrix product state manifold is obtained via a joint optimization on the Grassman manifold [Phys. Rev. Lett.","117, 210402] together with swap gates controlled permutations.","The minimization of the global quantity, the block entropy area, guarantees that the method fulfills all criteria with respect to partial derivatives.","Numerical results via large scale density matrix renormalization group simulations on strongly correlated molecular systems and two-dimensional fermionic lattice models are discussed."],"url":"http://arxiv.org/abs/2406.03449v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 16:26:55","title":"Quantitative metastability of the Tikhonov-Mann iteration for countable families of mappings","abstract":"In this paper, we obtain rates of metastability for the Tikhonov-Mann iteration for countable families of mappings in CAT(0) spaces. This iteration was recently defined by the author in the setting of W-hyperbolic spaces as a generalization of the strongly convergent version of the Krasnoselskii-Mann iteration introduced by Bot and Meier for finding common fixed points of families of nonexpansive mappings in Hilbert spaces, and as an extension of the Tikhonov-Mann iteration for single mappings, for which Leustean and the author obtained rates of asymptotic regularity in W-hyperbolic spaces.","sentences":["In this paper, we obtain rates of metastability for the Tikhonov-Mann iteration for countable families of mappings in CAT(0) spaces.","This iteration was recently defined by the author in the setting of W-hyperbolic spaces as a generalization of the strongly convergent version of the Krasnoselskii-Mann iteration introduced by Bot and Meier for finding common fixed points of families of nonexpansive mappings in Hilbert spaces, and as an extension of the Tikhonov-Mann iteration for single mappings, for which Leustean and the author obtained rates of asymptotic regularity in W-hyperbolic spaces."],"url":"http://arxiv.org/abs/2406.03429v1","category":"math.OC"}
{"created":"2024-06-05 16:20:52","title":"Computational lower bounds for multi-frequency group synchronization","abstract":"We consider a group synchronization problem with multiple frequencies which involves observing pairwise relative measurements of group elements on multiple frequency channels, corrupted by Gaussian noise. We study the computational phase transition in the problem of detecting whether a structured signal is present in such observations by analyzing low-degree polynomial algorithms. We show that, assuming the low-degree conjecture, in synchronization models over arbitrary finite groups as well as over the circle group $SO(2)$, a simple spectral algorithm is optimal among algorithms of runtime $\\exp(\\tilde{\\Omega}(n^{1/3}))$ for detection from an observation including a constant number of frequencies. Combined with an upper bound for the statistical threshold shown in Perry et al., our results indicate the presence of a statistical-to-computational gap in such models with a sufficiently large number of frequencies.","sentences":["We consider a group synchronization problem with multiple frequencies which involves observing pairwise relative measurements of group elements on multiple frequency channels, corrupted by Gaussian noise.","We study the computational phase transition in the problem of detecting whether a structured signal is present in such observations by analyzing low-degree polynomial algorithms.","We show that, assuming the low-degree conjecture, in synchronization models over arbitrary finite groups as well as over the circle group $SO(2)$, a simple spectral algorithm is optimal among algorithms of runtime $\\exp(\\tilde{\\Omega}(n^{1/3}))$ for detection from an observation including a constant number of frequencies.","Combined with an upper bound for the statistical threshold shown in Perry et al., our results indicate the presence of a statistical-to-computational gap in such models with a sufficiently large number of frequencies."],"url":"http://arxiv.org/abs/2406.03424v1","category":"math.ST"}
{"created":"2024-06-05 16:15:53","title":"Dynamic properties of a class of van der Pol-Duffing oscillators","abstract":"In this paper, we study the existence of bifurcation of a van der Pol-Duffing oscillator with quintic terms and its quasi-periodic solutions by means of qualitative and bifurcation theories. Firstly, we analyze the autonomous system and find that it has two kinds of local bifurcations and a global bifurcation: pitchfork bifurcation, Hopf bifurcation, homoclinic bifurcation. It is worth noting that the disappearance of the homoclinic orbit is synchronized with the emergence of a large limit cycle. Then, by discussing the stability of equilibria at infinity and the orientation of the trajectory, the existence and stability of limit circles of the autonomous system are analyzed by combining the Poincar\\'{e}-Bendixson theorem and the index theory. The global phase portrait and the numerical simulation of the autonomous system in different parameter values are given. Finally, the existence of periodic and quasi-periodic solutions to periodic forced system is proved by a KAM theorem.","sentences":["In this paper, we study the existence of bifurcation of a van der Pol-Duffing oscillator with quintic terms and its quasi-periodic solutions by means of qualitative and bifurcation theories.","Firstly, we analyze the autonomous system and find that it has two kinds of local bifurcations and a global bifurcation: pitchfork bifurcation, Hopf bifurcation, homoclinic bifurcation.","It is worth noting that the disappearance of the homoclinic orbit is synchronized with the emergence of a large limit cycle.","Then, by discussing the stability of equilibria at infinity and the orientation of the trajectory, the existence and stability of limit circles of the autonomous system are analyzed by combining the Poincar\\'{e}-Bendixson theorem and the index theory.","The global phase portrait and the numerical simulation of the autonomous system in different parameter values are given.","Finally, the existence of periodic and quasi-periodic solutions to periodic forced system is proved by a KAM theorem."],"url":"http://arxiv.org/abs/2406.03420v1","category":"math.DS"}
{"created":"2024-06-05 15:19:53","title":"Computational Supremacy of Quantum Eigensolver by Extension of Optimized Binary Configurations","abstract":"We developed a quantum eigensolver (QE) which is based on an extension of optimized binary configurations measured by quantum annealing (QA) on a D-Wave Quantum Annealer (D-Wave QA). This approach performs iterative QA measurements to optimize the eigenstates $\\vert \\psi \\rangle$ without the derivation of a classical computer. The computational cost is $\\eta M L$ for full eigenvalues $E$ and $\\vert \\psi \\rangle$ of the Hamiltonian $\\hat{H}$ of size $L \\times L$, where $M$ and $\\eta$ are the number of QA measurements required to reach the converged $\\vert \\psi \\rangle$ and the total annealing time of many QA shots, respectively. Unlike the exact diagonalized (ED) algorithm with $L^3$ iterations on a classical computer, the computation cost is not significantly affected by $L$ and $M$ because $\\eta$ represents a very short time within $10^{-2}$ seconds on the D-Wave QA. We selected the tight-binding $\\hat{H}$ that contains the exact $E$ values of all energy states in two systems with metallic and insulating phases. We confirmed that the proposed QE algorithm provides exact solutions within the errors of $5 \\times 10^{-3}$. The QE algorithm will not only show computational supremacy over the ED approach on a classical computer but will also be widely used for various applications such as material and drug design.","sentences":["We developed a quantum eigensolver (QE) which is based on an extension of optimized binary configurations measured by quantum annealing (QA) on a D-Wave Quantum Annealer (D-Wave QA).","This approach performs iterative QA measurements to optimize the eigenstates $\\vert \\psi \\rangle$ without the derivation of a classical computer.","The computational cost is $\\eta M L$ for full eigenvalues $E$ and $\\vert \\psi \\rangle$ of the Hamiltonian $\\hat{H}$ of size $L \\times L$, where $M$ and $\\eta$ are the number of QA measurements required to reach the converged $\\vert \\psi \\rangle$ and the total annealing time of many QA shots, respectively.","Unlike the exact diagonalized (ED) algorithm with $L^3$ iterations on a classical computer, the computation cost is not significantly affected by $L$ and $M$ because $\\eta$ represents a very short time within $10^{-2}$ seconds on the D-Wave QA.","We selected the tight-binding $\\hat{H}$ that contains the exact $E$ values of all energy states in two systems with metallic and insulating phases.","We confirmed that the proposed QE algorithm provides exact solutions within the errors of $5 \\times 10^{-3}$.","The QE algorithm will not only show computational supremacy over the ED approach on a classical computer but will also be widely used for various applications such as material and drug design."],"url":"http://arxiv.org/abs/2406.03366v1","category":"quant-ph"}
{"created":"2024-06-05 15:18:22","title":"Determination of Optimal Chain Coupling made by Embedding in D-Wave Quantum Annealer","abstract":"The qubits in a D-wave quantum annealer (D-wave QA) are designed on a Pegasus graph that is different from structure of a combinatorial optimization problem. This situation requires embedding with the chains connected by ferromagnetic (FM) coupling $J_c$ between the qubits. Weak and strong $J_c$ values induce chain breaking and enforcement of chain energy, which reduce the accuracy of quantum annealing (QA) measurements, respectively. In addition, we confirmed that even though the D-Wave Ocean package provides a default coupling $J_c^{\\text{default}}$, it is not an optimal coupling $J_c^{\\text{optimal}}$ that maximizes the possible correct rate of QA measurements. In this paper, we present an algorithm how $J_c^{\\text{optimal}}$ with the maximum probability $p$ for observing the possible lowest energy is determined. Finally, we confirm that the extracted $J_c^{\\text{optimal}}$ show much better $p$ than $J_c^{\\text{default}}$ in QA measurements of various parameters of frustrated and fully connected combinatorial optimization problems. The open code is available in \\textit{https://github.com/HunpyoLee/OptimizeChainStrength}.","sentences":["The qubits in a D-wave quantum annealer (D-wave QA) are designed on a Pegasus graph that is different from structure of a combinatorial optimization problem.","This situation requires embedding with the chains connected by ferromagnetic (FM) coupling $J_c$ between the qubits.","Weak and strong $J_c$ values induce chain breaking and enforcement of chain energy, which reduce the accuracy of quantum annealing (QA) measurements, respectively.","In addition, we confirmed that even though the D-Wave Ocean package provides a default coupling $J_c^{\\text{default}}$, it is not an optimal coupling $J_c^{\\text{optimal}}$ that maximizes the possible correct rate of QA measurements.","In this paper, we present an algorithm how $J_c^{\\text{optimal}}$ with the maximum probability $p$ for observing the possible lowest energy is determined.","Finally, we confirm that the extracted $J_c^{\\text{optimal}}$ show much better $p$ than $J_c^{\\text{default}}$ in QA measurements of various parameters of frustrated and fully connected combinatorial optimization problems.","The open code is available in \\textit{https://github.com/HunpyoLee/OptimizeChainStrength}."],"url":"http://arxiv.org/abs/2406.03364v1","category":"quant-ph"}
{"created":"2024-06-05 15:05:24","title":"Position: A Call to Action for a Human-Centered AutoML Paradigm","abstract":"Automated machine learning (AutoML) was formed around the fundamental objectives of automatically and efficiently configuring machine learning (ML) workflows, aiding the research of new ML algorithms, and contributing to the democratization of ML by making it accessible to a broader audience. Over the past decade, commendable achievements in AutoML have primarily focused on optimizing predictive performance. This focused progress, while substantial, raises questions about how well AutoML has met its broader, original goals. In this position paper, we argue that a key to unlocking AutoML's full potential lies in addressing the currently underexplored aspect of user interaction with AutoML systems, including their diverse roles, expectations, and expertise. We envision a more human-centered approach in future AutoML research, promoting the collaborative design of ML systems that tightly integrates the complementary strengths of human expertise and AutoML methodologies.","sentences":["Automated machine learning (AutoML) was formed around the fundamental objectives of automatically and efficiently configuring machine learning (ML) workflows, aiding the research of new ML algorithms, and contributing to the democratization of ML by making it accessible to a broader audience.","Over the past decade, commendable achievements in AutoML have primarily focused on optimizing predictive performance.","This focused progress, while substantial, raises questions about how well AutoML has met its broader, original goals.","In this position paper, we argue that a key to unlocking AutoML's full potential lies in addressing the currently underexplored aspect of user interaction with AutoML systems, including their diverse roles, expectations, and expertise.","We envision a more human-centered approach in future AutoML research, promoting the collaborative design of ML systems that tightly integrates the complementary strengths of human expertise and AutoML methodologies."],"url":"http://arxiv.org/abs/2406.03348v1","category":"cs.LG"}
{"created":"2024-06-05 14:59:58","title":"Relative-belief inference in quantum information theory","abstract":"We introduce the framework of Bayesian relative belief that directly evaluates whether or not the experimental data at hand supports a given hypothesis regarding a quantum system by directly comparing the prior and posterior probabilities for the hypothesis. In model-dimension certification tasks, we show that the relative belief procedure typically chooses Hilbert spaces that are never smaller in dimension than those selected from optimizing a broad class of information criteria, including Akaike's criterion. As a concrete and focused exposition of this powerful evidence-based technique, we apply the relative belief procedure to an important application: state reconstruction of imperfect quantum sources. In particular, just by comparing prior and posterior probabilities based on data, we demonstrate its capability of tracking multiphoton emissions using (realistically lossy) single-photon detectors in order to assess the actual quality of photon sources without making ad hoc assumptions, thereby reliably safeguarding source integrity for general quantum-information and communication tasks with Bayesian reasoning. Finally, we discuss how relative belief can be exploited to carry out parametric model certification and estimate the total dimension of the quantum state for the combined (measured) physical and interacting external systems described by the Tavis--Cummings model.","sentences":["We introduce the framework of Bayesian relative belief that directly evaluates whether or not the experimental data at hand supports a given hypothesis regarding a quantum system by directly comparing the prior and posterior probabilities for the hypothesis.","In model-dimension certification tasks, we show that the relative belief procedure typically chooses Hilbert spaces that are never smaller in dimension than those selected from optimizing a broad class of information criteria, including Akaike's criterion.","As a concrete and focused exposition of this powerful evidence-based technique, we apply the relative belief procedure to an important application: state reconstruction of imperfect quantum sources.","In particular, just by comparing prior and posterior probabilities based on data, we demonstrate its capability of tracking multiphoton emissions using (realistically lossy) single-photon detectors in order to assess the actual quality of photon sources without making ad hoc assumptions, thereby reliably safeguarding source integrity for general quantum-information and communication tasks with Bayesian reasoning.","Finally, we discuss how relative belief can be exploited to carry out parametric model certification and estimate the total dimension of the quantum state for the combined (measured) physical and interacting external systems described by the Tavis--Cummings model."],"url":"http://arxiv.org/abs/2406.03343v1","category":"quant-ph"}
{"created":"2024-06-05 14:49:14","title":"A Flexible Recursive Network for Video Stereo Matching Based on Residual Estimation","abstract":"Due to the high similarity of disparity between consecutive frames in video sequences, the area where disparity changes is defined as the residual map, which can be calculated. Based on this, we propose RecSM, a network based on residual estimation with a flexible recursive structure for video stereo matching. The RecSM network accelerates stereo matching using a Multi-scale Residual Estimation Module (MREM), which employs the temporal context as a reference and rapidly calculates the disparity for the current frame by computing only the residual values between the current and previous frames. To further reduce the error of estimated disparities, we use the Disparity Optimization Module (DOM) and Temporal Attention Module (TAM) to enforce constraints between each module, and together with MREM, form a flexible Stackable Computation Structure (SCS), which allows for the design of different numbers of SCS based on practical scenarios. Experimental results demonstrate that with a stack count of 3, RecSM achieves a 4x speed improvement compared to ACVNet, running at 0.054 seconds based on one NVIDIA RTX 2080TI GPU, with an accuracy decrease of only 0.7%. Code is available at https://github.com/Y0uchenZ/RecSM.","sentences":["Due to the high similarity of disparity between consecutive frames in video sequences, the area where disparity changes is defined as the residual map, which can be calculated.","Based on this, we propose RecSM, a network based on residual estimation with a flexible recursive structure for video stereo matching.","The RecSM network accelerates stereo matching using a Multi-scale Residual Estimation Module (MREM), which employs the temporal context as a reference and rapidly calculates the disparity for the current frame by computing only the residual values between the current and previous frames.","To further reduce the error of estimated disparities, we use the Disparity Optimization Module (DOM) and Temporal Attention Module (TAM) to enforce constraints between each module, and together with MREM, form a flexible Stackable Computation Structure (SCS), which allows for the design of different numbers of SCS based on practical scenarios.","Experimental results demonstrate that with a stack count of 3, RecSM achieves a 4x speed improvement compared to ACVNet, running at 0.054 seconds based on one NVIDIA RTX 2080TI GPU, with an accuracy decrease of only 0.7%.","Code is available at https://github.com/Y0uchenZ/RecSM."],"url":"http://arxiv.org/abs/2406.03333v1","category":"cs.CV"}
{"created":"2024-06-05 14:38:30","title":"EngineBench: Flow Reconstruction in the Transparent Combustion Chamber III Optical Engine","abstract":"We present EngineBench, the first machine learning (ML) oriented database to use high quality experimental data for the study of turbulent flows inside combustion machinery. Prior datasets for ML in fluid mechanics are synthetic or use overly simplistic geometries. EngineBench is comprised of real-world particle image velocimetry (PIV) data that captures the turbulent airflow patterns in a specially-designed optical engine. However, in PIV data from internal flows, such as from engines, it is often challenging to achieve a full field of view and large occlusions can be present. In order to design optimal combustion systems, insight into the turbulent flows in these obscured areas is needed, which can be provided via inpainting models. Here we propose a novel inpainting task using random edge gaps, a technique that emphasises realism by introducing occlusions at random sizes and orientations at the edges of the PIV images. We test five ML methods on random edge gaps using pixel-wise, vector-based, and multi-scale performance metrics. We find that UNet-based models are more accurate than the industry-norm non-parametric approach and the context encoder at this task on both small and large gap sizes. The dataset and inpainting task presented in this paper support the development of more general-purpose pre-trained ML models for engine design problems. The method comparisons allow for more informed selection of ML models for problems in experimental flow diagnostics. All data and code are publicly available at https://eng.ox.ac.uk/tpsrg/research/enginebench/.","sentences":["We present EngineBench, the first machine learning (ML) oriented database to use high quality experimental data for the study of turbulent flows inside combustion machinery.","Prior datasets for ML in fluid mechanics are synthetic or use overly simplistic geometries.","EngineBench is comprised of real-world particle image velocimetry (PIV) data that captures the turbulent airflow patterns in a specially-designed optical engine.","However, in PIV data from internal flows, such as from engines, it is often challenging to achieve a full field of view and large occlusions can be present.","In order to design optimal combustion systems, insight into the turbulent flows in these obscured areas is needed, which can be provided via inpainting models.","Here we propose a novel inpainting task using random edge gaps, a technique that emphasises realism by introducing occlusions at random sizes and orientations at the edges of the PIV images.","We test five ML methods on random edge gaps using pixel-wise, vector-based, and multi-scale performance metrics.","We find that UNet-based models are more accurate than the industry-norm non-parametric approach and the context encoder at this task on both small and large gap sizes.","The dataset and inpainting task presented in this paper support the development of more general-purpose pre-trained ML models for engine design problems.","The method comparisons allow for more informed selection of ML models for problems in experimental flow diagnostics.","All data and code are publicly available at https://eng.ox.ac.uk/tpsrg/research/enginebench/."],"url":"http://arxiv.org/abs/2406.03325v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 14:37:42","title":"UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning","abstract":"The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance. However, we find that its principle can lead to overestimation phenomenon for the value function. In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error. Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics. At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model. Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks.","sentences":["The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance.","However, we find that its principle can lead to overestimation phenomenon for the value function.","In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error.","Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics.","At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model.","Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks."],"url":"http://arxiv.org/abs/2406.03324v1","category":"cs.LG"}
{"created":"2024-06-05 14:33:18","title":"Synchronized Optimal Transport for Joint Modeling of Dynamics Across Multiple Spaces","abstract":"Optimal transport has been an essential tool for reconstructing dynamics from complex data. With the increasingly available multifaceted data, a system can often be characterized across multiple spaces. Therefore, it is crucial to maintain coherence in the dynamics across these diverse spaces. To address this challenge, we introduce Synchronized Optimal Transport (SyncOT), a novel approach to jointly model dynamics that represent the same system through multiple spaces. With given correspondence between the spaces, SyncOT minimizes the aggregated cost of the dynamics induced across all considered spaces. The problem is discretized into a finite-dimensional convex problem using a staggered grid. Primal-dual algorithm-based approaches are then developed to solve the discretized problem. Various numerical experiments demonstrate the capabilities and properties of SyncOT and validate the effectiveness of the proposed algorithms.","sentences":["Optimal transport has been an essential tool for reconstructing dynamics from complex data.","With the increasingly available multifaceted data, a system can often be characterized across multiple spaces.","Therefore, it is crucial to maintain coherence in the dynamics across these diverse spaces.","To address this challenge, we introduce Synchronized Optimal Transport (SyncOT), a novel approach to jointly model dynamics that represent the same system through multiple spaces.","With given correspondence between the spaces, SyncOT minimizes the aggregated cost of the dynamics induced across all considered spaces.","The problem is discretized into a finite-dimensional convex problem using a staggered grid.","Primal-dual algorithm-based approaches are then developed to solve the discretized problem.","Various numerical experiments demonstrate the capabilities and properties of SyncOT and validate the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2406.03319v1","category":"math.OC"}
{"created":"2024-06-05 14:31:41","title":"Single-cycle, 643-mW average power THz source based on tilted pulse front in lithium niobate","abstract":"We present, to the best of our knowledge, the highest average power from a laser-driven single-cycle THz source demonstrated so far, using optical rectification in the titled pulse-front geometry in cryogenically cooled lithium niobate, pumped by a commercially available 500 W ultrafast thin-disk Yb-amplifier. We study repetition rate dependent effects in our setup at 100 kHz and 40 kHz at this high average power, revealing different optimal fluence conditions for efficient conversion. The demonstrated sources with multi-100 mW average power at these high repetition rates combine high THz pulse energies and high repetition rate and is thus ideally suited for nonlinear THz spectroscopy experiments with significantly reduced measurement times. The presented result is a first benchmark for high average power THz time domain spectroscopy systems for nonlinear spectroscopy, driven by very high average power ultrafast Yb lasers.","sentences":["We present, to the best of our knowledge, the highest average power from a laser-driven single-cycle THz source demonstrated so far, using optical rectification in the titled pulse-front geometry in cryogenically cooled lithium niobate, pumped by a commercially available 500 W ultrafast thin-disk Yb-amplifier.","We study repetition rate dependent effects in our setup at 100 kHz and 40 kHz at this high average power, revealing different optimal fluence conditions for efficient conversion.","The demonstrated sources with multi-100 mW average power at these high repetition rates combine high THz pulse energies and high repetition rate and is thus ideally suited for nonlinear THz spectroscopy experiments with significantly reduced measurement times.","The presented result is a first benchmark for high average power THz time domain spectroscopy systems for nonlinear spectroscopy, driven by very high average power ultrafast Yb lasers."],"url":"http://arxiv.org/abs/2406.03318v1","category":"physics.optics"}
