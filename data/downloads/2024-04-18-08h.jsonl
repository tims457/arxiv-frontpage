{"created":"2024-04-17 17:59:59","title":"Factorized Diffusion: Perceptual Illusions by Noise Decomposition","abstract":"Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling. For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts. This produces hybrid images, which change appearance depending on viewing distance. By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts. We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting. And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring. Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts. We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control. Finally, we show that we can extend our approach to generate hybrid images from real images. We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem.","sentences":["Given a factorization of an image into a sum of linear components, we present a zero-shot method to control each individual component through diffusion model sampling.","For example, we can decompose an image into low and high spatial frequencies and condition these components on different text prompts.","This produces hybrid images, which change appearance depending on viewing distance.","By decomposing an image into three frequency subbands, we can generate hybrid images with three prompts.","We also use a decomposition into grayscale and color components to produce images whose appearance changes when they are viewed in grayscale, a phenomena that naturally occurs under dim lighting.","And we explore a decomposition by a motion blur kernel, which produces images that change appearance under motion blurring.","Our method works by denoising with a composite noise estimate, built from the components of noise estimates conditioned on different prompts.","We also show that for certain decompositions, our method recovers prior approaches to compositional generation and spatial control.","Finally, we show that we can extend our approach to generate hybrid images from real images.","We do this by holding one component fixed and generating the remaining components, effectively solving an inverse problem."],"url":"http://arxiv.org/abs/2404.11615v1","category":"cs.CV"}
{"created":"2024-04-17 17:59:55","title":"Dynamic Typography: Bringing Words to Life","abstract":"Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed \"Dynamic Typography\", which combines two challenging tasks. It deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. Our technique harnesses vector graphics representations and an end-to-end optimization-based framework. This framework employs neural displacement fields to convert letters into base shapes and applies per-frame motion, encouraging coherence with the intended textual concept. Shape preservation techniques and perceptual loss regularization are employed to maintain legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our end-to-end methodology over baseline methods, which might comprise separate tasks. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability. Our code is available at: https://animate-your-word.github.io/demo/.","sentences":["Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives.","Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation.","We present an automated text animation scheme, termed \"Dynamic Typography\", which combines two challenging tasks.","It deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts.","Our technique harnesses vector graphics representations and an end-to-end optimization-based framework.","This framework employs neural displacement fields to convert letters into base shapes and applies per-frame motion, encouraging coherence with the intended textual concept.","Shape preservation techniques and perceptual loss regularization are employed to maintain legibility and structural integrity throughout the animation process.","We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our end-to-end methodology over baseline methods, which might comprise separate tasks.","Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability.","Our code is available at: https://animate-your-word.github.io/demo/."],"url":"http://arxiv.org/abs/2404.11614v1","category":"cs.CV"}
{"created":"2024-04-17 17:58:14","title":"Unlocking New Physics with Joint Power Spectrum and Voxel Intensity Distribution Forecasts in Line-Intensity Mapping","abstract":"The power spectrum and voxel intensity distribution (VID) are two summary statistics that can be applied to condense the information encoded in line-intensity maps. The information contained in both summary statistics is highly complementary, and their combination allows for a major increase in precision of parameter estimation from line-intensity mapping (LIM) surveys. Until recently, combination of these statistics required simulation-based estimations of their covariance. In this work we leverage an analytical model of covariance between these observables to run a joint Fisher forecast focusing on the CO(1-0) rotational line targeted by the COMAP survey and a wider, shallower hypothetical iteration. We consider a generalized phenomenological non-CDM model, models with axion dark matter, and local primordial non-Gaussianity, to highlight where a combined analysis of the power spectrum and VID can be most useful. Our results demonstrate improvements in sensitivity to beyond-$\\Lambda$CDM physics over analyses using either the power spectrum or VID on their own, by factors ranging from 2 to 50, showcasing the potential of joint analyses in unlocking new insights into fundamental physics with LIM surveys.","sentences":["The power spectrum and voxel intensity distribution (VID) are two summary statistics that can be applied to condense the information encoded in line-intensity maps.","The information contained in both summary statistics is highly complementary, and their combination allows for a major increase in precision of parameter estimation from line-intensity mapping (LIM) surveys.","Until recently, combination of these statistics required simulation-based estimations of their covariance.","In this work we leverage an analytical model of covariance between these observables to run a joint Fisher forecast focusing on the CO(1-0) rotational line targeted by the COMAP survey and a wider, shallower hypothetical iteration.","We consider a generalized phenomenological non-CDM model, models with axion dark matter, and local primordial non-Gaussianity, to highlight where a combined analysis of the power spectrum and VID can be most useful.","Our results demonstrate improvements in sensitivity to beyond-$\\Lambda$CDM physics over analyses using either the power spectrum or VID on their own, by factors ranging from 2 to 50, showcasing the potential of joint analyses in unlocking new insights into fundamental physics with LIM surveys."],"url":"http://arxiv.org/abs/2404.11609v1","category":"astro-ph.CO"}
{"created":"2024-04-17 17:55:17","title":"Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models","abstract":"We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks. Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions. The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones. We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems.","sentences":["We propose a self-supervised learning approach for solving the following constrained optimization task in log-linear models or Markov networks.","Let $f$ and $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and $\\mathbf{Y}$ of random variables respectively.","Given an assignment $\\mathbf{x}$ to all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the constrained most-probable explanation (CMPE) task seeks to find an assignment $\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x}, \\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our proposed self-supervised approach, given assignments $\\mathbf{x}$ to $\\mathbf{X}$ (data), we train a deep neural network that learns to output near-optimal solutions to the CMPE problem without requiring access to any pre-computed solutions.","The key idea in our approach is to use first principles and approximate inference methods for CMPE to derive novel loss functions that seek to push infeasible solutions towards feasible ones and feasible solutions towards optimal ones.","We analyze the properties of our proposed method and experimentally demonstrate its efficacy on several benchmark problems."],"url":"http://arxiv.org/abs/2404.11606v1","category":"cs.LG"}
{"created":"2024-04-17 17:54:49","title":"VG4D: Vision-Language Model Goes 4D Video Recognition","abstract":"Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems. However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information. Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks. However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem. In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network. Our approach involves aligning the 4D encoder's representation with a VLM to learn a shared visual and text space from training on large-scale image-text pairs. By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance. To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos. Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.","sentences":["Understanding the real world through point cloud video is a crucial aspect of robotics and autonomous driving systems.","However, prevailing methods for 4D point cloud recognition have limitations due to sensor resolution, which leads to a lack of detailed information.","Recent advances have shown that Vision-Language Models (VLM) pre-trained on web-scale text-image datasets can learn fine-grained visual concepts that can be transferred to various downstream tasks.","However, effectively integrating VLM into the domain of 4D point clouds remains an unresolved problem.","In this work, we propose the Vision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from visual-text pre-trained models to a 4D point cloud network.","Our approach involves aligning the 4D encoder's representation with a VLM to learn a shared visual and text space from training on large-scale image-text pairs.","By transferring the knowledge of the VLM to the 4D encoder and combining the VLM, our VG4D achieves improved recognition performance.","To enhance the 4D encoder, we modernize the classic dynamic point cloud backbone and propose an improved version of PSTNet, im-PSTNet, which can efficiently model point cloud videos.","Experiments demonstrate that our method achieves state-of-the-art performance for action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120 dataset.","Code is available at \\url{https://github.com/Shark0-0/VG4D}."],"url":"http://arxiv.org/abs/2404.11605v1","category":"cs.CV"}
{"created":"2024-04-17 17:52:02","title":"Isoparametric Virtual Element Methods","abstract":"We present two approaches to constructing isoparametric Virtual Element Methods of arbitrary order for linear elliptic partial differential equations on general two-dimensional domains. The first method approximates the variational problem transformed onto a computational reference domain. The second method computes a virtual domain and uses bespoke polynomial approximation operators to construct a computable method. Both methods are shown to converge optimally, a behaviour confirmed in practice for the solution of problems posed on curved domains.","sentences":["We present two approaches to constructing isoparametric Virtual Element Methods of arbitrary order for linear elliptic partial differential equations on general two-dimensional domains.","The first method approximates the variational problem transformed onto a computational reference domain.","The second method computes a virtual domain and uses bespoke polynomial approximation operators to construct a computable method.","Both methods are shown to converge optimally, a behaviour confirmed in practice for the solution of problems posed on curved domains."],"url":"http://arxiv.org/abs/2404.11603v1","category":"math.NA"}
{"created":"2024-04-17 17:51:15","title":"Gravitational waves from sub-solar mass primordial black holes","abstract":"Gravitational waves from inspiraling sub-solar mass compact objects would provide almost definitive evidence for the existence of primordial black holes. In this chapter, we explain why these exotic objects are interesting candidates for current and future gravitational-wave observatories, and provide detailed explanations of how they are searched for. We describe one method, matched filtering, to search for binaries with masses between $[0.01,1]M_\\odot$. Furthermore, since signals from inspiraling planetary- and asteroid-mass mass compact binaries ($[10^{-9},10^{-2}]M_\\odot$) would spend hours to years in the detector frequency band, we explain the novel pattern recognition techniques that have been developed to search for them. Finally, we describe extreme mass ratio inspiral (EMRI) systems, and how these will be searched for in future space-based detectors. For all mass regimes, we comment on the prospects for detection.","sentences":["Gravitational waves from inspiraling sub-solar mass compact objects would provide almost definitive evidence for the existence of primordial black holes.","In this chapter, we explain why these exotic objects are interesting candidates for current and future gravitational-wave observatories, and provide detailed explanations of how they are searched for.","We describe one method, matched filtering, to search for binaries with masses between $[0.01,1]M_\\odot$. Furthermore, since signals from inspiraling planetary- and asteroid-mass mass compact binaries ($[10^{-9},10^{-2}]M_\\odot$) would spend hours to years in the detector frequency band, we explain the novel pattern recognition techniques that have been developed to search for them.","Finally, we describe extreme mass ratio inspiral (EMRI) systems, and how these will be searched for in future space-based detectors.","For all mass regimes, we comment on the prospects for detection."],"url":"http://arxiv.org/abs/2404.11601v1","category":"gr-qc"}
{"created":"2024-04-17 17:50:31","title":"Super Yang-Mills on Branched Covers and Weighted Projective Spaces","abstract":"In this work we conjecture the Coulomb branch partition function, including flux and instanton contributions, for the $\\mathcal{N}=2$ vector multiplet on weighted projective space $\\mathbb{CP}^2_{\\boldsymbol{N}}$ for equivariant Donaldson-Witten and ``Pestun-like'' theories. More precisely, we claim that this partition function agrees with the one computed on a certain branched cover of $\\mathbb{CP}^2$ upon matching conical deficit angles with corresponding branch indices. Our conjecture is substantiated by checking that similar partition functions on spindles agree with their equivalent on certain branched covers of $\\mathbb{CP}^1$. We compute the one-loop determinant on the branched cover of $\\mathbb{CP}^2$ for all flux sectors via dimensional reduction from the $\\mathcal{N}=1$ vector multiplet on a branched five-sphere along a free $S^1$-action. This work paves the way for obtaining partition functions on more generic symplectic toric orbifolds.","sentences":["In this work we conjecture the Coulomb branch partition function, including flux and instanton contributions, for the $\\mathcal{N}=2$ vector multiplet on weighted projective space $\\mathbb{CP}^2_{\\boldsymbol{N}}$ for equivariant Donaldson-Witten and ``Pestun-like'' theories.","More precisely, we claim that this partition function agrees with the one computed on a certain branched cover of $\\mathbb{CP}^2$ upon matching conical deficit angles with corresponding branch indices.","Our conjecture is substantiated by checking that similar partition functions on spindles agree with their equivalent on certain branched covers of $\\mathbb{CP}^1$. We compute the one-loop determinant on the branched cover of $\\mathbb{CP}^2$ for all flux sectors via dimensional reduction from the $\\mathcal{N}=1$ vector multiplet on a branched five-sphere along a free $S^1$-action.","This work paves the way for obtaining partition functions on more generic symplectic toric orbifolds."],"url":"http://arxiv.org/abs/2404.11600v1","category":"hep-th"}
{"created":"2024-04-17 17:49:43","title":"Lagrangian families of Bridgeland moduli spaces from Gushel-Mukai fourfolds and double EPW cubes","abstract":"Let $X$ be a very general Gushel-Mukai (GM) variety of dimension $n\\geq 4$, and let $Y$ be a smooth hyperplane section. There are natural pull-back and push-forward functors between the semi-orthogonal components (known as the Kuznetsov components) of the derived categories of $X$ and $Y$. In the first part of the paper, we prove that the Bridgeland stability of objects is preserved by both pull-back and push-forward functors. We then explore various applications of this result, such as constructing an 8-dimensional smooth family of Lagrangian subvarieties for each moduli space of stable objects in the Kuznetsov component of a very general GM fourfold, and proving the projectivity of the moduli spaces of semistable objects of any class in the Kuznetsov component of a general GM threefold, as conjectured by Perry, Pertusi, and Zhao.   In the second part, we study in detail the double EPW cube, a 6-dimensional hyperk\\\"ahler manifold associated with a very general GM fourfold $X$, through the Bridgeland moduli space, and show that it is the maximal rationally connected (MRC) quotient of the Hilbert scheme of twisted cubics on $X$. We also prove that it admits a covering by Lagrangian subvarieties, constructed from the Hilbert schemes of twisted cubics on GM threefolds.","sentences":["Let $X$ be a very general Gushel-Mukai (GM) variety of dimension $n\\geq 4$, and let $Y$ be a smooth hyperplane section.","There are natural pull-back and push-forward functors between the semi-orthogonal components (known as the Kuznetsov components) of the derived categories of $X$ and $Y$. In the first part of the paper, we prove that the Bridgeland stability of objects is preserved by both pull-back and push-forward functors.","We then explore various applications of this result, such as constructing an 8-dimensional smooth family of Lagrangian subvarieties for each moduli space of stable objects in the Kuznetsov component of a very general GM fourfold, and proving the projectivity of the moduli spaces of semistable objects of any class in the Kuznetsov component of a general GM threefold, as conjectured by Perry, Pertusi, and Zhao.   ","In the second part, we study in detail the double EPW cube, a 6-dimensional hyperk\\\"ahler manifold associated with a very general GM fourfold $X$, through the Bridgeland moduli space, and show that it is the maximal rationally connected (MRC) quotient of the Hilbert scheme of twisted cubics on $X$. We also prove that it admits a covering by Lagrangian subvarieties, constructed from the Hilbert schemes of twisted cubics on GM threefolds."],"url":"http://arxiv.org/abs/2404.11598v1","category":"math.AG"}
{"created":"2024-04-17 17:49:38","title":"Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review","abstract":"As the manufacturing industry advances with sensor integration and automation, the opaque nature of deep learning models in machine learning poses a significant challenge for fault detection and diagnosis. And despite the related predictive insights Artificial Intelligence (AI) can deliver, advanced machine learning engines often remain a black box. This paper reviews the eXplainable AI (XAI) tools and techniques in this context. We explore various XAI methodologies, focusing on their role in making AI decision-making transparent, particularly in critical scenarios where humans are involved. We also discuss current limitations and potential future research that aims to balance explainability with model performance while improving trustworthiness in the context of AI applications for critical industrial use cases.","sentences":["As the manufacturing industry advances with sensor integration and automation, the opaque nature of deep learning models in machine learning poses a significant challenge for fault detection and diagnosis.","And despite the related predictive insights Artificial Intelligence (AI) can deliver, advanced machine learning engines often remain a black box.","This paper reviews the eXplainable AI (XAI) tools and techniques in this context.","We explore various XAI methodologies, focusing on their role in making AI decision-making transparent, particularly in critical scenarios where humans are involved.","We also discuss current limitations and potential future research that aims to balance explainability with model performance while improving trustworthiness in the context of AI applications for critical industrial use cases."],"url":"http://arxiv.org/abs/2404.11597v1","category":"cs.AI"}
{"created":"2024-04-17 17:49:08","title":"Urban highways are barriers to social ties","abstract":"Urban highways are common, especially in the US, making cities more car-centric. They promise the annihilation of distance but obstruct pedestrian mobility, thus playing a key role in limiting social interactions locally. Although this limiting role is widely acknowledged in urban studies, the quantitative relationship between urban highways and social ties is barely tested. Here we define a Barrier Score that relates massive, geolocated online social network data to highways in the 50 largest US cities. At the unprecedented granularity of individual social ties, we show that urban highways are associated with decreased social connectivity. This barrier effect is especially strong for short distances and consistent with historical cases of highways that were built to purposefully disrupt or isolate Black neighborhoods. By combining spatial infrastructure with social tie data, our method adds a new dimension to demographic studies of social segregation. Our study can inform reparative planning for an evidence-based reduction of spatial inequality, and more generally, support a better integration of the social fabric in urban planning.","sentences":["Urban highways are common, especially in the US, making cities more car-centric.","They promise the annihilation of distance but obstruct pedestrian mobility, thus playing a key role in limiting social interactions locally.","Although this limiting role is widely acknowledged in urban studies, the quantitative relationship between urban highways and social ties is barely tested.","Here we define a Barrier Score that relates massive, geolocated online social network data to highways in the 50 largest US cities.","At the unprecedented granularity of individual social ties, we show that urban highways are associated with decreased social connectivity.","This barrier effect is especially strong for short distances and consistent with historical cases of highways that were built to purposefully disrupt or isolate Black neighborhoods.","By combining spatial infrastructure with social tie data, our method adds a new dimension to demographic studies of social segregation.","Our study can inform reparative planning for an evidence-based reduction of spatial inequality, and more generally, support a better integration of the social fabric in urban planning."],"url":"http://arxiv.org/abs/2404.11596v1","category":"physics.soc-ph"}
{"created":"2024-04-17 17:48:18","title":"A Deep Dive into Large Language Models for Automated Bug Localization and Repair","abstract":"Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug fixing utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment unit, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J.","sentences":["Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR).","In this study, we take a deep dive into automated bug fixing utilizing LLMs.","In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing.","This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases.","We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment unit, and a bug-fixing model.","Toggle takes a buggy function as input and generates a complete corrected function.","We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others.","Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J."],"url":"http://arxiv.org/abs/2404.11595v1","category":"cs.SE"}
{"created":"2024-04-17 17:45:39","title":"Generation of Low-Inclination, Neptune-Crossing TNOs by Planet Nine","abstract":"The solar system's distant reaches exhibit a wealth of anomalous dynamical structure, hinting at the presence of a yet-undetected, massive trans-Neptunian body - Planet 9. Previous analyses have shown how orbital evolution induced by this object can explain the origins of a broad assortment of exotic orbits, ranging from those characterized by high perihelia to those with extreme inclinations. In this work, we shift the focus toward a more conventional class of TNOs, and consider the observed census of long-period, nearly planar, Neptune-crossing objects as a hitherto-unexplored probe of the Planet 9 hypothesis. To this end, we carry out comprehensive $N-$body simulations that self-consistently model gravitational perturbations from all giant planets, the Galactic tide, as well as passing stars, stemming from initial conditions that account for the primordial giant planet migration and sun's early evolution within a star cluster. Accounting for observational biases, our results reveal that the orbital architecture of this group of objects aligns closely with the predictions of the P9-inclusive model. In stark contrast, the P9-free scenario is statistically rejected at a $\\sim5\\,\\sigma$ confidence-level. Accordingly, this work introduces a new line of evidence supporting the existence of Planet 9 and further delineates a series of observational predictions poised for near-term resolution.","sentences":["The solar system's distant reaches exhibit a wealth of anomalous dynamical structure, hinting at the presence of a yet-undetected, massive trans-Neptunian body - Planet 9.","Previous analyses have shown how orbital evolution induced by this object can explain the origins of a broad assortment of exotic orbits, ranging from those characterized by high perihelia to those with extreme inclinations.","In this work, we shift the focus toward a more conventional class of TNOs, and consider the observed census of long-period, nearly planar, Neptune-crossing objects as a hitherto-unexplored probe of the Planet 9 hypothesis.","To this end, we carry out comprehensive $N-$body simulations that self-consistently model gravitational perturbations from all giant planets, the Galactic tide, as well as passing stars, stemming from initial conditions that account for the primordial giant planet migration and sun's early evolution within a star cluster.","Accounting for observational biases, our results reveal that the orbital architecture of this group of objects aligns closely with the predictions of the P9-inclusive model.","In stark contrast, the P9-free scenario is statistically rejected at a $\\sim5\\,\\sigma$ confidence-level.","Accordingly, this work introduces a new line of evidence supporting the existence of Planet 9 and further delineates a series of observational predictions poised for near-term resolution."],"url":"http://arxiv.org/abs/2404.11594v1","category":"astro-ph.EP"}
{"created":"2024-04-17 17:45:08","title":"IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination","abstract":"This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at https://zju3dv.github.io/IntrinsicAnything.","sentences":["This paper aims to recover object materials from posed images captured under an unknown static lighting condition.","Recent methods solve this task by optimizing material parameters through differentiable physically based rendering.","However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results.","To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process.","We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular.","Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images.","In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results.","Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery.","The code will be available at https://zju3dv.github.io/IntrinsicAnything."],"url":"http://arxiv.org/abs/2404.11593v1","category":"cs.CV"}
{"created":"2024-04-17 17:42:48","title":"The EDGE Language: Extended General Einsums for Graph Algorithms","abstract":"In this work, we propose a unified abstraction for graph algorithms: the Extended General Einsums language, or EDGE. The EDGE language expresses graph algorithms in the language of tensor algebra, providing a rigorous, succinct, and expressive mathematical framework. EDGE leverages two ideas: (1) the well-known foundations provided by the graph-matrix duality, where a graph is simply a 2D tensor, and (2) the power and expressivity of Einsum notation in the tensor algebra world. In this work, we describe our design goals for EDGE and walk through the extensions we add to Einsums to support more complex operations common in graph algorithms. Additionally, we provide a few examples of how to express graph algorithms in our proposed notation. We hope that a single, mathematical notation for graph algorithms will (1) allow researchers to more easily compare different algorithms and different implementations of a graph algorithm; (2) enable developers to factor complexity by separating the concerns of what to compute (described with the extended Einsum notation) from the lower level details of how to compute; and (3) enable the discovery of different algorithmic variants of a problem through algebraic manipulations and transformations on a given EDGE expression.","sentences":["In this work, we propose a unified abstraction for graph algorithms: the Extended General Einsums language, or EDGE.","The EDGE language expresses graph algorithms in the language of tensor algebra, providing a rigorous, succinct, and expressive mathematical framework.","EDGE leverages two ideas: (1) the well-known foundations provided by the graph-matrix duality, where a graph is simply a 2D tensor, and (2) the power and expressivity of Einsum notation in the tensor algebra world.","In this work, we describe our design goals for EDGE and walk through the extensions we add to Einsums to support more complex operations common in graph algorithms.","Additionally, we provide a few examples of how to express graph algorithms in our proposed notation.","We hope that a single, mathematical notation for graph algorithms will (1) allow researchers to more easily compare different algorithms and different implementations of a graph algorithm; (2) enable developers to factor complexity by separating the concerns of what to compute (described with the extended Einsum notation) from the lower level details of how to compute; and (3) enable the discovery of different algorithmic variants of a problem through algebraic manipulations and transformations on a given EDGE expression."],"url":"http://arxiv.org/abs/2404.11591v1","category":"cs.DS"}
{"created":"2024-04-17 17:38:56","title":"Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding","abstract":"The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality. However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects. For example, text encoders can hardly express \"peace\", while can easily illustrate olive branches and white doves. This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts. We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts. The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects. Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts. Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts. We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation.","sentences":["The rapid evolution of text-to-image diffusion models has opened the door of generative AI, enabling the translation of textual descriptions into visually compelling images with remarkable quality.","However, a persistent challenge within this domain is the optimization of prompts to effectively convey abstract concepts into concrete objects.","For example, text encoders can hardly express \"peace\", while can easily illustrate olive branches and white doves.","This paper introduces a novel approach named Prompt Optimizer for Abstract Concepts (POAC) specifically designed to enhance the performance of text-to-image diffusion models in interpreting and generating images from abstract concepts.","We propose a Prompt Language Model (PLM), which is initialized from a pre-trained language model, and then fine-tuned with a curated dataset of abstract concept prompts.","The dataset is created with GPT-4 to extend the abstract concept to a scene and concrete objects.","Our framework employs a Reinforcement Learning (RL)-based optimization strategy, focusing on the alignment between the generated images by a stable diffusion model and optimized prompts.","Through extensive experiments, we demonstrate that our proposed POAC significantly improves the accuracy and aesthetic quality of generated images, particularly in the description of abstract concepts and alignment with optimized prompts.","We also present a comprehensive analysis of our model's performance across diffusion models under different settings, showcasing its versatility and effectiveness in enhancing abstract concept representation."],"url":"http://arxiv.org/abs/2404.11589v1","category":"cs.CV"}
{"created":"2024-04-17 17:37:30","title":"Related Work and Citation Text Generation: A Survey","abstract":"To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work. This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models. Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches. In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG.","sentences":["To convince readers of the novelty of their research paper, authors must perform a literature review and compose a coherent story that connects and relates prior works to the current work.","This challenging nature of literature review writing makes automatic related work generation (RWG) academically and computationally interesting, and also makes it an excellent test bed for examining the capability of SOTA natural language processing (NLP) models.","Since the initial proposal of the RWG task, its popularity has waxed and waned, following the capabilities of mainstream NLP approaches.","In this work, we survey the zoo of RWG historical works, summarizing the key approaches and task definitions and discussing the ongoing challenges of RWG."],"url":"http://arxiv.org/abs/2404.11588v1","category":"cs.CL"}
{"created":"2024-04-17 17:33:48","title":"Ring momentum distributions as a general feature of Vlasov dynamics in the synchrotron dominated regime","abstract":"We study how radiation reaction leads plasmas initially in kinetic equilibrium to develop features in momentum space, such as anisotropies and population inversion, resulting in a ring-shaped momentum distribution that can drive kinetic instabilities. We employ the Landau-Lifshiftz radiation reaction model for a plasma in a strong magnetic field, and we obtain the necessary condition for the development of population inversion, we show that isotropic Maxwellian and Maxwell-J\\\"uttner plasmas, with thermal temperature $T>m_e c^2/\\sqrt{3}$, will develop a ring-like momentum distribution. The timescales and features for forming ring-shaped momentum distributions, the effect of collisions and non-uniform magnetic fields are disscussed, and compared with typical astrophysical and laboratory plasmas parameters. Our results show the pervasiveness of ring-like momentum distribution functions in synchrotron dominated plasma conditions.","sentences":["We study how radiation reaction leads plasmas initially in kinetic equilibrium to develop features in momentum space, such as anisotropies and population inversion, resulting in a ring-shaped momentum distribution that can drive kinetic instabilities.","We employ the Landau-Lifshiftz radiation reaction model for a plasma in a strong magnetic field, and we obtain the necessary condition for the development of population inversion, we show that isotropic Maxwellian and Maxwell-J\\\"uttner plasmas, with thermal temperature $T>m_e","c^2/\\sqrt{3}$, will develop a ring-like momentum distribution.","The timescales and features for forming ring-shaped momentum distributions, the effect of collisions and non-uniform magnetic fields are disscussed, and compared with typical astrophysical and laboratory plasmas parameters.","Our results show the pervasiveness of ring-like momentum distribution functions in synchrotron dominated plasma conditions."],"url":"http://arxiv.org/abs/2404.11586v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 17:33:32","title":"Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition","abstract":"Handwritten Text Recognition (HTR) is a relevant problem in computer vision, and implies unique challenges owing to its inherent variability and the rich contextualization required for its interpretation. Despite the success of Self-Supervised Learning (SSL) in computer vision, its application to HTR has been rather scattered, leaving key SSL methodologies unexplored. This work focuses on one of them, namely Spatial Context-based SSL. We investigate how this family of approaches can be adapted and optimized for HTR and propose new workflows that leverage the unique features of handwritten text. Our experiments demonstrate that the methods considered lead to advancements in the state-of-the-art of SSL for HTR in a number of benchmark cases.","sentences":["Handwritten Text Recognition (HTR) is a relevant problem in computer vision, and implies unique challenges owing to its inherent variability and the rich contextualization required for its interpretation.","Despite the success of Self-Supervised Learning (SSL) in computer vision, its application to HTR has been rather scattered, leaving key SSL methodologies unexplored.","This work focuses on one of them, namely Spatial Context-based SSL.","We investigate how this family of approaches can be adapted and optimized for HTR and propose new workflows that leverage the unique features of handwritten text.","Our experiments demonstrate that the methods considered lead to advancements in the state-of-the-art of SSL for HTR in a number of benchmark cases."],"url":"http://arxiv.org/abs/2404.11585v1","category":"cs.AI"}
{"created":"2024-04-17 17:32:41","title":"The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey","abstract":"This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.","sentences":["This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities.","The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design.","We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal.","Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems."],"url":"http://arxiv.org/abs/2404.11584v1","category":"cs.AI"}
{"created":"2024-04-17 17:29:03","title":"Quasinormal Modes in Modified Gravity using Physics-Informed Neural Networks","abstract":"In this paper, we apply a novel approach based on physics-informed neural networks to the computation of quasinormal modes of black hole solutions in modified gravity. In particular, we focus on the case of Einstein-scalar-Gauss-Bonnet theory, with several choices of the coupling function between the scalar field and the Gauss-Bonnet invariant. This type of calculation introduces a number of challenges with respect to the case of General Relativity, mainly due to the extra complexity of the perturbation equations and to the fact that the background solution is known only numerically. The solution of these perturbation equations typically requires sophisticated numerical techniques that are not easy to develop in computational codes. We show that physics-informed neural networks have an accuracy which is comparable to traditional numerical methods in the case of numerical backgrounds, while being very simple to implement. Additionally, the use of GPU parallelization is straightforward thanks to the use of standard machine learning environments.","sentences":["In this paper, we apply a novel approach based on physics-informed neural networks to the computation of quasinormal modes of black hole solutions in modified gravity.","In particular, we focus on the case of Einstein-scalar-Gauss-Bonnet theory, with several choices of the coupling function between the scalar field and the Gauss-Bonnet invariant.","This type of calculation introduces a number of challenges with respect to the case of General Relativity, mainly due to the extra complexity of the perturbation equations and to the fact that the background solution is known only numerically.","The solution of these perturbation equations typically requires sophisticated numerical techniques that are not easy to develop in computational codes.","We show that physics-informed neural networks have an accuracy which is comparable to traditional numerical methods in the case of numerical backgrounds, while being very simple to implement.","Additionally, the use of GPU parallelization is straightforward thanks to the use of standard machine learning environments."],"url":"http://arxiv.org/abs/2404.11583v1","category":"gr-qc"}
{"created":"2024-04-17 17:28:05","title":"LLMTune: Accelerate Database Knob Tuning with Large Language Models","abstract":"Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads. DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations. Consequently, many machine learning-based tuning methods have been developed to automate this process. Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive. This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning. Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers. Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads. These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes. To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs. We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB. In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations. For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations.","sentences":["Database knob tuning is a critical challenge in the database community, aiming to optimize knob values to enhance database performance for specific workloads.","DBMS often feature hundreds of tunable knobs, posing a significant challenge for DBAs to recommend optimal configurations.","Consequently, many machine learning-based tuning methods have been developed to automate this process.","Despite the introduction of various optimizers, practical applications have unveiled a new problem: they typically require numerous workload runs to achieve satisfactory performance, a process that is both time-consuming and resource-intensive.","This inefficiency largely stems from the optimal configuration often being substantially different from the default setting, necessitating multiple iterations during tuning.","Recognizing this, we argue that an effective starting point could significantly reduce redundant exploration in less efficient areas, thereby potentially speeding up the tuning process for the optimizers.","Based on this assumption, we introduce LLMTune, a large language model-based configuration generator designed to produce an initial, high-quality configuration for new workloads.","These generated configurations can then serve as starting points for various base optimizers, accelerating their tuning processes.","To obtain training data for LLMTune's supervised fine-tuning, we have devised a new automatic data generation framework capable of efficiently creating a large number of <workload, configuration> pairs.","We have conducted thorough experiments to evaluate LLMTune's effectiveness with different workloads, such as TPC-H and JOB.","In comparison to leading methods, LLMTune demonstrates a quicker ability to identify superior configurations.","For instance, with the challenging TPC-H workload, our LLMTune achieves a significant 15.6x speed-up ratio in finding the best-performing configurations."],"url":"http://arxiv.org/abs/2404.11581v1","category":"cs.AI"}
{"created":"2024-04-17 17:24:44","title":"Deep Policy Optimization with Temporal Logic Constraints","abstract":"Temporal logics, such as linear temporal logic (LTL), offer a precise means of specifying tasks for (deep) reinforcement learning (RL) agents. In our work, we consider the setting where the task is specified by an LTL objective and there is an additional scalar reward that we need to optimize. Previous works focus either on learning a LTL task-satisfying policy alone or are restricted to finite state spaces. We make two contributions: First, we introduce an RL-friendly approach to this setting by formulating this problem as a single optimization objective. Our formulation guarantees that an optimal policy will be reward-maximal from the set of policies that maximize the likelihood of satisfying the LTL specification. Second, we address a sparsity issue that often arises for LTL-guided Deep RL policies by introducing Cycle Experience Replay (CyclER), a technique that automatically guides RL agents towards the satisfaction of an LTL specification. Our experiments demonstrate the efficacy of CyclER in finding performant deep RL policies in both continuous and discrete experimental domains.","sentences":["Temporal logics, such as linear temporal logic (LTL), offer a precise means of specifying tasks for (deep) reinforcement learning (RL) agents.","In our work, we consider the setting where the task is specified by an LTL objective and there is an additional scalar reward that we need to optimize.","Previous works focus either on learning a LTL task-satisfying policy alone or are restricted to finite state spaces.","We make two contributions: First, we introduce an RL-friendly approach to this setting by formulating this problem as a single optimization objective.","Our formulation guarantees that an optimal policy will be reward-maximal from the set of policies that maximize the likelihood of satisfying the LTL specification.","Second, we address a sparsity issue that often arises for LTL-guided Deep RL policies by introducing Cycle Experience Replay (CyclER), a technique that automatically guides RL agents towards the satisfaction of an LTL specification.","Our experiments demonstrate the efficacy of CyclER in finding performant deep RL policies in both continuous and discrete experimental domains."],"url":"http://arxiv.org/abs/2404.11578v1","category":"cs.LG"}
{"created":"2024-04-17 17:20:27","title":"Towards Reliable Empirical Machine Unlearning Evaluation: A Game-Theoretic View","abstract":"Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability. Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries. Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments. This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.","sentences":["Machine unlearning is the process of updating machine learning models to remove the information of specific training data samples, in order to comply with data protection regulations that allow individuals to request the removal of their personal data.","Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question.","In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics that lack reliability.","Specifically, we propose a game-theoretic framework that formalizes the evaluation process as a game between unlearning algorithms and MIA adversaries, measuring the data removal efficacy of unlearning algorithms by the capability of the MIA adversaries.","Through careful design of the game, we demonstrate that the natural evaluation metric induced from the game enjoys provable guarantees that the existing evaluation metrics fail to satisfy.","Furthermore, we propose a practical and efficient algorithm to estimate the evaluation metric induced from the game, and demonstrate its effectiveness through both theoretical analysis and empirical experiments.","This work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques."],"url":"http://arxiv.org/abs/2404.11577v1","category":"cs.LG"}
{"created":"2024-04-17 17:19:48","title":"State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend","abstract":"Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment. Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space. Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient. However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge. In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction. Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced. In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial. Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames. Experimental results demonstrate that our model outperforms baselines on multiple datasets.","sentences":["Stochastic video prediction enables the consideration of uncertainty in future motion, thereby providing a better reflection of the dynamic nature of the environment.","Stochastic video prediction methods based on image auto-regressive recurrent models need to feed their predictions back into the latent space.","Conversely, the state-space models, which decouple frame synthesis and temporal prediction, proves to be more efficient.","However, inferring long-term temporal information about motion and generalizing to dynamic scenarios under non-stationary assumptions remains an unresolved challenge.","In this paper, we propose a state-space decomposition stochastic video prediction model that decomposes the overall video frame generation into deterministic appearance prediction and stochastic motion prediction.","Through adaptive decomposition, the model's generalization capability to dynamic scenarios is enhanced.","In the context of motion prediction, obtaining a prior on the long-term trend of future motion is crucial.","Thus, in the stochastic motion prediction branch, we infer the long-term motion trend from conditional frames to guide the generation of future frames that exhibit high consistency with the conditional frames.","Experimental results demonstrate that our model outperforms baselines on multiple datasets."],"url":"http://arxiv.org/abs/2404.11576v1","category":"cs.CV"}
{"created":"2024-04-17 17:13:52","title":"The Low-Mass Stellar Initial Mass Function in Nearby Ultra-Faint Dwarf Galaxies","abstract":"The stellar initial mass function (IMF) describes the distribution of stellar masses that form in a given star-formation event. The long main-sequence lifetimes of low-mass stars mean that the IMF in this regime (below $\\sim~1~\\rm{M}_\\odot$) can be investigated through star counts. Ultra-faint dwarf galaxies are low luminosity systems with ancient, metal poor stellar populations. We investigate the low-mass IMF in four such systems (Reticulum II, Ursa Major II, Triangulum II, and Segue 1), using Hubble Space Telescope imaging data that reaches to $\\lesssim~0.2~\\rm{M}_\\odot$ in each galaxy. The analysis techniques that we adopt depend on the number of low-mass stars in each sample. We use Kolmogorov-Smirnov tests for all four galaxies to determine whether their observed apparent magnitude distributions can reject a given combination of IMF parameters and binary fraction for the underlying population. We forward model a thousand synthetic populations for each combination of parameters, and reject those parameters only if each of the thousand realizations reject the null hypothesis. We find that all four galaxies reject a variety of IMFs, and the IMFs that they cannot reject include those that are identical, or similar, to that of the stellar populations of the Milky Way. We determine the best-fit parameter values for the IMF in Reticulum II and Ursa Major II and find that the IMF in Reticulum II is generally consistent with that of the Milky Way, while the IMF in Ursa Major II is more bottom-heavy. The interpretation of the results for Ursa Major II is complicated by possible contamination from two known background galaxy clusters.","sentences":["The stellar initial mass function (IMF) describes the distribution of stellar masses that form in a given star-formation event.","The long main-sequence lifetimes of low-mass stars mean that the IMF in this regime (below $\\sim~1~\\rm{M}_\\odot$) can be investigated through star counts.","Ultra-faint dwarf galaxies are low luminosity systems with ancient, metal poor stellar populations.","We investigate the low-mass IMF in four such systems (Reticulum II, Ursa Major II, Triangulum II, and Segue 1), using Hubble Space Telescope imaging data that reaches to $\\lesssim~0.2~\\rm{M}_\\odot$ in each galaxy.","The analysis techniques that we adopt depend on the number of low-mass stars in each sample.","We use Kolmogorov-Smirnov tests for all four galaxies to determine whether their observed apparent magnitude distributions can reject a given combination of IMF parameters and binary fraction for the underlying population.","We forward model a thousand synthetic populations for each combination of parameters, and reject those parameters only if each of the thousand realizations reject the null hypothesis.","We find that all four galaxies reject a variety of IMFs, and the IMFs that they cannot reject include those that are identical, or similar, to that of the stellar populations of the Milky Way.","We determine the best-fit parameter values for the IMF in Reticulum II and Ursa Major II and find that the IMF in Reticulum II is generally consistent with that of the Milky Way, while the IMF in Ursa Major II is more bottom-heavy.","The interpretation of the results for Ursa Major II is complicated by possible contamination from two known background galaxy clusters."],"url":"http://arxiv.org/abs/2404.11571v1","category":"astro-ph.GA"}
{"created":"2024-04-17 17:13:43","title":"Explaining Fermions Mass and Mixing Hierarchies through $U(1)_X$ and $Z_2$ Symmetries","abstract":"For understanding the hierarchies of fermion masses and mixing, we extend the standard model gauge group with $U(1)_X$ and $Z_2$ symmetry. The field content of the Standard model is augmented by three heavy right-handed neutrinos and two new scalar singlets. $U(1)_X$ charges of different fields are considered after satisfying anomaly cancellation conditions. In this scenario, the fermion masses are generated through higher dimensional effective operators. The small neutrino masses are obtained through type-1 seesaw mechanism using the heavy right handed neutrino fields. We discuss the flavor-changing neutral current processes which is originated due to the sequential nature of $U(1)_X$ symmetry. We have written effective higher dimensional operators in terms of renormalizable dimension four operators by introducing vector like fermions.","sentences":["For understanding the hierarchies of fermion masses and mixing, we extend the standard model gauge group with $U(1)_X$ and $Z_2$ symmetry.","The field content of the Standard model is augmented by three heavy right-handed neutrinos and two new scalar singlets.","$U(1)_X$ charges of different fields are considered after satisfying anomaly cancellation conditions.","In this scenario, the fermion masses are generated through higher dimensional effective operators.","The small neutrino masses are obtained through type-1 seesaw mechanism using the heavy right handed neutrino fields.","We discuss the flavor-changing neutral current processes which is originated due to the sequential nature of $U(1)_X$ symmetry.","We have written effective higher dimensional operators in terms of renormalizable dimension four operators by introducing vector like fermions."],"url":"http://arxiv.org/abs/2404.11570v1","category":"hep-ph"}
{"created":"2024-04-17 17:11:31","title":"On the Scalability of GNNs for Molecular Graphs","abstract":"Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold. We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.","sentences":["Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation.","Practitioners have observed a strong relationship between model size, dataset size, and performance.","However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to the lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures.","We address this drawback of GNNs by studying their scaling behavior.","Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs.","For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules, number of labels, and the diversity in the pretraining datasets, resulting in a 30.25% improvement when scaling to 1 billion parameters and 28.98% improvement when increasing size of dataset to eightfold.","We further demonstrate strong finetuning scaling behavior on 38 tasks, outclassing previous large models.","We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery."],"url":"http://arxiv.org/abs/2404.11568v1","category":"cs.LG"}
{"created":"2024-04-17 17:08:05","title":"MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation","abstract":"We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA). Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch. MoA is designed to retain the original model's prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch. A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation. Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. Crucially, MoA enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable. Project page: https://snap-research.github.io/mixture-of-attention","sentences":["We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA).","Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch.","MoA is designed to retain the original model's prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch.","A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation.","Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model.","Crucially, MoA enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable.","Project page: https://snap-research.github.io/mixture-of-attention"],"url":"http://arxiv.org/abs/2404.11565v1","category":"cs.CV"}
{"created":"2024-04-17 17:06:34","title":"Non-linear conductances of Galton-Watson trees and application to the (near) critical random cluster model","abstract":"When considering statistical mechanics models on trees, such that the Ising model, percolation, or more generally the random cluster model, some concave tree recursions naturally emerge. Some of these recursions can be compared with non-linear conductances, or $p$-conductances, between the root and the leaves of the tree. In this article, we estimate the $p$-conductances of $T_n$, a supercritical Galton-Watson tree of depth $n$, for any $p>1$ (for a quenched realization of $T_n$). In particular, we find the sharp asymptotic behavior when $n$ goes to infinity, which depends on whether the offspring distribution admits a finite moment of order $q$, where $q=\\frac{p}{p-1}$ is the conjugate exponent of $p$. We then apply our results to the random cluster model on~$T_n$ (with wired boundary condition) and provide sharp estimates on the probability that the root is connected to the leaves. As an example, for the Ising model on $T_n$ with plus boundary conditions on the leaves, we find that, at criticality, the quenched magnetization of the root decays like: (i) $n^{-1/2}$ times an explicit tree-dependent constant if the offspring distribution admits a finite moment of order $3$; (ii) $n^{-1/(\\alpha-1)}$ if the offspring distribution has a heavy tail with exponent $\\alpha \\in (1,3)$.","sentences":["When considering statistical mechanics models on trees, such that the Ising model, percolation, or more generally the random cluster model, some concave tree recursions naturally emerge.","Some of these recursions can be compared with non-linear conductances, or $p$-conductances, between the root and the leaves of the tree.","In this article, we estimate the $p$-conductances of $T_n$, a supercritical Galton-Watson tree of depth $n$, for any $p>1$ (for a quenched realization of $T_n$).","In particular, we find the sharp asymptotic behavior when $n$ goes to infinity, which depends on whether the offspring distribution admits a finite moment of order $q$, where $q=\\frac{p}{p-1}$ is the conjugate exponent of $p$. We then apply our results to the random cluster model on~$T_n$ (with wired boundary condition) and provide sharp estimates on the probability that the root is connected to the leaves.","As an example, for the Ising model on $T_n$ with plus boundary conditions on the leaves, we find that, at criticality, the quenched magnetization of the root decays like: (i) $n^{-1/2}$ times an explicit tree-dependent constant if the offspring distribution admits a finite moment of order $3$; (ii) $n^{-1/(\\alpha-1)}$ if the offspring distribution has a heavy tail with exponent $\\alpha \\in (1,3)$."],"url":"http://arxiv.org/abs/2404.11564v1","category":"math.PR"}
{"created":"2024-04-17 17:03:52","title":"Chiral-odd GPDs in the bag model","abstract":"A study of chiral-odd generalized parton distributions (GPDs) of the nucleon is presented in the bag model demonstrating that in this model all four chiral-odd GPDs are non-zero contrary to other claims in literature. The bag model results for the GPDs $H_T^q(x,\\xi,t)$, $E_T^q(x,\\xi,t)$, $\\tilde{H}_T^q(x,\\xi,t)$ agree with other models within a typical quark model accuracy. We present one of the few quark model calculations where polynomiality is satisfied and the sum rule $\\int dx\\,\\tilde{E}_T^q(x,\\xi,t)=0$ holds. We confront our results with predictions from the large-$N_c$ limit, and with lattice QCD calculations. We conclude that the bag model successfully catches the main features of chiral-odd GPDs.","sentences":["A study of chiral-odd generalized parton distributions (GPDs) of the nucleon is presented in the bag model demonstrating that in this model all four chiral-odd GPDs are non-zero contrary to other claims in literature.","The bag model results for the GPDs $H_T^q(x,\\xi,t)$, $E_T^q(x,\\xi,t)$, $\\tilde{H}_T^q(x,\\xi,t)$ agree with other models within a typical quark model accuracy.","We present one of the few quark model calculations where polynomiality is satisfied and the sum rule $\\int dx\\,\\tilde{E}_T^q(x,\\xi,t)=0$ holds.","We confront our results with predictions from the large-$N_c$ limit, and with lattice QCD calculations.","We conclude that the bag model successfully catches the main features of chiral-odd GPDs."],"url":"http://arxiv.org/abs/2404.11563v1","category":"hep-ph"}
{"created":"2024-04-17 17:00:26","title":"Spatio-Temporal Motion Retargeting for Quadruped Robots","abstract":"This work introduces a motion retargeting approach for legged robots, which aims to create motion controllers that imitate the fine behavior of animals. Our approach, namely spatio-temporal motion retargeting (STMR), guides imitation learning procedures by transferring motion from source to target, effectively bridging the morphological disparities by ensuring the feasibility of imitation on the target system. Our STMR method comprises two components: spatial motion retargeting (SMR) and temporal motion retargeting (TMR). On the one hand, SMR tackles motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories. On the other hand, TMR aims to retarget motion at the dynamic level by optimizing motion in the temporal domain. We showcase the effectiveness of our method in facilitating Imitation Learning (IL) for complex animal movements through a series of simulation and hardware experiments. In these experiments, our STMR method successfully tailored complex animal motions from various media, including video captured by a hand-held camera, to fit the morphology and physical properties of the target robots. This enabled RL policy training for precise motion tracking, while baseline methods struggled with highly dynamic motion involving flying phases. Moreover, we validated that the control policy can successfully imitate six different motions in two quadruped robots with different dimensions and physical properties in real-world settings.","sentences":["This work introduces a motion retargeting approach for legged robots, which aims to create motion controllers that imitate the fine behavior of animals.","Our approach, namely spatio-temporal motion retargeting (STMR), guides imitation learning procedures by transferring motion from source to target, effectively bridging the morphological disparities by ensuring the feasibility of imitation on the target system.","Our STMR method comprises two components: spatial motion retargeting (SMR) and temporal motion retargeting (TMR).","On the one hand, SMR tackles motion retargeting at the kinematic level by generating kinematically feasible whole-body motions from keypoint trajectories.","On the other hand, TMR aims to retarget motion at the dynamic level by optimizing motion in the temporal domain.","We showcase the effectiveness of our method in facilitating Imitation Learning (IL) for complex animal movements through a series of simulation and hardware experiments.","In these experiments, our STMR method successfully tailored complex animal motions from various media, including video captured by a hand-held camera, to fit the morphology and physical properties of the target robots.","This enabled RL policy training for precise motion tracking, while baseline methods struggled with highly dynamic motion involving flying phases.","Moreover, we validated that the control policy can successfully imitate six different motions in two quadruped robots with different dimensions and physical properties in real-world settings."],"url":"http://arxiv.org/abs/2404.11557v1","category":"cs.RO"}
{"created":"2024-04-17 16:57:42","title":"The Reliability of Type Ia Supernovae Delay Time Distributions Recovered from Galaxy Star Formation Histories","abstract":"We present a numerical analysis investigating the reliability of type Ia supernova (SN~Ia) delay-time distributions recovered from individual host galaxy star-formation histories. We utilize star-formation histories of mock samples of galaxies generated from the IllustrisTNG simulation at two redshifts to recover delay-time distributions. The delay-time distributions are constructed through piecewise constants as opposed to typically employed parametric forms such as power-laws or Gaussian or skew/log-normal functions. The SN~Ia delay-time distributions are recovered through a Markov Chain Monte Carlo exploration of the likelihood space by comparing the expected SN Ia rate within each mock galaxy to the observed rate. We show that a reduced representative sample of \\emph{non-host} galaxies is sufficient to reliably recover delay-time distributions while simultaneously reducing the computational load. We also highlight a potential systematic between recovered delay-time distributions and the mass-weighted ages of the underlying host galaxy stellar population.","sentences":["We present a numerical analysis investigating the reliability of type Ia supernova (SN~Ia) delay-time distributions recovered from individual host galaxy star-formation histories.","We utilize star-formation histories of mock samples of galaxies generated from the IllustrisTNG simulation at two redshifts to recover delay-time distributions.","The delay-time distributions are constructed through piecewise constants as opposed to typically employed parametric forms such as power-laws or Gaussian or skew/log-normal functions.","The SN~Ia delay-time distributions are recovered through a Markov Chain Monte Carlo exploration of the likelihood space by comparing the expected SN Ia rate within each mock galaxy to the observed rate.","We show that a reduced representative sample of \\emph{non-host} galaxies is sufficient to reliably recover delay-time distributions while simultaneously reducing the computational load.","We also highlight a potential systematic between recovered delay-time distributions and the mass-weighted ages of the underlying host galaxy stellar population."],"url":"http://arxiv.org/abs/2404.11555v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:56:31","title":"Predicting Long-horizon Futures by Conditioning on Geometry and Time","abstract":"Our work explores the task of generating future sensor observations conditioned on the past. We are motivated by `predictive coding' concepts from neuroscience as well as robotic applications such as self-driving vehicles. Predictive video modeling is challenging because the future may be multi-modal and learning at scale remains computationally expensive for video processing. To address both challenges, our key insight is to leverage the large-scale pretraining of image diffusion models which can handle multi-modality. We repurpose image models for video prediction by conditioning on new frame timestamps. Such models can be trained with videos of both static and dynamic scenes. To allow them to be trained with modestly-sized datasets, we introduce invariances by factoring out illumination and texture by forcing the model to predict (pseudo) depth, readily obtained for in-the-wild videos via off-the-shelf monocular depth networks. In fact, we show that simply modifying networks to predict grayscale pixels already improves the accuracy of video prediction. Given the extra controllability with timestamp conditioning, we propose sampling schedules that work better than the traditional autoregressive and hierarchical sampling strategies. Motivated by probabilistic metrics from the object forecasting literature, we create a benchmark for video prediction on a diverse set of videos spanning indoor and outdoor scenes and a large vocabulary of objects. Our experiments illustrate the effectiveness of learning to condition on timestamps, and show the importance of predicting the future with invariant modalities.","sentences":["Our work explores the task of generating future sensor observations conditioned on the past.","We are motivated by `predictive coding' concepts from neuroscience as well as robotic applications such as self-driving vehicles.","Predictive video modeling is challenging because the future may be multi-modal and learning at scale remains computationally expensive for video processing.","To address both challenges, our key insight is to leverage the large-scale pretraining of image diffusion models which can handle multi-modality.","We repurpose image models for video prediction by conditioning on new frame timestamps.","Such models can be trained with videos of both static and dynamic scenes.","To allow them to be trained with modestly-sized datasets, we introduce invariances by factoring out illumination and texture by forcing the model to predict (pseudo) depth, readily obtained for in-the-wild videos via off-the-shelf monocular depth networks.","In fact, we show that simply modifying networks to predict grayscale pixels already improves the accuracy of video prediction.","Given the extra controllability with timestamp conditioning, we propose sampling schedules that work better than the traditional autoregressive and hierarchical sampling strategies.","Motivated by probabilistic metrics from the object forecasting literature, we create a benchmark for video prediction on a diverse set of videos spanning indoor and outdoor scenes and a large vocabulary of objects.","Our experiments illustrate the effectiveness of learning to condition on timestamps, and show the importance of predicting the future with invariant modalities."],"url":"http://arxiv.org/abs/2404.11554v1","category":"cs.CV"}
{"created":"2024-04-17 16:53:16","title":"Quantifying Multilingual Performance of Large Language Models Across Languages","abstract":"The training process of Large Language Models (LLMs) requires extensive text corpus. However, these data are often unevenly distributed in different languages. As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages. However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages. To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages. We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English. We have the following three findings: 1. The performance rankings of different LLMs in all languages are roughly the same. 2. LLMs with different sizes have the same partial order of performance. 3. There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus. These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs.","sentences":["The training process of Large Language Models (LLMs) requires extensive text corpus.","However, these data are often unevenly distributed in different languages.","As a result, LLMs perform well on common languages, such as English, German, and French, but perform poorly on low-resource languages.","However, currently there is no work to quantitatively measure the performance of LLMs in low-resource languages.","To fill this gap, we proposed the Language Ranker that aims to benchmark and rank different languages according to the performance of LLMs on those languages.","We employ the LLM's performance on the English corpus as a baseline to compare the performances of different languages and English.","We have the following three findings: 1.","The performance rankings of different LLMs in all languages are roughly the same.","2. LLMs with different sizes have the same partial order of performance.","3.","There is a strong correlation between LlaMa2's performance in different languages and the proportion of the pre-training corpus.","These findings illustrate that the Language Ranker can be used as an indicator to measure the language performance of LLMs."],"url":"http://arxiv.org/abs/2404.11553v1","category":"cs.CL"}
{"created":"2024-04-17 16:48:39","title":"Modular resurgent structures","abstract":"The theory of resurgence uniquely associates a factorially divergent formal power series with a collection of non-perturbative, exponential-type corrections paired with a set of complex numbers, known as Stokes constants. When the Borel plane displays a single infinite tower of singularities, the secondary resurgent series are trivial, and the Stokes constants are coefficients of an $L$-function, a rich analytic number-theoretic fabric underlies the resurgent structure of the asymptotic series. We propose a new paradigm of modular resurgence that focuses on the role of the Stokes constants and the interplay of the $q$-series acting as their generating functions with the corresponding $L$-functions. Guided by two pivotal examples arising from topological string theory and the theory of Maass cusp forms, we introduce the notion of modular resurgent series, which we conjecture to have specific summability properties as well as to be closely related to quantum modular forms.","sentences":["The theory of resurgence uniquely associates a factorially divergent formal power series with a collection of non-perturbative, exponential-type corrections paired with a set of complex numbers, known as Stokes constants.","When the Borel plane displays a single infinite tower of singularities, the secondary resurgent series are trivial, and the Stokes constants are coefficients of an $L$-function, a rich analytic number-theoretic fabric underlies the resurgent structure of the asymptotic series.","We propose a new paradigm of modular resurgence that focuses on the role of the Stokes constants and the interplay of the $q$-series acting as their generating functions with the corresponding $L$-functions.","Guided by two pivotal examples arising from topological string theory and the theory of Maass cusp forms, we introduce the notion of modular resurgent series, which we conjecture to have specific summability properties as well as to be closely related to quantum modular forms."],"url":"http://arxiv.org/abs/2404.11550v1","category":"math.NT"}
{"created":"2024-04-17 16:44:29","title":"Resolved properties of a luminous \"hinge clump\" in the compact group of galaxies NGC\\,6845","abstract":"We study the properties of one of the most luminous hinge clumps, located on the compact group of galaxies NGC6845. Using IFS from GMOS/Gemini, complemented with archival MUSE data, we obtain oxygen abundances, ages, star formation rates, velocity fields and we also performed a single stellar populations modeling to understand the SFH of the hinge clump localized in NGC6845. We found that the hinge clump sits in a tail, having a SFR of 3.4$M_{\\odot}yr^{-1}$, which is comparable with a few other extreme cases, e.g., the star clusters in the Antennae galaxy and other reported hinge clumps in the literature. In fact, this clump represents ~15\\% of total SFR of NGC6845A. Large-scale modeling of the observed velocity field of NGC6845A rules out the scenario on which this hinge clump was a satellite galaxy. Indeed, its kinematics is compatible with the galactic disk of NGC6845A. Its abundance, mean value of 0.4Z$_{\\odot}$, is also consistent with the metallicity gradient of the galaxy. Our analysis, suggest that the hinge clump is formed by multiple stellar populations instead of a single burst, thus having a large range of ages. We found that central clump is encompassed by a ring-like structure, suggesting that the ring-like structure represents a second-generation of star formation. In addition, the analysis of the diagnostic diagram indicates that this central region can also be being ionized by shock from stellar and supernovae winds. Finally, the derived SFR density $\\Sigma=9.7M_{\\odot}yr^{-1}kpc^{-2}$ of the central clump, place it in starburst regime, where gas inflows should provide gas to maintain the star formation. This work shows a resolved example of an extreme localized starburst in a compact group of galaxies.","sentences":["We study the properties of one of the most luminous hinge clumps, located on the compact group of galaxies NGC6845.","Using IFS from GMOS/Gemini, complemented with archival MUSE data, we obtain oxygen abundances, ages, star formation rates, velocity fields and we also performed a single stellar populations modeling to understand the SFH of the hinge clump localized in NGC6845.","We found that the hinge clump sits in a tail, having a SFR of 3.4$M_{\\odot}yr^{-1}$, which is comparable with a few other extreme cases, e.g., the star clusters in the Antennae galaxy and other reported hinge clumps in the literature.","In fact, this clump represents ~15\\% of total SFR of NGC6845A. Large-scale modeling of the observed velocity field of NGC6845A rules out the scenario on which this hinge clump was a satellite galaxy.","Indeed, its kinematics is compatible with the galactic disk of NGC6845A. Its abundance, mean value of 0.4Z$_{\\odot}$, is also consistent with the metallicity gradient of the galaxy.","Our analysis, suggest that the hinge clump is formed by multiple stellar populations instead of a single burst, thus having a large range of ages.","We found that central clump is encompassed by a ring-like structure, suggesting that the ring-like structure represents a second-generation of star formation.","In addition, the analysis of the diagnostic diagram indicates that this central region can also be being ionized by shock from stellar and supernovae winds.","Finally, the derived SFR density $\\Sigma=9.7M_{\\odot}yr^{-1}kpc^{-2}$ of the central clump, place it in starburst regime, where gas inflows should provide gas to maintain the star formation.","This work shows a resolved example of an extreme localized starburst in a compact group of galaxies."],"url":"http://arxiv.org/abs/2404.11549v1","category":"astro-ph.GA"}
{"created":"2024-04-17 16:42:35","title":"Strategic Network Inspection with Location-Specific Detection Capabilities","abstract":"We consider a two-person network inspection game, in which a defender positions a limited number of detectors to detect multiple attacks caused by an attacker. We assume that detection is imperfect, and each detector location is associated with a probability of detecting attacks within its set of monitored network components. The objective of the defender (resp. attacker) is to minimize (resp. maximize) the expected number of undetected attacks. To compute Nash Equilibria (NE) for this large-scale zero-sum game, we formulate a linear program with a small number of constraints, which we solve via column generation. We provide an exact mixed-integer program for the pricing problem, which entails computing a defender's pure best response, and leverage its supermodular structure to derive two efficient approaches to obtain approximate NE with theoretical guarantees: A column generation and a multiplicative weights update (MWU) algorithm with approximate best responses. To address the computational challenges posed by combinatorial attacker strategies, each iteration of our MWU algorithm requires computing a projection under the unnormalized relative entropy. We provide a closed-form solution and a linear-time algorithm for the projection problem. Our computational results in real-world gas distribution networks illustrate the performance and scalability of our solution approaches.","sentences":["We consider a two-person network inspection game, in which a defender positions a limited number of detectors to detect multiple attacks caused by an attacker.","We assume that detection is imperfect, and each detector location is associated with a probability of detecting attacks within its set of monitored network components.","The objective of the defender (resp. attacker) is to minimize (resp.","maximize) the expected number of undetected attacks.","To compute Nash Equilibria (NE) for this large-scale zero-sum game, we formulate a linear program with a small number of constraints, which we solve via column generation.","We provide an exact mixed-integer program for the pricing problem, which entails computing a defender's pure best response, and leverage its supermodular structure to derive two efficient approaches to obtain approximate NE with theoretical guarantees: A column generation and a multiplicative weights update (MWU) algorithm with approximate best responses.","To address the computational challenges posed by combinatorial attacker strategies, each iteration of our MWU algorithm requires computing a projection under the unnormalized relative entropy.","We provide a closed-form solution and a linear-time algorithm for the projection problem.","Our computational results in real-world gas distribution networks illustrate the performance and scalability of our solution approaches."],"url":"http://arxiv.org/abs/2404.11545v1","category":"cs.GT"}
{"created":"2024-04-17 16:37:33","title":"Ordinal Maximin Guarantees for Group Fair Division","abstract":"We investigate fairness in the allocation of indivisible items among groups of agents using the notion of maximin share (MMS). While previous work has shown that no nontrivial multiplicative MMS approximation can be guaranteed in this setting for general group sizes, we demonstrate that ordinal relaxations are much more useful. For example, we show that if $n$ agents are distributed equally across $g$ groups, there exists a $1$-out-of-$k$ MMS allocation for $k = O(g\\log(n/g))$, while if all but a constant number of agents are in the same group, we obtain $k = O(\\log n/\\log \\log n)$. We also establish the tightness of these bounds and provide non-asymptotic results for the case of two groups.","sentences":["We investigate fairness in the allocation of indivisible items among groups of agents using the notion of maximin share (MMS).","While previous work has shown that no nontrivial multiplicative MMS approximation can be guaranteed in this setting for general group sizes, we demonstrate that ordinal relaxations are much more useful.","For example, we show that if $n$ agents are distributed equally across $g$ groups, there exists a $1$-out-of-$k$ MMS allocation for $k = O(g\\log(n/g))$, while if all but a constant number of agents are in the same group, we obtain $k = O(\\log n/\\log \\log n)$. We also establish the tightness of these bounds and provide non-asymptotic results for the case of two groups."],"url":"http://arxiv.org/abs/2404.11543v1","category":"cs.GT"}
{"created":"2024-04-17 16:36:55","title":"A Lean Simulation Framework for Stress Testing IoT Cloud Systems","abstract":"The Internet of Things connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles and health monitoring. Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive. This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems. Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly. We propose a lean simulation framework designed for IoT cloud stress testing which enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud. To facilitate simulation construction for practitioners, we develop a domain-specific language (DSL), named IoTECS, for generating simulators from model-based specifications. We provide the syntax and semantics of IoTECS and implement IoTECS using Xtext and Xtend. We assess simulators generated from IoTECS specifications for stress testing two real-world systems: a cloud-based IoT monitoring system and an IoT-connected vehicle system. Our empirical results indicate that simulators created using IoTECS: (1)achieve best performance when configured with Docker containerization; (2)effectively assess the service capacity of our case-study systems, and (3)outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources. To gain initial insights about the usefulness of IoTECS in practice, we interviewed two engineers from our industry partner who have firsthand experience with IoTECS. Feedback from these interviews suggests that IoTECS is effective in stress testing IoT cloud systems, saving significant time and effort.","sentences":["The Internet of Things connects a plethora of smart devices globally across various applications like smart cities, autonomous vehicles and health monitoring.","Simulation plays a key role in the testing of IoT systems, noting that field testing of a complete IoT product may be infeasible or prohibitively expensive.","This paper addresses a specific yet important need in simulation-based testing for IoT: Stress testing of cloud systems.","Existing stress testing solutions for IoT demand significant computational resources, making them ill-suited and costly.","We propose a lean simulation framework designed for IoT cloud stress testing which enables efficient simulation of a large array of IoT and edge devices that communicate with the cloud.","To facilitate simulation construction for practitioners, we develop a domain-specific language (DSL), named IoTECS, for generating simulators from model-based specifications.","We provide the syntax and semantics of IoTECS and implement IoTECS using Xtext and Xtend.","We assess simulators generated from IoTECS specifications for stress testing two real-world systems: a cloud-based IoT monitoring system and an IoT-connected vehicle system.","Our empirical results indicate that simulators created using IoTECS: (1)achieve best performance when configured with Docker containerization; (2)effectively assess the service capacity of our case-study systems, and (3)outperform industrial stress-testing baseline tools, JMeter and Locust, by a factor of 3.5 in terms of the number of IoT and edge devices they can simulate using identical hardware resources.","To gain initial insights about the usefulness of IoTECS in practice, we interviewed two engineers from our industry partner who have firsthand experience with IoTECS.","Feedback from these interviews suggests that IoTECS is effective in stress testing IoT cloud systems, saving significant time and effort."],"url":"http://arxiv.org/abs/2404.11542v1","category":"cs.SE"}
{"created":"2024-04-17 16:33:22","title":"Evaluating Span Extraction in Generative Paradigm: A Reflection on Aspect-Based Sentiment Analysis","abstract":"In the era of rapid evolution of generative language models within the realm of natural language processing, there is an imperative call to revisit and reformulate evaluation methodologies, especially in the domain of aspect-based sentiment analysis (ABSA). This paper addresses the emerging challenges introduced by the generative paradigm, which has moderately blurred traditional boundaries between understanding and generation tasks. Building upon prevailing practices in the field, we analyze the advantages and shortcomings associated with the prevalent ABSA evaluation paradigms. Through an in-depth examination, supplemented by illustrative examples, we highlight the intricacies involved in aligning generative outputs with other evaluative metrics, specifically those derived from other tasks, including question answering. While we steer clear of advocating for a singular and definitive metric, our contribution lies in paving the path for a comprehensive guideline tailored for ABSA evaluations in this generative paradigm. In this position paper, we aim to provide practitioners with profound reflections, offering insights and directions that can aid in navigating this evolving landscape, ensuring evaluations that are both accurate and reflective of generative capabilities.","sentences":["In the era of rapid evolution of generative language models within the realm of natural language processing, there is an imperative call to revisit and reformulate evaluation methodologies, especially in the domain of aspect-based sentiment analysis (ABSA).","This paper addresses the emerging challenges introduced by the generative paradigm, which has moderately blurred traditional boundaries between understanding and generation tasks.","Building upon prevailing practices in the field, we analyze the advantages and shortcomings associated with the prevalent ABSA evaluation paradigms.","Through an in-depth examination, supplemented by illustrative examples, we highlight the intricacies involved in aligning generative outputs with other evaluative metrics, specifically those derived from other tasks, including question answering.","While we steer clear of advocating for a singular and definitive metric, our contribution lies in paving the path for a comprehensive guideline tailored for ABSA evaluations in this generative paradigm.","In this position paper, we aim to provide practitioners with profound reflections, offering insights and directions that can aid in navigating this evolving landscape, ensuring evaluations that are both accurate and reflective of generative capabilities."],"url":"http://arxiv.org/abs/2404.11539v1","category":"cs.CL"}
{"created":"2024-04-17 16:32:13","title":"GenFighter: A Generative and Evolutive Textual Attack Removal","abstract":"Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP). This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution. GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response. By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics. Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios. The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks.","sentences":["Adversarial attacks pose significant challenges to deep neural networks (DNNs) such as Transformer models in natural language processing (NLP).","This paper introduces a novel defense strategy, called GenFighter, which enhances adversarial robustness by learning and reasoning on the training classification distribution.","GenFighter identifies potentially malicious instances deviating from the distribution, transforms them into semantically equivalent instances aligned with the training data, and employs ensemble techniques for a unified and robust response.","By conducting extensive experiments, we show that GenFighter outperforms state-of-the-art defenses in accuracy under attack and attack success rate metrics.","Additionally, it requires a high number of queries per attack, making the attack more challenging in real scenarios.","The ablation study shows that our approach integrates transfer learning, a generative/evolutive procedure, and an ensemble method, providing an effective defense against NLP adversarial attacks."],"url":"http://arxiv.org/abs/2404.11538v1","category":"cs.LG"}
{"created":"2024-04-17 16:30:56","title":"SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing Pansharpening","abstract":"Pansharpening is a significant image fusion technique that merges the spatial content and spectral characteristics of remote sensing images to generate high-resolution multispectral images. Recently, denoising diffusion probabilistic models have been gradually applied to visual tasks, enhancing controllable image generation through low-rank adaptation (LoRA). In this paper, we introduce a spatial-spectral integrated diffusion model for the remote sensing pansharpening task, called SSDiff, which considers the pansharpening process as the fusion process of spatial and spectral components from the perspective of subspace decomposition. Specifically, SSDiff utilizes spatial and spectral branches to learn spatial details and spectral features separately, then employs a designed alternating projection fusion module (APFM) to accomplish the fusion. Furthermore, we propose a frequency modulation inter-branch module (FMIM) to modulate the frequency distribution between branches. The two components of SSDiff can perform favorably against the APFM when utilizing a LoRA-like branch-wise alternative fine-tuning method. It refines SSDiff to capture component-discriminating features more sufficiently. Finally, extensive experiments on four commonly used datasets, i.e., WorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiority of SSDiff both visually and quantitatively. The code will be made open source after possible acceptance.","sentences":["Pansharpening is a significant image fusion technique that merges the spatial content and spectral characteristics of remote sensing images to generate high-resolution multispectral images.","Recently, denoising diffusion probabilistic models have been gradually applied to visual tasks, enhancing controllable image generation through low-rank adaptation (LoRA).","In this paper, we introduce a spatial-spectral integrated diffusion model for the remote sensing pansharpening task, called SSDiff, which considers the pansharpening process as the fusion process of spatial and spectral components from the perspective of subspace decomposition.","Specifically, SSDiff utilizes spatial and spectral branches to learn spatial details and spectral features separately, then employs a designed alternating projection fusion module (APFM) to accomplish the fusion.","Furthermore, we propose a frequency modulation inter-branch module (FMIM) to modulate the frequency distribution between branches.","The two components of SSDiff can perform favorably against the APFM when utilizing a LoRA-like branch-wise alternative fine-tuning method.","It refines SSDiff to capture component-discriminating features more sufficiently.","Finally, extensive experiments on four commonly used datasets, i.e., WorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiority of SSDiff both visually and quantitatively.","The code will be made open source after possible acceptance."],"url":"http://arxiv.org/abs/2404.11537v1","category":"cs.CV"}
{"created":"2024-04-17 16:30:06","title":"FedPFT: Federated Proxy Fine-Tuning of Foundation Models","abstract":"Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients. In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules. First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons. Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees. Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT.","sentences":["Adapting Foundation Models (FMs) for downstream tasks through Federated Learning (FL) emerges a promising strategy for protecting data privacy and valuable FMs.","Existing methods fine-tune FM by allocating sub-FM to clients in FL, however, leading to suboptimal performance due to insufficient tuning and inevitable error accumulations of gradients.","In this paper, we propose Federated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation in downstream tasks through FL by two key modules.","First, the sub-FM construction module employs a layer-wise compression approach, facilitating comprehensive FM fine-tuning across all layers by emphasizing those crucial neurons.","Second, the sub-FM alignment module conducts a two-step distillations-layer-level and neuron-level-before and during FL fine-tuning respectively, to reduce error of gradient by accurately aligning sub-FM with FM under theoretical guarantees.","Experimental results on seven commonly used datasets (i.e., four text and three vision) demonstrate the superiority of FedPFT."],"url":"http://arxiv.org/abs/2404.11536v1","category":"cs.LG"}
{"created":"2024-04-17 16:28:08","title":"Decomposing and Editing Predictions by Modeling Model Computation","abstract":"How does the internal computation of a machine learning model transform inputs into predictions? In this paper, we introduce a task called component modeling that aims to address this question. The goal of component modeling is to decompose an ML model's prediction in terms of its components -- simple functions (e.g., convolution filters, attention heads) that are the \"building blocks\" of model computation. We focus on a special case of this task, component attribution, where the goal is to estimate the counterfactual impact of individual components on a given prediction. We then present COAR, a scalable algorithm for estimating component attributions; we demonstrate its effectiveness across models, datasets, and modalities. Finally, we show that component attributions estimated with COAR directly enable model editing across five tasks, namely: fixing model errors, ``forgetting'' specific classes, boosting subpopulation robustness, localizing backdoor attacks, and improving robustness to typographic attacks. We provide code for COAR at https://github.com/MadryLab/modelcomponents .","sentences":["How does the internal computation of a machine learning model transform inputs into predictions?","In this paper, we introduce a task called component modeling that aims to address this question.","The goal of component modeling is to decompose an ML model's prediction in terms of its components -- simple functions (e.g., convolution filters, attention heads) that are the \"building blocks\" of model computation.","We focus on a special case of this task, component attribution, where the goal is to estimate the counterfactual impact of individual components on a given prediction.","We then present COAR, a scalable algorithm for estimating component attributions; we demonstrate its effectiveness across models, datasets, and modalities.","Finally, we show that component attributions estimated with COAR directly enable model editing across five tasks, namely: fixing model errors, ``forgetting'' specific classes, boosting subpopulation robustness, localizing backdoor attacks, and improving robustness to typographic attacks.","We provide code for COAR at https://github.com/MadryLab/modelcomponents ."],"url":"http://arxiv.org/abs/2404.11534v1","category":"cs.LG"}
{"created":"2024-04-17 16:26:13","title":"The B\u00e1r\u00e1ny-Kalai conjecture for certain families of polytopes","abstract":"B\\'ar\\'any and Kalai conjectured the following generalization of Tverberg's theorem: if $f$ is a linear function from an $m$-dimensional polytope $P$ to $\\mathbb{R}^d$ and $m \\ge (d + 1)(r - 1)$, then there are $r$ pairwise disjoint faces of $P$ whose images have a point in common. We show that the conjecture holds for cross polytopes, cyclic polytopes, and more generally for $(d+1)$-neighborly polytopes. Moreover, we show that for cross polytopes, the conjecture holds if the map $f$ is assumed to be continuous (but not necessarily linear), and we give a lower bound on the number of sets of $r$ pairwise disjoint faces whose images under $f$ intersect. We also show that the conjecture holds for all polytopes when $d=1$ and $f$ is assumed to be continuous. Finally, when $r$ is prime or large enough with respect to $d$, we prove that there exists a constant $c=c(d,r)$, depending only on $d$ and $r$, such that the conjecture holds (with continuous functions) for the polytope obtained by taking $c$ subdivisions of $P$.","sentences":["B\\'ar\\'any and Kalai conjectured the following generalization of Tverberg's theorem: if $f$ is a linear function from an $m$-dimensional polytope $P$ to $\\mathbb{R}^d$ and $m \\ge (d + 1)(r - 1)$, then there are $r$ pairwise disjoint faces of $P$ whose images have a point in common.","We show that the conjecture holds for cross polytopes, cyclic polytopes, and more generally for $(d+1)$-neighborly polytopes.","Moreover, we show that for cross polytopes, the conjecture holds if the map $f$ is assumed to be continuous (but not necessarily linear), and we give a lower bound on the number of sets of $r$ pairwise disjoint faces whose images under $f$ intersect.","We also show that the conjecture holds for all polytopes when $d=1$ and $f$ is assumed to be continuous.","Finally, when $r$ is prime or large enough with respect to $d$, we prove that there exists a constant $c=c(d,r)$, depending only on $d$ and $r$, such that the conjecture holds (with continuous functions) for the polytope obtained by taking $c$ subdivisions of $P$."],"url":"http://arxiv.org/abs/2404.11533v1","category":"math.CO"}
{"created":"2024-04-17 16:24:07","title":"Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization","abstract":"Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task. However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input. In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference. We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt. PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized. First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise. Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm. The derived importance weights are used to combine the LLMs during inference. We conduct experiments with over 100 total LLMs on a diverse set of tasks. Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points.","sentences":["Fusing knowledge from multiple Large Language Models (LLMs) can combine their diverse strengths to achieve improved performance on a given task.","However, current fusion approaches either rely on learning-based fusers that do not generalize to new LLMs, or do not take into account how well each LLM understands the input.","In this work, we study LLM fusion at test-time, which enables leveraging knowledge from arbitrary user-specified LLMs during inference.","We introduce Pack of LLMs (PackLLM), an effective method for test-time fusion that leverages each LLM's expertise, given an input prompt.","PackLLM performs model fusion by solving an optimization problem for determining each LLM's importance, so that perplexity over the input prompt is minimized.","First, our simple PackLLM-sim variant validates that perplexity is a good indicator for measuring each LLM's expertise.","Second, our PackLLM-opt variant approximately solves the perplexity minimization problem via a greedy algorithm.","The derived importance weights are used to combine the LLMs during inference.","We conduct experiments with over 100 total LLMs on a diverse set of tasks.","Experimental results show that (i) perplexity is a reliable measure for LLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89% accuracy points, and (iii) PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92-11.94% accuracy points."],"url":"http://arxiv.org/abs/2404.11531v1","category":"cs.CL"}
{"created":"2024-04-17 16:23:47","title":"Theory of paraxial optical Skyrmions","abstract":"Vector light beams, characterised by a spatially varying polarisation, can exhibit localised structures reminiscent of the Skyrmions familiar from the study of magnetic media. We present a theory of such Skyrmions within paraxial optics, exploiting mathematical analogies with the study of superfluids, especially the A phase of superfluid $\\textrm{He}^3$. The key feature is the Skyrmion field which, together with the underlying Skyrmion vector potential, determines the properties of the Skyrmions and, more generally, the polarisation structure of every paraxial vector beam. In addition to structures with integer Skyrmion number we find polarisation patterns with non-integer Skyrmion number; these seem to have no analogue in other fields of physics.","sentences":["Vector light beams, characterised by a spatially varying polarisation, can exhibit localised structures reminiscent of the Skyrmions familiar from the study of magnetic media.","We present a theory of such Skyrmions within paraxial optics, exploiting mathematical analogies with the study of superfluids, especially the A phase of superfluid $\\textrm{He}^3$. The key feature is the Skyrmion field which, together with the underlying Skyrmion vector potential, determines the properties of the Skyrmions and, more generally, the polarisation structure of every paraxial vector beam.","In addition to structures with integer Skyrmion number we find polarisation patterns with non-integer Skyrmion number; these seem to have no analogue in other fields of physics."],"url":"http://arxiv.org/abs/2404.11530v1","category":"physics.optics"}
{"created":"2024-04-17 16:15:43","title":"On effective constructions of existentially closed groups","abstract":"Existentially closed groups are, informally, groups that contain solutions to every consistent finite system of equations and inequations. They were introduced in 1951 in an algebraic context and subsequent research elucidated deep connections with group theory and computability theory. We continue this investigation, with particular emphasis on illuminating the relationship with computability theory.   In particular, we show that existentially closed groups can be built at the level of the halting problem and that this is optimal. Moreover, using the the theory of the enumeration degrees and some work of Martin Ziegler in computable group theory, we show that the previous result relativises in a somewhat subtle way. We then tease apart the complexity contributed by ``global'' and ``local'' structure, showing that the finitely generated subgroups have complexity at the level of the PA degrees. Finally, we investigate the computability-theoretic complexity of omitting the non-principal quantifier-free types from a list of types, from which we obtain an upper bound on the complexity of building two existentially closed groups that are ``as different as possible''.","sentences":["Existentially closed groups are, informally, groups that contain solutions to every consistent finite system of equations and inequations.","They were introduced in 1951 in an algebraic context and subsequent research elucidated deep connections with group theory and computability theory.","We continue this investigation, with particular emphasis on illuminating the relationship with computability theory.   ","In particular, we show that existentially closed groups can be built at the level of the halting problem and that this is optimal.","Moreover, using the the theory of the enumeration degrees and some work of Martin Ziegler in computable group theory, we show that the previous result relativises in a somewhat subtle way.","We then tease apart the complexity contributed by ``global'' and ``local'' structure, showing that the finitely generated subgroups have complexity at the level of the PA degrees.","Finally, we investigate the computability-theoretic complexity of omitting the non-principal quantifier-free types from a list of types, from which we obtain an upper bound on the complexity of building two existentially closed groups that are ``as different as possible''."],"url":"http://arxiv.org/abs/2404.11524v1","category":"math.LO"}
{"created":"2024-04-17 16:10:42","title":"A central limit theorem for partially distinguishable bosons","abstract":"The quantum central limit theorem derived by Cushen and Hudson provides the foundations for understanding how subsystems of large bosonic systems evolving unitarily do reach equilibrium. It finds important applications in the context of quantum interferometry, for example, with photons. A practical feature of current photonic experiments, however, is that photons carry their own internal degrees of freedom pertaining to, e.g., the polarization or spatiotemporal mode they occupy, which makes them partially distinguishable. The ensuing deviation from ideal indistinguishability is well known to have observable consequences, for example in relation with boson bunching, but an understanding of its role in bosonic equilibration phenomena is still missing. Here, we generalize the Cushen-Hudson quantum central limit theorem to encompass scenarios with partial distinguishability, implying an asymptotic convergence of the subsystem's reduced state towards a multimode Gaussian state defined over the internal degrees of freedom. While these asymptotic internal states may not be directly accessible, we show that particle number distributions carry important signatures of distinguishability, which may be used to diagnose experimental imperfections in large boson sampling experiments.","sentences":["The quantum central limit theorem derived by Cushen and Hudson provides the foundations for understanding how subsystems of large bosonic systems evolving unitarily do reach equilibrium.","It finds important applications in the context of quantum interferometry, for example, with photons.","A practical feature of current photonic experiments, however, is that photons carry their own internal degrees of freedom pertaining to, e.g., the polarization or spatiotemporal mode they occupy, which makes them partially distinguishable.","The ensuing deviation from ideal indistinguishability is well known to have observable consequences, for example in relation with boson bunching, but an understanding of its role in bosonic equilibration phenomena is still missing.","Here, we generalize the Cushen-Hudson quantum central limit theorem to encompass scenarios with partial distinguishability, implying an asymptotic convergence of the subsystem's reduced state towards a multimode Gaussian state defined over the internal degrees of freedom.","While these asymptotic internal states may not be directly accessible, we show that particle number distributions carry important signatures of distinguishability, which may be used to diagnose experimental imperfections in large boson sampling experiments."],"url":"http://arxiv.org/abs/2404.11518v1","category":"quant-ph"}
{"created":"2024-04-17 16:07:53","title":"Embedding Privacy in Computational Social Science and Artificial Intelligence Research","abstract":"Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights. The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society. We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start. This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face. It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results.","sentences":["Privacy is a human right.","It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them.","Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights.","The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals - especially vulnerable groups - and society.","We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start.","This article contributes to the field by discussing the role of privacy and the primary issues that researchers working in CSS, AI, data science and related domains are likely to face.","It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results."],"url":"http://arxiv.org/abs/2404.11515v1","category":"cs.AI"}
{"created":"2024-04-17 16:06:57","title":"Dark matter, black holes, and gravitational waves","abstract":"The formation and growth of black holes can strongly influence the distribution of dark matter around them. I discuss here the different types of dark matter overdensities around black holes, including dark matter cusps, spikes, mounds, crests, and gravitational atoms. I then review recent results on the evolution of a black holes binary in presence of dark matter, focusing on the energy transfer between binary and dark matter induced by dynamical friction. Finally, I present the prospects for studying dark matter with gravitational wave observations, and argue that future interferometers might be able to detect and characterise dark matter overdensities around black holes.","sentences":["The formation and growth of black holes can strongly influence the distribution of dark matter around them.","I discuss here the different types of dark matter overdensities around black holes, including dark matter cusps, spikes, mounds, crests, and gravitational atoms.","I then review recent results on the evolution of a black holes binary in presence of dark matter, focusing on the energy transfer between binary and dark matter induced by dynamical friction.","Finally, I present the prospects for studying dark matter with gravitational wave observations, and argue that future interferometers might be able to detect and characterise dark matter overdensities around black holes."],"url":"http://arxiv.org/abs/2404.11513v1","category":"astro-ph.CO"}
{"created":"2024-04-17 16:05:03","title":"VC Theory for Inventory Policies","abstract":"Advances in computational power and AI have increased interest in reinforcement learning approaches to inventory management. This paper provides a theoretical foundation for these approaches and investigates the benefits of restricting to policy structures that are well-established by decades of inventory theory. In particular, we prove generalization guarantees for learning several well-known classes of inventory policies, including base-stock and (s, S) policies, by leveraging the celebrated Vapnik-Chervonenkis (VC) theory. We apply the concepts of the Pseudo-dimension and Fat-shattering dimension from VC theory to determine the generalizability of inventory policies, that is, the difference between an inventory policy's performance on training data and its expected performance on unseen data. We focus on a classical setting without contexts, but allow for an arbitrary distribution over demand sequences and do not make any assumptions such as independence over time. We corroborate our supervised learning results using numerical simulations.   Managerially, our theory and simulations translate to the following insights. First, there is a principle of \"learning less is more\" in inventory management: depending on the amount of data available, it may be beneficial to restrict oneself to a simpler, albeit suboptimal, class of inventory policies to minimize overfitting errors. Second, the number of parameters in a policy class may not be the correct measure of overfitting error: in fact, the class of policies defined by T time-varying base-stock levels exhibits a generalization error comparable to that of the two-parameter (s, S) policy class. Finally, our research suggests situations in which it could be beneficial to incorporate the concepts of base-stock and inventory position into black-box learning machines, instead of having these machines directly learn the order quantity actions.","sentences":["Advances in computational power and AI have increased interest in reinforcement learning approaches to inventory management.","This paper provides a theoretical foundation for these approaches and investigates the benefits of restricting to policy structures that are well-established by decades of inventory theory.","In particular, we prove generalization guarantees for learning several well-known classes of inventory policies, including base-stock and (s, S) policies, by leveraging the celebrated Vapnik-Chervonenkis (VC) theory.","We apply the concepts of the Pseudo-dimension and Fat-shattering dimension from VC theory to determine the generalizability of inventory policies, that is, the difference between an inventory policy's performance on training data and its expected performance on unseen data.","We focus on a classical setting without contexts, but allow for an arbitrary distribution over demand sequences and do not make any assumptions such as independence over time.","We corroborate our supervised learning results using numerical simulations.   ","Managerially, our theory and simulations translate to the following insights.","First, there is a principle of \"learning less is more\" in inventory management: depending on the amount of data available, it may be beneficial to restrict oneself to a simpler, albeit suboptimal, class of inventory policies to minimize overfitting errors.","Second, the number of parameters in a policy class may not be the correct measure of overfitting error: in fact, the class of policies defined by T time-varying base-stock levels exhibits a generalization error comparable to that of the two-parameter (s, S) policy class.","Finally, our research suggests situations in which it could be beneficial to incorporate the concepts of base-stock and inventory position into black-box learning machines, instead of having these machines directly learn the order quantity actions."],"url":"http://arxiv.org/abs/2404.11509v1","category":"stat.ML"}
{"created":"2024-04-17 16:04:57","title":"Stage-IV Cosmic Shear with Modified Gravity and Model-independent Screening","abstract":"We forecast constraints on minimal model-independent parametrisations of several Modified Gravity theories using mock Stage-IV cosmic shear data. We include nonlinear effects and screening, which ensures recovery of General Relativity on small scales. We introduce a power spectrum emulator to accelerate our analysis and evaluate the robustness of the growth index parametrisation with respect to two cosmologies: $\\Lambda$CDM and the normal branch of the DGP model. We forecast the uncertainties on the growth index $\\gamma$ to be of the order $\\sim 10\\%$. We find that our halo-model based screening approach demonstrates excellent performance, meeting the precision requirements of Stage-IV surveys. However, neglecting the screening transition results in biased predictions for cosmological parameters. We find that the screening transition shows significant degeneracy with baryonic feedback, requiring a much better understanding of baryonic physics for its detection. Massive neutrinos effects are less prominent and challenging to detect solely with cosmic shear data.","sentences":["We forecast constraints on minimal model-independent parametrisations of several Modified Gravity theories using mock Stage-IV cosmic shear data.","We include nonlinear effects and screening, which ensures recovery of General Relativity on small scales.","We introduce a power spectrum emulator to accelerate our analysis and evaluate the robustness of the growth index parametrisation with respect to two cosmologies: $\\Lambda$CDM and the normal branch of the DGP model.","We forecast the uncertainties on the growth index $\\gamma$ to be of the order $\\sim 10\\%$.","We find that our halo-model based screening approach demonstrates excellent performance, meeting the precision requirements of Stage-IV surveys.","However, neglecting the screening transition results in biased predictions for cosmological parameters.","We find that the screening transition shows significant degeneracy with baryonic feedback, requiring a much better understanding of baryonic physics for its detection.","Massive neutrinos effects are less prominent and challenging to detect solely with cosmic shear data."],"url":"http://arxiv.org/abs/2404.11508v1","category":"astro-ph.CO"}
{"created":"2024-04-17 16:01:56","title":"Oscillation of ergodic averages and other stochastic processes","abstract":"For an ergodic map $T$ and a non-constant, real-valued $f \\in L^1$, the ergodic averages $\\mathbb{A}_N f(x) = \\frac{1} {N} \\sum_{n=1}^N f(T^n x)$ converge a.e., but the convergence is never monotone. Depending on particular properties of the function $f$, the averages $\\mathbb{A}_N f(x)$ may or may not actually oscillate around the mean value infinitely often a.e. We will prove that a.e. oscillation around the mean is the generic behavior. That is, for a fixed ergodic $T$, the generic non-constant $f\\in L^1$ has the averages $\\mathbb{A}_N f(x)$ oscillating around the mean infinitely often for almost every $x$. We also consider oscillation for other stochastic processes like subsequences of the ergodic averages, convolution operators, weighted averages, uniform distribution and martingales. We will show that in general, in these settings oscillation around the limit infinitely often persists as the generic behavior.","sentences":["For an ergodic map $T$ and a non-constant, real-valued $f \\in L^1$, the ergodic averages $\\mathbb{A}_N f(x) = \\frac{1} {N} \\sum_{n=1}^N f(T^n x)$ converge a.e., but the convergence is never monotone.","Depending on particular properties of the function $f$, the averages $\\mathbb{A}_N f(x)$ may or may not actually oscillate around the mean value infinitely often a.e.","We will prove that a.e. oscillation around the mean is the generic behavior.","That is, for a fixed ergodic $T$, the generic non-constant $f\\in L^1$ has the averages $\\mathbb{A}_N f(x)$ oscillating around the mean infinitely often for almost every $x$.","We also consider oscillation for other stochastic processes like subsequences of the ergodic averages, convolution operators, weighted averages, uniform distribution and martingales.","We will show that in general, in these settings oscillation around the limit infinitely often persists as the generic behavior."],"url":"http://arxiv.org/abs/2404.11507v1","category":"math.DS"}
{"created":"2024-04-17 15:58:17","title":"Mixing Time of Open Quantum Systems via Hypocoercivity","abstract":"Understanding the mixing of open quantum systems is a fundamental problem in physics and quantum information science. Existing approaches for estimating the mixing time often rely on the spectral gap estimation of the Lindbladian generator, which can be challenging to obtain in practice. We propose a novel theoretical framework to estimate the mixing time of open quantum systems that treats the Hamiltonian and dissipative part separately, thus circumventing the need for a priori estimation of the spectral gap of the full Lindbladian generator. The technique is based on the construction of an energy functional inspired by the hypocoercivity of (classical) kinetic theory.","sentences":["Understanding the mixing of open quantum systems is a fundamental problem in physics and quantum information science.","Existing approaches for estimating the mixing time often rely on the spectral gap estimation of the Lindbladian generator, which can be challenging to obtain in practice.","We propose a novel theoretical framework to estimate the mixing time of open quantum systems that treats the Hamiltonian and dissipative part separately, thus circumventing the need for a priori estimation of the spectral gap of the full Lindbladian generator.","The technique is based on the construction of an energy functional inspired by the hypocoercivity of (classical) kinetic theory."],"url":"http://arxiv.org/abs/2404.11503v1","category":"quant-ph"}
{"created":"2024-04-17 15:57:50","title":"Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models","abstract":"In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications. For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it. Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms. In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries. To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications. We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture. Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies.","sentences":["In real world, large language models (LLMs) can serve as the assistant to help users accomplish their jobs, and also support the development of advanced applications.","For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it.","Nonetheless, users still find it challenging to compare the effectiveness of all the above methods and understand the underlying mechanisms.","In this work, we perform a detailed coarse-to-fine analysis of the inference performance of various code libraries.","To evaluate the overall effectiveness, we examine four usage scenarios within two practical applications.","We further provide both theoretical and empirical fine-grained analyses of each module in the Transformer architecture.","Our experiments yield comprehensive results that are invaluable for researchers to evaluate code libraries and improve inference strategies."],"url":"http://arxiv.org/abs/2404.11502v1","category":"cs.CL"}
{"created":"2024-04-17 15:56:16","title":"Solving the Yang-Baxter, tetrahedron and higher simplex equations using Clifford algebras","abstract":"Bethe Ansatz was discoverd in 1932. Half a century later its algebraic structure was unearthed: Yang-Baxter equation was discovered, as well as its multidimensional generalizations [tetrahedron equation and $d$-simplex equations]. Here we describe a universal method to solve these equations using Clifford algebras. The Yang-Baxter equation ($d=2$), Zamalodchikov's tetrahedron equation ($d=3$) and the Bazhanov-Stroganov equation ($d=4$) are special cases. Our solutions form a linear space. This helps us to include spectral parameters. Potential applications are discussed.","sentences":["Bethe Ansatz was discoverd in 1932.","Half a century later its algebraic structure was unearthed: Yang-Baxter equation was discovered, as well as its multidimensional generalizations","[tetrahedron equation and $d$-simplex equations].","Here we describe a universal method to solve these equations using Clifford algebras.","The Yang-Baxter equation ($d=2$), Zamalodchikov's tetrahedron equation ($d=3$) and the Bazhanov-Stroganov equation ($d=4$) are special cases.","Our solutions form a linear space.","This helps us to include spectral parameters.","Potential applications are discussed."],"url":"http://arxiv.org/abs/2404.11501v1","category":"hep-th"}
{"created":"2024-04-17 15:53:49","title":"Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models","abstract":"This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.","sentences":["This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models.","We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model's lack of robustness and sensitivity to the surface form in reasoning through complex problems.","To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem.","We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable.","Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation."],"url":"http://arxiv.org/abs/2404.11500v1","category":"cs.CL"}
{"created":"2024-04-17 15:52:38","title":"A Data-Driven Representation for Sign Language Production","abstract":"Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages. As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language. However, current state-of-the-art approaches rely on scarce linguistic resources to work. This has limited progress in the field. This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem. Thus, overcoming the need for costly annotation. Although, if available, we leverage the additional information to enhance our approach.   By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign. Where each token in the codebook can be thought of as the lexicon of our representation. Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens. Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network. Furthermore, we present a sign stitching method to effectively join tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%.","sentences":["Phonetic representations are used when recording spoken languages, but no equivalent exists for recording signed languages.","As a result, linguists have proposed several annotation systems that operate on the gloss or sub-unit level; however, these resources are notably irregular and scarce.   ","Sign Language Production (SLP) aims to automatically translate spoken language sentences into continuous sequences of sign language.","However, current state-of-the-art approaches rely on scarce linguistic resources to work.","This has limited progress in the field.","This paper introduces an innovative solution by transforming the continuous pose generation problem into a discrete sequence generation problem.","Thus, overcoming the need for costly annotation.","Although, if available, we leverage the additional information to enhance our approach.   ","By applying Vector Quantisation (VQ) to sign language data, we first learn a codebook of short motions that can be combined to create a natural sequence of sign.","Where each token in the codebook can be thought of as the lexicon of our representation.","Then using a transformer we perform a translation from spoken language text to a sequence of codebook tokens.","Each token can be directly mapped to a sequence of poses allowing the translation to be performed by a single network.","Furthermore, we present a sign stitching method to effectively join tokens together.","We evaluate on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets.","An extensive evaluation shows our approach outperforms previous methods, increasing the BLEU-1 back translation score by up to 72%."],"url":"http://arxiv.org/abs/2404.11499v1","category":"cs.CL"}
{"created":"2024-04-17 15:51:33","title":"Elastic scattering of Laguerre-Gaussian electron packets on atoms","abstract":"We explore elastic scattering of non-relativistic electrons in the form of standard Laguerre-Gaussian (sLG) and elegant Laguerre - Gaussian (eLG) packets on atomic targets in the generalized Born approximation and compare these results to the reference with Bessel-Gaussian (BG) packets. Scattering by hydrogen-like, iron, silver, and golden targets is considered. The incident electron carries a nonzero orbital angular momentum, while sLG and eLG packets have a definite radial quantum number n as well. In scattering of sLG and eLG wave packets by a macroscopic target sensitivity of the average cross section to the orbital angular momentum is observed, which is absent for BG packets. We highlight the opportunity to employ the differences in the experimental scattering results for the revelation of the properties of incident twisted electron wave packets","sentences":["We explore elastic scattering of non-relativistic electrons in the form of standard Laguerre-Gaussian (sLG) and elegant Laguerre - Gaussian (eLG) packets on atomic targets in the generalized Born approximation and compare these results to the reference with Bessel-Gaussian (BG) packets.","Scattering by hydrogen-like, iron, silver, and golden targets is considered.","The incident electron carries a nonzero orbital angular momentum, while sLG and eLG packets have a definite radial quantum number n as well.","In scattering of sLG and eLG wave packets by a macroscopic target sensitivity of the average cross section to the orbital angular momentum is observed, which is absent for BG packets.","We highlight the opportunity to employ the differences in the experimental scattering results for the revelation of the properties of incident twisted electron wave packets"],"url":"http://arxiv.org/abs/2404.11497v1","category":"physics.atom-ph"}
{"created":"2024-04-17 15:51:15","title":"Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem","abstract":"The diversity optimization is the class of optimization problems, in which we aim at finding a diverse set of good solutions. One of the frequently used approaches to solve such problems is to use evolutionary algorithms which evolve a desired diverse population. This approach is called evolutionary diversity optimization (EDO).   In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is a modification of the 2-objective benchmark function (LeadingOnes, TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations. We also analyze the runtime of the GSEMO$_D$ (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures, the total imbalance and the sorted imbalances vector. For the first measure we show that the GSEMO$_D$ optimizes it asymptotically faster than it finds a Pareto-optimal population, in $O(kn^2\\log(n))$ expected iterations, and for the second measure we show an upper bound of $O(k^2n^3\\log(n))$ expected iterations. We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures that is close to the theory predictions.","sentences":["The diversity optimization is the class of optimization problems, in which we aim at finding a diverse set of good solutions.","One of the frequently used approaches to solve such problems is to use evolutionary algorithms which evolve a desired diverse population.","This approach is called evolutionary diversity optimization (EDO).   ","In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is a modification of the 2-objective benchmark function (LeadingOnes, TrailingZeros).","We prove that the GSEMO computes a set of all Pareto-optimal solutions in $O(kn^3)$ expected iterations.","We also analyze the runtime of the GSEMO$_D$ (a modification of the GSEMO for diversity optimization) until it finds a population with the best possible diversity for two different diversity measures, the total imbalance and the sorted imbalances vector.","For the first measure we show that the GSEMO$_D$ optimizes it asymptotically faster than it finds a Pareto-optimal population, in $O(kn^2\\log(n))$ expected iterations, and for the second measure we show an upper bound of $O(k^2n^3\\log(n))$ expected iterations.","We complement our theoretical analysis with an empirical study, which shows a very similar behavior for both diversity measures that is close to the theory predictions."],"url":"http://arxiv.org/abs/2404.11496v1","category":"cs.NE"}
{"created":"2024-04-17 15:47:26","title":"arcjetCV: an open-source software to analyze material ablation","abstract":"arcjetCV is an open-source Python software designed to automate time-resolved measurements of heatshield material recession and recession rates from arcjet test video footage. This new automated and accessible capability greatly exceeds previous manual extraction methods, enabling rapid and detailed characterization of material recession for any sample with a profile video. arcjetCV automates the video segmentation process using machine learning models, including a one-dimensional (1D) Convolutional Neural Network (CNN) to infer the time-window of interest, a two-dimensional (2D) CNN for image and edge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. A graphical user interface (GUI) simplifies the user experience and an application programming interface (API) allows users to call the core functions from scripts, enabling video batch processing. arcjetCV's capability to measure time-resolved recession in turn enables characterization of non-linear processes (shrinkage, swelling, melt flows, etc.), contributing to higher fidelity validation and improved modeling of heatshield material performance. The source code associated with this article can be found at https://github.com/magnus-haw/arcjetCV.","sentences":["arcjetCV is an open-source Python software designed to automate time-resolved measurements of heatshield material recession and recession rates from arcjet test video footage.","This new automated and accessible capability greatly exceeds previous manual extraction methods, enabling rapid and detailed characterization of material recession for any sample with a profile video.","arcjetCV automates the video segmentation process using machine learning models, including a one-dimensional (1D) Convolutional Neural Network (CNN) to infer the time-window of interest, a two-dimensional (2D) CNN for image and edge segmentation, and a Local Outlier Factor (LOF) for outlier filtering.","A graphical user interface (GUI) simplifies the user experience and an application programming interface (API) allows users to call the core functions from scripts, enabling video batch processing.","arcjetCV's capability to measure time-resolved recession in turn enables characterization of non-linear processes (shrinkage, swelling, melt flows, etc.), contributing to higher fidelity validation and improved modeling of heatshield material performance.","The source code associated with this article can be found at https://github.com/magnus-haw/arcjetCV."],"url":"http://arxiv.org/abs/2404.11492v1","category":"cs.CV"}
{"created":"2024-04-17 15:45:49","title":"Multi-resolution Rescored ByteTrack for Video Object Detection on Ultra-low-power Embedded Systems","abstract":"This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a novel video object detection framework for ultra-low-power embedded processors. This method reduces the average compute load of an off-the-shelf Deep Neural Network (DNN) based object detector by up to 2.25$\\times$ by alternating the processing of high-resolution images (320$\\times$320 pixels) with multiple down-sized frames (192$\\times$192 pixels). To tackle the accuracy degradation due to the reduced image input size, MR2-ByteTrack correlates the output detections over time using the ByteTrack tracker and corrects potential misclassification using a novel probabilistic Rescore algorithm. By interleaving two down-sized images for every high-resolution one as the input of different state-of-the-art DNN object detectors with our MR2-ByteTrack, we demonstrate an average accuracy increase of 2.16% and a latency reduction of 43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference scheme using exclusively full-resolution images. Code available at: https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack","sentences":["This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a novel video object detection framework for ultra-low-power embedded processors.","This method reduces the average compute load of an off-the-shelf Deep Neural Network (DNN) based object detector by up to 2.25$\\times$ by alternating the processing of high-resolution images (320$\\times$320 pixels) with multiple down-sized frames (192$\\times$192 pixels).","To tackle the accuracy degradation due to the reduced image input size, MR2-ByteTrack correlates the output detections over time using the ByteTrack tracker and corrects potential misclassification using a novel probabilistic Rescore algorithm.","By interleaving two down-sized images for every high-resolution one as the input of different state-of-the-art DNN object detectors with our MR2-ByteTrack, we demonstrate an average accuracy increase of 2.16% and a latency reduction of 43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference scheme using exclusively full-resolution images.","Code available at: https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack"],"url":"http://arxiv.org/abs/2404.11488v1","category":"cs.CV"}
{"created":"2024-04-17 15:43:19","title":"Assessment of SDSS-Derived Galaxy Morphologies Using HST Imaging","abstract":"The Sloan Digital Sky Survey (SDSS) was foundational to the study of galaxy evolution, having revealed the bimodality of galaxies and the relationship between their structure and star-forming activity. However, ground-based optical surveys like SDSS are limited in resolution and depth which may lead to biases or poor quality in the derived morphological properties, potentially impacting our understanding of how and why galaxies cease their star formation (quench). We use archival HST imaging of ~2,000 SDSS objects to assess the reliability of SDSS-derived morphologies, taking advantage of both SDSS statistical samples and of HST's superior resolution and sensitivity. Single Sersic fitting and bulge-disk decomposition is performed on HST images for direct comparison with SDSS results. Of the three catalogs of SDSS-derived morphologies considered, none are significantly more accurate than the others. For disk-dominated galaxies (n<2.5), global Sersic indices (n) from Meert et al. 2015 (M15) are preferred. For bulge-dominated galaxies (n>2.5), Simard et al. 2011 (S11) and M15 overestimate n by ~20%, and NYU-derived global n are preferred. Global R_eff from S11 are preferred, but overestimate R_eff for the largest galaxies by 0.1 dex. SDSS-derived single-component parameters are generally significantly more robust than SDSS-derived two-component parameters. The bulge Sersic index (n_bulge) cannot be reliably constrained from SDSS imaging. The bulge-to-total (B/T) ratio can be reliably inferred from SDSS for galaxies with SDSS B/T<0.6 provided that n_bulge=4 is enforced. The difference in global n between HST and SDSS depends strongly on B/T; an empirical correction based only on it accounts for most of the systematics in global n.","sentences":["The Sloan Digital Sky Survey (SDSS) was foundational to the study of galaxy evolution, having revealed the bimodality of galaxies and the relationship between their structure and star-forming activity.","However, ground-based optical surveys like SDSS are limited in resolution and depth which may lead to biases or poor quality in the derived morphological properties, potentially impacting our understanding of how and why galaxies cease their star formation (quench).","We use archival HST imaging of ~2,000 SDSS objects to assess the reliability of SDSS-derived morphologies, taking advantage of both SDSS statistical samples and of HST's superior resolution and sensitivity.","Single Sersic fitting and bulge-disk decomposition is performed on HST images for direct comparison with SDSS results.","Of the three catalogs of SDSS-derived morphologies considered, none are significantly more accurate than the others.","For disk-dominated galaxies (n<2.5), global Sersic indices (n) from Meert et al. 2015 (M15) are preferred.","For bulge-dominated galaxies (n>2.5), Simard et al. 2011 (S11) and M15 overestimate n by ~20%, and NYU-derived global n are preferred.","Global R_eff from S11 are preferred, but overestimate R_eff for the largest galaxies by 0.1 dex.","SDSS-derived single-component parameters are generally significantly more robust than SDSS-derived two-component parameters.","The bulge Sersic index (n_bulge) cannot be reliably constrained from SDSS imaging.","The bulge-to-total (B/T) ratio can be reliably inferred from SDSS for galaxies with SDSS B/T<0.6 provided that n_bulge=4 is enforced.","The difference in global n between HST and SDSS depends strongly on B/T; an empirical correction based only on it accounts for most of the systematics in global n."],"url":"http://arxiv.org/abs/2404.11485v1","category":"astro-ph.GA"}
{"created":"2024-04-17 15:42:31","title":"Mesh Optimization for the Virtual Element Method: How Small Can an Agglomerated Mesh Become?","abstract":"We present an optimization procedure for generic polygonal or polyhedral meshes, tailored for the Virtual Element Method (VEM).   Once the local quality of the mesh elements is analyzed through a quality indicator specific to the VEM, groups of elements are agglomerated to optimize the global mesh quality.   The resulting discretization is significantly lighter: we can remove up to 80$\\%$ of the mesh elements, based on a user-set parameter, thus reducing the number of faces, edges, and vertices.   This results in a drastic reduction of the total number of degrees of freedom associated with a discrete problem defined over the mesh with the VEM, in particular, for high-order formulations.   We show how the VEM convergence rate is preserved in the optimized meshes, and the approximation errors are comparable with those obtained with the original ones.   We observe that the optimization has a regularization effect over low-quality meshes, removing the most pathological elements.   This regularization effect is evident in cases where the original meshes cause the VEM to diverge, while the optimized meshes lead to convergence.   We conclude by showing how the optimization of a real CAD model can be used effectively in the simulation of a time-dependent problem.","sentences":["We present an optimization procedure for generic polygonal or polyhedral meshes, tailored for the Virtual Element Method (VEM).   ","Once the local quality of the mesh elements is analyzed through a quality indicator specific to the VEM, groups of elements are agglomerated to optimize the global mesh quality.   ","The resulting discretization is significantly lighter: we can remove up to 80$\\%$ of the mesh elements, based on a user-set parameter, thus reducing the number of faces, edges, and vertices.   ","This results in a drastic reduction of the total number of degrees of freedom associated with a discrete problem defined over the mesh with the VEM, in particular, for high-order formulations.   ","We show how the VEM convergence rate is preserved in the optimized meshes, and the approximation errors are comparable with those obtained with the original ones.   ","We observe that the optimization has a regularization effect over low-quality meshes, removing the most pathological elements.   ","This regularization effect is evident in cases where the original meshes cause the VEM to diverge, while the optimized meshes lead to convergence.   ","We conclude by showing how the optimization of a real CAD model can be used effectively in the simulation of a time-dependent problem."],"url":"http://arxiv.org/abs/2404.11484v1","category":"math.NA"}
{"created":"2024-04-17 15:40:45","title":"AgentKit: Flow Engineering with Graphs, not Coding","abstract":"We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents. AgentKit offers a unified framework for explicitly constructing a complex \"thought process\" from simple natural language prompts. The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask. The user then puts together chains of nodes, like stacking LEGO pieces. The chains of nodes can be designed to explicitly enforce a naturally structured \"thought process\". For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc. The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions. In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience. Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter. These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications. https://github.com/holmeswww/AgentKit","sentences":["We propose an intuitive LLM prompting framework (AgentKit) for multifunctional agents.","AgentKit offers a unified framework for explicitly constructing a complex \"thought process\" from simple natural language prompts.","The basic building block in AgentKit is a node, containing a natural language prompt for a specific subtask.","The user then puts together chains of nodes, like stacking LEGO pieces.","The chains of nodes can be designed to explicitly enforce a naturally structured \"thought process\".","For example, for the task of writing a paper, one may start with the thought process of 1) identify a core message, 2) identify prior research gaps, etc.","The nodes in AgentKit can be designed and combined in different ways to implement multiple advanced capabilities including on-the-fly hierarchical planning, reflection, and learning from interactions.","In addition, due to the modular nature and the intuitive design to simulate explicit human thought process, a basic agent could be implemented as simple as a list of prompts for the subtasks and therefore could be designed and tuned by someone without any programming experience.","Quantitatively, we show that agents designed through AgentKit achieve SOTA performance on WebShop and Crafter.","These advances underscore AgentKit's potential in making LLM agents effective and accessible for a wider range of applications.","https://github.com/holmeswww/AgentKit"],"url":"http://arxiv.org/abs/2404.11483v1","category":"cs.AI"}
{"created":"2024-04-17 15:40:17","title":"BSDE-based stochastic control for optimal reinsurance in a dynamic contagion model","abstract":"We investigate the optimal reinsurance problem in the risk model with jump clustering features introduced in [7]. This modeling framework is inspired by the concept initially proposed in [15], combining Hawkes and Cox processes with shot noise intensity models. Specifically, these processes describe self-exciting and externally excited jumps in the claim arrival intensity, respectively. The insurer aims to maximize the expected exponential utility of terminal wealth for general reinsurance contracts and reinsurance premiums. We discuss two different methodologies: the classical stochastic control approach based on the Hamilton-Jacobi-Bellman (HJB) equation and a backward stochastic differential equation (BSDE) approach. In a Markovian setting, differently from the classical HJB-approach, the BSDE method enables us to solve the problem without imposing any requirements for regularity on the associated value function. We provide a Verification Theorem in terms of a suitable BSDE driven by a two-dimensional marked point process and we prove an existence result relaying on the theory developed in [27] for stochastic Lipschitz generators. After discussing the optimal strategy for general reinsurance contracts and reinsurance premiums, we provide more explicit results in some relevant cases. Finally, we provide comparison results that highlight the heightened risk stemming from the self-exciting component in contrast to the externally-excited counterpart and discuss the monotonicity property of the value function.","sentences":["We investigate the optimal reinsurance problem in the risk model with jump clustering features introduced in [7].","This modeling framework is inspired by the concept initially proposed in [15], combining Hawkes and Cox processes with shot noise intensity models.","Specifically, these processes describe self-exciting and externally excited jumps in the claim arrival intensity, respectively.","The insurer aims to maximize the expected exponential utility of terminal wealth for general reinsurance contracts and reinsurance premiums.","We discuss two different methodologies: the classical stochastic control approach based on the Hamilton-Jacobi-Bellman (HJB) equation and a backward stochastic differential equation (BSDE) approach.","In a Markovian setting, differently from the classical HJB-approach, the BSDE method enables us to solve the problem without imposing any requirements for regularity on the associated value function.","We provide a Verification Theorem in terms of a suitable BSDE driven by a two-dimensional marked point process and we prove an existence result relaying on the theory developed in [27] for stochastic Lipschitz generators.","After discussing the optimal strategy for general reinsurance contracts and reinsurance premiums, we provide more explicit results in some relevant cases.","Finally, we provide comparison results that highlight the heightened risk stemming from the self-exciting component in contrast to the externally-excited counterpart and discuss the monotonicity property of the value function."],"url":"http://arxiv.org/abs/2404.11482v1","category":"math.OC"}
{"created":"2024-04-17 15:33:35","title":"Zeros of generalized hypergeometric polynomials via finite free convolution. Applications to multiple orthogonality","abstract":"We address the problem of the weak asymptotic behavior of zeros of families of generalized hypergeometric polynomials as their degree tends to infinity. The main tool is the representation of such polynomials as a finite free convolution of simpler elements; this representation is preserved in the asymptotic regime, so we can formally write the limit zero distribution of these polynomials as a free convolution of explicitly computable measures. We derive a simple expression for the S-transform of the limit distribution, which turns out to be a rational function, and a representation of the Kamp\\'e de F\\'eriet polynomials in terms of finite free convolutions. We apply these tools, as well as those from [arXiv:2309.10970], to the study of some well-known families of multiple orthogonal polynomials (Jacobi-Pi\\~neiro and multiple Laguerre of the first and second kinds), obtaining results on their zeros, such as interlacing, monotonicity, and asymptotics.","sentences":["We address the problem of the weak asymptotic behavior of zeros of families of generalized hypergeometric polynomials as their degree tends to infinity.","The main tool is the representation of such polynomials as a finite free convolution of simpler elements; this representation is preserved in the asymptotic regime, so we can formally write the limit zero distribution of these polynomials as a free convolution of explicitly computable measures.","We derive a simple expression for the S-transform of the limit distribution, which turns out to be a rational function, and a representation of the Kamp\\'e de F\\'eriet polynomials in terms of finite free convolutions.","We apply these tools, as well as those from [arXiv:2309.10970], to the study of some well-known families of multiple orthogonal polynomials (Jacobi-Pi\\~neiro and multiple Laguerre of the first and second kinds), obtaining results on their zeros, such as interlacing, monotonicity, and asymptotics."],"url":"http://arxiv.org/abs/2404.11479v1","category":"math.CA"}
{"created":"2024-04-17 15:33:34","title":"Inverse problem in energy-dependent potentials using semi-classical methods","abstract":"Wave equations with energy-dependent potentials appear in many areas of physics, ranging from nuclear physics to black hole perturbation theory. In this work, we use the semi-classical WKB method to first revisit the computation of bound states of potential wells and reflection/transmission coefficients in terms of the Bohr-Sommerfeld rule and the Gamow formula. We then discuss the inverse problem, in which the latter observables are used as a starting point to reconstruct the properties of the potentials. By extending known inversion techniques to energy-dependent potentials, we demonstrate that so-called width-equivalent or WKB-equivalent potentials are not isospectral anymore. Instead, we explicitly demonstrate that constructing quasi-isospectral potentials with the inverse techniques is still possible. Those reconstructed, energy-independent potentials share key properties with the width-equivalent potentials. We report that including energy-dependent terms allows for a rich phenomenology, particularly for the energy-independent equivalent potentials.","sentences":["Wave equations with energy-dependent potentials appear in many areas of physics, ranging from nuclear physics to black hole perturbation theory.","In this work, we use the semi-classical WKB method to first revisit the computation of bound states of potential wells and reflection/transmission coefficients in terms of the Bohr-Sommerfeld rule and the Gamow formula.","We then discuss the inverse problem, in which the latter observables are used as a starting point to reconstruct the properties of the potentials.","By extending known inversion techniques to energy-dependent potentials, we demonstrate that so-called width-equivalent or WKB-equivalent potentials are not isospectral anymore.","Instead, we explicitly demonstrate that constructing quasi-isospectral potentials with the inverse techniques is still possible.","Those reconstructed, energy-independent potentials share key properties with the width-equivalent potentials.","We report that including energy-dependent terms allows for a rich phenomenology, particularly for the energy-independent equivalent potentials."],"url":"http://arxiv.org/abs/2404.11478v1","category":"hep-ph"}
{"created":"2024-04-17 15:32:58","title":"Discovering Nuclear Models from Symbolic Machine Learning","abstract":"Numerous phenomenological nuclear models have been proposed to describe specific observables within different regions of the nuclear chart. However, developing a unified model that describes the complex behavior of all nuclei remains an open challenge. Here, we explore whether novel symbolic Machine Learning (ML) can rediscover traditional nuclear physics models or identify alternatives with improved simplicity, fidelity, and predictive power. To address this challenge, we developed a Multi-objective Iterated Symbolic Regression approach that handles symbolic regressions over multiple target observables, accounts for experimental uncertainties and is robust against high-dimensional problems. As a proof of principle, we applied this method to describe the nuclear binding energies and charge radii of light and medium mass nuclei. Our approach identified simple analytical relationships based on the number of protons and neutrons, providing interpretable models with precision comparable to state-of-the-art nuclear models. Additionally, we integrated this ML-discovered model with an existing complementary model to estimate the limits of nuclear stability. These results highlight the potential of symbolic ML to develop accurate nuclear models and guide our description of complex many-body problems.","sentences":["Numerous phenomenological nuclear models have been proposed to describe specific observables within different regions of the nuclear chart.","However, developing a unified model that describes the complex behavior of all nuclei remains an open challenge.","Here, we explore whether novel symbolic Machine Learning (ML) can rediscover traditional nuclear physics models or identify alternatives with improved simplicity, fidelity, and predictive power.","To address this challenge, we developed a Multi-objective Iterated Symbolic Regression approach that handles symbolic regressions over multiple target observables, accounts for experimental uncertainties and is robust against high-dimensional problems.","As a proof of principle, we applied this method to describe the nuclear binding energies and charge radii of light and medium mass nuclei.","Our approach identified simple analytical relationships based on the number of protons and neutrons, providing interpretable models with precision comparable to state-of-the-art nuclear models.","Additionally, we integrated this ML-discovered model with an existing complementary model to estimate the limits of nuclear stability.","These results highlight the potential of symbolic ML to develop accurate nuclear models and guide our description of complex many-body problems."],"url":"http://arxiv.org/abs/2404.11477v1","category":"nucl-th"}
{"created":"2024-04-17 15:32:56","title":"Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and Regulatory Measures in the EU AI Act","abstract":"Technological innovations have shown remarkable capabilities to benefit and harm society alike. AI constitutes a democratized sophisticated technology accessible to large parts of society, including malicious actors. This work proposes a taxonomy focusing on on (geo)political risks associated with AI. It identifies 12 risks in total divided into four categories: (1) Geopolitical Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks, and (4) Privacy and Trust Violations. Incorporating a regulatory side, this paper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the landmark regulation has the potential to have a positive top-down impact concerning AI risk reduction but needs regulatory adjustments to mitigate risks more comprehensively. Regulatory exceptions for open-source models, excessively high parameters for the classification of GPAI models as a systemic risk, and the exclusion of systems designed exclusively for military purposes from the regulation's obligations leave room for future action.","sentences":["Technological innovations have shown remarkable capabilities to benefit and harm society alike.","AI constitutes a democratized sophisticated technology accessible to large parts of society, including malicious actors.","This work proposes a taxonomy focusing on on (geo)political risks associated with AI.","It identifies 12 risks in total divided into four categories: (1) Geopolitical Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks, and (4) Privacy and Trust Violations.","Incorporating a regulatory side, this paper conducts a policy assessment of the EU AI Act.","Adopted in March 2023, the landmark regulation has the potential to have a positive top-down impact concerning AI risk reduction but needs regulatory adjustments to mitigate risks more comprehensively.","Regulatory exceptions for open-source models, excessively high parameters for the classification of GPAI models as a systemic risk, and the exclusion of systems designed exclusively for military purposes from the regulation's obligations leave room for future action."],"url":"http://arxiv.org/abs/2404.11476v1","category":"cs.AI"}
{"created":"2024-04-17 15:31:06","title":"AdaIR: Exploiting Underlying Similarities of Image Restoration Tasks with Adapters","abstract":"Existing image restoration approaches typically employ extensive networks specifically trained for designated degradations. Despite being effective, such methods inevitably entail considerable storage costs and computational overheads due to the reliance on task-specific networks. In this work, we go beyond this well-established framework and exploit the inherent commonalities among image restoration tasks. The primary objective is to identify components that are shareable across restoration tasks and augment the shared components with modules specifically trained for individual tasks. Towards this goal, we propose AdaIR, a novel framework that enables low storage cost and efficient training without sacrificing performance. Specifically, a generic restoration network is first constructed through self-supervised pre-training using synthetic degradations. Subsequent to the pre-training phase, adapters are trained to adapt the pre-trained network to specific degradations. AdaIR requires solely the training of lightweight, task-specific modules, ensuring a more efficient storage and training regimen. We have conducted extensive experiments to validate the effectiveness of AdaIR and analyze the influence of the pre-training strategy on discovering shareable components. Extensive experimental results show that AdaIR achieves outstanding results on multi-task restoration while utilizing significantly fewer parameters (1.9 MB) and less training time (7 hours) for each restoration task. The source codes and trained models will be released.","sentences":["Existing image restoration approaches typically employ extensive networks specifically trained for designated degradations.","Despite being effective, such methods inevitably entail considerable storage costs and computational overheads due to the reliance on task-specific networks.","In this work, we go beyond this well-established framework and exploit the inherent commonalities among image restoration tasks.","The primary objective is to identify components that are shareable across restoration tasks and augment the shared components with modules specifically trained for individual tasks.","Towards this goal, we propose AdaIR, a novel framework that enables low storage cost and efficient training without sacrificing performance.","Specifically, a generic restoration network is first constructed through self-supervised pre-training using synthetic degradations.","Subsequent to the pre-training phase, adapters are trained to adapt the pre-trained network to specific degradations.","AdaIR requires solely the training of lightweight, task-specific modules, ensuring a more efficient storage and training regimen.","We have conducted extensive experiments to validate the effectiveness of AdaIR and analyze the influence of the pre-training strategy on discovering shareable components.","Extensive experimental results show that AdaIR achieves outstanding results on multi-task restoration while utilizing significantly fewer parameters (1.9 MB) and less training time (7 hours) for each restoration task.","The source codes and trained models will be released."],"url":"http://arxiv.org/abs/2404.11475v1","category":"cs.CV"}
{"created":"2024-04-17 15:28:53","title":"Towards Highly Realistic Artistic Style Transfer via Stable Diffusion with Step-aware and Layer-aware Prompt","abstract":"Artistic style transfer aims to transfer the learned artistic style onto an arbitrary content image, generating artistic stylized images. Existing generative adversarial network-based methods fail to generate highly realistic stylized images and always introduce obvious artifacts and disharmonious patterns. Recently, large-scale pre-trained diffusion models opened up a new way for generating highly realistic artistic stylized images. However, diffusion model-based methods generally fail to preserve the content structure of input content images well, introducing some undesired content structure and style patterns. To address the above problems, we propose a novel pre-trained diffusion-based artistic style transfer method, called LSAST, which can generate highly realistic artistic stylized images while preserving the content structure of input content images well, without bringing obvious artifacts and disharmonious style patterns. Specifically, we introduce a Step-aware and Layer-aware Prompt Space, a set of learnable prompts, which can learn the style information from the collection of artworks and dynamically adjusts the input images' content structure and style pattern. To train our prompt space, we propose a novel inversion method, called Step-ware and Layer-aware Prompt Inversion, which allows the prompt space to learn the style information of the artworks collection. In addition, we inject a pre-trained conditional branch of ControlNet into our LSAST, which further improved our framework's ability to maintain content structure. Extensive experiments demonstrate that our proposed method can generate more highly realistic artistic stylized images than the state-of-the-art artistic style transfer methods.","sentences":["Artistic style transfer aims to transfer the learned artistic style onto an arbitrary content image, generating artistic stylized images.","Existing generative adversarial network-based methods fail to generate highly realistic stylized images and always introduce obvious artifacts and disharmonious patterns.","Recently, large-scale pre-trained diffusion models opened up a new way for generating highly realistic artistic stylized images.","However, diffusion model-based methods generally fail to preserve the content structure of input content images well, introducing some undesired content structure and style patterns.","To address the above problems, we propose a novel pre-trained diffusion-based artistic style transfer method, called LSAST, which can generate highly realistic artistic stylized images while preserving the content structure of input content images well, without bringing obvious artifacts and disharmonious style patterns.","Specifically, we introduce a Step-aware and Layer-aware Prompt Space, a set of learnable prompts, which can learn the style information from the collection of artworks and dynamically adjusts the input images' content structure and style pattern.","To train our prompt space, we propose a novel inversion method, called Step-ware and Layer-aware Prompt Inversion, which allows the prompt space to learn the style information of the artworks collection.","In addition, we inject a pre-trained conditional branch of ControlNet into our LSAST, which further improved our framework's ability to maintain content structure.","Extensive experiments demonstrate that our proposed method can generate more highly realistic artistic stylized images than the state-of-the-art artistic style transfer methods."],"url":"http://arxiv.org/abs/2404.11474v1","category":"cs.CV"}
{"created":"2024-04-17 15:26:45","title":"A Course on Lie algebras and Chevalley groups","abstract":"These are notes of a graduate course on semisimple Lie algebras and Chevalley groups (over arbitrary fields). The aim is to give a self-contained introduction to these topics based on Lusztig's recent simplified approach, which is inspired by the general theory of ``canonical'' bases. Many constructions are of a purely combinatorial nature and, hence, can be implemented on a computer. We explicitly incorporate such algorithmic methods in our treatment. This is the first part of a planned book project.","sentences":["These are notes of a graduate course on semisimple Lie algebras and Chevalley groups (over arbitrary fields).","The aim is to give a self-contained introduction to these topics based on Lusztig's recent simplified approach, which is inspired by the general theory of ``canonical'' bases.","Many constructions are of a purely combinatorial nature and, hence, can be implemented on a computer.","We explicitly incorporate such algorithmic methods in our treatment.","This is the first part of a planned book project."],"url":"http://arxiv.org/abs/2404.11472v1","category":"math.RT"}
{"created":"2024-04-17 15:24:09","title":"Non-linear power spectrum and forecasts for Generalized Cubic Covariant Galileon","abstract":"To fully exploit the data from next generation surveys, we need an accurate modelling of the matter power spectrum up to non-linear scales. Therefore in this work we present the halo model reaction framework for the Generalized Cubic Covariant Galileon (GCCG) model, a modified gravity model within the Horndeski class of theories which extends the cubic covariant Galileon (G3) by including power laws of the derivatives of the scalar field in the K-essence and cubic terms. We modify the publicly available software ReACT for the GCCG in order to obtain an accurate prediction of the non-linear power spectrum. In the limit of the G3 model we compare the modified ReACT code to $N$-body simulations and we find agreement within 5\\% for a wide range of scales and redshifts. We then study the relevant effects of the modifications introduced by the GCCG on the non-linear matter power spectrum. Finally, we provide forecasts from spectroscopic and photometric primary probes by next generation surveys using a Fisher matrix method. We show that future data will be able to constrain at 1$\\sigma$ the two additional parameters of the model at the percent level and that considering non-linear corrections to the matter power spectrum beyond the linear regime is crucial to obtain this result.","sentences":["To fully exploit the data from next generation surveys, we need an accurate modelling of the matter power spectrum up to non-linear scales.","Therefore in this work we present the halo model reaction framework for the Generalized Cubic Covariant Galileon (GCCG) model, a modified gravity model within the Horndeski class of theories which extends the cubic covariant Galileon (G3) by including power laws of the derivatives of the scalar field in the K-essence and cubic terms.","We modify the publicly available software ReACT for the GCCG in order to obtain an accurate prediction of the non-linear power spectrum.","In the limit of the G3 model we compare the modified ReACT code to $N$-body simulations and we find agreement within 5\\% for a wide range of scales and redshifts.","We then study the relevant effects of the modifications introduced by the GCCG on the non-linear matter power spectrum.","Finally, we provide forecasts from spectroscopic and photometric primary probes by next generation surveys using a Fisher matrix method.","We show that future data will be able to constrain at 1$\\sigma$ the two additional parameters of the model at the percent level and that considering non-linear corrections to the matter power spectrum beyond the linear regime is crucial to obtain this result."],"url":"http://arxiv.org/abs/2404.11471v1","category":"astro-ph.CO"}
{"created":"2024-04-17 15:17:57","title":"Designing Touchscreen Menu Interfaces for In-Vehicle Infotainment Systems: the Effect of Depth and Breadth Trade-off and Task Types on Visual-Manual Distraction","abstract":"Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety. Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation. Depth and breadth trade-offs for structured navigation have been studied. However, little is known on how secondary task characteristics interact with those trade-offs. In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction. Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance. Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state). The results confirm our hypothesis. Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks. We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved.","sentences":["Multitasking with a touch screen user-interface while driving is known to impact negatively driving performance and safety.","Literature shows that list scrolling interfaces generate more visual-manual distraction than structured menus and sequential navigation.","Depth and breadth trade-offs for structured navigation have been studied.","However, little is known on how secondary task characteristics interact with those trade-offs.","In this study, we make the hypothesis that both menu's depth and task complexity interact in generating visual-manual distraction.","Using a driving simulation setup, we collected telemetry and eye-tracking data to evaluate driving performance.","Participants were multitasking with a mobile app, presenting a range of eight depth and breadth trade-offs under three types of secondary tasks, involving different cognitive operations (Systematic reading, Search for an item, Memorize items' state).","The results confirm our hypothesis.","Systematic interaction with menu items generated a visual demand that increased with menu's depth, while visual demand reach an optimum for Search and Memory tasks.","We discuss implications for design: In a multitasking context, display design effectiveness must be assessed while considering menu's layout but also cognitive processes involved."],"url":"http://arxiv.org/abs/2404.11469v1","category":"cs.HC"}
{"created":"2024-04-17 15:16:50","title":"Conformal Killing cosmology with Sinyukov tensors: geometry and growth of structures","abstract":"We introduce perfect fluid Sinyukov-like tensors, a special kind of conformal Killing tensors, and prove a characterization of generalized RW spacetimes. In cosmological (RW) framework, we study the Friedmann equations of Conformal Killing Gravity. In addition to ordinary matter they contain a dark fluid and a Lambda term emergent from the conformal Killing extension. We then solve the equation for the evolution of the density contrast in matter-dominated universe. The main result is that the simultaneous presence of the dark sector and the Lambda term gives a deviation from Lambda-CDM and ordinary GR towards an enhancement of the overdensity growth.","sentences":["We introduce perfect fluid Sinyukov-like tensors, a special kind of conformal Killing tensors, and prove a characterization of generalized RW spacetimes.","In cosmological (RW) framework, we study the Friedmann equations of Conformal Killing Gravity.","In addition to ordinary matter they contain a dark fluid and a Lambda term emergent from the conformal Killing extension.","We then solve the equation for the evolution of the density contrast in matter-dominated universe.","The main result is that the simultaneous presence of the dark sector and the Lambda term gives a deviation from Lambda-CDM and ordinary GR towards an enhancement of the overdensity growth."],"url":"http://arxiv.org/abs/2404.11468v1","category":"gr-qc"}
{"created":"2024-04-17 15:09:31","title":"Using Game Engines and Machine Learning to Create Synthetic Satellite Imagery for a Tabletop Verification Exercise","abstract":"Satellite imagery is regarded as a great opportunity for citizen-based monitoring of activities of interest. Relevant imagery may however not be available at sufficiently high resolution, quality, or cadence -- let alone be uniformly accessible to open-source analysts. This limits an assessment of the true long-term potential of citizen-based monitoring of nuclear activities using publicly available satellite imagery. In this article, we demonstrate how modern game engines combined with advanced machine-learning techniques can be used to generate synthetic imagery of sites of interest with the ability to choose relevant parameters upon request; these include time of day, cloud cover, season, or level of activity onsite. At the same time, resolution and off-nadir angle can be adjusted to simulate different characteristics of the satellite. While there are several possible use-cases for synthetic imagery, here we focus on its usefulness to support tabletop exercises in which simple monitoring scenarios can be examined to better understand verification capabilities enabled by new satellite constellations and very short revisit times.","sentences":["Satellite imagery is regarded as a great opportunity for citizen-based monitoring of activities of interest.","Relevant imagery may however not be available at sufficiently high resolution, quality, or cadence -- let alone be uniformly accessible to open-source analysts.","This limits an assessment of the true long-term potential of citizen-based monitoring of nuclear activities using publicly available satellite imagery.","In this article, we demonstrate how modern game engines combined with advanced machine-learning techniques can be used to generate synthetic imagery of sites of interest with the ability to choose relevant parameters upon request; these include time of day, cloud cover, season, or level of activity onsite.","At the same time, resolution and off-nadir angle can be adjusted to simulate different characteristics of the satellite.","While there are several possible use-cases for synthetic imagery, here we focus on its usefulness to support tabletop exercises in which simple monitoring scenarios can be examined to better understand verification capabilities enabled by new satellite constellations and very short revisit times."],"url":"http://arxiv.org/abs/2404.11461v1","category":"cs.CV"}
{"created":"2024-04-17 15:05:51","title":"Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem","abstract":"This paper aims to develop a learning method for a special class of traveling salesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which finds the shortest tour along a sequence of one-to-one pickup-and-delivery nodes. One-to-one here means that the transported people or goods are associated with designated pairs of pickup and delivery nodes, in contrast to that indistinguishable goods can be delivered to any nodes. In PDTSP, precedence constraints need to be satisfied that each pickup node must be visited before its corresponding delivery node. Classic operations research (OR) algorithms for PDTSP are difficult to scale to large-sized problems. Recently, reinforcement learning (RL) has been applied to TSPs. The basic idea is to explore and evaluate visiting sequences in a solution space. However, this approach could be less computationally efficient, as it has to potentially evaluate many infeasible solutions of which precedence constraints are violated. To restrict solution search within a feasible space, we utilize operators that always map one feasible solution to another, without spending time exploring the infeasible solution space. Such operators are evaluated and selected as policies to solve PDTSPs in an RL framework. We make a comparison of our method and baselines, including classic OR algorithms and existing learning methods. Results show that our approach can find tours shorter than baselines.","sentences":["This paper aims to develop a learning method for a special class of traveling salesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which finds the shortest tour along a sequence of one-to-one pickup-and-delivery nodes.","One-to-one here means that the transported people or goods are associated with designated pairs of pickup and delivery nodes, in contrast to that indistinguishable goods can be delivered to any nodes.","In PDTSP, precedence constraints need to be satisfied that each pickup node must be visited before its corresponding delivery node.","Classic operations research (OR) algorithms for PDTSP are difficult to scale to large-sized problems.","Recently, reinforcement learning (RL) has been applied to TSPs.","The basic idea is to explore and evaluate visiting sequences in a solution space.","However, this approach could be less computationally efficient, as it has to potentially evaluate many infeasible solutions of which precedence constraints are violated.","To restrict solution search within a feasible space, we utilize operators that always map one feasible solution to another, without spending time exploring the infeasible solution space.","Such operators are evaluated and selected as policies to solve PDTSPs in an RL framework.","We make a comparison of our method and baselines, including classic OR algorithms and existing learning methods.","Results show that our approach can find tours shorter than baselines."],"url":"http://arxiv.org/abs/2404.11458v1","category":"cs.AI"}
{"created":"2024-04-17 15:05:03","title":"Unifying Bias and Unfairness in Information Retrieval: A Survey of Challenges and Opportunities with Large Language Models","abstract":"With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.","sentences":["With the rapid advancement of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift.","This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem.","In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs.","We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment.","Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation.","In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues.","Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era.","We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey."],"url":"http://arxiv.org/abs/2404.11457v1","category":"cs.IR"}
{"created":"2024-04-17 14:55:49","title":"Real-Time Trajectory Synthesis with Local Differential Privacy","abstract":"Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems. Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues. Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis. Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications. To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams. Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection. We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality. The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy. We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios. The empirical results demonstrate the superiority and versatility of our proposed framework.","sentences":["Trajectory streams are being generated from location-aware devices, such as smartphones and in-vehicle navigation systems.","Due to the sensitive nature of the location data, directly sharing user trajectories suffers from privacy leakage issues.","Local differential privacy (LDP), which perturbs sensitive data on the user side before it is shared or analyzed, emerges as a promising solution for private trajectory stream collection and analysis.","Unfortunately, existing stream release approaches often neglect the rich spatial-temporal context information within trajectory streams, resulting in suboptimal utility and limited types of downstream applications.","To this end, we propose RetraSyn, a novel real-time trajectory synthesis framework, which is able to perform on-the-fly trajectory synthesis based on the mobility patterns privately extracted from users' trajectory streams.","Thus, the downstream trajectory analysis can be performed on the high-utility synthesized data with privacy protection.","We also take the genuine behaviors of real-world mobile travelers into consideration, ensuring authenticity and practicality.","The key components of RetraSyn include the global mobility model, dynamic mobility update mechanism, real-time synthesis, and adaptive allocation strategy.","We conduct extensive experiments on multiple real-world and synthetic trajectory datasets under various location-based utility metrics, encompassing both streaming and historical scenarios.","The empirical results demonstrate the superiority and versatility of our proposed framework."],"url":"http://arxiv.org/abs/2404.11450v1","category":"cs.DB"}
{"created":"2024-04-17 14:55:03","title":"Research on emotionally intelligent dialogue generation based on automatic dialogue system","abstract":"Automated dialogue systems are important applications of artificial intelligence, and traditional systems struggle to understand user emotions and provide empathetic feedback. This study integrates emotional intelligence technology into automated dialogue systems and creates a dialogue generation model with emotional intelligence through deep learning and natural language processing techniques. The model can detect and understand a wide range of emotions and specific pain signals in real time, enabling the system to provide empathetic interaction. By integrating the results of the study \"Can artificial intelligence detect pain and express pain empathy?\", the model's ability to understand the subtle elements of pain empathy has been enhanced, setting higher standards for emotional intelligence dialogue systems. The project aims to provide theoretical understanding and practical suggestions to integrate advanced emotional intelligence capabilities into dialogue systems, thereby improving user experience and interaction quality.","sentences":["Automated dialogue systems are important applications of artificial intelligence, and traditional systems struggle to understand user emotions and provide empathetic feedback.","This study integrates emotional intelligence technology into automated dialogue systems and creates a dialogue generation model with emotional intelligence through deep learning and natural language processing techniques.","The model can detect and understand a wide range of emotions and specific pain signals in real time, enabling the system to provide empathetic interaction.","By integrating the results of the study \"Can artificial intelligence detect pain and express pain empathy?","\", the model's ability to understand the subtle elements of pain empathy has been enhanced, setting higher standards for emotional intelligence dialogue systems.","The project aims to provide theoretical understanding and practical suggestions to integrate advanced emotional intelligence capabilities into dialogue systems, thereby improving user experience and interaction quality."],"url":"http://arxiv.org/abs/2404.11447v1","category":"cs.AI"}
{"created":"2024-04-17 14:54:58","title":"Open-Ended Wargames with Large Language Models","abstract":"Wargames are a powerful tool for understanding and rehearsing real-world decision making. Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes. There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses. Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames. We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames. With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof. We describe its software architecture conceptually and release an open-source implementation alongside this publication. As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis. We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem.","sentences":["Wargames are a powerful tool for understanding and rehearsing real-world decision making.","Automated play of wargames using artificial intelligence (AI) enables possibilities beyond those of human-conducted games, such as playing the game many times over to see a range of possible outcomes.","There are two categories of wargames: quantitative games, with discrete types of moves, and qualitative games, which revolve around open-ended responses.","Historically, automation efforts have focused on quantitative games, but large language models (LLMs) make it possible to automate qualitative wargames.","We introduce \"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative wargames.","With Snow Globe, every stage of a text-based qualitative wargame from scenario preparation to post-game analysis can be optionally carried out by AI, humans, or a combination thereof.","We describe its software architecture conceptually and release an open-source implementation alongside this publication.","As case studies, we simulate a tabletop exercise about an AI incident response and a political wargame about a geopolitical crisis.","We discuss potential applications of the approach and how it fits into the broader wargaming ecosystem."],"url":"http://arxiv.org/abs/2404.11446v1","category":"cs.CL"}
{"created":"2024-04-17 14:53:03","title":"Prediction of Unmanned Surface Vessel Motion Attitude Based on CEEMDAN-PSO-SVM","abstract":"Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment. However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process. This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum. Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work. A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats. Simulation results validate its superior prediction accuracy compared to traditional prediction models. For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model.","sentences":["Unmanned boats, while navigating at sea, utilize active compensation systems to mitigate wave disturbances experienced by onboard instruments and equipment.","However, there exists a lag in the measurement of unmanned boat attitudes, thus introducing unmanned boat motion attitude prediction to compensate for the lag in the signal acquisition process.","This paper, based on the basic principles of waves, derives the disturbance patterns of waves on unmanned boats from the wave energy spectrum.","Through simulation analysis of unmanned boat motion attitudes, motion attitude data is obtained, providing experimental data for subsequent work.","A combined prediction model based on Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm Optimization (PSO), and Support Vector Machine (SVM) is designed to predict the motion attitude of unmanned boats.","Simulation results validate its superior prediction accuracy compared to traditional prediction models.","For example, in terms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM model."],"url":"http://arxiv.org/abs/2404.11443v1","category":"cs.AI"}
{"created":"2024-04-17 14:49:21","title":"Solving Power Grid Optimization Problems with Rydberg Atoms","abstract":"The rapid development of neutral atom quantum hardware provides a unique opportunity to design hardware-centered algorithms for solving real-world problems aimed at establishing quantum utility. In this work, we study the performance of two such algorithms on solving MaxCut problem for various weighted graphs. The first method uses a state-of-the-art machine learning tool to optimize the pulse shape and embedding of the graph using an adiabatic Ansatz to find the ground state. We tested the performance of this method on finding maximum power section task of the IEEE 9-bus power system and obtaining MaxCut of randomly generated problems of size up to 12 on the Aquila quantum processor. To the best of our knowledge, this work presents the first MaxCut results on Quera's Aquila quantum hardware. Our experiments run on Aquila demonstrate that even though the probability of obtaining the solution is reduced, one can still solve the MaxCut problem on cloud-accessed neutral atom quantum hardware. The second method uses local detuning, which is an emergent update on the Aquila hardware, to obtain a near exact realization of the standard QAOA Ansatz with similar performance. Finally, we study the fidelity throughout the time evolution realized in the adiabatic method as a benchmark for the IEEE 9-bus power grid graph state.","sentences":["The rapid development of neutral atom quantum hardware provides a unique opportunity to design hardware-centered algorithms for solving real-world problems aimed at establishing quantum utility.","In this work, we study the performance of two such algorithms on solving MaxCut problem for various weighted graphs.","The first method uses a state-of-the-art machine learning tool to optimize the pulse shape and embedding of the graph using an adiabatic Ansatz to find the ground state.","We tested the performance of this method on finding maximum power section task of the IEEE 9-bus power system and obtaining MaxCut of randomly generated problems of size up to 12 on the Aquila quantum processor.","To the best of our knowledge, this work presents the first MaxCut results on Quera's Aquila quantum hardware.","Our experiments run on Aquila demonstrate that even though the probability of obtaining the solution is reduced, one can still solve the MaxCut problem on cloud-accessed neutral atom quantum hardware.","The second method uses local detuning, which is an emergent update on the Aquila hardware, to obtain a near exact realization of the standard QAOA Ansatz with similar performance.","Finally, we study the fidelity throughout the time evolution realized in the adiabatic method as a benchmark for the IEEE 9-bus power grid graph state."],"url":"http://arxiv.org/abs/2404.11440v1","category":"quant-ph"}
{"created":"2024-04-17 14:49:03","title":"A waypoint based approach to visibility in performance based fire safety design","abstract":"In performance-based fire safety design, ensuring safe egress, e.g. by visibility of safety signs, is a crucial safety goal. Compliance with the building requirements is often demonstrated by simulations of smoke spread. Numerical models like the Fire Dynamics Simulator generally compute visibility as a local quantity using the light extinction coefficient, without the consideration of the actual light path to a safety sign. Here, visibility maps are introduced, providing an approach for post-processing fire simulation data. They indicate safe areas along egress routes, with respect to visibility. At each location, the available visibility is calculated using Jin's law, as an integrated value of the extinction coefficient along the line of sight to the closest exit sign. The required visibility results from the distance between those points. Additional parameters like view angle or visual obstructions are considered. The presented method allows for temporal visibility assessment, e.g. in an ASET-RSET analysis.","sentences":["In performance-based fire safety design, ensuring safe egress, e.g. by visibility of safety signs, is a crucial safety goal.","Compliance with the building requirements is often demonstrated by simulations of smoke spread.","Numerical models like the Fire Dynamics Simulator generally compute visibility as a local quantity using the light extinction coefficient, without the consideration of the actual light path to a safety sign.","Here, visibility maps are introduced, providing an approach for post-processing fire simulation data.","They indicate safe areas along egress routes, with respect to visibility.","At each location, the available visibility is calculated using Jin's law, as an integrated value of the extinction coefficient along the line of sight to the closest exit sign.","The required visibility results from the distance between those points.","Additional parameters like view angle or visual obstructions are considered.","The presented method allows for temporal visibility assessment, e.g. in an ASET-RSET analysis."],"url":"http://arxiv.org/abs/2404.11439v1","category":"cs.CE"}
{"created":"2024-04-17 14:47:34","title":"$SO(4)$ Symmetry in Hydrogen Atom with Spin","abstract":"As the simplest atom in nature, the hydrogen atom has been explored thoroughly from the perspective of non-relativistic quantum mechanics to relativistic quantum mechanics. Among the research on hydrogen atom, its energy level is the most basic, which can be obtained more conveniently predicated on the $SO(4)$ symmetry than the wave-equation resolution. Moreover, ``spin'' is another indispensable topic in quantum mechanics, appearing as an intrinsic degree of freedom. In this work, we generalize the quantum Runge-Lenz vector to a spin-dependent one, and then extract a novel Hamiltonian of hydrogen atom with spin based on the requirement of $SO(4)$ symmetry. Furthermore, the energy spectrum of hydrogen atom with spin potentials is also determined by the remarkable approach of $SO(4)$ symmetry. Our findings extend the ground of hydrogen atom, and may contribute to other complicated models based on hydrogen atom.","sentences":["As the simplest atom in nature, the hydrogen atom has been explored thoroughly from the perspective of non-relativistic quantum mechanics to relativistic quantum mechanics.","Among the research on hydrogen atom, its energy level is the most basic, which can be obtained more conveniently predicated on the $SO(4)$ symmetry than the wave-equation resolution.","Moreover, ``spin'' is another indispensable topic in quantum mechanics, appearing as an intrinsic degree of freedom.","In this work, we generalize the quantum Runge-Lenz vector to a spin-dependent one, and then extract a novel Hamiltonian of hydrogen atom with spin based on the requirement of $SO(4)$ symmetry.","Furthermore, the energy spectrum of hydrogen atom with spin potentials is also determined by the remarkable approach of $SO(4)$ symmetry.","Our findings extend the ground of hydrogen atom, and may contribute to other complicated models based on hydrogen atom."],"url":"http://arxiv.org/abs/2404.11437v1","category":"quant-ph"}
{"created":"2024-04-17 14:36:47","title":"Instantiations and Computational Aspects of Non-Flat Assumption-based Argumentation","abstract":"Most existing computational tools for assumption-based argumentation (ABA) focus on so-called flat frameworks, disregarding the more general case. In this paper, we study an instantiation-based approach for reasoning in possibly non-flat ABA. We make use of a semantics-preserving translation between ABA and bipolar argumentation frameworks (BAFs). By utilizing compilability theory, we establish that the constructed BAFs will in general be of exponential size. In order to keep the number of arguments and computational cost low, we present three ways of identifying redundant arguments. Moreover, we identify fragments of ABA which admit a poly-sized instantiation. We propose two algorithmic approaches for reasoning in possibly non-flat ABA. The first approach utilizes the BAF instantiation while the second works directly without constructing arguments. An empirical evaluation shows that the former outperforms the latter on many instances, reflecting the lower complexity of BAF reasoning. This result is in contrast to flat ABA, where direct approaches dominate instantiation-based approaches.","sentences":["Most existing computational tools for assumption-based argumentation (ABA) focus on so-called flat frameworks, disregarding the more general case.","In this paper, we study an instantiation-based approach for reasoning in possibly non-flat ABA.","We make use of a semantics-preserving translation between ABA and bipolar argumentation frameworks (BAFs).","By utilizing compilability theory, we establish that the constructed BAFs will in general be of exponential size.","In order to keep the number of arguments and computational cost low, we present three ways of identifying redundant arguments.","Moreover, we identify fragments of ABA which admit a poly-sized instantiation.","We propose two algorithmic approaches for reasoning in possibly non-flat ABA.","The first approach utilizes the BAF instantiation while the second works directly without constructing arguments.","An empirical evaluation shows that the former outperforms the latter on many instances, reflecting the lower complexity of BAF reasoning.","This result is in contrast to flat ABA, where direct approaches dominate instantiation-based approaches."],"url":"http://arxiv.org/abs/2404.11431v1","category":"cs.AI"}
{"created":"2024-04-17 14:34:35","title":"Explainable Lung Disease Classification from Chest X-Ray Images Utilizing Deep Learning and XAI","abstract":"Lung diseases remain a critical global health concern, and it's crucial to have accurate and quick ways to diagnose them. This work focuses on classifying different lung diseases into five groups: viral pneumonia, bacterial pneumonia, COVID, tuberculosis, and normal lungs. Employing advanced deep learning techniques, we explore a diverse range of models including CNN, hybrid models, ensembles, transformers, and Big Transfer. The research encompasses comprehensive methodologies such as hyperparameter tuning, stratified k-fold cross-validation, and transfer learning with fine-tuning.Remarkably, our findings reveal that the Xception model, fine-tuned through 5-fold cross-validation, achieves the highest accuracy of 96.21\\%. This success shows that our methods work well in accurately identifying different lung diseases. The exploration of explainable artificial intelligence (XAI) methodologies further enhances our understanding of the decision-making processes employed by these models, contributing to increased trust in their clinical applications.","sentences":["Lung diseases remain a critical global health concern, and it's crucial to have accurate and quick ways to diagnose them.","This work focuses on classifying different lung diseases into five groups: viral pneumonia, bacterial pneumonia, COVID, tuberculosis, and normal lungs.","Employing advanced deep learning techniques, we explore a diverse range of models including CNN, hybrid models, ensembles, transformers, and Big Transfer.","The research encompasses comprehensive methodologies such as hyperparameter tuning, stratified k-fold cross-validation, and transfer learning with fine-tuning.","Remarkably, our findings reveal that the Xception model, fine-tuned through 5-fold cross-validation, achieves the highest accuracy of 96.21\\%.","This success shows that our methods work well in accurately identifying different lung diseases.","The exploration of explainable artificial intelligence (XAI) methodologies further enhances our understanding of the decision-making processes employed by these models, contributing to increased trust in their clinical applications."],"url":"http://arxiv.org/abs/2404.11428v1","category":"eess.IV"}
{"created":"2024-04-17 14:33:41","title":"SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow","abstract":"Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets. Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively. In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention. SPAM is built around two key insights: i) most tracking scenarios can be easily resolved. To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs. Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time. Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost. We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort. Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets. Our code and models will be available upon acceptance.","sentences":["Increasing the annotation efficiency of trajectory annotations from videos has the potential to enable the next generation of data-hungry tracking algorithms to thrive on large-scale datasets.","Despite the importance of this task, there are currently very few works exploring how to efficiently label tracking datasets comprehensively.","In this work, we introduce SPAM, a tracking data engine that provides high-quality labels with minimal human intervention.","SPAM is built around two key insights: i) most tracking scenarios can be easily resolved.","To take advantage of this, we utilize a pre-trained model to generate high-quality pseudo-labels, reserving human involvement for a smaller subset of more difficult instances; ii) handling the spatiotemporal dependencies of track annotations across time can be elegantly and efficiently formulated through graphs.","Therefore, we use a unified graph formulation to address the annotation of both detections and identity association for tracks across time.","Based on these insights, SPAM produces high-quality annotations with a fraction of ground truth labeling cost.","We demonstrate that trackers trained on SPAM labels achieve comparable performance to those trained on human annotations while requiring only 3-20% of the human labeling effort.","Hence, SPAM paves the way towards highly efficient labeling of large-scale tracking datasets.","Our code and models will be available upon acceptance."],"url":"http://arxiv.org/abs/2404.11426v1","category":"cs.CV"}
{"created":"2024-04-17 14:32:36","title":"Similarities between projective quantum fields and the Standard Model","abstract":"Many homogeneous, four-dimensional space-time geometries can be considered within real projective geometry, which yields a mathematically well-defined framework for their deformations and limits without the appearance of singularities. Focussing on generalized unitary transformation behavior, projective quantum fields can be axiomatically introduced, which transform smoothly under geometry deformations and limits. Connections on the related projective frame bundles provide gauge fields with gauge group $\\mathrm{PGL}_5\\mathbb{R}$. For Poincar\\'e geometry, on operator level only $\\mathrm{P}(\\mathrm{GL}_2\\mathbb{R}\\times \\mathrm{GL}_3\\mathbb{R})\\cong \\mathbb{R}_{\\neq 0}\\times \\mathrm{PGL}_2\\mathbb{R}\\times \\mathrm{PGL}_3\\mathbb{R}$ gauge bosons can interact non-trivially with other projective quantum fields from the non- to ultra-relativistic limits. The corresponding propagating, complexified gauge bosons come with the Standard Model gauge group $G_{\\mathrm{SM}}=(\\mathrm{U}(1)\\times \\mathrm{SU}(2)\\times \\mathrm{SU}(3))/\\mathbb{Z}_6$. Physical scale transformations can act as global gauge transformations and their spontaneous breaking can lead to masses for the projective quantum fields including the $\\mathrm{SU}(2)$ gauge bosons. Projective quantum fields, which are irreducible both with respect to the Lie algebra $\\mathfrak{pgl}_5\\mathbb{R}$ and the Poincar\\'e group, form Dirac fermions and $G_{\\mathrm{SM}}$ gauge bosons interact with them similar to the Standard Model. For homogeneous, curved Lorentz geometries a gauge group similar to the gauge group of metric-affine gravity appears.","sentences":["Many homogeneous, four-dimensional space-time geometries can be considered within real projective geometry, which yields a mathematically well-defined framework for their deformations and limits without the appearance of singularities.","Focussing on generalized unitary transformation behavior, projective quantum fields can be axiomatically introduced, which transform smoothly under geometry deformations and limits.","Connections on the related projective frame bundles provide gauge fields with gauge group $\\mathrm{PGL}_5\\mathbb{R}$. For Poincar\\'e geometry, on operator level only $\\mathrm{P}(\\mathrm{GL}_2\\mathbb{R}\\times \\mathrm{GL}_3\\mathbb{R})\\cong \\mathbb{R}_{\\neq 0}\\times \\mathrm{PGL}_2\\mathbb{R}\\times \\mathrm{PGL}_3\\mathbb{R}$ gauge bosons can interact non-trivially with other projective quantum fields from the non- to ultra-relativistic limits.","The corresponding propagating, complexified gauge bosons come with the Standard Model gauge group $G_{\\mathrm{SM}}=(\\mathrm{U}(1)\\times \\mathrm{SU}(2)\\times \\mathrm{SU}(3))/\\mathbb{Z}_6$. Physical scale transformations can act as global gauge transformations and their spontaneous breaking can lead to masses for the projective quantum fields including the $\\mathrm{SU}(2)$ gauge bosons.","Projective quantum fields, which are irreducible both with respect to the Lie algebra $\\mathfrak{pgl}_5\\mathbb{R}$ and the Poincar\\'e group, form Dirac fermions and $G_{\\mathrm{SM}}$ gauge bosons interact with them similar to the Standard Model.","For homogeneous, curved Lorentz geometries a gauge group similar to the gauge group of metric-affine gravity appears."],"url":"http://arxiv.org/abs/2404.11425v1","category":"hep-th"}
{"created":"2024-04-17 14:27:45","title":"Short-term wind speed forecasting model based on an attention-gated recurrent neural network and error correction strategy","abstract":"The accurate wind speed series forecast is very pivotal to security of grid dispatching and the application of wind power. Nevertheless, on account of their nonlinear and non-stationary nature, their short-term forecast is extremely challenging. Therefore, this dissertation raises one short-term wind speed forecast pattern on the foundation of attention with an improved gated recurrent neural network (AtGRU) and a tactic of error correction. That model uses the AtGRU model as the preliminary predictor and the GRU model as the error corrector. At the beginning, SSA (singular spectrum analysis) is employed in previous wind speed series for lessening the noise. Subsequently, historical wind speed series is going to be used for the predictor training. During this process, the prediction can have certain errors. The sequence of these errors processed by variational modal decomposition (VMD) is used to train the corrector of error. The eventual forecast consequence is just the sum of predictor forecast and error corrector. The proposed SSA-AtGRU-VMD-GRU model outperforms the compared models in three case studies on Woodburn, St. Thomas, and Santa Cruz. It is indicated that the model evidently enhances the correction of the wind speed forecast.","sentences":["The accurate wind speed series forecast is very pivotal to security of grid dispatching and the application of wind power.","Nevertheless, on account of their nonlinear and non-stationary nature, their short-term forecast is extremely challenging.","Therefore, this dissertation raises one short-term wind speed forecast pattern on the foundation of attention with an improved gated recurrent neural network (AtGRU) and a tactic of error correction.","That model uses the AtGRU model as the preliminary predictor and the GRU model as the error corrector.","At the beginning, SSA (singular spectrum analysis) is employed in previous wind speed series for lessening the noise.","Subsequently, historical wind speed series is going to be used for the predictor training.","During this process, the prediction can have certain errors.","The sequence of these errors processed by variational modal decomposition (VMD) is used to train the corrector of error.","The eventual forecast consequence is just the sum of predictor forecast and error corrector.","The proposed SSA-AtGRU-VMD-GRU model outperforms the compared models in three case studies on Woodburn, St. Thomas, and Santa Cruz.","It is indicated that the model evidently enhances the correction of the wind speed forecast."],"url":"http://arxiv.org/abs/2404.11422v1","category":"cs.LG"}
{"created":"2024-04-17 14:16:43","title":"Achromatic Full Stokes Polarimetry Metasurface for Full-color Polarization Imaging in the Visible","abstract":"Metasurfaces composed of anisotropic subwavelength structures provide an ultrathin platform for a compact, real-time polarimeter. However, applications in polychromatic scenes are restricted by the limited operating bandwidths and degraded imaging quality due to the loss of spectral information. Here, we demonstrated full-color polarization imaging based on an achromatic polarimeter consisting of four polarization-dependent metalenses. Boosted by an intelligent design scheme, arbitrary phase compensation and multi-objective matching are effectively compatible with a limited database. Broadband achromaticity for wavelengths ranging from 450 nm to 650 nm, with a relative bandwidth of nearly 0.435, is achieved for the full Stokes imaging. The experimental polarization reconstructed errors for operating wavelengths of 450 nm, 550 nm, and 650 nm are 7.5%, 5.9%, and 3.8%, respectively. The full-color and full-polarization imaging capability of the device is also verified with a customized object. The proposed scheme paves the way for further developing polarization imaging toward practical applications.","sentences":["Metasurfaces composed of anisotropic subwavelength structures provide an ultrathin platform for a compact, real-time polarimeter.","However, applications in polychromatic scenes are restricted by the limited operating bandwidths and degraded imaging quality due to the loss of spectral information.","Here, we demonstrated full-color polarization imaging based on an achromatic polarimeter consisting of four polarization-dependent metalenses.","Boosted by an intelligent design scheme, arbitrary phase compensation and multi-objective matching are effectively compatible with a limited database.","Broadband achromaticity for wavelengths ranging from 450 nm to 650 nm, with a relative bandwidth of nearly 0.435, is achieved for the full Stokes imaging.","The experimental polarization reconstructed errors for operating wavelengths of 450 nm, 550 nm, and 650 nm are 7.5%, 5.9%, and 3.8%, respectively.","The full-color and full-polarization imaging capability of the device is also verified with a customized object.","The proposed scheme paves the way for further developing polarization imaging toward practical applications."],"url":"http://arxiv.org/abs/2404.11415v1","category":"physics.optics"}
{"created":"2024-04-17 14:13:58","title":"Classifying Sums of Exponentially Damped Sinusoids Using an Associated Numerical Range","abstract":"The matrix pencil method (MPM) is a well-known technique for estimating the parameters of exponentially damped sinusoids in noise by solving a generalized eigenvalue problem. However, in several cases, this is an ill-conditioned problem whose solution is highly biased under small perturbations. When the estimation is performed to classify the observed signal into two categories, the estimation errors induce several misclassifications. In this work we propose a novel signal classification criteria by exploiting the relationship between the generalized eigenvalue problem posed in the MPM and the numerical range of a pair of rectangular matrices. In particular, the classification test is formulated as a set inclusion problem, and no spectrum estimation is required. The technique is applied to a problem of electromagnetic scattering to classify dielectric materials using the scattering signal observed when a target is illuminated by an ultra-wideband signal. The performance of the classification scheme is assessed in terms of error rate and it is compared to another classification technique, the generalized likelihood rate test (GLRT).","sentences":["The matrix pencil method (MPM) is a well-known technique for estimating the parameters of exponentially damped sinusoids in noise by solving a generalized eigenvalue problem.","However, in several cases, this is an ill-conditioned problem whose solution is highly biased under small perturbations.","When the estimation is performed to classify the observed signal into two categories, the estimation errors induce several misclassifications.","In this work we propose a novel signal classification criteria by exploiting the relationship between the generalized eigenvalue problem posed in the MPM and the numerical range of a pair of rectangular matrices.","In particular, the classification test is formulated as a set inclusion problem, and no spectrum estimation is required.","The technique is applied to a problem of electromagnetic scattering to classify dielectric materials using the scattering signal observed when a target is illuminated by an ultra-wideband signal.","The performance of the classification scheme is assessed in terms of error rate and it is compared to another classification technique, the generalized likelihood rate test (GLRT)."],"url":"http://arxiv.org/abs/2404.11413v1","category":"eess.SP"}
{"created":"2024-04-17 14:12:47","title":"EcoMLS: A Self-Adaptation Approach for Architecting Green ML-Enabled Systems","abstract":"The sustainability of Machine Learning-Enabled Systems (MLS), particularly with regard to energy efficiency, is an important challenge in their development and deployment. Self-adaptation techniques, recognized for their potential in energy savings within software systems, have yet to be extensively explored in Machine Learning-Enabled Systems (MLS), where runtime uncertainties can significantly impact model performance and energy consumption. This variability, alongside the fluctuating energy demands of ML models during operation, necessitates a dynamic approach. Addressing these challenges, we introduce EcoMLS approach, which leverages the Machine Learning Model Balancer concept to enhance the sustainability of MLS through runtime ML model switching. By adapting to monitored runtime conditions, EcoMLS optimally balances energy consumption with model confidence, demonstrating a significant advancement towards sustainable, energy-efficient machine learning solutions. Through an object detection exemplar, we illustrate the application of EcoMLS, showcasing its ability to reduce energy consumption while maintaining high model accuracy throughout its use. This research underscores the feasibility of enhancing MLS sustainability through intelligent runtime adaptations, contributing a valuable perspective to the ongoing discourse on energy-efficient machine learning.","sentences":["The sustainability of Machine Learning-Enabled Systems (MLS), particularly with regard to energy efficiency, is an important challenge in their development and deployment.","Self-adaptation techniques, recognized for their potential in energy savings within software systems, have yet to be extensively explored in Machine Learning-Enabled Systems (MLS), where runtime uncertainties can significantly impact model performance and energy consumption.","This variability, alongside the fluctuating energy demands of ML models during operation, necessitates a dynamic approach.","Addressing these challenges, we introduce EcoMLS approach, which leverages the Machine Learning Model Balancer concept to enhance the sustainability of MLS through runtime ML model switching.","By adapting to monitored runtime conditions, EcoMLS optimally balances energy consumption with model confidence, demonstrating a significant advancement towards sustainable, energy-efficient machine learning solutions.","Through an object detection exemplar, we illustrate the application of EcoMLS, showcasing its ability to reduce energy consumption while maintaining high model accuracy throughout its use.","This research underscores the feasibility of enhancing MLS sustainability through intelligent runtime adaptations, contributing a valuable perspective to the ongoing discourse on energy-efficient machine learning."],"url":"http://arxiv.org/abs/2404.11411v1","category":"cs.SE"}
{"created":"2024-04-17 14:10:27","title":"DUPE: Detection Undermining via Prompt Engineering for Deepfake Text","abstract":"As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well. The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments. Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors. Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty. In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays. We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates. Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors.","sentences":["As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well.","The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments.","Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors.","Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty.","In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays.","We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates.","Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors."],"url":"http://arxiv.org/abs/2404.11408v1","category":"cs.AI"}
{"created":"2024-04-17 14:09:49","title":"Generalizing Instant Runoff Voting to Allow Indifferences","abstract":"Instant Runoff Voting (IRV) is used in elections for many political offices around the world. It allows voters to specify their preferences among candidates as a ranking. We identify a generalization of the rule, called Approval-IRV, that allows voters more freedom by allowing them to give equal preference to several candidates. Such weak orders are a more expressive input format than linear orders, and they help reduce the cognitive effort of voting.   Just like standard IRV, Approval-IRV proceeds in rounds by successively eliminating candidates. It interprets each vote as an approval vote for its most-preferred candidates among those that have not been eliminated. At each step, it eliminates the candidate who is approved by the fewest voters. Among the large class of scoring elimination rules, we prove that Approval-IRV is the unique way of extending IRV to weak orders that preserves its characteristic axiomatic properties, in particular independence of clones and respecting a majority's top choices. We also show that Approval-IRV is the unique extension of IRV among rules in this class that satisfies a natural monotonicity property defined for weak orders.   Prior work has proposed a different generalization of IRV, which we call Split-IRV, where instead of approving, each vote is interpreted as splitting 1 point equally among its top choices (for example, 0.25 points each if a vote has 4 top choices), and then eliminating the candidate with the lowest score. Split-IRV fails independence of clones, may not respect majority wishes, and fails our monotonicity condition.   The multi-winner version of IRV is known as Single Transferable Vote (STV). We prove that Approval-STV continues to satisfy the strong proportional representation properties of STV, underlining that the approval way is the right way of extending the IRV/STV idea to weak orders.","sentences":["Instant Runoff Voting (IRV) is used in elections for many political offices around the world.","It allows voters to specify their preferences among candidates as a ranking.","We identify a generalization of the rule, called Approval-IRV, that allows voters more freedom by allowing them to give equal preference to several candidates.","Such weak orders are a more expressive input format than linear orders, and they help reduce the cognitive effort of voting.   ","Just like standard IRV, Approval-IRV proceeds in rounds by successively eliminating candidates.","It interprets each vote as an approval vote for its most-preferred candidates among those that have not been eliminated.","At each step, it eliminates the candidate who is approved by the fewest voters.","Among the large class of scoring elimination rules, we prove that Approval-IRV is the unique way of extending IRV to weak orders that preserves its characteristic axiomatic properties, in particular independence of clones and respecting a majority's top choices.","We also show that Approval-IRV is the unique extension of IRV among rules in this class that satisfies a natural monotonicity property defined for weak orders.   ","Prior work has proposed a different generalization of IRV, which we call Split-IRV, where instead of approving, each vote is interpreted as splitting 1 point equally among its top choices (for example, 0.25 points each if a vote has 4 top choices), and then eliminating the candidate with the lowest score.","Split-IRV fails independence of clones, may not respect majority wishes, and fails our monotonicity condition.   ","The multi-winner version of IRV is known as Single Transferable Vote (STV).","We prove that Approval-STV continues to satisfy the strong proportional representation properties of STV, underlining that the approval way is the right way of extending the IRV/STV idea to weak orders."],"url":"http://arxiv.org/abs/2404.11407v1","category":"cs.GT"}
{"created":"2024-04-17 14:08:22","title":"Multi-layer continuous carbon fiber pattern optimization and a spline based path planning interpretation","abstract":"A novel approach for creating tool paths for continuous carbon fiber-reinforced thermoplastic 3D printing is introduced. The aim is to enable load-bearing connections while avoiding non-manufacturable crossings of paths by generating layer specific patterns. We require a graph representation of the structural design with given desired continuous fiber connections. From this, optimal fiber patterns are obtained for each printing layer by solving linear integer optimization problems. Each layer may have a unique solution based on the history of the previous layers. Additionally, a path planning approach is presented which interprets the obtained layers via curves based on quadratic and cubic B\\'ezier splines and their offset curves in constant distance. The single parameter for the construction of the paths is the minimal turning radius of the fibers. The path planning provides a new interpretation for the final geometry of the design to be printed.","sentences":["A novel approach for creating tool paths for continuous carbon fiber-reinforced thermoplastic 3D printing is introduced.","The aim is to enable load-bearing connections while avoiding non-manufacturable crossings of paths by generating layer specific patterns.","We require a graph representation of the structural design with given desired continuous fiber connections.","From this, optimal fiber patterns are obtained for each printing layer by solving linear integer optimization problems.","Each layer may have a unique solution based on the history of the previous layers.","Additionally, a path planning approach is presented which interprets the obtained layers via curves based on quadratic and cubic B\\'ezier splines and their offset curves in constant distance.","The single parameter for the construction of the paths is the minimal turning radius of the fibers.","The path planning provides a new interpretation for the final geometry of the design to be printed."],"url":"http://arxiv.org/abs/2404.11404v1","category":"math.OC"}
{"created":"2024-04-17 14:08:17","title":"Six decades of the FitzHugh-Nagumo model: A guide through its spatio-temporal dynamics and influence across disciplines","abstract":"The FitzHugh-Nagumo equation, originally conceived in neuroscience during the 1960s, became a key model providing a simplified view of excitable neuron cell behavior. Its applicability, however, extends beyond neuroscience into fields like cardiac physiology, cell division, population dynamics, electronics, and other natural phenomena. In this review spanning six decades of research, we discuss the diverse spatio-temporal dynamical behaviors described by the FitzHugh-Nagumo equation. These include dynamics like bistability, oscillations, and excitability, but it also addresses more complex phenomena such as traveling waves and extended patterns in coupled systems. The review serves as a guide for modelers aiming to utilize the strengths of the FitzHugh-Nagumo model to capture generic dynamical behavior. It not only catalogs known dynamical states and bifurcations, but also extends previous studies by providing stability and bifurcation analyses for coupled spatial systems.","sentences":["The FitzHugh-Nagumo equation, originally conceived in neuroscience during the 1960s, became a key model providing a simplified view of excitable neuron cell behavior.","Its applicability, however, extends beyond neuroscience into fields like cardiac physiology, cell division, population dynamics, electronics, and other natural phenomena.","In this review spanning six decades of research, we discuss the diverse spatio-temporal dynamical behaviors described by the FitzHugh-Nagumo equation.","These include dynamics like bistability, oscillations, and excitability, but it also addresses more complex phenomena such as traveling waves and extended patterns in coupled systems.","The review serves as a guide for modelers aiming to utilize the strengths of the FitzHugh-Nagumo model to capture generic dynamical behavior.","It not only catalogs known dynamical states and bifurcations, but also extends previous studies by providing stability and bifurcation analyses for coupled spatial systems."],"url":"http://arxiv.org/abs/2404.11403v1","category":"nlin.PS"}
{"created":"2024-04-17 14:07:58","title":"Dynamical Detection of Topological Spectral Density","abstract":"Local density of states (LDOS) is emerging as powerful means of exploring synthetic topological phases. However, the current LDOS detection method remains rare and merely works for static situations. Here, we introduce a generic dynamical method to detect both the static and Floquet LDOS, based on an elegant connection between dynamics of chiral density and local spectral densities. Moreover, we find that the Floquet LDOS allows to measure out Floquet quasienergy spectra and identify topological $\\pi$ modes. As an example, we demonstrate that both the static and Floquet higher-order topological phase can be universally identified via LDOS detection, regardless of whether the topological corner modes are in energy gaps, bands or continue energy spectra without bandgaps. Our study opens a new avenue utilizing dynamics to detect topological spectral densities and provides a universal approach of identifying static and Floquet topological phases.","sentences":["Local density of states (LDOS) is emerging as powerful means of exploring synthetic topological phases.","However, the current LDOS detection method remains rare and merely works for static situations.","Here, we introduce a generic dynamical method to detect both the static and Floquet LDOS, based on an elegant connection between dynamics of chiral density and local spectral densities.","Moreover, we find that the Floquet LDOS allows to measure out Floquet quasienergy spectra and identify topological $\\pi$ modes.","As an example, we demonstrate that both the static and Floquet higher-order topological phase can be universally identified via LDOS detection, regardless of whether the topological corner modes are in energy gaps, bands or continue energy spectra without bandgaps.","Our study opens a new avenue utilizing dynamics to detect topological spectral densities and provides a universal approach of identifying static and Floquet topological phases."],"url":"http://arxiv.org/abs/2404.11402v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-17 14:04:49","title":"Understanding the instability-wave selectivity of hypersonic compression ramp laminar flow","abstract":"The hypersonic flow stability over a two-dimensional compression corner is studied using resolvent analysis, linear stability theory (LST) and parabolised stability equation (PSE) analysis. We find that the interaction between upstream convective-type disturbances and the laminar separation bubble can be divided into two regimes, whose behaviour can be well explained by comparative research. First, two-dimensional (2-D) high-frequency Mack modes neutrally oscillate with the presence of alternating stable and unstable regions inside the separation bubble. These discontinuous unstable regions are generated by repeated synchronisations between discrete modes with evolving branches. Through a modal sychronisation analysis, we report that the second modes upstream and downstream of the separation bubble can be essentially different from each other, since they originate from different branches of discrete modes due to flow separation. Second, the 2-D low-frequency `shear-layer mode' is found to be stable in the separation bubble by LST, whereas multiple unstable three-dimensional (3-D) eigenmodes are identified by LST. In general, three significant modes are dominant successively near the separation point, in the separation bubble and near the reattachment point. These modes are found to be sensitive to the streamline curvature effect. The locally dominant modes agree with the resolvent response in terms of the disturbance shape and the growth rate of energy. Thus, a combination of global and local analyses demonstrates that the separation bubble tends to selectively amplify low-frequency 3-D disturbances and `freeze' high-frequency Mack-mode disturbances in an explainable manner. These findings facilitate the understanding of the early evolution of low- and high-frequency instabilities in hypersonic separated flows.","sentences":["The hypersonic flow stability over a two-dimensional compression corner is studied using resolvent analysis, linear stability theory (LST) and parabolised stability equation (PSE) analysis.","We find that the interaction between upstream convective-type disturbances and the laminar separation bubble can be divided into two regimes, whose behaviour can be well explained by comparative research.","First, two-dimensional (2-D) high-frequency Mack modes neutrally oscillate with the presence of alternating stable and unstable regions inside the separation bubble.","These discontinuous unstable regions are generated by repeated synchronisations between discrete modes with evolving branches.","Through a modal sychronisation analysis, we report that the second modes upstream and downstream of the separation bubble can be essentially different from each other, since they originate from different branches of discrete modes due to flow separation.","Second, the 2-D low-frequency `shear-layer mode' is found to be stable in the separation bubble by LST, whereas multiple unstable three-dimensional (3-D) eigenmodes are identified by LST.","In general, three significant modes are dominant successively near the separation point, in the separation bubble and near the reattachment point.","These modes are found to be sensitive to the streamline curvature effect.","The locally dominant modes agree with the resolvent response in terms of the disturbance shape and the growth rate of energy.","Thus, a combination of global and local analyses demonstrates that the separation bubble tends to selectively amplify low-frequency 3-D disturbances and `freeze' high-frequency Mack-mode disturbances in an explainable manner.","These findings facilitate the understanding of the early evolution of low- and high-frequency instabilities in hypersonic separated flows."],"url":"http://arxiv.org/abs/2404.11400v1","category":"physics.flu-dyn"}
{"created":"2024-04-17 14:01:25","title":"Robust parameter estimation within minutes on gravitational wave signals from binary neutron star inspirals","abstract":"The gravitational waves emitted by binary neutron star inspirals contain information on nuclear matter above saturation density. However, extracting this information and conducting parameter estimation remains a computationally challenging and expensive task. Wong et al. introduced Jim arXiv:2302.05333, a parameter estimation pipeline that combines relative binning and jax features such as hardware acceleration and automatic differentiation into a normalizing flow-enhanced sampler for gravitational waves from binary black hole (BBH) mergers. In this work, we extend the Jim framework to analyze gravitational wave signals from binary neutron stars (BNS) mergers with tidal effects included. We demonstrate that Jim can be used for full Bayesian parameter estimation of gravitational waves from BNS mergers within a few tens of minutes, which includes the training of the normalizing flow and computing the reference parameters for relative binning. For instance, Jim can analyze GW170817 in 26 minutes (33 minutes) of total wall time using the TaylorF2 (IMRPhenomD_NRTidalv2) waveform, and GW190425 in around 21 minutes for both waveforms. We highlight the importance of such an efficient parameter estimation pipeline for several science cases as well as its ecologically friendly implementation of gravitational wave parameter estimation.","sentences":["The gravitational waves emitted by binary neutron star inspirals contain information on nuclear matter above saturation density.","However, extracting this information and conducting parameter estimation remains a computationally challenging and expensive task.","Wong et al. introduced Jim arXiv:2302.05333, a parameter estimation pipeline that combines relative binning and jax features such as hardware acceleration and automatic differentiation into a normalizing flow-enhanced sampler for gravitational waves from binary black hole (BBH) mergers.","In this work, we extend the Jim framework to analyze gravitational wave signals from binary neutron stars (BNS) mergers with tidal effects included.","We demonstrate that Jim can be used for full Bayesian parameter estimation of gravitational waves from BNS mergers within a few tens of minutes, which includes the training of the normalizing flow and computing the reference parameters for relative binning.","For instance, Jim can analyze GW170817 in 26 minutes (33 minutes) of total wall time using the TaylorF2 (IMRPhenomD_NRTidalv2) waveform, and GW190425 in around 21 minutes for both waveforms.","We highlight the importance of such an efficient parameter estimation pipeline for several science cases as well as its ecologically friendly implementation of gravitational wave parameter estimation."],"url":"http://arxiv.org/abs/2404.11397v1","category":"astro-ph.IM"}
{"created":"2024-04-17 13:58:44","title":"An Intrinsic Coordinate Reference Frame Procedure I: Tensorial Canonical Weyl Scalars","abstract":"Canonical quantization of gravity in general relativity is greatly simplified by the artificial decomposition of space and time into a 3+1 formalism. Such a simplification may appear to come at the cost of general covariance. This requires tangential and perpendicular infinitesimal diffeomorphisms generated by the symmetry group under the Legendre transformation of the given action. This gauge generator, along with the fact that Weyl curvature scalars may act as ``intrinsic coordinates\" (or a dynamical reference frame) which depend only on the spatial metric $g_{ab}$ and the conjugate momenta $p^{cd}$, allow for an alternative approach to canonical quantization of gravity. In this paper we present the tensorial solution of the set of Weyl scalars in terms of canonical phase-space variables.","sentences":["Canonical quantization of gravity in general relativity is greatly simplified by the artificial decomposition of space and time into a 3+1 formalism.","Such a simplification may appear to come at the cost of general covariance.","This requires tangential and perpendicular infinitesimal diffeomorphisms generated by the symmetry group under the Legendre transformation of the given action.","This gauge generator, along with the fact that Weyl curvature scalars may act as ``intrinsic coordinates\" (or a dynamical reference frame) which depend only on the spatial metric $g_{ab}$ and the conjugate momenta $p^{cd}$, allow for an alternative approach to canonical quantization of gravity.","In this paper we present the tensorial solution of the set of Weyl scalars in terms of canonical phase-space variables."],"url":"http://arxiv.org/abs/2404.11395v1","category":"gr-qc"}
{"created":"2024-04-17 13:55:05","title":"What-if Analysis Framework for Digital Twins in 6G Wireless Network Management","abstract":"This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model. The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks. We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios. These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage. Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance.","sentences":["This study explores implementing a digital twin network (DTN) for efficient 6G wireless network management, aligning with the fault, configuration, accounting, performance, and security (FCAPS) model.","The DTN architecture comprises the Physical Twin Layer, implemented using NS-3, and the Service Layer, featuring machine learning and reinforcement learning for optimizing carrier sensitivity threshold and transmit power control in wireless networks.","We introduce a robust \"What-if Analysis\" module, utilizing conditional tabular generative adversarial network (CTGAN) for synthetic data generation to mimic various network scenarios.","These scenarios assess four network performance metrics: throughput, latency, packet loss, and coverage.","Our findings demonstrate the efficiency of the proposed what-if analysis framework in managing complex network conditions, highlighting the importance of the scenario-maker step and the impact of twinning intervals on network performance."],"url":"http://arxiv.org/abs/2404.11394v1","category":"cs.NI"}
{"created":"2024-04-17 13:46:29","title":"The emergence of spacetime: what role for functionalism?","abstract":"Among the various attempts to formulate a theory of quantum gravity, a class of approaches suggests that spacetime, as modeled by general relativity, is destined to fade away. A major issue becomes then to identify which structures may inhabit the more fundamental, non-spatiotemporal environment, as well as to explain the relationship with the higher-level spatiotemporal physics. Recently, it has been suggested that a certain understanding of functionalism is the proper tool to suitably account for the recovery of spacetime. Here the viability and usefulness of such a conceptual strategy is explored, by looking at the various levels of spacetime emergence a theory of quantum gravity is expected to deal with. Our conclusion will be that, while its viability is clear also in a quantum gravity context, the import of spacetime functionalism remains rather unsettled.","sentences":["Among the various attempts to formulate a theory of quantum gravity, a class of approaches suggests that spacetime, as modeled by general relativity, is destined to fade away.","A major issue becomes then to identify which structures may inhabit the more fundamental, non-spatiotemporal environment, as well as to explain the relationship with the higher-level spatiotemporal physics.","Recently, it has been suggested that a certain understanding of functionalism is the proper tool to suitably account for the recovery of spacetime.","Here the viability and usefulness of such a conceptual strategy is explored, by looking at the various levels of spacetime emergence a theory of quantum gravity is expected to deal with.","Our conclusion will be that, while its viability is clear also in a quantum gravity context, the import of spacetime functionalism remains rather unsettled."],"url":"http://arxiv.org/abs/2404.11386v1","category":"physics.hist-ph"}
{"created":"2024-04-17 13:45:07","title":"Heart Rate Variability Series is the Output of a non-Chaotic System driven by Dynamical Noise","abstract":"Heart rate variability (HRV) series reflects the dynamical variation of heartbeat-to-heartbeat intervals in time and is one of the outputs of the cardiovascular system. Over the years, this system has been recognized for generating nonlinear and complex heartbeat dynamics, with the latter referring to a high sensitivity to small -- theoretically infinitesimal -- input changes. While early research associated chaotic behavior with the cardiovascular system, evidence of stochastic inputs to the system, i.e., a physiological noise, invalidated those conclusions. To date, a comprehensive characterization of the cardiovascular system dynamics, accounting for dynamical noise input, has not been undertaken. In this study, we propose a novel methodological framework for evaluating the presence of regular or chaotic dynamics in noisy dynamical systems. The method relies on the estimation of asymptotic growth rate of noisy mean square displacement series in a two-dimensional phase space. We validated the proposed method using synthetic series comprising well-known regular and chaotic maps. We applied the method to real HRV series from healthy subjects, as well as patients with atrial fibrillation and congestive heart failure, during unstructured long-term activity. Results indicate that HRV series are consistently generated by a regular system driven by dynamical noise.","sentences":["Heart rate variability (HRV) series reflects the dynamical variation of heartbeat-to-heartbeat intervals in time and is one of the outputs of the cardiovascular system.","Over the years, this system has been recognized for generating nonlinear and complex heartbeat dynamics, with the latter referring to a high sensitivity to small -- theoretically infinitesimal -- input changes.","While early research associated chaotic behavior with the cardiovascular system, evidence of stochastic inputs to the system, i.e., a physiological noise, invalidated those conclusions.","To date, a comprehensive characterization of the cardiovascular system dynamics, accounting for dynamical noise input, has not been undertaken.","In this study, we propose a novel methodological framework for evaluating the presence of regular or chaotic dynamics in noisy dynamical systems.","The method relies on the estimation of asymptotic growth rate of noisy mean square displacement series in a two-dimensional phase space.","We validated the proposed method using synthetic series comprising well-known regular and chaotic maps.","We applied the method to real HRV series from healthy subjects, as well as patients with atrial fibrillation and congestive heart failure, during unstructured long-term activity.","Results indicate that HRV series are consistently generated by a regular system driven by dynamical noise."],"url":"http://arxiv.org/abs/2404.11385v1","category":"eess.SP"}
{"created":"2024-04-17 13:44:29","title":"Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning","abstract":"Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining. Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters. This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments. Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points. To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning. Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point. Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights. We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph. Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets.","sentences":["Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining.","Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters.","This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments.","Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points.","To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning.","Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point.","Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights.","We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph.","Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets."],"url":"http://arxiv.org/abs/2404.11384v1","category":"cs.CL"}
{"created":"2024-04-17 13:35:52","title":"Non-hermitian magnonic knobbing between electromagnetically induced reflection and transparancy","abstract":"Manipulation of wave propagation through open resonant systems has attracted tremendous interest. When accessible to the open system, the system under study is prone to tempering to out of equilibrium, and a lack of reciprocity is the rule rather than the exception. Open systems correspond to non-hermitian Hamiltonians with very unique properties such as resulting exceptional points and ideal isolation. Here, we have found a highly sensitive modulation for the intersection of resonant patch antennas with respect to cavity magnonic coupling by means of an open coupling system of three resonant modes. Two types of crossings are implemented in this study: the first type of crossing remotely controls the sharp switching of the transmission line 's transmittance, while regulating the repulsive behavior of its zero-reflection states. The second type of crossing corresponds to the modulation of non-reciprocal phase transitions, which enables a more desirable isolation effect. Three different coupling models are realized by a non-Hermitian scattering Hamiltonian, revealing distinct spatial overlaps between modes. This elucidates that dissipative coupling of at least two modes to the environment is crucial for non-reciprocal transport. Our work not only reveals the versatility of cavity magnonic systems but also provides a way to design functional devices for general wave optics using patch antenna crossings.","sentences":["Manipulation of wave propagation through open resonant systems has attracted tremendous interest.","When accessible to the open system, the system under study is prone to tempering to out of equilibrium, and a lack of reciprocity is the rule rather than the exception.","Open systems correspond to non-hermitian Hamiltonians with very unique properties such as resulting exceptional points and ideal isolation.","Here, we have found a highly sensitive modulation for the intersection of resonant patch antennas with respect to cavity magnonic coupling by means of an open coupling system of three resonant modes.","Two types of crossings are implemented in this study: the first type of crossing remotely controls the sharp switching of the transmission line 's transmittance, while regulating the repulsive behavior of its zero-reflection states.","The second type of crossing corresponds to the modulation of non-reciprocal phase transitions, which enables a more desirable isolation effect.","Three different coupling models are realized by a non-Hermitian scattering Hamiltonian, revealing distinct spatial overlaps between modes.","This elucidates that dissipative coupling of at least two modes to the environment is crucial for non-reciprocal transport.","Our work not only reveals the versatility of cavity magnonic systems but also provides a way to design functional devices for general wave optics using patch antenna crossings."],"url":"http://arxiv.org/abs/2404.11380v1","category":"physics.app-ph"}
{"created":"2024-04-17 13:33:11","title":"From Image to UML: First Results of Image Based UML Diagram Generation Using LLMs","abstract":"In software engineering processes, systems are first specified using a modeling language such as UML. These initial designs are often collaboratively created, many times in meetings where different domain experts use whiteboards, paper or other types of quick supports to create drawings and blueprints that then will need to be formalized. These proper, machine-readable, models are key to ensure models can be part of automated processes (e.g. input of a low-code generation pipeline, a model-based testing system, ...). But going form hand-drawn diagrams to actual models is a time-consuming process that sometimes ends up with such drawings just added as informal images to the software documentation, reducing their value a lot. To avoid this tedious task, we explore the usage of Large Language Models (LLM) to generate the formal representation of (UML) models from a given drawing. More specifically, we have evaluated the capabilities of different LLMs to convert images of UML class diagrams into the actual models represented in the images. While the results are good enough to use such an approach as part of a model-driven engineering pipeline we also highlight some of their current limitations and the need to keep the human in the loop to overcome those limitations.","sentences":["In software engineering processes, systems are first specified using a modeling language such as UML.","These initial designs are often collaboratively created, many times in meetings where different domain experts use whiteboards, paper or other types of quick supports to create drawings and blueprints that then will need to be formalized.","These proper, machine-readable, models are key to ensure models can be part of automated processes (e.g. input of a low-code generation pipeline, a model-based testing system, ...).","But going form hand-drawn diagrams to actual models is a time-consuming process that sometimes ends up with such drawings just added as informal images to the software documentation, reducing their value a lot.","To avoid this tedious task, we explore the usage of Large Language Models (LLM) to generate the formal representation of (UML) models from a given drawing.","More specifically, we have evaluated the capabilities of different LLMs to convert images of UML class diagrams into the actual models represented in the images.","While the results are good enough to use such an approach as part of a model-driven engineering pipeline we also highlight some of their current limitations and the need to keep the human in the loop to overcome those limitations."],"url":"http://arxiv.org/abs/2404.11376v1","category":"cs.SE"}
{"created":"2024-04-17 13:33:09","title":"Text-controlled Motion Mamba: Text-Instructed Temporal Grounding of Human Motion","abstract":"Human motion understanding is a fundamental task with diverse practical applications, facilitated by the availability of large-scale motion capture datasets. Recent studies focus on text-motion tasks, such as text-based motion generation, editing and question answering. In this study, we introduce the novel task of text-based human motion grounding (THMG), aimed at precisely localizing temporal segments corresponding to given textual descriptions within untrimmed motion sequences. Capturing global temporal information is crucial for the THMG task. However, transformer-based models that rely on global temporal self-attention face challenges when handling long untrimmed sequences due to the quadratic computational cost. We address these challenges by proposing Text-controlled Motion Mamba (TM-Mamba), a unified model that integrates temporal global context, language query control, and spatial graph topology with only linear memory cost. The core of the model is a text-controlled selection mechanism which dynamically incorporates global temporal information based on text query. The model is further enhanced to be topology-aware through the integration of relational embeddings. For evaluation, we introduce BABEL-Grounding, the first text-motion dataset that provides detailed textual descriptions of human actions along with their corresponding temporal segments. Extensive evaluations demonstrate the effectiveness of TM-Mamba on BABEL-Grounding.","sentences":["Human motion understanding is a fundamental task with diverse practical applications, facilitated by the availability of large-scale motion capture datasets.","Recent studies focus on text-motion tasks, such as text-based motion generation, editing and question answering.","In this study, we introduce the novel task of text-based human motion grounding (THMG), aimed at precisely localizing temporal segments corresponding to given textual descriptions within untrimmed motion sequences.","Capturing global temporal information is crucial for the THMG task.","However, transformer-based models that rely on global temporal self-attention face challenges when handling long untrimmed sequences due to the quadratic computational cost.","We address these challenges by proposing Text-controlled Motion Mamba (TM-Mamba), a unified model that integrates temporal global context, language query control, and spatial graph topology with only linear memory cost.","The core of the model is a text-controlled selection mechanism which dynamically incorporates global temporal information based on text query.","The model is further enhanced to be topology-aware through the integration of relational embeddings.","For evaluation, we introduce BABEL-Grounding, the first text-motion dataset that provides detailed textual descriptions of human actions along with their corresponding temporal segments.","Extensive evaluations demonstrate the effectiveness of TM-Mamba on BABEL-Grounding."],"url":"http://arxiv.org/abs/2404.11375v1","category":"cs.CV"}
{"created":"2024-04-17 13:31:51","title":"Simulation-based inference of black hole ringdowns in the time domain","abstract":"Gravitational waves emitted by a ringing black hole allow us to perform precision tests of General Relativity in the strong field regime. With improvements to our current gravitational wave detectors and upcoming next-generation detectors, developing likelihood-free parameter inference infrastructure is critical as we will face complications like non-standard noise properties, partial data and incomplete signal modeling that may not allow for an analytically tractable likelihood function. In this work, we present a proof-of-concept strategy to perform likelihood-free Bayesian inference on ringdown gravitational waves using simulation based inference. Specifically, our method is based on truncated sequential neural posterior estimation, which trains a neural density estimator of the posterior for a specific observed data segment. We setup the ringdown parameter estimation directly in the time domain. We show that the parameter estimation results obtained using our trained networks are in agreement with well-established Markov-Chain methods for simulated injections as well as analysis on real detector data corresponding to GW150914. Additionally, to assess our approach's internal consistency, we show that the density estimators pass a Bayesian coverage test.","sentences":["Gravitational waves emitted by a ringing black hole allow us to perform precision tests of General Relativity in the strong field regime.","With improvements to our current gravitational wave detectors and upcoming next-generation detectors, developing likelihood-free parameter inference infrastructure is critical as we will face complications like non-standard noise properties, partial data and incomplete signal modeling that may not allow for an analytically tractable likelihood function.","In this work, we present a proof-of-concept strategy to perform likelihood-free Bayesian inference on ringdown gravitational waves using simulation based inference.","Specifically, our method is based on truncated sequential neural posterior estimation, which trains a neural density estimator of the posterior for a specific observed data segment.","We setup the ringdown parameter estimation directly in the time domain.","We show that the parameter estimation results obtained using our trained networks are in agreement with well-established Markov-Chain methods for simulated injections as well as analysis on real detector data corresponding to GW150914.","Additionally, to assess our approach's internal consistency, we show that the density estimators pass a Bayesian coverage test."],"url":"http://arxiv.org/abs/2404.11373v1","category":"gr-qc"}
{"created":"2024-04-17 13:31:50","title":"S3PHER: Secure and Searchable System for Patient-driven HEalth data shaRing","abstract":"Healthcare data contains some of the most sensitive information about an individual, yet sharing this data with healthcare practitioners can significantly enhance patient care and support research efforts. However, current systems for sharing health data between patients and caregivers do not fully address the critical security requirements of privacy, confidentiality, and consent management. Furthermore, compliance with regulatory laws such as GDPR and HIPAA is often deficient, largely because patients typically are asked to provide general consent for healthcare entities to access their data. Recognizing the limitations of existing systems, we present S3PHER, a novel approach to sharing health data that provides patients with control over who accesses their data, what data is accessed, and when. Our system ensures end to end privacy by integrating a Proxy ReEncryption Scheme with a Searchable Encryption Scheme, utilizing Homomorphic Encryption to enable healthcare practitioners to privately search and access patients' documents. The practicality and benefits of S3PHER are further validated through end to end deployment and use case analyses, with tests on real datasets demonstrating promising execution times.","sentences":["Healthcare data contains some of the most sensitive information about an individual, yet sharing this data with healthcare practitioners can significantly enhance patient care and support research efforts.","However, current systems for sharing health data between patients and caregivers do not fully address the critical security requirements of privacy, confidentiality, and consent management.","Furthermore, compliance with regulatory laws such as GDPR and HIPAA is often deficient, largely because patients typically are asked to provide general consent for healthcare entities to access their data.","Recognizing the limitations of existing systems, we present S3PHER, a novel approach to sharing health data that provides patients with control over who accesses their data, what data is accessed, and when.","Our system ensures end to end privacy by integrating a Proxy ReEncryption Scheme with a Searchable Encryption Scheme, utilizing Homomorphic Encryption to enable healthcare practitioners to privately search and access patients' documents.","The practicality and benefits of S3PHER are further validated through end to end deployment and use case analyses, with tests on real datasets demonstrating promising execution times."],"url":"http://arxiv.org/abs/2404.11372v1","category":"cs.CR"}
{"created":"2024-04-17 13:30:45","title":"Characterizing and modeling harms from interactions with design patterns in AI interfaces","abstract":"The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces. Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks. Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions. Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered. We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces. Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs. DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems. Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs.","sentences":["The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces.","Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks.","Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions.","Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered.","We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces.","Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs.","DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems.","Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs."],"url":"http://arxiv.org/abs/2404.11370v1","category":"cs.HC"}
{"created":"2024-04-17 13:25:43","title":"Quantum eraser experiments for the demonstration of entanglement between swift electrons and light","abstract":"We propose a tangible experimental scheme for demonstrating quantum entanglement between swift electrons and light, relying on coherent cathodoluminescence for photon generation in a transmission electron microscope, and a quantum eraser setup for formation and verification of entanglement. The entanglement of free electrons with light is key to developing free-electron quantum optics and its potential applications such as quantum sensing, novel photonic and electron state generation, and entanglement between free electrons.","sentences":["We propose a tangible experimental scheme for demonstrating quantum entanglement between swift electrons and light, relying on coherent cathodoluminescence for photon generation in a transmission electron microscope, and a quantum eraser setup for formation and verification of entanglement.","The entanglement of free electrons with light is key to developing free-electron quantum optics and its potential applications such as quantum sensing, novel photonic and electron state generation, and entanglement between free electrons."],"url":"http://arxiv.org/abs/2404.11368v1","category":"quant-ph"}
{"created":"2024-04-17 13:15:33","title":"Momentum dependent nucleon-nucleon contact interactions and their effect on p-d scattering observables","abstract":"Starting from a complete set of relativistic nucleon-nucleon contact operators up to order $O(p^4)$ of the expansion in the soft (relative or nucleon) momentum $p$, we show that non-relativistic expansions of relativistic operators involve twenty-six independent combinations, two starting at $O(p^0)$, seven at order $O(p^2)$ and seventeen at order $O(p^4)$. This demonstrates the existence of two low-energy free constants that parameterize interactions dependent on the total momentum of the pair of nucleons $P$. The latter, through the use of a unitary transformation, can be removed in the two-nucleon fourth-order contact interaction of the Chiral Effective Field Theory, generating a three-nucleon interaction at the same order. Within a hybrid approach in which this interaction is considered together with the phenomenological potential AV18, we show that the LECs involved can be used to fit very accurate data on the polarization observables of the low-energy $p-d$ scattering, in particular the $A_y$ asymmetry.","sentences":["Starting from a complete set of relativistic nucleon-nucleon contact operators up to order $O(p^4)$ of the expansion in the soft (relative or nucleon) momentum $p$, we show that non-relativistic expansions of relativistic operators involve twenty-six independent combinations, two starting at $O(p^0)$, seven at order $O(p^2)$ and seventeen at order $O(p^4)$. This demonstrates the existence of two low-energy free constants that parameterize interactions dependent on the total momentum of the pair of nucleons $P$. The latter, through the use of a unitary transformation, can be removed in the two-nucleon fourth-order contact interaction of the Chiral Effective Field Theory, generating a three-nucleon interaction at the same order.","Within a hybrid approach in which this interaction is considered together with the phenomenological potential AV18, we show that the LECs involved can be used to fit very accurate data on the polarization observables of the low-energy $p-d$ scattering, in particular the $A_y$ asymmetry."],"url":"http://arxiv.org/abs/2404.11359v1","category":"nucl-th"}
{"created":"2024-04-17 13:14:52","title":"DeblurGS: Gaussian Splatting for Camera Motion Blur","abstract":"Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos.","sentences":["Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging.","The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches.","To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization.","We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.","Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process.","Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise.","Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos."],"url":"http://arxiv.org/abs/2404.11358v1","category":"cs.CV"}
{"created":"2024-04-17 13:09:33","title":"Distributed Fractional Bayesian Learning for Adaptive Optimization","abstract":"This paper considers a distributed adaptive optimization problem, where all agents only have access to their local cost functions with a common unknown parameter, whereas they mean to collaboratively estimate the true parameter and find the optimal solution over a connected network. A general mathematical framework for such a problem has not been studied yet. We aim to provide valuable insights for addressing parameter uncertainty in distributed optimization problems and simultaneously find the optimal solution. Thus, we propose a novel Prediction while Optimization scheme, which utilizes distributed fractional Bayesian learning through weighted averaging on the log-beliefs to update the beliefs of unknown parameters, and distributed gradient descent for renewing the estimation of the optimal solution. Then under suitable assumptions, we prove that all agents' beliefs and decision variables converge almost surely to the true parameter and the optimal solution under the true parameter, respectively. We further establish a sublinear convergence rate for the belief sequence. Finally, numerical experiments are implemented to corroborate the theoretical analysis.","sentences":["This paper considers a distributed adaptive optimization problem, where all agents only have access to their local cost functions with a common unknown parameter, whereas they mean to collaboratively estimate the true parameter and find the optimal solution over a connected network.","A general mathematical framework for such a problem has not been studied yet.","We aim to provide valuable insights for addressing parameter uncertainty in distributed optimization problems and simultaneously find the optimal solution.","Thus, we propose a novel Prediction while Optimization scheme, which utilizes distributed fractional Bayesian learning through weighted averaging on the log-beliefs to update the beliefs of unknown parameters, and distributed gradient descent for renewing the estimation of the optimal solution.","Then under suitable assumptions, we prove that all agents' beliefs and decision variables converge almost surely to the true parameter and the optimal solution under the true parameter, respectively.","We further establish a sublinear convergence rate for the belief sequence.","Finally, numerical experiments are implemented to corroborate the theoretical analysis."],"url":"http://arxiv.org/abs/2404.11354v1","category":"math.OC"}
{"created":"2024-04-17 13:08:26","title":"Calibrating Bayesian Learning via Regularization, Confidence Minimization, and Selective Inference","abstract":"The application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision. A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs. A conventional approach to improve calibration is the application of Bayesian ensembling. However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration. This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization. The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs for which the calibration performance is expected to be insufficient. Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods. Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs.","sentences":["The application of artificial intelligence (AI) models in fields such as engineering is limited by the known difficulty of quantifying the reliability of an AI's decision.","A well-calibrated AI model must correctly report its accuracy on in-distribution (ID) inputs, while also enabling the detection of out-of-distribution (OOD) inputs.","A conventional approach to improve calibration is the application of Bayesian ensembling.","However, owing to computational limitations and model misspecification, practical ensembling strategies do not necessarily enhance calibration.","This paper proposes an extension of variational inference (VI)-based Bayesian learning that integrates calibration regularization for improved ID performance, confidence minimization for OOD detection, and selective calibration to ensure a synergistic use of calibration regularization and confidence minimization.","The scheme is constructed successively by first introducing calibration-regularized Bayesian learning (CBNN), then incorporating out-of-distribution confidence minimization (OCM) to yield CBNN-OCM, and finally integrating also selective calibration to produce selective CBNN-OCM (SCBNN-OCM).","Selective calibration rejects inputs for which the calibration performance is expected to be insufficient.","Numerical results illustrate the trade-offs between ID accuracy, ID calibration, and OOD calibration attained by both frequentist and Bayesian learning methods.","Among the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance as compared to existing state-of-the-art approaches at the cost of rejecting a sufficiently large number of inputs."],"url":"http://arxiv.org/abs/2404.11350v1","category":"cs.LG"}
{"created":"2024-04-17 13:07:56","title":"TeClass: A Human-Annotated Relevance-based Headline Classification and Generation Dataset for Telugu","abstract":"News headline generation is a crucial task in increasing productivity for both the readers and producers of news. This task can easily be aided by automated News headline-generation models. However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models. We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines. Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles. While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data. To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs. We experiment with various baseline models and provide a comprehensive analysis of their results. We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset. The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores. To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available.","sentences":["News headline generation is a crucial task in increasing productivity for both the readers and producers of news.","This task can easily be aided by automated News headline-generation models.","However, the presence of irrelevant headlines in scraped news articles results in sub-optimal performance of generation models.","We propose that relevance-based headline classification can greatly aid the task of generating relevant headlines.","Relevance-based headline classification involves categorizing news headlines based on their relevance to the corresponding news articles.","While this task is well-established in English, it remains under-explored in low-resource languages like Telugu due to a lack of annotated data.","To address this gap, we present TeClass, the first-ever human-annotated Telugu news headline classification dataset, containing 78,534 annotations across 26,178 article-headline pairs.","We experiment with various baseline models and provide a comprehensive analysis of their results.","We further demonstrate the impact of this work by fine-tuning various headline generation models using TeClass dataset.","The headlines generated by the models fine-tuned on highly relevant article-headline pairs, showed about a 5 point increment in the ROUGE-L scores.","To encourage future research, the annotated dataset as well as the annotation guidelines will be made publicly available."],"url":"http://arxiv.org/abs/2404.11349v1","category":"cs.CL"}
{"created":"2024-04-17 13:07:10","title":"Farthest Point Sampling in Property Designated Chemical Feature Space as a General Strategy for Enhancing the Machine Learning Model Performance for Small Scale Chemical Dataset","abstract":"Machine learning model development in chemistry and materials science often grapples with the challenge of small scale, unbalanced labelled datasets, a common limitation in scientific experiments. These dataset imbalances can precipitate overfit ting and diminish model generalization. Our study explores the efficacy of the farthest point sampling (FPS) strategy within target ed chemical feature spaces, demonstrating its capacity to generate well-distributed training datasets and consequently enhance model performance. We rigorously evaluated this strategy across various machine learning models, including artificial neural net works (ANN), support vector machines (SVM), and random forests (RF), using datasets encapsulating physicochemical properties like standard boiling points and enthalpy of vaporization. Our findings reveal that FPS-based models consistently surpass those trained via random sampling, exhibiting superior predictive accuracy and robustness, alongside a marked reduction in overfitting. This improvement is particularly pronounced in smaller training datasets, attributable to increased diversity within the training data's chemical feature space. Consequently, FPS emerges as a universally effective and adaptable approach in approaching high performance machine learning models by small and biased experimental datasets prevalent in chemistry and materials science.","sentences":["Machine learning model development in chemistry and materials science often grapples with the challenge of small scale, unbalanced labelled datasets, a common limitation in scientific experiments.","These dataset imbalances can precipitate overfit ting and diminish model generalization.","Our study explores the efficacy of the farthest point sampling (FPS) strategy within target ed chemical feature spaces, demonstrating its capacity to generate well-distributed training datasets and consequently enhance model performance.","We rigorously evaluated this strategy across various machine learning models, including artificial neural net works (ANN), support vector machines (SVM), and random forests (RF), using datasets encapsulating physicochemical properties like standard boiling points and enthalpy of vaporization.","Our findings reveal that FPS-based models consistently surpass those trained via random sampling, exhibiting superior predictive accuracy and robustness, alongside a marked reduction in overfitting.","This improvement is particularly pronounced in smaller training datasets, attributable to increased diversity within the training data's chemical feature space.","Consequently, FPS emerges as a universally effective and adaptable approach in approaching high performance machine learning models by small and biased experimental datasets prevalent in chemistry and materials science."],"url":"http://arxiv.org/abs/2404.11348v1","category":"physics.chem-ph"}
{"created":"2024-04-17 13:07:01","title":"Modern tools for computing neutron star properties","abstract":"Astronomical observations place increasingly tighter and more diverse constraints on the properties of neutron stars (NS). Examples include observations of radio or gamma-ray pulsars, accreting neutron stars and x-ray bursts, magnetar giant flares, and recently, the gravitational waves (GW) from coalescing binary neutron stars. Computing NS properties for a given EOS, such as mass, radius, moment of inertia, tidal deformability, and innermost stable circular orbits (ISCO), is therefore an important task. This task is unnecessarily difficult because relevant formulas are scattered throughout the literature and publicly available software tools are far from being complete and easy to use. Further, naive implementations are unreliable in numerical corner cases, most notably when using equations of state (EOS) with phase transitions. To improve the situation, we provide a public library for computing NS properties and handling of EOS data. Further, we include a collection of EOS based on existing nuclear physics models together with precomputed sequences of NS models. All methods are accessible via a Python interface. This article collects all relevant equations and numerical methods in full detail, including a novel formulation for the tidal deformability equations suitable for use with phase transitions. As a sidenote to the topic of ISCOs, we discuss the stability of non-interacting dark matter particle circular orbits inside NSs. Finally, we present some simple applications relevant for parameter estimation studies of GW data. For example, we explore the validity of universal relations, and discuss the appearance of multiple stable branches for parametrized EOS.","sentences":["Astronomical observations place increasingly tighter and more diverse constraints on the properties of neutron stars (NS).","Examples include observations of radio or gamma-ray pulsars, accreting neutron stars and x-ray bursts, magnetar giant flares, and recently, the gravitational waves (GW) from coalescing binary neutron stars.","Computing NS properties for a given EOS, such as mass, radius, moment of inertia, tidal deformability, and innermost stable circular orbits (ISCO), is therefore an important task.","This task is unnecessarily difficult because relevant formulas are scattered throughout the literature and publicly available software tools are far from being complete and easy to use.","Further, naive implementations are unreliable in numerical corner cases, most notably when using equations of state (EOS) with phase transitions.","To improve the situation, we provide a public library for computing NS properties and handling of EOS data.","Further, we include a collection of EOS based on existing nuclear physics models together with precomputed sequences of NS models.","All methods are accessible via a Python interface.","This article collects all relevant equations and numerical methods in full detail, including a novel formulation for the tidal deformability equations suitable for use with phase transitions.","As a sidenote to the topic of ISCOs, we discuss the stability of non-interacting dark matter particle circular orbits inside NSs.","Finally, we present some simple applications relevant for parameter estimation studies of GW data.","For example, we explore the validity of universal relations, and discuss the appearance of multiple stable branches for parametrized EOS."],"url":"http://arxiv.org/abs/2404.11346v1","category":"gr-qc"}
{"created":"2024-04-17 13:03:51","title":"Transition Graphs of Interacting Hysterons: Structure, Design, Organization and Statistics","abstract":"Collections of bistable elements called hysterons provide a powerful model to capture the sequential response and memory effects of frustrated, multistable media in the athermal, quasistatic limit. While a century of work has elucidated, in great detail, the properties of ensembles of non-interacting hysterons - the so-called Preisach model - comparatively little is known about the behavior and properties of interacting hysterons. Here we discuss a general framework for interacting hysterons, focussing on the relation between the design parameters that specify the hysterons and their interactions, and the resulting transition graphs (t-graphs). We show how the structure of such t-graphs can be thought of as being composed of a scaffold that is dressed by (avalanche) transitions selected from finite binary trees. Moreover, we provide a systematic framework to straightforwardly determine the design inequalities for a given t-graph, and discuss an effective method to determine if a certain t-graph topology is realizable by a set of interacting hysterons. Altogether, our work builds on the Preisach model by generalizing the hysteron-dependent switching fields to the state-dependent switching fields. As a result, while in the Preisach model, the main loop identifies the critical hysterons that trigger a transition, in the case of interacting hysterons, the scaffold contains this critical information and assumes a central role in determining permissible transitions. In addition, our work suggests strategies to deal with the combinatorial explosion of the number and variety of t-graphs for interacting hysterons. This approach paves the way for a deeper theoretical understanding of the properties and statistics of t-graphs, and opens a route to materializing complex pathways, memory effects and embodied computations in (meta)materials based on interacting hysterons.","sentences":["Collections of bistable elements called hysterons provide a powerful model to capture the sequential response and memory effects of frustrated, multistable media in the athermal, quasistatic limit.","While a century of work has elucidated, in great detail, the properties of ensembles of non-interacting hysterons - the so-called Preisach model - comparatively little is known about the behavior and properties of interacting hysterons.","Here we discuss a general framework for interacting hysterons, focussing on the relation between the design parameters that specify the hysterons and their interactions, and the resulting transition graphs (t-graphs).","We show how the structure of such t-graphs can be thought of as being composed of a scaffold that is dressed by (avalanche) transitions selected from finite binary trees.","Moreover, we provide a systematic framework to straightforwardly determine the design inequalities for a given t-graph, and discuss an effective method to determine if a certain t-graph topology is realizable by a set of interacting hysterons.","Altogether, our work builds on the Preisach model by generalizing the hysteron-dependent switching fields to the state-dependent switching fields.","As a result, while in the Preisach model, the main loop identifies the critical hysterons that trigger a transition, in the case of interacting hysterons, the scaffold contains this critical information and assumes a central role in determining permissible transitions.","In addition, our work suggests strategies to deal with the combinatorial explosion of the number and variety of t-graphs for interacting hysterons.","This approach paves the way for a deeper theoretical understanding of the properties and statistics of t-graphs, and opens a route to materializing complex pathways, memory effects and embodied computations in (meta)materials based on interacting hysterons."],"url":"http://arxiv.org/abs/2404.11344v1","category":"cond-mat.soft"}
{"created":"2024-04-17 13:03:07","title":"Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System","abstract":"Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms. However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs). Despite their effectiveness under cold scenarios, we observe that they underperform simple traditional collaborative filtering models under warm scenarios due to the lack of collaborative knowledge. In this work, we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario. Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited. This approach yields two advantages: (1) model-agnostic, allowing for integration with various existing CF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically required for LLM-based recommenders. Our extensive experiments on various real-world datasets demonstrate the superiority of A-LLMRec in various scenarios, including cold/warm, few-shot, cold user, and cross-domain scenarios. Beyond the recommendation task, we also show the potential of A-LLMRec in generating natural language outputs based on the understanding of the collaborative knowledge by performing a favorite genre prediction task. Our code is available at https://github.com/ghdtjr/A-LLMRec .","sentences":["Collaborative filtering recommender systems (CF-RecSys) have shown successive results in enhancing the user experience on social media and e-commerce platforms.","However, as CF-RecSys struggles under cold scenarios with sparse user-item interactions, recent strategies have focused on leveraging modality information of user/items (e.g., text or images) based on pre-trained modality encoders and Large Language Models (LLMs).","Despite their effectiveness under cold scenarios, we observe that they underperform simple traditional collaborative filtering models under warm scenarios due to the lack of collaborative knowledge.","In this work, we propose an efficient All-round LLM-based Recommender system, called A-LLMRec, that excels not only in the cold scenario but also in the warm scenario.","Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited.","This approach yields two advantages: (1) model-agnostic, allowing for integration with various existing CF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically required for LLM-based recommenders.","Our extensive experiments on various real-world datasets demonstrate the superiority of A-LLMRec in various scenarios, including cold/warm, few-shot, cold user, and cross-domain scenarios.","Beyond the recommendation task, we also show the potential of A-LLMRec in generating natural language outputs based on the understanding of the collaborative knowledge by performing a favorite genre prediction task.","Our code is available at https://github.com/ghdtjr/A-LLMRec ."],"url":"http://arxiv.org/abs/2404.11343v1","category":"cs.IR"}
{"created":"2024-04-17 13:00:52","title":"The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology","abstract":"In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.","sentences":["In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets.","Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems.","As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems.","The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields.","We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression.","For applications to causal inference, the chambers allow us to carefully perform interventions.","We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks.","All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber."],"url":"http://arxiv.org/abs/2404.11341v1","category":"cs.AI"}
{"created":"2024-04-17 12:53:45","title":"SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and Identification on a Minimap","abstract":"Tracking and identifying athletes on the pitch holds a central role in collecting essential insights from the game, such as estimating the total distance covered by players or understanding team tactics. This tracking and identification process is crucial for reconstructing the game state, defined by the athletes' positions and identities on a 2D top-view of the pitch, (i.e. a minimap). However, reconstructing the game state from videos captured by a single camera is challenging. It requires understanding the position of the athletes and the viewpoint of the camera to localize and identify players within the field. In this work, we formalize the task of Game State Reconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction dataset focusing on football videos. SoccerNet-GSR is composed of 200 video sequences of 30 seconds, annotated with 9.37 million line points for pitch localization and camera calibration, as well as over 2.36 million athlete positions on the pitch with their respective role, team, and jersey number. Furthermore, we introduce GS-HOTA, a novel metric to evaluate game state reconstruction methods. Finally, we propose and release an end-to-end baseline for game state reconstruction, bootstrapping the research on this task. Our experiments show that GSR is a challenging novel task, which opens the field for future research. Our dataset and codebase are publicly available at https://github.com/SoccerNet/sn-gamestate.","sentences":["Tracking and identifying athletes on the pitch holds a central role in collecting essential insights from the game, such as estimating the total distance covered by players or understanding team tactics.","This tracking and identification process is crucial for reconstructing the game state, defined by the athletes' positions and identities on a 2D top-view of the pitch, (i.e. a minimap).","However, reconstructing the game state from videos captured by a single camera is challenging.","It requires understanding the position of the athletes and the viewpoint of the camera to localize and identify players within the field.","In this work, we formalize the task of Game State Reconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction dataset focusing on football videos.","SoccerNet-GSR is composed of 200 video sequences of 30 seconds, annotated with 9.37 million line points for pitch localization and camera calibration, as well as over 2.36 million athlete positions on the pitch with their respective role, team, and jersey number.","Furthermore, we introduce GS-HOTA, a novel metric to evaluate game state reconstruction methods.","Finally, we propose and release an end-to-end baseline for game state reconstruction, bootstrapping the research on this task.","Our experiments show that GSR is a challenging novel task, which opens the field for future research.","Our dataset and codebase are publicly available at https://github.com/SoccerNet/sn-gamestate."],"url":"http://arxiv.org/abs/2404.11335v1","category":"cs.CV"}
{"created":"2024-04-17 12:52:08","title":"The dynamics of diversity on corporate boards","abstract":"Diversity in leadership positions and corporate boards is an important aspect of equality. It is important because it is the key to better decision-making and innovation, and above all, it paves the way for future generations to participate and shape our society. Many studies emphasize the importance of the visibility of role models and the effect that connectivity has on the success of minorities in leadership. However, the connectivity of firms, the dynamics of the adoption of minorities, and the long-term effects have not been well understood. Here, we present a model that shows how these effects work together in a dynamic model that is calibrated with empirical data of firm and board networks. We show that homophily -- the appointment of minorities is influenced by the presence of minorities in a board and its neighboring entities -- is an important effect shaping the trajectory towards equality. We further show how perception biases and feedback related to the centrality of minority members influence the dynamic. We find that reaching equality can be sped up or slowed down depending on the distribution of minorities in central firms. These insights bear significant implications for policy-making geared towards fostering equality and diversity within corporate boards.","sentences":["Diversity in leadership positions and corporate boards is an important aspect of equality.","It is important because it is the key to better decision-making and innovation, and above all, it paves the way for future generations to participate and shape our society.","Many studies emphasize the importance of the visibility of role models and the effect that connectivity has on the success of minorities in leadership.","However, the connectivity of firms, the dynamics of the adoption of minorities, and the long-term effects have not been well understood.","Here, we present a model that shows how these effects work together in a dynamic model that is calibrated with empirical data of firm and board networks.","We show that homophily -- the appointment of minorities is influenced by the presence of minorities in a board and its neighboring entities -- is an important effect shaping the trajectory towards equality.","We further show how perception biases and feedback related to the centrality of minority members influence the dynamic.","We find that reaching equality can be sped up or slowed down depending on the distribution of minorities in central firms.","These insights bear significant implications for policy-making geared towards fostering equality and diversity within corporate boards."],"url":"http://arxiv.org/abs/2404.11334v1","category":"econ.TH"}
{"created":"2024-04-17 12:51:26","title":"Accreting Black Holes in Dark Matter Halos","abstract":"We examine the thin accretion disk behaviors surrounding black holes embedded in cold dark matter halos and scalar field dark matter halos. We first calculate the event horizons and derive the equations of motion and effective potential in black hole geometries with different dark matter halos. We then compute the specific energy, specific angular momentum, and angular velocity of particles moving along circular orbits. We also derive the effective potentials to find the locations of the innermost stable circular orbit (ISCO) and compare them to the Schwarzschild and Kerr black holes without the dark matter haloes. We also use the observed ISCO of the supermassive black hole at the Galactic Center of the Milky Way, Sagittarius A*, to constrain the dark matter halos and discuss the astrophysical Gamma-ray bursts (GRBs) generation from systems.","sentences":["We examine the thin accretion disk behaviors surrounding black holes embedded in cold dark matter halos and scalar field dark matter halos.","We first calculate the event horizons and derive the equations of motion and effective potential in black hole geometries with different dark matter halos.","We then compute the specific energy, specific angular momentum, and angular velocity of particles moving along circular orbits.","We also derive the effective potentials to find the locations of the innermost stable circular orbit (ISCO) and compare them to the Schwarzschild and Kerr black holes without the dark matter haloes.","We also use the observed ISCO of the supermassive black hole at the Galactic Center of the Milky Way, Sagittarius A*, to constrain the dark matter halos and discuss the astrophysical Gamma-ray bursts (GRBs) generation from systems."],"url":"http://arxiv.org/abs/2404.11333v1","category":"hep-ph"}
{"created":"2024-04-17 12:44:07","title":"On the relaxation to equilibrium of a quantum oscillator interacting with a radiation field","abstract":"In this article we investigate from the point of view of spectral theory the problem of relaxation to thermodynamical equilibrium of a quantum harmonic oscillator interacting with a radiation field. Our starting point is a system of infinitely many Pauli master equations governing the time evolution of the occupation probabilities of the available quantum states. The system we consider is derived from the evolution equation for the reduced density operator obtained after the initial interaction of the oscillator with the radiation field, the latter acting as a heat bath. We provide a complete spectral analysis of the infinitesimal generator of the equations, showing thereby that it generates an infinite-dimensional system of hyperbolic type. This implies that every global solution to the equations converge exponentially rapidly toward the corresponding Gibbs equilibrium state. We also provide a complete spectral analysis of a linear pencil naturally associated with the Pauli equations. All of our considerations revolve around the notion of compactness, more specifically around the existence of a new compact embedding result involving a space of sequences of Sobolev type into a weighted space of square-integrable summable sequences.","sentences":["In this article we investigate from the point of view of spectral theory the problem of relaxation to thermodynamical equilibrium of a quantum harmonic oscillator interacting with a radiation field.","Our starting point is a system of infinitely many Pauli master equations governing the time evolution of the occupation probabilities of the available quantum states.","The system we consider is derived from the evolution equation for the reduced density operator obtained after the initial interaction of the oscillator with the radiation field, the latter acting as a heat bath.","We provide a complete spectral analysis of the infinitesimal generator of the equations, showing thereby that it generates an infinite-dimensional system of hyperbolic type.","This implies that every global solution to the equations converge exponentially rapidly toward the corresponding Gibbs equilibrium state.","We also provide a complete spectral analysis of a linear pencil naturally associated with the Pauli equations.","All of our considerations revolve around the notion of compactness, more specifically around the existence of a new compact embedding result involving a space of sequences of Sobolev type into a weighted space of square-integrable summable sequences."],"url":"http://arxiv.org/abs/2404.11329v1","category":"math-ph"}
{"created":"2024-04-17 12:38:58","title":"Single-temporal Supervised Remote Change Detection for Domain Generalization","abstract":"Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.","sentences":["Change detection is widely applied in remote sensing image analysis.","Existing methods require training models separately for each dataset, which leads to poor domain generalization.","Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical.","In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization.","Additionally, we propose a dynamic context optimization for prompt learning.","Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN).","This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization.","Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods.","Code will be available."],"url":"http://arxiv.org/abs/2404.11326v1","category":"cs.CV"}
{"created":"2024-04-17 12:35:55","title":"Weighted-Average Least Squares for Negative Binomial Regression","abstract":"Model averaging methods have become an increasingly popular tool for improving predictions and dealing with model uncertainty, especially in Bayesian settings. Recently, frequentist model averaging methods such as information theoretic and least squares model averaging have emerged. This work focuses on the issue of covariate uncertainty where managing the computational resources is key: The model space grows exponentially with the number of covariates such that averaged models must often be approximated. Weighted-average least squares (WALS), first introduced for (generalized) linear models in the econometric literature, combines Bayesian and frequentist aspects and additionally employs a semiorthogonal transformation of the regressors to reduce the computational burden. This paper extends WALS for generalized linear models to the negative binomial (NB) regression model for overdispersed count data. A simulation experiment and an empirical application using data on doctor visits were conducted to compare the predictive power of WALS for NB regression to traditional estimators. The results show that WALS for NB improves on the maximum likelihood estimator in sparse situations and is competitive with lasso while being computationally more efficient.","sentences":["Model averaging methods have become an increasingly popular tool for improving predictions and dealing with model uncertainty, especially in Bayesian settings.","Recently, frequentist model averaging methods such as information theoretic and least squares model averaging have emerged.","This work focuses on the issue of covariate uncertainty where managing the computational resources is key: The model space grows exponentially with the number of covariates such that averaged models must often be approximated.","Weighted-average least squares (WALS), first introduced for (generalized) linear models in the econometric literature, combines Bayesian and frequentist aspects and additionally employs a semiorthogonal transformation of the regressors to reduce the computational burden.","This paper extends WALS for generalized linear models to the negative binomial (NB) regression model for overdispersed count data.","A simulation experiment and an empirical application using data on doctor visits were conducted to compare the predictive power of WALS for NB regression to traditional estimators.","The results show that WALS for NB improves on the maximum likelihood estimator in sparse situations and is competitive with lasso while being computationally more efficient."],"url":"http://arxiv.org/abs/2404.11324v1","category":"econ.EM"}
{"created":"2024-04-17 12:34:44","title":"A \"lighthouse\" laser-driven staged proton accelerator allowing for ultrafast angular and spectral control","abstract":"Compact laser-plasma acceleration of fast ions has made great strides since its discovery over two decades ago, resulting in the current generation of high-energy ($\\geq 100\\,\\rm MeV$) ultracold beams over ultrashort ($\\leq 1\\,\\rm ps$) durations. To unlock broader applications of these beams, we need the ability to tailor the ion energy spectrum. Here, we present a scheme that achieves precisely this by accelerating protons in a \"lighthouse\" fashion, whereby the highest-energy component of the beam is emitted in a narrow cone, well separated from the lower-energy components. This is made possible by a two-stage interaction in which the rear surface of the target is first set into rapid motion before the main acceleration phase. This approach offers the additional advantages of leveraging a robust sheath acceleration process in standard micron-thick targets and being optically controllable.","sentences":["Compact laser-plasma acceleration of fast ions has made great strides since its discovery over two decades ago, resulting in the current generation of high-energy ($\\geq 100\\,\\rm MeV$) ultracold beams over ultrashort ($\\leq 1\\,\\rm ps$) durations.","To unlock broader applications of these beams, we need the ability to tailor the ion energy spectrum.","Here, we present a scheme that achieves precisely this by accelerating protons in a \"lighthouse\" fashion, whereby the highest-energy component of the beam is emitted in a narrow cone, well separated from the lower-energy components.","This is made possible by a two-stage interaction in which the rear surface of the target is first set into rapid motion before the main acceleration phase.","This approach offers the additional advantages of leveraging a robust sheath acceleration process in standard micron-thick targets and being optically controllable."],"url":"http://arxiv.org/abs/2404.11321v1","category":"physics.plasm-ph"}
{"created":"2024-04-17 12:32:26","title":"Computing renormalized curvature integrals on Poincar\u00e9-Einstein manifolds","abstract":"We describe a general procedure for computing renormalized curvature integrals on Poincar\\'e-Einstein manifolds. In particular, we explain the connection between the Gauss-Bonnet-type formulas of Albin and Chang-Qing-Yang for the renormalized volume, and explicitly identify a scalar conformal invariant in the latter formula. Our approach constructs scalar conformal invariants that are divergences at any Einstein manifold; these imply that the scalar invariant in the Chang-Qing-Yang formula is not unique in dimension at least eight. Our procedure also produces explicit conformally invariant Gauss-Bonnet-type formulas for compact Einstein manifolds.","sentences":["We describe a general procedure for computing renormalized curvature integrals on Poincar\\'e-Einstein manifolds.","In particular, we explain the connection between the Gauss-Bonnet-type formulas of Albin and Chang-Qing-Yang for the renormalized volume, and explicitly identify a scalar conformal invariant in the latter formula.","Our approach constructs scalar conformal invariants that are divergences at any Einstein manifold; these imply that the scalar invariant in the Chang-Qing-Yang formula is not unique in dimension at least eight.","Our procedure also produces explicit conformally invariant Gauss-Bonnet-type formulas for compact Einstein manifolds."],"url":"http://arxiv.org/abs/2404.11319v1","category":"math.DG"}
{"created":"2024-04-17 12:30:54","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives","abstract":"The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text. Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples. However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples. Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model. To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR. To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly. The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures. Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario.","sentences":["The Composed Image Retrieval (CIR) task aims to retrieve target images using a composed query consisting of a reference image and a modified text.","Advanced methods often utilize contrastive learning as the optimization objective, which benefits from adequate positive and negative examples.","However, the triplet for CIR incurs high manual annotation costs, resulting in limited positive examples.","Furthermore, existing methods commonly use in-batch negative sampling, which reduces the negative number available for the model.","To address the problem of lack of positives, we propose a data generation method by leveraging a multi-modal large language model to construct triplets for CIR.","To introduce more negatives during fine-tuning, we design a two-stage fine-tuning framework for CIR, whose second stage introduces plenty of static representations of negatives to optimize the representation space rapidly.","The above two improvements can be effectively stacked and designed to be plug-and-play, easily applied to existing CIR models without changing their original architectures.","Extensive experiments and ablation analysis demonstrate that our method effectively scales positives and negatives and achieves state-of-the-art results on both FashionIQ and CIRR datasets.","In addition, our methods also perform well in zero-shot composed image retrieval, providing a new CIR solution for the low-resources scenario."],"url":"http://arxiv.org/abs/2404.11317v1","category":"cs.CV"}
{"created":"2024-04-17 12:29:38","title":"Asymptotic, second-order homogenization of linear elastic beam networks","abstract":"We propose a general approach to the higher-order homogenization of discrete elastic networks made up of linear elastic beams or springs in dimension 2 or 3. The network may be nearly (rather than exactly) periodic: its elastic and geometric properties are allowed to vary slowly in space, in addition to being periodic at the scale of the unit cell. The reference configuration may be prestressed. A homogenized strain energy depending on both the macroscopic strain $\\boldsymbol{\\varepsilon}$ and its gradient $\\nabla \\boldsymbol{\\varepsilon}$ is obtained by means of a two-scale expansion. The homogenized energy is asymptotically exact two orders beyond that obtained by classical homogenization. The homogenization method is implemented in a symbolic calculation language and applied to various types of networks, such as a 2D honeycomb, a 2D Kagome lattice, a 3D truss and a 1D pantograph. It is validated by comparing the predictions of the microscopic displacement to that obtained by full, discrete simulations. This second-order method remains highly accurate even when the strain gradient effects are significant, such as near the lips of a crack tip or in regions where a gradient of pre-strain is imposed.","sentences":["We propose a general approach to the higher-order homogenization of discrete elastic networks made up of linear elastic beams or springs in dimension 2 or 3.","The network may be nearly (rather than exactly) periodic: its elastic and geometric properties are allowed to vary slowly in space, in addition to being periodic at the scale of the unit cell.","The reference configuration may be prestressed.","A homogenized strain energy depending on both the macroscopic strain $\\boldsymbol{\\varepsilon}$ and its gradient $\\nabla \\boldsymbol{\\varepsilon}$ is obtained by means of a two-scale expansion.","The homogenized energy is asymptotically exact two orders beyond that obtained by classical homogenization.","The homogenization method is implemented in a symbolic calculation language and applied to various types of networks, such as a 2D honeycomb, a 2D Kagome lattice, a 3D truss and a 1D pantograph.","It is validated by comparing the predictions of the microscopic displacement to that obtained by full, discrete simulations.","This second-order method remains highly accurate even when the strain gradient effects are significant, such as near the lips of a crack tip or in regions where a gradient of pre-strain is imposed."],"url":"http://arxiv.org/abs/2404.11316v1","category":"cond-mat.soft"}
{"created":"2024-04-17 12:26:32","title":"Destructive and constructive RIS beamforming in an ISAC-multi-user MIMO network","abstract":"Integrated sensing and communication (ISAC) has already established itself as a promising solution to the spectrum scarcity problem, even more so when paired with a reconfigurable intelligent surface (RIS) as RISs can shape the propagation environment by adjusting their phase-shift coefficients. Albeit the potential performance gain, a RIS also poses a security threat to the system: in this paper, we explore both sides of the RIS presence in a multi-user MIMO (multiple-input multiple-output) ISAC network. We first develop an alternating optimization algorithm, obtaining the active and passive beamforming vectors maximizing the sensing signal-to-noise ratio (SNR) under minimum signal-to-interference-plus-noise ratio (SINR) constraints for the communication users and finite power budget. We also investigate the destructive potential of RIS by devising a RIS phase-shift optimization algorithm that minimizes sensing SNR while preserving the same minimum communication SINR previously guaranteed by the system. We further investigate the impact of the RIS's individual element failures on the system performances. The simulation results show that the RIS performance-boosting potential is as good as its destructive one and that both of our optimization strategies show some resilience towards the investigated impairments.","sentences":["Integrated sensing and communication (ISAC) has already established itself as a promising solution to the spectrum scarcity problem, even more so when paired with a reconfigurable intelligent surface (RIS) as RISs can shape the propagation environment by adjusting their phase-shift coefficients.","Albeit the potential performance gain, a RIS also poses a security threat to the system: in this paper, we explore both sides of the RIS presence in a multi-user MIMO (multiple-input multiple-output) ISAC network.","We first develop an alternating optimization algorithm, obtaining the active and passive beamforming vectors maximizing the sensing signal-to-noise ratio (SNR) under minimum signal-to-interference-plus-noise ratio (SINR) constraints for the communication users and finite power budget.","We also investigate the destructive potential of RIS by devising a RIS phase-shift optimization algorithm that minimizes sensing SNR while preserving the same minimum communication SINR previously guaranteed by the system.","We further investigate the impact of the RIS's individual element failures on the system performances.","The simulation results show that the RIS performance-boosting potential is as good as its destructive one and that both of our optimization strategies show some resilience towards the investigated impairments."],"url":"http://arxiv.org/abs/2404.11314v1","category":"cs.IT"}
{"created":"2024-04-17 12:26:13","title":"NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods and Results","abstract":"This paper reviews the NTIRE 2024 Challenge on Shortform UGC Video Quality Assessment (S-UGC VQA), where various excellent solutions are submitted and evaluated on the collected dataset KVQ from popular short-form video platform, i.e., Kuaishou/Kwai Platform. The KVQ database is divided into three parts, including 2926 videos for training, 420 videos for validation, and 854 videos for testing. The purpose is to build new benchmarks and advance the development of S-UGC VQA. The competition had 200 participants and 13 teams submitted valid solutions for the final testing phase. The proposed solutions achieved state-of-the-art performances for S-UGC VQA. The project can be found at https://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.","sentences":["This paper reviews the NTIRE 2024 Challenge on Shortform UGC Video Quality Assessment (S-UGC VQA), where various excellent solutions are submitted and evaluated on the collected dataset KVQ from popular short-form video platform, i.e., Kuaishou/Kwai Platform.","The KVQ database is divided into three parts, including 2926 videos for training, 420 videos for validation, and 854 videos for testing.","The purpose is to build new benchmarks and advance the development of S-UGC VQA.","The competition had 200 participants and 13 teams submitted valid solutions for the final testing phase.","The proposed solutions achieved state-of-the-art performances for S-UGC VQA.","The project can be found at https://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024."],"url":"http://arxiv.org/abs/2404.11313v1","category":"eess.IV"}
{"created":"2024-04-17 12:18:12","title":"On the number of subsequence sums of a sequence in finite abelian groups related to the support of the sequence","abstract":"Let $G$ be a finite abelian group and $S$ a sequence with elements of $G$. Let $|S|$ denote the length of $S$ and $k$ an integer with $k\\in [1, |S|]$. Let $\\Sigma_{k}(S) \\subset G$ denote the set of group elements which can be expressed as a sum of a subsequence of $S$ with length $k$. Let $\\Sigma(S)=\\cup_{k=1}^{|S|}\\Sigma_{k}(S)$ and $\\Sigma_{\\geq k}(S)=\\cup_{t=k}^{|S|}\\Sigma_{t}(S)$. It is known that if $0\\not\\in \\Sigma(S)$, then $|\\Sigma(S)|\\geq |S|+|\\mathrm{supp}(S)|-1$, where $|\\mathrm{supp}(S)|$ denotes the number of distinct terms in $S$. In this paper, we study the above inequality. On one hand, we determine the sequence $S$ with $0\\notin \\Sigma(S)$ such that $|\\Sigma(S)|= |S|+|\\mathrm{supp}(S)|-1$. As a corollary, we disprove a conjecture of Gao, Grynkiewicz, and Xia. On the other hand, we generate the above inequality as if $|S|>k$ and $0\\not\\in \\Sigma_{\\geq k}(S)\\cup \\Sigma_{1}(S)$, then $|\\Sigma_{\\geq k}(S)|\\geq |S|-k+|\\mathrm{supp}(S)|$. Then among other results, we give an alternative proof of a conjecture of Hamidoune, which was first proved by Gao, Grynkiewicz, and Xia.","sentences":["Let $G$ be a finite abelian group and $S$ a sequence with elements of $G$. Let $|S|$ denote the length of $S$ and $k$ an integer with $k\\in [1, |S|]$. Let $\\Sigma_{k}(S)","\\subset G$ denote the set of group elements which can be expressed as a sum of a subsequence of $S$ with length $k$. Let $\\Sigma(S)=\\cup_{k=1}^{|S|}\\Sigma_{k}(S)$ and $\\Sigma_{\\geq k}(S)=\\cup_{t=k}^{|S|}\\Sigma_{t}(S)$. It is known that if $0\\not\\in \\Sigma(S)$, then $|\\Sigma(S)|\\geq |S|+|\\mathrm{supp}(S)|-1$, where $|\\mathrm{supp}(S)|$ denotes the number of distinct terms in $S$. In this paper, we study the above inequality.","On one hand, we determine the sequence $S$ with $0\\notin \\Sigma(S)$ such that $|\\Sigma(S)|= |S|+|\\mathrm{supp}(S)|-1$. As a corollary, we disprove a conjecture of Gao, Grynkiewicz, and Xia.","On the other hand, we generate the above inequality as if $|S|>k$ and $0\\not\\in \\Sigma_{\\geq k}(S)\\cup \\Sigma_{1}(S)$, then $|\\Sigma_{\\geq k}(S)|\\geq |S|-k+|\\mathrm{supp}(S)|$. Then among other results, we give an alternative proof of a conjecture of Hamidoune, which was first proved by Gao, Grynkiewicz, and Xia."],"url":"http://arxiv.org/abs/2404.11307v1","category":"math.CO"}
{"created":"2024-04-17 12:16:41","title":"Existential Unforgeability in Quantum Authentication From Quantum Physical Unclonable Functions Based on Random von Neumann Measurement","abstract":"Physical Unclonable Functions (PUFs) are hardware devices with the assumption of possessing inherent non-clonable physical randomness which leads to unique pairs of inputs and outputs that provide a secure fingerprint for cryptographic protocols like authentication. In the case of quantum PUFs (QPUFs), the input-output pairs consist of quantum states instead of classical bitstrings, offering advantages over classical PUFs (CPUFs) such as challenge reusability via public channels and non-reliance over any trusted party due to the no-cloning theorem. In recent literature, a generalized mathematical framework for studying QPUFs was developed, which paved the way for having QPUF models with provable security. It was proved that \\emph{existential unforgeability} against Quantum Polynomial Time (QPT) adversaries cannot be achieved by any random unitary QPUF. Since measurements are non-unitary quantum processes, we define a QPUF based on random von Neumann measurements. We prove that such a QPUF is existentially unforgeable. Thus, we introduce the first model in existing literature that depicts such a high level of provable security. We also prove that the Quantum Phase Estimation (QPE) protocol applied on a Haar random unitary serves as an approximate implementation for this kind of QPUF as it approximates a von Neumann measurement on the eigenbasis of the unitary.","sentences":["Physical Unclonable Functions (PUFs) are hardware devices with the assumption of possessing inherent non-clonable physical randomness which leads to unique pairs of inputs and outputs that provide a secure fingerprint for cryptographic protocols like authentication.","In the case of quantum PUFs (QPUFs), the input-output pairs consist of quantum states instead of classical bitstrings, offering advantages over classical PUFs (CPUFs) such as challenge reusability via public channels and non-reliance over any trusted party due to the no-cloning theorem.","In recent literature, a generalized mathematical framework for studying QPUFs was developed, which paved the way for having QPUF models with provable security.","It was proved that \\emph{existential unforgeability} against Quantum Polynomial Time (QPT) adversaries cannot be achieved by any random unitary QPUF.","Since measurements are non-unitary quantum processes, we define a QPUF based on random von Neumann measurements.","We prove that such a QPUF is existentially unforgeable.","Thus, we introduce the first model in existing literature that depicts such a high level of provable security.","We also prove that the Quantum Phase Estimation (QPE) protocol applied on a Haar random unitary serves as an approximate implementation for this kind of QPUF as it approximates a von Neumann measurement on the eigenbasis of the unitary."],"url":"http://arxiv.org/abs/2404.11306v1","category":"quant-ph"}
{"created":"2024-04-17 12:14:07","title":"Dynamic Phasor Modeling of Single-Phase Grid-Forming Converters","abstract":"In modern power systems, grid-forming power converters (GFMCs) have emerged as an enabling technology. However, the modeling of single-phase GFMCs faces new challenges. In particular, the nonlinear orthogonal signal generation unit, crucial for power measurement, still lacks an accurate model. To overcome the challenges, this letter proposes a dynamic phasor model of single-phase GFMCs. Moreover, we linearize the proposed model and perform stability analysis, which confirm that the proposed model is more accurate than existing models. Experimental results validate the improved accuracy of the proposed dynamic phasor model.","sentences":["In modern power systems, grid-forming power converters (GFMCs) have emerged as an enabling technology.","However, the modeling of single-phase GFMCs faces new challenges.","In particular, the nonlinear orthogonal signal generation unit, crucial for power measurement, still lacks an accurate model.","To overcome the challenges, this letter proposes a dynamic phasor model of single-phase GFMCs.","Moreover, we linearize the proposed model and perform stability analysis, which confirm that the proposed model is more accurate than existing models.","Experimental results validate the improved accuracy of the proposed dynamic phasor model."],"url":"http://arxiv.org/abs/2404.11304v1","category":"eess.SY"}
{"created":"2024-04-17 12:06:17","title":"How to Exhibit More Predictable Behaviors","abstract":"This paper looks at predictability problems, i.e., wherein an agent must choose its strategy in order to optimize the predictions that an external observer could make. We address these problems while taking into account uncertainties on the environment dynamics and on the observed agent's policy. To that end, we assume that the observer 1. seeks to predict the agent's future action or state at each time step, and 2. models the agent using a stochastic policy computed from a known underlying problem, and we leverage on the framework of observer-aware Markov decision processes (OAMDPs). We propose action and state predictability performance criteria through reward functions built on the observer's belief about the agent policy; show that these induced predictable OAMDPs can be represented by goal-oriented or discounted MDPs; and analyze the properties of the proposed reward functions both theoretically and empirically on two types of grid-world problems.","sentences":["This paper looks at predictability problems, i.e., wherein an agent must choose its strategy in order to optimize the predictions that an external observer could make.","We address these problems while taking into account uncertainties on the environment dynamics and on the observed agent's policy.","To that end, we assume that the observer 1. seeks to predict the agent's future action or state at each time step, and 2. models the agent using a stochastic policy computed from a known underlying problem, and we leverage on the framework of observer-aware Markov decision processes (OAMDPs).","We propose action and state predictability performance criteria through reward functions built on the observer's belief about the agent policy; show that these induced predictable OAMDPs can be represented by goal-oriented or discounted MDPs; and analyze the properties of the proposed reward functions both theoretically and empirically on two types of grid-world problems."],"url":"http://arxiv.org/abs/2404.11296v1","category":"cs.AI"}
{"created":"2024-04-17 11:59:19","title":"Statistical convex-cocompactness for mapping class groups of non-orientable surfaces","abstract":"We show that a finite volume deformation retract $\\mathcal{T}_{\\varepsilon_t}^{-}(\\mathcal{N}_g)/\\mathrm{MCG}(\\mathcal{N}_g)$ of the moduli space $\\mathcal{M}(\\mathcal{N}_g)$ of non-orientable surfaces $\\mathcal{N}_g$ behaves like the convex core of $\\mathcal{M}(\\mathcal{N}_g)$, despite not even being quasi-convex. We then show that geodesics in the convex core leave compact regions with exponentially low probabilities, showing that the action of $\\mathrm{MCG}(\\mathcal{N}_g)$ on $\\mathcal{T}_{\\varepsilon_t}^{-}(\\mathcal{N}_g)$ is statistically convex-cocompact. Combined with results of Coulon and Yang, this shows that the growth rate of orbit points under the mapping class group action is purely exponential, pseudo-Anosov elements in mapping class groups of non-orientable surfaces are exponentially generic, and the action of mapping class group on the limit set in the horofunction boundary is ergodic with respect to the Patterson-Sullivan measure. A key step of our proof relies on complexity length, developed by Dowdall and Masur, which is an alternative notion of distance on Teichm\\\"uller space that accounts for geodesics that spend a considerable fraction of their time in the thin part.","sentences":["We show that a finite volume deformation retract $\\mathcal{T}_{\\varepsilon_t}^{-}(\\mathcal{N}_g)/\\mathrm{MCG}(\\mathcal{N}_g)$ of the moduli space $\\mathcal{M}(\\mathcal{N}_g)$ of non-orientable surfaces $\\mathcal{N}_g$ behaves like the convex core of $\\mathcal{M}(\\mathcal{N}_g)$, despite not even being quasi-convex.","We then show that geodesics in the convex core leave compact regions with exponentially low probabilities, showing that the action of $\\mathrm{MCG}(\\mathcal{N}_g)$ on $\\mathcal{T}_{\\varepsilon_t}^{-}(\\mathcal{N}_g)$ is statistically convex-cocompact.","Combined with results of Coulon and Yang, this shows that the growth rate of orbit points under the mapping class group action is purely exponential, pseudo-Anosov elements in mapping class groups of non-orientable surfaces are exponentially generic, and the action of mapping class group on the limit set in the horofunction boundary is ergodic with respect to the Patterson-Sullivan measure.","A key step of our proof relies on complexity length, developed by Dowdall and Masur, which is an alternative notion of distance on Teichm\\\"uller space that accounts for geodesics that spend a considerable fraction of their time in the thin part."],"url":"http://arxiv.org/abs/2404.11293v1","category":"math.GT"}
{"created":"2024-04-17 11:55:43","title":"Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Online Intelligent Education Systems","abstract":"Cognitive diagnosis aims to gauge students' mastery levels based on their response logs. Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing. WOIESs are open learning environment where numerous new students constantly register and complete exercises. In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning. However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training. To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. Specifically, in ICDM, we propose a novel student-centered graph (SCG). Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG. Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining. To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts. Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students.","sentences":["Cognitive diagnosis aims to gauge students' mastery levels based on their response logs.","Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing.","WOIESs are open learning environment where numerous new students constantly register and complete exercises.","In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning.","However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training.","To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs.","Specifically, in ICDM, we propose a novel student-centered graph (SCG).","Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG.","Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining.","To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts.","Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students."],"url":"http://arxiv.org/abs/2404.11290v1","category":"cs.AI"}
{"created":"2024-04-17 11:55:27","title":"Dirichlet problems associated to abstract nonlocal space-time differential operators","abstract":"Let the abstract fractional space--time operator $(\\partial_t + A)^s$ be given, where $s \\in (0,\\infty)$ and $-A \\colon \\mathsf{D}(A) \\subseteq X \\to X$ is a linear operator generating a uniformly bounded strongly measurable semigroup $(S(t))_{t\\ge0}$ on a complex Banach space $X$. We consider the corresponding Dirichlet problem of finding a function $u \\colon \\mathbb{R} \\to X$ such that $(\\partial_t + A)^s u(t) = 0$ on $(t_0, \\infty)$ and $u(t) = g(t)$ on $(-\\infty, t_0]$, for given $t_0 \\in \\mathbb{R}$ and $g \\colon (-\\infty,t_0] \\to X$. We derive a solution formula which expresses $u$ in terms of $g$ and $(S(t))_{t\\ge0}$ and generalizes the well-known variation of constants formula for the mild solution to the abstract Cauchy problem $u' + Au = 0$ on $(t_0, \\infty)$ with $u(t_0) = x \\in \\overline{\\mathsf{D}(A)}$. Moreover, we include a comparison to analogous solution concepts arising from Riemann-Liouville and Caputo type initial value problems.","sentences":["Let the abstract fractional space--time operator $(\\partial_t + A)^s$ be given, where $s \\in (0,\\infty)$ and $-A \\colon \\mathsf{D}(A)","\\subseteq X \\to X$ is a linear operator generating a uniformly bounded strongly measurable semigroup $(S(t))_{t\\ge0}$ on a complex Banach space $X$. We consider the corresponding Dirichlet problem of finding a function $u \\colon \\mathbb{R} \\to X$ such that $(\\partial_t + A)^s u(t) = 0$ on $(t_0, \\infty)$ and $u(t)","= g(t)$ on $(-\\infty, t_0]$, for given $t_0 \\in \\mathbb{R}$ and $g \\colon (-\\infty,t_0]","\\to X$. We derive a solution formula which expresses $u$ in terms of $g$ and $(S(t))_{t\\ge0}$ and generalizes the well-known variation of constants formula for the mild solution to the abstract Cauchy problem $u' +","Au = 0$ on $(t_0, \\infty)$ with $u(t_0)","= x \\in \\overline{\\mathsf{D}(A)}$.","Moreover, we include a comparison to analogous solution concepts arising from Riemann-Liouville and Caputo type initial value problems."],"url":"http://arxiv.org/abs/2404.11289v1","category":"math.AP"}
{"created":"2024-04-17 11:42:39","title":"Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks","abstract":"To reduce network traffic and support environments with limited resources, a method for transmitting images with low amounts of transmission data is required. Machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed. However, in certain situations, reconstructing a part of semantic information of images at the receiver end may be sufficient. To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication. This method transmits only the semantic information of an image, and the receiver reconstructs the image using an image-generation model. This method utilizes one type of semantic information, but reconstructing images similar to the original image using only it is challenging. This study proposes a multi-modal image transmission method that leverages diverse semantic information for efficient semantic communication. The proposed method extracts multi-modal semantic information from an image and transmits only it. Subsequently, the receiver generates multiple images using an image-generation model and selects an output based on semantic similarity. The receiver must select the output based only on the received features; however, evaluating semantic similarity using conventional metrics is challenging. Therefore, this study explored new metrics to evaluate the similarity between semantic features of images and proposes two scoring procedures. The results indicate that the proposed procedures can compare semantic similarities, such as position and composition, between semantic features of the original and generated images. Thus, the proposed method can facilitate the transmission and utilization of photographs through mobile networks for various service applications.","sentences":["To reduce network traffic and support environments with limited resources, a method for transmitting images with low amounts of transmission data is required.","Machine learning-based image compression methods, which compress the data size of images while maintaining their features, have been proposed.","However, in certain situations, reconstructing a part of semantic information of images at the receiver end may be sufficient.","To realize this concept, semantic-information-based communication, called semantic communication, has been proposed, along with an image transmission method using semantic communication.","This method transmits only the semantic information of an image, and the receiver reconstructs the image using an image-generation model.","This method utilizes one type of semantic information, but reconstructing images similar to the original image using only it is challenging.","This study proposes a multi-modal image transmission method that leverages diverse semantic information for efficient semantic communication.","The proposed method extracts multi-modal semantic information from an image and transmits only it.","Subsequently, the receiver generates multiple images using an image-generation model and selects an output based on semantic similarity.","The receiver must select the output based only on the received features; however, evaluating semantic similarity using conventional metrics is challenging.","Therefore, this study explored new metrics to evaluate the similarity between semantic features of images and proposes two scoring procedures.","The results indicate that the proposed procedures can compare semantic similarities, such as position and composition, between semantic features of the original and generated images.","Thus, the proposed method can facilitate the transmission and utilization of photographs through mobile networks for various service applications."],"url":"http://arxiv.org/abs/2404.11280v1","category":"cs.NI"}
{"created":"2024-04-17 11:33:21","title":"RD2Bench: Toward Data-Centric Automatic R&D","abstract":"The progress of humanity is driven by those successful discoveries accompanied by countless failed experiments. Researchers often seek the potential research directions by reading and then verifying them through experiments. The process imposes a significant burden on researchers. In the past decade, the data-driven black-box deep learning method demonstrates its effectiveness in a wide range of real-world scenarios, which exacerbates the experimental burden of researchers and thus renders the potential successful discoveries veiled. Therefore, automating such a research and development (R&D) process is an urgent need. In this paper, we serve as the first effort to formalize the goal by proposing a Real-world Data-centric automatic R&D Benchmark, namely RD2Bench. RD2Bench benchmarks all the operations in data-centric automatic R&D (D-CARD) as a whole to navigate future work toward our goal directly. We focuses on evaluating the interaction and synergistic effects of various model capabilities and aiding to select the well-performed trustworthy models. Although RD2Bench is very challenging to the state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating ample research opportunities and more research efforts, LLMs possess promising potential to bring more significant development to D-CARD: They are able to implement some simple methods without adopting any additional techniques. We appeal to future work to take developing techniques for tackling automatic R&D into consideration, thus bringing the opportunities of the potential revolutionary upgrade to human productivity.","sentences":["The progress of humanity is driven by those successful discoveries accompanied by countless failed experiments.","Researchers often seek the potential research directions by reading and then verifying them through experiments.","The process imposes a significant burden on researchers.","In the past decade, the data-driven black-box deep learning method demonstrates its effectiveness in a wide range of real-world scenarios, which exacerbates the experimental burden of researchers and thus renders the potential successful discoveries veiled.","Therefore, automating such a research and development (R&D) process is an urgent need.","In this paper, we serve as the first effort to formalize the goal by proposing a Real-world Data-centric automatic R&D Benchmark, namely RD2Bench.","RD2Bench benchmarks all the operations in data-centric automatic R&D (D-CARD) as a whole to navigate future work toward our goal directly.","We focuses on evaluating the interaction and synergistic effects of various model capabilities and aiding to select the well-performed trustworthy models.","Although RD2Bench is very challenging to the state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating ample research opportunities and more research efforts, LLMs possess promising potential to bring more significant development to D-CARD: They are able to implement some simple methods without adopting any additional techniques.","We appeal to future work to take developing techniques for tackling automatic R&D into consideration, thus bringing the opportunities of the potential revolutionary upgrade to human productivity."],"url":"http://arxiv.org/abs/2404.11276v1","category":"cs.AI"}
{"created":"2024-04-17 11:26:11","title":"Two-sided bell-shaped sequences","abstract":"A nonnegative real function f is bell-shaped if it converges to zero at plus and minus infinity and the nth derivative of f changes sign n times for every n = 0, 1, 2, ... Similarly, a two-sided nonnegative sequence a(k) is bell-shaped if it converges to zero at plus and minus infinity and the nth iterated difference of a(k) changes sign n times for every n = 0, 1, 2, ... A characterisation of bell-shaped functions was given by Thomas Simon and the first named author, and recently a similar result for one-sided bell-shaped sequences was found by the authors. In the present article we give a complete description of two-sided bell-shaped sequences. Our main result proves that bell-shaped sequences are convolutions of P\\'olya frequency sequences and what we call absolutely monotone-then-completely monotone sequences, and it provides an equivalent, and relatively easy to verify, condition in terms of holomorphic extensions of the generating function.","sentences":["A nonnegative real function f is bell-shaped if it converges to zero at plus and minus infinity and the nth derivative of f changes sign n times for every n = 0, 1, 2, ...","Similarly, a two-sided nonnegative sequence a(k) is bell-shaped if it converges to zero at plus and minus infinity and the nth iterated difference of a(k) changes sign n times for every n = 0, 1, 2, ...","A characterisation of bell-shaped functions was given by Thomas Simon and the first named author, and recently a similar result for one-sided bell-shaped sequences was found by the authors.","In the present article we give a complete description of two-sided bell-shaped sequences.","Our main result proves that bell-shaped sequences are convolutions of P\\'olya frequency sequences and what we call absolutely monotone-then-completely monotone sequences, and it provides an equivalent, and relatively easy to verify, condition in terms of holomorphic extensions of the generating function."],"url":"http://arxiv.org/abs/2404.11274v1","category":"math.CA"}
{"created":"2024-04-17 11:20:14","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series","abstract":"Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models. Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset. Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains. In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning. DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains. This method significantly broadens the model's adaptability and robustness. Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features. Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain. Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness. The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection.","sentences":["Time series anomaly detection (TAD) faces a significant challenge due to the scarcity of labelled data, which hinders the development of accurate detection models.","Unsupervised domain adaptation (UDA) addresses this challenge by leveraging a labelled dataset from a related domain to detect anomalies in a target dataset.","Existing domain adaptation techniques assume that the number of anomalous classes does not change between the source and target domains.","In this paper, we propose a novel Domain Adaptation Contrastive learning for Anomaly Detection in multivariate time series (DACAD) model to address this issue by combining UDA and contrastive representation learning.","DACAD's approach includes an anomaly injection mechanism that introduces various types of synthetic anomalies, enhancing the model's ability to generalise across unseen anomalous classes in different domains.","This method significantly broadens the model's adaptability and robustness.","Additionally, we propose a supervised contrastive loss for the source domain and a self-supervised contrastive triplet loss for the target domain, improving comprehensive feature representation learning and extraction of domain-invariant features.","Finally, an effective Centre-based Entropy Classifier (CEC) is proposed specifically for anomaly detection, facilitating accurate learning of normal boundaries in the source domain.","Our extensive evaluation across multiple real-world datasets against leading models in time series anomaly detection and UDA underscores DACAD's effectiveness.","The results validate DACAD's superiority in transferring knowledge across domains and its potential to mitigate the challenge of limited labelled data in time series anomaly detection."],"url":"http://arxiv.org/abs/2404.11269v1","category":"cs.LG"}
{"created":"2024-04-17 11:20:13","title":"The maximum number of cliques in graphs with given fractional matching number and minimum degree","abstract":"Recently, Ma, Qian and Shi determined the maximum size of an $n$-vertex graph with given fractional matching number $s$ and maximum degree at most $d$. Motivated by this result, we determine the maximum number of $\\ell$-cliques in a graph with given fractional matching number and minimum degree, which generalizes Shi and Ma's result about the maximum size of a graph with given fractional matching number and minimum degree at least one. We also determine the maximum number of complete bipartite graphs in a graph with prescribed fractional matching number and minimum degree.","sentences":["Recently, Ma, Qian and Shi determined the maximum size of an $n$-vertex graph with given fractional matching number $s$ and maximum degree at most $d$. Motivated by this result, we determine the maximum number of $\\ell$-cliques in a graph with given fractional matching number and minimum degree, which generalizes Shi and Ma's result about the maximum size of a graph with given fractional matching number and minimum degree at least one.","We also determine the maximum number of complete bipartite graphs in a graph with prescribed fractional matching number and minimum degree."],"url":"http://arxiv.org/abs/2404.11268v1","category":"math.CO"}
{"created":"2024-04-17 11:12:59","title":"Sampling-based Pseudo-Likelihood for Membership Inference Attacks","abstract":"Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text. This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data. Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention. Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs. However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user. In this study, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for MIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks. The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data. Even without likelihoods, SaMIA performed on par with existing likelihood-based methods.","sentences":["Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text.","This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data.","Membership Inference Attacks (MIA), which determine whether a given text is included in the model's training data, have been attracting attention.","Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs.","However, the existing methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood is unavailable to the user.","In this study, we propose a Sampling-based Pseudo-Likelihood (\\textbf{SPL}) method for MIA (\\textbf{SaMIA}) that calculates SPL using only the text generated by an LLM to detect leaks.","The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of $n$-gram match as SPL, and determines the membership of the text in the training data.","Even without likelihoods, SaMIA performed on par with existing likelihood-based methods."],"url":"http://arxiv.org/abs/2404.11262v1","category":"cs.CL"}
{"created":"2024-04-17 10:51:36","title":"Learning Social Navigation from Demonstrations with Deep Neural Networks","abstract":"Traditional path-planning techniques treat humans as obstacles. This has changed since robots started to enter human environments. On modern robots, social navigation has become an important aspect of navigation systems. To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required. In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot. Two distinct deep models are trained with respective objectives: one for global planning and one for local planning. These models are then employed in the simulated robot. In the end, it has been shown that our model can successfully carry out both global and local planning tasks. We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks.","sentences":["Traditional path-planning techniques treat humans as obstacles.","This has changed since robots started to enter human environments.","On modern robots, social navigation has become an important aspect of navigation systems.","To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required.","In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot.","Two distinct deep models are trained with respective objectives: one for global planning and one for local planning.","These models are then employed in the simulated robot.","In the end, it has been shown that our model can successfully carry out both global and local planning tasks.","We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks."],"url":"http://arxiv.org/abs/2404.11246v1","category":"cs.RO"}
{"created":"2024-04-17 10:49:59","title":"Constraining the modified friction in gravitational wave propagation with precessing black hole binaries","abstract":"A broad class of modified gravities can result in a modified friction effect in the propagation of gravitational waves (GWs). This effect changes the amplitude-damping rate of GWs during their propagation in the cosmological distance and thus modifies the standard luminosity distance of GWs in general relativity. Therefore, one can constrain this modified friction by measuring both the luminosity distance and redshift of the GW sources. In this paper, we investigate the prospects of constraining such modified friction effect by using the precessing binary black holes with ground-based GW detectors. For this purpose, we consider 20 precessing events detected by the GW detector network consisting of two LIGO detectors and two third-generation GW detectors (the Einstein Telescope and the Cosmic Explorer). The redshift information of these events is obtained by identifying their possible host galaxies in the GLADE+ galaxy catalog. We show that the precession in the binary system can improve significantly the precision of luminosity distance and thus lead to a tighter constraint on the modified friction. By assuming narrow priors on cosmological parameters that are consistent with the uncertainties of Planck 2018 results, our analysis shows that the modified friction effect, characterized by two parameters $(\\Xi_0, n)$, can be constrained to be $\\Xi_0 = 1.002^{+0.004}_{-0.004}$ and $n=3.257^{+2.595}_{-2.192}$, in which the result of $\\Xi_0$ is about two orders of magnitude better than current result from an analysis with GWTC-3. Our result sets the stage for future research with third-generation GW detectors, offering new insights into gravitational parameter modifications. It also contributes to the understanding of the properties and applications of binary black hole systems with precession.","sentences":["A broad class of modified gravities can result in a modified friction effect in the propagation of gravitational waves (GWs).","This effect changes the amplitude-damping rate of GWs during their propagation in the cosmological distance and thus modifies the standard luminosity distance of GWs in general relativity.","Therefore, one can constrain this modified friction by measuring both the luminosity distance and redshift of the GW sources.","In this paper, we investigate the prospects of constraining such modified friction effect by using the precessing binary black holes with ground-based GW detectors.","For this purpose, we consider 20 precessing events detected by the GW detector network consisting of two LIGO detectors and two third-generation GW detectors (the Einstein Telescope and the Cosmic Explorer).","The redshift information of these events is obtained by identifying their possible host galaxies in the GLADE+ galaxy catalog.","We show that the precession in the binary system can improve significantly the precision of luminosity distance and thus lead to a tighter constraint on the modified friction.","By assuming narrow priors on cosmological parameters that are consistent with the uncertainties of Planck 2018 results, our analysis shows that the modified friction effect, characterized by two parameters $(\\Xi_0, n)$, can be constrained to be $\\Xi_0 = 1.002^{+0.004}_{-0.004}$ and $n=3.257^{+2.595}_{-2.192}$, in which the result of $\\Xi_0$ is about two orders of magnitude better than current result from an analysis with GWTC-3.","Our result sets the stage for future research with third-generation GW detectors, offering new insights into gravitational parameter modifications.","It also contributes to the understanding of the properties and applications of binary black hole systems with precession."],"url":"http://arxiv.org/abs/2404.11245v1","category":"gr-qc"}
{"created":"2024-04-17 10:49:44","title":"A prelude to the multibeam photoacoustics","abstract":"The report focuses on the introduction and validation of photoacoustics measurements in the presence of two simultaneous probing beams. The experiments, performed for three model systems: a perfect absorber - carbon black membrane, a multilayer system - acrylic paint-covered copper plate, and luminescent sample - ruby, were aimed for comparative analyses of frequency domain photoacoustic responses to a single probing beam (classical approach) and two probing beams (of different wavelengths). A modulation frequency shift criterion, allowing for the simultaneous acquisition of two independent signals, was recognized. Any synergistic or cross effects of sample dual beam excitation on the photoacoustic signal generation (impacts of the beams intensities or modulation frequency effects) were not observed. The multibeam photoacoustics data appears to be (at least) equivalent to classically acquired results for stationary systems. It is considered, that the multibeam approach will broaden the applicability of the photothermal methods to the time-dependent systems, where the evolutions of distinct absorption bands is of special interest.","sentences":["The report focuses on the introduction and validation of photoacoustics measurements in the presence of two simultaneous probing beams.","The experiments, performed for three model systems: a perfect absorber - carbon black membrane, a multilayer system - acrylic paint-covered copper plate, and luminescent sample - ruby, were aimed for comparative analyses of frequency domain photoacoustic responses to a single probing beam (classical approach) and two probing beams (of different wavelengths).","A modulation frequency shift criterion, allowing for the simultaneous acquisition of two independent signals, was recognized.","Any synergistic or cross effects of sample dual beam excitation on the photoacoustic signal generation (impacts of the beams intensities or modulation frequency effects) were not observed.","The multibeam photoacoustics data appears to be (at least) equivalent to classically acquired results for stationary systems.","It is considered, that the multibeam approach will broaden the applicability of the photothermal methods to the time-dependent systems, where the evolutions of distinct absorption bands is of special interest."],"url":"http://arxiv.org/abs/2404.11244v1","category":"physics.app-ph"}
{"created":"2024-04-17 10:49:00","title":"Optical Image-to-Image Translation Using Denoising Diffusion Models: Heterogeneous Change Detection as a Use Case","abstract":"We introduce an innovative deep learning-based method that uses a denoising diffusion-based model to translate low-resolution images to high-resolution ones from different optical sensors while preserving the contents and avoiding undesired artifacts. The proposed method is trained and tested on a large and diverse data set of paired Sentinel-II and Planet Dove images. We show that it can solve serious image generation issues observed when the popular classifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is used in the task of Image-to-Image Translation of multi-sensor optical remote sensing images and that it can generate large images with highly consistent patches, both in colors and in features. Moreover, we demonstrate how our method improves heterogeneous change detection results in two urban areas: Beirut, Lebanon, and Austin, USA. Our contributions are: i) a new training and testing algorithm based on denoising diffusion models for optical image translation; ii) a comprehensive image quality evaluation and ablation study; iii) a comparison with the classifier-free guided DDIM framework; and iv) change detection experiments on heterogeneous data.","sentences":["We introduce an innovative deep learning-based method that uses a denoising diffusion-based model to translate low-resolution images to high-resolution ones from different optical sensors while preserving the contents and avoiding undesired artifacts.","The proposed method is trained and tested on a large and diverse data set of paired Sentinel-II and Planet Dove images.","We show that it can solve serious image generation issues observed when the popular classifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is used in the task of Image-to-Image Translation of multi-sensor optical remote sensing images and that it can generate large images with highly consistent patches, both in colors and in features.","Moreover, we demonstrate how our method improves heterogeneous change detection results in two urban areas: Beirut, Lebanon, and Austin, USA.","Our contributions are: i) a new training and testing algorithm based on denoising diffusion models for optical image translation; ii) a comprehensive image quality evaluation and ablation study; iii) a comparison with the classifier-free guided DDIM framework; and iv) change detection experiments on heterogeneous data."],"url":"http://arxiv.org/abs/2404.11243v1","category":"cs.CV"}
{"created":"2024-04-17 10:44:39","title":"Two-generation of traceless matrices over finite fields","abstract":"We prove that the Lie algebra $\\mathfrak{sl}_n(\\textbf{F}_q)$ of traceless matrices over a finite field of odd characteristic $p$ can be generated by $2$ elements with a single exception $(n, p)= (3, 3)$. We establish a curious trace identity in $\\mathfrak{sl}_3(\\textbf{F})$ over any field $\\textbf{F}$ of characteristic $3$ that obstructs $2$-generation.","sentences":["We prove that the Lie algebra $\\mathfrak{sl}_n(\\textbf{F}_q)$ of traceless matrices over a finite field of odd characteristic $p$ can be generated by $2$ elements with a single exception $(n, p)= (3, 3)$. We establish a curious trace identity in $\\mathfrak{sl}_3(\\textbf{F})$ over any field $\\textbf{F}$ of characteristic $3$ that obstructs $2$-generation."],"url":"http://arxiv.org/abs/2404.11240v1","category":"math.RA"}
{"created":"2024-04-17 10:40:12","title":"Runtime Analysis of a Multi-Valued Compact Genetic Algorithm on Generalized OneMax","abstract":"A class of metaheuristic techniques called estimation-of-distribution algorithms (EDAs) are employed in optimization as more sophisticated substitutes for traditional strategies like evolutionary algorithms. EDAs generally drive the search for the optimum by creating explicit probabilistic models of potential candidate solutions through repeated sampling and selection from the underlying search space.   Most theoretical research on EDAs has focused on pseudo-Boolean optimization. Jedidia et al. (GECCO 2023) proposed the first EDAs for optimizing problems involving multi-valued decision variables. By building a framework, they have analyzed the runtime of a multi-valued UMDA on the r-valued LeadingOnes function. Using their framework, here we focus on the multi-valued compact genetic algorithm (r-cGA) and provide a first runtime analysis of a generalized OneMax function.   To prove our results, we investigate the effect of genetic drift and progress of the probabilistic model towards the optimum. After finding the right algorithm parameters, we prove that the r-cGA solves this r-valued OneMax problem efficiently. We show that with high probability, the runtime bound is O(r2 n log2 r log3 n). At the end of experiments, we state one conjecture related to the expected runtime of another variant of multi-valued OneMax function.","sentences":["A class of metaheuristic techniques called estimation-of-distribution algorithms (EDAs) are employed in optimization as more sophisticated substitutes for traditional strategies like evolutionary algorithms.","EDAs generally drive the search for the optimum by creating explicit probabilistic models of potential candidate solutions through repeated sampling and selection from the underlying search space.   ","Most theoretical research on EDAs has focused on pseudo-Boolean optimization.","Jedidia et al. (GECCO 2023) proposed the first EDAs for optimizing problems involving multi-valued decision variables.","By building a framework, they have analyzed the runtime of a multi-valued UMDA on the r-valued LeadingOnes function.","Using their framework, here we focus on the multi-valued compact genetic algorithm (r-cGA) and provide a first runtime analysis of a generalized OneMax function.   ","To prove our results, we investigate the effect of genetic drift and progress of the probabilistic model towards the optimum.","After finding the right algorithm parameters, we prove that the r-cGA solves this r-valued OneMax problem efficiently.","We show that with high probability, the runtime bound is O(r2 n log2 r log3 n).","At the end of experiments, we state one conjecture related to the expected runtime of another variant of multi-valued OneMax function."],"url":"http://arxiv.org/abs/2404.11239v1","category":"cs.NE"}
{"created":"2024-04-17 10:39:35","title":"Physics of nova outbursts: Theoretical models of classical nova outbursts with optically thick winds on $1.2~M_\\odot$ and $1.3~M_\\odot$ white dwarfs","abstract":"We present time-dependent nova outburst models with optically thick winds for a 1.2 and 1.35 $M_\\odot$ white dwarfs (WDs) with a mass accretion rate of $5 \\times 10^{-9}~M_\\odot$ yr$^{-1}$ and for a 1.3 $M_\\odot$ WD with $2 \\times 10^{-9}~M_\\odot$ yr$^{-1}$. The X-ray flash occurs 11 days before the optical peak of the 1.2 $M_\\odot$ WD and 2.5 days before the peak of the 1.3 $M_\\odot$ WD. The wind mass loss rate of the 1.2 $M_\\odot$ WD (1.3 $M_\\odot$ WD) reaches a peak of $6.4 \\times 10^{-5}~M_\\odot$ yr$^{-1}$ ($7.4 \\times 10^{-5}~M_\\odot$ yr$^{-1}$) at the epoch of the maximum photospheric expansion with the lowest photospheric temperature of $\\log T_{\\rm ph}$ (K)=4.33 (4.35). The nuclear energy generated during the outburst is lost in a form of radiation (61% for the 1.2 $M_\\odot$ WD; 47% for the 1.3 $M_\\odot$ WD), gravitational energy of ejecta (39%; 52%), and kinetic energy of the wind (0.28%; 0.29%). We found an empirical relation for fast novae between the time to optical maximum from the outburst $t_{\\rm peak}$ and the expansion timescale $\\tau_{\\rm exp}$ at $t=0$. With this relation, we are able to predict the time to optical maximum $t_{\\rm peak}$ from the ignition model (at $t=0$) without following a time-consuming nova wind evolution.","sentences":["We present time-dependent nova outburst models with optically thick winds for a 1.2 and 1.35 $M_\\odot$ white dwarfs (WDs) with a mass accretion rate of $5 \\times 10^{-9}~M_\\odot$ yr$^{-1}$","and for a 1.3 $M_\\odot$ WD with $2 \\times 10^{-9}~M_\\odot$ yr$^{-1}$. The X-ray flash occurs 11 days before the optical peak of the 1.2 $M_\\odot$ WD and 2.5 days before the peak of the 1.3 $M_\\odot$ WD.","The wind mass loss rate of the 1.2 $M_\\odot$ WD (1.3 $M_\\odot$ WD) reaches a peak of $6.4 \\times 10^{-5}~M_\\odot$ yr$^{-1}$","($7.4 \\times 10^{-5}~M_\\odot$ yr$^{-1}$) at the epoch of the maximum photospheric expansion with the lowest photospheric temperature of $\\log T_{\\rm ph}$ (K)=4.33 (4.35).","The nuclear energy generated during the outburst is lost in a form of radiation (61% for the 1.2 $M_\\odot$ WD; 47% for the 1.3 $M_\\odot$ WD), gravitational energy of ejecta (39%; 52%), and kinetic energy of the wind (0.28%; 0.29%).","We found an empirical relation for fast novae between the time to optical maximum from the outburst $t_{\\rm peak}$ and the expansion timescale $\\tau_{\\rm exp}$ at $t=0$. With this relation, we are able to predict the time to optical maximum $t_{\\rm peak}$ from the ignition model (at $t=0$) without following a time-consuming nova wind evolution."],"url":"http://arxiv.org/abs/2404.11237v1","category":"astro-ph.SR"}
{"created":"2024-04-17 10:38:51","title":"ONOT: a High-Quality ICAO-compliant Synthetic Mugshot Dataset","abstract":"Nowadays, state-of-the-art AI-based generative models represent a viable solution to overcome privacy issues and biases in the collection of datasets containing personal information, such as faces. Following this intuition, in this paper we introduce ONOT, a synthetic dataset specifically focused on the generation of high-quality faces in adherence to the requirements of the ISO/IEC 39794-5 standards that, following the guidelines of the International Civil Aviation Organization (ICAO), defines the interchange formats of face images in electronic Machine-Readable Travel Documents (eMRTD). The strictly controlled and varied mugshot images included in ONOT are useful in research fields related to the analysis of face images in eMRTD, such as Morphing Attack Detection and Face Quality Assessment. The dataset is publicly released, in combination with the generation procedure details in order to improve the reproducibility and enable future extensions.","sentences":["Nowadays, state-of-the-art AI-based generative models represent a viable solution to overcome privacy issues and biases in the collection of datasets containing personal information, such as faces.","Following this intuition, in this paper we introduce ONOT, a synthetic dataset specifically focused on the generation of high-quality faces in adherence to the requirements of the ISO/IEC 39794-5 standards that, following the guidelines of the International Civil Aviation Organization (ICAO), defines the interchange formats of face images in electronic Machine-Readable Travel Documents (eMRTD).","The strictly controlled and varied mugshot images included in ONOT are useful in research fields related to the analysis of face images in eMRTD, such as Morphing Attack Detection and Face Quality Assessment.","The dataset is publicly released, in combination with the generation procedure details in order to improve the reproducibility and enable future extensions."],"url":"http://arxiv.org/abs/2404.11236v1","category":"cs.CV"}
{"created":"2024-04-17 10:37:52","title":"Bayesian Markov-Switching Vector Autoregressive Process","abstract":"This study introduces marginal density functions of the general Bayesian Markov-Switching Vector Autoregressive (MS-VAR) process. In the case of the Bayesian MS-VAR process, we provide closed--form density functions and Monte-Carlo simulation algorithms, including the importance sampling method. The Monte--Carlo simulation method departs from the previous simulation methods because it removes the duplication in a regime vector.","sentences":["This study introduces marginal density functions of the general Bayesian Markov-Switching Vector Autoregressive (MS-VAR) process.","In the case of the Bayesian MS-VAR process, we provide closed--form density functions and Monte-Carlo simulation algorithms, including the importance sampling method.","The Monte--Carlo simulation method departs from the previous simulation methods because it removes the duplication in a regime vector."],"url":"http://arxiv.org/abs/2404.11235v1","category":"econ.EM"}
{"created":"2024-04-17 10:31:03","title":"Binary forms with the same value set I","abstract":"Given a binary form $F \\in \\mathbb{Z}[X, Y]$, we define its value set to be $\\{F(x, y) : (x, y) \\in \\mathbb{Z}^2\\}$. Let $F, G \\in \\mathbb{Z}[X, Y]$ be two binary forms of degree $d \\geq 3$ and with non-zero discriminant. In a series of three papers, we will give necessary and sufficient conditions on $F$ and $G$ to have the same value set. These conditions will be entirely in terms of the automorphism groups of the forms.   In this paper, we will build the general theory that reduces the problem to a question about lattice coverings of $\\mathbb{Z}^2$, and we solve this problem when $F$ and $G$ have a small automorphism group. The larger automorphism groups $D_4$ and $D_3, D_6$ will respectively be treated in part II and part III.","sentences":["Given a binary form $F \\in \\mathbb{Z}[X, Y]$, we define its value set to be $\\{F(x, y) : (x, y) \\in \\mathbb{Z}^2\\}$. Let $F, G \\in \\mathbb{Z}[X, Y]$ be two binary forms of degree $d \\geq 3$ and with non-zero discriminant.","In a series of three papers, we will give necessary and sufficient conditions on $F$ and $G$ to have the same value set.","These conditions will be entirely in terms of the automorphism groups of the forms.   ","In this paper, we will build the general theory that reduces the problem to a question about lattice coverings of $\\mathbb{Z}^2$, and we solve this problem when $F$ and $G$ have a small automorphism group.","The larger automorphism groups $D_4$ and $D_3, D_6$ will respectively be treated in part II and part III."],"url":"http://arxiv.org/abs/2404.11231v1","category":"math.NT"}
{"created":"2024-04-17 10:26:49","title":"Energy-Efficient Uncertainty-Aware Biomass Composition Prediction at the Edge","abstract":"Clover fixates nitrogen from the atmosphere to the ground, making grass-clover mixtures highly desirable to reduce external nitrogen fertilization. Herbage containing clover additionally promotes higher food intake, resulting in higher milk production. Herbage probing however remains largely unused as it requires a time-intensive manual laboratory analysis. Without this information, farmers are unable to perform localized clover sowing or take targeted fertilization decisions. Deep learning algorithms have been proposed with the goal to estimate the dry biomass composition from images of the grass directly in the fields. The energy-intensive nature of deep learning however limits deployment to practical edge devices such as smartphones. This paper proposes to fill this gap by applying filter pruning to reduce the energy requirement of existing deep learning solutions. We report that although pruned networks are accurate on controlled, high-quality images of the grass, they struggle to generalize to real-world smartphone images that are blurry or taken from challenging angles. We address this challenge by training filter-pruned models using a variance attenuation loss so they can predict the uncertainty of their predictions. When the uncertainty exceeds a threshold, we re-infer using a more accurate unpruned model. This hybrid approach allows us to reduce energy consumption while retaining a high accuracy. We evaluate our algorithm on two datasets: the GrassClover and the Irish clover using an NVIDIA Jetson Nano edge device. We find that we reduce energy reduction with respect to state-of-the-art solutions by 50% on average with only 4% accuracy loss.","sentences":["Clover fixates nitrogen from the atmosphere to the ground, making grass-clover mixtures highly desirable to reduce external nitrogen fertilization.","Herbage containing clover additionally promotes higher food intake, resulting in higher milk production.","Herbage probing however remains largely unused as it requires a time-intensive manual laboratory analysis.","Without this information, farmers are unable to perform localized clover sowing or take targeted fertilization decisions.","Deep learning algorithms have been proposed with the goal to estimate the dry biomass composition from images of the grass directly in the fields.","The energy-intensive nature of deep learning however limits deployment to practical edge devices such as smartphones.","This paper proposes to fill this gap by applying filter pruning to reduce the energy requirement of existing deep learning solutions.","We report that although pruned networks are accurate on controlled, high-quality images of the grass, they struggle to generalize to real-world smartphone images that are blurry or taken from challenging angles.","We address this challenge by training filter-pruned models using a variance attenuation loss so they can predict the uncertainty of their predictions.","When the uncertainty exceeds a threshold, we re-infer using a more accurate unpruned model.","This hybrid approach allows us to reduce energy consumption while retaining a high accuracy.","We evaluate our algorithm on two datasets: the GrassClover and the Irish clover using an NVIDIA Jetson Nano edge device.","We find that we reduce energy reduction with respect to state-of-the-art solutions by 50% on average with only 4% accuracy loss."],"url":"http://arxiv.org/abs/2404.11230v1","category":"cs.CV"}
{"created":"2024-04-17 10:20:42","title":"Unlocking Memories with AI: Exploring the Role of AI-Generated Cues in Personal Reminiscing","abstract":"While technology-mediated reminiscing has been studied for decades, generating relevant cues to trigger personal reminiscing remains challenging. The potential of AI in generating relevant content across various domains has been recently recognized, yet its use in facilitating reminiscing is still less explored. This work aims to explore the use of AI in supporting the recall of personal memories associated with significant objects at home. We designed Treasurefinder, a device powered by a large language model (LLM) that generates open-ended questions based on stories stored in NFC-tagged physical objects or cards. We conducted an exploratory study with 12 participants, grouped in pairs, to observe reminiscing behaviors when using Treasurefinder. The results showed the AI-generated questions 1) supported individuals to recall the past, 2) provided new insights about the other person, and 3) encouraged reflection. Notably, the device facilitated active memory retrieval related to cherished objects that are often overlooked.","sentences":["While technology-mediated reminiscing has been studied for decades, generating relevant cues to trigger personal reminiscing remains challenging.","The potential of AI in generating relevant content across various domains has been recently recognized, yet its use in facilitating reminiscing is still less explored.","This work aims to explore the use of AI in supporting the recall of personal memories associated with significant objects at home.","We designed Treasurefinder, a device powered by a large language model (LLM) that generates open-ended questions based on stories stored in NFC-tagged physical objects or cards.","We conducted an exploratory study with 12 participants, grouped in pairs, to observe reminiscing behaviors when using Treasurefinder.","The results showed the AI-generated questions 1) supported individuals to recall the past, 2) provided new insights about the other person, and 3) encouraged reflection.","Notably, the device facilitated active memory retrieval related to cherished objects that are often overlooked."],"url":"http://arxiv.org/abs/2404.11227v1","category":"cs.HC"}
{"created":"2024-04-17 10:19:15","title":"In-Context Learning State Vector with Inner and Momentum Optimization","abstract":"Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector","sentences":["Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples.","Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer.","However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored.","In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector.","Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation.","Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge.","We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting.","The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks.","Code is available at https://github.com/HITsz-TMG/ICL-State-Vector"],"url":"http://arxiv.org/abs/2404.11225v1","category":"cs.CL"}
{"created":"2024-04-17 10:13:44","title":"AndroLog: Android Instrumentation and Code Coverage Analysis","abstract":"Dynamic analysis has emerged as a pivotal technique for testing Android apps, enabling the detection of bugs, malicious code, and vulnerabilities. A key metric in evaluating the efficacy of tools employed by both research and practitioner communities for this purpose is code coverage. Obtaining code coverage typically requires planting probes within apps to gather coverage data during runtime. Due to the general unavailability of source code to analysts, there is a necessity for instrumenting apps to insert these probes in black-box environments. However, the tools available for such instrumentation are limited in their reliability and require intrusive changes interfering with apps' functionalities.   This paper introduces AndroLog a novel tool developed on top of the Soot framework, designed to provide fine-grained coverage information at multiple levels, including class, methods, statements, and Android components. In contrast to existing tools, AndroLog leaves the responsibility to test apps to analysts, and its motto is simplicity. As demonstrated in this paper, AndroLog can instrument up to 98% of recent Android apps compared to existing tools with 79% and 48% respectively for COSMO and ACVTool. AndroLog also stands out for its potential for future enhancements to increase granularity on demand. We make AndroLog available to the community and provide a video demonstration of AndroLog (see section 8).","sentences":["Dynamic analysis has emerged as a pivotal technique for testing Android apps, enabling the detection of bugs, malicious code, and vulnerabilities.","A key metric in evaluating the efficacy of tools employed by both research and practitioner communities for this purpose is code coverage.","Obtaining code coverage typically requires planting probes within apps to gather coverage data during runtime.","Due to the general unavailability of source code to analysts, there is a necessity for instrumenting apps to insert these probes in black-box environments.","However, the tools available for such instrumentation are limited in their reliability and require intrusive changes interfering with apps' functionalities.   ","This paper introduces AndroLog a novel tool developed on top of the Soot framework, designed to provide fine-grained coverage information at multiple levels, including class, methods, statements, and Android components.","In contrast to existing tools, AndroLog leaves the responsibility to test apps to analysts, and its motto is simplicity.","As demonstrated in this paper, AndroLog can instrument up to 98% of recent Android apps compared to existing tools with 79% and 48% respectively for COSMO and ACVTool.","AndroLog also stands out for its potential for future enhancements to increase granularity on demand.","We make AndroLog available to the community and provide a video demonstration of AndroLog (see section 8)."],"url":"http://arxiv.org/abs/2404.11223v1","category":"cs.SE"}
{"created":"2024-04-17 10:11:01","title":"On the algebra of equal-input matrices in time-inhomogeneous Markov flows","abstract":"Markov matrices of equal-input type constitute a widely used model class. The corresponding equal-input generators span an interesting subalgebra of the real matrices with zero row sums. Here, we summarise some of their amazing properties and discuss the corresponding Markov embedding problem, both homogeneous and inhomogeneous in time. In particular, we derive exact and explicit solutions for time-inhomogeneous Markov flows with non-commuting generator families of equal-input type and beyond.","sentences":["Markov matrices of equal-input type constitute a widely used model class.","The corresponding equal-input generators span an interesting subalgebra of the real matrices with zero row sums.","Here, we summarise some of their amazing properties and discuss the corresponding Markov embedding problem, both homogeneous and inhomogeneous in time.","In particular, we derive exact and explicit solutions for time-inhomogeneous Markov flows with non-commuting generator families of equal-input type and beyond."],"url":"http://arxiv.org/abs/2404.11222v1","category":"math.PR"}
{"created":"2024-04-17 10:09:47","title":"Generalized Aubry-Andre-Harper Models in Optical Superlattices","abstract":"Ultracold atoms trapped in optical superlattices provide a simple platform for realizing the seminal Aubry-Andr\\'{e}-Harper (AAH) model. However, the periodic modulations on the nearest-neighbour hoppings have been ignored in this model. In this paper, we find that optical superlattice system actually can be approximately described by a generalized AAH model in the case of $V_1\\gg V_2$, with periodic modulations on both on-site energies and nearest-neighbour hoppings, supporting much richer topological properties that are absent in the standard AAH model. Specifically, by calculating Chern numbers and topological edge states, we show that the generalized AAH model possesses multifarious topological phases and topological phase transitions, as compared to the standard AAH model only supporting a single topological phase. Our findings can open up more opportunities for using optical superlattices to study topological and localization physics.","sentences":["Ultracold atoms trapped in optical superlattices provide a simple platform for realizing the seminal Aubry-Andr\\'{e}-Harper (AAH) model.","However, the periodic modulations on the nearest-neighbour hoppings have been ignored in this model.","In this paper, we find that optical superlattice system actually can be approximately described by a generalized AAH model in the case of $V_1\\gg V_2$, with periodic modulations on both on-site energies and nearest-neighbour hoppings, supporting much richer topological properties that are absent in the standard AAH model.","Specifically, by calculating Chern numbers and topological edge states, we show that the generalized AAH model possesses multifarious topological phases and topological phase transitions, as compared to the standard AAH model only supporting a single topological phase.","Our findings can open up more opportunities for using optical superlattices to study topological and localization physics."],"url":"http://arxiv.org/abs/2404.11220v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-17 10:01:44","title":"Quasibound and quasinormal modes of a thick brane in Rastall gravity","abstract":"In this work, we study the gravitational quasinormal modes of the thick brane in Rastall gravity. Using the asymptotic iteration and direct integration methods, we solve the quasinormal frequencies of the Rastall thick brane. We also obtained the waveforms of these quasinormal modes through numerical evolution. The results indicate that although the Rastall thick brane lacks a bound zero mode, when the Rastall parameter $\\lambda\\gtrsim0$, a long-lived quasinormal mode appears. This long-lived quasinormal mode may restore the four-dimensional effective Newtonian potential on the brane on a large scale. This may provide a new perspective for the localization of gravity on thick branes, that a thick brane does not necessarily require the gravity to be localized, perhaps quasi-localized is sufficient.","sentences":["In this work, we study the gravitational quasinormal modes of the thick brane in Rastall gravity.","Using the asymptotic iteration and direct integration methods, we solve the quasinormal frequencies of the Rastall thick brane.","We also obtained the waveforms of these quasinormal modes through numerical evolution.","The results indicate that although the Rastall thick brane lacks a bound zero mode, when the Rastall parameter $\\lambda\\gtrsim0$, a long-lived quasinormal mode appears.","This long-lived quasinormal mode may restore the four-dimensional effective Newtonian potential on the brane on a large scale.","This may provide a new perspective for the localization of gravity on thick branes, that a thick brane does not necessarily require the gravity to be localized, perhaps quasi-localized is sufficient."],"url":"http://arxiv.org/abs/2404.11217v1","category":"gr-qc"}
{"created":"2024-04-17 10:00:56","title":"Position Engineering: Boosting Large Language Models through Positional Information Manipulation","abstract":"The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.","sentences":["The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided.","In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance.","In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models.","Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself.","We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL).","Our findings show that position engineering substantially improves upon the baseline in both cases.","Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models."],"url":"http://arxiv.org/abs/2404.11216v1","category":"cs.CL"}
{"created":"2024-04-17 09:58:53","title":"Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions","abstract":"A significant challenge in the field of object detection lies in the system's performance under non-ideal imaging conditions, such as rain, fog, low illumination, or raw Bayer images that lack ISP processing. Our study introduces \"Feature Corrective Transfer Learning\", a novel approach that leverages transfer learning and a bespoke loss function to facilitate the end-to-end detection of objects in these challenging scenarios without the need to convert non-ideal images into their RGB counterparts. In our methodology, we initially train a comprehensive model on a pristine RGB image dataset. Subsequently, non-ideal images are processed by comparing their feature maps against those from the initial ideal RGB model. This comparison employs the Extended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function designed to quantify similarities and integrate them into the detection loss. This approach refines the model's ability to perform object detection across varying conditions through direct feature map correction, encapsulating the essence of Feature Corrective Transfer Learning. Experimental validation on variants of the KITTI dataset demonstrates a significant improvement in mean Average Precision (mAP), resulting in a 3.8-8.1% relative enhancement in detection under non-ideal conditions compared to the baseline model, and a less marginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved under ideal conditions by the standard Faster RCNN algorithm.","sentences":["A significant challenge in the field of object detection lies in the system's performance under non-ideal imaging conditions, such as rain, fog, low illumination, or raw Bayer images that lack ISP processing.","Our study introduces \"Feature Corrective Transfer Learning\", a novel approach that leverages transfer learning and a bespoke loss function to facilitate the end-to-end detection of objects in these challenging scenarios without the need to convert non-ideal images into their RGB counterparts.","In our methodology, we initially train a comprehensive model on a pristine RGB image dataset.","Subsequently, non-ideal images are processed by comparing their feature maps against those from the initial ideal RGB model.","This comparison employs the Extended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function designed to quantify similarities and integrate them into the detection loss.","This approach refines the model's ability to perform object detection across varying conditions through direct feature map correction, encapsulating the essence of Feature Corrective Transfer Learning.","Experimental validation on variants of the KITTI dataset demonstrates a significant improvement in mean Average Precision (mAP), resulting in a 3.8-8.1% relative enhancement in detection under non-ideal conditions compared to the baseline model, and a less marginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved under ideal conditions by the standard Faster RCNN algorithm."],"url":"http://arxiv.org/abs/2404.11214v1","category":"cs.CV"}
{"created":"2024-04-17 09:57:40","title":"Revisiting Noise Resilience Strategies in Gesture Recognition: Short-Term Enhancement in Surface Electromyographic Signal Analysis","abstract":"Gesture recognition based on surface electromyography (sEMG) has been gaining importance in many 3D Interactive Scenes. However, sEMG is easily influenced by various forms of noise in real-world environments, leading to challenges in providing long-term stable interactions through sEMG. Existing methods often struggle to enhance model noise resilience through various predefined data augmentation techniques. In this work, we revisit the problem from a short term enhancement perspective to improve precision and robustness against various common noisy scenarios with learnable denoise using sEMG intrinsic pattern information and sliding-window attention. We propose a Short Term Enhancement Module(STEM) which can be easily integrated with various models. STEM offers several benefits: 1) Learnable denoise, enabling noise reduction without manual data augmentation; 2) Scalability, adaptable to various models; and 3) Cost-effectiveness, achieving short-term enhancement through minimal weight-sharing in an efficient attention mechanism. In particular, we incorporate STEM into a transformer, creating the Short Term Enhanced Transformer (STET). Compared with best-competing approaches, the impact of noise on STET is reduced by more than 20%. We also report promising results on both classification and regression datasets and demonstrate that STEM generalizes across different gesture recognition tasks.","sentences":["Gesture recognition based on surface electromyography (sEMG) has been gaining importance in many 3D Interactive Scenes.","However, sEMG is easily influenced by various forms of noise in real-world environments, leading to challenges in providing long-term stable interactions through sEMG.","Existing methods often struggle to enhance model noise resilience through various predefined data augmentation techniques.","In this work, we revisit the problem from a short term enhancement perspective to improve precision and robustness against various common noisy scenarios with learnable denoise using sEMG intrinsic pattern information and sliding-window attention.","We propose a Short Term Enhancement Module(STEM) which can be easily integrated with various models.","STEM offers several benefits: 1) Learnable denoise, enabling noise reduction without manual data augmentation; 2) Scalability, adaptable to various models; and 3) Cost-effectiveness, achieving short-term enhancement through minimal weight-sharing in an efficient attention mechanism.","In particular, we incorporate STEM into a transformer, creating the Short Term Enhanced Transformer (STET).","Compared with best-competing approaches, the impact of noise on STET is reduced by more than 20%.","We also report promising results on both classification and regression datasets and demonstrate that STEM generalizes across different gesture recognition tasks."],"url":"http://arxiv.org/abs/2404.11213v1","category":"eess.SP"}
{"created":"2024-04-17 09:47:16","title":"Closeby Habitable Exoplanet Survey (CHES). I. Astrometric Noise and Planetary Detection Efficiency due to Stellar Spots and Faculae","abstract":"The Closeby Habitable Exoplanet Survey (CHES) is dedicated to the astrometric exploration for habitable-zone Earth-like planets orbiting solar-type stars in close proximity, achieving unprecedented micro-arcsecond precision. Given the elevated precision, thorough consideration of photocenter jitters induced by stellar activity becomes imperative. This study endeavors to model the stellar activity of solar-type stars, compute astrometric noise, and delineate the detection limits of habitable planets within the astrometric domain. Simulations were conducted for identified primary targets of CHES, involving the generation of simulated observed data for astrometry and photometry, accounting for the impact of stellar activity. Estimation of activity levels in our samples was achieved through chromospheric activity indices, revealing that over 90% of stars exhibited photocenter jitters below 1 $\\mu\\mathrm{as}$. Notably, certain proximate stars, such as $\\alpha$ Cen A and B, displayed more discernible noise arising from stellar activity. Subsequent tests were performed to evaluate detection performance, unveiling that stellar activity tends to have a less pronounced impact on planetary detectability for the majority of stars. Approximately 95% of targets demonstrated a detection efficiency exceeding 80%. However, for several cold stars, e.g., HD 32450 and HD 21531, with the habitable zones close to the stars, a reduction in detection efficiency was observed. These findings offer invaluable insights into the intricate interplay between stellar activity and astrometric precision, significantly advancing our understanding in the search for habitable planets.","sentences":["The Closeby Habitable Exoplanet Survey (CHES) is dedicated to the astrometric exploration for habitable-zone Earth-like planets orbiting solar-type stars in close proximity, achieving unprecedented micro-arcsecond precision.","Given the elevated precision, thorough consideration of photocenter jitters induced by stellar activity becomes imperative.","This study endeavors to model the stellar activity of solar-type stars, compute astrometric noise, and delineate the detection limits of habitable planets within the astrometric domain.","Simulations were conducted for identified primary targets of CHES, involving the generation of simulated observed data for astrometry and photometry, accounting for the impact of stellar activity.","Estimation of activity levels in our samples was achieved through chromospheric activity indices, revealing that over 90% of stars exhibited photocenter jitters below 1 $\\mu\\mathrm{as}$. Notably, certain proximate stars, such as $\\alpha$ Cen A and B, displayed more discernible noise arising from stellar activity.","Subsequent tests were performed to evaluate detection performance, unveiling that stellar activity tends to have a less pronounced impact on planetary detectability for the majority of stars.","Approximately 95% of targets demonstrated a detection efficiency exceeding 80%.","However, for several cold stars, e.g., HD 32450 and HD 21531, with the habitable zones close to the stars, a reduction in detection efficiency was observed.","These findings offer invaluable insights into the intricate interplay between stellar activity and astrometric precision, significantly advancing our understanding in the search for habitable planets."],"url":"http://arxiv.org/abs/2404.11210v1","category":"astro-ph.EP"}
{"created":"2024-04-17 09:45:43","title":"Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM","abstract":"Medical report generation automates radiology descriptions from images, easing the burden on physicians and minimizing errors. However, current methods lack structured outputs and physician interactivity for clear, clinically relevant reports. Our method introduces a prompt-guided approach to generate structured chest X-ray reports using a pre-trained large language model (LLM). First, we identify anatomical regions in chest X-rays to generate focused sentences that center on key visual elements, thereby establishing a structured report foundation with anatomy-based sentences. We also convert the detected anatomy into textual prompts conveying anatomical comprehension to the LLM. Additionally, the clinical context prompts guide the LLM to emphasize interactivity and clinical requirements. By integrating anatomy-focused sentences and anatomy/clinical prompts, the pre-trained LLM can generate structured chest X-ray reports tailored to prompted anatomical regions and clinical contexts. We evaluate using language generation and clinical effectiveness metrics, demonstrating strong performance.","sentences":["Medical report generation automates radiology descriptions from images, easing the burden on physicians and minimizing errors.","However, current methods lack structured outputs and physician interactivity for clear, clinically relevant reports.","Our method introduces a prompt-guided approach to generate structured chest X-ray reports using a pre-trained large language model (LLM).","First, we identify anatomical regions in chest X-rays to generate focused sentences that center on key visual elements, thereby establishing a structured report foundation with anatomy-based sentences.","We also convert the detected anatomy into textual prompts conveying anatomical comprehension to the LLM.","Additionally, the clinical context prompts guide the LLM to emphasize interactivity and clinical requirements.","By integrating anatomy-focused sentences and anatomy/clinical prompts, the pre-trained LLM can generate structured chest X-ray reports tailored to prompted anatomical regions and clinical contexts.","We evaluate using language generation and clinical effectiveness metrics, demonstrating strong performance."],"url":"http://arxiv.org/abs/2404.11209v1","category":"cs.AI"}
{"created":"2024-04-17 09:43:54","title":"CAGE: Causality-Aware Shapley Value for Global Explanations","abstract":"As Artificial Intelligence (AI) is having more influence on our everyday lives, it becomes important that AI-based decisions are transparent and explainable. As a consequence, the field of eXplainable AI (or XAI) has become popular in recent years. One way to explain AI models is to elucidate the predictive importance of the input features for the AI model in general, also referred to as global explanations. Inspired by cooperative game theory, Shapley values offer a convenient way for quantifying the feature importance as explanations. However many methods based on Shapley values are built on the assumption of feature independence and often overlook causal relations of the features which could impact their importance for the ML model. Inspired by studies of explanations at the local level, we propose CAGE (Causally-Aware Shapley Values for Global Explanations). In particular, we introduce a novel sampling procedure for out-coalition features that respects the causal relations of the input features. We derive a practical approach that incorporates causal knowledge into global explanation and offers the possibility to interpret the predictive feature importance considering their causal relation. We evaluate our method on synthetic data and real-world data. The explanations from our approach suggest that they are not only more intuitive but also more faithful compared to previous global explanation methods.","sentences":["As Artificial Intelligence (AI) is having more influence on our everyday lives, it becomes important that AI-based decisions are transparent and explainable.","As a consequence, the field of eXplainable AI (or XAI) has become popular in recent years.","One way to explain AI models is to elucidate the predictive importance of the input features for the AI model in general, also referred to as global explanations.","Inspired by cooperative game theory, Shapley values offer a convenient way for quantifying the feature importance as explanations.","However many methods based on Shapley values are built on the assumption of feature independence and often overlook causal relations of the features which could impact their importance for the ML model.","Inspired by studies of explanations at the local level, we propose CAGE (Causally-Aware Shapley Values for Global Explanations).","In particular, we introduce a novel sampling procedure for out-coalition features that respects the causal relations of the input features.","We derive a practical approach that incorporates causal knowledge into global explanation and offers the possibility to interpret the predictive feature importance considering their causal relation.","We evaluate our method on synthetic data and real-world data.","The explanations from our approach suggest that they are not only more intuitive but also more faithful compared to previous global explanation methods."],"url":"http://arxiv.org/abs/2404.11208v1","category":"cs.AI"}
{"created":"2024-04-17 09:39:07","title":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","abstract":"Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.","sentences":["Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility.","However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads.","In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task.","To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model.","We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance.","We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction."],"url":"http://arxiv.org/abs/2404.11207v1","category":"cs.CV"}
{"created":"2024-04-17 09:39:02","title":"Prompt-tuning for Clickbait Detection via Text Summarization","abstract":"Clickbaits are surprising social posts or deceptive news headlines that attempt to lure users for more clicks, which have posted at unprecedented rates for more profit or commercial revenue. The spread of clickbait has significant negative impacts on the users, which brings users misleading or even click-jacking attacks. Different from fake news, the crucial problem in clickbait detection is determining whether the headline matches the corresponding content. Most existing methods compute the semantic similarity between the headlines and contents for detecting clickbait. However, due to significant differences in length and semantic features between headlines and contents, directly calculating semantic similarity is often difficult to summarize the relationship between them. To address this problem, we propose a prompt-tuning method for clickbait detection via text summarization in this paper, text summarization is introduced to summarize the contents, and clickbait detection is performed based on the similarity between the generated summary and the contents. Specifically, we first introduce a two-stage text summarization model to produce high-quality news summaries based on pre-trained language models, and then both the headlines and new generated summaries are incorporated as the inputs for prompt-tuning. Additionally, a variety of strategies are conducted to incorporate external knowledge for improving the performance of clickbait detection. The extensive experiments on well-known clickbait detection datasets demonstrate that our method achieved state-of-the-art performance.","sentences":["Clickbaits are surprising social posts or deceptive news headlines that attempt to lure users for more clicks, which have posted at unprecedented rates for more profit or commercial revenue.","The spread of clickbait has significant negative impacts on the users, which brings users misleading or even click-jacking attacks.","Different from fake news, the crucial problem in clickbait detection is determining whether the headline matches the corresponding content.","Most existing methods compute the semantic similarity between the headlines and contents for detecting clickbait.","However, due to significant differences in length and semantic features between headlines and contents, directly calculating semantic similarity is often difficult to summarize the relationship between them.","To address this problem, we propose a prompt-tuning method for clickbait detection via text summarization in this paper, text summarization is introduced to summarize the contents, and clickbait detection is performed based on the similarity between the generated summary and the contents.","Specifically, we first introduce a two-stage text summarization model to produce high-quality news summaries based on pre-trained language models, and then both the headlines and new generated summaries are incorporated as the inputs for prompt-tuning.","Additionally, a variety of strategies are conducted to incorporate external knowledge for improving the performance of clickbait detection.","The extensive experiments on well-known clickbait detection datasets demonstrate that our method achieved state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.11206v1","category":"cs.CL"}
{"created":"2024-04-17 09:31:01","title":"RiboDiffusion: Tertiary Structure-based RNA Inverse Folding with Generative Diffusion Models","abstract":"RNA design shows growing applications in synthetic biology and therapeutics, driven by the crucial role of RNA in various biological processes. A fundamental challenge is to find functional RNA sequences that satisfy given structural constraints, known as the inverse folding problem. Computational approaches have emerged to address this problem based on secondary structures. However, designing RNA sequences directly from 3D structures is still challenging, due to the scarcity of data, the non-unique structure-sequence mapping, and the flexibility of RNA conformation. In this study, we propose RiboDiffusion, a generative diffusion model for RNA inverse folding that can learn the conditional distribution of RNA sequences given 3D backbone structures. Our model consists of a graph neural network-based structure module and a Transformer-based sequence module, which iteratively transforms random sequences into desired sequences. By tuning the sampling weight, our model allows for a trade-off between sequence recovery and diversity to explore more candidates. We split test sets based on RNA clustering with different cut-offs for sequence or structure similarity. Our model outperforms baselines in sequence recovery, with an average relative improvement of $11\\%$ for sequence similarity splits and $16\\%$ for structure similarity splits. Moreover, RiboDiffusion performs consistently well across various RNA length categories and RNA types. We also apply in-silico folding to validate whether the generated sequences can fold into the given 3D RNA backbones. Our method could be a powerful tool for RNA design that explores the vast sequence space and finds novel solutions to 3D structural constraints.","sentences":["RNA design shows growing applications in synthetic biology and therapeutics, driven by the crucial role of RNA in various biological processes.","A fundamental challenge is to find functional RNA sequences that satisfy given structural constraints, known as the inverse folding problem.","Computational approaches have emerged to address this problem based on secondary structures.","However, designing RNA sequences directly from 3D structures is still challenging, due to the scarcity of data, the non-unique structure-sequence mapping, and the flexibility of RNA conformation.","In this study, we propose RiboDiffusion, a generative diffusion model for RNA inverse folding that can learn the conditional distribution of RNA sequences given 3D backbone structures.","Our model consists of a graph neural network-based structure module and a Transformer-based sequence module, which iteratively transforms random sequences into desired sequences.","By tuning the sampling weight, our model allows for a trade-off between sequence recovery and diversity to explore more candidates.","We split test sets based on RNA clustering with different cut-offs for sequence or structure similarity.","Our model outperforms baselines in sequence recovery, with an average relative improvement of $11\\%$ for sequence similarity splits and $16\\%$ for structure similarity splits.","Moreover, RiboDiffusion performs consistently well across various RNA length categories and RNA types.","We also apply in-silico folding to validate whether the generated sequences can fold into the given 3D RNA backbones.","Our method could be a powerful tool for RNA design that explores the vast sequence space and finds novel solutions to 3D structural constraints."],"url":"http://arxiv.org/abs/2404.11199v1","category":"q-bio.BM"}
{"created":"2024-04-17 09:29:24","title":"Uniform Regularity for Incompressible MHD Equations in a Bounded Domain with Curved Boundary in 3D","abstract":"For the initial boundary problem of the incompressible MHD equations in a bounded domain with general curved boundary in 3D with the general Navier-slip boundary conditions for the velocity field and the perfect conducting condition for the magnetic field, we establish the uniform regularity of conormal Sobolev norms and Lipschitz norms to addressing the anisotropic regularity of tangential and normal directions, which enable us to prove the vanishing dissipation limit as the viscosity and the magnetic diffusion coefficients tend to zero. We overcome the difficulties caused by the intricate interaction of boundary curvature, velocity field, and magnetic fields and resolve the issue caused by the problem that the viscosity and the magnetic diffusion coefficients are not required to be equal.","sentences":["For the initial boundary problem of the incompressible MHD equations in a bounded domain with general curved boundary in 3D with the general Navier-slip boundary conditions for the velocity field and the perfect conducting condition for the magnetic field, we establish the uniform regularity of conormal Sobolev norms and Lipschitz norms to addressing the anisotropic regularity of tangential and normal directions, which enable us to prove the vanishing dissipation limit as the viscosity and the magnetic diffusion coefficients tend to zero.","We overcome the difficulties caused by the intricate interaction of boundary curvature, velocity field, and magnetic fields and resolve the issue caused by the problem that the viscosity and the magnetic diffusion coefficients are not required to be equal."],"url":"http://arxiv.org/abs/2404.11197v1","category":"math.AP"}
{"created":"2024-04-17 09:18:24","title":"Photonic indistinguishability characterization and optimization for cavity-based single-photon source","abstract":"Indistinguishability of single photons from independent sources is critically important for scalable quantum technologies. We provide a comprehensive comparison of single-photon indistinguishability of different kinds of cavity quantum electrodynamics (CQED) systems by numerically simulating Hong-Ou-Mandel (HOM) two-photon interference. We find that the CQED system using nature atoms exhibit superiority in indistinguishability, benefiting from the inherently identical features. Moreover, a $\\Lambda-$type three-level atoms show essential robust against variation of various system parameters because it exploits the two ground states with considerable smaller decay rates for single-photon generation. Furthermore, a machine learning-based framework is proposed to significantly and robustly improve single-photon indistinguishability for non-identical two CQED systems. This work may pave the way for designing and engineering reliable and scalable photon-based quantum technologies.","sentences":["Indistinguishability of single photons from independent sources is critically important for scalable quantum technologies.","We provide a comprehensive comparison of single-photon indistinguishability of different kinds of cavity quantum electrodynamics (CQED) systems by numerically simulating Hong-Ou-Mandel (HOM) two-photon interference.","We find that the CQED system using nature atoms exhibit superiority in indistinguishability, benefiting from the inherently identical features.","Moreover, a $\\Lambda-$type three-level atoms show essential robust against variation of various system parameters because it exploits the two ground states with considerable smaller decay rates for single-photon generation.","Furthermore, a machine learning-based framework is proposed to significantly and robustly improve single-photon indistinguishability for non-identical two CQED systems.","This work may pave the way for designing and engineering reliable and scalable photon-based quantum technologies."],"url":"http://arxiv.org/abs/2404.11193v1","category":"quant-ph"}
{"created":"2024-04-17 09:04:44","title":"Generating hypergraphs of finite groups","abstract":"In a recent paper Cameron, Lakshmanan and Ajith began an exploration of hypergraphs defined on algebraic structures, especially groups, to investigate whether this can add a new perspective. Following their suggestions, we consider suitable hypergraphs encoding the generating properties of a finite group. In particular, answering a question asked in their paper, we classified the finite solvable groups whose generating hypergraph is the basis hypergraph of a matroid.","sentences":["In a recent paper Cameron, Lakshmanan and Ajith began an exploration of hypergraphs defined on algebraic structures, especially groups, to investigate whether this can add a new perspective.","Following their suggestions, we consider suitable hypergraphs encoding the generating properties of a finite group.","In particular, answering a question asked in their paper, we classified the finite solvable groups whose generating hypergraph is the basis hypergraph of a matroid."],"url":"http://arxiv.org/abs/2404.11186v1","category":"math.GR"}
{"created":"2024-04-17 08:53:59","title":"KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections","abstract":"Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems. However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users. To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately. Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections. Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively. These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field.","sentences":["Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems.","However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users.","To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately.","Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections.","Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle.","When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively.","These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field."],"url":"http://arxiv.org/abs/2404.11181v1","category":"cs.LG"}
{"created":"2024-04-17 08:50:24","title":"A Fourier analytic approach to exceptional set estimates for orthogonal projections","abstract":"Marstrand's celebrated projection theorem gives the Hausdorff dimension of the orthogonal projection of a Borel set in Euclidean space for almost all orthogonal projections. It is straightforward to see that sets for which the Fourier and Hausdorff dimension coincide have no exceptional projections, that is, \\emph{all} orthogonal projections satisfy the conclusion of Marstrand's theorem. With this in mind, one might believe that the Fourier dimension (or at least, Fourier decay) could be used to give better estimates for the Hausdorff dimension of the exceptional set in general. We obtain projection theorems and exceptional set estimates based on the Fourier spectrum; a family of dimensions that interpolates between the Fourier and Hausdorff dimensions. We apply these results to show that the Fourier spectrum can be used to improve several results for the Hausdorff dimension in certain cases, such as Ren--Wang's sharp bound for the exceptional set in the plane, Peres--Schlag's exceptional set bound and Bourgain--Oberlin's sharp $0$-dimensional exceptional set estimate.","sentences":["Marstrand's celebrated projection theorem gives the Hausdorff dimension of the orthogonal projection of a Borel set in Euclidean space for almost all orthogonal projections.","It is straightforward to see that sets for which the Fourier and Hausdorff dimension coincide have no exceptional projections, that is, \\emph{all} orthogonal projections satisfy the conclusion of Marstrand's theorem.","With this in mind, one might believe that the Fourier dimension (or at least, Fourier decay) could be used to give better estimates for the Hausdorff dimension of the exceptional set in general.","We obtain projection theorems and exceptional set estimates based on the Fourier spectrum; a family of dimensions that interpolates between the Fourier and Hausdorff dimensions.","We apply these results to show that the Fourier spectrum can be used to improve several results for the Hausdorff dimension in certain cases, such as Ren--Wang's sharp bound for the exceptional set in the plane, Peres--Schlag's exceptional set bound and Bourgain--Oberlin's sharp $0$-dimensional exceptional set estimate."],"url":"http://arxiv.org/abs/2404.11179v1","category":"math.CA"}
{"created":"2024-04-17 08:48:07","title":"Compression of quantum shallow-circuit states","abstract":"Shallow quantum circuits feature not only computational advantage over their classical counterparts but also cutting-edge applications. Storing quantum information generated by shallow circuits is a fundamental question of both theoretical and practical importance that remained largely unexplored. In this work, we show that $N$ copies of an unknown $n$-qubit state generated by a fixed-depth circuit can be compressed into a hybrid memory of $O(n \\log_2 N)$ (qu)bits, which achieves the optimal scaling of memory cost. Our work shows that the computational complexity of resources can significantly impact the rate of quantum information processing, offering a unique and unified view of quantum Shannon theory and quantum computing in the NISQ era.","sentences":["Shallow quantum circuits feature not only computational advantage over their classical counterparts but also cutting-edge applications.","Storing quantum information generated by shallow circuits is a fundamental question of both theoretical and practical importance that remained largely unexplored.","In this work, we show that $N$ copies of an unknown $n$-qubit state generated by a fixed-depth circuit can be compressed into a hybrid memory of $O(n \\log_2 N)$ (qu)bits, which achieves the optimal scaling of memory cost.","Our work shows that the computational complexity of resources can significantly impact the rate of quantum information processing, offering a unique and unified view of quantum Shannon theory and quantum computing in the NISQ era."],"url":"http://arxiv.org/abs/2404.11177v1","category":"quant-ph"}
{"created":"2024-04-17 08:42:46","title":"A numerical view on \u03b1-dissipative solutions of the Hunter-Saxton equation","abstract":"We propose a new numerical method for $\\alpha$-dissipative solutions of the Hunter-Saxton equation, where $\\alpha$ belongs to $W^{1, \\infty}(\\mathbb{R}, [0, 1))$. The method combines a projection operator with a generalized method of characteristics and an iteration scheme, which is based on enforcing minimal time steps whenever breaking times cluster. Numerical examples illustrate that these minimal time steps increase the efficiency of the algorithm substantially. Moreover, convergence of the wave profile is shown in $C([0, T], L^{\\infty}(\\mathbb{R}))$ for any finite $T \\geq 0$.","sentences":["We propose a new numerical method for $\\alpha$-dissipative solutions of the Hunter-Saxton equation, where $\\alpha$ belongs to $W^{1, \\infty}(\\mathbb{R}, [0, 1))$.","The method combines a projection operator with a generalized method of characteristics and an iteration scheme, which is based on enforcing minimal time steps whenever breaking times cluster.","Numerical examples illustrate that these minimal time steps increase the efficiency of the algorithm substantially.","Moreover, convergence of the wave profile is shown in $C([0, T], L^{\\infty}(\\mathbb{R}))$ for any finite $T \\geq 0$."],"url":"http://arxiv.org/abs/2404.11174v1","category":"math.NA"}
{"created":"2024-04-17 08:42:42","title":"Deep Neural Networks via Complex Network Theory: a Perspective","abstract":"Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally. Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures. However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data. In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks. In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning. For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers. We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function. Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis.","sentences":["Deep Neural Networks (DNNs) can be represented as graphs whose links and vertices iteratively process data and solve tasks sub-optimally.","Complex Network Theory (CNT), merging statistical physics with graph theory, provides a method for interpreting neural networks by analysing their weights and neuron structures.","However, classic works adapt CNT metrics that only permit a topological analysis as they do not account for the effect of the input data.","In addition, CNT metrics have been applied to a limited range of architectures, mainly including Fully Connected neural networks.","In this work, we extend the existing CNT metrics with measures that sample from the DNNs' training distribution, shifting from a purely topological analysis to one that connects with the interpretability of deep learning.","For the novel metrics, in addition to the existing ones, we provide a mathematical formalisation for Fully Connected, AutoEncoder, Convolutional and Recurrent neural networks, of which we vary the activation functions and the number of hidden layers.","We show that these metrics differentiate DNNs based on the architecture, the number of hidden layers, and the activation function.","Our contribution provides a method rooted in physics for interpreting DNNs that offers insights beyond the traditional input-output relationship and the CNT topological analysis."],"url":"http://arxiv.org/abs/2404.11172v1","category":"cs.LG"}
{"created":"2024-04-17 08:40:54","title":"Personalized Heart Disease Detection via ECG Digital Twin Generation","abstract":"Heart diseases rank among the leading causes of global mortality, demonstrating a crucial need for early diagnosis and intervention. Most traditional electrocardiogram (ECG) based automated diagnosis methods are trained at population level, neglecting the customization of personalized ECGs to enhance individual healthcare management. A potential solution to address this limitation is to employ digital twins to simulate symptoms of diseases in real patients. In this paper, we present an innovative prospective learning approach for personalized heart disease detection, which generates digital twins of healthy individuals' anomalous ECGs and enhances the model sensitivity to the personalized symptoms. In our approach, a vector quantized feature separator is proposed to locate and isolate the disease symptom and normal segments in ECG signals with ECG report guidance. Thus, the ECG digital twins can simulate specific heart diseases used to train a personalized heart disease detection model. Experiments demonstrate that our approach not only excels in generating high-fidelity ECG signals but also improves personalized heart disease detection. Moreover, our approach ensures robust privacy protection, safeguarding patient data in model development.","sentences":["Heart diseases rank among the leading causes of global mortality, demonstrating a crucial need for early diagnosis and intervention.","Most traditional electrocardiogram (ECG) based automated diagnosis methods are trained at population level, neglecting the customization of personalized ECGs to enhance individual healthcare management.","A potential solution to address this limitation is to employ digital twins to simulate symptoms of diseases in real patients.","In this paper, we present an innovative prospective learning approach for personalized heart disease detection, which generates digital twins of healthy individuals' anomalous ECGs and enhances the model sensitivity to the personalized symptoms.","In our approach, a vector quantized feature separator is proposed to locate and isolate the disease symptom and normal segments in ECG signals with ECG report guidance.","Thus, the ECG digital twins can simulate specific heart diseases used to train a personalized heart disease detection model.","Experiments demonstrate that our approach not only excels in generating high-fidelity ECG signals but also improves personalized heart disease detection.","Moreover, our approach ensures robust privacy protection, safeguarding patient data in model development."],"url":"http://arxiv.org/abs/2404.11171v1","category":"cs.LG"}
{"created":"2024-04-17 08:32:11","title":"Microwave photonic short-time Fourier transform based on stabilized period-one nonlinear laser dynamics and stimulated Brillouin scattering","abstract":"A microwave photonic short-time Fourier transform (STFT) system based on stabilized period-one (P1) nonlinear laser dynamics and stimulated Brillouin scattering (SBS) is proposed. By using an optoelectronic feedback loop, the frequency-sweep optical signal generated by the P1 nonlinear laser dynamics is stabilized, which is further used in conjunction with an optical bandpass filter implemented by stimulated Brillouin scattering (SBS) to achieve the frequency-to-time mapping of microwave signals and the final STFT. By comparing the experimental results with and without optoelectronic feedback, it is found that the time-frequency diagram of the signal under test (SUT) obtained by STFT is clearer and more regular, and the frequency of the SUT measured in each frequency-sweep period is more accurate. The mean absolute error is reduced by 50% under the optimal filter bandwidth.","sentences":["A microwave photonic short-time Fourier transform (STFT) system based on stabilized period-one (P1) nonlinear laser dynamics and stimulated Brillouin scattering (SBS) is proposed.","By using an optoelectronic feedback loop, the frequency-sweep optical signal generated by the P1 nonlinear laser dynamics is stabilized, which is further used in conjunction with an optical bandpass filter implemented by stimulated Brillouin scattering (SBS) to achieve the frequency-to-time mapping of microwave signals and the final STFT.","By comparing the experimental results with and without optoelectronic feedback, it is found that the time-frequency diagram of the signal under test (SUT) obtained by STFT is clearer and more regular, and the frequency of the SUT measured in each frequency-sweep period is more accurate.","The mean absolute error is reduced by 50% under the optimal filter bandwidth."],"url":"http://arxiv.org/abs/2404.11168v1","category":"physics.optics"}
{"created":"2024-04-17 08:32:08","title":"Ito's formula for flows of conditional measures on semimartingales","abstract":"Motivated by recent development of mean-field systems with common noise, this paper establishes Ito's formula for flows of conditional probability measures under a common filtration associated with general semimartingales. This generalizes existing works on flows of conditional measures on Ito processes and flows of deterministic measure on general semimartingales. The key technical components involve constructing conditional independent copies and establishing the equivalence between stochastic integrals with respect to the conditional law of semimartingales and the conditional expectation of stochastic integrals with respect to copies of semimartingales. Ito's formula is then established for cylindrical functions through conditional independent copies, and extended to the general case through function approximations.","sentences":["Motivated by recent development of mean-field systems with common noise, this paper establishes Ito's formula for flows of conditional probability measures under a common filtration associated with general semimartingales.","This generalizes existing works on flows of conditional measures on Ito processes and flows of deterministic measure on general semimartingales.","The key technical components involve constructing conditional independent copies and establishing the equivalence between stochastic integrals with respect to the conditional law of semimartingales and the conditional expectation of stochastic integrals with respect to copies of semimartingales.","Ito's formula is then established for cylindrical functions through conditional independent copies, and extended to the general case through function approximations."],"url":"http://arxiv.org/abs/2404.11167v1","category":"math.PR"}
{"created":"2024-04-17 08:29:34","title":"783-MHz fundamental repetition rate, Er-doped all-fiber ring laser mode-locked by carbon nanotubes","abstract":"We have demonstrated 783-MHz fundamental repetition rate from a compact and robust Er-doped all-fiber ring laser mode-locked by carbon nanotubes (CNT), with pulse width of 623 fs at center wavelength of 1562 nm. To the best of our knowledge, this is the highest fundamental repetition rate has been reported from an all-fiber ring laser. By using CNT saturable absorber (SA), a relatively low self-starting pump threshold of 108 mW is achieved. The laser has a very compact footprint less than 10 cm X 10 cm, benefiting from the all-active-fiber cavity design. The robust mode-locking is confirmed by the low relative intensity noise (RIN) and a long-term stability test. We propose a new scheme for generating high repetition rate femtosecond optical pulses from a compact and stable all-fiber ring laser.","sentences":["We have demonstrated 783-MHz fundamental repetition rate from a compact and robust Er-doped all-fiber ring laser mode-locked by carbon nanotubes (CNT), with pulse width of 623 fs at center wavelength of 1562 nm.","To the best of our knowledge, this is the highest fundamental repetition rate has been reported from an all-fiber ring laser.","By using CNT saturable absorber (SA), a relatively low self-starting pump threshold of 108 mW is achieved.","The laser has a very compact footprint less than 10 cm X 10 cm, benefiting from the all-active-fiber cavity design.","The robust mode-locking is confirmed by the low relative intensity noise (RIN) and a long-term stability test.","We propose a new scheme for generating high repetition rate femtosecond optical pulses from a compact and stable all-fiber ring laser."],"url":"http://arxiv.org/abs/2404.11165v1","category":"physics.optics"}
{"created":"2024-04-17 08:28:13","title":"Heterogeneous mean-field analysis of the generalized Lotka-Volterra model on a network","abstract":"We study the dynamics of the generalized Lotka-Volterra model with a network structure. Performing a high connectivity expansion for graphs, we write down a mean-field dynamical theory that incorporates degree heterogeneity. This allows us to describe the fixed points of the model in terms of a few simple order parameters. We extend the analysis even for diverging abundances, using a mapping to the replicator model. With this we present a unified approach for both cooperative and competitive systems that display complementary behaviors. In particular we show the central role of an order parameter called the critical degree, $g_c$. In the competitive regime $g_c$ serves to distinguish high degree nodes that are more likely to go extinct, while in the cooperative regime it has the reverse role, it will determine the low degree nodes that tend to go relatively extinct.","sentences":["We study the dynamics of the generalized Lotka-Volterra model with a network structure.","Performing a high connectivity expansion for graphs, we write down a mean-field dynamical theory that incorporates degree heterogeneity.","This allows us to describe the fixed points of the model in terms of a few simple order parameters.","We extend the analysis even for diverging abundances, using a mapping to the replicator model.","With this we present a unified approach for both cooperative and competitive systems that display complementary behaviors.","In particular we show the central role of an order parameter called the critical degree, $g_c$. In the competitive regime $g_c$ serves to distinguish high degree nodes that are more likely to go extinct, while in the cooperative regime it has the reverse role, it will determine the low degree nodes that tend to go relatively extinct."],"url":"http://arxiv.org/abs/2404.11164v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 08:25:42","title":"AI-equipped scanning probe microscopy for autonomous site-specific atomic-level characterization at room temperature","abstract":"We present an advanced scanning probe microscopy system enhanced with artificial intelligence (AI-SPM) designed for self-driving atomic-scale measurements. This system expertly identifies and manipulates atomic positions with high precision, autonomously performing tasks such as spectroscopic data acquisition and atomic adjustment. An outstanding feature of AI-SPM is its ability to detect and adapt to surface defects, targeting or avoiding them as necessary. It's also engineered to address typical challenges such as positional drift and tip apex atomic variations due to the thermal effect, ensuring accurate, site-specific surface analyses. Our tests under the demanding conditions of room temperature have demonstrated the robustness of the system, successfully navigating thermal drift and tip fluctuations. During these tests on the Si(111)-(7x7) surface, AI-SPM autonomously identified defect-free regions and performed a large number of current-voltage spectroscopy measurements at different adatom sites, while autonomously compensating for thermal drift and monitoring probe health. These experiments produce extensive data sets that are critical for reliable materials characterization and demonstrate the potential of AI-SPM to significantly improve data acquisition. The integration of AI into SPM technologies represents a step toward more effective, precise and reliable atomic-level surface analysis, revolutionizing materials characterization methods.","sentences":["We present an advanced scanning probe microscopy system enhanced with artificial intelligence (AI-SPM) designed for self-driving atomic-scale measurements.","This system expertly identifies and manipulates atomic positions with high precision, autonomously performing tasks such as spectroscopic data acquisition and atomic adjustment.","An outstanding feature of AI-SPM is its ability to detect and adapt to surface defects, targeting or avoiding them as necessary.","It's also engineered to address typical challenges such as positional drift and tip apex atomic variations due to the thermal effect, ensuring accurate, site-specific surface analyses.","Our tests under the demanding conditions of room temperature have demonstrated the robustness of the system, successfully navigating thermal drift and tip fluctuations.","During these tests on the Si(111)-(7x7) surface, AI-SPM autonomously identified defect-free regions and performed a large number of current-voltage spectroscopy measurements at different adatom sites, while autonomously compensating for thermal drift and monitoring probe health.","These experiments produce extensive data sets that are critical for reliable materials characterization and demonstrate the potential of AI-SPM to significantly improve data acquisition.","The integration of AI into SPM technologies represents a step toward more effective, precise and reliable atomic-level surface analysis, revolutionizing materials characterization methods."],"url":"http://arxiv.org/abs/2404.11162v1","category":"physics.comp-ph"}
{"created":"2024-04-17 08:16:48","title":"Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation","abstract":"Large Language Models (LLMs) have become the go-to solution for many Natural Language Processing (NLP) tasks due to their ability to tackle various problems and produce high-quality results. Specifically, they are increasingly used to automatically generate code, easing the burden on developers by handling repetitive tasks. However, this improvement in quality has led to high computational and memory demands, making LLMs inaccessible to users with limited resources. In this paper, we focus on Central Processing Unit (CPU)-compatible models and conduct a thorough semi-manual evaluation of their strengths and weaknesses in generating Python code. We enhance their performance by introducing a Chain-of-Thought prompt that guides the model in problem-solving. Additionally, we propose a dataset of 60 programming problems with varying difficulty levels for evaluation purposes. Our assessment also includes testing these models on two state-of-the-art datasets: HumanEval and EvalPlus. We commit to sharing our dataset and experimental results publicly to ensure transparency.","sentences":["Large Language Models (LLMs) have become the go-to solution for many Natural Language Processing (NLP) tasks due to their ability to tackle various problems and produce high-quality results.","Specifically, they are increasingly used to automatically generate code, easing the burden on developers by handling repetitive tasks.","However, this improvement in quality has led to high computational and memory demands, making LLMs inaccessible to users with limited resources.","In this paper, we focus on Central Processing Unit (CPU)-compatible models and conduct a thorough semi-manual evaluation of their strengths and weaknesses in generating Python code.","We enhance their performance by introducing a Chain-of-Thought prompt that guides the model in problem-solving.","Additionally, we propose a dataset of 60 programming problems with varying difficulty levels for evaluation purposes.","Our assessment also includes testing these models on two state-of-the-art datasets: HumanEval and EvalPlus.","We commit to sharing our dataset and experimental results publicly to ensure transparency."],"url":"http://arxiv.org/abs/2404.11160v1","category":"cs.AI"}
{"created":"2024-04-17 08:15:25","title":"Deep Portrait Quality Assessment. A NTIRE 2024 Challenge Survey","abstract":"This paper reviews the NTIRE 2024 Portrait Quality Assessment Challenge, highlighting the proposed solutions and results. This challenge aims to obtain an efficient deep neural network capable of estimating the perceptual quality of real portrait photos. The methods must generalize to diverse scenes and diverse lighting conditions (indoor, outdoor, low-light), movement, blur, and other challenging conditions. In the challenge, 140 participants registered, and 35 submitted results during the challenge period. The performance of the top 5 submissions is reviewed and provided here as a gauge for the current state-of-the-art in Portrait Quality Assessment.","sentences":["This paper reviews the NTIRE 2024 Portrait Quality Assessment Challenge, highlighting the proposed solutions and results.","This challenge aims to obtain an efficient deep neural network capable of estimating the perceptual quality of real portrait photos.","The methods must generalize to diverse scenes and diverse lighting conditions (indoor, outdoor, low-light), movement, blur, and other challenging conditions.","In the challenge, 140 participants registered, and 35 submitted results during the challenge period.","The performance of the top 5 submissions is reviewed and provided here as a gauge for the current state-of-the-art in Portrait Quality Assessment."],"url":"http://arxiv.org/abs/2404.11159v1","category":"cs.CV"}
{"created":"2024-04-17 08:05:04","title":"Multi-target and multi-stage liver lesion segmentation and detection in multi-phase computed tomography scans","abstract":"Multi-phase computed tomography (CT) scans use contrast agents to highlight different anatomical structures within the body to improve the probability of identifying and detecting anatomical structures of interest and abnormalities such as liver lesions. Yet, detecting these lesions remains a challenging task as these lesions vary significantly in their size, shape, texture, and contrast with respect to surrounding tissue. Therefore, radiologists need to have an extensive experience to be able to identify and detect these lesions. Segmentation-based neural networks can assist radiologists with this task. Current state-of-the-art lesion segmentation networks use the encoder-decoder design paradigm based on the UNet architecture where the multi-phase CT scan volume is fed to the network as a multi-channel input. Although this approach utilizes information from all the phases and outperform single-phase segmentation networks, we demonstrate that their performance is not optimal and can be further improved by incorporating the learning from models trained on each single-phase individually. Our approach comprises three stages. The first stage identifies the regions within the liver where there might be lesions at three different scales (4, 8, and 16 mm). The second stage includes the main segmentation model trained using all the phases as well as a segmentation model trained on each of the phases individually. The third stage uses the multi-phase CT volumes together with the predictions from each of the segmentation models to generate the final segmentation map. Overall, our approach improves relative liver lesion segmentation performance by 1.6% while reducing performance variability across subjects by 8% when compared to the current state-of-the-art models.","sentences":["Multi-phase computed tomography (CT) scans use contrast agents to highlight different anatomical structures within the body to improve the probability of identifying and detecting anatomical structures of interest and abnormalities such as liver lesions.","Yet, detecting these lesions remains a challenging task as these lesions vary significantly in their size, shape, texture, and contrast with respect to surrounding tissue.","Therefore, radiologists need to have an extensive experience to be able to identify and detect these lesions.","Segmentation-based neural networks can assist radiologists with this task.","Current state-of-the-art lesion segmentation networks use the encoder-decoder design paradigm based on the UNet architecture where the multi-phase CT scan volume is fed to the network as a multi-channel input.","Although this approach utilizes information from all the phases and outperform single-phase segmentation networks, we demonstrate that their performance is not optimal and can be further improved by incorporating the learning from models trained on each single-phase individually.","Our approach comprises three stages.","The first stage identifies the regions within the liver where there might be lesions at three different scales (4, 8, and 16 mm).","The second stage includes the main segmentation model trained using all the phases as well as a segmentation model trained on each of the phases individually.","The third stage uses the multi-phase CT volumes together with the predictions from each of the segmentation models to generate the final segmentation map.","Overall, our approach improves relative liver lesion segmentation performance by 1.6% while reducing performance variability across subjects by 8% when compared to the current state-of-the-art models."],"url":"http://arxiv.org/abs/2404.11152v1","category":"eess.IV"}
{"created":"2024-04-17 08:01:55","title":"REACTO: Reconstructing Articulated Objects from a Single Video","abstract":"In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO.","sentences":["In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video.","Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models.","To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints.","Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation.","Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets.","Project page: https://chaoyuesong.github.io/REACTO."],"url":"http://arxiv.org/abs/2404.11151v1","category":"cs.CV"}
{"created":"2024-04-17 08:01:00","title":"Physics-informed Actor-Critic for Coordination of Virtual Inertia from Power Distribution Systems","abstract":"The vanishing inertia of synchronous generators in transmission systems requires the utilization of renewables for inertial support. These are often connected to the distribution system and their support should be coordinated to avoid violation of grid limits. To this end, this paper presents the Physics-informed Actor-Critic (PI-AC) algorithm for coordination of Virtual Inertia (VI) from renewable Inverter-based Resources (IBRs) in power distribution systems. Acquiring a model of the distribution grid can be difficult, since certain parts are often unknown or the parameters are highly uncertain. To favor model-free coordination, Reinforcement Learning (RL) methods can be employed, necessitating a substantial level of training beforehand. The PI-AC is a RL algorithm that integrates the physical behavior of the power system into the Actor-Critic (AC) approach in order to achieve faster learning. To this end, we regularize the loss function with an aggregated power system dynamics model based on the swing equation. Throughout this paper, we explore the PI-AC functionality in a case study with the CIGRE 14-bus and IEEE 37-bus power distribution system in various grid settings. The PI-AC is able to achieve better rewards and faster learning than the exclusively data-driven AC algorithm and the metaheuristic Genetic Algorithm (GA).","sentences":["The vanishing inertia of synchronous generators in transmission systems requires the utilization of renewables for inertial support.","These are often connected to the distribution system and their support should be coordinated to avoid violation of grid limits.","To this end, this paper presents the Physics-informed Actor-Critic (PI-AC) algorithm for coordination of Virtual Inertia (VI) from renewable Inverter-based Resources (IBRs) in power distribution systems.","Acquiring a model of the distribution grid can be difficult, since certain parts are often unknown or the parameters are highly uncertain.","To favor model-free coordination, Reinforcement Learning (RL) methods can be employed, necessitating a substantial level of training beforehand.","The PI-AC is a RL algorithm that integrates the physical behavior of the power system into the Actor-Critic (AC) approach in order to achieve faster learning.","To this end, we regularize the loss function with an aggregated power system dynamics model based on the swing equation.","Throughout this paper, we explore the PI-AC functionality in a case study with the CIGRE 14-bus and IEEE 37-bus power distribution system in various grid settings.","The PI-AC is able to achieve better rewards and faster learning than the exclusively data-driven AC algorithm and the metaheuristic Genetic Algorithm (GA)."],"url":"http://arxiv.org/abs/2404.11149v1","category":"eess.SY"}
{"created":"2024-04-17 07:59:33","title":"Explainable Machine Learning System for Predicting Chronic Kidney Disease in High-Risk Cardiovascular Patients","abstract":"As the global population ages, the incidence of Chronic Kidney Disease (CKD) is rising. CKD often remains asymptomatic until advanced stages, which significantly burdens both the healthcare system and patient quality of life. This research developed an explainable machine learning system for predicting CKD in patients with cardiovascular risks, utilizing medical history and laboratory data. The Random Forest model achieved the highest sensitivity of 88.2%. The study introduces a comprehensive explainability framework that extends beyond traditional feature importance methods, incorporating global and local interpretations, bias inspection, biomedical relevance, and safety assessments. Key predictive features identified in global interpretation were the use of diabetic and ACEI/ARB medications, and initial eGFR values. Local interpretation provided model insights through counterfactual explanations, which aligned with other system parts. After conducting a bias inspection, it was found that the initial eGFR values and CKD predictions exhibited some bias, but no significant gender bias was identified. The model's logic, extracted by scoped rules, was confirmed to align with existing medical literature. The safety assessment tested potentially dangerous cases and confirmed that the model behaved safely. This system enhances the explainability, reliability, and accountability of the model, promoting its potential integration into healthcare settings and compliance with upcoming regulatory standards, and showing promise for broader applications in healthcare machine learning.","sentences":["As the global population ages, the incidence of Chronic Kidney Disease (CKD) is rising.","CKD often remains asymptomatic until advanced stages, which significantly burdens both the healthcare system and patient quality of life.","This research developed an explainable machine learning system for predicting CKD in patients with cardiovascular risks, utilizing medical history and laboratory data.","The Random Forest model achieved the highest sensitivity of 88.2%.","The study introduces a comprehensive explainability framework that extends beyond traditional feature importance methods, incorporating global and local interpretations, bias inspection, biomedical relevance, and safety assessments.","Key predictive features identified in global interpretation were the use of diabetic and ACEI/ARB medications, and initial eGFR values.","Local interpretation provided model insights through counterfactual explanations, which aligned with other system parts.","After conducting a bias inspection, it was found that the initial eGFR values and CKD predictions exhibited some bias, but no significant gender bias was identified.","The model's logic, extracted by scoped rules, was confirmed to align with existing medical literature.","The safety assessment tested potentially dangerous cases and confirmed that the model behaved safely.","This system enhances the explainability, reliability, and accountability of the model, promoting its potential integration into healthcare settings and compliance with upcoming regulatory standards, and showing promise for broader applications in healthcare machine learning."],"url":"http://arxiv.org/abs/2404.11148v1","category":"cs.AI"}
{"created":"2024-04-17 07:47:35","title":"On the use of time series to improve signal processing of electrical data","abstract":"Electrical Resistivity Tomography (ERT) has become widely used for engineering and environmental applications in the last couple of decades due to (1) the simplification and automating of resistivity meters and (2) the new generation of inversion software. Although the initial domain of application remain relevant today, these techniques are increasingly used for deep investigation and shallow environmental/geotechnical applications. These new applications involve working with complex resistivity, which can be measured in the time domain (TDIP: Time Domain Induce Polarization) or frequency domain (SIP: Spectral Induce Polarization). The new algorithm could process data in time and frequency domain. The first step is to estimate the no linear self-potential in order to remove it. Second step is to compute resistivity and chargeability in time or frequency domain. The results show that the improvement of the processing is weak for the resistivity and for no noisy data but we have a better estimation of chargeability. The correlation highlight a good correlation between calculated out-phasing (in frequency domain) and chargeability (in time domain).","sentences":["Electrical Resistivity Tomography (ERT) has become widely used for engineering and environmental applications in the last couple of decades due to (1) the simplification and automating of resistivity meters and (2) the new generation of inversion software.","Although the initial domain of application remain relevant today, these techniques are increasingly used for deep investigation and shallow environmental/geotechnical applications.","These new applications involve working with complex resistivity, which can be measured in the time domain (TDIP: Time Domain Induce Polarization) or frequency domain (SIP: Spectral Induce Polarization).","The new algorithm could process data in time and frequency domain.","The first step is to estimate the no linear self-potential in order to remove it.","Second step is to compute resistivity and chargeability in time or frequency domain.","The results show that the improvement of the processing is weak for the resistivity and for no noisy data but we have a better estimation of chargeability.","The correlation highlight a good correlation between calculated out-phasing (in frequency domain) and chargeability (in time domain)."],"url":"http://arxiv.org/abs/2404.11146v1","category":"physics.geo-ph"}
{"created":"2024-04-17 07:40:57","title":"Self-adaptive PSRO: Towards an Automatic Population-based Game Solver","abstract":"Policy-Space Response Oracles (PSRO) as a general algorithmic framework has achieved state-of-the-art performance in learning equilibrium policies of two-player zero-sum games. However, the hand-crafted hyperparameter value selection in most of the existing works requires extensive domain knowledge, forming the main barrier to applying PSRO to different games. In this work, we make the first attempt to investigate the possibility of self-adaptively determining the optimal hyperparameter values in the PSRO framework. Our contributions are three-fold: (1) Using several hyperparameters, we propose a parametric PSRO that unifies the gradient descent ascent (GDA) and different PSRO variants. (2) We propose the self-adaptive PSRO (SPSRO) by casting the hyperparameter value selection of the parametric PSRO as a hyperparameter optimization (HPO) problem where our objective is to learn an HPO policy that can self-adaptively determine the optimal hyperparameter values during the running of the parametric PSRO. (3) To overcome the poor performance of online HPO methods, we propose a novel offline HPO approach to optimize the HPO policy based on the Transformer architecture. Experiments on various two-player zero-sum games demonstrate the superiority of SPSRO over different baselines.","sentences":["Policy-Space Response Oracles (PSRO) as a general algorithmic framework has achieved state-of-the-art performance in learning equilibrium policies of two-player zero-sum games.","However, the hand-crafted hyperparameter value selection in most of the existing works requires extensive domain knowledge, forming the main barrier to applying PSRO to different games.","In this work, we make the first attempt to investigate the possibility of self-adaptively determining the optimal hyperparameter values in the PSRO framework.","Our contributions are three-fold: (1) Using several hyperparameters, we propose a parametric PSRO that unifies the gradient descent ascent (GDA) and different PSRO variants.","(2) We propose the self-adaptive PSRO (SPSRO) by casting the hyperparameter value selection of the parametric PSRO as a hyperparameter optimization (HPO) problem where our objective is to learn an HPO policy that can self-adaptively determine the optimal hyperparameter values during the running of the parametric PSRO.","(3) To overcome the poor performance of online HPO methods, we propose a novel offline HPO approach to optimize the HPO policy based on the Transformer architecture.","Experiments on various two-player zero-sum games demonstrate the superiority of SPSRO over different baselines."],"url":"http://arxiv.org/abs/2404.11144v1","category":"cs.AI"}
{"created":"2024-04-17 07:33:28","title":"Neutron-capture elements in a sample of field metal-poor N-rich dwarfs","abstract":"The aim of this work is to measure the abundances of n-capture elements in a sample of six metal-poor N-rich dwarfs that were formed in globular clusters, and subsequently became unbound from the cluster. These N-rich stars, HD 25329, HD 74000, HD 160617, G 24-3, G 53-41, and G 90-3, were previously studied in Paper I. The abundances of the n-capture elements in these stars were compared to the abundances in normal metal-poor dwarfs and in globular cluster stars in the same metallicity range in order to find evidence of an enrichment of the material from which these N-rich stars were formed, by the ejecta of massive asymptotic giant branch stars (AGB) inside the cluster.The abundances of 15 elements, from Sr to Yb, were derived line by line by comparing the observed profiles to synthetic spectra in a sample of six metal-poor N-rich dwarfs and nine classical metal-poor dwarfs. We show that, generally speaking, the behaviours of the intermediate metal-poor stars here studied and the extremely metal-poor stars are very different. In particular, the scatter of the [X/Fe] ratios is much smaller since many more stars contribute to the enrichment.Among our six metal-poor N-rich stars, three stars (G24-3 and HD 74000 and maybe also HD 160617) present an enrichment in elements formed by the s-process, typical of a contribution of AGB stars, unexpected at the metallicity of these stars. This suggests that the intracluster medium from which these stars were formed was enriched by a first generation of massive AGB stars. Another N-rich star, G53-41, is also rich in s-process elements, but since it is more metal-rich this could be due to the normal galactic enrichment by low-mass AGB stars before the formation of the cluster. In contrast, two stars (G90-3 and HD 25329) have an abundance pattern compatible with a pure r-process such as that seen in metal-poor stars with [Fe/H]<--1.5.","sentences":["The aim of this work is to measure the abundances of n-capture elements in a sample of six metal-poor N-rich dwarfs that were formed in globular clusters, and subsequently became unbound from the cluster.","These N-rich stars, HD 25329, HD 74000, HD 160617, G 24-3, G 53-41, and G 90-3, were previously studied in Paper I.","The abundances of the n-capture elements in these stars were compared to the abundances in normal metal-poor dwarfs and in globular cluster stars in the same metallicity range in order to find evidence of an enrichment of the material from which these N-rich stars were formed, by the ejecta of massive asymptotic giant branch stars (AGB) inside the cluster.","The abundances of 15 elements, from Sr to Yb, were derived line by line by comparing the observed profiles to synthetic spectra in a sample of six metal-poor N-rich dwarfs and nine classical metal-poor dwarfs.","We show that, generally speaking, the behaviours of the intermediate metal-poor stars here studied and the extremely metal-poor stars are very different.","In particular, the scatter of the [X/Fe] ratios is much smaller since many more stars contribute to the enrichment.","Among our six metal-poor N-rich stars, three stars (G24-3 and HD 74000 and maybe also HD 160617) present an enrichment in elements formed by the s-process, typical of a contribution of AGB stars, unexpected at the metallicity of these stars.","This suggests that the intracluster medium from which these stars were formed was enriched by a first generation of massive AGB stars.","Another N-rich star, G53-41, is also rich in s-process elements, but since it is more metal-rich this could be due to the normal galactic enrichment by low-mass AGB stars before the formation of the cluster.","In contrast, two stars (G90-3 and HD 25329) have an abundance pattern compatible with a pure r-process such as that seen in metal-poor stars with [Fe/H]<--1.5."],"url":"http://arxiv.org/abs/2404.11138v1","category":"astro-ph.SR"}
{"created":"2024-04-17 07:31:21","title":"Reconstruction of the local contractility of the cardiac muscle from deficient apparent kinematics","abstract":"Active solids are a large class of materials, including both living soft tissues and artificial matter, that share the ability to undergo strain even in absence of external loads. While in engineered materials the actuation is typically designed a priori, in natural materials it is an unknown of the problem. In such a framework, the identification of inactive regions in active materials is of particular interest. An example of paramount relevance is cardiac mechanics and the assessment of regions of the cardiac muscle with impaired contractility. The impossibility to measure the local active forces directly, suggests us to develop a novel methodology exploiting kinematic data from clinical images by a variational approach to reconstruct the local contractility in the cardiac muscle. Introducing a suitable cost functional and effective regularization methods, we minimize the discrepancy between observed and simulated displacement and we recover the contractility map of the muscle. Numerical experiments, including severe conditions with added noise to model uncertainties, and data knowledge limited to the boundary, demonstrate the effectiveness of our approach. Unlike other methods, we provide a spatially continuous recovery of the contractility map at a low computational cost.","sentences":["Active solids are a large class of materials, including both living soft tissues and artificial matter, that share the ability to undergo strain even in absence of external loads.","While in engineered materials the actuation is typically designed a priori, in natural materials it is an unknown of the problem.","In such a framework, the identification of inactive regions in active materials is of particular interest.","An example of paramount relevance is cardiac mechanics and the assessment of regions of the cardiac muscle with impaired contractility.","The impossibility to measure the local active forces directly, suggests us to develop a novel methodology exploiting kinematic data from clinical images by a variational approach to reconstruct the local contractility in the cardiac muscle.","Introducing a suitable cost functional and effective regularization methods, we minimize the discrepancy between observed and simulated displacement and we recover the contractility map of the muscle.","Numerical experiments, including severe conditions with added noise to model uncertainties, and data knowledge limited to the boundary, demonstrate the effectiveness of our approach.","Unlike other methods, we provide a spatially continuous recovery of the contractility map at a low computational cost."],"url":"http://arxiv.org/abs/2404.11137v1","category":"physics.med-ph"}
{"created":"2024-04-17 17:34:52","title":"Study of Entropy-Driven Polymorphic Stability for Aspirin Using Accurate Neural Network Interatomic Potential","abstract":"In this study, we present a systematic computational investigation to analyze the long debated crystal stability of two well known aspirin polymorphs, labeled as Form I and Form II. Specifically, we developed a strategy to collect training configurations covering diverse interatomic interactions between representative functional groups in the aspirin crystals. Utilizing a state-of-the-art neural network interatomic potential (NNIP) model, we developed an accurate machine learning potential to simulate aspirin crystal dynamics under finite temperature conditions with $\\sim$0.46 kJ/mol/molecule accuracy. Employing the trained NNIP model, we performed thermodynamic integration to assess the free energy difference between aspirin Forms I and II, accounting for the anharmonic effects in a large supercell consisting of 512 molecules. For the first time, our results convincingly demonstrated that Form I is more stable than Form II at 300 K, ranging from 0.74 to 1.83 kJ/mol/molecule, aligning with the experimental observations. Unlike the majority of previous simulations based on (quasi)harmonic approximations in a small super cell, which often found the degenerate energies between aspirin I and II, our findings underscore the importance of anharmonic effects in determining polymorphic stability ranking. Furthermore, we proposed the use of rotational degrees of freedom of methyl and ester/phenyl groups in the aspirin crystal, as characteristic motions to highlight rotational entropic contribution that favors the stability of Form I. Beyond the aspirin polymorphism, we anticipate that such entropy-driven stabilization can be broadly applicable to many other organic systems and thus our approach, suggesting our approach holds a great promise for stability studies in small molecule drug design.","sentences":["In this study, we present a systematic computational investigation to analyze the long debated crystal stability of two well known aspirin polymorphs, labeled as Form I and Form II.","Specifically, we developed a strategy to collect training configurations covering diverse interatomic interactions between representative functional groups in the aspirin crystals.","Utilizing a state-of-the-art neural network interatomic potential (NNIP) model, we developed an accurate machine learning potential to simulate aspirin crystal dynamics under finite temperature conditions with $\\sim$0.46 kJ/mol/molecule accuracy.","Employing the trained NNIP model, we performed thermodynamic integration to assess the free energy difference between aspirin Forms I and II, accounting for the anharmonic effects in a large supercell consisting of 512 molecules.","For the first time, our results convincingly demonstrated that Form I is more stable than Form II at 300 K, ranging from 0.74 to 1.83 kJ/mol/molecule, aligning with the experimental observations.","Unlike the majority of previous simulations based on (quasi)harmonic approximations in a small super cell, which often found the degenerate energies between aspirin I and II, our findings underscore the importance of anharmonic effects in determining polymorphic stability ranking.","Furthermore, we proposed the use of rotational degrees of freedom of methyl and ester/phenyl groups in the aspirin crystal, as characteristic motions to highlight rotational entropic contribution that favors the stability of Form I. Beyond the aspirin polymorphism, we anticipate that such entropy-driven stabilization can be broadly applicable to many other organic systems and thus our approach, suggesting our approach holds a great promise for stability studies in small molecule drug design."],"url":"http://arxiv.org/abs/2404.11587v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-17 16:14:56","title":"Uncertainty estimation and anomaly detection in chiral effective field theory studies of key nuclear electroweak processes","abstract":"Chiral effective field theory ($\\chi$EFT) is a powerful tool for studying electroweak processes in nuclei. I discuss $\\chi$EFT calculations of three key nuclear electroweak processes: primordial deuterium production, proton-proton fusion, and magnetic dipole excitations of $^{48}\\mathrm{Ca}$. This article showcases $\\chi$EFT's ability to quantify theory uncertainties at the appropriate level of rigor for addressing the different precision demands of these three processes.","sentences":["Chiral effective field theory ($\\chi$EFT) is a powerful tool for studying electroweak processes in nuclei.","I discuss $\\chi$EFT calculations of three key nuclear electroweak processes: primordial deuterium production, proton-proton fusion, and magnetic dipole excitations of $^{48}\\mathrm{Ca}$. This article showcases $\\chi$EFT's ability to quantify theory uncertainties at the appropriate level of rigor for addressing the different precision demands of these three processes."],"url":"http://arxiv.org/abs/2404.11522v1","category":"nucl-th"}
{"created":"2024-04-17 15:16:01","title":"A Large-scale Fine-grained Analysis of Packages in Open-Source Software Ecosystems","abstract":"Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages. The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages. In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions. Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages. Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages. Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance.","sentences":["Package managers such as NPM, Maven, and PyPI play a pivotal role in open-source software (OSS) ecosystems, streamlining the distribution and management of various freely available packages.","The fine-grained details within software packages can unveil potential risks within existing OSS ecosystems, offering valuable insights for detecting malicious packages.","In this study, we undertake a large-scale empirical analysis focusing on fine-grained information (FGI): the metadata, static, and dynamic functions.","Specifically, we investigate the FGI usage across a diverse set of 50,000+ legitimate and 1,000+ malicious packages.","Based on this diverse data collection, we conducted a comparative analysis between legitimate and malicious packages.","Our findings reveal that (1) malicious packages have less metadata content and utilize fewer static and dynamic functions than legitimate ones; (2) malicious packages demonstrate a higher tendency to invoke HTTP/URL functions as opposed to other application services, such as FTP or SMTP; (3) FGI serves as a distinguishable indicator between legitimate and malicious packages; and (4) one dimension in FGI has sufficient distinguishable capability to detect malicious packages, and combining all dimensions in FGI cannot significantly improve overall performance."],"url":"http://arxiv.org/abs/2404.11467v1","category":"cs.SE"}
{"created":"2024-04-17 14:07:22","title":"RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled Neural Rendering","abstract":"We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available.","sentences":["We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images.","RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene.","Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation.","Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components.","Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance.","The constructed high-quality dataset and source code will be publicly available."],"url":"http://arxiv.org/abs/2404.11401v1","category":"cs.CV"}
{"created":"2024-04-17 13:21:04","title":"Sinking an Algorithmic Isthmus: (1 + \u03b5)-Approximate Min-Sum Subset Convolution","abstract":"Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq [n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) + g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms. However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$. In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring. This is disadvantageous due to the dependence on $M$, limiting its practicality.   In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution. To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals. We also discuss two other applications in computational biology.   Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings. In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki [STOC 2019] to the context of subset convolutions.","sentences":["Given functions $f$ and $g$ defined on the subset lattice of order $n$, their min-sum subset convolution, defined for all $S \\subseteq","[n]$ as \\[   (f \\star g)(S) = \\min_{T \\subseteq S}\\:\\big(f(T) +","g(S \\setminus T)\\big), \\] is a fundamental tool in parameterized algorithms.","However, since its na\\\"ive $O(3^n)$-time evaluation is also the fastest known, it has been used only in settings where the input functions have a bounded integer range $\\{-M, \\ldots, M\\}$.","In this case, the running time becomes $\\tilde O(2^n M)$ by resorting to fast subset convolution in the sum-product ring.","This is disadvantageous due to the dependence on $M$, limiting its practicality.   ","In this light, we study whether the problem admits an $(1 + \\varepsilon)$-approximation scheme in time independent of $M$. Our main result is the first $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$-time algorithm for the $(1 + \\varepsilon)$-approximate min-sum subset convolution.","To show its applicability, we present $(1 + \\varepsilon)$-approximation schemes in the same exponential time bound for several NP-hard problems using this convolution, such as the minimum-cost $k$-coloring problem -- in time $\\tilde O(2^\\frac{3n}{2} / \\sqrt{\\varepsilon})$, and the prize-collecting Steiner tree problem -- in time $\\tilde O(2^\\frac{3s^+}{2} / \\sqrt{\\varepsilon})$, where $n$ is the number of vertices and $s^+$ is the number of proper potential terminals.","We also discuss two other applications in computational biology.   ","Our algorithms lie at the intersection of two lines of research that have been considered separately: $\\textit{sequence}$ and $\\textit{subset}$ convolutions in semi-rings.","In particular, we extend the recent framework of Bringmann, K\\\"unnemann, and W\\k{e}grzycki","[STOC 2019] to the context of subset convolutions."],"url":"http://arxiv.org/abs/2404.11364v1","category":"cs.DS"}
{"created":"2024-04-17 12:56:38","title":"Global topological synchronization of weighted simplicial complexes","abstract":"Higher-order networks are able to capture the many-body interactions present in complex systems and to unveil new fundamental phenomena revealing the rich interplay between topology, geometry, and dynamics. Simplicial complexes are higher-order networks that encode higher-order topology and dynamics of complex systems. Specifically, simplicial complexes can sustain topological signals, i.e., dynamical variables not only defined on nodes of the network but also on their edges, triangles, and so on. Topological signals can undergo collective phenomena such as synchronization, however, only some higher-order network topologies can sustain global synchronization of topological signals. Here we consider global topological synchronization of topological signals on weighted simplicial complexes. We demonstrate that topological signals can globally synchronize on weighted simplicial complexes, even if they are odd-dimensional, e.g., edge signals, overcoming thus a limitation of the unweighted case. These results thus demonstrate that weighted simplicial complexes are more advantageous for observing these collective phenomena than their unweighted counterpart. In particular, we present two weighted simplicial complexes the Weighted Triangulated Torus and the Weighted Waffle. We completely characterize their higher-order spectral properties and we demonstrate that, under suitable conditions on their weights, they can sustain global synchronization of edge signals. Our results are interpreted geometrically by showing, among the other results, that in some cases edge weights can be associated with the lengths of the sides of curved simplices.","sentences":["Higher-order networks are able to capture the many-body interactions present in complex systems and to unveil new fundamental phenomena revealing the rich interplay between topology, geometry, and dynamics.","Simplicial complexes are higher-order networks that encode higher-order topology and dynamics of complex systems.","Specifically, simplicial complexes can sustain topological signals, i.e., dynamical variables not only defined on nodes of the network but also on their edges, triangles, and so on.","Topological signals can undergo collective phenomena such as synchronization, however, only some higher-order network topologies can sustain global synchronization of topological signals.","Here we consider global topological synchronization of topological signals on weighted simplicial complexes.","We demonstrate that topological signals can globally synchronize on weighted simplicial complexes, even if they are odd-dimensional, e.g., edge signals, overcoming thus a limitation of the unweighted case.","These results thus demonstrate that weighted simplicial complexes are more advantageous for observing these collective phenomena than their unweighted counterpart.","In particular, we present two weighted simplicial complexes the Weighted Triangulated Torus and the Weighted Waffle.","We completely characterize their higher-order spectral properties and we demonstrate that, under suitable conditions on their weights, they can sustain global synchronization of edge signals.","Our results are interpreted geometrically by showing, among the other results, that in some cases edge weights can be associated with the lengths of the sides of curved simplices."],"url":"http://arxiv.org/abs/2404.11337v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-17 12:46:21","title":"Quantum walks and entanglement in cavity networks","abstract":"For harnessing the full potential of quantum phenomena, light-matter interfaces and complexly connected quantum networks are required, relying on the joint quantum operation of different physical platforms. In this work, we analyze the quantum properties of multipartite quantum systems, consisting of an arbitrarily large collection of optical cavities with two-level atoms. In particular, we explore quantum walks in such systems and determine the resulting entanglement. Realistic imperfections are included in the model as optical losses and spontaneous decays of atoms. The topology of torus and the non-orientable M\\\"obius strip serve as examples of complex networks we consider, demonstrating the versatility of our approach and resulting in interesting quantum dynamics and interference effects for quantum simulation applications.","sentences":["For harnessing the full potential of quantum phenomena, light-matter interfaces and complexly connected quantum networks are required, relying on the joint quantum operation of different physical platforms.","In this work, we analyze the quantum properties of multipartite quantum systems, consisting of an arbitrarily large collection of optical cavities with two-level atoms.","In particular, we explore quantum walks in such systems and determine the resulting entanglement.","Realistic imperfections are included in the model as optical losses and spontaneous decays of atoms.","The topology of torus and the non-orientable M\\\"obius strip serve as examples of complex networks we consider, demonstrating the versatility of our approach and resulting in interesting quantum dynamics and interference effects for quantum simulation applications."],"url":"http://arxiv.org/abs/2404.11331v1","category":"quant-ph"}
{"created":"2024-04-17 12:35:36","title":"Bayesian Optimization for Identification of Optimal Biological Dose Combinations in Personalized Dose-Finding Trials","abstract":"Early phase, personalized dose-finding trials for combination therapies seek to identify patient-specific optimal biological dose (OBD) combinations, which are defined as safe dose combinations which maximize therapeutic benefit for a specific covariate pattern. Given the small sample sizes which are typical of these trials, it is challenging for traditional parametric approaches to identify OBD combinations across multiple dosing agents and covariate patterns. To address these challenges, we propose a Bayesian optimization approach to dose-finding which formally incorporates toxicity information into both the initial data collection process and the sequential search strategy. Independent Gaussian processes are used to model the efficacy and toxicity surfaces, and an acquisition function is utilized to define the dose-finding strategy and an early stopping rule. This work is motivated by a personalized dose-finding trial which considers a dual-agent therapy for obstructive sleep apnea, where OBD combinations are tailored to obstructive sleep apnea severity. To compare the performance of the personalized approach to a standard approach where covariate information is ignored, a simulation study is performed. We conclude that personalized dose-finding is essential in the presence of heterogeneity.","sentences":["Early phase, personalized dose-finding trials for combination therapies seek to identify patient-specific optimal biological dose (OBD) combinations, which are defined as safe dose combinations which maximize therapeutic benefit for a specific covariate pattern.","Given the small sample sizes which are typical of these trials, it is challenging for traditional parametric approaches to identify OBD combinations across multiple dosing agents and covariate patterns.","To address these challenges, we propose a Bayesian optimization approach to dose-finding which formally incorporates toxicity information into both the initial data collection process and the sequential search strategy.","Independent Gaussian processes are used to model the efficacy and toxicity surfaces, and an acquisition function is utilized to define the dose-finding strategy and an early stopping rule.","This work is motivated by a personalized dose-finding trial which considers a dual-agent therapy for obstructive sleep apnea, where OBD combinations are tailored to obstructive sleep apnea severity.","To compare the performance of the personalized approach to a standard approach where covariate information is ignored, a simulation study is performed.","We conclude that personalized dose-finding is essential in the presence of heterogeneity."],"url":"http://arxiv.org/abs/2404.11323v1","category":"stat.ME"}
{"created":"2024-04-17 12:34:49","title":"VBR: A Vision Benchmark in Rome","abstract":"This paper presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision. This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency. It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization. During recording, we cover multi-floor buildings, gardens, urban and highway scenarios. Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment. All sequences divided in training and testing are accessible through our website.","sentences":["This paper presents a vision and perception research dataset collected in Rome, featuring RGB data, 3D point clouds, IMU, and GPS data.","We introduce a new benchmark targeting visual odometry and SLAM, to advance the research in autonomous robotics and computer vision.","This work complements existing datasets by simultaneously addressing several issues, such as environment diversity, motion patterns, and sensor frequency.","It uses up-to-date devices and presents effective procedures to accurately calibrate the intrinsic and extrinsic of the sensors while addressing temporal synchronization.","During recording, we cover multi-floor buildings, gardens, urban and highway scenarios.","Combining handheld and car-based data collections, our setup can simulate any robot (quadrupeds, quadrotors, autonomous vehicles).","The dataset includes an accurate 6-dof ground truth based on a novel methodology that refines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment.","All sequences divided in training and testing are accessible through our website."],"url":"http://arxiv.org/abs/2404.11322v1","category":"cs.CV"}
{"created":"2024-04-17 12:32:10","title":"Leveraging Fine-Grained Information and Noise Decoupling for Remote Sensing Change Detection","abstract":"Change detection aims to identify remote sense object changes by analyzing data between bitemporal image pairs. Due to the large temporal and spatial span of data collection in change detection image pairs, there are often a significant amount of task-specific and task-agnostic noise. Previous effort has focused excessively on denoising, with this goes a great deal of loss of fine-grained information. In this paper, we revisit the importance of fine-grained features in change detection and propose a series of operations for fine-grained information compensation and noise decoupling (FINO). First, the context is utilized to compensate for the fine-grained information in the feature space. Next, a shape-aware and a brightness-aware module are designed to improve the capacity for representation learning. The shape-aware module guides the backbone for more precise shape estimation, guiding the backbone network in extracting object shape features. The brightness-aware module learns a overall brightness estimation to improve the model's robustness to task-agnostic noise. Finally, a task-specific noise decoupling structure is designed as a way to improve the model's ability to separate noise interference from feature similarity. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in multiple change detection benchmarks. The code will be made available.","sentences":["Change detection aims to identify remote sense object changes by analyzing data between bitemporal image pairs.","Due to the large temporal and spatial span of data collection in change detection image pairs, there are often a significant amount of task-specific and task-agnostic noise.","Previous effort has focused excessively on denoising, with this goes a great deal of loss of fine-grained information.","In this paper, we revisit the importance of fine-grained features in change detection and propose a series of operations for fine-grained information compensation and noise decoupling (FINO).","First, the context is utilized to compensate for the fine-grained information in the feature space.","Next, a shape-aware and a brightness-aware module are designed to improve the capacity for representation learning.","The shape-aware module guides the backbone for more precise shape estimation, guiding the backbone network in extracting object shape features.","The brightness-aware module learns a overall brightness estimation to improve the model's robustness to task-agnostic noise.","Finally, a task-specific noise decoupling structure is designed as a way to improve the model's ability to separate noise interference from feature similarity.","With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in multiple change detection benchmarks.","The code will be made available."],"url":"http://arxiv.org/abs/2404.11318v1","category":"cs.CV"}
{"created":"2024-04-17 12:26:52","title":"To Drop or Not to Drop? Predicting Argument Ellipsis Judgments: A Case Study in Japanese","abstract":"Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages. This study addresses a question about ellipsis -- what can explain the native speakers' ellipsis decisions? -- motivated by the interest in human discourse processing and writing assistance for this choice. To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language. The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus. Furthermore, the performance of the language model-based argument ellipsis judgment model is examined, and the gap between the systems' prediction and human judgments in specific linguistic aspects is revealed. We hope our fundamental resource encourages further studies on natural human ellipsis judgment.","sentences":["Speakers sometimes omit certain arguments of a predicate in a sentence; such omission is especially frequent in pro-drop languages.","This study addresses a question about ellipsis -- what can explain the native speakers' ellipsis decisions?","-- motivated by the interest in human discourse processing and writing assistance for this choice.","To this end, we first collect large-scale human annotations of whether and why a particular argument should be omitted across over 2,000 data points in the balanced corpus of Japanese, a prototypical pro-drop language.","The data indicate that native speakers overall share common criteria for such judgments and further clarify their quantitative characteristics, e.g., the distribution of related linguistic factors in the balanced corpus.","Furthermore, the performance of the language model-based argument ellipsis judgment model is examined, and the gap between the systems' prediction and human judgments in specific linguistic aspects is revealed.","We hope our fundamental resource encourages further studies on natural human ellipsis judgment."],"url":"http://arxiv.org/abs/2404.11315v1","category":"cs.CL"}
{"created":"2024-04-17 12:13:49","title":"Nanojet Visualization and Dark-field Imaging of Optically Trapped Vaterite Capsules with Endoscopic Illumination","abstract":"Optical responsivity grants biomedical capsules additional capabilities, promoting them towards multifunctional theragnostic nanodevices. In this endeavor, screening candidates under conditions that closely resemble in situ environments is crucial for both the initial optimization and the subsequent inspection stages of development and operation. Optical tweezers equipped with dark-field spectroscopy are among the preferable tools for nanoparticle imaging and refractometry. However, the effectiveness of conventional illumination and light collection arrangements for inspecting anisotropic complex inner composition particles is quite limited due to reduced collection angles, which can result in the omission of features in scattering diagrams. Here we introduce an endoscopic dark-field illumination scheme, where light is launched on an optically trapped particle from a single-mode fiber, immersed into a fluid cell. This arrangement disentangles illumination and collection paths, thus allowing the collection of scattered light with a very high numerical aperture. This methodology is applied to vaterite nanocapsules, which are known to possess strong anisotropic responses. Tweezer configuration allows revealing optical properties for different crystallographic orientations of vaterite, which is complex to do otherwise. Furthermore, endoscopic dark-field images reveal the emergence of polarization-dependent long-range photonic nanojets, which are capable of interacting with nearby particles, demonstrating a new pathway for nanojet image formation.","sentences":["Optical responsivity grants biomedical capsules additional capabilities, promoting them towards multifunctional theragnostic nanodevices.","In this endeavor, screening candidates under conditions that closely resemble in situ environments is crucial for both the initial optimization and the subsequent inspection stages of development and operation.","Optical tweezers equipped with dark-field spectroscopy are among the preferable tools for nanoparticle imaging and refractometry.","However, the effectiveness of conventional illumination and light collection arrangements for inspecting anisotropic complex inner composition particles is quite limited due to reduced collection angles, which can result in the omission of features in scattering diagrams.","Here we introduce an endoscopic dark-field illumination scheme, where light is launched on an optically trapped particle from a single-mode fiber, immersed into a fluid cell.","This arrangement disentangles illumination and collection paths, thus allowing the collection of scattered light with a very high numerical aperture.","This methodology is applied to vaterite nanocapsules, which are known to possess strong anisotropic responses.","Tweezer configuration allows revealing optical properties for different crystallographic orientations of vaterite, which is complex to do otherwise.","Furthermore, endoscopic dark-field images reveal the emergence of polarization-dependent long-range photonic nanojets, which are capable of interacting with nearby particles, demonstrating a new pathway for nanojet image formation."],"url":"http://arxiv.org/abs/2404.11303v1","category":"physics.optics"}
{"created":"2024-04-17 10:56:06","title":"A Progressive Framework of Vision-language Knowledge Distillation and Alignment for Multilingual Scene","abstract":"Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks. However, most of them are only applicable to the English context. Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages. Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices. In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context. In this framework, we collect high-quality Chinese and English text-image pairs and design two training stages, including multilingual vision-language feature distillation and alignment. During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively. Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model's multilingual performance. Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude. The evaluation demonstrates the effectiveness of our designed training mechanism.","sentences":["Pre-trained vision-language (V-L) models such as CLIP have shown excellent performance in many downstream cross-modal tasks.","However, most of them are only applicable to the English context.","Subsequent research has focused on this problem and proposed improved models, such as CN-CLIP and AltCLIP, to facilitate their applicability to Chinese and even other languages.","Nevertheless, these models suffer from high latency and a large memory footprint in inference, which limits their further deployment on resource-constrained edge devices.","In this work, we propose a conceptually simple yet effective multilingual CLIP Compression framework and train a lightweight multilingual vision-language model, called DC-CLIP, for both Chinese and English context.","In this framework, we collect high-quality Chinese and English text-image pairs and design two training stages, including multilingual vision-language feature distillation and alignment.","During the first stage, lightweight image/text student models are designed to learn robust visual/multilingual textual feature representation ability from corresponding teacher models, respectively.","Subsequently, the multilingual vision-language alignment stage enables effective alignment of visual and multilingual textual features to further improve the model's multilingual performance.","Comprehensive experiments in zero-shot image classification, conducted based on the ELEVATER benchmark, showcase that DC-CLIP achieves superior performance in the English context and competitive performance in the Chinese context, even with less training data, when compared to existing models of similar parameter magnitude.","The evaluation demonstrates the effectiveness of our designed training mechanism."],"url":"http://arxiv.org/abs/2404.11249v1","category":"cs.CV"}
{"created":"2024-04-17 07:39:57","title":"Functional Brain-to-Brain Transformation with No Shared Data","abstract":"Combining Functional MRI (fMRI) data across different subjects and datasets is crucial for many neuroscience tasks. Relying solely on shared anatomy for brain-to-brain mapping is inadequate. Existing functional transformation methods thus depend on shared stimuli across subjects and fMRI datasets, which are often unavailable. In this paper, we propose an approach for computing functional brain-to-brain transformations without any shared data, a feat not previously achieved in functional transformations. This presents exciting research prospects for merging and enriching diverse datasets, even when they involve distinct stimuli that were collected using different fMRI machines of varying resolutions (e.g., 3-Tesla and 7-Tesla). Our approach combines brain-to-brain transformation with image-to-fMRI encoders, thus enabling to learn functional transformations on stimuli to which subjects were never exposed. Furthermore, we demonstrate the applicability of our method for improving image-to-fMRI encoding of subjects scanned on older low-resolution 3T fMRI datasets, by using a new high-resolution 7T fMRI dataset (scanned on different subjects and different stimuli).","sentences":["Combining Functional MRI (fMRI) data across different subjects and datasets is crucial for many neuroscience tasks.","Relying solely on shared anatomy for brain-to-brain mapping is inadequate.","Existing functional transformation methods thus depend on shared stimuli across subjects and fMRI datasets, which are often unavailable.","In this paper, we propose an approach for computing functional brain-to-brain transformations without any shared data, a feat not previously achieved in functional transformations.","This presents exciting research prospects for merging and enriching diverse datasets, even when they involve distinct stimuli that were collected using different fMRI machines of varying resolutions (e.g., 3-Tesla and 7-Tesla).","Our approach combines brain-to-brain transformation with image-to-fMRI encoders, thus enabling to learn functional transformations on stimuli to which subjects were never exposed.","Furthermore, we demonstrate the applicability of our method for improving image-to-fMRI encoding of subjects scanned on older low-resolution 3T fMRI datasets, by using a new high-resolution 7T fMRI dataset (scanned on different subjects and different stimuli)."],"url":"http://arxiv.org/abs/2404.11143v1","category":"q-bio.NC"}
{"created":"2024-04-17 07:30:34","title":"On the Performance of RIS-assisted Networks with HQAM","abstract":"In this paper, we investigate the application of hexagonal quadrature amplitude modulation (HQAM) in reconfigurable intelligent surface (RIS)-assisted networks, specifically focusing on its efficiency in reducing the number of required reflecting elements. Specifically, we present analytical expressions for the average symbol error probability (ASEP) and propose a new metric for conditioned energy efficiency, which assesses the network energy consumption while ensuring the ASEP remains below a certain threshold. Additionally, we introduce an innovative detection algorithm for HQAM constellations that implements sphere decoding in O(1) complexity. Finally, our study reveals that HQAM significantly enhances both the ASEP and energy efficiency compared to traditional quadrature amplitude modulation (QAM) schemes.","sentences":["In this paper, we investigate the application of hexagonal quadrature amplitude modulation (HQAM) in reconfigurable intelligent surface (RIS)-assisted networks, specifically focusing on its efficiency in reducing the number of required reflecting elements.","Specifically, we present analytical expressions for the average symbol error probability (ASEP) and propose a new metric for conditioned energy efficiency, which assesses the network energy consumption while ensuring the ASEP remains below a certain threshold.","Additionally, we introduce an innovative detection algorithm for HQAM constellations that implements sphere decoding in O(1) complexity.","Finally, our study reveals that HQAM significantly enhances both the ASEP and energy efficiency compared to traditional quadrature amplitude modulation (QAM) schemes."],"url":"http://arxiv.org/abs/2404.11136v1","category":"cs.IT"}
{"created":"2024-04-17 07:15:07","title":"What's under the hood: Investigating Automatic Metrics on Meeting Summarization","abstract":"Meeting summarization has become a critical task considering the increase in online interactions. While new techniques are introduced regularly, their evaluation uses metrics not designed to capture meeting-specific errors, undermining effective evaluation. This paper investigates what the frequently used automatic metrics capture and which errors they mask by correlating automatic metric scores with human evaluations across a broad error taxonomy. We commence with a comprehensive literature review on English meeting summarization to define key challenges like speaker dynamics and contextual turn-taking and error types such as missing information and linguistic inaccuracy, concepts previously loosely defined in the field. We examine the relationship between characteristic challenges and errors by using annotated transcripts and summaries from Transformer-based sequence-to-sequence and autoregressive models from the general summary QMSum dataset. Through experimental validation, we find that different model architectures respond variably to challenges in meeting transcripts, resulting in different pronounced links between challenges and errors. Current default-used metrics struggle to capture observable errors, showing weak to mid-correlations, while a third of the correlations show trends of error masking. Only a subset reacts accurately to specific errors, while most correlations show either unresponsiveness or failure to reflect the error's impact on summary quality.","sentences":["Meeting summarization has become a critical task considering the increase in online interactions.","While new techniques are introduced regularly, their evaluation uses metrics not designed to capture meeting-specific errors, undermining effective evaluation.","This paper investigates what the frequently used automatic metrics capture and which errors they mask by correlating automatic metric scores with human evaluations across a broad error taxonomy.","We commence with a comprehensive literature review on English meeting summarization to define key challenges like speaker dynamics and contextual turn-taking and error types such as missing information and linguistic inaccuracy, concepts previously loosely defined in the field.","We examine the relationship between characteristic challenges and errors by using annotated transcripts and summaries from Transformer-based sequence-to-sequence and autoregressive models from the general summary QMSum dataset.","Through experimental validation, we find that different model architectures respond variably to challenges in meeting transcripts, resulting in different pronounced links between challenges and errors.","Current default-used metrics struggle to capture observable errors, showing weak to mid-correlations, while a third of the correlations show trends of error masking.","Only a subset reacts accurately to specific errors, while most correlations show either unresponsiveness or failure to reflect the error's impact on summary quality."],"url":"http://arxiv.org/abs/2404.11124v1","category":"cs.CL"}
{"created":"2024-04-17 07:10:28","title":"Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification","abstract":"This study is part of the debate on the efficiency of large versus small language models for text classification by prompting.We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models.Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions. Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts.We developed and shared a comprehensive open-source repository that encapsulates our methodologies. This research underscores the notion that bigger isn't always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges.","sentences":["This study is part of the debate on the efficiency of large versus small language models for text classification by prompting.","We assess the performance of small language models in zero-shot text classification, challenging the prevailing dominance of large models.","Across 15 datasets, our investigation benchmarks language models from 77M to 40B parameters using different architectures and scoring functions.","Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts.","We developed and shared a comprehensive open-source repository that encapsulates our methodologies.","This research underscores the notion that bigger isn't always better, suggesting that resource-efficient small models may offer viable solutions for specific data classification challenges."],"url":"http://arxiv.org/abs/2404.11122v1","category":"cs.AI"}
{"created":"2024-04-17 07:08:45","title":"TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment","abstract":"Proprietary large language models (LLMs) have been widely applied in various scenarios. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct effective model stealing (MS) attacks. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead. To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices. The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE. The authorization module can freshly authorize each request based on its input. Extensive experiments show that TransLinkGuard achieves the same security protection as the black-box security guarantees with negligible overhead.","sentences":["Proprietary large language models (LLMs) have been widely applied in various scenarios.","Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons.","However, edge deployment of proprietary LLMs introduces new security challenges: edge-deployed models are exposed as white-box accessible to users, enabling adversaries to conduct effective model stealing (MS) attacks.","Unfortunately, existing defense mechanisms fail to provide effective protection.","Specifically, we identify four critical protection properties that existing methods fail to simultaneously satisfy: (1) maintaining protection after a model is physically copied; (2) authorizing model access at request level; (3) safeguarding runtime reverse engineering; (4) achieving high security with negligible runtime overhead.","To address the above issues, we propose TransLinkGuard, a plug-and-play model protection approach against model stealing on edge devices.","The core part of TransLinkGuard is a lightweight authorization module residing in a secure environment, e.g., TEE.","The authorization module can freshly authorize each request based on its input.","Extensive experiments show that TransLinkGuard achieves the same security protection as the black-box security guarantees with negligible overhead."],"url":"http://arxiv.org/abs/2404.11121v1","category":"cs.CR"}
{"created":"2024-04-17 07:01:41","title":"Variational quantization for state space models","abstract":"Forecasting tasks using large datasets gathering thousands of heterogeneous time series is a crucial statistical problem in numerous sectors. The main challenge is to model a rich variety of time series, leverage any available external signals and provide sharp predictions with statistical guarantees. In this work, we propose a new forecasting model that combines discrete state space hidden Markov models with recent neural network architectures and training procedures inspired by vector quantized variational autoencoders. We introduce a variational discrete posterior distribution of the latent states given the observations and a two-stage training procedure to alternatively train the parameters of the latent states and of the emission distributions. By learning a collection of emission laws and temporarily activating them depending on the hidden process dynamics, the proposed method allows to explore large datasets and leverage available external signals. We assess the performance of the proposed method using several datasets and show that it outperforms other state-of-the-art solutions.","sentences":["Forecasting tasks using large datasets gathering thousands of heterogeneous time series is a crucial statistical problem in numerous sectors.","The main challenge is to model a rich variety of time series, leverage any available external signals and provide sharp predictions with statistical guarantees.","In this work, we propose a new forecasting model that combines discrete state space hidden Markov models with recent neural network architectures and training procedures inspired by vector quantized variational autoencoders.","We introduce a variational discrete posterior distribution of the latent states given the observations and a two-stage training procedure to alternatively train the parameters of the latent states and of the emission distributions.","By learning a collection of emission laws and temporarily activating them depending on the hidden process dynamics, the proposed method allows to explore large datasets and leverage available external signals.","We assess the performance of the proposed method using several datasets and show that it outperforms other state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.11117v1","category":"cs.LG"}
{"created":"2024-04-17 07:01:29","title":"Music Enhancement with Deep Filters: A Technical Report for The ICASSP 2024 Cadenza Challenge","abstract":"In this challenge, we disentangle the deep filters from the original DeepfilterNet and incorporate them into our Spec-UNet-based network to further improve a hybrid Demucs (hdemucs) based remixing pipeline. The motivation behind the use of the deep filter component lies at its potential in better handling temporal fine structures. We demonstrate an incremental improvement in both the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality Index (HAAQI) metrics when comparing the performance of hdemucs against different versions of our model.","sentences":["In this challenge, we disentangle the deep filters from the original DeepfilterNet and incorporate them into our Spec-UNet-based network to further improve a hybrid Demucs (hdemucs) based remixing pipeline.","The motivation behind the use of the deep filter component lies at its potential in better handling temporal fine structures.","We demonstrate an incremental improvement in both the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality Index (HAAQI) metrics when comparing the performance of hdemucs against different versions of our model."],"url":"http://arxiv.org/abs/2404.11116v1","category":"cs.SD"}
{"created":"2024-04-17 07:00:20","title":"Reuse out-of-year data to enhance land cover mappingvia feature disentanglement and contrastive learning","abstract":"Timely up-to-date land use/land cover (LULC) maps play a pivotal role in supporting agricultural territory management, environmental monitoring and facilitating well-informed and sustainable decision-making. Typically, when creating a land cover (LC) map, precise ground truth data is collected through time-consuming and expensive field campaigns. This data is then utilized in conjunction with satellite image time series (SITS) through advanced machine learning algorithms to get the final map. Unfortunately, each time this process is repeated (e.g., annually over a region to estimate agricultural production or potential biodiversity loss), new ground truth data must be collected, leading to the complete disregard of previously gathered reference data despite the substantial financial and time investment they have required. How to make value of historical data, from the same or similar study sites, to enhance the current LULC mapping process constitutes a significant challenge that could enable the financial and human-resource efforts invested in previous data campaigns to be valued again. Aiming to tackle this important challenge, we here propose a deep learning framework based on recent advances in domain adaptation and generalization to combine remote sensing and reference data coming from two different domains (e.g. historical data and fresh ones) to ameliorate the current LC mapping process. Our approach, namely REFeD (data Reuse with Effective Feature Disentanglement for land cover mapping), leverages a disentanglement strategy, based on contrastive learning, where invariant and specific per-domain features are derived to recover the intrinsic information related to the downstream LC mapping task and alleviate possible distribution shifts between domains. Additionally, REFeD is equipped with an effective supervision scheme where feature disentanglement is further enforced via multiple levels of supervision at different granularities. The experimental assessment over two study areas covering extremely diverse and contrasted landscapes, namely Koumbia (located in the West-Africa region, in Burkina Faso) and Centre Val de Loire (located in centre Europe, France), underlines the quality of our framework and the obtained findings demonstrate that out-of-year information coming from the same (or similar) study site, at different periods of time, can constitute a valuable additional source of information to enhance the LC mapping process.","sentences":["Timely up-to-date land use/land cover (LULC) maps play a pivotal role in supporting agricultural territory management, environmental monitoring and facilitating well-informed and sustainable decision-making.","Typically, when creating a land cover (LC) map, precise ground truth data is collected through time-consuming and expensive field campaigns.","This data is then utilized in conjunction with satellite image time series (SITS) through advanced machine learning algorithms to get the final map.","Unfortunately, each time this process is repeated (e.g., annually over a region to estimate agricultural production or potential biodiversity loss), new ground truth data must be collected, leading to the complete disregard of previously gathered reference data despite the substantial financial and time investment they have required.","How to make value of historical data, from the same or similar study sites, to enhance the current LULC mapping process constitutes a significant challenge that could enable the financial and human-resource efforts invested in previous data campaigns to be valued again.","Aiming to tackle this important challenge, we here propose a deep learning framework based on recent advances in domain adaptation and generalization to combine remote sensing and reference data coming from two different domains (e.g. historical data and fresh ones) to ameliorate the current LC mapping process.","Our approach, namely REFeD (data Reuse with Effective Feature Disentanglement for land cover mapping), leverages a disentanglement strategy, based on contrastive learning, where invariant and specific per-domain features are derived to recover the intrinsic information related to the downstream LC mapping task and alleviate possible distribution shifts between domains.","Additionally, REFeD is equipped with an effective supervision scheme where feature disentanglement is further enforced via multiple levels of supervision at different granularities.","The experimental assessment over two study areas covering extremely diverse and contrasted landscapes, namely Koumbia (located in the West-Africa region, in Burkina Faso) and Centre Val de Loire (located in centre Europe, France), underlines the quality of our framework and the obtained findings demonstrate that out-of-year information coming from the same (or similar) study site, at different periods of time, can constitute a valuable additional source of information to enhance the LC mapping process."],"url":"http://arxiv.org/abs/2404.11114v1","category":"cs.LG"}
{"created":"2024-04-17 06:26:32","title":"Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues","abstract":"Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which can be achieved by raising diverse, in-depth, and insightful instructions that deepen interactions. Existing methods target instructions from real instruction dialogues as a learning goal and fine-tune a user simulator for posing instructions. However, the user simulator struggles to implicitly model complex dialogue flows and pose high-quality instructions. In this paper, we take inspiration from the cognitive abilities inherent in human learning and propose the explicit modeling of complex dialogue flows through instructional strategy reuse. Specifically, we first induce high-level strategies from various real instruction dialogues. These strategies are applied to new dialogue scenarios deductively, where the instructional strategies facilitate high-quality instructions. Experimental results show that our method can generate diverse, in-depth, and insightful instructions for a given dialogue history. The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model.","sentences":["Aligning large language models (LLMs) with human expectations requires high-quality instructional dialogues, which can be achieved by raising diverse, in-depth, and insightful instructions that deepen interactions.","Existing methods target instructions from real instruction dialogues as a learning goal and fine-tune a user simulator for posing instructions.","However, the user simulator struggles to implicitly model complex dialogue flows and pose high-quality instructions.","In this paper, we take inspiration from the cognitive abilities inherent in human learning and propose the explicit modeling of complex dialogue flows through instructional strategy reuse.","Specifically, we first induce high-level strategies from various real instruction dialogues.","These strategies are applied to new dialogue scenarios deductively, where the instructional strategies facilitate high-quality instructions.","Experimental results show that our method can generate diverse, in-depth, and insightful instructions for a given dialogue history.","The constructed multi-turn instructional dialogues can outperform competitive baselines on the downstream chat model."],"url":"http://arxiv.org/abs/2404.11095v1","category":"cs.CL"}
{"created":"2024-04-17 06:17:08","title":"Neural Network Approach for Non-Markovian Dissipative Dynamics of Many-Body Open Quantum Systems","abstract":"Simulating the dynamics of open quantum systems coupled to non-Markovian environments remains an outstanding challenge due to exponentially scaling computational costs. We present an artificial intelligence strategy to overcome this obstacle by integrating the neural quantum states approach into the dissipaton-embedded quantum master equation in second quantization (DQME-SQ). Our approach utilizes restricted Boltzmann machines (RBMs) to compactly represent the reduced density tensor, explicitly encoding the combined effects of system-environment correlations and nonMarkovian memory. Applied to model systems exhibiting prominent effects of system-environment correlation and non-Markovian memory, our approach achieves comparable accuracy to conventional hierarchical equations of motion, while requiring significantly fewer dynamical variables. The novel RBM-based DQME-SQ approach paves the way for investigating non-Markovian open quantum dynamics in previously intractable regimes, with implications spanning various frontiers of modern science.","sentences":["Simulating the dynamics of open quantum systems coupled to non-Markovian environments remains an outstanding challenge due to exponentially scaling computational costs.","We present an artificial intelligence strategy to overcome this obstacle by integrating the neural quantum states approach into the dissipaton-embedded quantum master equation in second quantization (DQME-SQ).","Our approach utilizes restricted Boltzmann machines (RBMs) to compactly represent the reduced density tensor, explicitly encoding the combined effects of system-environment correlations and nonMarkovian memory.","Applied to model systems exhibiting prominent effects of system-environment correlation and non-Markovian memory, our approach achieves comparable accuracy to conventional hierarchical equations of motion, while requiring significantly fewer dynamical variables.","The novel RBM-based DQME-SQ approach paves the way for investigating non-Markovian open quantum dynamics in previously intractable regimes, with implications spanning various frontiers of modern science."],"url":"http://arxiv.org/abs/2404.11093v1","category":"quant-ph"}
{"created":"2024-04-17 05:57:17","title":"ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large Language Models","abstract":"The rapid advancement of large language models (LLMs) necessitates the development of new benchmarks to accurately assess their capabilities. To address this need for Vietnamese, this work aims to introduce ViLLM-Eval, the comprehensive evaluation suite designed to measure the advanced knowledge and reasoning abilities of foundation models within a Vietnamese context. ViLLM-Eval consists of multiple-choice questions and predict next word tasks spanning various difficulty levels and diverse disciplines, ranging from humanities to science and engineering. A thorough evaluation of the most advanced LLMs on ViLLM-Eval revealed that even the best performing models have significant room for improvement in understanding and responding to Vietnamese language tasks. ViLLM-Eval is believed to be instrumental in identifying key strengths and weaknesses of foundation models, ultimately promoting their development and enhancing their performance for Vietnamese users.","sentences":["The rapid advancement of large language models (LLMs) necessitates the development of new benchmarks to accurately assess their capabilities.","To address this need for Vietnamese, this work aims to introduce ViLLM-Eval, the comprehensive evaluation suite designed to measure the advanced knowledge and reasoning abilities of foundation models within a Vietnamese context.","ViLLM-Eval consists of multiple-choice questions and predict next word tasks spanning various difficulty levels and diverse disciplines, ranging from humanities to science and engineering.","A thorough evaluation of the most advanced LLMs on ViLLM-Eval revealed that even the best performing models have significant room for improvement in understanding and responding to Vietnamese language tasks.","ViLLM-Eval is believed to be instrumental in identifying key strengths and weaknesses of foundation models, ultimately promoting their development and enhancing their performance for Vietnamese users."],"url":"http://arxiv.org/abs/2404.11086v1","category":"cs.CL"}
{"created":"2024-04-17 05:48:58","title":"Urban traffic resilience control -- An ecological resilience perspective","abstract":"Urban traffic resilience has gained increased attention, with most studies adopting an engineering perspective that assumes a single optimal equilibrium and prioritizes local recovery. On the other hand, systems may possess multiple metastable states, and ecological resilience is the ability to switch between these states according to perturbations. Control strategies from these two resilience perspectives yield distinct outcomes. In fact, ecological resilience oriented control has rarely been viewed in urban traffic, despite the fact that traffic system is a complex system in highly uncertain environment with possible multiple metastable states. This absence highlights the necessity for urban traffic ecological resilience definition. To bridge this gap, we defines urban traffic ecological resilience as the ability to absorb uncertain perturbations by shifting to alternative states. The goal is to generate a system with greater adaptability, without necessarily returning to the original equilibrium. Our control framework comprises three aspects: portraying the recoverable scopes; designing alternative steady states; and controlling system to shift to alternative steady states for adapting large disturbances. Among them, the recoverable scopes are portrayed by attraction region; the alternative steady states are set close to the optimal state and outside the attraction region of the original equilibrium; the controller needs to ensure the local stability of the alternative steady states, without changing the trajectories inside the attraction region of the original equilibrium. Comparisons with classical engineering resilience oriented urban traffic resilience control schemes show that, proposed ecological resilience oriented control schemes can generate greater resilience. These results will contribute to the fundamental theory of future resilient intelligent transportation system.","sentences":["Urban traffic resilience has gained increased attention, with most studies adopting an engineering perspective that assumes a single optimal equilibrium and prioritizes local recovery.","On the other hand, systems may possess multiple metastable states, and ecological resilience is the ability to switch between these states according to perturbations.","Control strategies from these two resilience perspectives yield distinct outcomes.","In fact, ecological resilience oriented control has rarely been viewed in urban traffic, despite the fact that traffic system is a complex system in highly uncertain environment with possible multiple metastable states.","This absence highlights the necessity for urban traffic ecological resilience definition.","To bridge this gap, we defines urban traffic ecological resilience as the ability to absorb uncertain perturbations by shifting to alternative states.","The goal is to generate a system with greater adaptability, without necessarily returning to the original equilibrium.","Our control framework comprises three aspects: portraying the recoverable scopes; designing alternative steady states; and controlling system to shift to alternative steady states for adapting large disturbances.","Among them, the recoverable scopes are portrayed by attraction region; the alternative steady states are set close to the optimal state and outside the attraction region of the original equilibrium; the controller needs to ensure the local stability of the alternative steady states, without changing the trajectories inside the attraction region of the original equilibrium.","Comparisons with classical engineering resilience oriented urban traffic resilience control schemes show that, proposed ecological resilience oriented control schemes can generate greater resilience.","These results will contribute to the fundamental theory of future resilient intelligent transportation system."],"url":"http://arxiv.org/abs/2404.11082v1","category":"nlin.AO"}
{"created":"2024-04-17 05:38:54","title":"Recommender Systems in Financial Trading: Using machine-based conviction analysis in an explainable AI investment framework","abstract":"Traditionally, assets are selected for inclusion in a portfolio (long or short) by human analysts. Teams of human portfolio managers (PMs) seek to weigh and balance these securities using optimisation methods and other portfolio construction processes. Often, human PMs consider human analyst recommendations against the backdrop of the analyst's recommendation track record and the applicability of the analyst to the recommendation they provide. Many firms regularly ask analysts to provide a \"conviction\" level on their recommendations. In the eyes of PMs, understanding a human analyst's track record has typically come down to basic spread sheet tabulation or, at best, a \"virtual portfolio\" paper trading book to keep track of results of recommendations.   Analysts' conviction around their recommendations and their \"paper trading\" track record are two crucial workflow components between analysts and portfolio construction. Many human PMs may not even appreciate that they factor these data points into their decision-making logic. This chapter explores how Artificial Intelligence (AI) can be used to replicate these two steps and bridge the gap between AI data analytics and AI-based portfolio construction methods. This field of AI is referred to as Recommender Systems (RS). This chapter will further explore what metadata that RS systems functionally supply to downstream systems and their features.","sentences":["Traditionally, assets are selected for inclusion in a portfolio (long or short) by human analysts.","Teams of human portfolio managers (PMs) seek to weigh and balance these securities using optimisation methods and other portfolio construction processes.","Often, human PMs consider human analyst recommendations against the backdrop of the analyst's recommendation track record and the applicability of the analyst to the recommendation they provide.","Many firms regularly ask analysts to provide a \"conviction\" level on their recommendations.","In the eyes of PMs, understanding a human analyst's track record has typically come down to basic spread sheet tabulation or, at best, a \"virtual portfolio\" paper trading book to keep track of results of recommendations.   ","Analysts' conviction around their recommendations and their \"paper trading\" track record are two crucial workflow components between analysts and portfolio construction.","Many human PMs may not even appreciate that they factor these data points into their decision-making logic.","This chapter explores how Artificial Intelligence (AI) can be used to replicate these two steps and bridge the gap between AI data analytics and AI-based portfolio construction methods.","This field of AI is referred to as Recommender Systems (RS).","This chapter will further explore what metadata that RS systems functionally supply to downstream systems and their features."],"url":"http://arxiv.org/abs/2404.11080v1","category":"q-fin.PM"}
{"created":"2024-04-17 05:16:12","title":"EEG_GLT-Net: Optimising EEG Graphs for Real-time Motor Imagery Signals Classification","abstract":"Brain-Computer Interfaces connect the brain to external control devices, necessitating the accurate translation of brain signals such as from electroencephalography (EEG) into executable commands. Graph Neural Networks (GCN) have been increasingly applied for classifying EEG Motor Imagery signals, primarily because they incorporates the spatial relationships among EEG channels, resulting in improved accuracy over traditional convolutional methods. Recent advances by GCNs-Net in real-time EEG MI signal classification utilised Pearson Coefficient Correlation (PCC) for constructing adjacency matrices, yielding significant results on the PhysioNet dataset. Our paper introduces the EEG Graph Lottery Ticket (EEG_GLT) algorithm, an innovative technique for constructing adjacency matrices for EEG channels. It does not require pre-existing knowledge of inter-channel relationships, and it can be tailored to suit both individual subjects and GCN model architectures. Our findings demonstrated that the PCC method outperformed the Geodesic approach by 9.65% in mean accuracy, while our EEG_GLT matrix consistently exceeded the performance of the PCC method by a mean accuracy of 13.39%. Also, we found that the construction of the adjacency matrix significantly influenced accuracy, to a greater extent than GCN model configurations. A basic GCN configuration utilising our EEG_GLT matrix exceeded the performance of even the most complex GCN setup with a PCC matrix in average accuracy. Our EEG_GLT method also reduced MACs by up to 97% compared to the PCC method, while maintaining or enhancing accuracy. In conclusion, the EEG_GLT algorithm marks a breakthrough in the development of optimal adjacency matrices, effectively boosting both computational accuracy and efficiency, making it well-suited for real-time classification of EEG MI signals that demand intensive computational resources.","sentences":["Brain-Computer Interfaces connect the brain to external control devices, necessitating the accurate translation of brain signals such as from electroencephalography (EEG) into executable commands.","Graph Neural Networks (GCN) have been increasingly applied for classifying EEG Motor Imagery signals, primarily because they incorporates the spatial relationships among EEG channels, resulting in improved accuracy over traditional convolutional methods.","Recent advances by GCNs-Net in real-time EEG MI signal classification utilised Pearson Coefficient Correlation (PCC) for constructing adjacency matrices, yielding significant results on the PhysioNet dataset.","Our paper introduces the EEG Graph Lottery Ticket (EEG_GLT)","algorithm, an innovative technique for constructing adjacency matrices for EEG channels.","It does not require pre-existing knowledge of inter-channel relationships, and it can be tailored to suit both individual subjects and GCN model architectures.","Our findings demonstrated that the PCC method outperformed the Geodesic approach by 9.65% in mean accuracy, while our EEG_GLT matrix consistently exceeded the performance of the PCC method by a mean accuracy of 13.39%.","Also, we found that the construction of the adjacency matrix significantly influenced accuracy, to a greater extent than GCN model configurations.","A basic GCN configuration utilising our EEG_GLT matrix exceeded the performance of even the most complex GCN setup with a PCC matrix in average accuracy.","Our EEG_GLT method also reduced MACs by up to 97% compared to the PCC method, while maintaining or enhancing accuracy.","In conclusion, the EEG_GLT algorithm marks a breakthrough in the development of optimal adjacency matrices, effectively boosting both computational accuracy and efficiency, making it well-suited for real-time classification of EEG MI signals that demand intensive computational resources."],"url":"http://arxiv.org/abs/2404.11075v1","category":"cs.LG"}
{"created":"2024-04-17 05:05:05","title":"Large Language Models Meet User Interfaces: The Case of Provisioning Feedback","abstract":"Incorporating Generative AI (GenAI) and Large Language Models (LLMs) in education can enhance teaching efficiency and enrich student learning. Current LLM usage involves conversational user interfaces (CUIs) for tasks like generating materials or providing feedback. However, this presents challenges including the need for educator expertise in AI and CUIs, ethical concerns with high-stakes decisions, and privacy risks. CUIs also struggle with complex tasks. To address these, we propose transitioning from CUIs to user-friendly applications leveraging LLMs via API calls. We present a framework for ethically incorporating GenAI into educational tools and demonstrate its application in our tool, Feedback Copilot, which provides personalized feedback on student assignments. Our evaluation shows the effectiveness of this approach, with implications for GenAI researchers, educators, and technologists. This work charts a course for the future of GenAI in education.","sentences":["Incorporating Generative AI (GenAI) and Large Language Models (LLMs) in education can enhance teaching efficiency and enrich student learning.","Current LLM usage involves conversational user interfaces (CUIs) for tasks like generating materials or providing feedback.","However, this presents challenges including the need for educator expertise in AI and CUIs, ethical concerns with high-stakes decisions, and privacy risks.","CUIs also struggle with complex tasks.","To address these, we propose transitioning from CUIs to user-friendly applications leveraging LLMs via API calls.","We present a framework for ethically incorporating GenAI into educational tools and demonstrate its application in our tool, Feedback Copilot, which provides personalized feedback on student assignments.","Our evaluation shows the effectiveness of this approach, with implications for GenAI researchers, educators, and technologists.","This work charts a course for the future of GenAI in education."],"url":"http://arxiv.org/abs/2404.11072v1","category":"cs.HC"}
{"created":"2024-04-17 04:55:33","title":"ScaleFold: Reducing AlphaFold Initial Training Time to 10 Hours","abstract":"AlphaFold2 has been hailed as a breakthrough in protein folding. It can rapidly predict protein structures with lab-grade accuracy. However, its implementation does not include the necessary training code. OpenFold is the first trainable public reimplementation of AlphaFold. AlphaFold training procedure is prohibitively time-consuming, and gets diminishing benefits from scaling to more compute resources. In this work, we conducted a comprehensive analysis on the AlphaFold training procedure based on Openfold, identified that inefficient communications and overhead-dominated computations were the key factors that prevented the AlphaFold training from effective scaling. We introduced ScaleFold, a systematic training method that incorporated optimizations specifically for these factors. ScaleFold successfully scaled the AlphaFold training to 2080 NVIDIA H100 GPUs with high resource utilization. In the MLPerf HPC v3.0 benchmark, ScaleFold finished the OpenFold benchmark in 7.51 minutes, shown over $6\\times$ speedup than the baseline. For training the AlphaFold model from scratch, ScaleFold completed the pretraining in 10 hours, a significant improvement over the seven days required by the original AlphaFold pretraining baseline.","sentences":["AlphaFold2 has been hailed as a breakthrough in protein folding.","It can rapidly predict protein structures with lab-grade accuracy.","However, its implementation does not include the necessary training code.","OpenFold is the first trainable public reimplementation of AlphaFold.","AlphaFold training procedure is prohibitively time-consuming, and gets diminishing benefits from scaling to more compute resources.","In this work, we conducted a comprehensive analysis on the AlphaFold training procedure based on Openfold, identified that inefficient communications and overhead-dominated computations were the key factors that prevented the AlphaFold training from effective scaling.","We introduced ScaleFold, a systematic training method that incorporated optimizations specifically for these factors.","ScaleFold successfully scaled the AlphaFold training to 2080 NVIDIA H100 GPUs with high resource utilization.","In the MLPerf HPC v3.0 benchmark, ScaleFold finished the OpenFold benchmark in 7.51 minutes, shown over $6\\times$ speedup than the baseline.","For training the AlphaFold model from scratch, ScaleFold completed the pretraining in 10 hours, a significant improvement over the seven days required by the original AlphaFold pretraining baseline."],"url":"http://arxiv.org/abs/2404.11068v1","category":"cs.LG"}
{"created":"2024-04-17 04:46:27","title":"Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization","abstract":"3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial tasks in various 3D applications, which require both shared and complementary information in localization and visual-language relationships. Therefore, existing approaches adopt the two-stage \"detect-then-describe/discriminate\" pipeline, which relies heavily on the performance of the detector, resulting in suboptimal performance. Inspired by DETR, we propose a unified framework, 3DGCTR, to jointly solve these two distinct but closely related tasks in an end-to-end fashion. The key idea is to reconsider the prompt-based localization ability of the 3DVG model. In this way, the 3DVG model with a well-designed prompt as input can assist the 3DDC task by extracting localization information from the prompt. In terms of implementation, we integrate a Lightweight Caption Head into the existing 3DVG network with a Caption Text Prompt as a connection, effectively harnessing the existing 3DVG model's inherent localization capacity, thereby boosting 3DDC capability. This integration facilitates simultaneous multi-task training on both tasks, mutually enhancing their performance. Extensive experimental results demonstrate the effectiveness of this approach. Specifically, on the ScanRefer dataset, 3DGCTR surpasses the state-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and improves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU.","sentences":["3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial tasks in various 3D applications, which require both shared and complementary information in localization and visual-language relationships.","Therefore, existing approaches adopt the two-stage \"detect-then-describe/discriminate\" pipeline, which relies heavily on the performance of the detector, resulting in suboptimal performance.","Inspired by DETR, we propose a unified framework, 3DGCTR, to jointly solve these two distinct but closely related tasks in an end-to-end fashion.","The key idea is to reconsider the prompt-based localization ability of the 3DVG model.","In this way, the 3DVG model with a well-designed prompt as input can assist the 3DDC task by extracting localization information from the prompt.","In terms of implementation, we integrate a Lightweight Caption Head into the existing 3DVG network with a Caption Text Prompt as a connection, effectively harnessing the existing 3DVG model's inherent localization capacity, thereby boosting 3DDC capability.","This integration facilitates simultaneous multi-task training on both tasks, mutually enhancing their performance.","Extensive experimental results demonstrate the effectiveness of this approach.","Specifically, on the ScanRefer dataset, 3DGCTR surpasses the state-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and improves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU."],"url":"http://arxiv.org/abs/2404.11064v1","category":"cs.CV"}
{"created":"2024-04-17 04:08:38","title":"LMEraser: Large Model Unlearning through Adaptive Prompt Tuning","abstract":"To address the growing demand for privacy protection in machine learning, we propose a novel and efficient machine unlearning approach for \\textbf{L}arge \\textbf{M}odels, called \\textbf{LM}Eraser. Existing unlearning research suffers from entangled training data and complex model architectures, incurring extremely high computational costs for large models. LMEraser takes a divide-and-conquer strategy with a prompt tuning architecture to isolate data influence. The training dataset is partitioned into public and private datasets. Public data are used to train the backbone of the model. Private data are adaptively clustered based on their diversity, and each cluster is used to optimize a prompt separately. This adaptive prompt tuning mechanism reduces unlearning costs and maintains model performance. Experiments demonstrate that LMEraser achieves a $100$-fold reduction in unlearning costs without compromising accuracy compared to prior work. Our code is available at: \\url{https://github.com/lmeraser/lmeraser}.","sentences":["To address the growing demand for privacy protection in machine learning, we propose a novel and efficient machine unlearning approach for \\textbf{L}arge \\textbf{M}odels, called \\textbf{LM}Eraser.","Existing unlearning research suffers from entangled training data and complex model architectures, incurring extremely high computational costs for large models.","LMEraser takes a divide-and-conquer strategy with a prompt tuning architecture to isolate data influence.","The training dataset is partitioned into public and private datasets.","Public data are used to train the backbone of the model.","Private data are adaptively clustered based on their diversity, and each cluster is used to optimize a prompt separately.","This adaptive prompt tuning mechanism reduces unlearning costs and maintains model performance.","Experiments demonstrate that LMEraser achieves a $100$-fold reduction in unlearning costs without compromising accuracy compared to prior work.","Our code is available at: \\url{https://github.com/lmeraser/lmeraser}."],"url":"http://arxiv.org/abs/2404.11056v1","category":"cs.LG"}
{"created":"2024-04-17 03:44:58","title":"Stepwise Alignment for Constrained Language Model Policy Optimization","abstract":"Safety and trustworthiness are indispensable requirements for applying AI systems based on large language models (LLMs) in real-world applications. This paper formulates a human value alignment as a language model policy optimization problem to maximize reward under a safety constraint and then proposes an algorithm called Stepwise Alignment for Constrained Policy Optimization (SACPO). A key idea behind SACPO, supported by theory, is that the optimal policy incorporating both reward and safety can be directly obtained from a reward-aligned policy. Based on this key idea, SACPO aligns the LLMs with each metric step-wise while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO provides many benefits such as simplicity, stability, computational efficiency, and flexibility regarding algorithms and dataset selection. Under mild assumption, our theoretical analysis provides the upper bounds regarding near-optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness","sentences":["Safety and trustworthiness are indispensable requirements for applying AI systems based on large language models (LLMs) in real-world applications.","This paper formulates a human value alignment as a language model policy optimization problem to maximize reward under a safety constraint and then proposes an algorithm called Stepwise Alignment for Constrained Policy Optimization (SACPO).","A key idea behind SACPO, supported by theory, is that the optimal policy incorporating both reward and safety can be directly obtained from a reward-aligned policy.","Based on this key idea, SACPO aligns the LLMs with each metric step-wise while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO).","SACPO provides many benefits such as simplicity, stability, computational efficiency, and flexibility regarding algorithms and dataset selection.","Under mild assumption, our theoretical analysis provides the upper bounds regarding near-optimality and safety constraint violation.","Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness"],"url":"http://arxiv.org/abs/2404.11049v1","category":"cs.LG"}
{"created":"2024-04-17 03:42:48","title":"Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model","abstract":"Federated learning aims to tackle the ``isolated data island\" problem, where it trains a collective model from physically isolated clients while safeguarding the privacy of users' data. However, supervised federated learning necessitates that each client labels their data for training, which can be both time-consuming and resource-intensive, and may even be impractical for edge devices. Moreover, the training and transmission of deep models present challenges to the computation and communication capabilities of the clients. To address these two inherent challenges in supervised federated learning, we propose a novel lightweight unsupervised federated learning approach that leverages unlabeled data on each client to perform lightweight model training and communication by harnessing pretrained vision-language models, such as CLIP. By capitalizing on the zero-shot prediction capability and the well-trained image encoder of the pre-trained CLIP model, we have carefully crafted an efficient and resilient self-training approach. This method refines the initial zero-shot predicted pseudo-labels of unlabeled instances through the sole training of a linear classifier on top of the fixed image encoder. Additionally, to address data heterogeneity within each client, we propose a class-balanced text feature sampling strategy for generating synthetic instances in the feature space to support local training. Experiments are conducted on multiple benchmark datasets. The experimental results demonstrate that our proposed method greatly enhances model performance in comparison to CLIP's zero-shot predictions and even outperforms supervised federated learning benchmark methods given limited computational and communication overhead.","sentences":["Federated learning aims to tackle the ``isolated data island\" problem, where it trains a collective model from physically isolated clients while safeguarding the privacy of users' data.","However, supervised federated learning necessitates that each client labels their data for training, which can be both time-consuming and resource-intensive, and may even be impractical for edge devices.","Moreover, the training and transmission of deep models present challenges to the computation and communication capabilities of the clients.","To address these two inherent challenges in supervised federated learning, we propose a novel lightweight unsupervised federated learning approach that leverages unlabeled data on each client to perform lightweight model training and communication by harnessing pretrained vision-language models, such as CLIP.","By capitalizing on the zero-shot prediction capability and the well-trained image encoder of the pre-trained CLIP model, we have carefully crafted an efficient and resilient self-training approach.","This method refines the initial zero-shot predicted pseudo-labels of unlabeled instances through the sole training of a linear classifier on top of the fixed image encoder.","Additionally, to address data heterogeneity within each client, we propose a class-balanced text feature sampling strategy for generating synthetic instances in the feature space to support local training.","Experiments are conducted on multiple benchmark datasets.","The experimental results demonstrate that our proposed method greatly enhances model performance in comparison to CLIP's zero-shot predictions and even outperforms supervised federated learning benchmark methods given limited computational and communication overhead."],"url":"http://arxiv.org/abs/2404.11046v1","category":"cs.AI"}
{"created":"2024-04-17 03:34:27","title":"On the Empirical Complexity of Reasoning and Planning in LLMs","abstract":"Large Language Models (LLMs) work surprisingly well for some complex reasoning problems via chain-of-thought (CoT) or tree-of-thought (ToT), but the underlying reasons remain unclear. We seek to understand the performance of these methods by conducting experimental case studies and linking the outcomes to sample and computational complexity in machine learning. We found that if problems can be decomposed into a sequence of reasoning steps and learning to predict the next step has a low sample and computational complexity, explicitly outlining the reasoning chain with all necessary information for predicting the next step may improve performance. Conversely, for problems where predicting the next step is computationally hard, adopting ToT may yield better reasoning outcomes than attempting to formulate a short reasoning chain.","sentences":["Large Language Models (LLMs) work surprisingly well for some complex reasoning problems via chain-of-thought (CoT) or tree-of-thought (ToT), but the underlying reasons remain unclear.","We seek to understand the performance of these methods by conducting experimental case studies and linking the outcomes to sample and computational complexity in machine learning.","We found that if problems can be decomposed into a sequence of reasoning steps and learning to predict the next step has a low sample and computational complexity, explicitly outlining the reasoning chain with all necessary information for predicting the next step may improve performance.","Conversely, for problems where predicting the next step is computationally hard, adopting ToT may yield better reasoning outcomes than attempting to formulate a short reasoning chain."],"url":"http://arxiv.org/abs/2404.11041v1","category":"cs.AI"}
{"created":"2024-04-17 03:06:32","title":"Empowering Large Language Models on Robotic Manipulation with Affordance Prompting","abstract":"While large language models (LLMs) are successful in completing various language processing tasks, they easily fail to interact with the physical world by generating control sequences properly. We find that the main reason is that LLMs are not grounded in the physical world. Existing LLM-based approaches circumvent this problem by relying on additional pre-defined skills or pre-trained sub-policies, making it hard to adapt to new tasks. In contrast, we aim to address this problem and explore the possibility to prompt pre-trained LLMs to accomplish a series of robotic manipulation tasks in a training-free paradigm. Accordingly, we propose a framework called LLM+A(ffordance) where the LLM serves as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences). To ground these plans and control sequences on the physical world, we develop the affordance prompting technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects. Empirically, we evaluate the effectiveness of LLM+A in various language-conditioned robotic manipulation tasks, which show that our approach substantially improves performance by enhancing the feasibility of generated plans and control and can easily generalize to different environments.","sentences":["While large language models (LLMs) are successful in completing various language processing tasks, they easily fail to interact with the physical world by generating control sequences properly.","We find that the main reason is that LLMs are not grounded in the physical world.","Existing LLM-based approaches circumvent this problem by relying on additional pre-defined skills or pre-trained sub-policies, making it hard to adapt to new tasks.","In contrast, we aim to address this problem and explore the possibility to prompt pre-trained LLMs to accomplish a series of robotic manipulation tasks in a training-free paradigm.","Accordingly, we propose a framework called LLM+A(ffordance) where the LLM serves as both the sub-task planner (that generates high-level plans) and the motion controller (that generates low-level control sequences).","To ground these plans and control sequences on the physical world, we develop the affordance prompting technique that stimulates the LLM to 1) predict the consequences of generated plans and 2) generate affordance values for relevant objects.","Empirically, we evaluate the effectiveness of LLM+A in various language-conditioned robotic manipulation tasks, which show that our approach substantially improves performance by enhancing the feasibility of generated plans and control and can easily generalize to different environments."],"url":"http://arxiv.org/abs/2404.11027v1","category":"cs.AI"}
{"created":"2024-04-17 02:57:42","title":"Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions","abstract":"Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial). Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech. Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world. In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI. We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research.","sentences":["Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial).","Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech.","Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world.","In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI.","We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research."],"url":"http://arxiv.org/abs/2404.11023v1","category":"cs.HC"}
{"created":"2024-04-17 02:49:26","title":"Many-Shot In-Context Learning","abstract":"Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.","sentences":["Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates.","Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime.","Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks.","While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples.","To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL.","Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples.","Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions.","We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks.","Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs.","Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance."],"url":"http://arxiv.org/abs/2404.11018v1","category":"cs.LG"}
{"created":"2024-04-17 02:47:39","title":"MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for Infrared and Visible Image Fusion via Guided Training","abstract":"In this research, we introduce MaeFuse, a novel autoencoder model designed for infrared and visible image fusion (IVIF). The existing approaches for image fusion often rely on training combined with downstream tasks to obtain high-level visual information, which is effective in emphasizing target objects and delivering impressive results in visual quality and task-specific applications. MaeFuse, however, deviates from the norm. Instead of being driven by downstream tasks, our model utilizes a pretrained encoder from Masked Autoencoders (MAE), which facilities the omni features extraction for low-level reconstruction and high-level vision tasks, to obtain perception friendly features with a low cost. In order to eliminate the domain gap of different modal features and the block effect caused by the MAE encoder, we further develop a guided training strategy. This strategy is meticulously crafted to ensure that the fusion layer seamlessly adjusts to the feature space of the encoder, gradually enhancing the fusion effect. It facilitates the comprehensive integration of feature vectors from both infrared and visible modalities, preserving the rich details inherent in each. MaeFuse not only introduces a novel perspective in the realm of fusion techniques but also stands out with impressive performance across various public datasets.","sentences":["In this research, we introduce MaeFuse, a novel autoencoder model designed for infrared and visible image fusion (IVIF).","The existing approaches for image fusion often rely on training combined with downstream tasks to obtain high-level visual information, which is effective in emphasizing target objects and delivering impressive results in visual quality and task-specific applications.","MaeFuse, however, deviates from the norm.","Instead of being driven by downstream tasks, our model utilizes a pretrained encoder from Masked Autoencoders (MAE), which facilities the omni features extraction for low-level reconstruction and high-level vision tasks, to obtain perception friendly features with a low cost.","In order to eliminate the domain gap of different modal features and the block effect caused by the MAE encoder, we further develop a guided training strategy.","This strategy is meticulously crafted to ensure that the fusion layer seamlessly adjusts to the feature space of the encoder, gradually enhancing the fusion effect.","It facilitates the comprehensive integration of feature vectors from both infrared and visible modalities, preserving the rich details inherent in each.","MaeFuse not only introduces a novel perspective in the realm of fusion techniques but also stands out with impressive performance across various public datasets."],"url":"http://arxiv.org/abs/2404.11016v1","category":"cs.CV"}
{"created":"2024-04-17 02:46:59","title":"FedFa: A Fully Asynchronous Training Paradigm for Federated Learning","abstract":"Federated learning has been identified as an efficient decentralized training paradigm for scaling the machine learning model training on a large number of devices while guaranteeing the data privacy of the trainers. FedAvg has become a foundational parameter update strategy for federated learning, which has been promising to eliminate the effect of the heterogeneous data across clients and guarantee convergence. However, the synchronization parameter update barriers for each communication round during the training significant time on waiting, slowing down the training procedure. Therefore, recent state-of-the-art solutions propose using semi-asynchronous approaches to mitigate the waiting time cost with guaranteed convergence. Nevertheless, emerging semi-asynchronous approaches are unable to eliminate the waiting time completely.   We propose a full asynchronous training paradigm, called FedFa, which can guarantee model convergence and eliminate the waiting time completely for federated learning by using a few buffered results on the server for parameter updating. Further, we provide theoretical proof of the convergence rate for our proposed FedFa. Extensive experimental results indicate our approach effectively improves the training performance of federated learning by up to 6x and 4x speedup compared to the state-of-the-art synchronous and semi-asynchronous strategies while retaining high accuracy in both IID and Non-IID scenarios.","sentences":["Federated learning has been identified as an efficient decentralized training paradigm for scaling the machine learning model training on a large number of devices while guaranteeing the data privacy of the trainers.","FedAvg has become a foundational parameter update strategy for federated learning, which has been promising to eliminate the effect of the heterogeneous data across clients and guarantee convergence.","However, the synchronization parameter update barriers for each communication round during the training significant time on waiting, slowing down the training procedure.","Therefore, recent state-of-the-art solutions propose using semi-asynchronous approaches to mitigate the waiting time cost with guaranteed convergence.","Nevertheless, emerging semi-asynchronous approaches are unable to eliminate the waiting time completely.   ","We propose a full asynchronous training paradigm, called FedFa, which can guarantee model convergence and eliminate the waiting time completely for federated learning by using a few buffered results on the server for parameter updating.","Further, we provide theoretical proof of the convergence rate for our proposed FedFa.","Extensive experimental results indicate our approach effectively improves the training performance of federated learning by up to 6x and 4x speedup compared to the state-of-the-art synchronous and semi-asynchronous strategies while retaining high accuracy in both IID and Non-IID scenarios."],"url":"http://arxiv.org/abs/2404.11015v1","category":"cs.LG"}
{"created":"2024-04-17 02:46:18","title":"Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs","abstract":"Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the entire road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatial and temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent and reactive urban traffic management solutions.","sentences":["Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow.","Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control.","To address this, we propose a novel TSCS framework to realize intelligent traffic control.","This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network.","To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm.","Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the entire road network collectively.","Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network.","This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatial and temporal correlations between multiple intersections.","Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance.","This work facilitates the development of more intelligent and reactive urban traffic management solutions."],"url":"http://arxiv.org/abs/2404.11014v1","category":"cs.MA"}
{"created":"2024-04-17 02:36:02","title":"AKGNet: Attribute Knowledge-Guided Unsupervised Lung-Infected Area Segmentation","abstract":"Lung-infected area segmentation is crucial for assessing the severity of lung diseases. However, existing image-text multi-modal methods typically rely on labour-intensive annotations for model training, posing challenges regarding time and expertise. To address this issue, we propose a novel attribute knowledge-guided framework for unsupervised lung-infected area segmentation (AKGNet), which achieves segmentation solely based on image-text data without any mask annotation. AKGNet facilitates text attribute knowledge learning, attribute-image cross-attention fusion, and high-confidence-based pseudo-label exploration simultaneously. It can learn statistical information and capture spatial correlations between image and text attributes in the embedding space, iteratively refining the mask to enhance segmentation. Specifically, we introduce a text attribute knowledge learning module by extracting attribute knowledge and incorporating it into feature representations, enabling the model to learn statistical information and adapt to different attributes. Moreover, we devise an attribute-image cross-attention module by calculating the correlation between attributes and images in the embedding space to capture spatial dependency information, thus selectively focusing on relevant regions while filtering irrelevant areas. Finally, a self-training mask improvement process is employed by generating pseudo-labels using high-confidence predictions to iteratively enhance the mask and segmentation. Experimental results on a benchmark medical image dataset demonstrate the superior performance of our method compared to state-of-the-art segmentation techniques in unsupervised scenarios.","sentences":["Lung-infected area segmentation is crucial for assessing the severity of lung diseases.","However, existing image-text multi-modal methods typically rely on labour-intensive annotations for model training, posing challenges regarding time and expertise.","To address this issue, we propose a novel attribute knowledge-guided framework for unsupervised lung-infected area segmentation (AKGNet), which achieves segmentation solely based on image-text data without any mask annotation.","AKGNet facilitates text attribute knowledge learning, attribute-image cross-attention fusion, and high-confidence-based pseudo-label exploration simultaneously.","It can learn statistical information and capture spatial correlations between image and text attributes in the embedding space, iteratively refining the mask to enhance segmentation.","Specifically, we introduce a text attribute knowledge learning module by extracting attribute knowledge and incorporating it into feature representations, enabling the model to learn statistical information and adapt to different attributes.","Moreover, we devise an attribute-image cross-attention module by calculating the correlation between attributes and images in the embedding space to capture spatial dependency information, thus selectively focusing on relevant regions while filtering irrelevant areas.","Finally, a self-training mask improvement process is employed by generating pseudo-labels using high-confidence predictions to iteratively enhance the mask and segmentation.","Experimental results on a benchmark medical image dataset demonstrate the superior performance of our method compared to state-of-the-art segmentation techniques in unsupervised scenarios."],"url":"http://arxiv.org/abs/2404.11008v1","category":"cs.CV"}
{"created":"2024-04-17 02:04:10","title":"Function Approximation for Reinforcement Learning Controller for Energy from Spread Waves","abstract":"The industrial multi-generator Wave Energy Converters (WEC) must handle multiple simultaneous waves coming from different directions called spread waves. These complex devices in challenging circumstances need controllers with multiple objectives of energy capture efficiency, reduction of structural stress to limit maintenance, and proactive protection against high waves. The Multi-Agent Reinforcement Learning (MARL) controller trained with the Proximal Policy Optimization (PPO) algorithm can handle these complexities. In this paper, we explore different function approximations for the policy and critic networks in modeling the sequential nature of the system dynamics and find that they are key to better performance. We investigated the performance of a fully connected neural network (FCN), LSTM, and Transformer model variants with varying depths and gated residual connections. Our results show that the transformer model of moderate depth with gated residual connections around the multi-head attention, multi-layer perceptron, and the transformer block (STrXL) proposed in this paper is optimal and boosts energy efficiency by an average of 22.1% for these complex spread waves over the existing spring damper (SD) controller. Furthermore, unlike the default SD controller, the transformer controller almost eliminated the mechanical stress from the rotational yaw motion for angled waves. Demo: https://tinyurl.com/yueda3jh","sentences":["The industrial multi-generator Wave Energy Converters (WEC) must handle multiple simultaneous waves coming from different directions called spread waves.","These complex devices in challenging circumstances need controllers with multiple objectives of energy capture efficiency, reduction of structural stress to limit maintenance, and proactive protection against high waves.","The Multi-Agent Reinforcement Learning (MARL) controller trained with the Proximal Policy Optimization (PPO) algorithm can handle these complexities.","In this paper, we explore different function approximations for the policy and critic networks in modeling the sequential nature of the system dynamics and find that they are key to better performance.","We investigated the performance of a fully connected neural network (FCN), LSTM, and Transformer model variants with varying depths and gated residual connections.","Our results show that the transformer model of moderate depth with gated residual connections around the multi-head attention, multi-layer perceptron, and the transformer block (STrXL) proposed in this paper is optimal and boosts energy efficiency by an average of 22.1% for these complex spread waves over the existing spring damper (SD) controller.","Furthermore, unlike the default SD controller, the transformer controller almost eliminated the mechanical stress from the rotational yaw motion for angled waves.","Demo:","https://tinyurl.com/yueda3jh"],"url":"http://arxiv.org/abs/2404.10991v1","category":"cs.AI"}
{"created":"2024-04-17 01:27:42","title":"A Survey on Retrieval-Augmented Text Generation for Large Language Models","abstract":"Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.","sentences":["Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information.","This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data.","As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint.","It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies.","Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions.","By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs."],"url":"http://arxiv.org/abs/2404.10981v1","category":"cs.IR"}
{"created":"2024-04-17 01:26:15","title":"Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty","abstract":"Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. This scenario necessitates the use of composite class labels. In this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty due to composite class labels in training data in the context of the belief theory called Subjective Logic (SL). By placing a grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data. We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs. Our results demonstrate that HENN outperforms its state-of-the-art counterparts based on four image datasets. The code and datasets are available at: https://github.com/Hugo101/HyperEvidentialNN.","sentences":["Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks.","However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them.","This scenario necessitates the use of composite class labels.","In this paper, we propose a novel framework called Hyper-Evidential Neural Network (HENN) that explicitly models predictive uncertainty due to composite class labels in training data in the context of the belief theory called Subjective Logic (SL).","By placing a grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic DNN from data.","We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs.","Our results demonstrate that HENN outperforms its state-of-the-art counterparts based on four image datasets.","The code and datasets are available at: https://github.com/Hugo101/HyperEvidentialNN."],"url":"http://arxiv.org/abs/2404.10980v1","category":"cs.CV"}
{"created":"2024-04-17 01:23:49","title":"Leveraging 3D LiDAR Sensors to Enable Enhanced Urban Safety and Public Health: Pedestrian Monitoring and Abnormal Activity Detection","abstract":"The integration of Light Detection and Ranging (LiDAR) and Internet of Things (IoT) technologies offers transformative opportunities for public health informatics in urban safety and pedestrian well-being. This paper proposes a novel framework utilizing these technologies for enhanced 3D object detection and activity classification in urban traffic scenarios. By employing elevated LiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian activity monitoring. To overcome urban data scarcity, we create a specialized dataset through simulated traffic environments in Blender, facilitating targeted model training. Our approach employs a modified Point Voxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D detection and PointNet for classifying pedestrian activities, significantly benefiting urban traffic management and public health by offering insights into pedestrian behavior and promoting safer urban environments. Our dual-model approach not only enhances urban traffic management but also contributes significantly to public health by providing insights into pedestrian behavior and promoting safer urban environment.","sentences":["The integration of Light Detection and Ranging (LiDAR) and Internet of Things (IoT) technologies offers transformative opportunities for public health informatics in urban safety and pedestrian well-being.","This paper proposes a novel framework utilizing these technologies for enhanced 3D object detection and activity classification in urban traffic scenarios.","By employing elevated LiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian activity monitoring.","To overcome urban data scarcity, we create a specialized dataset through simulated traffic environments in Blender, facilitating targeted model training.","Our approach employs a modified Point Voxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D detection and PointNet for classifying pedestrian activities, significantly benefiting urban traffic management and public health by offering insights into pedestrian behavior and promoting safer urban environments.","Our dual-model approach not only enhances urban traffic management but also contributes significantly to public health by providing insights into pedestrian behavior and promoting safer urban environment."],"url":"http://arxiv.org/abs/2404.10978v1","category":"cs.CV"}
{"created":"2024-04-17 01:17:10","title":"Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning","abstract":"Cooperative Multi-Agent Reinforcement Learning (MARL) necessitates seamless collaboration among agents, often represented by an underlying relation graph. Existing methods for learning this graph primarily focus on agent-pair relations, neglecting higher-order relationships. While several approaches attempt to extend cooperation modelling to encompass behaviour similarities within groups, they commonly fall short in concurrently learning the latent graph, thereby constraining the information exchange among partially observed agents. To overcome these limitations, we present a novel approach to infer the Group-Aware Coordination Graph (GACG), which is designed to capture both the cooperation between agent pairs based on current observations and group-level dependencies from behaviour patterns observed across trajectories. This graph is further used in graph convolution for information exchange between agents during decision-making. To further ensure behavioural consistency among agents within the same group, we introduce a group distance loss, which promotes group cohesion and encourages specialization between groups. Our evaluations, conducted on StarCraft II micromanagement tasks, demonstrate GACG's superior performance. An ablation study further provides experimental evidence of the effectiveness of each component of our method.","sentences":["Cooperative Multi-Agent Reinforcement Learning (MARL) necessitates seamless collaboration among agents, often represented by an underlying relation graph.","Existing methods for learning this graph primarily focus on agent-pair relations, neglecting higher-order relationships.","While several approaches attempt to extend cooperation modelling to encompass behaviour similarities within groups, they commonly fall short in concurrently learning the latent graph, thereby constraining the information exchange among partially observed agents.","To overcome these limitations, we present a novel approach to infer the Group-Aware Coordination Graph (GACG), which is designed to capture both the cooperation between agent pairs based on current observations and group-level dependencies from behaviour patterns observed across trajectories.","This graph is further used in graph convolution for information exchange between agents during decision-making.","To further ensure behavioural consistency among agents within the same group, we introduce a group distance loss, which promotes group cohesion and encourages specialization between groups.","Our evaluations, conducted on StarCraft II micromanagement tasks, demonstrate GACG's superior performance.","An ablation study further provides experimental evidence of the effectiveness of each component of our method."],"url":"http://arxiv.org/abs/2404.10976v1","category":"cs.LG"}
{"created":"2024-04-17 01:13:04","title":"Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models","abstract":"As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning. To test whether they do, we need to develop systematic evaluations. We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates. With this framework, we procedurally generated a large and diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of 50 scenarios and 400 unique test items. We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions. We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models. The same pattern was observed for evitable versus inevitable harmful outcomes. However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act. We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects.","sentences":["As AI systems like language models are increasingly integrated into decision-making processes affecting people's lives, it's critical to ensure that these systems have sound moral reasoning.","To test whether they do, we need to develop systematic evaluations.","We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates.","With this framework, we procedurally generated a large and diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of 50 scenarios and 400 unique test items.","We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions.","We find that moral dilemmas in which the harm is a necessary means (as compared to a side effect) resulted in lower permissibility and higher intention ratings for both participants and language models.","The same pattern was observed for evitable versus inevitable harmful outcomes.","However, there was no clear effect of whether the harm resulted from an agent's action versus from having omitted to act.","We discuss limitations of our prompt generation pipeline and opportunities for improving scenarios to increase the strength of experimental effects."],"url":"http://arxiv.org/abs/2404.10975v1","category":"cs.CL"}
{"created":"2024-04-16 23:56:38","title":"Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations","abstract":"A major barrier towards the practical deployment of large language models (LLMs) is their lack of reliability. Three situations where this is particularly apparent are correctness, hallucinations when given unanswerable questions, and safety. In all three cases, models should ideally abstain from responding, much like humans, whose ability to understand uncertainty makes us refrain from answering questions we don't know. Inspired by analogous approaches in classification, this study explores the feasibility and efficacy of abstaining while uncertain in the context of LLMs within the domain of question-answering. We investigate two kinds of uncertainties, statistical uncertainty metrics and a distinct verbalized measure, termed as In-Dialogue Uncertainty (InDU). Using these uncertainty measures combined with models with and without Reinforcement Learning with Human Feedback (RLHF), we show that in all three situations, abstention based on the right kind of uncertainty measure can boost the reliability of LLMs. By sacrificing only a few highly uncertain samples we can improve correctness by 2% to 8%, avoid 50% hallucinations via correctly identifying unanswerable questions and increase safety by 70% up to 99% with almost no additional computational overhead.","sentences":["A major barrier towards the practical deployment of large language models (LLMs) is their lack of reliability.","Three situations where this is particularly apparent are correctness, hallucinations when given unanswerable questions, and safety.","In all three cases, models should ideally abstain from responding, much like humans, whose ability to understand uncertainty makes us refrain from answering questions we don't know.","Inspired by analogous approaches in classification, this study explores the feasibility and efficacy of abstaining while uncertain in the context of LLMs within the domain of question-answering.","We investigate two kinds of uncertainties, statistical uncertainty metrics and a distinct verbalized measure, termed as In-Dialogue Uncertainty (InDU).","Using these uncertainty measures combined with models with and without Reinforcement Learning with Human Feedback (RLHF), we show that in all three situations, abstention based on the right kind of uncertainty measure can boost the reliability of LLMs.","By sacrificing only a few highly uncertain samples we can improve correctness by 2% to 8%, avoid 50% hallucinations via correctly identifying unanswerable questions and increase safety by 70% up to 99% with almost no additional computational overhead."],"url":"http://arxiv.org/abs/2404.10960v1","category":"cs.CL"}
{"created":"2024-04-16 23:27:38","title":"Can Language Models Solve Olympiad Programming?","abstract":"Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language models (LMs). In this paper, we introduce the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem. These resources enable us to construct and test a range of LM inference methods for competitive programming for the first time. We find GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and our best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge. However, this is far from solving the benchmark. To better understand the remaining challenges, we design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. Our benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning.","sentences":["Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code.","However, it has been understudied as a domain to evaluate language models (LMs).","In this paper, we introduce the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem.","These resources enable us to construct and test a range of LM inference methods for competitive programming for the first time.","We find GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and our best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge.","However, this is far from solving the benchmark.","To better understand the remaining challenges, we design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method.","Our benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning."],"url":"http://arxiv.org/abs/2404.10952v1","category":"cs.CL"}
{"created":"2024-04-16 23:14:21","title":"First double-differential cross section measurement of neutral-current $\u03c0^0$ production in neutrino-argon scattering in the MicroBooNE detector","abstract":"We report the first double-differential cross section measurement of neutral-current neutral pion (NC$\\pi^0$) production in neutrino-argon scattering, as well as single-differential measurements of the same channel in terms of final states with and without protons. The kinematic variables of interest for these measurements are the $\\pi^0$ momentum and the $\\pi^0$ scattering angle with respect to the neutrino beam. A total of 4971 candidate NC$\\pi^0$ events fully-contained within the MicroBooNE detector are selected using data collected at a mean neutrino energy of $\\sim 0.8$ GeV from $6.4\\times10^{20}$ protons on target from the Booster Neutrino Beam at the Fermi National Accelerator Laboratory. After extensive data-driven model validation to ensure unbiased unfolding, the Wiener-SVD method is used to extract nominal flux-averaged cross sections. The results are compared to predictions from commonly used neutrino event generators, which tend to overpredict the measured NC$\\pi^0$ cross section, especially in the 0.2-0.5 GeV/c $\\pi^0$ momentum range, at forward scattering angles, and when at least one proton is present in the final state. These measurements show sensitivity to a variety of features that complicate the description of NC$\\pi^0$ production including the form factors describing the elementary neutrino interaction and the final state interactions of the outgoing particles in the residual argon nucleus. This data will help improve the modeling of NC$\\pi^0$ production, which represents a major background in measurements of charge-parity violation in the neutrino sector and in searches for new physics beyond the Standard Model.","sentences":["We report the first double-differential cross section measurement of neutral-current neutral pion (NC$\\pi^0$) production in neutrino-argon scattering, as well as single-differential measurements of the same channel in terms of final states with and without protons.","The kinematic variables of interest for these measurements are the $\\pi^0$ momentum and the $\\pi^0$ scattering angle with respect to the neutrino beam.","A total of 4971 candidate NC$\\pi^0$ events fully-contained within the MicroBooNE detector are selected using data collected at a mean neutrino energy of $\\sim 0.8$ GeV from $6.4\\times10^{20}$ protons on target from the Booster Neutrino Beam at the Fermi National Accelerator Laboratory.","After extensive data-driven model validation to ensure unbiased unfolding, the Wiener-SVD method is used to extract nominal flux-averaged cross sections.","The results are compared to predictions from commonly used neutrino event generators, which tend to overpredict the measured NC$\\pi^0$ cross section, especially in the 0.2-0.5 GeV/c $\\pi^0$ momentum range, at forward scattering angles, and when at least one proton is present in the final state.","These measurements show sensitivity to a variety of features that complicate the description of NC$\\pi^0$ production including the form factors describing the elementary neutrino interaction and the final state interactions of the outgoing particles in the residual argon nucleus.","This data will help improve the modeling of NC$\\pi^0$ production, which represents a major background in measurements of charge-parity violation in the neutrino sector and in searches for new physics beyond the Standard Model."],"url":"http://arxiv.org/abs/2404.10948v1","category":"hep-ex"}
{"created":"2024-04-16 22:59:40","title":"Information encoding and decoding in in-vitro neural networks on micro electrode arrays through stimulation timing","abstract":"A primary challenge in utilizing in-vitro biological neural networks for computations is finding good encoding and decoding schemes for inputting and decoding data to and from the networks. Furthermore, identifying the optimal parameter settings for a given combination of encoding and decoding schemes adds additional complexity to this challenge. In this study we explore stimulation timing as an encoding method, i.e. we encode information as the delay between stimulation pulses and identify the bounds and acuity of stimulation timings which produce linearly separable spike responses. We also examine the optimal readout parameters for a linear decoder in the form of epoch length, time bin size and epoch offset. Our results suggest that stimulation timings between 36 and 436ms may be optimal for encoding and that different combinations of readout parameters may be optimal at different parts of the evoked spike response.","sentences":["A primary challenge in utilizing in-vitro biological neural networks for computations is finding good encoding and decoding schemes for inputting and decoding data to and from the networks.","Furthermore, identifying the optimal parameter settings for a given combination of encoding and decoding schemes adds additional complexity to this challenge.","In this study we explore stimulation timing as an encoding method, i.e. we encode information as the delay between stimulation pulses and identify the bounds and acuity of stimulation timings which produce linearly separable spike responses.","We also examine the optimal readout parameters for a linear decoder in the form of epoch length, time bin size and epoch offset.","Our results suggest that stimulation timings between 36 and 436ms may be optimal for encoding and that different combinations of readout parameters may be optimal at different parts of the evoked spike response."],"url":"http://arxiv.org/abs/2404.10946v1","category":"q-bio.NC"}
{"created":"2024-04-16 22:57:06","title":"Threat Behavior Textual Search by Attention Graph Isomorphism","abstract":"Cyber attacks cause over \\$1 trillion loss every year. An important task for cyber security analysts is attack forensics. It entails understanding malware behaviors and attack origins. However, existing automated or manual malware analysis can only disclose a subset of behaviors due to inherent difficulties (e.g., malware cloaking and obfuscation). As such, analysts often resort to text search techniques to identify existing malware reports based on the symptoms they observe, exploiting the fact that malware samples share a lot of similarity, especially those from the same origin. In this paper, we propose a novel malware behavior search technique that is based on graph isomorphism at the attention layers of Transformer models. We also compose a large dataset collected from various agencies to facilitate such research. Our technique outperforms state-of-the-art methods, such as those based on sentence embeddings and keywords by 6-14%. In the case study of 10 real-world malwares, our technique can correctly attribute 8 of them to their ground truth origins while using Google only works for 3 cases.","sentences":["Cyber attacks cause over \\$1 trillion loss every year.","An important task for cyber security analysts is attack forensics.","It entails understanding malware behaviors and attack origins.","However, existing automated or manual malware analysis can only disclose a subset of behaviors due to inherent difficulties (e.g., malware cloaking and obfuscation).","As such, analysts often resort to text search techniques to identify existing malware reports based on the symptoms they observe, exploiting the fact that malware samples share a lot of similarity, especially those from the same origin.","In this paper, we propose a novel malware behavior search technique that is based on graph isomorphism at the attention layers of Transformer models.","We also compose a large dataset collected from various agencies to facilitate such research.","Our technique outperforms state-of-the-art methods, such as those based on sentence embeddings and keywords by 6-14%.","In the case study of 10 real-world malwares, our technique can correctly attribute 8 of them to their ground truth origins while using Google only works for 3 cases."],"url":"http://arxiv.org/abs/2404.10944v1","category":"cs.IR"}
{"created":"2024-04-16 22:47:59","title":"What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning","abstract":"In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning.","sentences":["In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns.","Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear.","In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens.","We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components.","We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past.","This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant.","To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data.","Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning."],"url":"http://arxiv.org/abs/2404.10942v1","category":"cs.LG"}
{"created":"2024-04-16 22:12:36","title":"Shears: Unstructured Sparsity with Neural Low-rank Adapter Search","abstract":"Recently, several approaches successfully demonstrated that weight-sharing Neural Architecture Search (NAS) can effectively explore a search space of elastic low-rank adapters (LoRA), allowing the parameter-efficient fine-tuning (PEFT) and compression of large language models. In this paper, we introduce a novel approach called Shears, demonstrating how the integration of cost-effective sparsity and a proposed Neural Low-rank adapter Search (NLS) algorithm can further improve the efficiency of PEFT approaches. Results demonstrate the benefits of Shears compared to other methods, reaching high sparsity levels while improving or with little drop in accuracy, utilizing a single GPU for a pair of hours.","sentences":["Recently, several approaches successfully demonstrated that weight-sharing Neural Architecture Search (NAS) can effectively explore a search space of elastic low-rank adapters (LoRA), allowing the parameter-efficient fine-tuning (PEFT) and compression of large language models.","In this paper, we introduce a novel approach called Shears, demonstrating how the integration of cost-effective sparsity and a proposed Neural Low-rank adapter Search (NLS) algorithm can further improve the efficiency of PEFT approaches.","Results demonstrate the benefits of Shears compared to other methods, reaching high sparsity levels while improving or with little drop in accuracy, utilizing a single GPU for a pair of hours."],"url":"http://arxiv.org/abs/2404.10934v1","category":"cs.LG"}
{"created":"2024-04-16 22:11:35","title":"LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs","abstract":"Fine-tuning pre-trained large language models (LLMs) with limited hardware presents challenges due to GPU memory constraints. Various distributed fine-tuning methods have been proposed to alleviate memory constraints on GPU. However, determining the most effective method for achieving rapid fine-tuning while preventing GPU out-of-memory issues in a given environment remains unclear. To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed fine-tuning methods across multiple GPUs and identifies the optimal method. We conduct GPU memory usage estimation prior to fine-tuning, leveraging the fundamental structure of transformer-based decoder models and the memory usage distribution of each method. Experimental results show that LLMem accurately estimates peak GPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally, it shows an average error rate of 3.0% when applying distributed fine-tuning methods to LLMs with more than a billion parameters on multi-GPU setups.","sentences":["Fine-tuning pre-trained large language models (LLMs) with limited hardware presents challenges due to GPU memory constraints.","Various distributed fine-tuning methods have been proposed to alleviate memory constraints on GPU.","However, determining the most effective method for achieving rapid fine-tuning while preventing GPU out-of-memory issues in a given environment remains unclear.","To address this challenge, we introduce LLMem, a solution that estimates the GPU memory consumption when applying distributed fine-tuning methods across multiple GPUs and identifies the optimal method.","We conduct GPU memory usage estimation prior to fine-tuning, leveraging the fundamental structure of transformer-based decoder models and the memory usage distribution of each method.","Experimental results show that LLMem accurately estimates peak GPU memory usage on a single GPU, with error rates of up to 1.6%.","Additionally, it shows an average error rate of 3.0% when applying distributed fine-tuning methods to LLMs with more than a billion parameters on multi-GPU setups."],"url":"http://arxiv.org/abs/2404.10933v1","category":"cs.AI"}
{"created":"2024-04-16 21:52:55","title":"Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors","abstract":"For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges.","sentences":["For natural language understanding and generation, embedding concepts using an order-based representation is an essential task.","Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts.","In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding.","Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation.","Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space.","In this work, we propose Binder, a novel approach for order-based representation.","Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods.","Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity.","Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task.","But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges."],"url":"http://arxiv.org/abs/2404.10924v1","category":"cs.CL"}
{"created":"2024-04-16 20:53:58","title":"Causal Effect Estimation Using Random Hyperplane Tessellations","abstract":"Matching is one of the simplest approaches for estimating causal effects from observational data. Matching techniques compare the observed outcomes across pairs of individuals with similar covariate values but different treatment statuses in order to estimate causal effects. However, traditional matching techniques are unreliable given high-dimensional covariates due to the infamous curse of dimensionality. To overcome this challenge, we propose a simple, fast, yet highly effective approach to matching using Random Hyperplane Tessellations (RHPT). First, we prove that the RHPT representation is an approximate balancing score -- thus maintaining the strong ignorability assumption -- and provide empirical evidence for this claim. Second, we report results of extensive experiments showing that matching using RHPT outperforms traditional matching techniques and is competitive with state-of-the-art deep learning methods for causal effect estimation. In addition, RHPT avoids the need for computationally expensive training of deep neural networks.","sentences":["Matching is one of the simplest approaches for estimating causal effects from observational data.","Matching techniques compare the observed outcomes across pairs of individuals with similar covariate values but different treatment statuses in order to estimate causal effects.","However, traditional matching techniques are unreliable given high-dimensional covariates due to the infamous curse of dimensionality.","To overcome this challenge, we propose a simple, fast, yet highly effective approach to matching using Random Hyperplane Tessellations (RHPT).","First, we prove that the RHPT representation is an approximate balancing score -- thus maintaining the strong ignorability assumption -- and provide empirical evidence for this claim.","Second, we report results of extensive experiments showing that matching using RHPT outperforms traditional matching techniques and is competitive with state-of-the-art deep learning methods for causal effect estimation.","In addition, RHPT avoids the need for computationally expensive training of deep neural networks."],"url":"http://arxiv.org/abs/2404.10907v1","category":"cs.AI"}
{"created":"2024-04-16 20:53:17","title":"Towards a Research Community in Interpretable Reinforcement Learning: the InterpPol Workshop","abstract":"Embracing the pursuit of intrinsically explainable reinforcement learning raises crucial questions: what distinguishes explainability from interpretability? Should explainable and interpretable agents be developed outside of domains where transparency is imperative? What advantages do interpretable policies offer over neural networks? How can we rigorously define and measure interpretability in policies, without user studies? What reinforcement learning paradigms,are the most suited to develop interpretable agents? Can Markov Decision Processes integrate interpretable state representations? In addition to motivate an Interpretable RL community centered around the aforementioned questions, we propose the first venue dedicated to Interpretable RL: the InterpPol Workshop.","sentences":["Embracing the pursuit of intrinsically explainable reinforcement learning raises crucial questions: what distinguishes explainability from interpretability?","Should explainable and interpretable agents be developed outside of domains where transparency is imperative?","What advantages do interpretable policies offer over neural networks?","How can we rigorously define and measure interpretability in policies, without user studies?","What reinforcement learning paradigms,are the most suited to develop interpretable agents?","Can Markov Decision Processes integrate interpretable state representations?","In addition to motivate an Interpretable RL community centered around the aforementioned questions, we propose the first venue dedicated to Interpretable RL: the InterpPol Workshop."],"url":"http://arxiv.org/abs/2404.10906v1","category":"cs.AI"}
{"created":"2024-04-16 20:40:59","title":"CrossGP: Cross-Day Glucose Prediction Excluding Physiological Information","abstract":"The increasing number of diabetic patients is a serious issue in society today, which has significant negative impacts on people's health and the country's financial expenditures. Because diabetes may develop into potential serious complications, early glucose prediction for diabetic patients is necessary for timely medical treatment. Existing glucose prediction methods typically utilize patients' private data (e.g. age, gender, ethnicity) and physiological parameters (e.g. blood pressure, heart rate) as reference features for glucose prediction, which inevitably leads to privacy protection concerns. Moreover, these models generally focus on either long-term (monthly-based) or short-term (minute-based) predictions. Long-term prediction methods are generally inaccurate because of the external uncertainties that can greatly affect the glucose values, while short-term ones fail to provide timely medical guidance. Based on the above issues, we propose CrossGP, a novel machine-learning framework for cross-day glucose prediction solely based on the patient's external activities without involving any physiological parameters. Meanwhile, we implement three baseline models for comparison. Extensive experiments on Anderson's dataset strongly demonstrate the superior performance of CrossGP and prove its potential for future real-life applications.","sentences":["The increasing number of diabetic patients is a serious issue in society today, which has significant negative impacts on people's health and the country's financial expenditures.","Because diabetes may develop into potential serious complications, early glucose prediction for diabetic patients is necessary for timely medical treatment.","Existing glucose prediction methods typically utilize patients' private data (e.g. age, gender, ethnicity) and physiological parameters (e.g. blood pressure, heart rate) as reference features for glucose prediction, which inevitably leads to privacy protection concerns.","Moreover, these models generally focus on either long-term (monthly-based) or short-term (minute-based) predictions.","Long-term prediction methods are generally inaccurate because of the external uncertainties that can greatly affect the glucose values, while short-term ones fail to provide timely medical guidance.","Based on the above issues, we propose CrossGP, a novel machine-learning framework for cross-day glucose prediction solely based on the patient's external activities without involving any physiological parameters.","Meanwhile, we implement three baseline models for comparison.","Extensive experiments on Anderson's dataset strongly demonstrate the superior performance of CrossGP and prove its potential for future real-life applications."],"url":"http://arxiv.org/abs/2404.10901v1","category":"cs.AI"}
{"created":"2024-04-16 20:39:42","title":"The Future of Research on Social Technologies: CCC Workshop Visioning Report","abstract":"Social technologies are the systems, interfaces, features, infrastructures, and architectures that allow people to interact with each other online. These technologies dramatically shape the fabric of our everyday lives, from the information we consume to the people we interact with to the foundations of our culture and politics. While the benefits of social technologies are well documented, the harms, too, have cast a long shadow. To address widespread problems like harassment, disinformation, information access, and mental health concerns, we need to rethink the foundations of how social technologies are designed, sustained, and governed.   This report is based on discussions at the Computing Community Consortium Workshop, The Future of Research on Social Technologies, that was held November 2-3, 2023 in Washington, DC. The visioning workshop came together to focus on two questions. What should we know about social technologies, and what is needed to get there? The workshop brought together over 50 information and computer scientists, social scientists, communication and journalism scholars, and policy experts. We used a discussion format, with one day of guiding topics and a second day using an unconference model where participants created discussion topics. The interdisciplinary group of attendees discussed gaps in existing scholarship and the methods, resources, access, and collective effort needed to address those gaps. We also discussed approaches for translating scholarship for various audiences including citizens, funders, educators, industry professionals, and policymakers.   This report presents a synthesis of major themes during our discussions. The themes presented are not a summary of what we know already, they are an exploration of what we do not know enough about, and what we should spend more effort and investment on in the coming years.","sentences":["Social technologies are the systems, interfaces, features, infrastructures, and architectures that allow people to interact with each other online.","These technologies dramatically shape the fabric of our everyday lives, from the information we consume to the people we interact with to the foundations of our culture and politics.","While the benefits of social technologies are well documented, the harms, too, have cast a long shadow.","To address widespread problems like harassment, disinformation, information access, and mental health concerns, we need to rethink the foundations of how social technologies are designed, sustained, and governed.   ","This report is based on discussions at the Computing Community Consortium Workshop, The Future of Research on Social Technologies, that was held November 2-3, 2023 in Washington, DC.","The visioning workshop came together to focus on two questions.","What should we know about social technologies, and what is needed to get there?","The workshop brought together over 50 information and computer scientists, social scientists, communication and journalism scholars, and policy experts.","We used a discussion format, with one day of guiding topics and a second day using an unconference model where participants created discussion topics.","The interdisciplinary group of attendees discussed gaps in existing scholarship and the methods, resources, access, and collective effort needed to address those gaps.","We also discussed approaches for translating scholarship for various audiences including citizens, funders, educators, industry professionals, and policymakers.   ","This report presents a synthesis of major themes during our discussions.","The themes presented are not a summary of what we know already, they are an exploration of what we do not know enough about, and what we should spend more effort and investment on in the coming years."],"url":"http://arxiv.org/abs/2404.10897v1","category":"cs.SI"}
{"created":"2024-04-16 20:37:54","title":"From a Lossless (~1.5:1) Compression Algorithm for Llama2 7B Weights to Variable Precision, Variable Range, Compressed Numeric Data Types for CNNs and LLMs","abstract":"This paper starts with a simple lossless ~1.5:1 compression algorithm for the weights of the Large Language Model (LLM) Llama2 7B [1] that can be implemented in ~200 LUTs in AMD FPGAs, processing over 800 million bfloat16 numbers per second. This framework is then extended to variable precision, variable range, compressed numerical data types that are a user defined super set of both floats and posits [2]. The paper then discusses a simple hardware implementation of such format based on ANS (Asymmetrical Numeral Systems) [3] that acts as a bridge between this flexible data format and a computational engine while, at the same time, achieving bandwidth reduction. An example of a token factory using weight compression and sharing is also given.","sentences":["This paper starts with a simple lossless ~1.5:1 compression algorithm for the weights of the Large Language Model (LLM) Llama2 7B","[1] that can be implemented in ~200 LUTs in AMD FPGAs, processing over 800 million bfloat16 numbers per second.","This framework is then extended to variable precision, variable range, compressed numerical data types that are a user defined super set of both floats and posits [2].","The paper then discusses a simple hardware implementation of such format based on ANS (Asymmetrical Numeral Systems)","[3] that acts as a bridge between this flexible data format and a computational engine while, at the same time, achieving bandwidth reduction.","An example of a token factory using weight compression and sharing is also given."],"url":"http://arxiv.org/abs/2404.10896v1","category":"cs.CV"}
{"created":"2024-04-16 20:34:43","title":"Characterization of Capacity and Outage of RIS-aided Downlink Systems under Rician Fading","abstract":"This letter presents optimal beamforming and outage analysis for a Reconfigurable Intelligent Surface (RIS)-aided multiple input single output downlink system under Rician fading on both the direct and the RIS-assisted indirect links. We focus on maximizing the capacity for two transmitter architectures: fully digital (FD) and fully analog (FA). This capacity maximization problem with optimally configured RIS is shown to be $L_1$ norm-maximization with respect to the transmit beamformer. To obtain the optimal FD beamformer, we propose a complex $L_1$-PCA-based algorithm whose complexity is significantly lower than the existing semi-definite relaxation-based solutions. We also propose a low-complexity optimal beamforming algorithm to obtain the FA beamformer solution. Further, we derive analytical upper bounds on the SNR achievable by the proposed algorithms and utilize them to characterize the lower bounds on outage probabilities. The derived bounds are numerically shown to closely match the achievable performance for a low-rank channel matrix and are shown to be exact for a unit-rank channel matrix.","sentences":["This letter presents optimal beamforming and outage analysis for a Reconfigurable Intelligent Surface (RIS)-aided multiple input single output downlink system under Rician fading on both the direct and the RIS-assisted indirect links.","We focus on maximizing the capacity for two transmitter architectures: fully digital (FD) and fully analog (FA).","This capacity maximization problem with optimally configured RIS is shown to be $L_1$ norm-maximization with respect to the transmit beamformer.","To obtain the optimal FD beamformer, we propose a complex $L_1$-PCA-based algorithm whose complexity is significantly lower than the existing semi-definite relaxation-based solutions.","We also propose a low-complexity optimal beamforming algorithm to obtain the FA beamformer solution.","Further, we derive analytical upper bounds on the SNR achievable by the proposed algorithms and utilize them to characterize the lower bounds on outage probabilities.","The derived bounds are numerically shown to closely match the achievable performance for a low-rank channel matrix and are shown to be exact for a unit-rank channel matrix."],"url":"http://arxiv.org/abs/2404.10893v1","category":"cs.IT"}
{"created":"2024-04-16 20:30:16","title":"Automatic classification of prostate MR series type using image content and metadata","abstract":"With the wealth of medical image data, efficient curation is essential. Assigning the sequence type to magnetic resonance images is necessary for scientific studies and artificial intelligence-based analysis. However, incomplete or missing metadata prevents effective automation. We therefore propose a deep-learning method for classification of prostate cancer scanning sequences based on a combination of image data and DICOM metadata. We demonstrate superior results compared to metadata or image data alone, and make our code publicly available at https://github.com/deepakri201/DICOMScanClassification.","sentences":["With the wealth of medical image data, efficient curation is essential.","Assigning the sequence type to magnetic resonance images is necessary for scientific studies and artificial intelligence-based analysis.","However, incomplete or missing metadata prevents effective automation.","We therefore propose a deep-learning method for classification of prostate cancer scanning sequences based on a combination of image data and DICOM metadata.","We demonstrate superior results compared to metadata or image data alone, and make our code publicly available at https://github.com/deepakri201/DICOMScanClassification."],"url":"http://arxiv.org/abs/2404.10892v1","category":"eess.IV"}
{"created":"2024-04-16 20:22:12","title":"Exploring Augmentation and Cognitive Strategies for AI based Synthetic Personae","abstract":"Large language models (LLMs) hold potential for innovative HCI research, including the creation of synthetic personae. However, their black-box nature and propensity for hallucinations pose challenges. To address these limitations, this position paper advocates for using LLMs as data augmentation systems rather than zero-shot generators. We further propose the development of robust cognitive and memory frameworks to guide LLM responses. Initial explorations suggest that data enrichment, episodic memory, and self-reflection techniques can improve the reliability of synthetic personae and open up new avenues for HCI research.","sentences":["Large language models (LLMs) hold potential for innovative HCI research, including the creation of synthetic personae.","However, their black-box nature and propensity for hallucinations pose challenges.","To address these limitations, this position paper advocates for using LLMs as data augmentation systems rather than zero-shot generators.","We further propose the development of robust cognitive and memory frameworks to guide LLM responses.","Initial explorations suggest that data enrichment, episodic memory, and self-reflection techniques can improve the reliability of synthetic personae and open up new avenues for HCI research."],"url":"http://arxiv.org/abs/2404.10890v1","category":"cs.AI"}
{"created":"2024-04-16 20:20:23","title":"Cognitive-Motor Integration in Assessing Bimanual Motor Skills","abstract":"Accurate assessment of bimanual motor skills is essential across various professions, yet, traditional methods often rely on subjective assessments or focus solely on motor actions, overlooking the integral role of cognitive processes. This study introduces a novel approach by leveraging deep neural networks (DNNs) to analyze and integrate both cognitive decision-making and motor execution. We tested this methodology by assessing laparoscopic surgery skills within the Fundamentals of Laparoscopic Surgery program, which is a prerequisite for general surgery certification. Utilizing video capture of motor actions and non-invasive functional near-infrared spectroscopy (fNIRS) for measuring neural activations, our approach precisely classifies subjects by expertise level and predicts FLS behavioral performance scores, significantly surpassing traditional single-modality assessments.","sentences":["Accurate assessment of bimanual motor skills is essential across various professions, yet, traditional methods often rely on subjective assessments or focus solely on motor actions, overlooking the integral role of cognitive processes.","This study introduces a novel approach by leveraging deep neural networks (DNNs) to analyze and integrate both cognitive decision-making and motor execution.","We tested this methodology by assessing laparoscopic surgery skills within the Fundamentals of Laparoscopic Surgery program, which is a prerequisite for general surgery certification.","Utilizing video capture of motor actions and non-invasive functional near-infrared spectroscopy (fNIRS) for measuring neural activations, our approach precisely classifies subjects by expertise level and predicts FLS behavioral performance scores, significantly surpassing traditional single-modality assessments."],"url":"http://arxiv.org/abs/2404.10889v1","category":"cs.AI"}
{"created":"2024-04-16 20:15:32","title":"Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning","abstract":"Traditional search systems focus on query formulation for effective results but face challenges in scenarios such as product searches where crucial product details (e.g., size, color) remain concealed until users visit specific product pages. This highlights the need for intelligent web navigation agents capable of formulating queries and navigating web pages according to users' high-level intents. In response to this need, this work introduces a Grounded Language Agent for Intelligent Web Interactions, called GLAINTEL. Drawing upon advancements in language modeling and reinforcement learning, GLAINTEL investigates the efficacy of transformer-based models in enhancing the search capabilities of interactive web environments. Given the dynamic action space for each state in web navigation, GLAINTEL employs the Flan-T5 architecture and incorporates language modeling and value estimation heads. This work focuses on training smaller language models as agents across various scenarios, systematically evaluating the impact of human demonstrations on the training process. Specifically, we investigate scenarios where no human demonstrations are available and subsequently assess the effective utilization of such demonstrations. We also explore unsupervised domain adaptation for situations where demonstrations are confined to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of training agents in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised learning-based methods. Additionally, combining human demonstrations with Reinforcement Learning-based training yields results comparable to models utilizing GPT-4.","sentences":["Traditional search systems focus on query formulation for effective results but face challenges in scenarios such as product searches where crucial product details (e.g., size, color) remain concealed until users visit specific product pages.","This highlights the need for intelligent web navigation agents capable of formulating queries and navigating web pages according to users' high-level intents.","In response to this need, this work introduces a Grounded Language Agent for Intelligent Web Interactions, called GLAINTEL.","Drawing upon advancements in language modeling and reinforcement learning, GLAINTEL investigates the efficacy of transformer-based models in enhancing the search capabilities of interactive web environments.","Given the dynamic action space for each state in web navigation, GLAINTEL employs the Flan-T5 architecture and incorporates language modeling and value estimation heads.","This work focuses on training smaller language models as agents across various scenarios, systematically evaluating the impact of human demonstrations on the training process.","Specifically, we investigate scenarios where no human demonstrations are available and subsequently assess the effective utilization of such demonstrations.","We also explore unsupervised domain adaptation for situations where demonstrations are confined to a specific domain.","Experimental evaluations across diverse setups demonstrate the effectiveness of training agents in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters.","Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised learning-based methods.","Additionally, combining human demonstrations with Reinforcement Learning-based training yields results comparable to models utilizing GPT-4."],"url":"http://arxiv.org/abs/2404.10887v1","category":"cs.CL"}
{"created":"2024-04-16 20:04:29","title":"Automated Discovery of Functional Actual Causes in Complex Environments","abstract":"Reinforcement learning (RL) algorithms often struggle to learn policies that generalize to novel situations due to issues such as causal confusion, overfitting to irrelevant factors, and failure to isolate control of state factors. These issues stem from a common source: a failure to accurately identify and exploit state-specific causal relationships in the environment. While some prior works in RL aim to identify these relationships explicitly, they rely on informal domain-specific heuristics such as spatial and temporal proximity. Actual causality offers a principled and general framework for determining the causes of particular events. However, existing definitions of actual cause often attribute causality to a large number of events, even if many of them rarely influence the outcome. Prior work on actual causality proposes normality as a solution to this problem, but its existing implementations are challenging to scale to complex and continuous-valued RL environments. This paper introduces functional actual cause (FAC), a framework that uses context-specific independencies in the environment to restrict the set of actual causes. We additionally introduce Joint Optimization for Actual Cause Inference (JACI), an algorithm that learns from observational data to infer functional actual causes. We demonstrate empirically that FAC agrees with known results on a suite of examples from the actual causality literature, and JACI identifies actual causes with significantly higher accuracy than existing heuristic methods in a set of complex, continuous-valued environments.","sentences":["Reinforcement learning (RL) algorithms often struggle to learn policies that generalize to novel situations due to issues such as causal confusion, overfitting to irrelevant factors, and failure to isolate control of state factors.","These issues stem from a common source: a failure to accurately identify and exploit state-specific causal relationships in the environment.","While some prior works in RL aim to identify these relationships explicitly, they rely on informal domain-specific heuristics such as spatial and temporal proximity.","Actual causality offers a principled and general framework for determining the causes of particular events.","However, existing definitions of actual cause often attribute causality to a large number of events, even if many of them rarely influence the outcome.","Prior work on actual causality proposes normality as a solution to this problem, but its existing implementations are challenging to scale to complex and continuous-valued RL environments.","This paper introduces functional actual cause (FAC), a framework that uses context-specific independencies in the environment to restrict the set of actual causes.","We additionally introduce Joint Optimization for Actual Cause Inference (JACI), an algorithm that learns from observational data to infer functional actual causes.","We demonstrate empirically that FAC agrees with known results on a suite of examples from the actual causality literature, and JACI identifies actual causes with significantly higher accuracy than existing heuristic methods in a set of complex, continuous-valued environments."],"url":"http://arxiv.org/abs/2404.10883v1","category":"cs.AI"}
{"created":"2024-04-16 19:59:21","title":"HumMUSS: Human Motion Understanding using State Space Models","abstract":"Understanding human motion from video is essential for a range of applications, including pose estimation, mesh recovery and action recognition. While state-of-the-art methods predominantly rely on transformer-based architectures, these approaches have limitations in practical scenarios. Transformers are slower when sequentially predicting on a continuous stream of frames in real-time, and do not generalize to new frame rates. In light of these constraints, we propose a novel attention-free spatiotemporal model for human motion understanding building upon recent advancements in state space models. Our model not only matches the performance of transformer-based models in various motion understanding tasks but also brings added benefits like adaptability to different video frame rates and enhanced training speed when working with longer sequence of keypoints. Moreover, the proposed model supports both offline and real-time applications. For real-time sequential prediction, our model is both memory efficient and several times faster than transformer-based approaches while maintaining their high accuracy.","sentences":["Understanding human motion from video is essential for a range of applications, including pose estimation, mesh recovery and action recognition.","While state-of-the-art methods predominantly rely on transformer-based architectures, these approaches have limitations in practical scenarios.","Transformers are slower when sequentially predicting on a continuous stream of frames in real-time, and do not generalize to new frame rates.","In light of these constraints, we propose a novel attention-free spatiotemporal model for human motion understanding building upon recent advancements in state space models.","Our model not only matches the performance of transformer-based models in various motion understanding tasks but also brings added benefits like adaptability to different video frame rates and enhanced training speed when working with longer sequence of keypoints.","Moreover, the proposed model supports both offline and real-time applications.","For real-time sequential prediction, our model is both memory efficient and several times faster than transformer-based approaches while maintaining their high accuracy."],"url":"http://arxiv.org/abs/2404.10880v1","category":"cs.CV"}
{"created":"2024-04-16 19:52:26","title":"A Dataset for Large Language Model-Driven AI Accelerator Generation","abstract":"In the ever-evolving landscape of Deep Neural Networks (DNN) hardware acceleration, unlocking the true potential of systolic array accelerators has long been hindered by the daunting challenges of expertise and time investment. Large Language Models (LLMs) offer a promising solution for automating code generation which is key to unlocking unprecedented efficiency and performance in various domains, including hardware descriptive code. However, the successful application of LLMs to hardware accelerator design is contingent upon the availability of specialized datasets tailored for this purpose. To bridge this gap, we introduce the Systolic Array-based Accelerator DataSet (SA-DS). SA-DS comprises of a diverse collection of spatial arrays following the standardized Berkeley's Gemmini accelerator generator template, enabling design reuse, adaptation, and customization. SA-DS is intended to spark LLM-centred research on DNN hardware accelerator architecture. We envision that SA-DS provides a framework which will shape the course of DNN hardware acceleration research for generations to come. SA-DS is open-sourced under the permissive MIT license at this https://github.com/ACADLab/SA-DS.","sentences":["In the ever-evolving landscape of Deep Neural Networks (DNN) hardware acceleration, unlocking the true potential of systolic array accelerators has long been hindered by the daunting challenges of expertise and time investment.","Large Language Models (LLMs) offer a promising solution for automating code generation which is key to unlocking unprecedented efficiency and performance in various domains, including hardware descriptive code.","However, the successful application of LLMs to hardware accelerator design is contingent upon the availability of specialized datasets tailored for this purpose.","To bridge this gap, we introduce the Systolic Array-based Accelerator DataSet (SA-DS).","SA-DS comprises of a diverse collection of spatial arrays following the standardized Berkeley's Gemmini accelerator generator template, enabling design reuse, adaptation, and customization.","SA-DS is intended to spark LLM-centred research on DNN hardware accelerator architecture.","We envision that SA-DS provides a framework which will shape the course of DNN hardware acceleration research for generations to come.","SA-DS is open-sourced under the permissive MIT license at this https://github.com/ACADLab/SA-DS."],"url":"http://arxiv.org/abs/2404.10875v1","category":"cs.AR"}
{"created":"2024-04-16 19:48:35","title":"Measurement of the branching fraction of the decay $B^- \\to D^0 \u03c1(770)^-$ at Belle II","abstract":"We measure the branching fraction of the decay $B^- \\to D^0 \\rho(770)^-$ using data collected with the Belle II detector. The data contain 387 million $B\\overline{B}$ pairs produced in $e^+e^-$ collisions at the $\\Upsilon(4S)$ resonance. We reconstruct $8360\\pm 180$ decays from an analysis of the distributions of the $B^-$ energy and the $\\rho(770)^-$ helicity angle. We determine the branching fraction to be $(0.939 \\pm 0.021\\mathrm{(stat)} \\pm 0.050\\mathrm{(syst)})\\%$, in agreement with previous results. Our measurement improves the relative precision of the world average by more than a factor of two.","sentences":["We measure the branching fraction of the decay $B^- \\to D^0 \\rho(770)^-$ using data collected with the Belle II detector.","The data contain 387 million $B\\overline{B}$ pairs produced in $e^+e^-$ collisions at the $\\Upsilon(4S)$ resonance.","We reconstruct $8360\\pm 180$ decays from an analysis of the distributions of the $B^-$ energy and the $\\rho(770)^-$ helicity angle.","We determine the branching fraction to be $(0.939 \\pm 0.021\\mathrm{(stat)} \\pm 0.050\\mathrm{(syst)})\\%$, in agreement with previous results.","Our measurement improves the relative precision of the world average by more than a factor of two."],"url":"http://arxiv.org/abs/2404.10874v1","category":"hep-ex"}
{"created":"2024-04-16 19:12:03","title":"D3CODE: Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation","abstract":"While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection. Recent studies that have critically examined this issue are often situated in the Western context, and solely document differences across age, gender, or racial groups. As a result, NLP research on subjectivity have overlooked the fact that individuals within demographic groups may hold diverse values, which can influence their perceptions beyond their group norms. To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from various social and cultural groups. In this paper we introduce the \\dataset dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions. The dataset contains annotators' moral values captured along six moral foundations: care, equality, proportionality, authority, loyalty, and purity. Our analyses reveal substantial regional variations in annotators' perceptions that are shaped by individual moral values, offering crucial insights for building pluralistic, culturally sensitive NLP models.","sentences":["While human annotations play a crucial role in language technologies, annotator subjectivity has long been overlooked in data collection.","Recent studies that have critically examined this issue are often situated in the Western context, and solely document differences across age, gender, or racial groups.","As a result, NLP research on subjectivity have overlooked the fact that individuals within demographic groups may hold diverse values, which can influence their perceptions beyond their group norms.","To effectively incorporate these considerations into NLP pipelines, we need datasets with extensive parallel annotations from various social and cultural groups.","In this paper we introduce the \\dataset dataset: a large-scale cross-cultural dataset of parallel annotations for offensive language in over 4.5K sentences annotated by a pool of over 4k annotators, balanced across gender and age, from across 21 countries, representing eight geo-cultural regions.","The dataset contains annotators' moral values captured along six moral foundations: care, equality, proportionality, authority, loyalty, and purity.","Our analyses reveal substantial regional variations in annotators' perceptions that are shaped by individual moral values, offering crucial insights for building pluralistic, culturally sensitive NLP models."],"url":"http://arxiv.org/abs/2404.10857v1","category":"cs.CL"}
{"created":"2024-04-16 19:10:40","title":"UruDendro, a public dataset of cross-section images of Pinus taeda","abstract":"The automatic detection of tree-ring boundaries and other anatomical features using image analysis has progressed substantially over the past decade with advances in machine learning and imagery technology, as well as increasing demands from the dendrochronology community. This paper presents a publicly available database of 64 scanned images of transverse sections of commercially grown Pinus taeda trees from northern Uruguay, ranging from 17 to 24 years old. The collection contains several challenging features for automatic ring detection, including illumination and surface preparation variation, fungal infection (blue stains), knot formation, missing cortex or interruptions in outer rings, and radial cracking. This dataset can be used to develop and test automatic tree ring detection algorithms. This paper presents to the dendrochronology community one such method, Cross-Section Tree-Ring Detection (CS-TRD), which identifies and marks complete annual rings in cross-sections for tree species presenting a clear definition between early and latewood. We compare the CS-TRD performance against the ground truth manual delineation of all rings over the UruDendro dataset. The CS-TRD software identified rings with an average F-score of 89% and RMSE error of 5.27px for the entire database in less than 20 seconds per image. Finally, we propose a robust measure of the ring growth using the \\emph{equivalent radius} of a circle having the same area enclosed by the detected tree ring. Overall, this study contributes to the dendrochronologist's toolbox of fast and low-cost methods to automatically detect rings in conifer species, particularly for measuring diameter growth rates and stem transverse area using entire cross-sections.","sentences":["The automatic detection of tree-ring boundaries and other anatomical features using image analysis has progressed substantially over the past decade with advances in machine learning and imagery technology, as well as increasing demands from the dendrochronology community.","This paper presents a publicly available database of 64 scanned images of transverse sections of commercially grown Pinus taeda trees from northern Uruguay, ranging from 17 to 24 years old.","The collection contains several challenging features for automatic ring detection, including illumination and surface preparation variation, fungal infection (blue stains), knot formation, missing cortex or interruptions in outer rings, and radial cracking.","This dataset can be used to develop and test automatic tree ring detection algorithms.","This paper presents to the dendrochronology community one such method, Cross-Section Tree-Ring Detection (CS-TRD), which identifies and marks complete annual rings in cross-sections for tree species presenting a clear definition between early and latewood.","We compare the CS-TRD performance against the ground truth manual delineation of all rings over the UruDendro dataset.","The CS-TRD software identified rings with an average F-score of 89% and RMSE error of 5.27px for the entire database in less than 20 seconds per image.","Finally, we propose a robust measure of the ring growth using the \\emph{equivalent radius} of a circle having the same area enclosed by the detected tree ring.","Overall, this study contributes to the dendrochronologist's toolbox of fast and low-cost methods to automatically detect rings in conifer species, particularly for measuring diameter growth rates and stem transverse area using entire cross-sections."],"url":"http://arxiv.org/abs/2404.10856v1","category":"cs.CV"}
{"created":"2024-04-16 19:01:25","title":"Efficient 6-dimensional phase space reconstruction from experimental measurements using generative machine learning","abstract":"Next-generation accelerator concepts which hinge on the precise shaping of beam distributions, demand equally precise diagnostic methods capable of reconstructing beam distributions within 6-dimensional position-momentum spaces. However, the characterization of intricate features within 6-dimensional beam distributions using conventional diagnostic techniques necessitates hundreds of measurements, using many hours of valuable beam time. Novel phase space reconstruction techniques are needed to substantially reduce the number of measurements required to reconstruct detailed, high-dimensional beam features in order to resolve complex beam phenomena, and as feedback in precision beam shaping applications. In this study, we present a novel approach to reconstructing detailed 6-dimensional phase space distributions from experimental measurements using generative machine learning and differentiable beam dynamics simulations. We demonstrate that for a collection of synthetic beam distribution test cases that this approach can be used to resolve 6-dimensional phase space distributions using basic beam manipulations and as few as 20 2-dimensional measurements of the beam profile, without the need for prior data collection or model training. We also demonstrate an application of the reconstruction method in an experimental setting at the Argonne Wakefield Accelerator, where it is able to reconstruct the beam distribution and accurately predict previously unseen measurements 75x faster than previous methods.","sentences":["Next-generation accelerator concepts which hinge on the precise shaping of beam distributions, demand equally precise diagnostic methods capable of reconstructing beam distributions within 6-dimensional position-momentum spaces.","However, the characterization of intricate features within 6-dimensional beam distributions using conventional diagnostic techniques necessitates hundreds of measurements, using many hours of valuable beam time.","Novel phase space reconstruction techniques are needed to substantially reduce the number of measurements required to reconstruct detailed, high-dimensional beam features in order to resolve complex beam phenomena, and as feedback in precision beam shaping applications.","In this study, we present a novel approach to reconstructing detailed 6-dimensional phase space distributions from experimental measurements using generative machine learning and differentiable beam dynamics simulations.","We demonstrate that for a collection of synthetic beam distribution test cases that this approach can be used to resolve 6-dimensional phase space distributions using basic beam manipulations and as few as 20 2-dimensional measurements of the beam profile, without the need for prior data collection or model training.","We also demonstrate an application of the reconstruction method in an experimental setting at the Argonne Wakefield Accelerator, where it is able to reconstruct the beam distribution and accurately predict previously unseen measurements 75x faster than previous methods."],"url":"http://arxiv.org/abs/2404.10853v1","category":"physics.acc-ph"}
{"created":"2024-04-16 18:53:49","title":"Convergence of Recursive Least Squares Based Input/Output System Identification with Model Order Mismatch","abstract":"Discrete-time input/output models, also called infinite impulse response (IIR) models or autoregressive moving average (ARMA) models, are useful for online identification as they can be efficiently updated using recursive least squares (RLS) as new data is collected. Several works have studied the convergence of the input/output model coefficients identified using RLS under the assumption that the order of the identified model is the same as that of the true system. However, the case of model order mismatch is not as well addressed. This work begins by introducing the notion of \\textit{equivalence} of input/output models of different orders. Next, this work analyzes online identification of input/output models in the case where the order of the identified model is higher than that of the true system. It is shown that, given persistently exciting data, the higher-order identified model converges to the model equivalent to the true system that minimizes the regularization term of RLS.","sentences":["Discrete-time input/output models, also called infinite impulse response (IIR) models or autoregressive moving average (ARMA) models, are useful for online identification as they can be efficiently updated using recursive least squares (RLS) as new data is collected.","Several works have studied the convergence of the input/output model coefficients identified using RLS under the assumption that the order of the identified model is the same as that of the true system.","However, the case of model order mismatch is not as well addressed.","This work begins by introducing the notion of \\textit{equivalence} of input/output models of different orders.","Next, this work analyzes online identification of input/output models in the case where the order of the identified model is higher than that of the true system.","It is shown that, given persistently exciting data, the higher-order identified model converges to the model equivalent to the true system that minimizes the regularization term of RLS."],"url":"http://arxiv.org/abs/2404.10850v1","category":"eess.SY"}
{"created":"2024-04-16 18:51:58","title":"End-To-End Training and Testing Gamification Framework to Learn Human Highway Driving","abstract":"The current autonomous stack is well modularized and consists of perception, decision making and control in a handcrafted framework. With the advances in artificial intelligence (AI) and computing resources, researchers have been pushing the development of end-to-end AI for autonomous driving, at least in problems of small searching space such as in highway scenarios, and more and more photorealistic simulation will be critical for efficient learning. In this research, we propose a novel game-based end-to-end learning and testing framework for autonomous vehicle highway driving, by learning from human driving skills. Firstly, we utilize the popular game Grand Theft Auto V (GTA V) to collect highway driving data with our proposed programmable labels. Then, an end-to-end architecture predicts the steering and throttle values that control the vehicle by the image of the game screen. The predicted control values are sent to the game via a virtual controller to keep the vehicle in lane and avoid collisions with other vehicles on the road. The proposed solution is validated in GTA V games, and the results demonstrate the effectiveness of this end-to-end gamification framework for learning human driving skills.","sentences":["The current autonomous stack is well modularized and consists of perception, decision making and control in a handcrafted framework.","With the advances in artificial intelligence (AI) and computing resources, researchers have been pushing the development of end-to-end AI for autonomous driving, at least in problems of small searching space such as in highway scenarios, and more and more photorealistic simulation will be critical for efficient learning.","In this research, we propose a novel game-based end-to-end learning and testing framework for autonomous vehicle highway driving, by learning from human driving skills.","Firstly, we utilize the popular game Grand Theft Auto V (GTA V) to collect highway driving data with our proposed programmable labels.","Then, an end-to-end architecture predicts the steering and throttle values that control the vehicle by the image of the game screen.","The predicted control values are sent to the game via a virtual controller to keep the vehicle in lane and avoid collisions with other vehicles on the road.","The proposed solution is validated in GTA V games, and the results demonstrate the effectiveness of this end-to-end gamification framework for learning human driving skills."],"url":"http://arxiv.org/abs/2404.10849v1","category":"cs.RO"}
{"created":"2024-04-16 18:50:57","title":"A LayoutLMv3-Based Model for Enhanced Relation Extraction in Visually-Rich Documents","abstract":"Document Understanding is an evolving field in Natural Language Processing (NLP). In particular, visual and spatial features are essential in addition to the raw text itself and hence, several multimodal models were developed in the field of Visual Document Understanding (VDU). However, while research is mainly focused on Key Information Extraction (KIE), Relation Extraction (RE) between identified entities is still under-studied. For instance, RE is crucial to regroup entities or obtain a comprehensive hierarchy of data in a document. In this paper, we present a model that, initialized from LayoutLMv3, can match or outperform the current state-of-the-art results in RE applied to Visually-Rich Documents (VRD) on FUNSD and CORD datasets, without any specific pre-training and with fewer parameters. We also report an extensive ablation study performed on FUNSD, highlighting the great impact of certain features and modelization choices on the performances.","sentences":["Document Understanding is an evolving field in Natural Language Processing (NLP).","In particular, visual and spatial features are essential in addition to the raw text itself and hence, several multimodal models were developed in the field of Visual Document Understanding (VDU).","However, while research is mainly focused on Key Information Extraction (KIE), Relation Extraction (RE) between identified entities is still under-studied.","For instance, RE is crucial to regroup entities or obtain a comprehensive hierarchy of data in a document.","In this paper, we present a model that, initialized from LayoutLMv3, can match or outperform the current state-of-the-art results in RE applied to Visually-Rich Documents (VRD) on FUNSD and CORD datasets, without any specific pre-training and with fewer parameters.","We also report an extensive ablation study performed on FUNSD, highlighting the great impact of certain features and modelization choices on the performances."],"url":"http://arxiv.org/abs/2404.10848v1","category":"cs.CL"}
{"created":"2024-04-16 18:43:27","title":"Geometric Neural Operators (GNPs) for Data-Driven Deep Learning of Non-Euclidean Operators","abstract":"We introduce Geometric Neural Operators (GNPs) for accounting for geometric contributions in data-driven deep learning of operators. We show how GNPs can be used (i) to estimate geometric properties, such as the metric and curvatures, (ii) to approximate Partial Differential Equations (PDEs) on manifolds, (iii) learn solution maps for Laplace-Beltrami (LB) operators, and (iv) to solve Bayesian inverse problems for identifying manifold shapes. The methods allow for handling geometries of general shape including point-cloud representations. The developed GNPs provide approaches for incorporating the roles of geometry in data-driven learning of operators.","sentences":["We introduce Geometric Neural Operators (GNPs) for accounting for geometric contributions in data-driven deep learning of operators.","We show how GNPs can be used (i) to estimate geometric properties, such as the metric and curvatures, (ii) to approximate Partial Differential Equations (PDEs) on manifolds, (iii) learn solution maps for Laplace-Beltrami (LB) operators, and (iv) to solve Bayesian inverse problems for identifying manifold shapes.","The methods allow for handling geometries of general shape including point-cloud representations.","The developed GNPs provide approaches for incorporating the roles of geometry in data-driven learning of operators."],"url":"http://arxiv.org/abs/2404.10843v1","category":"cs.LG"}
{"created":"2024-04-16 18:22:30","title":"Sustainability of Data Center Digital Twins with Reinforcement Learning","abstract":"The rapid growth of machine learning (ML) has led to an increased demand for computational power, resulting in larger data centers (DCs) and higher energy consumption. To address this issue and reduce carbon emissions, intelligent design and control of DC components such as IT servers, cabinets, HVAC cooling, flexible load shifting, and battery energy storage are essential. However, the complexity of designing and controlling them in tandem presents a significant challenge. While some individual components like CFD-based design and Reinforcement Learning (RL) based HVAC control have been researched, there's a gap in the holistic design and optimization covering all elements simultaneously. To tackle this, we've developed DCRL-Green, a multi-agent RL environment that empowers the ML community to design data centers and research, develop, and refine RL controllers for carbon footprint reduction in DCs. It is a flexible, modular, scalable, and configurable platform that can handle large High Performance Computing (HPC) clusters. Furthermore, in its default setup, DCRL-Green provides a benchmark for evaluating single as well as multi-agent RL algorithms. It easily allows users to subclass the default implementations and design their own control approaches, encouraging community development for sustainable data centers. Open Source Link: https://github.com/HewlettPackard/dc-rl","sentences":["The rapid growth of machine learning (ML) has led to an increased demand for computational power, resulting in larger data centers (DCs) and higher energy consumption.","To address this issue and reduce carbon emissions, intelligent design and control of DC components such as IT servers, cabinets, HVAC cooling, flexible load shifting, and battery energy storage are essential.","However, the complexity of designing and controlling them in tandem presents a significant challenge.","While some individual components like CFD-based design and Reinforcement Learning (RL) based HVAC control have been researched, there's a gap in the holistic design and optimization covering all elements simultaneously.","To tackle this, we've developed DCRL-Green, a multi-agent RL environment that empowers the ML community to design data centers and research, develop, and refine RL controllers for carbon footprint reduction in DCs.","It is a flexible, modular, scalable, and configurable platform that can handle large High Performance Computing (HPC) clusters.","Furthermore, in its default setup, DCRL-Green provides a benchmark for evaluating single as well as multi-agent RL algorithms.","It easily allows users to subclass the default implementations and design their own control approaches, encouraging community development for sustainable data centers.","Open Source Link: https://github.com/HewlettPackard/dc-rl"],"url":"http://arxiv.org/abs/2404.10786v1","category":"cs.DC"}
{"created":"2024-04-16 18:08:29","title":"Fewer Truncations Improve Language Modeling","abstract":"In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.","sentences":["In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens.","Despite its efficiency, the concatenation approach compromises data integrity -- it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context.","To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization.","Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation.","Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., relatively +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%."],"url":"http://arxiv.org/abs/2404.10830v1","category":"cs.CL"}
{"created":"2024-04-16 18:02:15","title":"Decoupled Weight Decay for Any $p$ Norm","abstract":"With the success of deep neural networks (NNs) in a variety of domains, the computational and storage requirements for training and deploying large NNs have become a bottleneck for further improvements. Sparsification has consequently emerged as a leading approach to tackle these issues. In this work, we consider a simple yet effective approach to sparsification, based on the Bridge, or $L_p$ regularization during training. We introduce a novel weight decay scheme, which generalizes the standard $L_2$ weight decay to any $p$ norm. We show that this scheme is compatible with adaptive optimizers, and avoids the gradient divergence associated with $0<p<1$ norms. We empirically demonstrate that it leads to highly sparse networks, while maintaining generalization performance comparable to standard $L_2$ regularization.","sentences":["With the success of deep neural networks (NNs) in a variety of domains, the computational and storage requirements for training and deploying large NNs have become a bottleneck for further improvements.","Sparsification has consequently emerged as a leading approach to tackle these issues.","In this work, we consider a simple yet effective approach to sparsification, based on the Bridge, or $L_p$ regularization during training.","We introduce a novel weight decay scheme, which generalizes the standard $L_2$ weight decay to any $p$ norm.","We show that this scheme is compatible with adaptive optimizers, and avoids the gradient divergence associated with $0<p<1$ norms.","We empirically demonstrate that it leads to highly sparse networks, while maintaining generalization performance comparable to standard $L_2$ regularization."],"url":"http://arxiv.org/abs/2404.10824v1","category":"cs.LG"}
