{"created":"2024-04-16 17:59:55","title":"Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback","abstract":"Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandit (\\algo), which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an $\\tilde O(d\\sqrt{T}+dC)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, and $ 0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Additionally, we conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.","sentences":["Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM).","However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction.","To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary.","We propose an algorithm namely robust contextual dueling bandit (\\algo), which is based on uncertainty-weighted maximum likelihood estimation.","Our algorithm achieves an $\\tilde O(d\\sqrt{T}+dC)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, and $ 0 \\le C \\le T$ is the total number of adversarial feedback.","We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback.","Additionally, we conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback.","Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback."],"url":"http://arxiv.org/abs/2404.10776v1","category":"cs.LG"}
{"created":"2024-04-16 17:59:11","title":"COMBO: Compositional World Models for Embodied Multi-Agent Cooperation","abstract":"In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only partial egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. To evaluate the efficacy of our methods, we create two challenging embodied multi-agent long-horizon cooperation tasks using the ThreeDWorld simulator and conduct experiments with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed framework. More videos can be found at https://vis-www.cs.umass.edu/combo/.","sentences":["In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only partial egocentric views of the world.","To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents' actions given only partial egocentric visual observations of the world.","To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations.","To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video.","By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning.","To evaluate the efficacy of our methods, we create two challenging embodied multi-agent long-horizon cooperation tasks using the ThreeDWorld simulator and conduct experiments with 2-4 agents.","The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed framework.","More videos can be found at https://vis-www.cs.umass.edu/combo/."],"url":"http://arxiv.org/abs/2404.10775v1","category":"cs.CV"}
{"created":"2024-04-16 17:59:10","title":"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents","abstract":"Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.","sentences":["Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more.","Current approaches to this kind of \"fact-checking\" are based on verifying each piece of a model generation against potential evidence using an LLM.","However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response.","In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost.","We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure.","Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences.","For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations.","Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy.","We release LLM-AggreFact, code for data synthesis, and models."],"url":"http://arxiv.org/abs/2404.10774v1","category":"cs.CL"}
{"created":"2024-04-16 17:57:57","title":"Searching for cold gas traced by MgII quasar absorbers in massive X-ray-selected galaxy clusters","abstract":"Almost 50% of galaxies in the local Universe are in clusters or groups coexisting with both hot and cold gas components. In the present study, we observationally probed the cold-gas content of X-ray-selected massive galaxy clusters with spectroscopic redshift measured from the SDSS/SPIDERS survey. This paper focuses on the most massive structures: galaxy clusters with a mean mass of M$_{500c}$ = 2.7$\\times 10^{14}$ M$_{\\odot}$. We used a large number of background quasar optical spectra from SDSS DR16 to probe the diffuse T$=$10$^4$K gas in their intracluster medium. We first analysed a sample of spectra with known MgII absorbers, and then blindly stacked about 16,000 archival spectra at the redshifts of the foreground galaxy clusters. We tentatively ($3.7 \\sigma$ significance) detect MgII in the clusters with an equivalent width EW(MgII $\\lambda$2796) of 0.056$\\pm$0.015 \\r{A}, corresponding to a column density of log [N(MgII)/cm$^{-2}$]=12.12$\\pm0.1$. We tested our methodology by generating 22,000 mock SDSS spectra with MgII absorbers from Illustris-TNG50 cosmological magnetohydrodynamical simulations, combining photo-ionisation modelling and ray tracing. We also performed bootstrapping stacking at different cluster redshifts and stacked quasar spectra with no intervening clusters in the line of sight to measure the significance of our detection. These results are in line with the findings of recent, similar observational studies but challenge predictions from Illustris-TNG simulations. Together, our findings indicate that large amounts of cold gas may be found in the most massive structures of the Universe.","sentences":["Almost 50% of galaxies in the local Universe are in clusters or groups coexisting with both hot and cold gas components.","In the present study, we observationally probed the cold-gas content of X-ray-selected massive galaxy clusters with spectroscopic redshift measured from the SDSS/SPIDERS survey.","This paper focuses on the most massive structures: galaxy clusters with a mean mass of M$_{500c}$ = 2.7$\\times 10^{14}$ M$_{\\odot}$. We used a large number of background quasar optical spectra from SDSS DR16 to probe the diffuse T$=$10$^4$K gas in their intracluster medium.","We first analysed a sample of spectra with known MgII absorbers, and then blindly stacked about 16,000 archival spectra at the redshifts of the foreground galaxy clusters.","We tentatively ($3.7 \\sigma$ significance) detect MgII in the clusters with an equivalent width EW(MgII $\\lambda$2796) of 0.056$\\pm$0.015 \\r{A}, corresponding to a column density of log [N(MgII)/cm$^{-2}$]=12.12$\\pm0.1$. We tested our methodology by generating 22,000 mock SDSS spectra with MgII absorbers from Illustris-TNG50 cosmological magnetohydrodynamical simulations, combining photo-ionisation modelling and ray tracing.","We also performed bootstrapping stacking at different cluster redshifts and stacked quasar spectra with no intervening clusters in the line of sight to measure the significance of our detection.","These results are in line with the findings of recent, similar observational studies but challenge predictions from Illustris-TNG simulations.","Together, our findings indicate that large amounts of cold gas may be found in the most massive structures of the Universe."],"url":"http://arxiv.org/abs/2404.10773v1","category":"astro-ph.GA"}
{"created":"2024-04-16 17:55:31","title":"TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net","abstract":"Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.","sentences":["Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering.","The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems.","In this paper, we introduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions.","Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency.","TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving machine precision in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation."],"url":"http://arxiv.org/abs/2404.10771v1","category":"cs.LG"}
{"created":"2024-04-16 17:52:16","title":"Generalized Linear Response Theory for Pumped Systems and its Application to Transient Optical Properties within DPOA","abstract":"We derive the two-time linear response theory for out-of-equilibrium pumped systems, generic pump-probe delays and probe frequencies. Such a theory enormously simplifies the numerical calculations, for instance, of the optical conductivity with respect to the actual procedure, which requires computing the effect of the probe pulse for each time delay with respect to the pump pulse\\textcolor{red}{.} The theory is given for a generic observable and pumped Hamiltonian and then specialized for a system with a quadratic Hamiltonian and its transient optical properties, exploiting the Dynamical Projective Operatorial Approach (DPOA). The theory is complemented by a set of crucial numerical guidelines that help perform actual calculations in a computationally affordable way. The optical response (differential transient reflectivity and absorption) of a prototypical three-band (core, valence, and conduction) model in the XUV regime is analyzed in detail to illustrate the theory and its application. Using some generalizations of the density of states, we provide a systematic approach to exploring the optical properties in terms of the system band structure features and the pump parameters. Such an analysis can be extremely helpful in understanding the actual results of experimental optical measurements. Moreover, we study the effects of inter-band and intra-band transitions, the local dipole coupling, and single and multi-photon processes. The latter is further investigated by varying the central frequency of the pump pulse to have different regions of the first Brillouin zone in resonance with it. We also study the effect of varying the pump pulse intensity. Finally, we study and analyze the transient optical properties in the probe pulse regime of IR and visible.","sentences":["We derive the two-time linear response theory for out-of-equilibrium pumped systems, generic pump-probe delays and probe frequencies.","Such a theory enormously simplifies the numerical calculations, for instance, of the optical conductivity with respect to the actual procedure, which requires computing the effect of the probe pulse for each time delay with respect to the pump pulse\\textcolor{red}{.}","The theory is given for a generic observable and pumped Hamiltonian and then specialized for a system with a quadratic Hamiltonian and its transient optical properties, exploiting the Dynamical Projective Operatorial Approach (DPOA).","The theory is complemented by a set of crucial numerical guidelines that help perform actual calculations in a computationally affordable way.","The optical response (differential transient reflectivity and absorption) of a prototypical three-band (core, valence, and conduction) model in the XUV regime is analyzed in detail to illustrate the theory and its application.","Using some generalizations of the density of states, we provide a systematic approach to exploring the optical properties in terms of the system band structure features and the pump parameters.","Such an analysis can be extremely helpful in understanding the actual results of experimental optical measurements.","Moreover, we study the effects of inter-band and intra-band transitions, the local dipole coupling, and single and multi-photon processes.","The latter is further investigated by varying the central frequency of the pump pulse to have different regions of the first Brillouin zone in resonance with it.","We also study the effect of varying the pump pulse intensity.","Finally, we study and analyze the transient optical properties in the probe pulse regime of IR and visible."],"url":"http://arxiv.org/abs/2404.10768v1","category":"physics.optics"}
{"created":"2024-04-16 17:50:02","title":"RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting","abstract":"Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction.","sentences":["Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge.","In this work, we propose an approach for 3D scene inpainting -- the task of coherently replacing parts of the reconstructed scene with desired content.","Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content.","A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control.","Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal.","Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view.","The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details.","Our framework achieves state-of-the-art results for object removal while maintaining high controllability.","We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction."],"url":"http://arxiv.org/abs/2404.10765v1","category":"cs.CV"}
{"created":"2024-04-16 17:47:16","title":"LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?","abstract":"Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation.","sentences":["Diffusion models have exhibited remarkable capabilities in text-to-image generation.","However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks.","In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding.","With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints.","Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data.","In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths.","Our framework also includes a diffuser for semantic image-to-text conversion and a Back&Refine technique to enhance token interactivity during inference.","LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules.","This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation."],"url":"http://arxiv.org/abs/2404.10763v1","category":"cs.AI"}
{"created":"2024-04-16 17:38:26","title":"Learning Feature Inversion for Multi-class Anomaly Detection under General-purpose COCO-AD Benchmark","abstract":"Anomaly detection (AD) is often focused on detecting anomaly areas for industrial quality inspection and medical lesion examination. However, due to the specific scenario targets, the data scale for AD is relatively small, and evaluation metrics are still deficient compared to classic vision tasks, such as object detection and semantic segmentation. To fill these gaps, this work first constructs a large-scale and general-purpose COCO-AD dataset by extending COCO to the AD field. This enables fair evaluation and sustainable development for different methods on this challenging benchmark. Moreover, current metrics such as AU-ROC have nearly reached saturation on simple datasets, which prevents a comprehensive evaluation of different methods. Inspired by the metrics in the segmentation field, we further propose several more practical threshold-dependent AD-specific metrics, ie, m$F_1$$^{.2}_{.8}$, mAcc$^{.2}_{.8}$, mIoU$^{.2}_{.8}$, and mIoU-max. Motivated by GAN inversion's high-quality reconstruction capability, we propose a simple but more powerful InvAD framework to achieve high-quality feature reconstruction. Our method improves the effectiveness of reconstruction-based methods on popular MVTec AD, VisA, and our newly proposed COCO-AD datasets under a multi-class unsupervised setting, where only a single detection model is trained to detect anomalies from different classes. Extensive ablation experiments have demonstrated the effectiveness of each component of our InvAD. Full codes and models are available at https://github.com/zhangzjn/ader.","sentences":["Anomaly detection (AD) is often focused on detecting anomaly areas for industrial quality inspection and medical lesion examination.","However, due to the specific scenario targets, the data scale for AD is relatively small, and evaluation metrics are still deficient compared to classic vision tasks, such as object detection and semantic segmentation.","To fill these gaps, this work first constructs a large-scale and general-purpose COCO-AD dataset by extending COCO to the AD field.","This enables fair evaluation and sustainable development for different methods on this challenging benchmark.","Moreover, current metrics such as AU-ROC have nearly reached saturation on simple datasets, which prevents a comprehensive evaluation of different methods.","Inspired by the metrics in the segmentation field, we further propose several more practical threshold-dependent AD-specific metrics, ie, m$F_1$$^{.2}_{.8}$, mAcc$^{.2}_{.8}$, mIoU$^{.2}_{.8}$, and mIoU-max.","Motivated by GAN inversion's high-quality reconstruction capability, we propose a simple but more powerful InvAD framework to achieve high-quality feature reconstruction.","Our method improves the effectiveness of reconstruction-based methods on popular MVTec AD, VisA, and our newly proposed COCO-AD datasets under a multi-class unsupervised setting, where only a single detection model is trained to detect anomalies from different classes.","Extensive ablation experiments have demonstrated the effectiveness of each component of our InvAD.","Full codes and models are available at https://github.com/zhangzjn/ader."],"url":"http://arxiv.org/abs/2404.10760v1","category":"cs.CV"}
{"created":"2024-04-16 17:34:26","title":"A Young Super Star Cluster Powering a Nebula of Retained Massive Star Ejecta","abstract":"We suggest that \"Godzilla\", an intriguing source in the lensed Sunburst galaxy at $z=2.37$, is a young super star cluster powering a compact nebula within gravitationally trapped stellar ejecta. Employing HST photometry and spectroscopy from MUSE and X-Shooter at VLT, we infer physical and chemical properties of the cluster and nebula, finding Godzilla is young (4-6 Myr), massive ($\\sim 10^{6-7}M_\\odot$), a stellar metallicity $Z \\simeq 0.25Z_\\odot$, and has the FUV component more compact than a few pc. The nebula gas is significantly enriched with N and He, indicating stellar wind material, and has highly elevated O relative to the sub-solar stellar metallicity, which indicates entrainment of CCSNe ejecta. The high gas density $n_{\\rm e} \\simeq 10^{7-8}{\\rm cm}^{-3}$ implies a highly pressurized intracluster environment. We propose the high pressure is due to CCSN-driven supersonic turbulence in warm, self-shielding gas, which has accumulated in the cluster center after runaway radiative cooling and is dense enough to resist removal by CCSNe. The nebula gas shows sub-solar C/O, Ne/O and Si/O values, which may reflect the CCSN element yields for initial stellar masses $>40M_\\odot$. A comparison to element yield synthesis models for young star clusters shows that the gas abundance pattern is consistent with complete retention and mixture of stellar winds and CCSNe ejecta until the inferred cluster age. The O and He enhancement we find may have implications for the formation of multiple stellar populations in globular clusters, as Godzilla likely has already formed second-generation stars prior to the onset of CCSNe and evolved star winds, in order not to contradict the non-observation of O and large He enhancement in second-generation stars.","sentences":["We suggest that \"Godzilla\", an intriguing source in the lensed Sunburst galaxy at $z=2.37$, is a young super star cluster powering a compact nebula within gravitationally trapped stellar ejecta.","Employing HST photometry and spectroscopy from MUSE and X-Shooter at VLT, we infer physical and chemical properties of the cluster and nebula, finding Godzilla is young (4-6 Myr), massive ($\\sim 10^{6-7}M_\\odot$), a stellar metallicity $Z \\simeq 0.25Z_\\odot$, and has the FUV component more compact than a few pc.","The nebula gas is significantly enriched with N and He, indicating stellar wind material, and has highly elevated O relative to the sub-solar stellar metallicity, which indicates entrainment of CCSNe ejecta.","The high gas density $n_{\\rm e} \\simeq 10^{7-8}{\\rm cm}^{-3}$ implies a highly pressurized intracluster environment.","We propose the high pressure is due to CCSN-driven supersonic turbulence in warm, self-shielding gas, which has accumulated in the cluster center after runaway radiative cooling and is dense enough to resist removal by CCSNe.","The nebula gas shows sub-solar C/O, Ne/O and Si/O values, which may reflect the CCSN element yields for initial stellar masses $>40M_\\odot$. A comparison to element yield synthesis models for young star clusters shows that the gas abundance pattern is consistent with complete retention and mixture of stellar winds and CCSNe ejecta until the inferred cluster age.","The O and He enhancement we find may have implications for the formation of multiple stellar populations in globular clusters, as Godzilla likely has already formed second-generation stars prior to the onset of CCSNe and evolved star winds, in order not to contradict the non-observation of O and large He enhancement in second-generation stars."],"url":"http://arxiv.org/abs/2404.10755v1","category":"astro-ph.GA"}
{"created":"2024-04-16 17:33:26","title":"Computing Inductive Invariants of Regular Abstraction Frameworks","abstract":"Regular transition systems (RTS) are a popular formalism for modeling infinite-state systems in general, and parameterised systems in particular. In a CONCUR 22 paper, Esparza et al. introduce a novel approach to the verification of RTS, based on inductive invariants. The approach computes the intersection of all inductive invariants of a given RTS that can be expressed as CNF formulas with a bounded number of clauses, and uses it to construct an automaton recognising an overapproximation of the reachable configurations. The paper shows that the problem of deciding if the language of this automaton intersects a given regular set of unsafe configurations is in $\\textsf{EXPSPACE}$ and $\\textsf{PSPACE}$-hard.   We introduce $\\textit{regular abstraction frameworks}$, a generalisation of the approach of Esparza et al., very similar to the regular abstractions of Hong and Lin. A framework consists of a regular language of $\\textit{constraints}$, and a transducer, called the $\\textit{interpretation}$, that assigns to each constraint the set of configurations of the RTS satisfying it. Examples of regular abstraction frameworks include the formulas of Esparza et al., octagons, bounded difference matrices, and views. We show that the generalisation of the decision problem above to regular abstraction frameworks remains in $\\textsf{EXPSPACE}$, and prove a matching (highly non-trivial) $\\textsf{EXPSPACE}$-hardness bound.   $\\textsf{EXPSPACE}$-hardness implies that, in the worst case, the automaton recognising the overapproximation of the reachable configurations has a double-exponential number of states. We introduce a learning algorithm that computes this automaton in a lazy manner, stopping whenever the current hypothesis is already strong enough to prove safety. We report on an implementation and show that our experimental results improve on those of Esparza et al.","sentences":["Regular transition systems (RTS) are a popular formalism for modeling infinite-state systems in general, and parameterised systems in particular.","In a CONCUR 22 paper, Esparza et al. introduce a novel approach to the verification of RTS, based on inductive invariants.","The approach computes the intersection of all inductive invariants of a given RTS that can be expressed as CNF formulas with a bounded number of clauses, and uses it to construct an automaton recognising an overapproximation of the reachable configurations.","The paper shows that the problem of deciding if the language of this automaton intersects a given regular set of unsafe configurations is in $\\textsf{EXPSPACE}$ and $\\textsf{PSPACE}$-hard.   ","We introduce $\\textit{regular abstraction frameworks}$, a generalisation of the approach of Esparza et al., very similar to the regular abstractions of Hong and Lin.","A framework consists of a regular language of $\\textit{constraints}$, and a transducer, called the $\\textit{interpretation}$, that assigns to each constraint the set of configurations of the RTS satisfying it.","Examples of regular abstraction frameworks include the formulas of Esparza et al., octagons, bounded difference matrices, and views.","We show that the generalisation of the decision problem above to regular abstraction frameworks remains in $\\textsf{EXPSPACE}$, and prove a matching (highly non-trivial) $\\textsf{EXPSPACE}$-hardness bound.   ","$\\textsf{EXPSPACE}$-hardness implies that, in the worst case, the automaton recognising the overapproximation of the reachable configurations has a double-exponential number of states.","We introduce a learning algorithm that computes this automaton in a lazy manner, stopping whenever the current hypothesis is already strong enough to prove safety.","We report on an implementation and show that our experimental results improve on those of Esparza et al."],"url":"http://arxiv.org/abs/2404.10752v1","category":"cs.FL"}
{"created":"2024-04-16 17:27:46","title":"Dimensions of infinitely generated self-affine sets and restricted digit sets for signed L\u00fcroth expansions","abstract":"For countably infinite IFSs on $\\mathbb R^2$ consisting of affine contractions with diagonal linear parts, we give conditions under which the affinity dimension is an upper bound for the Hausdorff dimension and a lower bound for the lower box-counting dimension. Moreover, we identify a family of countably infinite IFSs for which the Hausdorff and affinity dimension are equal, and which have full dimension spectrum. The corresponding self-affine sets are related to restricted digit sets for signed L\\\"uroth expansions.","sentences":["For countably infinite IFSs on $\\mathbb R^2$ consisting of affine contractions with diagonal linear parts, we give conditions under which the affinity dimension is an upper bound for the Hausdorff dimension and a lower bound for the lower box-counting dimension.","Moreover, we identify a family of countably infinite IFSs for which the Hausdorff and affinity dimension are equal, and which have full dimension spectrum.","The corresponding self-affine sets are related to restricted digit sets for signed L\\\"uroth expansions."],"url":"http://arxiv.org/abs/2404.10749v1","category":"math.DS"}
{"created":"2024-04-16 17:27:03","title":"Classical and Quantum Distributed Algorithms for the Survivable Network Design Problem","abstract":"We investigate distributed classical and quantum approaches for the survivable network design problem (SNDP), sometimes called the generalized Steiner problem. These problems generalize many complex graph problems of interest, such as the traveling salesperson problem, the Steiner tree problem, and the k-connected network problem. To our knowledge, no classical or quantum algorithms for the SNDP have been formulated in the distributed settings we consider. We describe algorithms that are heuristics for the general problem but give concrete approximation bounds under specific parameterizations of the SNDP, which in particular hold for the three aforementioned problems that SNDP generalizes. We use a classical, centralized algorithmic framework first studied in (Goemans & Bertsimas 1993) and provide a distributed implementation thereof. Notably, we obtain asymptotic quantum speedups by leveraging quantum shortest path computations in this framework, generalizing recent work of (Kerger et al. 2023). These results raise the question of whether there is a separation between the classical and quantum models for application-scale instances of the problems considered.","sentences":["We investigate distributed classical and quantum approaches for the survivable network design problem (SNDP), sometimes called the generalized Steiner problem.","These problems generalize many complex graph problems of interest, such as the traveling salesperson problem, the Steiner tree problem, and the k-connected network problem.","To our knowledge, no classical or quantum algorithms for the SNDP have been formulated in the distributed settings we consider.","We describe algorithms that are heuristics for the general problem but give concrete approximation bounds under specific parameterizations of the SNDP, which in particular hold for the three aforementioned problems that SNDP generalizes.","We use a classical, centralized algorithmic framework first studied in (Goemans & Bertsimas 1993) and provide a distributed implementation thereof.","Notably, we obtain asymptotic quantum speedups by leveraging quantum shortest path computations in this framework, generalizing recent work of (Kerger et al. 2023).","These results raise the question of whether there is a separation between the classical and quantum models for application-scale instances of the problems considered."],"url":"http://arxiv.org/abs/2404.10748v1","category":"quant-ph"}
{"created":"2024-04-16 17:18:07","title":"Superradiant instability of area quantized Kerr black hole with discrete reflectivity","abstract":"Ultralight bosons can condense to form the so-called bosonic clouds around spinning black holes by superradiance instability. When quantum effects are taken into account, the classical black holes were replaced by exotic compact objects including area quantized black holes. In this work, we consider the superradiant instabilities of massive scalar fields around area quantized Kerr black hole. We introduce the reflectivity of area quantized black hole possesses the distinct discrete feature, and the scalar fields have the superradiant modes solution only within the specific mass range. In addition, the area quantization may terminate the superradiance when the black hole spins down, or even suppress the formation of the bosionic cloud.","sentences":["Ultralight bosons can condense to form the so-called bosonic clouds around spinning black holes by superradiance instability.","When quantum effects are taken into account, the classical black holes were replaced by exotic compact objects including area quantized black holes.","In this work, we consider the superradiant instabilities of massive scalar fields around area quantized Kerr black hole.","We introduce the reflectivity of area quantized black hole possesses the distinct discrete feature, and the scalar fields have the superradiant modes solution only within the specific mass range.","In addition, the area quantization may terminate the superradiance when the black hole spins down, or even suppress the formation of the bosionic cloud."],"url":"http://arxiv.org/abs/2404.10742v1","category":"gr-qc"}
{"created":"2024-04-16 17:15:37","title":"Maser Flares Driven by Isothermal Shock Waves","abstract":"We use 3D computer modelling to investigate the timescales and radiative output from maser flares generated by the impact of shock-waves on astronomical unit-scale clouds in interstellar and star-forming regions, and in circumstellar regions in some circumstances. Physical conditions are derived from simple models of isothermal hydrodynamic (single-fluid) and C-type (ionic and neutral fluid) shock-waves, and based on the ortho-H$_2$O 22-GHz transition. Maser saturation is comprehensively included, and we find that the most saturated maser inversions are found predominantly in the shocked material. We study the effect on the intensity, flux density and duration of flares of the following parameters: the pre-shock level of saturation, the observer's viewpoint, and the shock speed. Our models are able to reproduce observed flare rise times of a few times 10 days, specific intensities of up to 10$^5$ times the saturation intensity and flux densities of order $100(R/d)^2$Jy from a source of radius $R$ astronomical units at a distance of $d$ kiloparsec. We found that flares from C-type shocks are approximately 5 times more likely to be seen by a randomly placed observer than flares from hydrodynamically shocked clouds of similar dimensions. We computed intrinsic beaming patterns of the maser emission, finding substantial extension of the pattern parallel to the shock front in the hydrodynamic models. Beaming solid angles for hydrodynamic models can be as small as $1.3\\times 10^{-5}$sr, but are an order of magnitude larger for C-type models.","sentences":["We use 3D computer modelling to investigate the timescales and radiative output from maser flares generated by the impact of shock-waves on astronomical unit-scale clouds in interstellar and star-forming regions, and in circumstellar regions in some circumstances.","Physical conditions are derived from simple models of isothermal hydrodynamic (single-fluid) and C-type (ionic and neutral fluid) shock-waves, and based on the ortho-H$_2$O 22-GHz transition.","Maser saturation is comprehensively included, and we find that the most saturated maser inversions are found predominantly in the shocked material.","We study the effect on the intensity, flux density and duration of flares of the following parameters: the pre-shock level of saturation, the observer's viewpoint, and the shock speed.","Our models are able to reproduce observed flare rise times of a few times 10 days, specific intensities of up to 10$^5$ times the saturation intensity and flux densities of order $100(R/d)^2$Jy from a source of radius $R$ astronomical units at a distance of $d$ kiloparsec.","We found that flares from C-type shocks are approximately 5 times more likely to be seen by a randomly placed observer than flares from hydrodynamically shocked clouds of similar dimensions.","We computed intrinsic beaming patterns of the maser emission, finding substantial extension of the pattern parallel to the shock front in the hydrodynamic models.","Beaming solid angles for hydrodynamic models can be as small as $1.3\\times 10^{-5}$sr, but are an order of magnitude larger for C-type models."],"url":"http://arxiv.org/abs/2404.10741v1","category":"astro-ph.GA"}
{"created":"2024-04-16 17:13:08","title":"N-Agent Ad Hoc Teamwork","abstract":"Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time. This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.","sentences":["Current approaches to learning cooperative behaviors in multi-agent settings assume relatively restrictive settings.","In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls \\textit{all} agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\\textit{single}$ agent in the scenario.","However, many cooperative settings in the real world are much less restrictive.","For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company.","Towards generalizing the class of scenarios that cooperative learning methods can address, we introduce $N$-agent ad hoc teamwork, in which a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates at evaluation time.","This paper formalizes the problem, and proposes the $\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm.","POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors.","Empirical evaluation on StarCraft II tasks shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates."],"url":"http://arxiv.org/abs/2404.10740v1","category":"cs.AI"}
{"created":"2024-04-16 17:11:44","title":"Quantum Teleportation Coexisting with Conventional Classical Communications in Optical Fiber","abstract":"The ability for quantum and classical networks to operate in the same optical fibers would aid the deployment of quantum network technology. However, quantum performance can be susceptible to noise photons generated by spontaneous Raman scattering of high-power coexisting classical light. Quantum teleportation is a fundamental operation in quantum networking, but has yet to be demonstrated in fibers populated with high data rate conventional optical signals. In this paper, we demonstrate a three-node quantum state teleportation system coexisting with 400-Gbps C-band classical communications in 30.2 km of fiber. To protect quantum fidelity, Raman noise rates are suppressed using optimized O-band quantum channels and filtering in multiple degrees of freedom. Fidelity is shown to be well maintained with elevated classical powers as high as 18.7 dBm, which could support multiple classical channels with many terabits/s aggregate data rates. These results show the feasibility of advanced quantum and classical network applications operating within a unified fiber infrastructure.","sentences":["The ability for quantum and classical networks to operate in the same optical fibers would aid the deployment of quantum network technology.","However, quantum performance can be susceptible to noise photons generated by spontaneous Raman scattering of high-power coexisting classical light.","Quantum teleportation is a fundamental operation in quantum networking, but has yet to be demonstrated in fibers populated with high data rate conventional optical signals.","In this paper, we demonstrate a three-node quantum state teleportation system coexisting with 400-Gbps C-band classical communications in 30.2 km of fiber.","To protect quantum fidelity, Raman noise rates are suppressed using optimized O-band quantum channels and filtering in multiple degrees of freedom.","Fidelity is shown to be well maintained with elevated classical powers as high as 18.7 dBm, which could support multiple classical channels with many terabits/s aggregate data rates.","These results show the feasibility of advanced quantum and classical network applications operating within a unified fiber infrastructure."],"url":"http://arxiv.org/abs/2404.10738v1","category":"quant-ph"}
{"created":"2024-04-16 17:10:52","title":"Integer-valued o-minimal functions","abstract":"We study $\\mathbb{R}_{\\textrm{an},\\exp}$-definable functions $f:\\mathbb{R}\\to \\mathbb{R}$ that take integer values at all sufficiently large positive integers. If $|f(x)|= O\\big(2^{(1+10^{-5})x}\\big)$, then we find polynomials $P_1, P_2$ such that $f(x)=P_1(x)+P_2(x)2^x$ for all sufficiently large $x$. Our result parallels classical theorems of P\\'olya and Selberg for entire functions and generalizes Wilkie's classification for the case of $|f(x)|= O(C^x)$, for some $C<2$.   Let $k\\in \\mathbb{N}$ and $\\gamma_k=\\sum_{j=1}^{k} 1/j$. Extending Wilkie's theorem in a separate direction, we show that if $f$ is $k$-$\\textit{concordant}$ and $|f(x)|= O(C^{x})$, for some $C<e^{\\gamma_k}+1$, then $f$ must eventually be given by a polynomial. This is an analog of a result by Pila for entire functions.","sentences":["We study $\\mathbb{R}_{\\textrm{an},\\exp}$-definable functions $f:\\mathbb{R}\\to \\mathbb{R}$ that take integer values at all sufficiently large positive integers.","If $|f(x)|= O\\big(2^{(1+10^{-5})x}\\big)$, then we find polynomials $P_1, P_2$ such that $f(x)=P_1(x)+P_2(x)2^x$ for all sufficiently large $x$.","Our result parallels classical theorems of P\\'olya and Selberg for entire functions and generalizes Wilkie's classification for the case of $|f(x)|= O(C^x)$, for some $C<2$.   Let $k\\in \\mathbb{N}$ and $\\gamma_k=\\sum_{j=1}^{k} 1/j$. Extending Wilkie's theorem in a separate direction, we show that if $f$ is $k$-$\\textit{concordant}$ and $|f(x)|= O(C^{x})$, for some $C<e^{\\gamma_k}+1$, then $f$ must eventually be given by a polynomial.","This is an analog of a result by Pila for entire functions."],"url":"http://arxiv.org/abs/2404.10737v1","category":"math.LO"}
{"created":"2024-04-16 17:09:46","title":"On forcing axioms and weakenings of the Axiom of Choice","abstract":"We prove forcing axiom equivalents of two families of weakenings of the axiom of choice: a trichotomy principle for cardinals isolated by L\\'evy, ${\\rm H\\hskip0.05pt}_\\kappa$, and ${\\rm DC}_\\kappa$, the principle of dependent choices generalized to cardinals $\\kappa$, for regular cardinals $\\kappa$. Using these equivalents we obtain new forcing axiom formulations of the axiom of choice.   A point of interest is that we use a new template for forcing axioms. For the class of forcings to which we asks that the axioms apply, we do not ask that they apply to all collections of dense sets of a certain cardinality, but rather only for each particular forcing to a specific family of dense sets of the cardinality in question.","sentences":["We prove forcing axiom equivalents of two families of weakenings of the axiom of choice: a trichotomy principle for cardinals isolated by L\\'evy, ${\\rm H\\hskip0.05pt}_\\kappa$, and ${\\rm DC}_\\kappa$, the principle of dependent choices generalized to cardinals $\\kappa$, for regular cardinals $\\kappa$. Using these equivalents we obtain new forcing axiom formulations of the axiom of choice.   ","A point of interest is that we use a new template for forcing axioms.","For the class of forcings to which we asks that the axioms apply, we do not ask that they apply to all collections of dense sets of a certain cardinality, but rather only for each particular forcing to a specific family of dense sets of the cardinality in question."],"url":"http://arxiv.org/abs/2404.10736v1","category":"math.LO"}
{"created":"2024-04-16 17:05:43","title":"Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration","abstract":"Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions. Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets. Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior. In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment. However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration. We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models. For code, please see our project page https://sites.google.com/view/blr-hac.","sentences":["Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions.","Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets.","Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior.","In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment.","However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations.","We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration.","We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models.","For code, please see our project page https://sites.google.com/view/blr-hac."],"url":"http://arxiv.org/abs/2404.10733v1","category":"cs.AI"}
{"created":"2024-04-16 17:03:50","title":"What is Meant by AGI? On the Definition of Artificial General Intelligence","abstract":"This paper aims to establish a consensus on AGI's definition. General intelligence refers to the adaptation to open environments according to certain principles using limited resources. It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives.","sentences":["This paper aims to establish a consensus on AGI's definition.","General intelligence refers to the adaptation to open environments according to certain principles using limited resources.","It emphasizes that adaptation or learning is an indispensable property of intelligence, and places the controversial part within the principles of intelligence, which can be described from different perspectives."],"url":"http://arxiv.org/abs/2404.10731v1","category":"cs.AI"}
{"created":"2024-04-16 17:02:52","title":"Insight Gained from Migrating a Machine Learning Model to Intelligence Processing Units","abstract":"The discoveries in this paper show that Intelligence Processing Units (IPUs) offer a viable accelerator alternative to GPUs for machine learning (ML) applications within the fields of materials science and battery research. We investigate the process of migrating a model from GPU to IPU and explore several optimization techniques, including pipelining and gradient accumulation, aimed at enhancing the performance of IPU-based models. Furthermore, we have effectively migrated a specialized model to the IPU platform. This model is employed for predicting effective conductivity, a parameter crucial in ion transport processes, which govern the performance of multiple charge and discharge cycles of batteries. The model utilizes a Convolutional Neural Network (CNN) architecture to perform prediction tasks for effective conductivity. The performance of this model on the IPU is found to be comparable to its execution on GPUs. We also analyze the utilization and performance of Graphcore's Bow IPU. Through benchmark tests, we observe significantly improved performance with the Bow IPU when compared to its predecessor, the Colossus IPU.","sentences":["The discoveries in this paper show that Intelligence Processing Units (IPUs) offer a viable accelerator alternative to GPUs for machine learning (ML) applications within the fields of materials science and battery research.","We investigate the process of migrating a model from GPU to IPU and explore several optimization techniques, including pipelining and gradient accumulation, aimed at enhancing the performance of IPU-based models.","Furthermore, we have effectively migrated a specialized model to the IPU platform.","This model is employed for predicting effective conductivity, a parameter crucial in ion transport processes, which govern the performance of multiple charge and discharge cycles of batteries.","The model utilizes a Convolutional Neural Network (CNN) architecture to perform prediction tasks for effective conductivity.","The performance of this model on the IPU is found to be comparable to its execution on GPUs.","We also analyze the utilization and performance of Graphcore's Bow IPU.","Through benchmark tests, we observe significantly improved performance with the Bow IPU when compared to its predecessor, the Colossus IPU."],"url":"http://arxiv.org/abs/2404.10730v1","category":"cs.LG"}
{"created":"2024-04-16 17:01:27","title":"How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model","abstract":"Understanding what makes high-dimensional data learnable is a fundamental question in machine learning. On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts. On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance. In this work, we aim to explain this correlation and unify these two viewpoints. We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations. In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance. Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task.","sentences":["Understanding what makes high-dimensional data learnable is a fundamental question in machine learning.","On the one hand, it is believed that the success of deep learning lies in its ability to build a hierarchy of representations that become increasingly more abstract with depth, going from simple features like edges to more complex concepts.","On the other hand, learning to be insensitive to invariances of the task, such as smooth transformations for image datasets, has been argued to be important for deep networks and it strongly correlates with their performance.","In this work, we aim to explain this correlation and unify these two viewpoints.","We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations.","In particular, we introduce the Sparse Random Hierarchy Model (SRHM), where we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt, thereby explaining the strong correlation between the latter and performance.","Moreover, we quantify how the sample complexity of CNNs learning the SRHM depends on both the sparsity and hierarchical structure of the task."],"url":"http://arxiv.org/abs/2404.10727v1","category":"stat.ML"}
{"created":"2024-04-16 16:59:41","title":"Hilbert space delocalization under random unitary circuits","abstract":"Unitary dynamics of a quantum system initialized in a selected basis state yields, generically, a state that is a superposition of all the basis states. This process, associated with the quantum information scrambling and intimately tied to the resource theory of coherence, may be viewed as a gradual delocalization of the system's state in the Hilbert space. This work analyzes the Hilbert space delocalization under dynamics of random quantum circuits, which serve as a minimal model of chaotic dynamics of quantum many-body systems. We employ analytical methods based on the replica trick and Weingarten calculus to investigate the time evolution of the participation entropies which quantify the Hilbert space delocalization. We demonstrate that the participation entropies approach, up to a fixed accuracy, their long-time saturation value in times that scale logarithmically with the system size. Exact numerical simulations and tensor network techniques corroborate our findings.","sentences":["Unitary dynamics of a quantum system initialized in a selected basis state yields, generically, a state that is a superposition of all the basis states.","This process, associated with the quantum information scrambling and intimately tied to the resource theory of coherence, may be viewed as a gradual delocalization of the system's state in the Hilbert space.","This work analyzes the Hilbert space delocalization under dynamics of random quantum circuits, which serve as a minimal model of chaotic dynamics of quantum many-body systems.","We employ analytical methods based on the replica trick and Weingarten calculus to investigate the time evolution of the participation entropies which quantify the Hilbert space delocalization.","We demonstrate that the participation entropies approach, up to a fixed accuracy, their long-time saturation value in times that scale logarithmically with the system size.","Exact numerical simulations and tensor network techniques corroborate our findings."],"url":"http://arxiv.org/abs/2404.10725v1","category":"quant-ph"}
{"created":"2024-04-16 16:58:14","title":"The Topological Cartier--Raynaud Ring","abstract":"We prove that the $\\infty$-category of $p$-typical topological Cartier modules, recently introduced by Antieau--Nikolaus, over some base $A$ is equivalent to the $\\infty$-category of modules over a ring spectrum $\\mathcal R_A$, which we call the topological Cartier--Raynaud ring. Our main result is an identification of the homotopy groups of $\\mathcal R_A$. In particular, for $A=W(k)$, the Witt vectors over $k$, the homotopy groups $\\pi_*\\mathcal R_{W(k)}$ recover the classical Cartier--Raynaud ring constructed by Illusie--Raynaud. Moreover, along the way we will describe the compact generator of $p$-typical topological Cartier modules and classifies all natural operations on homotopy groups of $p$-typical topological Cartier modules.","sentences":["We prove that the $\\infty$-category of $p$-typical topological Cartier modules, recently introduced by Antieau--Nikolaus, over some base $A$ is equivalent to the $\\infty$-category of modules over a ring spectrum $\\mathcal R_A$, which we call the topological Cartier--Raynaud ring.","Our main result is an identification of the homotopy groups of $\\mathcal R_A$. In particular, for $A=W(k)$, the Witt vectors over $k$, the homotopy groups $\\pi_*\\mathcal R_{W(k)}$ recover the classical Cartier--Raynaud ring constructed by Illusie--Raynaud.","Moreover, along the way we will describe the compact generator of $p$-typical topological Cartier modules and classifies all natural operations on homotopy groups of $p$-typical topological Cartier modules."],"url":"http://arxiv.org/abs/2404.10724v1","category":"math.AT"}
{"created":"2024-04-16 16:54:20","title":"Evidence of Pop~III stars' chemical signature in neutral gas at $z\\sim6$. A study based on the E-XQR-30 spectroscopic sample","abstract":"This study explores the metal enrichment signatures attributed to the first generation of stars (PopIII) in the Universe, focusing on the E-XQR-30 sample. We aim to identify traces of Pop III metal enrichment by analyzing neutral gas in the interstellar medium of primordial galaxies and their satellite clumps, detected in absorption. To chase the chemical signature of PopIII stars, we studied metal absorption systems in the E-XQR-30 sample, selected through the detection of the OI absorption line at 1302A. The OI line is a reliable tracer of HI and allowed us to overcome the challenges posed by the Lyman-$\\alpha$ forest's increasing saturation at redshifts above $\\sim5$ to identify Damped Lyman-$\\alpha$ systems (DLA). We detected and analyzed 29 OI systems at $z\\geq5.4$, differentiating between proximate DLAs (PDLA) and intervening DLAs. Voigt function fits were applied to obtain ionic column densities, and relative chemical abundances were determined for 28 systems. These were then compared with the predictions of theoretical models. Our findings expand the study of OI systems at $z\\geq5.4$ fourfold. No systematic differences were observed in the average chemical abundances between PDLAs and intervening DLAs. The chemical abundances in our sample align with literature systems at $z>4.5$, suggesting a similar enrichment pattern for this class of absorption systems. A comparison between these DLA-analogues at $4.5<z<6.5$ with a sample of very metal-poor DLAs at $2<z<4.5$ shows in general similar average values for the relative abundances, with the exception of [C/O], [Si/Fe] and [Si/O] which are significantly larger for the high-$z$ sample. Furthermore, the dispersion of the measurements significantly increases in the high-redshift bin. This increase is predicted by the theoretical models and indicates a potential retention of PopIII signatures in the probed gas. (Abridged)","sentences":["This study explores the metal enrichment signatures attributed to the first generation of stars (PopIII) in the Universe, focusing on the E-XQR-30 sample.","We aim to identify traces of Pop III metal enrichment by analyzing neutral gas in the interstellar medium of primordial galaxies and their satellite clumps, detected in absorption.","To chase the chemical signature of PopIII stars, we studied metal absorption systems in the E-XQR-30 sample, selected through the detection of the OI absorption line at 1302A.","The OI line is a reliable tracer of HI and allowed us to overcome the challenges posed by the Lyman-$\\alpha$ forest's increasing saturation at redshifts above $\\sim5$ to identify Damped Lyman-$\\alpha$ systems (DLA).","We detected and analyzed 29 OI systems at $z\\geq5.4$, differentiating between proximate DLAs (PDLA) and intervening DLAs.","Voigt function fits were applied to obtain ionic column densities, and relative chemical abundances were determined for 28 systems.","These were then compared with the predictions of theoretical models.","Our findings expand the study of OI systems at $z\\geq5.4$ fourfold.","No systematic differences were observed in the average chemical abundances between PDLAs and intervening DLAs.","The chemical abundances in our sample align with literature systems at $z>4.5$, suggesting a similar enrichment pattern for this class of absorption systems.","A comparison between these DLA-analogues at $4.5<z<6.5$ with a sample of very metal-poor DLAs at $2<z<4.5$ shows in general similar average values for the relative abundances, with the exception of [C/O], [Si/Fe] and","[Si/O] which are significantly larger for the high-$z$ sample.","Furthermore, the dispersion of the measurements significantly increases in the high-redshift bin.","This increase is predicted by the theoretical models and indicates a potential retention of PopIII signatures in the probed gas.","(Abridged)"],"url":"http://arxiv.org/abs/2404.10722v1","category":"astro-ph.GA"}
{"created":"2024-04-16 16:54:00","title":"The impact of an antisymmetric tensor on charged black holes: evaporation process, geodesics, deflection angle, scattering effects and quasinormal modes","abstract":"In this paper, we investigate the influence of anti-symmetric tensor effects, which trigger the Lorentz symmetry breaking, on charged spherically symmetric black holes. Initially, we address an overview of the model, laying the groundwork for deriving solutions to black holes. With this, we analyze the horizons, critical orbits, and geodesics. We compute quasinormal modes with a particular emphasis on vectorial perturbations. In addition, we derive the Hawking temperature to perform the calculation of the remnant mass. Additionally, we estimate the lifetime of the black holes until they reach their final stage after the evaporation process. Furthermore, we explore the emission rate, and the deflection angle. Finally, we investigate the correlation between quasinormal modes and shadows.","sentences":["In this paper, we investigate the influence of anti-symmetric tensor effects, which trigger the Lorentz symmetry breaking, on charged spherically symmetric black holes.","Initially, we address an overview of the model, laying the groundwork for deriving solutions to black holes.","With this, we analyze the horizons, critical orbits, and geodesics.","We compute quasinormal modes with a particular emphasis on vectorial perturbations.","In addition, we derive the Hawking temperature to perform the calculation of the remnant mass.","Additionally, we estimate the lifetime of the black holes until they reach their final stage after the evaporation process.","Furthermore, we explore the emission rate, and the deflection angle.","Finally, we investigate the correlation between quasinormal modes and shadows."],"url":"http://arxiv.org/abs/2404.10721v1","category":"gr-qc"}
{"created":"2024-04-16 16:51:53","title":"Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study","abstract":"Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across various a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences.","Existing RLHF methods can be roughly categorized as either reward-based or reward-free.","Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO).","However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO).","Is DPO truly superior to PPO?","Why does PPO perform poorly on these benchmarks?","In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations.","Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs.","Finally, we benchmark DPO and PPO across various a collection of RLHF testbeds, ranging from dialogue to code generation.","Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions."],"url":"http://arxiv.org/abs/2404.10719v1","category":"cs.CL"}
{"created":"2024-04-16 16:51:12","title":"Mixed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation","abstract":"Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance. However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding. To address this problem, we propose the Mixed Prototype Consistency Learning (MPCL) framework, which includes a Mean Teacher and an auxiliary network. The Mean Teacher generates prototypes for labeled and unlabeled data, while the auxiliary network produces additional prototypes for mixed data processed by CutMix. Through prototype fusion, mixed prototypes provide extra semantic information to both labeled and unlabeled prototypes. High-quality global prototypes for each class are formed by fusing two enhanced prototypes, optimizing the distribution of hidden embeddings used in consistency learning. Extensive experiments on the left atrium and type B aortic dissection datasets demonstrate MPCL's superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework. The code will be released soon.","sentences":["Recently, prototype learning has emerged in semi-supervised medical image segmentation and achieved remarkable performance.","However, the scarcity of labeled data limits the expressiveness of prototypes in previous methods, potentially hindering the complete representation of prototypes for class embedding.","To address this problem, we propose the Mixed Prototype Consistency Learning (MPCL) framework, which includes a Mean Teacher and an auxiliary network.","The Mean Teacher generates prototypes for labeled and unlabeled data, while the auxiliary network produces additional prototypes for mixed data processed by CutMix.","Through prototype fusion, mixed prototypes provide extra semantic information to both labeled and unlabeled prototypes.","High-quality global prototypes for each class are formed by fusing two enhanced prototypes, optimizing the distribution of hidden embeddings used in consistency learning.","Extensive experiments on the left atrium and type B aortic dissection datasets demonstrate MPCL's superiority over previous state-of-the-art approaches, confirming the effectiveness of our framework.","The code will be released soon."],"url":"http://arxiv.org/abs/2404.10717v1","category":"cs.CV"}
{"created":"2024-04-16 16:50:35","title":"MOWA: Multiple-in-One Image Warping Model","abstract":"While recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations. To address diverse types of warping in practice, we propose a Multiple-in-One image WArping model (named MOWA) in this work. Specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level. To further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for better estimation. To our knowledge, this is the first work that solves multiple practical warping tasks in one single model. Extensive experiments demonstrate that our MOWA, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks. Moreover, MOWA also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code will be made publicly available.","sentences":["While recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations.","To address diverse types of warping in practice, we propose a Multiple-in-One image WArping model (named MOWA) in this work.","Specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level.","To further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for better estimation.","To our knowledge, this is the first work that solves multiple practical warping tasks in one single model.","Extensive experiments demonstrate that our MOWA, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks.","Moreover, MOWA also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2404.10716v1","category":"cs.CV"}
{"created":"2024-04-16 16:43:36","title":"AV-GAN: Attention-Based Varifocal Generative Adversarial Network for Uneven Medical Image Translation","abstract":"Different types of staining highlight different structures in organs, thereby assisting in diagnosis. However, due to the impossibility of repeated staining, we cannot obtain different types of stained slides of the same tissue area. Translating the slide that is easy to obtain (e.g., H&E) to slides of staining types difficult to obtain (e.g., MT, PAS) is a promising way to solve this problem. However, some regions are closely connected to other regions, and to maintain this connection, they often have complex structures and are difficult to translate, which may lead to wrong translations. In this paper, we propose the Attention-Based Varifocal Generative Adversarial Network (AV-GAN), which solves multiple problems in pathologic image translation tasks, such as uneven translation difficulty in different regions, mutual interference of multiple resolution information, and nuclear deformation. Specifically, we develop an Attention-Based Key Region Selection Module, which can attend to regions with higher translation difficulty. We then develop a Varifocal Module to translate these regions at multiple resolutions. Experimental results show that our proposed AV-GAN outperforms existing image translation methods with two virtual kidney tissue staining tasks and improves FID values by 15.9 and 4.16 respectively in the H&E-MT and H&E-PAS tasks.","sentences":["Different types of staining highlight different structures in organs, thereby assisting in diagnosis.","However, due to the impossibility of repeated staining, we cannot obtain different types of stained slides of the same tissue area.","Translating the slide that is easy to obtain (e.g., H&E) to slides of staining types difficult to obtain (e.g., MT, PAS) is a promising way to solve this problem.","However, some regions are closely connected to other regions, and to maintain this connection, they often have complex structures and are difficult to translate, which may lead to wrong translations.","In this paper, we propose the Attention-Based Varifocal Generative Adversarial Network (AV-GAN), which solves multiple problems in pathologic image translation tasks, such as uneven translation difficulty in different regions, mutual interference of multiple resolution information, and nuclear deformation.","Specifically, we develop an Attention-Based Key Region Selection Module, which can attend to regions with higher translation difficulty.","We then develop a Varifocal Module to translate these regions at multiple resolutions.","Experimental results show that our proposed AV-GAN outperforms existing image translation methods with two virtual kidney tissue staining tasks and improves FID values by 15.9 and 4.16 respectively in the H&E-MT and H&E-PAS tasks."],"url":"http://arxiv.org/abs/2404.10714v1","category":"eess.IV"}
{"created":"2024-04-16 16:36:50","title":"Dual Modalities of Text: Visual and Textual Generative Pre-training","abstract":"Harnessing visual texts represents a burgeoning frontier in the evolution of language modeling. In this paper, we introduce a novel pre-training framework for a suite of pixel-based autoregressive language models, pre-training on a corpus of over 400 million documents rendered as RGB images. Our approach is characterized by a dual-modality training regimen, engaging both visual data through next patch prediction with a regression head and textual data via next token prediction with a classification head. This study is particularly focused on investigating the synergistic interplay between visual and textual modalities of language. Our comprehensive evaluation across a diverse array of benchmarks reveals that the confluence of visual and textual data substantially augments the efficacy of pixel-based language models. Notably, our findings show that a unidirectional pixel-based model, devoid of textual data during training, can match the performance levels of advanced bidirectional pixel-based models on various language understanding benchmarks. This work highlights the considerable untapped potential of integrating visual and textual information for language modeling purposes. We will release our code, data, and checkpoints to inspire further research advancement.","sentences":["Harnessing visual texts represents a burgeoning frontier in the evolution of language modeling.","In this paper, we introduce a novel pre-training framework for a suite of pixel-based autoregressive language models, pre-training on a corpus of over 400 million documents rendered as RGB images.","Our approach is characterized by a dual-modality training regimen, engaging both visual data through next patch prediction with a regression head and textual data via next token prediction with a classification head.","This study is particularly focused on investigating the synergistic interplay between visual and textual modalities of language.","Our comprehensive evaluation across a diverse array of benchmarks reveals that the confluence of visual and textual data substantially augments the efficacy of pixel-based language models.","Notably, our findings show that a unidirectional pixel-based model, devoid of textual data during training, can match the performance levels of advanced bidirectional pixel-based models on various language understanding benchmarks.","This work highlights the considerable untapped potential of integrating visual and textual information for language modeling purposes.","We will release our code, data, and checkpoints to inspire further research advancement."],"url":"http://arxiv.org/abs/2404.10710v1","category":"cs.CL"}
{"created":"2024-04-16 16:32:10","title":"Keeping the photon in the dark: Enabling full quantum dot control by chirped pulses and magnetic fields","abstract":"Because dark excitons in quantum dots are not directly optically accessible, so far they have not played a significant role in using quantum dots for photon generation. They possess significantly longer lifetimes than their brighter counterparts and hence offer enormous potential for photon storage or manipulation. In this work, we demonstrate an all-optical storage and retrieval of the spin-forbidden dark exciton in a quantum dot from the ground state employing chirped pulses and an in-plane magnetic field. Our experimental findings are in excellent agreement with theoretical predictions of the dynamics calculated using state-of-the-art product tensor methods. Our scheme enables an all-optical control of dark states without relying on any preceding decays. This opens up a new dimension for optimal quantum control and time-bin entangled photon pair generation from quantum dots.","sentences":["Because dark excitons in quantum dots are not directly optically accessible, so far they have not played a significant role in using quantum dots for photon generation.","They possess significantly longer lifetimes than their brighter counterparts and hence offer enormous potential for photon storage or manipulation.","In this work, we demonstrate an all-optical storage and retrieval of the spin-forbidden dark exciton in a quantum dot from the ground state employing chirped pulses and an in-plane magnetic field.","Our experimental findings are in excellent agreement with theoretical predictions of the dynamics calculated using state-of-the-art product tensor methods.","Our scheme enables an all-optical control of dark states without relying on any preceding decays.","This opens up a new dimension for optimal quantum control and time-bin entangled photon pair generation from quantum dots."],"url":"http://arxiv.org/abs/2404.10708v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-16 16:23:10","title":"Question Difficulty Ranking for Multiple-Choice Reading Comprehension","abstract":"Multiple-choice (MC) tests are an efficient method to assess English learners. It is useful for test creators to rank candidate MC questions by difficulty during exam curation. Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage. However, this is expensive and not scalable. Therefore, we explore automated approaches to rank MC questions by difficulty. However, there is limited data for explicit training of a system for difficulty scores. Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative. It is found that level classification transfers better than reading comprehension. Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%. Combining the systems is observed to further boost the correlation.","sentences":["Multiple-choice (MC) tests are an efficient method to assess English learners.","It is useful for test creators to rank candidate MC questions by difficulty during exam curation.","Typically, the difficulty is determined by having human test takers trial the questions in a pretesting stage.","However, this is expensive and not scalable.","Therefore, we explore automated approaches to rank MC questions by difficulty.","However, there is limited data for explicit training of a system for difficulty scores.","Hence, we compare task transfer and zero-shot approaches: task transfer adapts level classification and reading comprehension systems for difficulty ranking while zero-shot prompting of instruction finetuned language models contrasts absolute assessment against comparative.","It is found that level classification transfers better than reading comprehension.","Additionally, zero-shot comparative assessment is more effective at difficulty ranking than the absolute assessment and even the task transfer approaches at question difficulty ranking with a Spearman's correlation of 40.4%.","Combining the systems is observed to further boost the correlation."],"url":"http://arxiv.org/abs/2404.10704v1","category":"cs.CL"}
{"created":"2024-04-16 16:19:22","title":"Retrieval Augmented Verification : Unveiling Disinformation with Structured Representations for Zero-Shot Real-Time Evidence-guided Fact-Checking of Multi-modal Social media posts","abstract":"Social Media posts, where real images are unscrupulously reused along with provocative text to promote a particular idea, have been one of the major sources of disinformation. By design, these claims are without editorial oversight and accessible to a vast population who otherwise may not have access to multiple information sources. This implies the need to fact-check these posts and clearly explain which parts of the posts are fake. In the supervised learning setup, this is often reduced to a binary classification problem, neglecting all intermediate stages. Further, these claims often involve recent events on which systems trained on historical data are prone to fail. In this work, we propose a zero-shot approach by retrieving real-time web-scraped evidence from multiple news websites and matching them with the claim text and image using pretrained language vision systems. We propose a graph structured representation, which a) allows us to gather evidence automatically and b) helps generate interpretable results by explicitly pointing out which parts of the claim can not be verified. Our zero-shot method, with improved interpretability, generates competitive results against the state-of-the-art methods","sentences":["Social Media posts, where real images are unscrupulously reused along with provocative text to promote a particular idea, have been one of the major sources of disinformation.","By design, these claims are without editorial oversight and accessible to a vast population who otherwise may not have access to multiple information sources.","This implies the need to fact-check these posts and clearly explain which parts of the posts are fake.","In the supervised learning setup, this is often reduced to a binary classification problem, neglecting all intermediate stages.","Further, these claims often involve recent events on which systems trained on historical data are prone to fail.","In this work, we propose a zero-shot approach by retrieving real-time web-scraped evidence from multiple news websites and matching them with the claim text and image using pretrained language vision systems.","We propose a graph structured representation, which a) allows us to gather evidence automatically and b) helps generate interpretable results by explicitly pointing out which parts of the claim can not be verified.","Our zero-shot method, with improved interpretability, generates competitive results against the state-of-the-art methods"],"url":"http://arxiv.org/abs/2404.10702v1","category":"cs.MM"}
{"created":"2024-04-16 16:18:00","title":"Spectra of subrings of cohomology generated by characteristic classes for fusion systems","abstract":"If $\\mathcal{F}$ is a saturated fusion system on a finite $p$-group $S$, we define the Chern subring $Ch(\\mathcal{F})$ of $\\mathcal{F}$ to be the subring of the mod-$p$ cohomology $H^*(S)$ of $S$ generated by the Chern classes of $\\mathcal{F}$-stable representations of $S$. We show that $Ch(\\mathcal{F})$ is contained in $H^*(\\mathcal{F})$ and apply a result of Green and the first author to describe its spectrum in terms of a certain category of elementary abelian subgroups of $S$. We obtain similar results for various related subrings, including those generated by characteristic classes of $\\mathcal{F}$-stable $S$-sets.","sentences":["If $\\mathcal{F}$ is a saturated fusion system on a finite $p$-group $S$, we define the Chern subring $Ch(\\mathcal{F})$ of $\\mathcal{F}$ to be the subring of the mod-$p$ cohomology $H^*(S)$ of $S$ generated by the Chern classes of $\\mathcal{F}$-stable representations of $S$. We show that $Ch(\\mathcal{F})$ is contained in $H^*(\\mathcal{F})$ and apply a result of Green and the first author to describe its spectrum in terms of a certain category of elementary abelian subgroups of $S$. We obtain similar results for various related subrings, including those generated by characteristic classes of $\\mathcal{F}$-stable $S$-sets."],"url":"http://arxiv.org/abs/2404.10701v1","category":"math.GR"}
{"created":"2024-04-16 16:17:48","title":"Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs","abstract":"Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB). Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs. However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images. This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras. Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation. It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras. Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images.","sentences":["Modern smartphone camera quality heavily relies on the image signal processor (ISP) to enhance captured raw images, utilizing carefully designed modules to produce final output images encoded in a standard color space (e.g., sRGB).","Neural-based end-to-end learnable ISPs offer promising advancements, potentially replacing traditional ISPs with their ability to adapt without requiring extensive tuning for each new camera model, as is often the case for nearly every module in traditional ISPs.","However, the key challenge with the recent learning-based ISPs is the urge to collect large paired datasets for each distinct camera model due to the influence of intrinsic camera characteristics on the formation of input raw images.","This paper tackles this challenge by introducing a novel method for unpaired learning of raw-to-raw translation across diverse cameras.","Specifically, we propose Rawformer, an unsupervised Transformer-based encoder-decoder method for raw-to-raw translation.","It accurately maps raw images captured by a certain camera to the target camera, facilitating the generalization of learnable ISPs to new unseen cameras.","Our method demonstrates superior performance on real camera datasets, achieving higher accuracy compared to previous state-of-the-art techniques, and preserving a more robust correlation between the original and translated raw images."],"url":"http://arxiv.org/abs/2404.10700v1","category":"eess.IV"}
{"created":"2024-04-16 16:15:26","title":"Two-time quantities as elements of physical reality","abstract":"In recent years, time correlators have received renewed attention, especially under the guise of identifiers of nonclassical correlations. However, the physical interpretation of these objects, and more generally of multi-times variables, remains ambiguous, which may be one of the reasons why they are so difficult to measure. In this work, we introduce and advance the perspective that a two-time correlator should actually be regarded as an average involving a novel single physical observable, one that cannot be rephrased in terms of the primitive ones, according to quantum principles. In particular, we provide examples showing that the presumed constituents of a two-time correlator and the proposed two-time operator itself cannot be simultaneous elements of the physical reality.","sentences":["In recent years, time correlators have received renewed attention, especially under the guise of identifiers of nonclassical correlations.","However, the physical interpretation of these objects, and more generally of multi-times variables, remains ambiguous, which may be one of the reasons why they are so difficult to measure.","In this work, we introduce and advance the perspective that a two-time correlator should actually be regarded as an average involving a novel single physical observable, one that cannot be rephrased in terms of the primitive ones, according to quantum principles.","In particular, we provide examples showing that the presumed constituents of a two-time correlator and the proposed two-time operator itself cannot be simultaneous elements of the physical reality."],"url":"http://arxiv.org/abs/2404.10697v1","category":"quant-ph"}
{"created":"2024-04-16 16:14:53","title":"Strong-weak symmetry and quantum modularity of resurgent topological strings on local $\\mathbb{P}^2$","abstract":"Quantizing the mirror curve to a toric Calabi-Yau threefold gives rise to quantum operators whose fermionic spectral traces produce factorially divergent formal power series in the Planck constant and its inverse. These are conjecturally captured by the Nekrasov-Shatashvili and standard topological string free energies, respectively, via the TS/ST correspondence. The resurgent structures of the first fermionic spectral trace of local $\\mathbb{P}^2$ in both weak and strong coupling limits were solved exactly by the second author in [1]. Here, we take the perspective of the Stokes constants and their generating functions. We prove that a full-fledged strong-weak resurgent symmetry is at play, exchanging the perturbative/nonperturbative contributions to the holomorphic and anti-holomorphic blocks in the factorization of the spectral trace. This relies on a global net of relations connecting the perturbative series and the discontinuities in the dual regimes, which is built upon the analytic properties of the $L$-functions with coefficients given by the Stokes constants and the $q$-series acting as their generating functions. Then, we show that the latter are holomorphic quantum modular forms for $\\Gamma_1(3)$ and are reconstructed by the median resummation of their asymptotic expansions.","sentences":["Quantizing the mirror curve to a toric Calabi-Yau threefold gives rise to quantum operators whose fermionic spectral traces produce factorially divergent formal power series in the Planck constant and its inverse.","These are conjecturally captured by the Nekrasov-Shatashvili and standard topological string free energies, respectively, via the TS/ST correspondence.","The resurgent structures of the first fermionic spectral trace of local $\\mathbb{P}^2$ in both weak and strong coupling limits were solved exactly by the second author in [1].","Here, we take the perspective of the Stokes constants and their generating functions.","We prove that a full-fledged strong-weak resurgent symmetry is at play, exchanging the perturbative/nonperturbative contributions to the holomorphic and anti-holomorphic blocks in the factorization of the spectral trace.","This relies on a global net of relations connecting the perturbative series and the discontinuities in the dual regimes, which is built upon the analytic properties of the $L$-functions with coefficients given by the Stokes constants and the $q$-series acting as their generating functions.","Then, we show that the latter are holomorphic quantum modular forms for $\\Gamma_1(3)$ and are reconstructed by the median resummation of their asymptotic expansions."],"url":"http://arxiv.org/abs/2404.10695v1","category":"hep-th"}
{"created":"2024-04-16 16:11:56","title":"A hybrid Quantum-Classical Algorithm for Mixed-Integer Optimization in Power Systems","abstract":"Mixed Integer Linear Programming (MILP) can be considered the backbone of the modern power system optimization process, with a large application spectrum, from Unit Commitment and Optimal Transmission Switching to verifying Neural Networks for power system applications. The main issue of these formulations is the computational complexity of the solution algorithms, as they are considered NP-Hard problems. Quantum computing has been tested as a potential solution towards reducing the computational burden imposed by these problems, providing promising results, motivating the can be used to speedup the solution of MILPs. In this work, we present a general framework for solving power system optimization problems with a Quantum Computer (QC), which leverages mathematical tools and QCs' sampling ability to provide accelerated solutions. Our guiding applications are the optimal transmission switching and the verification of neural networks trained to solve a DC Optimal Power Flow. Specifically, using an accelerated version of Benders Decomposition , we split a given MILP into an Integer Master Problem and a linear Subproblem and solve it through a hybrid ``quantum-classical'' approach, getting the best of both worlds. We provide 2 use cases, and benchmark the developed framework against other classical and hybrid methodologies, to demonstrate the opportunities and challenges of hybrid quantum-classical algorithms for power system mixed integer optimization problems.","sentences":["Mixed Integer Linear Programming (MILP) can be considered the backbone of the modern power system optimization process, with a large application spectrum, from Unit Commitment and Optimal Transmission Switching to verifying Neural Networks for power system applications.","The main issue of these formulations is the computational complexity of the solution algorithms, as they are considered NP-Hard problems.","Quantum computing has been tested as a potential solution towards reducing the computational burden imposed by these problems, providing promising results, motivating the can be used to speedup the solution of MILPs.","In this work, we present a general framework for solving power system optimization problems with a Quantum Computer (QC), which leverages mathematical tools and QCs' sampling ability to provide accelerated solutions.","Our guiding applications are the optimal transmission switching and the verification of neural networks trained to solve a DC Optimal Power Flow.","Specifically, using an accelerated version of Benders Decomposition , we split a given MILP into an Integer Master Problem and a linear Subproblem and solve it through a hybrid ``quantum-classical'' approach, getting the best of both worlds.","We provide 2 use cases, and benchmark the developed framework against other classical and hybrid methodologies, to demonstrate the opportunities and challenges of hybrid quantum-classical algorithms for power system mixed integer optimization problems."],"url":"http://arxiv.org/abs/2404.10693v1","category":"quant-ph"}
{"created":"2024-04-16 16:08:59","title":"Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution","abstract":"Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.","sentences":["Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image.","Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods.","However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting.","In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution.","To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation.","Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales.","Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution.","Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption.","Our code is available at https://github.com/Yuan-Yutao/ECDP."],"url":"http://arxiv.org/abs/2404.10688v1","category":"cs.CV"}
{"created":"2024-04-16 16:06:44","title":"Swarm-Based Trajectory Generation and Optimization for Stress-Aligned 3D Printing","abstract":"In this study, we present a novel swarm-based approach for generating optimized stress-aligned trajectories for 3D printing applications. The method utilizes swarming dynamics to simulate the motion of virtual agents along the stress produced in a loaded part. Agent trajectories are then used as print trajectories. With this approach, the complex global trajectory generation problem is subdivided into a set of sequential and computationally efficient quadratic programs. Through comprehensive evaluations in both simulation and experiments, we compare our method with state-of-the-art approaches. Our results highlight a remarkable improvement in computational efficiency, achieving a 115x faster computation speed than existing methods. This efficiency, combined with the possibility to tune the trajectories spacing to match the deposition process constraints, makes the potential integration of our approach into existing 3D printing processes seamless. Additionally, the open-hole tensile specimen produced on a conventional fused filament fabrication set-up with our algorithm achieve a notable ~10% improvement in specific modulus compared to existing trajectory optimization methods.","sentences":["In this study, we present a novel swarm-based approach for generating optimized stress-aligned trajectories for 3D printing applications.","The method utilizes swarming dynamics to simulate the motion of virtual agents along the stress produced in a loaded part.","Agent trajectories are then used as print trajectories.","With this approach, the complex global trajectory generation problem is subdivided into a set of sequential and computationally efficient quadratic programs.","Through comprehensive evaluations in both simulation and experiments, we compare our method with state-of-the-art approaches.","Our results highlight a remarkable improvement in computational efficiency, achieving a 115x faster computation speed than existing methods.","This efficiency, combined with the possibility to tune the trajectories spacing to match the deposition process constraints, makes the potential integration of our approach into existing 3D printing processes seamless.","Additionally, the open-hole tensile specimen produced on a conventional fused filament fabrication set-up with our algorithm achieve a notable ~10% improvement in specific modulus compared to existing trajectory optimization methods."],"url":"http://arxiv.org/abs/2404.10686v1","category":"math.OC"}
{"created":"2024-04-16 16:04:38","title":"Generating Human Interaction Motions in Scenes with Text Control","abstract":"We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions. Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo.","sentences":["We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models.","Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes.","Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets.","We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes.","To facilitate training, we embed annotated navigation and interaction motions within scenes.","The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses.","Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions.","Code will be released upon publication of this work at https://research.nvidia.com/labs/toronto-ai/tesmo."],"url":"http://arxiv.org/abs/2404.10685v1","category":"cs.CV"}
{"created":"2024-04-16 16:00:59","title":"Simplex Decomposition for Portfolio Allocation Constraints in Reinforcement Learning","abstract":"Portfolio optimization tasks describe sequential decision problems in which the investor's wealth is distributed across a set of assets. Allocation constraints are used to enforce minimal or maximal investments into particular subsets of assets to control for objectives such as limiting the portfolio's exposure to a certain sector due to environmental concerns. Although methods for constrained Reinforcement Learning (CRL) can optimize policies while considering allocation constraints, it can be observed that these general methods yield suboptimal results. In this paper, we propose a novel approach to handle allocation constraints based on a decomposition of the constraint action space into a set of unconstrained allocation problems. In particular, we examine this approach for the case of two constraints. For example, an investor may wish to invest at least a certain percentage of the portfolio into green technologies while limiting the investment in the fossil energy sector. We show that the action space of the task is equivalent to the decomposed action space, and introduce a new reinforcement learning (RL) approach CAOSD, which is built on top of the decomposition. The experimental evaluation on real-world Nasdaq-100 data demonstrates that our approach consistently outperforms state-of-the-art CRL benchmarks for portfolio optimization.","sentences":["Portfolio optimization tasks describe sequential decision problems in which the investor's wealth is distributed across a set of assets.","Allocation constraints are used to enforce minimal or maximal investments into particular subsets of assets to control for objectives such as limiting the portfolio's exposure to a certain sector due to environmental concerns.","Although methods for constrained Reinforcement Learning (CRL) can optimize policies while considering allocation constraints, it can be observed that these general methods yield suboptimal results.","In this paper, we propose a novel approach to handle allocation constraints based on a decomposition of the constraint action space into a set of unconstrained allocation problems.","In particular, we examine this approach for the case of two constraints.","For example, an investor may wish to invest at least a certain percentage of the portfolio into green technologies while limiting the investment in the fossil energy sector.","We show that the action space of the task is equivalent to the decomposed action space, and introduce a new reinforcement learning (RL) approach CAOSD, which is built on top of the decomposition.","The experimental evaluation on real-world Nasdaq-100 data demonstrates that our approach consistently outperforms state-of-the-art CRL benchmarks for portfolio optimization."],"url":"http://arxiv.org/abs/2404.10683v1","category":"cs.AI"}
{"created":"2024-04-16 15:58:49","title":"StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text Reference via Progressive Optimization","abstract":"Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences.","sentences":["Creating large-scale virtual urban scenes with variant styles is inherently challenging.","To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity.","Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background.","To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally.","During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content.","We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views.","Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization.","Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process.","The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process.","Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences."],"url":"http://arxiv.org/abs/2404.10681v1","category":"cs.CV"}
{"created":"2024-04-16 15:58:20","title":"HSVI-based Online Minimax Strategies for Partially Observable Stochastic Games with Neural Perception Mechanisms","abstract":"We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure. One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state. We present, for the first time, an efficient online method to compute an $\\varepsilon$-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values. For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values. This inherits the soundness of continual resolving at the cost of pre-computing the bound. For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing $\\varepsilon$-distance to the value of the game at the initial belief known to both agents.","sentences":["We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure.","One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state.","We present, for the first time, an efficient online method to compute an $\\varepsilon$-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values.","For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values.","This inherits the soundness of continual resolving at the cost of pre-computing the bound.","For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing $\\varepsilon$-distance to the value of the game at the initial belief known to both agents."],"url":"http://arxiv.org/abs/2404.10679v1","category":"cs.GT"}
{"created":"2024-04-16 15:53:41","title":"Automating REST API Postman Test Cases Using LLM","abstract":"In the contemporary landscape of technological advancements, the automation of manual processes is crucial, compelling the demand for huge datasets to effectively train and test machines. This research paper is dedicated to the exploration and implementation of an automated approach to generate test cases specifically using Large Language Models. The methodology integrates the use of Open AI to enhance the efficiency and effectiveness of test case generation for training and evaluating Large Language Models. This formalized approach with LLMs simplifies the testing process, making it more efficient and comprehensive. Leveraging natural language understanding, LLMs can intelligently formulate test cases that cover a broad range of REST API properties, ensuring comprehensive testing. The model that is developed during the research is trained using manually collected postman test cases or instances for various Rest APIs. LLMs enhance the creation of Postman test cases by automating the generation of varied and intricate test scenarios. Postman test cases offer streamlined automation, collaboration, and dynamic data handling, providing a user-friendly and efficient approach to API testing compared to traditional test cases. Thus, the model developed not only conforms to current technological standards but also holds the promise of evolving into an idea of substantial importance in future technological advancements.","sentences":["In the contemporary landscape of technological advancements, the automation of manual processes is crucial, compelling the demand for huge datasets to effectively train and test machines.","This research paper is dedicated to the exploration and implementation of an automated approach to generate test cases specifically using Large Language Models.","The methodology integrates the use of Open AI to enhance the efficiency and effectiveness of test case generation for training and evaluating Large Language Models.","This formalized approach with LLMs simplifies the testing process, making it more efficient and comprehensive.","Leveraging natural language understanding, LLMs can intelligently formulate test cases that cover a broad range of REST API properties, ensuring comprehensive testing.","The model that is developed during the research is trained using manually collected postman test cases or instances for various Rest APIs.","LLMs enhance the creation of Postman test cases by automating the generation of varied and intricate test scenarios.","Postman test cases offer streamlined automation, collaboration, and dynamic data handling, providing a user-friendly and efficient approach to API testing compared to traditional test cases.","Thus, the model developed not only conforms to current technological standards but also holds the promise of evolving into an idea of substantial importance in future technological advancements."],"url":"http://arxiv.org/abs/2404.10678v1","category":"cs.SE"}
{"created":"2024-04-16 15:50:19","title":"SCALE: Self-Correcting Visual Navigation for Mobile Robots via Anti-Novelty Estimation","abstract":"Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task. Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation. This significantly drops the success rates and even induces collision. In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention. Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL). When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND). A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places. We collect offline data and conduct evaluation experiments in three real-world urban scenarios. Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention. Code is available at https://github.com/KubeEdge4Robotics/ScaleNav.","sentences":["Although visual navigation has been extensively studied using deep reinforcement learning, online learning for real-world robots remains a challenging task.","Recent work directly learned from offline dataset to achieve broader generalization in the real-world tasks, which, however, faces the out-of-distribution (OOD) issue and potential robot localization failures in a given map for unseen observation.","This significantly drops the success rates and even induces collision.","In this paper, we present a self-correcting visual navigation method, SCALE, that can autonomously prevent the robot from the OOD situations without human intervention.","Specifically, we develop an image-goal conditioned offline reinforcement learning method based on implicit Q-learning (IQL).","When facing OOD observation, our novel localization recovery method generates the potential future trajectories by learning from the navigation affordance, and estimates the future novelty via random network distillation (RND).","A tailored cost function searches for the candidates with the least novelty that can lead the robot to the familiar places.","We collect offline data and conduct evaluation experiments in three real-world urban scenarios.","Experiment results show that SCALE outperforms the previous state-of-the-art methods for open-world navigation with a unique capability of localization recovery, significantly reducing the need for human intervention.","Code is available at https://github.com/KubeEdge4Robotics/ScaleNav."],"url":"http://arxiv.org/abs/2404.10675v1","category":"cs.RO"}
{"created":"2024-04-16 15:48:01","title":"The XUV-driven escape of the planets around TOI-431 & $\u03bd^2$ Lupi","abstract":"One of the leading mechanisms invoked to explain the existence of the radius valley is atmospheric mass loss driven by X-ray and extreme-ultraviolet irradiation, with this process stripping the primordial envelopes of young, small planets to produce the observed bimodal distribution. We present an investigation into the TOI-431 and $\\nu^2$ Lupi planetary systems, both of which host planets either side of the radius valley, to determine if their architectures are consistent with evolution by the XUV mechanism. With $\\textit{XMM-Newton}$, we measure the current X-ray flux of each star, and see evidence for a stellar flare in the TOI-431 observations. We then simulate the evolution of all of the transiting planets across the two systems in response to the high-energy irradiation over their lifetimes. We use the measured X-ray fluxes as an anchor point for the XUV time evolution in our simulations, and employ several different models of estimating mass loss rates. While the simulations for TOI-431b encountered a problem with the initial calculated radii, we estimate a likely short ($\\sim$ Myr) timespan for primordial envelope removal using reasonable assumptions for the initial planet. $\\nu^2$ Lupi b is likely harder to strip, but is achieved in a moderate fraction of our simulations. None of our simulations stripped any of the lower density planets of their envelope, in line with prediction. We conclude that both systems are consistent with expectations for generation of the radius valley through XUV photoevaporation.","sentences":["One of the leading mechanisms invoked to explain the existence of the radius valley is atmospheric mass loss driven by X-ray and extreme-ultraviolet irradiation, with this process stripping the primordial envelopes of young, small planets to produce the observed bimodal distribution.","We present an investigation into the TOI-431 and $\\nu^2$ Lupi planetary systems, both of which host planets either side of the radius valley, to determine if their architectures are consistent with evolution by the XUV mechanism.","With $\\textit{XMM-Newton}$, we measure the current X-ray flux of each star, and see evidence for a stellar flare in the TOI-431 observations.","We then simulate the evolution of all of the transiting planets across the two systems in response to the high-energy irradiation over their lifetimes.","We use the measured X-ray fluxes as an anchor point for the XUV time evolution in our simulations, and employ several different models of estimating mass loss rates.","While the simulations for TOI-431b encountered a problem with the initial calculated radii, we estimate a likely short ($\\sim$ Myr) timespan for primordial envelope removal using reasonable assumptions for the initial planet.","$\\nu^2$ Lupi b is likely harder to strip, but is achieved in a moderate fraction of our simulations.","None of our simulations stripped any of the lower density planets of their envelope, in line with prediction.","We conclude that both systems are consistent with expectations for generation of the radius valley through XUV photoevaporation."],"url":"http://arxiv.org/abs/2404.10673v1","category":"astro-ph.EP"}
{"created":"2024-04-16 15:46:22","title":"The Simultaneous Interval Number: A New Width Parameter that Measures the Similarity to Interval Graphs","abstract":"We propose a novel way of generalizing the class of interval graphs, via a graph width parameter called the simultaneous interval number. This parameter is related to the simultaneous representation problem for interval graphs and defined as the smallest number $d$ of labels such that the graph admits a $d$-simultaneous interval representation, that is, an assignment of intervals and label sets to the vertices such that two vertices are adjacent if and only if the corresponding intervals, as well as their label sets, intersect. We show that this parameter is $\\mathsf{NP}$-hard to compute and give several bounds for the parameter, showing in particular that it is sandwiched between pathwidth and linear mim-width. For classes of graphs with bounded parameter values, assuming that the graph is equipped with a simultaneous interval representation with a constant number of labels, we give $\\mathsf{FPT}$ algorithms for the clique, independent set, and dominating set problems, and hardness results for the independent dominating set and coloring problems. The $\\mathsf{FPT}$ results for independent set and dominating set are for the simultaneous interval number plus solution size. In contrast, both problems are known to be $\\mathsf{W}[1]$-hard for linear mim-width plus solution size.","sentences":["We propose a novel way of generalizing the class of interval graphs, via a graph width parameter called the simultaneous interval number.","This parameter is related to the simultaneous representation problem for interval graphs and defined as the smallest number $d$ of labels such that the graph admits a $d$-simultaneous interval representation, that is, an assignment of intervals and label sets to the vertices such that two vertices are adjacent if and only if the corresponding intervals, as well as their label sets, intersect.","We show that this parameter is $\\mathsf{NP}$-hard to compute and give several bounds for the parameter, showing in particular that it is sandwiched between pathwidth and linear mim-width.","For classes of graphs with bounded parameter values, assuming that the graph is equipped with a simultaneous interval representation with a constant number of labels, we give $\\mathsf{FPT}$ algorithms for the clique, independent set, and dominating set problems, and hardness results for the independent dominating set and coloring problems.","The $\\mathsf{FPT}$ results for independent set and dominating set are for the simultaneous interval number plus solution size.","In contrast, both problems are known to be $\\mathsf{W}[1]$-hard for linear mim-width plus solution size."],"url":"http://arxiv.org/abs/2404.10670v1","category":"cs.DM"}
{"created":"2024-04-16 15:46:20","title":"Arsenic diffusion in MOVPE-Grown GaAs/Ge epitaxial structures","abstract":"Germanium is reemerging as a prominent material in the semiconductor field, particularly for electronic applications, photonics, photovoltaics and thermophotovoltaics. Its combination with III-V compound semiconductors through epitaxial growth by metal organic vapor phase epitaxy (MOVPE) is instrumental and thus, the comprehension of the sequential stages in such epitaxial processes is of great importance. During the deposition of GaAs on p-type Ge, the formation of n/p junctions occurs when As diffuses into Ge. It is found that this formation begins in the so-called AsH3 preexposure where Ge substrate is firstly exposed to AsH3. Also important is the fact that both free carrier profiles and As profiles indicate that prolonged AsH3 preexposure times lead to deeper diffusion depths for the same process time. This effect is concomitant with the degradation of the Ge surface morphology, characterized by higher roughness as the AsH3 preexposure duration is extended. Contrary to ion-implanted As in germanium, which shows quadratic dependent diffusivity, our MOVPE investigation using AsH3 indicates a linear relationship, consistent with Takenaka et al.'s MOVPE study using TBAs. Analyzing As profiles alongside simulations, with and without subsequent GaAs epitaxy, suggests the generation of Ge vacancies during the process, contributing to deeper As diffusion.","sentences":["Germanium is reemerging as a prominent material in the semiconductor field, particularly for electronic applications, photonics, photovoltaics and thermophotovoltaics.","Its combination with III-V compound semiconductors through epitaxial growth by metal organic vapor phase epitaxy (MOVPE) is instrumental and thus, the comprehension of the sequential stages in such epitaxial processes is of great importance.","During the deposition of GaAs on p-type Ge, the formation of n/p junctions occurs when As diffuses into Ge.","It is found that this formation begins in the so-called AsH3 preexposure where Ge substrate is firstly exposed to AsH3.","Also important is the fact that both free carrier profiles and As profiles indicate that prolonged AsH3 preexposure times lead to deeper diffusion depths for the same process time.","This effect is concomitant with the degradation of the Ge surface morphology, characterized by higher roughness as the AsH3 preexposure duration is extended.","Contrary to ion-implanted As in germanium, which shows quadratic dependent diffusivity, our MOVPE investigation using AsH3 indicates a linear relationship, consistent with Takenaka et al.'s MOVPE study using TBAs.","Analyzing As profiles alongside simulations, with and without subsequent GaAs epitaxy, suggests the generation of Ge vacancies during the process, contributing to deeper As diffusion."],"url":"http://arxiv.org/abs/2404.10669v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 15:43:22","title":"VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time","abstract":"We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.","sentences":["We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip.","Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness.","The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos.","Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively.","Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency.","It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors."],"url":"http://arxiv.org/abs/2404.10667v1","category":"cs.CV"}
{"created":"2024-04-16 15:39:11","title":"Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay","abstract":"We study continual offline reinforcement learning, a practical paradigm that facilitates forward transfer and mitigates catastrophic forgetting to tackle sequential offline tasks. We propose a dual generative replay framework that retains previous knowledge by concurrent replay of generated pseudo-data. First, we decouple the continual learning policy into a diffusion-based generative behavior model and a multi-head action evaluation model, allowing the policy to inherit distributional expressivity for encompassing a progressive range of diverse behaviors. Second, we train a task-conditioned diffusion model to mimic state distributions of past tasks. Generated states are paired with corresponding responses from the behavior generator to represent old tasks with high-fidelity replayed samples. Finally, by interleaving pseudo samples with real ones of the new task, we continually update the state and behavior generators to model progressively diverse behaviors, and regularize the multi-head critic via behavior cloning to mitigate forgetting. Experiments demonstrate that our method achieves better forward transfer with less forgetting, and closely approximates the results of using previous ground-truth data due to its high-fidelity replay of the sample space. Our code is available at \\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}.","sentences":["We study continual offline reinforcement learning, a practical paradigm that facilitates forward transfer and mitigates catastrophic forgetting to tackle sequential offline tasks.","We propose a dual generative replay framework that retains previous knowledge by concurrent replay of generated pseudo-data.","First, we decouple the continual learning policy into a diffusion-based generative behavior model and a multi-head action evaluation model, allowing the policy to inherit distributional expressivity for encompassing a progressive range of diverse behaviors.","Second, we train a task-conditioned diffusion model to mimic state distributions of past tasks.","Generated states are paired with corresponding responses from the behavior generator to represent old tasks with high-fidelity replayed samples.","Finally, by interleaving pseudo samples with real ones of the new task, we continually update the state and behavior generators to model progressively diverse behaviors, and regularize the multi-head critic via behavior cloning to mitigate forgetting.","Experiments demonstrate that our method achieves better forward transfer with less forgetting, and closely approximates the results of using previous ground-truth data due to its high-fidelity replay of the sample space.","Our code is available at \\href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}."],"url":"http://arxiv.org/abs/2404.10662v1","category":"cs.LG"}
{"created":"2024-04-16 15:39:07","title":"PD-Insighter: A Visual Analytics System to Monitor Daily Actions for Parkinson's Disease Treatment","abstract":"People with Parkinson's Disease (PD) can slow the progression of their symptoms with physical therapy. However, clinicians lack insight into patients' motor function during daily life, preventing them from tailoring treatment protocols to patient needs. This paper introduces PD-Insighter, a system for comprehensive analysis of a person's daily movements for clinical review and decision-making. PD-Insighter provides an overview dashboard for discovering motor patterns and identifying critical deficits during activities of daily living and an immersive replay for closely studying the patient's body movements with environmental context. Developed using an iterative design study methodology in consultation with clinicians, we found that PD-Insighter's ability to aggregate and display data with respect to time, actions, and local environment enabled clinicians to assess a person's overall functioning during daily life outside the clinic. PD-Insighter's design offers future guidance for generalized multiperspective body motion analytics, which may significantly improve clinical decision-making and slow the functional decline of PD and other medical conditions.","sentences":["People with Parkinson's Disease (PD) can slow the progression of their symptoms with physical therapy.","However, clinicians lack insight into patients' motor function during daily life, preventing them from tailoring treatment protocols to patient needs.","This paper introduces PD-Insighter, a system for comprehensive analysis of a person's daily movements for clinical review and decision-making.","PD-Insighter provides an overview dashboard for discovering motor patterns and identifying critical deficits during activities of daily living and an immersive replay for closely studying the patient's body movements with environmental context.","Developed using an iterative design study methodology in consultation with clinicians, we found that PD-Insighter's ability to aggregate and display data with respect to time, actions, and local environment enabled clinicians to assess a person's overall functioning during daily life outside the clinic.","PD-Insighter's design offers future guidance for generalized multiperspective body motion analytics, which may significantly improve clinical decision-making and slow the functional decline of PD and other medical conditions."],"url":"http://arxiv.org/abs/2404.10661v1","category":"cs.HC"}
{"created":"2024-04-16 15:35:34","title":"Trajectory Planning using Reinforcement Learning for Interactive Overtaking Maneuvers in Autonomous Racing Scenarios","abstract":"Conventional trajectory planning approaches for autonomous racing are based on the sequential execution of prediction of the opposing vehicles and subsequent trajectory planning for the ego vehicle. If the opposing vehicles do not react to the ego vehicle, they can be predicted accurately. However, if there is interaction between the vehicles, the prediction loses its validity. For high interaction, instead of a planning approach that reacts exclusively to the fixed prediction, a trajectory planning approach is required that incorporates the interaction with the opposing vehicles. This paper demonstrates the limitations of a widely used conventional sampling-based approach within a highly interactive blocking scenario. We show that high success rates are achieved for less aggressive blocking behavior but that the collision rate increases with more significant interaction. We further propose a novel Reinforcement Learning (RL)-based trajectory planning approach for racing that explicitly exploits the interaction with the opposing vehicle without requiring a prediction. In contrast to the conventional approach, the RL-based approach achieves high success rates even for aggressive blocking behavior. Furthermore, we propose a novel safety layer (SL) that intervenes when the trajectory generated by the RL-based approach is infeasible. In that event, the SL generates a sub-optimal but feasible trajectory, avoiding termination of the scenario due to a not found valid solution.","sentences":["Conventional trajectory planning approaches for autonomous racing are based on the sequential execution of prediction of the opposing vehicles and subsequent trajectory planning for the ego vehicle.","If the opposing vehicles do not react to the ego vehicle, they can be predicted accurately.","However, if there is interaction between the vehicles, the prediction loses its validity.","For high interaction, instead of a planning approach that reacts exclusively to the fixed prediction, a trajectory planning approach is required that incorporates the interaction with the opposing vehicles.","This paper demonstrates the limitations of a widely used conventional sampling-based approach within a highly interactive blocking scenario.","We show that high success rates are achieved for less aggressive blocking behavior but that the collision rate increases with more significant interaction.","We further propose a novel Reinforcement Learning (RL)-based trajectory planning approach for racing that explicitly exploits the interaction with the opposing vehicle without requiring a prediction.","In contrast to the conventional approach, the RL-based approach achieves high success rates even for aggressive blocking behavior.","Furthermore, we propose a novel safety layer (SL) that intervenes when the trajectory generated by the RL-based approach is infeasible.","In that event, the SL generates a sub-optimal but feasible trajectory, avoiding termination of the scenario due to a not found valid solution."],"url":"http://arxiv.org/abs/2404.10658v1","category":"cs.RO"}
{"created":"2024-04-16 15:34:00","title":"The foundation of generalized parallel connections, 2-sums, and segment-cosegment exchanges of matroids","abstract":"We show that, under suitable hypotheses, the foundation of a generalized parallel connection of matroids is the relative tensor product of the foundations. Using this result, we show that the foundation of a 2-sum of matroids is the absolute tensor product of the foundations, and that the foundation of a matroid is invariant under segment-cosegment exchange.","sentences":["We show that, under suitable hypotheses, the foundation of a generalized parallel connection of matroids is the relative tensor product of the foundations.","Using this result, we show that the foundation of a 2-sum of matroids is the absolute tensor product of the foundations, and that the foundation of a matroid is invariant under segment-cosegment exchange."],"url":"http://arxiv.org/abs/2404.10656v1","category":"math.CO"}
{"created":"2024-04-16 15:21:35","title":"The General and Finite Satisfiability Problems for PCTL are Undecidable","abstract":"The general/finite PCTL satisfiability problem asks whether a given PCTL formula has a general/finite model. We show that the finite PCTL satisfiability problem is undecidable, and the general PCTL satisfiability problem is even highly undecidable (beyond the arithmetical hierarchy). Consequently, there are no sound deductive systems proving all generally/finitely valid PCTL formulae.","sentences":["The general/finite PCTL satisfiability problem asks whether a given PCTL formula has a general/finite model.","We show that the finite PCTL satisfiability problem is undecidable, and the general PCTL satisfiability problem is even highly undecidable (beyond the arithmetical hierarchy).","Consequently, there are no sound deductive systems proving all generally/finitely valid PCTL formulae."],"url":"http://arxiv.org/abs/2404.10648v1","category":"cs.LO"}
{"created":"2024-04-16 15:21:23","title":"Cosmic Inflation at the Crossroads","abstract":"The capability of Cosmic Inflation to explain the latest Cosmic Microwave Background and Baryonic Acoustic Oscillation data is assessed by performing Bayesian model comparison within the landscape of nearly three-hundred models of single-field slow-roll inflation. We present the first Bayesian data analysis based on the third-order slow-roll primordial power spectra. In particular, the fourth Hubble-flow function $\\epsilon_4$ remains unbounded while the third function verifies, at two-sigma, $\\epsilon_{3}\\in[-0.4,0.5]$, which is perfectly compatible with the slow-roll predictions for the running of the spectral index. We also observe some residual excess of $B$-modes within the BICEP/Keck data favoring, at a non-statistically significant level, non-vanishing primordial tensor modes: $\\log(\\epsilon_{1}) > -3.9$, at $68\\%$ confidence level. Then, for 283 models of single-field inflation, we compute the Bayesian evidence, the Bayesian dimensionality and the marginalized posteriors of all the models' parameters, including the ones associated with the reheating era. The average information gain on the reheating parameter $R_\\mathrm{reh}$ reaches $1.3 \\pm 0.18$ bits, which is more than a factor two improvement compared to the first Planck data release. As such, inflationary model predictions cannot meet data accuracy without specifying, or marginalizing over, the reheating kinematics. We also find that more than $40\\%$ of the scenarios are now strongly disfavored, which shows that the constraining power of cosmological data is winning against the increase of the number of proposed models. In addition, about $20\\%$ of all models have evidences within the most probable region and are all favored according to the Jeffreys' scale of Bayesian evidences.","sentences":["The capability of Cosmic Inflation to explain the latest Cosmic Microwave Background and Baryonic Acoustic Oscillation data is assessed by performing Bayesian model comparison within the landscape of nearly three-hundred models of single-field slow-roll inflation.","We present the first Bayesian data analysis based on the third-order slow-roll primordial power spectra.","In particular, the fourth Hubble-flow function $\\epsilon_4$ remains unbounded while the third function verifies, at two-sigma, $\\epsilon_{3}\\in[-0.4,0.5]$, which is perfectly compatible with the slow-roll predictions for the running of the spectral index.","We also observe some residual excess of $B$-modes within the BICEP/Keck data favoring, at a non-statistically significant level, non-vanishing primordial tensor modes: $\\log(\\epsilon_{1}) > -3.9$, at $68\\%$ confidence level.","Then, for 283 models of single-field inflation, we compute the Bayesian evidence, the Bayesian dimensionality and the marginalized posteriors of all the models' parameters, including the ones associated with the reheating era.","The average information gain on the reheating parameter $R_\\mathrm{reh}$ reaches $1.3 \\pm 0.18$ bits, which is more than a factor two improvement compared to the first Planck data release.","As such, inflationary model predictions cannot meet data accuracy without specifying, or marginalizing over, the reheating kinematics.","We also find that more than $40\\%$ of the scenarios are now strongly disfavored, which shows that the constraining power of cosmological data is winning against the increase of the number of proposed models.","In addition, about $20\\%$ of all models have evidences within the most probable region and are all favored according to the Jeffreys' scale of Bayesian evidences."],"url":"http://arxiv.org/abs/2404.10647v1","category":"astro-ph.CO"}
{"created":"2024-04-16 15:20:28","title":"Efficient Parking Search using Shared Fleet Data","abstract":"Finding an available on-street parking spot is a relevant problem of day-to-day life. In recent years, cities such as Melbourne and San Francisco deployed sensors that provide real-time information about the occupation of parking spots. Finding a free parking spot in such a smart environment can be modeled and solved as a Markov decision process (MDP). The problem has to consider uncertainty as available parking spots might not remain available until arrival due to other vehicles also claiming spots in the meantime. Knowing the parking intention of every vehicle in the environment would eliminate this uncertainty. Unfortunately, it does currently not seem realistic to have such data from all vehicles. In contrast, acquiring data from a subset of vehicles or a vehicle fleet appears feasible and has the potential to reduce uncertainty.   In this paper, we examine the question of how useful sharing data within a vehicle fleet might be for the search times of particular drivers. We use fleet data to better estimate the availability of parking spots at arrival. Since optimal solutions for large scenarios are infeasible, we base our method on approximate solutions, which have been shown to perform well in single-agent settings. Our experiments are conducted on a simulation using real-world and synthetic data from the city of Melbourne. The results indicate that fleet data can significantly reduce search times for an available parking spot.","sentences":["Finding an available on-street parking spot is a relevant problem of day-to-day life.","In recent years, cities such as Melbourne and San Francisco deployed sensors that provide real-time information about the occupation of parking spots.","Finding a free parking spot in such a smart environment can be modeled and solved as a Markov decision process (MDP).","The problem has to consider uncertainty as available parking spots might not remain available until arrival due to other vehicles also claiming spots in the meantime.","Knowing the parking intention of every vehicle in the environment would eliminate this uncertainty.","Unfortunately, it does currently not seem realistic to have such data from all vehicles.","In contrast, acquiring data from a subset of vehicles or a vehicle fleet appears feasible and has the potential to reduce uncertainty.   ","In this paper, we examine the question of how useful sharing data within a vehicle fleet might be for the search times of particular drivers.","We use fleet data to better estimate the availability of parking spots at arrival.","Since optimal solutions for large scenarios are infeasible, we base our method on approximate solutions, which have been shown to perform well in single-agent settings.","Our experiments are conducted on a simulation using real-world and synthetic data from the city of Melbourne.","The results indicate that fleet data can significantly reduce search times for an available parking spot."],"url":"http://arxiv.org/abs/2404.10646v1","category":"cs.AI"}
{"created":"2024-04-16 15:18:40","title":"Continuous Control Reinforcement Learning: Distributed Distributional DrQ Algorithms","abstract":"Distributed Distributional DrQ is a model-free and off-policy RL algorithm for continuous control tasks based on the state and observation of the agent, which is an actor-critic method with the data-augmentation and the distributional perspective of critic value function. Aim to learn to control the agent and master some tasks in a high-dimensional continuous space. DrQ-v2 uses DDPG as the backbone and achieves out-performance in various continuous control tasks. Here Distributed Distributional DrQ uses Distributed Distributional DDPG as the backbone, and this modification aims to achieve better performance in some hard continuous control tasks through the better expression ability of distributional value function and distributed actor policies.","sentences":["Distributed Distributional DrQ is a model-free and off-policy RL algorithm for continuous control tasks based on the state and observation of the agent, which is an actor-critic method with the data-augmentation and the distributional perspective of critic value function.","Aim to learn to control the agent and master some tasks in a high-dimensional continuous space.","DrQ-v2 uses DDPG as the backbone and achieves out-performance in various continuous control tasks.","Here Distributed Distributional DrQ uses Distributed Distributional DDPG as the backbone, and this modification aims to achieve better performance in some hard continuous control tasks through the better expression ability of distributional value function and distributed actor policies."],"url":"http://arxiv.org/abs/2404.10645v1","category":"cs.LG"}
{"created":"2024-04-16 15:18:08","title":"Lattice QCD determination of the normalization of the leading-twist photon distribution amplitude and susceptibility of the quark condensate","abstract":"The normalization of the leading-twist photon distribution amplitude (DA), $f_{\\gamma}^{\\perp}$, is an important ingredient in the study of exclusive processes involving the photon emission by means of QCD sum-rules. In this paper we determine the up- , down- and strange-quark contribution to $f_{\\gamma}^{\\perp}$ by exploiting its relation to the zero-momentum two-point correlation function of the electromagnetic current $J_{\\rm em}^{\\mu}$ and the electric component of the tensor current $T^{\\mu\\nu}$. To that end we employ the gauge ensembles obtained by using $N_{f}=2+1+1$ Wilson-Clover twisted-mass quark flavors, generated by the Extended Twisted Mass (ETM) Collaboration, and after adding all sources of systematic uncertainties, we obtain a total error of $1.5\\%$ and $3.5\\%$, respectively, for the light- ($u$ and $d$) and strange-quark contribution to $f_{\\gamma}^{\\perp}(2~{\\rm GeV})$ in the $\\overline{\\mathrm{MS}}$ scheme, thus improving their accuracy by a factor of $2.3$ and $2.8$, respectively. For the strange-quark contribution $f_{\\gamma,s}^{\\perp}(2~{\\rm GeV})$, we observe a discrepancy with respect to previous lattice calculations. By combining our result with the world average lattice value of the chiral condensate, we obtain for the susceptibility of the quark condensate $\\chi_d^{\\overline{\\mathrm{MS}}} (2\\, {\\rm GeV}) \\simeq \\chi_u^{\\overline{\\mathrm{MS}}} (2\\, {\\rm GeV}) =2.17(12)~{\\rm GeV^{-2}}$.","sentences":["The normalization of the leading-twist photon distribution amplitude (DA), $f_{\\gamma}^{\\perp}$, is an important ingredient in the study of exclusive processes involving the photon emission by means of QCD sum-rules.","In this paper we determine the up- , down- and strange-quark contribution to $f_{\\gamma}^{\\perp}$ by exploiting its relation to the zero-momentum two-point correlation function of the electromagnetic current $J_{\\rm em}^{\\mu}$ and the electric component of the tensor current $T^{\\mu\\nu}$. To that end we employ the gauge ensembles obtained by using $N_{f}=2+1+1$ Wilson-Clover twisted-mass quark flavors, generated by the Extended Twisted Mass (ETM) Collaboration, and after adding all sources of systematic uncertainties, we obtain a total error of $1.5\\%$ and $3.5\\%$, respectively, for the light- ($u$ and $d$) and strange-quark contribution to $f_{\\gamma}^{\\perp}(2~{\\rm GeV})$ in the $\\overline{\\mathrm{MS}}$ scheme, thus improving their accuracy by a factor of $2.3$ and $2.8$, respectively.","For the strange-quark contribution $f_{\\gamma,s}^{\\perp}(2~{\\rm GeV})$, we observe a discrepancy with respect to previous lattice calculations.","By combining our result with the world average lattice value of the chiral condensate, we obtain for the susceptibility of the quark condensate $\\chi_d^{\\overline{\\mathrm{MS}}} (2\\, {\\rm GeV})","\\simeq \\chi_u^{\\overline{\\mathrm{MS}}} (2\\, {\\rm GeV})","=2.17(12)~{\\rm GeV^{-2}}$."],"url":"http://arxiv.org/abs/2404.10644v1","category":"hep-lat"}
{"created":"2024-04-16 15:11:08","title":"A Fast 3-Approximation for the Capacitated Tree Cover Problem with Edge Loads","abstract":"The capacitated tree cover problem with edge loads is a variant of the tree cover problem, where we are given facility opening costs, edge costs and loads, as well as vertex loads. We try to find a tree cover of minimum cost such that the total edge and vertex load of each tree does not exceed a given bound. We present an $\\mathcal{O}(m\\log n)$ time 3-approximation algorithm for this problem.   This is achieved by starting with a certain LP formulation. We give a combinatorial algorithm that solves the LP optimally in time $\\mathcal{O}(m\\log n)$. Then, we show that a linear time rounding and splitting technique leads to an integral solution that costs at most 3 times as much as the LP solution. Finally, we prove that the integrality gap of the LP is $3$, which shows that we can not improve the rounding step in general.","sentences":["The capacitated tree cover problem with edge loads is a variant of the tree cover problem, where we are given facility opening costs, edge costs and loads, as well as vertex loads.","We try to find a tree cover of minimum cost such that the total edge and vertex load of each tree does not exceed a given bound.","We present an $\\mathcal{O}(m\\log n)$ time 3-approximation algorithm for this problem.   ","This is achieved by starting with a certain LP formulation.","We give a combinatorial algorithm that solves the LP optimally in time $\\mathcal{O}(m\\log n)$. Then, we show that a linear time rounding and splitting technique leads to an integral solution that costs at most 3 times as much as the LP solution.","Finally, we prove that the integrality gap of the LP is $3$, which shows that we can not improve the rounding step in general."],"url":"http://arxiv.org/abs/2404.10638v1","category":"cs.DS"}
{"created":"2024-04-16 15:03:59","title":"Constrained Object Placement Using Reinforcement Learning","abstract":"Close and precise placement of irregularly shaped objects requires a skilled robotic system. Particularly challenging is the manipulation of objects that have sensitive top surfaces and a fixed set of neighbors. To avoid damaging the surface, they have to be grasped from the side, and during placement, their neighbor relations have to be maintained. In this work, we train a reinforcement learning agent that generates smooth end-effector motions to place objects as close as possible next to each other. During the placement, our agent considers neighbor constraints defined in a given layout of the objects while trying to avoid collisions. Our approach learns to place compact object assemblies without the need for predefined spacing between objects as required by traditional methods. We thoroughly evaluated our approach using a two-finger gripper mounted to a robotic arm with six degrees of freedom. The results show that our agent outperforms two baseline approaches in terms of object assembly compactness, thereby reducing the needed space to place the objects according to the given neighbor constraints. On average, our approach reduces the distances between all placed objects by at least 60%, with fewer collisions at the same compactness compared to both baselines.","sentences":["Close and precise placement of irregularly shaped objects requires a skilled robotic system.","Particularly challenging is the manipulation of objects that have sensitive top surfaces and a fixed set of neighbors.","To avoid damaging the surface, they have to be grasped from the side, and during placement, their neighbor relations have to be maintained.","In this work, we train a reinforcement learning agent that generates smooth end-effector motions to place objects as close as possible next to each other.","During the placement, our agent considers neighbor constraints defined in a given layout of the objects while trying to avoid collisions.","Our approach learns to place compact object assemblies without the need for predefined spacing between objects as required by traditional methods.","We thoroughly evaluated our approach using a two-finger gripper mounted to a robotic arm with six degrees of freedom.","The results show that our agent outperforms two baseline approaches in terms of object assembly compactness, thereby reducing the needed space to place the objects according to the given neighbor constraints.","On average, our approach reduces the distances between all placed objects by at least 60%, with fewer collisions at the same compactness compared to both baselines."],"url":"http://arxiv.org/abs/2404.10632v1","category":"cs.RO"}
{"created":"2024-04-16 15:02:46","title":"HLAT: High-quality Large Language Model Pre-trained on AWS Trainium","abstract":"Getting large language models (LLMs) to perform well on the downstream tasks requires pre-training over trillions of tokens. This typically demands a large number of powerful computational devices in addition to a stable distributed training framework to accelerate the training. The growing number of applications leveraging AI/ML had led to a scarcity of the expensive conventional accelerators (such as GPUs), which begs the need for the alternative specialized-accelerators that are scalable and cost-efficient. AWS Trainium is the second-generation machine learning accelerator that has been purposely built for training large deep learning models. Its corresponding instance, Amazon EC2 trn1, is an alternative to GPU instances for LLM training. However, training LLMs with billions of parameters on trn1 is challenging due to its relatively nascent software ecosystem. In this paper, we showcase HLAT: a 7 billion parameter decoder-only LLM pre-trained using trn1 instances over 1.8 trillion tokens. The performance of HLAT is benchmarked against popular open source baseline models including LLaMA and OpenLLaMA, which have been trained on NVIDIA GPUs and Google TPUs, respectively. On various evaluation tasks, we show that HLAT achieves model quality on par with the baselines. We also share the best practice of using the Neuron Distributed Training Library (NDTL), a customized distributed training library for AWS Trainium to achieve efficient training. Our work demonstrates that AWS Trainium powered by the NDTL is able to successfully pre-train state-of-the-art LLM models with high performance and cost-effectiveness.","sentences":["Getting large language models (LLMs) to perform well on the downstream tasks requires pre-training over trillions of tokens.","This typically demands a large number of powerful computational devices in addition to a stable distributed training framework to accelerate the training.","The growing number of applications leveraging AI/ML had led to a scarcity of the expensive conventional accelerators (such as GPUs), which begs the need for the alternative specialized-accelerators that are scalable and cost-efficient.","AWS Trainium is the second-generation machine learning accelerator that has been purposely built for training large deep learning models.","Its corresponding instance, Amazon EC2 trn1, is an alternative to GPU instances for LLM training.","However, training LLMs with billions of parameters on trn1 is challenging due to its relatively nascent software ecosystem.","In this paper, we showcase HLAT: a 7 billion parameter decoder-only LLM pre-trained using trn1 instances over 1.8 trillion tokens.","The performance of HLAT is benchmarked against popular open source baseline models including LLaMA and OpenLLaMA, which have been trained on NVIDIA GPUs and Google TPUs, respectively.","On various evaluation tasks, we show that HLAT achieves model quality on par with the baselines.","We also share the best practice of using the Neuron Distributed Training Library (NDTL), a customized distributed training library for AWS Trainium to achieve efficient training.","Our work demonstrates that AWS Trainium powered by the NDTL is able to successfully pre-train state-of-the-art LLM models with high performance and cost-effectiveness."],"url":"http://arxiv.org/abs/2404.10630v1","category":"cs.CL"}
{"created":"2024-04-16 14:55:31","title":"Noncommutative black hole in de Rham-Gabadadze-Tolley like massive gravity","abstract":"We examine the behavior of non-commutative Schwarzschild black holes in the context of massive gravity. According to the investigation, corresponding to a minimal mass, the black hole can have two horizons, one horizon, or no horizon at all. Our results imply the existence of a stable black hole remnant, whose mass can be uniquely calculated in terms of the non-commutative parameter $\\theta$ and gravity mass $m$. Thermodynamic features such as heat capacity and Hawking temperature are studied. We also examine a scalar linear perturbation on the black hole. Quasinormal frequencies are computed via Wentzel-Kramers-Brillouin(WKB) method with Pade improvement. All quasinormal frequencies considered in this work have a negative imaginary part. In the eikonal limit, we investigate the angular velocity and the Lyapunov exponent as a function of $M/\\sqrt{\\theta}$. Additionally, we explore the black hole's shadow across various model parameters. Our findings indicate that non-commutativity leads to a reduction in the black hole's shadow, with this effect exhibiting a nonlinear relationship. Furthermore, we observe that the inclusion of a massive graviton in the theory results in an increase in the black hole's shadow radius, particularly at greater observer distances.","sentences":["We examine the behavior of non-commutative Schwarzschild black holes in the context of massive gravity.","According to the investigation, corresponding to a minimal mass, the black hole can have two horizons, one horizon, or no horizon at all.","Our results imply the existence of a stable black hole remnant, whose mass can be uniquely calculated in terms of the non-commutative parameter $\\theta$ and gravity mass $m$. Thermodynamic features such as heat capacity and Hawking temperature are studied.","We also examine a scalar linear perturbation on the black hole.","Quasinormal frequencies are computed via Wentzel-Kramers-Brillouin(WKB) method with Pade improvement.","All quasinormal frequencies considered in this work have a negative imaginary part.","In the eikonal limit, we investigate the angular velocity and the Lyapunov exponent as a function of $M/\\sqrt{\\theta}$. Additionally, we explore the black hole's shadow across various model parameters.","Our findings indicate that non-commutativity leads to a reduction in the black hole's shadow, with this effect exhibiting a nonlinear relationship.","Furthermore, we observe that the inclusion of a massive graviton in the theory results in an increase in the black hole's shadow radius, particularly at greater observer distances."],"url":"http://arxiv.org/abs/2404.10627v1","category":"gr-qc"}
{"created":"2024-04-16 14:48:40","title":"Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks","abstract":"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes.","sentences":["NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety.","However, rendering with Neural Radiance Fields poses challenges for 3D applications:","First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets.","Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games.","3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates.","In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS.","By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time.","Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes."],"url":"http://arxiv.org/abs/2404.10625v1","category":"cs.CV"}
{"created":"2024-04-16 14:46:04","title":"Quantum algorithm for copula-based risk aggregation using orthogonal series density estimation","abstract":"Quantum Monte Carlo integration (QMCI) provides a quadratic speed-up over its classical counterpart, and its applications have been investigated in various fields, including finance. This paper considers its application to risk aggregation, one of the most important numerical tasks in financial risk management. Risk aggregation combines several risk variables and quantifies the total amount of risk, taking into account the correlation among them. For this task, there exists a useful tool called copula, with which the joint distribution can be generated from marginal distributions with a flexible correlation structure. Classically, the copula-based method utilizes sampling of risk variables. However, this procedure is not directly applicable to the quantum setting, where sampled values are not stored as classical data, and thus no efficient quantum algorithm is known. In this paper, we propose a quantum algorithm for copula-based risk aggregation that is compatible with QMCI. In our algorithm, we first estimate each marginal distribution as a series of orthogonal functions, where the coefficients can be calculated with QMCI. Then, by plugging the marginal distributions into the copula and obtaining the joint distribution, we estimate risk measures using QMCI again. With this algorithm, nearly quadratic quantum speed-up can be obtained for sufficiently smooth marginal distributions.","sentences":["Quantum Monte Carlo integration (QMCI) provides a quadratic speed-up over its classical counterpart, and its applications have been investigated in various fields, including finance.","This paper considers its application to risk aggregation, one of the most important numerical tasks in financial risk management.","Risk aggregation combines several risk variables and quantifies the total amount of risk, taking into account the correlation among them.","For this task, there exists a useful tool called copula, with which the joint distribution can be generated from marginal distributions with a flexible correlation structure.","Classically, the copula-based method utilizes sampling of risk variables.","However, this procedure is not directly applicable to the quantum setting, where sampled values are not stored as classical data, and thus no efficient quantum algorithm is known.","In this paper, we propose a quantum algorithm for copula-based risk aggregation that is compatible with QMCI.","In our algorithm, we first estimate each marginal distribution as a series of orthogonal functions, where the coefficients can be calculated with QMCI.","Then, by plugging the marginal distributions into the copula and obtaining the joint distribution, we estimate risk measures using QMCI again.","With this algorithm, nearly quadratic quantum speed-up can be obtained for sufficiently smooth marginal distributions."],"url":"http://arxiv.org/abs/2404.10624v1","category":"quant-ph"}
{"created":"2024-04-16 14:45:27","title":"Learning Deep Dynamical Systems using Stable Neural ODEs","abstract":"Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS). State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time. We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information. A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed. We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task.","sentences":["Learning complex trajectories from demonstrations in robotic tasks has been effectively addressed through the utilization of Dynamical Systems (DS).","State-of-the-art DS learning methods ensure stability of the generated trajectories; however, they have three shortcomings: a) the DS is assumed to have a single attractor, which limits the diversity of tasks it can achieve, b) state derivative information is assumed to be available in the learning process and c) the state of the DS is assumed to be measurable at inference time.","We propose a class of provably stable latent DS with possibly multiple attractors, that inherit the training methods of Neural Ordinary Differential Equations, thus, dropping the dependency on state derivative information.","A diffeomorphic mapping for the output and a loss that captures time-invariant trajectory similarity are proposed.","We validate the efficacy of our approach through experiments conducted on a public dataset of handwritten shapes and within a simulated object manipulation task."],"url":"http://arxiv.org/abs/2404.10622v1","category":"cs.RO"}
{"created":"2024-04-16 14:44:58","title":"Modulating Hamiltonian Approach to Quantum Many-Body Systems and Crystalline Topological Phases Protected by Generalized Magnetic Translations","abstract":"We discuss the topology of the parameter space of invertible phases with an onsite symmetry $G$, i.e., quantum many-body ground states that have neither fractionalization nor spontaneous breaking of the symmetry. The classification of invertible phases is known to be obtained by counting the connected components in the parameter space of the invertible phases. We consider its generalization -- the deformation classes of the mappings from $n$-dimensional spheres $S^n$ to this parameter space for arbitrary integer $n$. We argue a direct one-to-one correspondence in the framework of lattice models between the non-contractibility of $S^n$ and (i) the classification of invertible phases in $d$ dimensions when $d\\geq n$; or (ii) zero-dimensional invertible Hamiltonians parametrized by $S^{n-d}$ when $d<n$, using an isotropic modulating Hamiltonian approach. Explicitly, we construct the noncontractible spheres of two-dimensional invertible phases, i.e., $n=2$ and $d=2$. We also propose a large class of crystalline topological phases protected by a generalized magnetic translations.","sentences":["We discuss the topology of the parameter space of invertible phases with an onsite symmetry $G$, i.e., quantum many-body ground states that have neither fractionalization nor spontaneous breaking of the symmetry.","The classification of invertible phases is known to be obtained by counting the connected components in the parameter space of the invertible phases.","We consider its generalization -- the deformation classes of the mappings from $n$-dimensional spheres $S^n$ to this parameter space for arbitrary integer $n$. We argue a direct one-to-one correspondence in the framework of lattice models between the non-contractibility of $S^n$ and (i) the classification of invertible phases in $d$ dimensions when $d\\geq n$; or (ii) zero-dimensional invertible Hamiltonians parametrized by $S^{n-d}$ when $d<n$, using an isotropic modulating Hamiltonian approach.","Explicitly, we construct the noncontractible spheres of two-dimensional invertible phases, i.e., $n=2$ and $d=2$. We also propose a large class of crystalline topological phases protected by a generalized magnetic translations."],"url":"http://arxiv.org/abs/2404.10621v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 14:42:49","title":"Private Attribute Inference from Images with Vision-Language Models","abstract":"As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that the increase in models' capabilities has enabled LLMs to make accurate privacy-infringing inferences from previously unseen texts. With the rise of multimodal vision-language models (VLMs), capable of understanding both images and text, a pertinent question is whether such results transfer to the previously unexplored domain of benign images posted online. To investigate the risks associated with the image reasoning capabilities of newly emerging VLMs, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the additional privacy risk posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger adversaries, establishing an imperative for the development of adequate defenses.","sentences":["As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus.","While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that the increase in models' capabilities has enabled LLMs to make accurate privacy-infringing inferences from previously unseen texts.","With the rise of multimodal vision-language models (VLMs), capable of understanding both images and text, a pertinent question is whether such results transfer to the previously unexplored domain of benign images posted online.","To investigate the risks associated with the image reasoning capabilities of newly emerging VLMs, we compile an image dataset with human-annotated labels of the image owner's personal attributes.","In order to understand the additional privacy risk posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans.","On this dataset, we evaluate the inferential capabilities of 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy.","Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger adversaries, establishing an imperative for the development of adequate defenses."],"url":"http://arxiv.org/abs/2404.10618v1","category":"cs.AI"}
{"created":"2024-04-16 14:41:35","title":"One is all you need: Second-order Unification without First-order Variables","abstract":"We consider the fragment of Second-Order unification with the following properties: (i) only one second-order variable allowed, (ii) first-order variables do not occur. We show that Hilbert's 10$^{th}$ problem is reducible to this fragment if the signature contains a binary function symbol and two constants. This generalizes known undecidability results. Furthermore, We show that adding the following restriction: (i) the second-order variable has arity 1, (ii) the signature is finite, and (iii) the problem has \\emph{bounded congruence}, results in a decidable fragment. The latter fragment is related to \\emph{Bounded second-order unification}, i.e. the number of holes is a function of the problem structure.","sentences":["We consider the fragment of Second-Order unification with the following properties: (i) only one second-order variable allowed, (ii) first-order variables do not occur.","We show that Hilbert's 10$^{th}$ problem is reducible to this fragment if the signature contains a binary function symbol and two constants.","This generalizes known undecidability results.","Furthermore, We show that adding the following restriction: (i) the second-order variable has arity 1, (ii) the signature is finite, and (iii) the problem has \\emph{bounded congruence}, results in a decidable fragment.","The latter fragment is related to \\emph{Bounded second-order unification}, i.e. the number of holes is a function of the problem structure."],"url":"http://arxiv.org/abs/2404.10616v1","category":"cs.LO"}
{"created":"2024-04-16 14:40:50","title":"Emergent intelligence of buckling-driven elasto-active structures","abstract":"Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors. Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries. Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging. Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors. Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams. These microbots exert forces that suffice to buckle the beam and set the structure in motion. We first rationalize the physics of the interaction between the beam and the microbots. Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation. The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment.","sentences":["Active systems of self-propelled agents, e.g., birds, fish, and bacteria, can organize their collective motion into myriad autonomous behaviors.","Ubiquitous in nature and across length scales, such phenomena are also amenable to artificial settings, e.g., where brainless self-propelled robots orchestrate their movements into spatio-temportal patterns via the application of external cues or when confined within flexible boundaries.","Very much like their natural counterparts, these approaches typically require many units to initiate collective motion such that controlling the ensuing dynamics is challenging.","Here, we demonstrate a novel yet simple mechanism that leverages nonlinear elasticity to tame near-diffusive motile particles in forming structures capable of directed motion and other emergent intelligent behaviors.","Our elasto-active system comprises two centimeter-sized self-propelled microbots connected with elastic beams.","These microbots exert forces that suffice to buckle the beam and set the structure in motion.","We first rationalize the physics of the interaction between the beam and the microbots.","Then we use reduced order models to predict the interactions of our elasto-active structure with boundaries, e.g., walls and constrictions, and demonstrate how they can exhibit intelligent behaviors such as maze navigation.","The findings are relevant to designing intelligent materials or soft robots capable of autonomous space exploration, adaptation, and interaction with the surrounding environment."],"url":"http://arxiv.org/abs/2404.10614v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:37:53","title":"Shining Light into the Tunnel: Understanding and Classifying Network Traffic of Residential Proxies","abstract":"Emerging in recent years, residential proxies (RESIPs) feature multiple unique characteristics when compared with traditional network proxies (e.g., commercial VPNs), particularly, the deployment in residential networks rather than data center networks, the worldwide distribution in tens of thousands of cities and ISPs, and the large scale of millions of exit nodes. All these factors allow RESIP users to effectively masquerade their traffic flows as ones from authentic residential users, which leads to the increasing adoption of RESIP services, especially in malicious online activities. However, regarding the (malicious) usage of RESIPs (i.e., what traffic is relayed by RESIPs), current understanding turns out to be insufficient. Particularly, previous works on RESIP traffic studied only the maliciousness of web traffic destinations and the suspicious patterns of visiting popular websites. Also, a general methodology is missing regarding capturing large-scale RESIP traffic and analyzing RESIP traffic for security risks. Furthermore, considering many RESIP nodes are found to be located in corporate networks and are deployed without proper authorization from device owners or network administrators, it is becoming increasingly necessary to detect and block RESIP traffic flows, which unfortunately is impeded by the scarcity of realistic RESIP traffic datasets and effective detection methodologies.   To fill in these gaps, multiple novel tools have been designed and implemented in this study, which include a general framework to deploy RESIP nodes and collect RESIP traffic in a distributed manner, a RESIP traffic analyzer to efficiently process RESIP traffic logs and surface out suspicious traffic flows, and multiple machine learning based RESIP traffic classifiers to timely and accurately detect whether a given traffic flow is RESIP traffic or not.","sentences":["Emerging in recent years, residential proxies (RESIPs) feature multiple unique characteristics when compared with traditional network proxies (e.g., commercial VPNs), particularly, the deployment in residential networks rather than data center networks, the worldwide distribution in tens of thousands of cities and ISPs, and the large scale of millions of exit nodes.","All these factors allow RESIP users to effectively masquerade their traffic flows as ones from authentic residential users, which leads to the increasing adoption of RESIP services, especially in malicious online activities.","However, regarding the (malicious) usage of RESIPs (i.e., what traffic is relayed by RESIPs), current understanding turns out to be insufficient.","Particularly, previous works on RESIP traffic studied only the maliciousness of web traffic destinations and the suspicious patterns of visiting popular websites.","Also, a general methodology is missing regarding capturing large-scale RESIP traffic and analyzing RESIP traffic for security risks.","Furthermore, considering many RESIP nodes are found to be located in corporate networks and are deployed without proper authorization from device owners or network administrators, it is becoming increasingly necessary to detect and block RESIP traffic flows, which unfortunately is impeded by the scarcity of realistic RESIP traffic datasets and effective detection methodologies.   ","To fill in these gaps, multiple novel tools have been designed and implemented in this study, which include a general framework to deploy RESIP nodes and collect RESIP traffic in a distributed manner, a RESIP traffic analyzer to efficiently process RESIP traffic logs and surface out suspicious traffic flows, and multiple machine learning based RESIP traffic classifiers to timely and accurately detect whether a given traffic flow is RESIP traffic or not."],"url":"http://arxiv.org/abs/2404.10610v1","category":"cs.CR"}
{"created":"2024-04-16 14:29:05","title":"Stability of planar rarefaction waves in the vanishing dissipation limit of the Navier-Stokes-Fourier system","abstract":"We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system. We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm.","sentences":["We consider the vanishing dissipation limit of the compressible Navier-Stokes-Fourier system, where the initial data approach a profile generating a planar rarefaction wave for the limit Euler system.","We show that the associated weak solutions converge unconditionally to the planar rarefaction wave strongly in the energy norm."],"url":"http://arxiv.org/abs/2404.10604v1","category":"math.AP"}
{"created":"2024-04-16 14:22:58","title":"Hardware-aware training of models with synaptic delays for digital event-driven neuromorphic processors","abstract":"Configurable synaptic delays are a basic feature in many neuromorphic neural network hardware accelerators. However, they have been rarely used in model implementations, despite their promising impact on performance and efficiency in tasks that exhibit complex (temporal) dynamics, as it has been unclear how to optimize them. In this work, we propose a framework to train and deploy, in digital neuromorphic hardware, highly performing spiking neural network models (SNNs) where apart from the synaptic weights, the per-synapse delays are also co-optimized. Leveraging spike-based back-propagation-through-time, the training accounts for both platform constraints, such as synaptic weight precision and the total number of parameters per core, as a function of the network size. In addition, a delay pruning technique is used to reduce memory footprint with a low cost in performance. We evaluate trained models in two neuromorphic digital hardware platforms: Intel Loihi and Imec Seneca. Loihi offers synaptic delay support using the so-called Ring-Buffer hardware structure. Seneca does not provide native hardware support for synaptic delays. A second contribution of this paper is therefore a novel area- and memory-efficient hardware structure for acceleration of synaptic delays, which we have integrated in Seneca. The evaluated benchmark involves several models for solving the SHD (Spiking Heidelberg Digits) classification task, where minimal accuracy degradation during the transition from software to hardware is demonstrated. To our knowledge, this is the first work showcasing how to train and deploy hardware-aware models parameterized with synaptic delays, on multicore neuromorphic hardware accelerators.","sentences":["Configurable synaptic delays are a basic feature in many neuromorphic neural network hardware accelerators.","However, they have been rarely used in model implementations, despite their promising impact on performance and efficiency in tasks that exhibit complex (temporal) dynamics, as it has been unclear how to optimize them.","In this work, we propose a framework to train and deploy, in digital neuromorphic hardware, highly performing spiking neural network models (SNNs) where apart from the synaptic weights, the per-synapse delays are also co-optimized.","Leveraging spike-based back-propagation-through-time, the training accounts for both platform constraints, such as synaptic weight precision and the total number of parameters per core, as a function of the network size.","In addition, a delay pruning technique is used to reduce memory footprint with a low cost in performance.","We evaluate trained models in two neuromorphic digital hardware platforms: Intel Loihi and Imec Seneca.","Loihi offers synaptic delay support using the so-called Ring-Buffer hardware structure.","Seneca does not provide native hardware support for synaptic delays.","A second contribution of this paper is therefore a novel area- and memory-efficient hardware structure for acceleration of synaptic delays, which we have integrated in Seneca.","The evaluated benchmark involves several models for solving the SHD (Spiking Heidelberg Digits) classification task, where minimal accuracy degradation during the transition from software to hardware is demonstrated.","To our knowledge, this is the first work showcasing how to train and deploy hardware-aware models parameterized with synaptic delays, on multicore neuromorphic hardware accelerators."],"url":"http://arxiv.org/abs/2404.10597v1","category":"cs.NE"}
{"created":"2024-04-16 14:20:55","title":"Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases","abstract":"Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of interpretable end-to-end autonomous driving. However, current evaluations of LVLMs primarily focus on the multi-faceted capabilities in common scenarios, lacking quantifiable and automated assessment in autonomous driving contexts, let alone severe road corner cases that even the state-of-the-art autonomous driving perception systems struggle to handle. In this paper, we propose CODA-LM, a novel vision-language benchmark for self-driving, which provides the first automatic and quantitative evaluation of LVLMs for interpretable autonomous driving including general perception, regional perception, and driving suggestions. CODA-LM utilizes the texts to describe the road images, exploiting powerful text-only large language models (LLMs) without image inputs to assess the capabilities of LVLMs in autonomous driving scenarios, which reveals stronger alignment with human preferences than LVLM judges. Experiments demonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot deal with road corner cases well, suggesting that we are still far from a strong LVLM-powered intelligent driving agent, and we hope our CODA-LM can become the catalyst to promote future development.","sentences":["Large Vision-Language Models (LVLMs), due to the remarkable visual reasoning ability to understand images and videos, have received widespread attention in the autonomous driving domain, which significantly advances the development of interpretable end-to-end autonomous driving.","However, current evaluations of LVLMs primarily focus on the multi-faceted capabilities in common scenarios, lacking quantifiable and automated assessment in autonomous driving contexts, let alone severe road corner cases that even the state-of-the-art autonomous driving perception systems struggle to handle.","In this paper, we propose CODA-LM, a novel vision-language benchmark for self-driving, which provides the first automatic and quantitative evaluation of LVLMs for interpretable autonomous driving including general perception, regional perception, and driving suggestions.","CODA-LM utilizes the texts to describe the road images, exploiting powerful text-only large language models (LLMs) without image inputs to assess the capabilities of LVLMs in autonomous driving scenarios, which reveals stronger alignment with human preferences than LVLM judges.","Experiments demonstrate that even the closed-sourced commercial LVLMs like GPT-4V cannot deal with road corner cases well, suggesting that we are still far from a strong LVLM-powered intelligent driving agent, and we hope our CODA-LM can become the catalyst to promote future development."],"url":"http://arxiv.org/abs/2404.10595v1","category":"cs.CV"}
{"created":"2024-04-16 14:14:34","title":"Learning Symbolic Task Representation from a Human-Led Demonstration: A Memory to Store, Retrieve, Consolidate, and Forget Experiences","abstract":"We present a symbolic learning framework inspired by cognitive-like memory functionalities (i.e., storing, retrieving, consolidating and forgetting) to generate task representations to support high-level task planning and knowledge bootstrapping. We address a scenario involving a non-expert human, who performs a single task demonstration, and a robot, which online learns structured knowledge to re-execute the task based on experiences, i.e., observations. We consider a one-shot learning process based on non-annotated data to store an intelligible representation of the task, which can be refined through interaction, e.g., via verbal or visual communication. Our general-purpose framework relies on fuzzy Description Logic, which has been used to extend the previously developed Scene Identification and Tagging algorithm. In this paper, we exploit such an algorithm to implement cognitive-like memory functionalities employing scores that rank memorised observations over time based on simple heuristics. Our main contribution is the formalisation of a framework that can be used to systematically investigate different heuristics for bootstrapping hierarchical knowledge representations based on robot observations. Through an illustrative assembly task scenario, the paper presents the performance of our framework to discuss its benefits and limitations.","sentences":["We present a symbolic learning framework inspired by cognitive-like memory functionalities (i.e., storing, retrieving, consolidating and forgetting) to generate task representations to support high-level task planning and knowledge bootstrapping.","We address a scenario involving a non-expert human, who performs a single task demonstration, and a robot, which online learns structured knowledge to re-execute the task based on experiences, i.e., observations.","We consider a one-shot learning process based on non-annotated data to store an intelligible representation of the task, which can be refined through interaction, e.g., via verbal or visual communication.","Our general-purpose framework relies on fuzzy Description Logic, which has been used to extend the previously developed Scene Identification and Tagging algorithm.","In this paper, we exploit such an algorithm to implement cognitive-like memory functionalities employing scores that rank memorised observations over time based on simple heuristics.","Our main contribution is the formalisation of a framework that can be used to systematically investigate different heuristics for bootstrapping hierarchical knowledge representations based on robot observations.","Through an illustrative assembly task scenario, the paper presents the performance of our framework to discuss its benefits and limitations."],"url":"http://arxiv.org/abs/2404.10591v1","category":"cs.RO"}
{"created":"2024-04-16 14:14:20","title":"Ray-Tracing Calibration from Channel Sounding Measurements in a Millimeter-Wave Industrial Scenario","abstract":"New-generation communication and sensing systems are gaining strong interest in the context of Industry 4.0 e.g., related to mapping techniques, environmental sensing, automation or hyper-vision. The radio propagation in confined, cluttered and heavily metalized factory environments is a critical challenge; thus an evaluation by accurate propagation channel models is necessary. Site-specific channel emulation can be obtained from Ray-tracing (RT); but RT validation for factory environments is still an on-going work. For this purpose, a measurement campaign was performed in a machine room with many metallic objects and machines, using a mmWave channel sounder. Wideband channel responses were collected and compared to RT simulations. The RT prediction tool was calibrated to minimize the error observed on some large scale statistics, thus reaching a very good agreement between the simulation and the measurement. Average error in received power, delay spread and azimuth spread is below 1.5 dB, 5 ns and 2{\\deg} respectively.","sentences":["New-generation communication and sensing systems are gaining strong interest in the context of Industry 4.0 e.g., related to mapping techniques, environmental sensing, automation or hyper-vision.","The radio propagation in confined, cluttered and heavily metalized factory environments is a critical challenge; thus an evaluation by accurate propagation channel models is necessary.","Site-specific channel emulation can be obtained from Ray-tracing (RT); but RT validation for factory environments is still an on-going work.","For this purpose, a measurement campaign was performed in a machine room with many metallic objects and machines, using a mmWave channel sounder.","Wideband channel responses were collected and compared to RT simulations.","The RT prediction tool was calibrated to minimize the error observed on some large scale statistics, thus reaching a very good agreement between the simulation and the measurement.","Average error in received power, delay spread and azimuth spread is below 1.5 dB, 5 ns and 2{\\deg} respectively."],"url":"http://arxiv.org/abs/2404.10590v1","category":"eess.SP"}
{"created":"2024-04-16 14:13:44","title":"Do Counterfactual Examples Complicate Adversarial Training?","abstract":"We leverage diffusion models to study the robustness-performance tradeoff of robust classifiers. Our approach introduces a simple, pretrained diffusion method to generate low-norm counterfactual examples (CEs): semantically altered data which results in different true class membership. We report that the confidence and accuracy of robust models on their clean training data are associated with the proximity of the data to their CEs. Moreover, robust models perform very poorly when evaluated on the CEs directly, as they become increasingly invariant to the low-norm, semantic changes brought by CEs. The results indicate a significant overlap between non-robust and semantic features, countering the common assumption that non-robust features are not interpretable.","sentences":["We leverage diffusion models to study the robustness-performance tradeoff of robust classifiers.","Our approach introduces a simple, pretrained diffusion method to generate low-norm counterfactual examples (CEs): semantically altered data which results in different true class membership.","We report that the confidence and accuracy of robust models on their clean training data are associated with the proximity of the data to their CEs.","Moreover, robust models perform very poorly when evaluated on the CEs directly, as they become increasingly invariant to the low-norm, semantic changes brought by CEs.","The results indicate a significant overlap between non-robust and semantic features, countering the common assumption that non-robust features are not interpretable."],"url":"http://arxiv.org/abs/2404.10588v1","category":"cs.LG"}
{"created":"2024-04-16 14:12:12","title":"Semi-device-independent quantum random number generator with a broadband squeezed state of light","abstract":"Random numbers are a basic ingredient of simulation algorithms and cryptography, and play a significant part in computer simulation and information processing. One prominent feature of a squeezed light is its lower fluctuation and more randomness in a pair","sentences":["Random numbers are a basic ingredient of simulation algorithms and cryptography, and play a significant part in computer simulation and information processing.","One prominent feature of a squeezed light is its lower fluctuation and more randomness in a pair"],"url":"http://arxiv.org/abs/2404.10586v1","category":"quant-ph"}
{"created":"2024-04-16 14:10:42","title":"ReWiTe: Realistic Wide-angle and Telephoto Dual Camera Fusion Dataset via Beam Splitter Camera Rig","abstract":"The fusion of images from dual camera systems featuring a wide-angle and a telephoto camera has become a hotspot problem recently. By integrating simultaneously captured wide-angle and telephoto images from these systems, the resulting fused image achieves a wide field of view (FOV) coupled with high-definition quality. Existing approaches are mostly deep learning methods, and predominantly rely on supervised learning, where the training dataset plays a pivotal role. However, current datasets typically adopt a data synthesis approach generate input pairs of wide-angle and telephoto images alongside ground-truth images. Notably, the wide-angle inputs are synthesized rather than captured using real wide-angle cameras, and the ground-truth image is captured by wide-angle camera whose quality is substantially lower than that of input telephoto images captured by telephoto cameras. To address these limitations, we introduce a novel hardware setup utilizing a beam splitter to simultaneously capture three images, i.e. input pairs and ground-truth images, from two authentic cellphones equipped with wide-angle and telephoto dual cameras. Specifically, the wide-angle and telephoto images captured by cellphone 2 serve as the input pair, while the telephoto image captured by cellphone 1, which is calibrated to match the optical path of the wide-angle image from cellphone 2, serves as the ground-truth image, maintaining quality on par with the input telephoto image. Experiments validate the efficacy of our newly introduced dataset, named ReWiTe, significantly enhances the performance of various existing methods for real-world wide-angle and telephoto dual image fusion tasks.","sentences":["The fusion of images from dual camera systems featuring a wide-angle and a telephoto camera has become a hotspot problem recently.","By integrating simultaneously captured wide-angle and telephoto images from these systems, the resulting fused image achieves a wide field of view (FOV) coupled with high-definition quality.","Existing approaches are mostly deep learning methods, and predominantly rely on supervised learning, where the training dataset plays a pivotal role.","However, current datasets typically adopt a data synthesis approach generate input pairs of wide-angle and telephoto images alongside ground-truth images.","Notably, the wide-angle inputs are synthesized rather than captured using real wide-angle cameras, and the ground-truth image is captured by wide-angle camera whose quality is substantially lower than that of input telephoto images captured by telephoto cameras.","To address these limitations, we introduce a novel hardware setup utilizing a beam splitter to simultaneously capture three images, i.e. input pairs and ground-truth images, from two authentic cellphones equipped with wide-angle and telephoto dual cameras.","Specifically, the wide-angle and telephoto images captured by cellphone 2 serve as the input pair, while the telephoto image captured by cellphone 1, which is calibrated to match the optical path of the wide-angle image from cellphone 2, serves as the ground-truth image, maintaining quality on par with the input telephoto image.","Experiments validate the efficacy of our newly introduced dataset, named ReWiTe, significantly enhances the performance of various existing methods for real-world wide-angle and telephoto dual image fusion tasks."],"url":"http://arxiv.org/abs/2404.10584v1","category":"cs.CV"}
{"created":"2024-04-16 14:06:40","title":"Extended Automatic Repeat Request For Integrated Sensing And Communication Networks","abstract":"6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness. In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks. Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks. We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result. This technique improves the sensing quality via retransmissions using adaptive parameters. We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks.","sentences":["6G wireless networks will integrate communication, computing, localization, and sensing capabilities while meeting the needs of high reliability and trustworthiness.","In this paper, we develop similar techniques as those used by communication modules of previous generations for the sensing functionality of 6G networks.","Specifically, this paper introduces the concept of extended automatic repeat request (e-ARQ) for integrated sensing and communications (ISAC) networks.","We focus on multi-static sensing schemes, in which the nodes receiving the reflected sensing signals provide the transmitting nodes with configurable levels of feedback about the sensing result.","This technique improves the sensing quality via retransmissions using adaptive parameters.","We show that our proposed e-ARQ boosts the sensing quality in terms of detection accuracy and provides a sense of adaptability for applications supported by ISAC networks."],"url":"http://arxiv.org/abs/2404.10583v1","category":"eess.SP"}
{"created":"2024-04-16 14:06:27","title":"Roles of band gap and Kane electronic dispersion in the THz-frequency nonlinear optical response in HgCdTe","abstract":"Materials with linear electronic dispersion often feature high carrier mobilities and unusually strong nonlinear optical interactions. In this work, we investigate the THz nonlinear dynamics of one such material, HgCdTe, with an electronic band dispersion heavily dependent on both temperature and stoichiometry. We show how the band gap, carrier concentration and band shape together determine the nonlinear response of the system. At low temperatures, carrier generation from Zener tunneling dominates the nonlinear response with a reduction in the overall transmission. At room temperature, quasi-ballistic electronic dynamics drive the largest observed nonlinear optical interactions, leading to a transmission increase. Our results demonstrate the sensitivity of these nonlinear optical properties of narrow-gap materials to small changes in the electronic dispersion and carrier concentration.","sentences":["Materials with linear electronic dispersion often feature high carrier mobilities and unusually strong nonlinear optical interactions.","In this work, we investigate the THz nonlinear dynamics of one such material, HgCdTe, with an electronic band dispersion heavily dependent on both temperature and stoichiometry.","We show how the band gap, carrier concentration and band shape together determine the nonlinear response of the system.","At low temperatures, carrier generation from Zener tunneling dominates the nonlinear response with a reduction in the overall transmission.","At room temperature, quasi-ballistic electronic dynamics drive the largest observed nonlinear optical interactions, leading to a transmission increase.","Our results demonstrate the sensitivity of these nonlinear optical properties of narrow-gap materials to small changes in the electronic dispersion and carrier concentration."],"url":"http://arxiv.org/abs/2404.10582v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 14:06:04","title":"Self-diffusion is temperature independent on active membranes","abstract":"Molecular transport maintains cellular structures and functions. For example, lipid and protein diffusion sculpts the dynamic shapes and structures on the cell membrane that perform essential cellular functions, such as cell signaling. Temperature variations in thermal equilibrium rapidly change molecular transport properties. The coefficient of lipid self-diffusion increases exponentially with temperature in thermal equilibrium, for example. Hence, in the noisy cellular environment, where temperatures can fluctuate widely due to local heat generation, maintaining cellular homeostasis through molecular transport is hard in thermal equilibrium. In this paper, using both molecular and lattice-based modeling of membrane transport, we show that the presence of active transport originating from the cell's cytoskeleton can make the self-diffusion of the molecules on the membrane robust to temperature fluctuations. The resultant temperature-independence of self-diffusion keeps the precision of cellular signaling invariant over a broad range of ambient temperatures, allowing cells to make robust decisions. We have also found that the Kawasaki algorithm, the widely used model of lipid transport on lattices, predicts incorrect temperature dependence of lipid self-diffusion in equilibrium. We propose a new algorithm that correctly captures the equilibrium properties of lipid self-diffusion and reproduces experimental observations.","sentences":["Molecular transport maintains cellular structures and functions.","For example, lipid and protein diffusion sculpts the dynamic shapes and structures on the cell membrane that perform essential cellular functions, such as cell signaling.","Temperature variations in thermal equilibrium rapidly change molecular transport properties.","The coefficient of lipid self-diffusion increases exponentially with temperature in thermal equilibrium, for example.","Hence, in the noisy cellular environment, where temperatures can fluctuate widely due to local heat generation, maintaining cellular homeostasis through molecular transport is hard in thermal equilibrium.","In this paper, using both molecular and lattice-based modeling of membrane transport, we show that the presence of active transport originating from the cell's cytoskeleton can make the self-diffusion of the molecules on the membrane robust to temperature fluctuations.","The resultant temperature-independence of self-diffusion keeps the precision of cellular signaling invariant over a broad range of ambient temperatures, allowing cells to make robust decisions.","We have also found that the Kawasaki algorithm, the widely used model of lipid transport on lattices, predicts incorrect temperature dependence of lipid self-diffusion in equilibrium.","We propose a new algorithm that correctly captures the equilibrium properties of lipid self-diffusion and reproduces experimental observations."],"url":"http://arxiv.org/abs/2404.10581v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:04:46","title":"The application of Augmented Reality (AR) in Remote Work and Education","abstract":"With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods. Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects. This paper delves into the application potential and actual effects of AR technology in remote work and education. Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology. Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models. Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications. Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields.","sentences":["With the rapid advancement of technology, Augmented Reality (AR) technology, known for its ability to deeply integrate virtual information with the real world, is gradually transforming traditional work modes and teaching methods.","Particularly in the realms of remote work and online education, AR technology demonstrates a broad spectrum of application prospects.","This paper delves into the application potential and actual effects of AR technology in remote work and education.","Through a systematic literature review, this study outlines the key features, advantages, and challenges of AR technology.","Based on theoretical analysis, it discusses the scientific basis and technical support that AR technology provides for enhancing remote work efficiency and promoting innovation in educational teaching models.","Additionally, by designing an empirical research plan and analyzing experimental data, this article reveals the specific performance and influencing factors of AR technology in practical applications.","Finally, based on the results of the experiments, this research summarizes the application value of AR technology in remote work and education, looks forward to its future development trends, and proposes forward-looking research directions and strategic suggestions, offering empirical foundation and theoretical guidance for further promoting the in-depth application of AR technology in related fields."],"url":"http://arxiv.org/abs/2404.10579v1","category":"cs.AI"}
{"created":"2024-04-16 13:55:58","title":"Quantum phase transition and critical behavior between the gapless topological phases","abstract":"The phase transition between gapped topological phases represents a class of unconventional criticality beyond the Landau paradigm. However, recent research has shifted attention to topological phases without a bulk gap, where the phase transitions between them are still elusive. In this work, based on large-scale density matrix renormalization group techniques, we investigate the critical behaviors of the extended quantum XXZ model obtained by the Kennedy-Tasaki transformation. Using fidelity susceptibility as a diagnostic, we obtain a complete phase diagram, which includes both topological nontrivial and trivial gapless phases. Furthermore, as the XXZ-type anisotropy parameter $\\Delta$ varies, both the critical points $h_c$ and correlation length exponent $\\nu$ remain the same as in the $\\Delta=0$ case, characterized by $c=3/2$ (Ising + free boson) conformal field theory. Our results indicate that fidelity susceptibility can effectively detect and reveal a stable unconventional critical line between the topologically distinct gapless phases for general $\\Delta$. This work serves as a valuable reference for further research on phase transitions within the gapless topological phase of matter.","sentences":["The phase transition between gapped topological phases represents a class of unconventional criticality beyond the Landau paradigm.","However, recent research has shifted attention to topological phases without a bulk gap, where the phase transitions between them are still elusive.","In this work, based on large-scale density matrix renormalization group techniques, we investigate the critical behaviors of the extended quantum XXZ model obtained by the Kennedy-Tasaki transformation.","Using fidelity susceptibility as a diagnostic, we obtain a complete phase diagram, which includes both topological nontrivial and trivial gapless phases.","Furthermore, as the XXZ-type anisotropy parameter $\\Delta$ varies, both the critical points $h_c$ and correlation length exponent $\\nu$ remain the same as in the $\\Delta=0$ case, characterized by $c=3/2$ (Ising + free boson) conformal field theory.","Our results indicate that fidelity susceptibility can effectively detect and reveal a stable unconventional critical line between the topologically distinct gapless phases for general $\\Delta$. This work serves as a valuable reference for further research on phase transitions within the gapless topological phase of matter."],"url":"http://arxiv.org/abs/2404.10576v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 13:53:58","title":"EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence","abstract":"A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100.","sentences":["A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data.","These negative samples often follow a softmax distribution which are dynamically updated during the training process.","However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function.","In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$).","We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization.","We prove that EMC$^2$ finds an $\\mathcal{O}(1/\\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations.","Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost.","Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms.","We report the results for pre-training image encoders on STL-10 and Imagenet-100."],"url":"http://arxiv.org/abs/2404.10575v1","category":"cs.LG"}
{"created":"2024-04-16 13:52:00","title":"Uncertainty-guided Open-Set Source-Free Unsupervised Domain Adaptation with Target-private Class Segregation","abstract":"Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data. Moreover, UDA approaches commonly assume that source and target domains share the same labels space. Yet, these two assumptions are hardly satisfied in real-world scenarios. This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped. We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes. Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module. Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance. Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery.","sentences":["Standard Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target but usually requires simultaneous access to both source and target data.","Moreover, UDA approaches commonly assume that source and target domains share the same labels space.","Yet, these two assumptions are hardly satisfied in real-world scenarios.","This paper considers the more challenging Source-Free Open-set Domain Adaptation (SF-OSDA) setting, where both assumptions are dropped.","We propose a novel approach for SF-OSDA that exploits the granularity of target-private categories by segregating their samples into multiple unknown classes.","Starting from an initial clustering-based assignment, our method progressively improves the segregation of target-private samples by refining their pseudo-labels with the guide of an uncertainty-based sample selection module.","Additionally, we propose a novel contrastive loss, named NL-InfoNCELoss, that, integrating negative learning into self-supervised contrastive learning, enhances the model robustness to noisy pseudo-labels.","Extensive experiments on benchmark datasets demonstrate the superiority of the proposed method over existing approaches, establishing new state-of-the-art performance.","Notably, additional analyses show that our method is able to learn the underlying semantics of novel classes, opening the possibility to perform novel class discovery."],"url":"http://arxiv.org/abs/2404.10574v1","category":"cs.CV"}
{"created":"2024-04-16 13:51:43","title":"AAVDiff: Experimental Validation of Enhanced Viability and Diversity in Recombinant Adeno-Associated Virus (AAV) Capsids through Diffusion Generation","abstract":"Recombinant adeno-associated virus (rAAV) vectors have revolutionized gene therapy, but their broad tropism and suboptimal transduction efficiency limit their clinical applications. To overcome these limitations, researchers have focused on designing and screening capsid libraries to identify improved vectors. However, the large sequence space and limited resources present challenges in identifying viable capsid variants. In this study, we propose an end-to-end diffusion model to generate capsid sequences with enhanced viability. Using publicly available AAV2 data, we generated 38,000 diverse AAV2 viral protein (VP) sequences, and evaluated 8,000 for viral selection. The results attested the superiority of our model compared to traditional methods. Additionally, in the absence of AAV9 capsid data, apart from one wild-type sequence, we used the same model to directly generate a number of viable sequences with up to 9 mutations. we transferred the remaining 30,000 samples to the AAV9 domain. Furthermore, we conducted mutagenesis on AAV9 VP hypervariable regions VI and V, contributing to the continuous improvement of the AAV9 VP sequence. This research represents a significant advancement in the design and functional validation of rAAV vectors, offering innovative solutions to enhance specificity and transduction efficiency in gene therapy applications.","sentences":["Recombinant adeno-associated virus (rAAV) vectors have revolutionized gene therapy, but their broad tropism and suboptimal transduction efficiency limit their clinical applications.","To overcome these limitations, researchers have focused on designing and screening capsid libraries to identify improved vectors.","However, the large sequence space and limited resources present challenges in identifying viable capsid variants.","In this study, we propose an end-to-end diffusion model to generate capsid sequences with enhanced viability.","Using publicly available AAV2 data, we generated 38,000 diverse AAV2 viral protein (VP) sequences, and evaluated 8,000 for viral selection.","The results attested the superiority of our model compared to traditional methods.","Additionally, in the absence of AAV9 capsid data, apart from one wild-type sequence, we used the same model to directly generate a number of viable sequences with up to 9 mutations.","we transferred the remaining 30,000 samples to the AAV9 domain.","Furthermore, we conducted mutagenesis on AAV9 VP hypervariable regions VI and V, contributing to the continuous improvement of the AAV9 VP sequence.","This research represents a significant advancement in the design and functional validation of rAAV vectors, offering innovative solutions to enhance specificity and transduction efficiency in gene therapy applications."],"url":"http://arxiv.org/abs/2404.10573v1","category":"cs.AI"}
{"created":"2024-04-16 13:47:05","title":"More variables or more bins? Impact on the EFT interpretation of Drell-Yan measurements","abstract":"We generalize previous studies on constraining operators of the Standard Model Effective Field Theory using Drell-Yan (DY) measurements to include at the same time all relevant operators and uncertainties. It has been shown that fully differential measurements (triple differential for neutral and double differential for charged) are more sensitive to EFT effects. Nevertheless, due to the finite statistics, the fully differential measurements sacrifice some statistical power on the shape (less invariant mass or transverse momentum bins) in favour of more kinematic variables. We show that when the observables are particularly sensitive to the shape of the distributions, such as the invariant mass of the two leptons in neutral DY, the single differential measurement with more bins, may be as sensitive as the fully differential one, at least for specific EFT operators. This suggests to always supplement fully differential analyses with projections into the relevant distributions evaluated with finer bins.","sentences":["We generalize previous studies on constraining operators of the Standard Model Effective Field Theory using Drell-Yan (DY) measurements to include at the same time all relevant operators and uncertainties.","It has been shown that fully differential measurements (triple differential for neutral and double differential for charged) are more sensitive to EFT effects.","Nevertheless, due to the finite statistics, the fully differential measurements sacrifice some statistical power on the shape (less invariant mass or transverse momentum bins) in favour of more kinematic variables.","We show that when the observables are particularly sensitive to the shape of the distributions, such as the invariant mass of the two leptons in neutral DY, the single differential measurement with more bins, may be as sensitive as the fully differential one, at least for specific EFT operators.","This suggests to always supplement fully differential analyses with projections into the relevant distributions evaluated with finer bins."],"url":"http://arxiv.org/abs/2404.10569v1","category":"hep-ph"}
{"created":"2024-04-16 13:40:22","title":"Exploring Homological Properties of Independent Complexes of Kneser Graphs","abstract":"We discuss the topological properties of the independence complex of Kneser graphs, Ind(KG$(n, k))$, with $n\\geq 3$ and $k\\geq 1$. By identifying one kind of maximal simplices through projective planes, we obtain homology generators for the $6$-dimensional homology of the complex Ind(KG$(3, k))$. Using cross-polytopal generators, we provide lower bounds for the rank of $p$-dimensional homology of the complex Ind(KG$(n, k))$ where $p=1/2\\cdot {2n+k\\choose 2n}$.   Denote $\\mathcal{F}_n^{[m]}$ to be the collection of $n$-subsets of $[m]$ equipped with the symmetric difference metric. We prove that if $\\ell$ is the minimal integer with the $q$th dimensional reduced homology $\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}^{[\\ell]}_n; 2(n-1)))$ being non-trivial, then $$\\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[m]}; 2(n-1)))\\geq \\sum_{i=\\ell}^m{i-2\\choose \\ell-2}\\cdot \\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[\\ell]}; 2(n-1))). $$ Since the independence complex Ind(KG$(n, k))$ and the Vietoris-Rips complex $\\mathcal{VR}(\\mathcal{F}^{[2n+k]}_n; 2(n-1))$ are the same, we obtain a homology propagation result in the setting of independence complexes of Kneser graphs. Connectivity of these complexes is also discussed in this paper.","sentences":["We discuss the topological properties of the independence complex of Kneser graphs, Ind(KG$(n, k))$, with $n\\geq 3$ and $k\\geq 1$. By identifying one kind of maximal simplices through projective planes, we obtain homology generators for the $6$-dimensional homology of the complex Ind(KG$(3, k))$. Using cross-polytopal generators, we provide lower bounds for the rank of $p$-dimensional homology of the complex Ind(KG$(n, k))$ where $p=1/2\\cdot {2n+k\\choose 2n}$.   Denote $\\mathcal{F}_n^{[m]}$ to be the collection of $n$-subsets of $[m]$ equipped with the symmetric difference metric.","We prove that if $\\ell$ is the minimal integer with the $q$th dimensional reduced homology $\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}^{[\\ell]}_n; 2(n-1)))$ being non-trivial, then $$\\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[m]}; 2(n-1)))\\geq \\sum_{i=\\ell}^m{i-2\\choose \\ell-2}\\cdot \\text{rank} (\\tilde{H}_q(\\mathcal{VR}(\\mathcal{F}_n^{[\\ell]}; 2(n-1))).","$$ Since the independence complex Ind(KG$(n, k))$ and the Vietoris-Rips complex $\\mathcal{VR}(\\mathcal{F}^{[2n+k]}_n; 2(n-1))$ are the same, we obtain a homology propagation result in the setting of independence complexes of Kneser graphs.","Connectivity of these complexes is also discussed in this paper."],"url":"http://arxiv.org/abs/2404.10566v1","category":"math.CO"}
{"created":"2024-04-16 13:39:25","title":"Photonic Neuromorphic Accelerators for Event-Based Imaging Flow Cytometry","abstract":"In this work, we present experimental results of a high-speed label-free imaging cytometry system that seamlessly merges the high-capturing rate and data sparsity of an event-based CMOS camera with lightweight photonic neuromorphic processing. This combination offers high classification accuracy and a massive reduction in the number of trainable parameters of the digital machine-learning back-end. The photonic neuromorphic accelerator is based on a hardware-friendly passive optical spectrum slicing technique that is able to extract meaningful features from the generated spike-trains. The experimental scenario comprises the discrimination of artificial polymethyl methacrylate calibrated beads, having different diameters, flowing at a mean speed of 0.01m/sec. Classification accuracy, using only lightweight, digital machine-learning schemes has topped at 98.2%. On the other hand, by experimentally pre-processing the raw spike data through the proposed photonic neuromorphic spectrum slicer we achieved an accuracy of 98.6%. This performance was accompanied by a reduction in the number of trainable parameters at the classification back-end by a factor ranging from 8 to 22, depending on the configuration of the digital neural network.","sentences":["In this work, we present experimental results of a high-speed label-free imaging cytometry system that seamlessly merges the high-capturing rate and data sparsity of an event-based CMOS camera with lightweight photonic neuromorphic processing.","This combination offers high classification accuracy and a massive reduction in the number of trainable parameters of the digital machine-learning back-end.","The photonic neuromorphic accelerator is based on a hardware-friendly passive optical spectrum slicing technique that is able to extract meaningful features from the generated spike-trains.","The experimental scenario comprises the discrimination of artificial polymethyl methacrylate calibrated beads, having different diameters, flowing at a mean speed of 0.01m/sec.","Classification accuracy, using only lightweight, digital machine-learning schemes has topped at 98.2%.","On the other hand, by experimentally pre-processing the raw spike data through the proposed photonic neuromorphic spectrum slicer we achieved an accuracy of 98.6%.","This performance was accompanied by a reduction in the number of trainable parameters at the classification back-end by a factor ranging from 8 to 22, depending on the configuration of the digital neural network."],"url":"http://arxiv.org/abs/2404.10564v1","category":"physics.optics"}
{"created":"2024-04-16 13:39:14","title":"A Mathematica program for numerically computing real and complex critical points in 4-dimensional Lorentzian spinfoam amplitude","abstract":"This work develops a comprehensive algorithm and a Mathematica program to construct boundary data and compute real and complex critical points in spinfoam amplitudes. Our approach covers both spacelike tetrahedra and triangles in the EPRL model and timelike tetrahedra and triangles in the Conrady-Hnybida extension, aiming at addressing a wide range of physical scenarios such as cosmology and black holes. Starting with a single 4-simplex, we explain how to numerically construct boundary data and corresponding real critical points from any nondegenerate 4-simplex geometry. Extending this to the simplicial complex, we demonstrate the algorithm for constructing boundary data and critical points using examples with two 4-simplices sharing an internal tetrahedron. By revisiting the $\\Delta_3$ triangulation with curved geometry, we demonstrate the numerical computation of the real critical point corresponding to the flat geometry and the deformation to the complex critical points. Additionally, the program evaluates the spinfoam action at the critical points and compare to the Regge action.","sentences":["This work develops a comprehensive algorithm and a Mathematica program to construct boundary data and compute real and complex critical points in spinfoam amplitudes.","Our approach covers both spacelike tetrahedra and triangles in the EPRL model and timelike tetrahedra and triangles in the Conrady-Hnybida extension, aiming at addressing a wide range of physical scenarios such as cosmology and black holes.","Starting with a single 4-simplex, we explain how to numerically construct boundary data and corresponding real critical points from any nondegenerate 4-simplex geometry.","Extending this to the simplicial complex, we demonstrate the algorithm for constructing boundary data and critical points using examples with two 4-simplices sharing an internal tetrahedron.","By revisiting the $\\Delta_3$ triangulation with curved geometry, we demonstrate the numerical computation of the real critical point corresponding to the flat geometry and the deformation to the complex critical points.","Additionally, the program evaluates the spinfoam action at the critical points and compare to the Regge action."],"url":"http://arxiv.org/abs/2404.10563v1","category":"gr-qc"}
{"created":"2024-04-16 13:37:11","title":"On the Lorenzoni-Magri hierarchy of hydrodynamic type","abstract":"In 2005 Lorenzoni and Magri showed that a hydrodynamic-type hierarchy determined by the powers of a type (1,1) tensor field (on a smooth manifold) with vanishing Nijenhuis torsion can be deformed to a more general hierarchy, with the help of a chain of conservation laws of the new hierarchy. We review this construction. The (1,1) tensor fields of the resulting hierarchy have non-vanishing Nijenhuis torsion, in general, but their Haantjes tensor vanishes.","sentences":["In 2005 Lorenzoni and Magri showed that a hydrodynamic-type hierarchy determined by the powers of a type (1,1) tensor field (on a smooth manifold) with vanishing Nijenhuis torsion can be deformed to a more general hierarchy, with the help of a chain of conservation laws of the new hierarchy.","We review this construction.","The (1,1) tensor fields of the resulting hierarchy have non-vanishing Nijenhuis torsion, in general, but their Haantjes tensor vanishes."],"url":"http://arxiv.org/abs/2404.10562v1","category":"math-ph"}
{"created":"2024-04-16 13:35:24","title":"HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target Interaction Prediction","abstract":"The discovery of drug-target interactions (DTIs) plays a crucial role in pharmaceutical development. The deep learning model achieves more accurate results in DTI prediction due to its ability to extract robust and expressive features from drug and target chemical structures. However, existing deep learning methods typically generate drug features via aggregating molecular atom representations, ignoring the chemical properties carried by motifs, i.e., substructures of the molecular graph. The atom-drug double-level molecular representation learning can not fully exploit structure information and fails to interpret the DTI mechanism from the motif perspective. In addition, sequential model-based target feature extraction either fuses limited contextual information or requires expensive computational resources. To tackle the above issues, we propose a hierarchical graph representation learning-based DTI prediction method (HiGraphDTI). Specifically, HiGraphDTI learns hierarchical drug representations from triple-level molecular graphs to thoroughly exploit chemical information embedded in atoms, motifs, and molecules. Then, an attentional feature fusion module incorporates information from different receptive fields to extract expressive target features.Last, the hierarchical attention mechanism identifies crucial molecular segments, which offers complementary views for interpreting interaction mechanisms. The experiment results not only demonstrate the superiority of HiGraphDTI to the state-of-the-art methods, but also confirm the practical ability of our model in interaction interpretation and new DTI discovery.","sentences":["The discovery of drug-target interactions (DTIs) plays a crucial role in pharmaceutical development.","The deep learning model achieves more accurate results in DTI prediction due to its ability to extract robust and expressive features from drug and target chemical structures.","However, existing deep learning methods typically generate drug features via aggregating molecular atom representations, ignoring the chemical properties carried by motifs, i.e., substructures of the molecular graph.","The atom-drug double-level molecular representation learning can not fully exploit structure information and fails to interpret the DTI mechanism from the motif perspective.","In addition, sequential model-based target feature extraction either fuses limited contextual information or requires expensive computational resources.","To tackle the above issues, we propose a hierarchical graph representation learning-based DTI prediction method (HiGraphDTI).","Specifically, HiGraphDTI learns hierarchical drug representations from triple-level molecular graphs to thoroughly exploit chemical information embedded in atoms, motifs, and molecules.","Then, an attentional feature fusion module incorporates information from different receptive fields to extract expressive target features.","Last, the hierarchical attention mechanism identifies crucial molecular segments, which offers complementary views for interpreting interaction mechanisms.","The experiment results not only demonstrate the superiority of HiGraphDTI to the state-of-the-art methods, but also confirm the practical ability of our model in interaction interpretation and new DTI discovery."],"url":"http://arxiv.org/abs/2404.10561v1","category":"cs.LG"}
{"created":"2024-04-16 13:33:28","title":"Few-mode squeezing in type-I parametric downconversion by complete group velocity matching","abstract":"Frequency-degenerate pulsed type-I parametric downconversion is a widely used source of squeezed light for numerous quantum optical applications. However, this source is typically spectrally multimode and the generated squeezing is distributed between many spectral modes with a limited degree of squeezing per mode. We show that in a nonlinear crystal, where the condition of complete group velocity matching for the pump and the signal is satisfied, the number of generated modes may be as low as 2 or 3 modes. We illustrate the general theory with the example of the MgO-doped lithium niobate crystal pumped at 775 nm and generating squeezed light at 1.55 $\\mu$m. Our model includes the derivation of the degree of squeezing from the properties of the pump and the crystal and shows that 12 dB of squeezing can be obtained in a periodically poled crystal of length 80 mm.","sentences":["Frequency-degenerate pulsed type-I parametric downconversion is a widely used source of squeezed light for numerous quantum optical applications.","However, this source is typically spectrally multimode and the generated squeezing is distributed between many spectral modes with a limited degree of squeezing per mode.","We show that in a nonlinear crystal, where the condition of complete group velocity matching for the pump and the signal is satisfied, the number of generated modes may be as low as 2 or 3 modes.","We illustrate the general theory with the example of the MgO-doped lithium niobate crystal pumped at 775 nm and generating squeezed light at 1.55 $\\mu$m.","Our model includes the derivation of the degree of squeezing from the properties of the pump and the crystal and shows that 12 dB of squeezing can be obtained in a periodically poled crystal of length 80 mm."],"url":"http://arxiv.org/abs/2404.10560v1","category":"quant-ph"}
{"created":"2024-04-16 13:33:12","title":"Nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function","abstract":"For the binary classification problem, a novel nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function (QSSVM$_{0/1}$) is proposed. Specifically, the task of QSSVM$_{0/1}$ is to seek a quadratic separating hyper-surface to divide the samples into two categories. And it has better interpretability than the methods using kernel functions, since each feature of the sample acts both independently and synergistically. By introducing the 0-1 loss function to construct the optimization model makes the model obtain strong sample sparsity. The proximal stationary point of the optimization problem is defined by the proximal operator of the 0-1 loss function, which figures out the problem of non-convex discontinuity of the optimization problem due to the 0-1 loss function. A new iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is designed to solve the optimization problem, which relates to the working set defined by support vectors. The computational complexity and convergence of the algorithm are discussed. Numerical experiments on 4 artificial datasets and 14 benchmark datasets demonstrate that our QSSVM$_{0/1}$ achieves higher classification accuracy, fewer support vectors and less CPU time cost than other state-of-the-art methods.","sentences":["For the binary classification problem, a novel nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function (QSSVM$_{0/1}$) is proposed.","Specifically, the task of QSSVM$_{0/1}$ is to seek a quadratic separating hyper-surface to divide the samples into two categories.","And it has better interpretability than the methods using kernel functions, since each feature of the sample acts both independently and synergistically.","By introducing the 0-1 loss function to construct the optimization model makes the model obtain strong sample sparsity.","The proximal stationary point of the optimization problem is defined by the proximal operator of the 0-1 loss function, which figures out the problem of non-convex discontinuity of the optimization problem due to the 0-1 loss function.","A new iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is designed to solve the optimization problem, which relates to the working set defined by support vectors.","The computational complexity and convergence of the algorithm are discussed.","Numerical experiments on 4 artificial datasets and 14 benchmark datasets demonstrate that our QSSVM$_{0/1}$ achieves higher classification accuracy, fewer support vectors and less CPU time cost than other state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10559v1","category":"math.OC"}
{"created":"2024-04-16 13:29:11","title":"Generative AI for Advanced UAV Networking","abstract":"With the impressive achievements of chatGPT and Sora, generative artificial intelligence (GAI) has received increasing attention. Not limited to the field of content generation, GAI is also widely used to solve the problems in wireless communication scenarios due to its powerful learning and generalization capabilities. Therefore, we discuss key applications of GAI in improving unmanned aerial vehicle (UAV) communication and networking performance in this article. Specifically, we first review the key technologies of GAI and the important roles of UAV networking. Then, we show how GAI can improve the communication, networking, and security performances of UAV systems. Subsequently, we propose a novel framework of GAI for advanced UAV networking, and then present a case study of UAV-enabled spectrum map estimation and transmission rate optimization based on the proposed framework to verify the effectiveness of GAI-enabled UAV systems. Finally, we discuss some important open directions.","sentences":["With the impressive achievements of chatGPT and Sora, generative artificial intelligence (GAI) has received increasing attention.","Not limited to the field of content generation, GAI is also widely used to solve the problems in wireless communication scenarios due to its powerful learning and generalization capabilities.","Therefore, we discuss key applications of GAI in improving unmanned aerial vehicle (UAV) communication and networking performance in this article.","Specifically, we first review the key technologies of GAI and the important roles of UAV networking.","Then, we show how GAI can improve the communication, networking, and security performances of UAV systems.","Subsequently, we propose a novel framework of GAI for advanced UAV networking, and then present a case study of UAV-enabled spectrum map estimation and transmission rate optimization based on the proposed framework to verify the effectiveness of GAI-enabled UAV systems.","Finally, we discuss some important open directions."],"url":"http://arxiv.org/abs/2404.10556v1","category":"cs.NI"}
{"created":"2024-04-16 13:22:54","title":"Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning","abstract":"The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress. This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values. Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief. By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions. To systematically assess these risks, we introduce a novel set of risk evaluation metrics. Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes. This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols.","sentences":["The open-sourcing of large language models (LLMs) accelerates application development, innovation, and scientific progress.","This includes both base models, which are pre-trained on extensive datasets without alignment, and aligned models, deliberately designed to align with ethical standards and human values.","Contrary to the prevalent assumption that the inherent instruction-following limitations of base LLMs serve as a safeguard against misuse, our investigation exposes a critical oversight in this belief.","By deploying carefully designed demonstrations, our research demonstrates that base LLMs could effectively interpret and execute malicious instructions.","To systematically assess these risks, we introduce a novel set of risk evaluation metrics.","Empirical results reveal that the outputs from base LLMs can exhibit risk levels on par with those of models fine-tuned for malicious purposes.","This vulnerability, requiring neither specialized knowledge nor training, can be manipulated by almost anyone, highlighting the substantial risk and the critical need for immediate attention to the base LLMs' security protocols."],"url":"http://arxiv.org/abs/2404.10552v1","category":"cs.CL"}
{"created":"2024-04-16 13:19:57","title":"The Evolution of Learning: Assessing the Transformative Impact of Generative AI on Higher Education","abstract":"Generative Artificial Intelligence (GAI) models such as ChatGPT have experienced a surge in popularity, attracting 100 million active users in 2 months and generating an estimated 10 million daily queries. Despite this remarkable adoption, there remains a limited understanding to which extent this innovative technology influences higher education. This research paper investigates the impact of GAI on university students and Higher Education Institutions (HEIs). The study adopts a mixed-methods approach, combining a comprehensive survey with scenario analysis to explore potential benefits, drawbacks, and transformative changes the new technology brings. Using an online survey with 130 participants we assessed students' perspectives and attitudes concerning present ChatGPT usage in academics. Results show that students use the current technology for tasks like assignment writing and exam preparation and believe it to be a effective help in achieving academic goals. The scenario analysis afterwards projected potential future scenarios, providing valuable insights into the possibilities and challenges associated with incorporating GAI into higher education. The main motivation is to gain a tangible and precise understanding of the potential consequences for HEIs and to provide guidance responding to the evolving learning environment. The findings indicate that irresponsible and excessive use of the technology could result in significant challenges. Hence, HEIs must develop stringent policies, reevaluate learning objectives, upskill their lecturers, adjust the curriculum and reconsider examination approaches.","sentences":["Generative Artificial Intelligence (GAI) models such as ChatGPT have experienced a surge in popularity, attracting 100 million active users in 2 months and generating an estimated 10 million daily queries.","Despite this remarkable adoption, there remains a limited understanding to which extent this innovative technology influences higher education.","This research paper investigates the impact of GAI on university students and Higher Education Institutions (HEIs).","The study adopts a mixed-methods approach, combining a comprehensive survey with scenario analysis to explore potential benefits, drawbacks, and transformative changes the new technology brings.","Using an online survey with 130 participants we assessed students' perspectives and attitudes concerning present ChatGPT usage in academics.","Results show that students use the current technology for tasks like assignment writing and exam preparation and believe it to be a effective help in achieving academic goals.","The scenario analysis afterwards projected potential future scenarios, providing valuable insights into the possibilities and challenges associated with incorporating GAI into higher education.","The main motivation is to gain a tangible and precise understanding of the potential consequences for HEIs and to provide guidance responding to the evolving learning environment.","The findings indicate that irresponsible and excessive use of the technology could result in significant challenges.","Hence, HEIs must develop stringent policies, reevaluate learning objectives, upskill their lecturers, adjust the curriculum and reconsider examination approaches."],"url":"http://arxiv.org/abs/2404.10551v1","category":"cs.AI"}
{"created":"2024-04-16 13:19:46","title":"Analytical Approximation of the ELBO Gradient in the Context of the Clutter Problem","abstract":"We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as the clutter problem. The method employs the reparameterization trick to move the gradient operator inside the expectation and relies on the assumption that, because the likelihood factorizes over the observed data, the variational distribution is generally more compactly supported than the Gaussian distribution in the likelihood factors. This allows efficient local approximation of the individual likelihood factors, which leads to an analytical solution for the integral defining the gradient expectation. We integrate the proposed gradient approximation as the expectation step in an EM (Expectation Maximization) algorithm for maximizing ELBO and test against classical deterministic approaches in Bayesian inference, such as the Laplace approximation, Expectation Propagation and Mean-Field Variational Inference. The proposed method demonstrates good accuracy and rate of convergence together with linear computational complexity.","sentences":["We propose an analytical solution for approximating the gradient of the Evidence Lower Bound (ELBO) in variational inference problems where the statistical model is a Bayesian network consisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated clutter, known as the clutter problem.","The method employs the reparameterization trick to move the gradient operator inside the expectation and relies on the assumption that, because the likelihood factorizes over the observed data, the variational distribution is generally more compactly supported than the Gaussian distribution in the likelihood factors.","This allows efficient local approximation of the individual likelihood factors, which leads to an analytical solution for the integral defining the gradient expectation.","We integrate the proposed gradient approximation as the expectation step in an EM (Expectation Maximization) algorithm for maximizing ELBO and test against classical deterministic approaches in Bayesian inference, such as the Laplace approximation, Expectation Propagation and Mean-Field Variational Inference.","The proposed method demonstrates good accuracy and rate of convergence together with linear computational complexity."],"url":"http://arxiv.org/abs/2404.10550v1","category":"cs.LG"}
{"created":"2024-04-16 13:19:08","title":"Asymmetric dynamical charges in two-dimensional ferroelectrics","abstract":"Ferroelectricity is commonly understood in terms of dynamical charges, which represent the dipole moments generated by atomic displacements or the forces induced by electric fields. In ferroelectrics with a high degree of symmetry, the dynamical charges are typically symmetric tensors, and can be visualized as ellipsoids. In van der Waals (vdW) materials which break centrosymmetry, a new type of ferroelectricity arises which differs greatly from conventional ferroelectrics. The polarization is purely electronic, arising from an interlayer charge transfer, and most of the polarization generated is perpendicular to atomic motion. We show that the unconventional properties of vdW ferroelectrics are manifested in their dynamical charges, which exhibit spatial modulation and intrinsic asymmetry. Dynamical charges in vdW ferroelectrics, and more generally, any strongly anisotropic ferroelectric, can be visualized as deformable, non-ideal ellipsoids dependent on the atomic configuration. Furthermore, we show that, due to the mixed electrostatic boundary conditions employed for two-dimensional materials, non-diagonal dynamical charges in 2D materials are always asymmetric.","sentences":["Ferroelectricity is commonly understood in terms of dynamical charges, which represent the dipole moments generated by atomic displacements or the forces induced by electric fields.","In ferroelectrics with a high degree of symmetry, the dynamical charges are typically symmetric tensors, and can be visualized as ellipsoids.","In van der Waals (vdW) materials which break centrosymmetry, a new type of ferroelectricity arises which differs greatly from conventional ferroelectrics.","The polarization is purely electronic, arising from an interlayer charge transfer, and most of the polarization generated is perpendicular to atomic motion.","We show that the unconventional properties of vdW ferroelectrics are manifested in their dynamical charges, which exhibit spatial modulation and intrinsic asymmetry.","Dynamical charges in vdW ferroelectrics, and more generally, any strongly anisotropic ferroelectric, can be visualized as deformable, non-ideal ellipsoids dependent on the atomic configuration.","Furthermore, we show that, due to the mixed electrostatic boundary conditions employed for two-dimensional materials, non-diagonal dynamical charges in 2D materials are always asymmetric."],"url":"http://arxiv.org/abs/2404.10549v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 13:16:19","title":"Warm-Start Variational Quantum Policy Iteration","abstract":"Reinforcement learning is a powerful framework aiming to determine optimal behavior in highly complex decision-making scenarios. This objective can be achieved using policy iteration, which requires to solve a typically large linear system of equations. We propose the variational quantum policy iteration (VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhanced subroutine. Its scalability is supported by an analysis of the structure of generic reinforcement learning environments, laying the foundation for potential quantum advantage with utility-scale quantum computers. Furthermore, we introduce the warm-start initialization variant (WS-VarQPI) that significantly reduces resource overhead. The algorithm solves a large FrozenLake environment with an underlying 256x256-dimensional linear system, indicating its practical robustness.","sentences":["Reinforcement learning is a powerful framework aiming to determine optimal behavior in highly complex decision-making scenarios.","This objective can be achieved using policy iteration, which requires to solve a typically large linear system of equations.","We propose the variational quantum policy iteration (VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhanced subroutine.","Its scalability is supported by an analysis of the structure of generic reinforcement learning environments, laying the foundation for potential quantum advantage with utility-scale quantum computers.","Furthermore, we introduce the warm-start initialization variant (WS-VarQPI) that significantly reduces resource overhead.","The algorithm solves a large FrozenLake environment with an underlying 256x256-dimensional linear system, indicating its practical robustness."],"url":"http://arxiv.org/abs/2404.10546v1","category":"quant-ph"}
{"created":"2024-04-16 13:11:34","title":"Ideals preserved by linear changes of coordinates in positive characteristic","abstract":"We consider the polynomial ring in finitely many variables over an algebraically closed field of positive characteristic, and initiate the systematic study of ideals preserved by the action of the general linear group by changes of coordinates. We show that these ideals are classified by sets of carry patterns, which are finite sequences of integers introduced by Doty in the study of representation theory of the polynomial ring. We provide an algorithm to decompose an invariant ideal as a sum of carry ideals with no redundancies. Next, we study the conditions under which one carry ideal is contained in another, and completely characterize the image of the multiplication map between the space of linear forms and a subrepresentation of forms of degree d. Finally, we begin an investigation into free resolutions of these ideals. Our results are most explicit in the case of carry ideals in two variables, where we completely describe the monomial generators and syzygies using base-p expansions of the parameters involved, and we provide a formula for the structure of the Tor modules in the Grothendieck group of representations.","sentences":["We consider the polynomial ring in finitely many variables over an algebraically closed field of positive characteristic, and initiate the systematic study of ideals preserved by the action of the general linear group by changes of coordinates.","We show that these ideals are classified by sets of carry patterns, which are finite sequences of integers introduced by Doty in the study of representation theory of the polynomial ring.","We provide an algorithm to decompose an invariant ideal as a sum of carry ideals with no redundancies.","Next, we study the conditions under which one carry ideal is contained in another, and completely characterize the image of the multiplication map between the space of linear forms and a subrepresentation of forms of degree d.","Finally, we begin an investigation into free resolutions of these ideals.","Our results are most explicit in the case of carry ideals in two variables, where we completely describe the monomial generators and syzygies using base-p expansions of the parameters involved, and we provide a formula for the structure of the Tor modules in the Grothendieck group of representations."],"url":"http://arxiv.org/abs/2404.10544v1","category":"math.AC"}
{"created":"2024-04-16 13:09:48","title":"MPCOM: Robotic Data Gathering with Radio Mapping and Model Predictive Communication","abstract":"Robotic data gathering (RDG) is an emerging paradigm that navigates a robot to harvest data from remote sensors. However, motion planning in this paradigm needs to maximize the RDG efficiency instead of the navigation efficiency, for which the existing motion planning methods become inefficient, as they plan robot trajectories merely according to motion factors. This paper proposes radio map guided model predictive communication (MPCOM), which navigates the robot with both grid and radio maps for shape-aware collision avoidance and communication-aware trajectory generation in a dynamic environment. The proposed MPCOM is able to trade off the time spent on reaching goal, avoiding collision, and improving communication. MPCOM captures high-order signal propagation characteristics using radio maps and incorporates the map-guided communication regularizer to the motion planning block. Experiments in IRSIM and CARLA simulators show that the proposed MPCOM outperforms other benchmarks in both LOS and NLOS cases. Real-world testing based on car-like robots is also provided to demonstrate the effectiveness of MPCOM in indoor environments.","sentences":["Robotic data gathering (RDG) is an emerging paradigm that navigates a robot to harvest data from remote sensors.","However, motion planning in this paradigm needs to maximize the RDG efficiency instead of the navigation efficiency, for which the existing motion planning methods become inefficient, as they plan robot trajectories merely according to motion factors.","This paper proposes radio map guided model predictive communication (MPCOM), which navigates the robot with both grid and radio maps for shape-aware collision avoidance and communication-aware trajectory generation in a dynamic environment.","The proposed MPCOM is able to trade off the time spent on reaching goal, avoiding collision, and improving communication.","MPCOM captures high-order signal propagation characteristics using radio maps and incorporates the map-guided communication regularizer to the motion planning block.","Experiments in IRSIM and CARLA simulators show that the proposed MPCOM outperforms other benchmarks in both LOS and NLOS cases.","Real-world testing based on car-like robots is also provided to demonstrate the effectiveness of MPCOM in indoor environments."],"url":"http://arxiv.org/abs/2404.10541v1","category":"cs.RO"}
{"created":"2024-04-16 13:07:09","title":"Purification of Noisy Measurements and Faithful Distillation of Entanglement","abstract":"We consider entanglement distillation in a realistic scenario with noisy operations in which quantum measurements that constitute a general quantum operation are particularly noisy. We present a protocol for purifying noisy measurements and show that with the help of the purification, imperfect local operations can be used to distill entanglement. We show that the purification protocol is robust against noise in implementation and analyze the purification in a practical realization: for measurement and gate errors up to 10%, we suggest that the purification with two additional qubits is cost-effective for distilling entanglement. The purification protocol is feasible with currently available quantum technologies and readily applied to entanglement applications.","sentences":["We consider entanglement distillation in a realistic scenario with noisy operations in which quantum measurements that constitute a general quantum operation are particularly noisy.","We present a protocol for purifying noisy measurements and show that with the help of the purification, imperfect local operations can be used to distill entanglement.","We show that the purification protocol is robust against noise in implementation and analyze the purification in a practical realization: for measurement and gate errors up to 10%, we suggest that the purification with two additional qubits is cost-effective for distilling entanglement.","The purification protocol is feasible with currently available quantum technologies and readily applied to entanglement applications."],"url":"http://arxiv.org/abs/2404.10538v1","category":"quant-ph"}
{"created":"2024-04-16 13:06:38","title":"A bridge between spatial and first-passage properties of continuous and discrete time stochastic processes: from hard walls to absorbing boundary conditions","abstract":"We consider a generic one-dimensional stochastic process $x(t)$, or a random walk $X_n$, which describes the position of a particle evolving inside an interval $[a,b]$, with absorbing walls located at $a$ and $b$. In continuous time, $x(t)$ is driven by some equilibrium process $\\mathbf{\\theta}(t)$, while in discrete time, the jumps of $X_n$ follow a stationary process that obeys a time reversal property. An important observable to characterize its behaviour is the exit probability $E_b(x,t)$, which is the probability for the particle to be absorbed first at the wall $b$, before or at time $t$, given its initial position $x$. In this paper we show that the derivation of this quantity can be tackled by studying a dual process $y(t)$ very similar to $x(t)$ but with hard walls at $a$ and $b$. More precisely, we show that the quantity $E_b(x,t)$ for the process $x(t)$ is equal to the probability $\\tilde \\Phi(x,t|b)$ of finding the dual process inside the interval $[a,x]$ at time $t$, with $y(0) =b$. We show that this duality applies to various processes which are of interest in physics, including models of active particles, diffusing diffusivity models, a large class of discrete and continuous time random walks, and even processes subjected to stochastic resetting. For all these cases, we provide an explicit construction of the dual process. We also give simple derivations of this identity both in the continuous and in the discrete time setting, as well as numerical tests for a large number of models of interest. Finally, we use simulations to show that the duality is also likely to hold for more complex processes such as fractional Brownian motion.","sentences":["We consider a generic one-dimensional stochastic process $x(t)$, or a random walk $X_n$, which describes the position of a particle evolving inside an interval $","[a,b]$, with absorbing walls located at $a$ and $b$. In continuous time, $x(t)$ is driven by some equilibrium process $\\mathbf{\\theta}(t)$, while in discrete time, the jumps of $X_n$ follow a stationary process that obeys a time reversal property.","An important observable to characterize its behaviour is the exit probability $E_b(x,t)$, which is the probability for the particle to be absorbed first at the wall $b$, before or at time $t$, given its initial position $x$.","In this paper we show that the derivation of this quantity can be tackled by studying a dual process $y(t)$ very similar to $x(t)$ but with hard walls at $a$ and $b$. More precisely, we show that the quantity $E_b(x,t)$ for the process $x(t)$ is equal to the probability $\\tilde \\Phi(x,t|b)$ of finding the dual process inside the interval $[a,x]$ at time $t$, with $y(0) =b$. We show that this duality applies to various processes which are of interest in physics, including models of active particles, diffusing diffusivity models, a large class of discrete and continuous time random walks, and even processes subjected to stochastic resetting.","For all these cases, we provide an explicit construction of the dual process.","We also give simple derivations of this identity both in the continuous and in the discrete time setting, as well as numerical tests for a large number of models of interest.","Finally, we use simulations to show that the duality is also likely to hold for more complex processes such as fractional Brownian motion."],"url":"http://arxiv.org/abs/2404.10537v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-16 12:59:15","title":"Macdonald identities, Weyl-Kac denominator formulas and affine Grassmannians","abstract":"We expand the affine Weyl denominator formulas as signed $q$-series of ordinary Weyl characters running over the affine Grassmannian. Here the grading in $q$ coincides with the (dual) atomic length of the root system considered as introduced by Chapelier-Laget and Gerber. Next, we give simple expressions of the atomic lengths in terms of self-conjugate core partitions. This permits in particular to rederive, from the general theory of affine root systems, some results of the second author obtained by case-by-case computations on determinants and the use of particular families of strict partitions. These families are proved to be in simple one-to-one correspondences with the previous core partition model and, through this correspondence, the atomic length on cores equates the rank of the strict partitions considered. Finally, we make explicit some interactions between the affine Grassmannian elements and the Nekrasov-Okounkov type formulas.","sentences":["We expand the affine Weyl denominator formulas as signed $q$-series of ordinary Weyl characters running over the affine Grassmannian.","Here the grading in $q$ coincides with the (dual) atomic length of the root system considered as introduced by Chapelier-Laget and Gerber.","Next, we give simple expressions of the atomic lengths in terms of self-conjugate core partitions.","This permits in particular to rederive, from the general theory of affine root systems, some results of the second author obtained by case-by-case computations on determinants and the use of particular families of strict partitions.","These families are proved to be in simple one-to-one correspondences with the previous core partition model and, through this correspondence, the atomic length on cores equates the rank of the strict partitions considered.","Finally, we make explicit some interactions between the affine Grassmannian elements and the Nekrasov-Okounkov type formulas."],"url":"http://arxiv.org/abs/2404.10532v1","category":"math.CO"}
{"created":"2024-04-16 12:55:57","title":"AllTheDocks road safety dataset: A cyclist's perspective and experience","abstract":"Active travel is an essential component in intelligent transportation systems. Cycling, as a form of active travel, shares the road space with motorised traffic which often affects the cyclists' safety and comfort and therefore peoples' propensity to uptake cycling instead of driving. This paper presents a unique dataset, collected by cyclists across London, that includes video footage, accelerometer, GPS, and gyroscope data. The dataset is then labelled by an independent group of London cyclists to rank the safety level of each frame and to identify objects in the cyclist's field of vision that might affect their experience. Furthermore, in this dataset, the quality of the road is measured by the international roughness index of the surface, which indicates the comfort of cycling on the road. The dataset will be made available for open access in the hope of motivating more research in this area to underpin the requirements for cyclists' safety and comfort and encourage more people to replace vehicle travel with cycling.","sentences":["Active travel is an essential component in intelligent transportation systems.","Cycling, as a form of active travel, shares the road space with motorised traffic which often affects the cyclists' safety and comfort and therefore peoples' propensity to uptake cycling instead of driving.","This paper presents a unique dataset, collected by cyclists across London, that includes video footage, accelerometer, GPS, and gyroscope data.","The dataset is then labelled by an independent group of London cyclists to rank the safety level of each frame and to identify objects in the cyclist's field of vision that might affect their experience.","Furthermore, in this dataset, the quality of the road is measured by the international roughness index of the surface, which indicates the comfort of cycling on the road.","The dataset will be made available for open access in the hope of motivating more research in this area to underpin the requirements for cyclists' safety and comfort and encourage more people to replace vehicle travel with cycling."],"url":"http://arxiv.org/abs/2404.10528v1","category":"cs.MM"}
{"created":"2024-04-16 12:55:15","title":"SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization in Unseen Environments","abstract":"In this paper, we present SPVLoc, a global indoor localization method that accurately determines the six-dimensional (6D) camera pose of a query image and requires minimal scene-specific prior knowledge and no scene-specific training. Our approach employs a novel matching procedure to localize the perspective camera's viewport, given as an RGB image, within a set of panoramic semantic layout representations of the indoor environment. The panoramas are rendered from an untextured 3D reference model, which only comprises approximate structural information about room shapes, along with door and window annotations. We demonstrate that a straightforward convolutional network structure can successfully achieve image-to-panorama and ultimately image-to-model matching. Through a viewport classification score, we rank reference panoramas and select the best match for the query image. Then, a 6D relative pose is estimated between the chosen panorama and query image. Our experiments demonstrate that this approach not only efficiently bridges the domain gap but also generalizes well to previously unseen scenes that are not part of the training data. Moreover, it achieves superior localization accuracy compared to the state of the art methods and also estimates more degrees of freedom of the camera pose. We will make our source code publicly available at https://github.com/fraunhoferhhi/spvloc .","sentences":["In this paper, we present SPVLoc, a global indoor localization method that accurately determines the six-dimensional (6D) camera pose of a query image and requires minimal scene-specific prior knowledge and no scene-specific training.","Our approach employs a novel matching procedure to localize the perspective camera's viewport, given as an RGB image, within a set of panoramic semantic layout representations of the indoor environment.","The panoramas are rendered from an untextured 3D reference model, which only comprises approximate structural information about room shapes, along with door and window annotations.","We demonstrate that a straightforward convolutional network structure can successfully achieve image-to-panorama and ultimately image-to-model matching.","Through a viewport classification score, we rank reference panoramas and select the best match for the query image.","Then, a 6D relative pose is estimated between the chosen panorama and query image.","Our experiments demonstrate that this approach not only efficiently bridges the domain gap but also generalizes well to previously unseen scenes that are not part of the training data.","Moreover, it achieves superior localization accuracy compared to the state of the art methods and also estimates more degrees of freedom of the camera pose.","We will make our source code publicly available at https://github.com/fraunhoferhhi/spvloc ."],"url":"http://arxiv.org/abs/2404.10527v1","category":"cs.CV"}
{"created":"2024-04-16 12:54:55","title":"Variabilities in the polar field and solar cycle due to irregular properties of Bipolar Magnetic Regions","abstract":"Decay and dispersal of the tilted Bipolar Magnetic Regions (BMRs) on the solar surface are observed to produce large-scale poloidal field, which acts as the seed for the toroidal field and, thus, the next sunspot cycle. However, various properties of BMR, namely, the tilt, time delay between successive emergences, location, and flux, all have irregular variations. Previous studies show that these variations can lead to changes in the polar field. In this study, we first demonstrate that our 3D kinematic dynamo model, STABLE, reproduces the robust feature of the surface flux transport (SFT) model, namely the variation of the generated dipole moment with the latitude of the BMR position. Using STABLE in both SFT and dynamo modes, we perform simulations by varying the individual properties of BMR and keeping their distributions the same in all the cycles as inspired by the observations. We find that randomness due to the distribution in either the time delay or the BMR latitude produces negligible variation in the polar field and the solar cycle. However, randomness due to BMR flux distribution produces substantial effects, while the scatter in the tilt around Joy law produces the largest variation. Our comparative analyses suggest that the scatter of BMR tilt around Joy law is the major cause of variation in the solar cycle. Furthermore, our simulations also show that the magnetic field-dependent time delay of BMR emergence produces more realistic features of the magnetic cycle, consistent with observation.","sentences":["Decay and dispersal of the tilted Bipolar Magnetic Regions (BMRs) on the solar surface are observed to produce large-scale poloidal field, which acts as the seed for the toroidal field and, thus, the next sunspot cycle.","However, various properties of BMR, namely, the tilt, time delay between successive emergences, location, and flux, all have irregular variations.","Previous studies show that these variations can lead to changes in the polar field.","In this study, we first demonstrate that our 3D kinematic dynamo model, STABLE, reproduces the robust feature of the surface flux transport (SFT) model, namely the variation of the generated dipole moment with the latitude of the BMR position.","Using STABLE in both SFT and dynamo modes, we perform simulations by varying the individual properties of BMR and keeping their distributions the same in all the cycles as inspired by the observations.","We find that randomness due to the distribution in either the time delay or the BMR latitude produces negligible variation in the polar field and the solar cycle.","However, randomness due to BMR flux distribution produces substantial effects, while the scatter in the tilt around Joy law produces the largest variation.","Our comparative analyses suggest that the scatter of BMR tilt around Joy law is the major cause of variation in the solar cycle.","Furthermore, our simulations also show that the magnetic field-dependent time delay of BMR emergence produces more realistic features of the magnetic cycle, consistent with observation."],"url":"http://arxiv.org/abs/2404.10526v1","category":"astro-ph.SR"}
{"created":"2024-04-16 12:49:24","title":"Scalar field dark matter with time-varying equation of state","abstract":"We propose a new model of scalar field dark matter interacting with dark energy. Adopting a fluid description of the dark matter field in the regime of rapid oscillations, we find that the equation of state for dark matter is non-zero and even becomes increasingly negative at late times during dark energy domination. Furthermore, the speed of sound of dark matter is non-vanishing at all length scales, and a non-adiabatic pressure contribution arises. The results indicate that there are still unexplored possible interactions within the dark sector that lead to novel background effects and can impact structure formation processes.","sentences":["We propose a new model of scalar field dark matter interacting with dark energy.","Adopting a fluid description of the dark matter field in the regime of rapid oscillations, we find that the equation of state for dark matter is non-zero and even becomes increasingly negative at late times during dark energy domination.","Furthermore, the speed of sound of dark matter is non-vanishing at all length scales, and a non-adiabatic pressure contribution arises.","The results indicate that there are still unexplored possible interactions within the dark sector that lead to novel background effects and can impact structure formation processes."],"url":"http://arxiv.org/abs/2404.10524v1","category":"astro-ph.CO"}
{"created":"2024-04-16 12:48:27","title":"Direct imaging of exoplanets: Legacy and prospects","abstract":"Understanding how giant and terrestrial planets form and evolve, what is their internal structure and that of their atmosphere, represents one of the major challenges of modern astronomy, which is directly connected to the ultimate search for life at the horizon 2040. However, several astrophysical, biological and technological obstacles must be overcome. From the astrophysical point of view, it is indeed crucial to understand the mechanisms of formation and evolution of giant planets, including planet and disk interactions, which will completely sculpt the planetary architectures and thus dominate the formation of terrestrial planets, especially in regions around the host star capable of supporting life. It is also important to develop dedicated instrumentation and techniques to study in their totality the population of giant and terrestrial planets, but also to reveal in the near future the first biological clues of life in the atmospheres of terrestrial planets. In that perspective, direct imaging from ground-based observatories or in space is playing a central role in concert with other observing techniques. This review introduces the genesis of this observing technique, the main instrumental innovation and challenges, stellar targets and surveys, to then present the main results obtained so far about the physics and the mechanisms of formation and evolution of young giant planets and planetary system architectures. I will then present the exciting perspectives offered by the upcoming generation of planet imagers about to come online, particularly on the future extremely large telescopes. On the timescale of a human Life, we may well be witnessing the first discovery of an exoplanet and the first detection of indices of life in the atmosphere of a nearby exo-Earth!","sentences":["Understanding how giant and terrestrial planets form and evolve, what is their internal structure and that of their atmosphere, represents one of the major challenges of modern astronomy, which is directly connected to the ultimate search for life at the horizon 2040.","However, several astrophysical, biological and technological obstacles must be overcome.","From the astrophysical point of view, it is indeed crucial to understand the mechanisms of formation and evolution of giant planets, including planet and disk interactions, which will completely sculpt the planetary architectures and thus dominate the formation of terrestrial planets, especially in regions around the host star capable of supporting life.","It is also important to develop dedicated instrumentation and techniques to study in their totality the population of giant and terrestrial planets, but also to reveal in the near future the first biological clues of life in the atmospheres of terrestrial planets.","In that perspective, direct imaging from ground-based observatories or in space is playing a central role in concert with other observing techniques.","This review introduces the genesis of this observing technique, the main instrumental innovation and challenges, stellar targets and surveys, to then present the main results obtained so far about the physics and the mechanisms of formation and evolution of young giant planets and planetary system architectures.","I will then present the exciting perspectives offered by the upcoming generation of planet imagers about to come online, particularly on the future extremely large telescopes.","On the timescale of a human Life, we may well be witnessing the first discovery of an exoplanet and the first detection of indices of life in the atmosphere of a nearby exo-Earth!"],"url":"http://arxiv.org/abs/2404.10522v1","category":"astro-ph.EP"}
{"created":"2024-04-16 12:41:25","title":"MobileNetV4 - Universal Models for the Mobile Ecosystem","abstract":"We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices. At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant. Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup. An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested. Finally, to further boost accuracy, we introduce a novel distillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms.","sentences":["We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices.","At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant.","Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup.","An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness.","The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested.","Finally, to further boost accuracy, we introduce a novel distillation technique.","Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms."],"url":"http://arxiv.org/abs/2404.10518v1","category":"cs.CV"}
{"created":"2024-04-16 12:37:10","title":"CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity","abstract":"State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses. One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output. However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems. We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions. This approach focuses the reasoning process on generating an attribution-centric output. Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions. In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases.","sentences":["State-of-the-art performance in QA tasks is currently achieved by systems employing Large Language Models (LLMs), however these models tend to hallucinate information in their responses.","One approach focuses on enhancing the generation process by incorporating attribution from the given input to the output.","However, the challenge of identifying appropriate attributions and verifying their accuracy against a source is a complex task that requires significant improvements in assessing such systems.","We introduce an attribution-oriented Chain-of-Thought reasoning method to enhance the accuracy of attributions.","This approach focuses the reasoning process on generating an attribution-centric output.","Evaluations on two context-enhanced question-answering datasets using GPT-4 demonstrate improved accuracy and correctness of attributions.","In addition, the combination of our method with finetuning enhances the response and attribution accuracy of two smaller LLMs, showing their potential to outperform GPT-4 in some cases."],"url":"http://arxiv.org/abs/2404.10513v1","category":"cs.CL"}
{"created":"2024-04-16 12:33:44","title":"Four-hour thunderstorm nowcasting using deep diffusion models of satellite","abstract":"Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose a deep diffusion model of satellite (DDMS) to establish an AI-based convection nowcasting system. On one hand, it employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time. On the other hand, it utilizes geostationary satellite brightness temperature data, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system operates efficiently (forecasting 4 hours of convection in 8 minutes), and is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.","sentences":["Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to nature and society.","After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches.","However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response.","Here, we propose a deep diffusion model of satellite (DDMS) to establish an AI-based convection nowcasting system.","On one hand, it employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, significantly improving the forecast lead time.","On the other hand, it utilizes geostationary satellite brightness temperature data, thereby achieving planetary-scale forecast coverage.","During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km).","Its performance reaches a new height in convection nowcasting compared to the existing models.","In terms of application, our system operates efficiently (forecasting 4 hours of convection in 8 minutes), and is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting.","Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies."],"url":"http://arxiv.org/abs/2404.10512v1","category":"cs.LG"}
{"created":"2024-04-16 12:27:54","title":"White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency","abstract":"Social biases can manifest in language agency. For instance, White individuals and men are often described as \"agentic\" and achievement-oriented, whereas Black individuals and women are frequently described as \"communal\" and as assisting roles. This study establishes agency as an important aspect of studying social biases in both human-written and Large Language Model (LLM)-generated texts. To accurately measure \"language agency\" at sentence level, we propose a Language Agency Classification dataset to train reliable agency classifiers. We then use an agency classifier to reveal notable language agency biases in 6 datasets of human- or LLM-written texts, including biographies, professor reviews, and reference letters. While most prior NLP research on agency biases focused on single dimensions, we comprehensively explore language agency biases in gender, race, and intersectional identities. We observe that (1) language agency biases in human-written texts align with real-world social observations; (2) LLM-generated texts demonstrate remarkably higher levels of language agency bias than human-written texts; and (3) critical biases in language agency target people of minority groups--for instance, languages used to describe Black females exhibit the lowest level of agency across datasets. Our findings reveal intricate social biases in human- and LLM-written texts through the lens of language agency, warning against using LLM generations in social contexts without scrutiny.","sentences":["Social biases can manifest in language agency.","For instance, White individuals and men are often described as \"agentic\" and achievement-oriented, whereas Black individuals and women are frequently described as \"communal\" and as assisting roles.","This study establishes agency as an important aspect of studying social biases in both human-written and Large Language Model (LLM)-generated texts.","To accurately measure \"language agency\" at sentence level, we propose a Language Agency Classification dataset to train reliable agency classifiers.","We then use an agency classifier to reveal notable language agency biases in 6 datasets of human- or LLM-written texts, including biographies, professor reviews, and reference letters.","While most prior NLP research on agency biases focused on single dimensions, we comprehensively explore language agency biases in gender, race, and intersectional identities.","We observe that (1) language agency biases in human-written texts align with real-world social observations; (2) LLM-generated texts demonstrate remarkably higher levels of language agency bias than human-written texts; and (3) critical biases in language agency target people of minority groups--for instance, languages used to describe Black females exhibit the lowest level of agency across datasets.","Our findings reveal intricate social biases in human- and LLM-written texts through the lens of language agency, warning against using LLM generations in social contexts without scrutiny."],"url":"http://arxiv.org/abs/2404.10508v1","category":"cs.CL"}
{"created":"2024-04-16 12:26:49","title":"Superradiant Darwinism: survival of the lightest axion","abstract":"We study the dynamical evolution of superradiant instabilities of rotating black holes for multiple axion fields with comparable masses, motivated by string theory constructions, which typically exhibit a large number of light axions, with a broad range of masses. We show, in particular, that even though superradiant clouds for the heavier axion species grow faster, they are eventually reabsorbed by the black hole as the latter amplifies the lighter axion field(s), analogously to the dynamics of different species competing for the same resources in an ecosystem. We also incorporate in our study the effects of accretion and gravitational wave emission. We further demonstrate that the existence of multiple axion species with comparable masses may have a substantial impact on the stochastic gravitational wave background produced by axion clouds around black hole binary merger remnants, which could be probed with planned detectors.","sentences":["We study the dynamical evolution of superradiant instabilities of rotating black holes for multiple axion fields with comparable masses, motivated by string theory constructions, which typically exhibit a large number of light axions, with a broad range of masses.","We show, in particular, that even though superradiant clouds for the heavier axion species grow faster, they are eventually reabsorbed by the black hole as the latter amplifies the lighter axion field(s), analogously to the dynamics of different species competing for the same resources in an ecosystem.","We also incorporate in our study the effects of accretion and gravitational wave emission.","We further demonstrate that the existence of multiple axion species with comparable masses may have a substantial impact on the stochastic gravitational wave background produced by axion clouds around black hole binary merger remnants, which could be probed with planned detectors."],"url":"http://arxiv.org/abs/2404.10507v1","category":"hep-ph"}
{"created":"2024-04-16 12:23:59","title":"Data Collection of Real-Life Knowledge Work in Context: The RLKWiC Dataset","abstract":"Over the years, various approaches have been employed to enhance the productivity of knowledge workers, from addressing psychological well-being to the development of personal knowledge assistants. A significant challenge in this research area has been the absence of a comprehensive, publicly accessible dataset that mirrors real-world knowledge work. Although a handful of datasets exist, many are restricted in access or lack vital information dimensions, complicating meaningful comparison and benchmarking in the domain. This paper presents RLKWiC, a novel dataset of Real-Life Knowledge Work in Context, derived from monitoring the computer interactions of eight participants over a span of two months. As the first publicly available dataset offering a wealth of essential information dimensions (such as explicated contexts, textual contents, and semantics), RLKWiC seeks to address the research gap in the personal information management domain, providing valuable insights for modeling user behavior.","sentences":["Over the years, various approaches have been employed to enhance the productivity of knowledge workers, from addressing psychological well-being to the development of personal knowledge assistants.","A significant challenge in this research area has been the absence of a comprehensive, publicly accessible dataset that mirrors real-world knowledge work.","Although a handful of datasets exist, many are restricted in access or lack vital information dimensions, complicating meaningful comparison and benchmarking in the domain.","This paper presents RLKWiC, a novel dataset of Real-Life Knowledge Work in Context, derived from monitoring the computer interactions of eight participants over a span of two months.","As the first publicly available dataset offering a wealth of essential information dimensions (such as explicated contexts, textual contents, and semantics), RLKWiC seeks to address the research gap in the personal information management domain, providing valuable insights for modeling user behavior."],"url":"http://arxiv.org/abs/2404.10505v1","category":"cs.AI"}
{"created":"2024-04-16 12:20:49","title":"A Sentiment Analysis of Medical Text Based on Deep Learning","abstract":"The field of natural language processing (NLP) has made significant progress with the rapid development of deep learning technologies. One of the research directions in text sentiment analysis is sentiment analysis of medical texts, which holds great potential for application in clinical diagnosis. However, the medical field currently lacks sufficient text datasets, and the effectiveness of sentiment analysis is greatly impacted by different model design approaches, which presents challenges. Therefore, this paper focuses on the medical domain, using bidirectional encoder representations from transformers (BERT) as the basic pre-trained model and experimenting with modules such as convolutional neural network (CNN), fully connected network (FCN), and graph convolutional networks (GCN) at the output layer. Experiments and analyses were conducted on the METS-CoV dataset to explore the training performance after integrating different deep learning networks. The results indicate that CNN models outperform other networks when trained on smaller medical text datasets in combination with pre-trained models like BERT. This study highlights the significance of model selection in achieving effective sentiment analysis in the medical domain and provides a reference for future research to develop more efficient model architectures.","sentences":["The field of natural language processing (NLP) has made significant progress with the rapid development of deep learning technologies.","One of the research directions in text sentiment analysis is sentiment analysis of medical texts, which holds great potential for application in clinical diagnosis.","However, the medical field currently lacks sufficient text datasets, and the effectiveness of sentiment analysis is greatly impacted by different model design approaches, which presents challenges.","Therefore, this paper focuses on the medical domain, using bidirectional encoder representations from transformers (BERT) as the basic pre-trained model and experimenting with modules such as convolutional neural network (CNN), fully connected network (FCN), and graph convolutional networks (GCN) at the output layer.","Experiments and analyses were conducted on the METS-CoV dataset to explore the training performance after integrating different deep learning networks.","The results indicate that CNN models outperform other networks when trained on smaller medical text datasets in combination with pre-trained models like BERT.","This study highlights the significance of model selection in achieving effective sentiment analysis in the medical domain and provides a reference for future research to develop more efficient model architectures."],"url":"http://arxiv.org/abs/2404.10503v1","category":"cs.CL"}
{"created":"2024-04-16 12:20:45","title":"Representations of Noisy $N$-Ports","abstract":"Much has been written about the representation of noisy linear 2-ports. Here we present a theory of noisy $N$-ports. We show how in the general case there are $(2N)!/(N!)^2$ equivalent representations and give the transformations relating them. We also discuss singular cases in which some of the transformations are not possible as well as how to measure the noise properties of an $N$-port. This work is motivated by the REACH experiment to observe the global 21 cm signal for which modelling noise with exquisite precision is essential for a reliable calibration.","sentences":["Much has been written about the representation of noisy linear 2-ports.","Here we present a theory of noisy $N$-ports.","We show how in the general case there are $(2N)!/(N!)^2$ equivalent representations and give the transformations relating them.","We also discuss singular cases in which some of the transformations are not possible as well as how to measure the noise properties of an $N$-port.","This work is motivated by the REACH experiment to observe the global 21 cm signal for which modelling noise with exquisite precision is essential for a reliable calibration."],"url":"http://arxiv.org/abs/2404.10502v1","category":"astro-ph.IM"}
{"created":"2024-04-16 12:19:54","title":"Self-Supervised Visual Preference Alignment","abstract":"This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs). We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization. It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers. The whole pipeline no longer hinges on supervision from GPT4 or human involvement during alignment, and is highly efficient with few lines of code. With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet. Visualizations shows its improved ability to align with user-intentions. A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling. Code will be available.","sentences":["This paper makes the first attempt towards unsupervised preference alignment in Vision-Language Models (VLMs).","We generate chosen and rejected responses with regard to the original and augmented image pairs, and conduct preference alignment with direct preference optimization.","It is based on a core idea: properly designed augmentation to the image input will induce VLM to generate false but hard negative responses, which helps the model to learn from and produce more robust and powerful answers.","The whole pipeline no longer hinges on supervision from GPT4 or human involvement during alignment, and is highly efficient with few lines of code.","With only 8k randomly sampled unsupervised data, it achieves 90\\% relative score to GPT-4 on complex reasoning in LLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex multi-modal benchmark MM-Vet.","Visualizations shows its improved ability to align with user-intentions.","A series of ablations are firmly conducted to reveal the latent mechanism of the approach, which also indicates its potential towards further scaling.","Code will be available."],"url":"http://arxiv.org/abs/2404.10501v1","category":"cs.CV"}
{"created":"2024-04-16 12:19:08","title":"When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical Paradigm","abstract":"With the development of Large Language Models (LLM), numerous prompts have been proposed, each with a rich set of features and their own merits. This paper summarizes the prompt words for large language models (LLMs), categorizing them into stimulating and framework types, and proposes an Auto-Prompt Graphical Paradigm(APGP) that combines both stimulating and framework prompts to enhance the problem-solving capabilities of LLMs across multiple domains, then exemplifies it with a framework that adheres to this paradigm. The framework involves automated prompt generation and consideration of emotion-stimulus factors, guiding LLMs in problem abstraction, diversified solutions generation, comprehensive optimization, and self-verification after providing answers, ensuring solution accuracy. Compared to traditional stimuli and framework prompts, this framework integrates the advantages of both by adopting automated approaches inspired by APE work, overcoming the limitations of manually designed prompts. Test results on the ruozhiba and BBH datasets demonstrate that this framework can effectively improve the efficiency and accuracy of LLMs in problem-solving, paving the way for new applications of LLMs.","sentences":["With the development of Large Language Models (LLM), numerous prompts have been proposed, each with a rich set of features and their own merits.","This paper summarizes the prompt words for large language models (LLMs), categorizing them into stimulating and framework types, and proposes an Auto-Prompt Graphical Paradigm(APGP) that combines both stimulating and framework prompts to enhance the problem-solving capabilities of LLMs across multiple domains, then exemplifies it with a framework that adheres to this paradigm.","The framework involves automated prompt generation and consideration of emotion-stimulus factors, guiding LLMs in problem abstraction, diversified solutions generation, comprehensive optimization, and self-verification after providing answers, ensuring solution accuracy.","Compared to traditional stimuli and framework prompts, this framework integrates the advantages of both by adopting automated approaches inspired by APE work, overcoming the limitations of manually designed prompts.","Test results on the ruozhiba and BBH datasets demonstrate that this framework can effectively improve the efficiency and accuracy of LLMs in problem-solving, paving the way for new applications of LLMs."],"url":"http://arxiv.org/abs/2404.10500v1","category":"cs.CL"}
{"created":"2024-04-16 12:18:08","title":"Robust Noisy Label Learning via Two-Stream Sample Distillation","abstract":"Noisy label learning aims to learn robust networks under the supervision of noisy labels, which plays a critical role in deep learning. Existing work either conducts sample selection or label correction to deal with noisy labels during the model training process. In this paper, we design a simple yet effective sample selection framework, termed Two-Stream Sample Distillation (TSSD), for noisy label learning, which can extract more high-quality samples with clean labels to improve the robustness of network training. Firstly, a novel Parallel Sample Division (PSD) module is designed to generate a certain training set with sufficient reliable positive and negative samples by jointly considering the sample structure in feature space and the human prior in loss space. Secondly, a novel Meta Sample Purification (MSP) module is further designed to mine adequate semi-hard samples from the remaining uncertain training set by learning a strong meta classifier with extra golden data. As a result, more and more high-quality samples will be distilled from the noisy training set to train networks robustly in every iteration. Extensive experiments on four benchmark datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet, and Clothing-1M, show that our method has achieved state-of-the-art results over its competitors.","sentences":["Noisy label learning aims to learn robust networks under the supervision of noisy labels, which plays a critical role in deep learning.","Existing work either conducts sample selection or label correction to deal with noisy labels during the model training process.","In this paper, we design a simple yet effective sample selection framework, termed Two-Stream Sample Distillation (TSSD), for noisy label learning, which can extract more high-quality samples with clean labels to improve the robustness of network training.","Firstly, a novel Parallel Sample Division (PSD) module is designed to generate a certain training set with sufficient reliable positive and negative samples by jointly considering the sample structure in feature space and the human prior in loss space.","Secondly, a novel Meta Sample Purification (MSP) module is further designed to mine adequate semi-hard samples from the remaining uncertain training set by learning a strong meta classifier with extra golden data.","As a result, more and more high-quality samples will be distilled from the noisy training set to train networks robustly in every iteration.","Extensive experiments on four benchmark datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet, and Clothing-1M, show that our method has achieved state-of-the-art results over its competitors."],"url":"http://arxiv.org/abs/2404.10499v1","category":"cs.CV"}
{"created":"2024-04-16 12:12:06","title":"LAECIPS: Large Vision Model Assisted Adaptive Edge-Cloud Collaboration for IoT-based Perception System","abstract":"Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy. Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics. Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency. However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments. To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework. In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play. We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency. We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams. Theoretical analysis of LAECIPS proves its feasibility. Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments.","sentences":["Recent large vision models (e.g., SAM) enjoy great potential to facilitate intelligent perception with high accuracy.","Yet, the resource constraints in the IoT environment tend to limit such large vision models to be locally deployed, incurring considerable inference latency thereby making it difficult to support real-time applications, such as autonomous driving and robotics.","Edge-cloud collaboration with large-small model co-inference offers a promising approach to achieving high inference accuracy and low latency.","However, existing edge-cloud collaboration methods are tightly coupled with the model architecture and cannot adapt to the dynamic data drifts in heterogeneous IoT environments.","To address the issues, we propose LAECIPS, a new edge-cloud collaboration framework.","In LAECIPS, both the large vision model on the cloud and the lightweight model on the edge are plug-and-play.","We design an edge-cloud collaboration strategy based on hard input mining, optimized for both high accuracy and low latency.","We propose to update the edge model and its collaboration strategy with the cloud under the supervision of the large vision model, so as to adapt to the dynamic IoT data streams.","Theoretical analysis of LAECIPS proves its feasibility.","Experiments conducted in a robotic semantic segmentation system using real-world datasets show that LAECIPS outperforms its state-of-the-art competitors in accuracy, latency, and communication overhead while having better adaptability to dynamic environments."],"url":"http://arxiv.org/abs/2404.10498v1","category":"cs.AI"}
{"created":"2024-04-16 12:11:57","title":"Subsequences With Generalised Gap Constraints: Upper and Lower Complexity Bounds","abstract":"For two strings u, v over some alphabet A, we investigate the problem of embedding u into w as a subsequence under the presence of generalised gap constraints. A generalised gap constraint is a triple (i, j, C_{i, j}), where 1 <= i < j <= |u| and C_{i, j} is a subset of A^*. Embedding u as a subsequence into v such that (i, j, C_{i, j}) is satisfied means that if u[i] and u[j] are mapped to v[k] and v[l], respectively, then the induced gap v[k + 1..l - 1] must be a string from C_{i, j}. This generalises the setting recently investigated in [Day et al., ISAAC 2022], where only gap constraints of the form C_{i, i + 1} are considered, as well as the setting from [Kosche et al., RP 2022], where only gap constraints of the form C_{1, |u|} are considered.   We show that subsequence matching under generalised gap constraints is NP-hard, and we complement this general lower bound with a thorough (parameterised) complexity analysis. Moreover, we identify several efficiently solvable subclasses that result from restricting the interval structure induced by the generalised gap constraints.","sentences":["For two strings u, v over some alphabet A, we investigate the problem of embedding u into w as a subsequence under the presence of generalised gap constraints.","A generalised gap constraint is a triple (i, j, C_{i, j}), where 1 <= i < j <= |u| and C_{i, j} is a subset of A^*.","Embedding u as a subsequence into v such that (i, j, C_{i, j}) is satisfied means that if u[i] and u[j] are mapped to v[k] and v[l], respectively, then the induced gap v[k + 1..l - 1] must be a string from C_{i, j}.","This generalises the setting recently investigated in [Day et al., ISAAC 2022], where only gap constraints of the form C_{i, i + 1} are considered, as well as the setting from [Kosche et al., RP 2022], where only gap constraints of the form C_{1, |u|} are considered.   ","We show that subsequence matching under generalised gap constraints is NP-hard, and we complement this general lower bound with a thorough (parameterised) complexity analysis.","Moreover, we identify several efficiently solvable subclasses that result from restricting the interval structure induced by the generalised gap constraints."],"url":"http://arxiv.org/abs/2404.10497v1","category":"cs.DS"}
{"created":"2024-04-16 12:10:01","title":"Spiral of Silences: How is Large Language Model Killing Information Retrieval? -- A Case Study on Open Domain Question Answering","abstract":"The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital \"Spiral of Silence\" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape.","sentences":["The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent.","However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories.","In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems.","Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital \"Spiral of Silence\" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online.","This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information.","We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape."],"url":"http://arxiv.org/abs/2404.10496v1","category":"cs.IR"}
{"created":"2024-04-16 12:03:38","title":"BDAN: Mitigating Temporal Difference Across Electrodes in Cross-Subject Motor Imagery Classification via Generative Bridging Domain","abstract":"Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model. Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper. Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance. In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor. With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions. The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain. To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset. The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods.","sentences":["Because of \"the non-repeatability of the experiment settings and conditions\" and \"the variability of brain patterns among subjects\", the data distributions across sessions and electrodes are different in cross-subject motor imagery (MI) studies, eventually reducing the performance of the classification model.","Systematically summarised based on the existing studies, a novel temporal-electrode data distribution problem is investigated under both intra-subject and inter-subject scenarios in this paper.","Based on the presented issue, a novel bridging domain adaptation network (BDAN) is proposed, aiming to minimise the data distribution difference across sessions in the aspect of the electrode, thus improving and enhancing model performance.","In the proposed BDAN, deep features of all the EEG data are extracted via a specially designed spatial feature extractor.","With the obtained spatio-temporal features, a special generative bridging domain is established, bridging the data from all the subjects across sessions.","The difference across sessions and electrodes is then minimized using the customized bridging loss functions, and the known knowledge is automatically transferred through the constructed bridging domain.","To show the effectiveness of the proposed BDAN, comparison experiments and ablation studies are conducted on a public EEG dataset.","The overall comparison results demonstrate the superior performance of the proposed BDAN compared with the other advanced deep learning and domain adaptation methods."],"url":"http://arxiv.org/abs/2404.10494v1","category":"cs.HC"}
{"created":"2024-04-16 11:52:14","title":"Early-time gamma-ray constraints on cosmic-ray acceleration in the core-collapse SN 2023ixf with the Fermi Large Area Telescope","abstract":"While SNRs have been considered the most relevant Galactic CR accelerators for decades, CCSNe could accelerate particles during the earliest stages of their evolution and hence contribute to the CR energy budget in the Galaxy. Some SNRs have indeed been associated with TeV gamma-rays, yet proton acceleration efficiency during the early stages of an SN expansion remains mostly unconstrained. The multi-wavelength observation of SN 2023ixf, a Type II SN in the nearby galaxy M101, opens the possibility to constrain CR acceleration within a few days after the collapse of the RSG stellar progenitor. With this work, we intend to provide a phenomenological, quasi-model-independent constraint on the CR acceleration efficiency during this event at photon energies above 100 MeV. We performed a maximum-likelihood analysis of gamma-ray data from the Fermi Large Area Telescope up to one month after the SN explosion. We searched for high-energy emission from its expanding shock, and estimated the underlying hadronic CR energy reservoir assuming a power-law proton distribution consistent with standard diffusive shock acceleration. We do not find significant gamma-ray emission from SN 2023ixf. Nonetheless, our non-detection provides the first limit on the energy transferred to the population of hadronic CRs during the very early expansion of a CCSN. Under reasonable assumptions, our limits would imply a maximum efficiency on the CR acceleration of as low as 1%, which is inconsistent with the common estimate of 10% in generic SNe. However, this result is highly dependent on the assumed geometry of the circumstellar medium, and could be relaxed back to 10% by challenging spherical symmetry. A more sophisticated, inhomogeneous characterisation of the shock and the progenitor's environment is required before establishing whether or not Type II SNe are indeed efficient CR accelerators at early times.","sentences":["While SNRs have been considered the most relevant Galactic CR accelerators for decades, CCSNe could accelerate particles during the earliest stages of their evolution and hence contribute to the CR energy budget in the Galaxy.","Some SNRs have indeed been associated with TeV gamma-rays, yet proton acceleration efficiency during the early stages of an SN expansion remains mostly unconstrained.","The multi-wavelength observation of SN 2023ixf, a Type II SN in the nearby galaxy M101, opens the possibility to constrain CR acceleration within a few days after the collapse of the RSG stellar progenitor.","With this work, we intend to provide a phenomenological, quasi-model-independent constraint on the CR acceleration efficiency during this event at photon energies above 100 MeV.","We performed a maximum-likelihood analysis of gamma-ray data from the Fermi Large Area Telescope up to one month after the SN explosion.","We searched for high-energy emission from its expanding shock, and estimated the underlying hadronic CR energy reservoir assuming a power-law proton distribution consistent with standard diffusive shock acceleration.","We do not find significant gamma-ray emission from SN 2023ixf.","Nonetheless, our non-detection provides the first limit on the energy transferred to the population of hadronic CRs during the very early expansion of a CCSN.","Under reasonable assumptions, our limits would imply a maximum efficiency on the CR acceleration of as low as 1%, which is inconsistent with the common estimate of 10% in generic SNe.","However, this result is highly dependent on the assumed geometry of the circumstellar medium, and could be relaxed back to 10% by challenging spherical symmetry.","A more sophisticated, inhomogeneous characterisation of the shock and the progenitor's environment is required before establishing whether or not Type II SNe are indeed efficient CR accelerators at early times."],"url":"http://arxiv.org/abs/2404.10487v1","category":"astro-ph.HE"}
{"created":"2024-04-16 11:33:36","title":"Conversations as a Source for Teaching Scientific Concepts at Different Education Levels","abstract":"Open conversations are one of the most engaging forms of teaching. However, creating those conversations in educational software is a complex endeavor, especially if we want to address the needs of different audiences. While language models hold great promise for educational applications, there are substantial challenges in training them to engage in meaningful and effective conversational teaching, especially when considering the diverse needs of various audiences. No official data sets exist for this task to facilitate the training of language models for conversational teaching, considering the diverse needs of various audiences. This paper presents a novel source for facilitating conversational teaching of scientific concepts at various difficulty levels (from preschooler to expert), namely dialogues taken from video transcripts. We analyse this data source in various ways to show that it offers a diverse array of examples that can be used to generate contextually appropriate and natural responses to scientific topics for specific target audiences. It is a freely available valuable resource for training and evaluating conversation models, encompassing organically occurring dialogues. While the raw data is available online, we provide additional metadata for conversational analysis of dialogues at each level in all available videos.","sentences":["Open conversations are one of the most engaging forms of teaching.","However, creating those conversations in educational software is a complex endeavor, especially if we want to address the needs of different audiences.","While language models hold great promise for educational applications, there are substantial challenges in training them to engage in meaningful and effective conversational teaching, especially when considering the diverse needs of various audiences.","No official data sets exist for this task to facilitate the training of language models for conversational teaching, considering the diverse needs of various audiences.","This paper presents a novel source for facilitating conversational teaching of scientific concepts at various difficulty levels (from preschooler to expert), namely dialogues taken from video transcripts.","We analyse this data source in various ways to show that it offers a diverse array of examples that can be used to generate contextually appropriate and natural responses to scientific topics for specific target audiences.","It is a freely available valuable resource for training and evaluating conversation models, encompassing organically occurring dialogues.","While the raw data is available online, we provide additional metadata for conversational analysis of dialogues at each level in all available videos."],"url":"http://arxiv.org/abs/2404.10475v1","category":"cs.CL"}
{"created":"2024-04-16 11:08:20","title":"Forward lateral photovoltage scanning problem: Perturbation approach and existence-uniqueness analysis","abstract":"In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem. The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions. The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance. Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal. Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter. While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device.","sentences":["In this paper, we present analytical results for the so-called forward lateral photovoltage scanning (LPS) problem.","The (inverse) LPS model predicts doping variations in crystal by measuring the current leaving the crystal generated by a laser at various positions.","The forward model consists of a set of nonlinear elliptic equations coupled with a measuring device modeled by a resistance.","Standard methods to ensure the existence and uniqueness of the forward model cannot be used in a straightforward manner due to the presence of an additional generation term modeling the effect of the laser on the crystal.","Hence, we scale the original forward LPS problem and employ a perturbation approach to derive the leading order system and the correction up to the second order in an appropriate small parameter.","While these simplifications pose no issues from a physical standpoint, they enable us to demonstrate the analytic existence and uniqueness of solutions for the simplified system using standard arguments from elliptic theory adapted to the coupling with the measuring device."],"url":"http://arxiv.org/abs/2404.10466v1","category":"math-ph"}
{"created":"2024-04-16 11:07:52","title":"The Beauville-Voisin-Franchetta conjecture and LLSS eightfolds","abstract":"The Chow rings of hyper-K\\\"ahler varieties are conjectured to have a particularly rich structure. In this paper, we formulate a conjecture that combines the Beauville-Voisin conjecture regarding the subring generated by divisors and the Franchetta conjecture regarding generically defined cycles. As motivation, we show that this Beauville-Voisin-Franchetta conjecture for a hyper-K\\\"ahler variety $X$ follows from a combination of Grothendieck's standard conjectures for a very general deformation of $X$, Murre's conjecture (D) for $X$ and the Franchetta conjecture for $X^3$. As evidence, beyond the case of Fano varieties of lines on smooth cubic fourfolds, we show that this conjecture holds for codimension-2 and codimension-8 cycles on Lehn-Lehn-Sorger-van Straten eightfolds. Moreover, we establish that the subring of the Chow ring generated by primitive divisors injects into cohomology.","sentences":["The Chow rings of hyper-K\\\"ahler varieties are conjectured to have a particularly rich structure.","In this paper, we formulate a conjecture that combines the Beauville-Voisin conjecture regarding the subring generated by divisors and the Franchetta conjecture regarding generically defined cycles.","As motivation, we show that this Beauville-Voisin-Franchetta conjecture for a hyper-K\\\"ahler variety $X$ follows from a combination of Grothendieck's standard conjectures for a very general deformation of $X$, Murre's conjecture (D) for $X$ and the Franchetta conjecture for $X^3$. As evidence, beyond the case of Fano varieties of lines on smooth cubic fourfolds, we show that this conjecture holds for codimension-2 and codimension-8 cycles on Lehn-Lehn-Sorger-van Straten eightfolds.","Moreover, we establish that the subring of the Chow ring generated by primitive divisors injects into cohomology."],"url":"http://arxiv.org/abs/2404.10465v1","category":"math.AG"}
{"created":"2024-04-16 11:07:48","title":"DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion","abstract":"Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern. Current solutions involving fine-tuning or auxiliary models usually require extensive memory and computational resources, rendering them less practical for deployment in large language models (LLMs). In this paper, we propose DeStein, a novel method that detoxififies LMs by altering their internal representations in the activation space with lower resource and time cost. Specifically, we leverage self-induced steering pairs to identify detoxification vectors through arithmetic operations in the activation space. During inference, detoxification is achieved by blending the detoxification vectors with the original representations. Empirical results demonstrate that our method significantly outperforms previous state-of-the-art approaches on popular detoxification metrics, while also maintaining satisfactory generation quality and diversity. Furthermore, we extend our method to multiple LLMs, demonstrating its practicality and scalability. Warning: some example model outputs contain highly offensive or disturbing text.","sentences":["Despite the remarkable achievements of language models (LMs) across a broad spectrum of tasks, their propensity for generating toxic outputs remains a prevalent concern.","Current solutions involving fine-tuning or auxiliary models usually require extensive memory and computational resources, rendering them less practical for deployment in large language models (LLMs).","In this paper, we propose DeStein, a novel method that detoxififies LMs by altering their internal representations in the activation space with lower resource and time cost.","Specifically, we leverage self-induced steering pairs to identify detoxification vectors through arithmetic operations in the activation space.","During inference, detoxification is achieved by blending the detoxification vectors with the original representations.","Empirical results demonstrate that our method significantly outperforms previous state-of-the-art approaches on popular detoxification metrics, while also maintaining satisfactory generation quality and diversity.","Furthermore, we extend our method to multiple LLMs, demonstrating its practicality and scalability.","Warning: some example model outputs contain highly offensive or disturbing text."],"url":"http://arxiv.org/abs/2404.10464v1","category":"cs.CL"}
{"created":"2024-04-16 11:02:49","title":"Pulse Engineering via Projection of Response Functions","abstract":"We present an iterative optimal control method of quantum systems, aimed at an implementation of a desired operation with optimal fidelity. The update step of the method is based on the linear response of the fidelity to the control operators, and its projection onto the mode functions of the corresponding operator. Our method extends methods such as gradient ascent pulse engineering and variational quantum algorithms, by determining the fidelity gradient in a hyperparameter-free manner, and using it for a multi-parameter update, capitalizing on the multi-mode overlap of the perturbation and the mode functions. This directly reduces the number of dynamical trajectories that need to be evaluated in order to update a set of parameters. We demonstrate this approach, and compare it to the standard GRAPE algorithm, for the example of a quantum gate on two qubits, demonstrating a clear improvement in convergence and optimal fidelity of the generated protocol.","sentences":["We present an iterative optimal control method of quantum systems, aimed at an implementation of a desired operation with optimal fidelity.","The update step of the method is based on the linear response of the fidelity to the control operators, and its projection onto the mode functions of the corresponding operator.","Our method extends methods such as gradient ascent pulse engineering and variational quantum algorithms, by determining the fidelity gradient in a hyperparameter-free manner, and using it for a multi-parameter update, capitalizing on the multi-mode overlap of the perturbation and the mode functions.","This directly reduces the number of dynamical trajectories that need to be evaluated in order to update a set of parameters.","We demonstrate this approach, and compare it to the standard GRAPE algorithm, for the example of a quantum gate on two qubits, demonstrating a clear improvement in convergence and optimal fidelity of the generated protocol."],"url":"http://arxiv.org/abs/2404.10462v1","category":"quant-ph"}
{"created":"2024-04-16 11:01:17","title":"Gauge theory is about the geometry of internal spaces","abstract":"In general relativity, the strong equivalence principle is underpinned by a geometrical interpretation of fields on spacetime: all fields and bodies probe the same geometry. This geometric interpretation implies that the parallel transport of all spacetime tensors and spinors is dictated by a single affine connection. Can something similar be said about gauge theory? Agreed, in gauge theory different symmetry groups rule the interactions of different types of charges, so we cannot expect to find the same kind of universality found in the gravitational case. Nonetheless, the parallel transport of all the fields that are charged under the same symmetry group is dictated by a single 'gauge connection', and they all transform jointly under a gauge transformation. Is this kind of 'restricted universality' as geometrically underpinned as in general relativity? Here I argue that it is. The key difference is that the gauge geometry concerns 'internal', as opposed to 'external', spaces. The gauge symmetry of the standard model is thus understood as merely the automorphism group of an internal geometric structure -- $C^3\\otimes C^2\\otimes C^1$ endowed with an orientation and canonical inner product -- in the same way as spacetime symmetries (such as Poincare transformations), are understood as the automorphism group of an external geometric structure (respectively, a Minkowski metric). And the Ehresmann connection can then be understood as determining parallelism for this internal geometry.","sentences":["In general relativity, the strong equivalence principle is underpinned by a geometrical interpretation of fields on spacetime: all fields and bodies probe the same geometry.","This geometric interpretation implies that the parallel transport of all spacetime tensors and spinors is dictated by a single affine connection.","Can something similar be said about gauge theory?","Agreed, in gauge theory different symmetry groups rule the interactions of different types of charges, so we cannot expect to find the same kind of universality found in the gravitational case.","Nonetheless, the parallel transport of all the fields that are charged under the same symmetry group is dictated by a single 'gauge connection', and they all transform jointly under a gauge transformation.","Is this kind of 'restricted universality' as geometrically underpinned as in general relativity?","Here I argue that it is.","The key difference is that the gauge geometry concerns 'internal', as opposed to 'external', spaces.","The gauge symmetry of the standard model is thus understood as merely the automorphism group of an internal geometric structure -- $C^3\\otimes C^2\\otimes C^1$ endowed with an orientation and canonical inner product -- in the same way as spacetime symmetries (such as Poincare transformations), are understood as the automorphism group of an external geometric structure (respectively, a Minkowski metric).","And the Ehresmann connection can then be understood as determining parallelism for this internal geometry."],"url":"http://arxiv.org/abs/2404.10461v1","category":"hep-th"}
{"created":"2024-04-16 10:56:33","title":"Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A Patch and Transformer-Based Approach","abstract":"In the context of increasing demands for long-term multi-energy load forecasting in real-world applications, this paper introduces Patchformer, a novel model that integrates patch embedding with encoder-decoder Transformer-based architectures. To address the limitation in existing Transformer-based models, which struggle with intricate temporal patterns in long-term forecasting, Patchformer employs patch embedding, which predicts multivariate time-series data by separating it into multiple univariate data and segmenting each of them into multiple patches. This method effectively enhances the model's ability to capture local and global semantic dependencies. The numerical analysis shows that the Patchformer obtains overall better prediction accuracy in both multivariate and univariate long-term forecasting on the novel Multi-Energy dataset and other benchmark datasets. In addition, the positive effect of the interdependence among energy-related products on the performance of long-term time-series forecasting across Patchformer and other compared models is discovered, and the superiority of the Patchformer against other models is also demonstrated, which presents a significant advancement in handling the interdependence and complexities of long-term multi-energy forecasting. Lastly, Patchformer is illustrated as the only model that follows the positive correlation between model performance and the length of the past sequence, which states its ability to capture long-range past local semantic information.","sentences":["In the context of increasing demands for long-term multi-energy load forecasting in real-world applications, this paper introduces Patchformer, a novel model that integrates patch embedding with encoder-decoder Transformer-based architectures.","To address the limitation in existing Transformer-based models, which struggle with intricate temporal patterns in long-term forecasting, Patchformer employs patch embedding, which predicts multivariate time-series data by separating it into multiple univariate data and segmenting each of them into multiple patches.","This method effectively enhances the model's ability to capture local and global semantic dependencies.","The numerical analysis shows that the Patchformer obtains overall better prediction accuracy in both multivariate and univariate long-term forecasting on the novel Multi-Energy dataset and other benchmark datasets.","In addition, the positive effect of the interdependence among energy-related products on the performance of long-term time-series forecasting across Patchformer and other compared models is discovered, and the superiority of the Patchformer against other models is also demonstrated, which presents a significant advancement in handling the interdependence and complexities of long-term multi-energy forecasting.","Lastly, Patchformer is illustrated as the only model that follows the positive correlation between model performance and the length of the past sequence, which states its ability to capture long-range past local semantic information."],"url":"http://arxiv.org/abs/2404.10458v1","category":"cs.LG"}
{"created":"2024-04-16 10:54:48","title":"Revealing data leakage in protein interaction benchmarks","abstract":"In recent years, there has been remarkable progress in machine learning for protein-protein interactions. However, prior work has predominantly focused on improving learning algorithms, with less attention paid to evaluation strategies and data preparation. Here, we demonstrate that further development of machine learning methods may be hindered by the quality of existing train-test splits. Specifically, we find that commonly used splitting strategies for protein complexes, based on protein sequence or metadata similarity, introduce major data leakage. This may result in overoptimistic evaluation of generalization, as well as unfair benchmarking of the models, biased towards assessing their overfitting capacity rather than practical utility. To overcome the data leakage, we recommend constructing data splits based on 3D structural similarity of protein-protein interfaces and suggest corresponding algorithms. We believe that addressing the data leakage problem is critical for further progress in this research area.","sentences":["In recent years, there has been remarkable progress in machine learning for protein-protein interactions.","However, prior work has predominantly focused on improving learning algorithms, with less attention paid to evaluation strategies and data preparation.","Here, we demonstrate that further development of machine learning methods may be hindered by the quality of existing train-test splits.","Specifically, we find that commonly used splitting strategies for protein complexes, based on protein sequence or metadata similarity, introduce major data leakage.","This may result in overoptimistic evaluation of generalization, as well as unfair benchmarking of the models, biased towards assessing their overfitting capacity rather than practical utility.","To overcome the data leakage, we recommend constructing data splits based on 3D structural similarity of protein-protein interfaces and suggest corresponding algorithms.","We believe that addressing the data leakage problem is critical for further progress in this research area."],"url":"http://arxiv.org/abs/2404.10457v1","category":"cs.LG"}
{"created":"2024-04-16 10:53:25","title":"Linear instability of hairy black holes in Horndeski theory","abstract":"The Horndeski theory gives the most general model of scalar-tensor theories. It draws a lot of attentions in recent years on its black holes, celestial dynamics, stability analysis, etc. It is important to notice that, for certain subclasses of Horndeski models, one can obtain analytic solutions to the background fields. This provides us with a good opportunity to investigate the corresponding stability problems in details. Specially, we may find out the constraints to the model or theory, under which the stability conditions can be satisfied. In this paper, we focus on a subclass of the Horndeski theory and a set of analytic background solutions are considered. On top of that, the odd-parity gravitational perturbation and the 2nd-order Lagrangian are investigated. With careful analysis, the instability is identified within the neighborhood of event horizon. We are thus able to exclude a specific geometry for the model. It is interesting to notice that, such an instability is implanted in the structure of the corresponding Lagrangian, and will not by erased by simply adding numerical constraints on the coupling parameters. As a starting point of our research, this current work provides insights into further exploration of Horndeski theories.","sentences":["The Horndeski theory gives the most general model of scalar-tensor theories.","It draws a lot of attentions in recent years on its black holes, celestial dynamics, stability analysis, etc.","It is important to notice that, for certain subclasses of Horndeski models, one can obtain analytic solutions to the background fields.","This provides us with a good opportunity to investigate the corresponding stability problems in details.","Specially, we may find out the constraints to the model or theory, under which the stability conditions can be satisfied.","In this paper, we focus on a subclass of the Horndeski theory and a set of analytic background solutions are considered.","On top of that, the odd-parity gravitational perturbation and the 2nd-order Lagrangian are investigated.","With careful analysis, the instability is identified within the neighborhood of event horizon.","We are thus able to exclude a specific geometry for the model.","It is interesting to notice that, such an instability is implanted in the structure of the corresponding Lagrangian, and will not by erased by simply adding numerical constraints on the coupling parameters.","As a starting point of our research, this current work provides insights into further exploration of Horndeski theories."],"url":"http://arxiv.org/abs/2404.10456v1","category":"gr-qc"}
{"created":"2024-04-16 10:50:16","title":"A Computer Vision-Based Quality Assessment Technique for the automatic control of consumables for analytical laboratories","abstract":"The rapid growth of the Industry 4.0 paradigm is increasing the pressure to develop effective automated monitoring systems. Artificial Intelligence (AI) is a convenient tool to improve the efficiency of industrial processes while reducing errors and waste. In fact, it allows the use of real-time data to increase the effectiveness of monitoring systems, minimize errors, make the production process more sustainable, and save costs. In this paper, a novel automatic monitoring system is proposed in the context of production process of plastic consumables used in analysis laboratories, with the aim to increase the effectiveness of the control process currently performed by a human operator. In particular, we considered the problem of classifying the presence or absence of a transparent anticoagulant substance inside test tubes. Specifically, a hand-designed deep network model is used and compared with some state-of-the-art models for its ability to categorize different images of vials that can be either filled with the anticoagulant or empty. Collected results indicate that the proposed approach is competitive with state-of-the-art models in terms of accuracy. Furthermore, we increased the complexity of the task by training the models on the ability to discriminate not only the presence or absence of the anticoagulant inside the vial, but also the size of the test tube. The analysis performed in the latter scenario confirms the competitiveness of our approach. Moreover, our model is remarkably superior in terms of its generalization ability and requires significantly fewer resources. These results suggest the possibility of successfully implementing such a model in the production process of a plastic consumables company.","sentences":["The rapid growth of the Industry 4.0 paradigm is increasing the pressure to develop effective automated monitoring systems.","Artificial Intelligence (AI) is a convenient tool to improve the efficiency of industrial processes while reducing errors and waste.","In fact, it allows the use of real-time data to increase the effectiveness of monitoring systems, minimize errors, make the production process more sustainable, and save costs.","In this paper, a novel automatic monitoring system is proposed in the context of production process of plastic consumables used in analysis laboratories, with the aim to increase the effectiveness of the control process currently performed by a human operator.","In particular, we considered the problem of classifying the presence or absence of a transparent anticoagulant substance inside test tubes.","Specifically, a hand-designed deep network model is used and compared with some state-of-the-art models for its ability to categorize different images of vials that can be either filled with the anticoagulant or empty.","Collected results indicate that the proposed approach is competitive with state-of-the-art models in terms of accuracy.","Furthermore, we increased the complexity of the task by training the models on the ability to discriminate not only the presence or absence of the anticoagulant inside the vial, but also the size of the test tube.","The analysis performed in the latter scenario confirms the competitiveness of our approach.","Moreover, our model is remarkably superior in terms of its generalization ability and requires significantly fewer resources.","These results suggest the possibility of successfully implementing such a model in the production process of a plastic consumables company."],"url":"http://arxiv.org/abs/2404.10454v1","category":"cs.CV"}
{"created":"2024-04-16 10:48:12","title":"Detecting quantum vacuum fluctuations of the electromagnetic field","abstract":"We identify two signatures of quantum vacuum fluctuations of the electromagnetic field on a harmonically trapped charged particle. They are a shift from the natural trap frequency and generation of quantum coherences. The frequency shift, estimated for a single-electron cyclotron, should be observable in future experiments. We also suggest a possible route to detecting vacuum-generated quantum coherences. We assess the role of the long-wavelength and rotating-wave approximations in arriving at our estimates. These experiments should settle the debate on the choice of approximations and gauge in capturing the effect of the quantum vacuum fluctuations.","sentences":["We identify two signatures of quantum vacuum fluctuations of the electromagnetic field on a harmonically trapped charged particle.","They are a shift from the natural trap frequency and generation of quantum coherences.","The frequency shift, estimated for a single-electron cyclotron, should be observable in future experiments.","We also suggest a possible route to detecting vacuum-generated quantum coherences.","We assess the role of the long-wavelength and rotating-wave approximations in arriving at our estimates.","These experiments should settle the debate on the choice of approximations and gauge in capturing the effect of the quantum vacuum fluctuations."],"url":"http://arxiv.org/abs/2404.10453v1","category":"quant-ph"}
{"created":"2024-04-16 10:34:21","title":"Numerical investigation of a family of solitary-wave solutions for the nonlinear Schr\u00f6dinger equation perturbed by third-, and fourth-order dispersion","abstract":"We study solitary wave solutions for the nonlinear Schr\\\"odinger equation perturbed by the effects of third-, and fourth-order dispersion, maintaining a wavenumber gap between the solitary waves and the propagation constant. We numerically construct members of a family of such solitary waves, including Kruglov and Harvey's exact solution, using the spectral renormalization method and establish empirical relations between the pulse parameters. A deeper insight into the properties of solitary waves and solitons can be obtained through collisions. Therefore we perform pulse propagation simulations demonstrating different collision regimes. Depending on the pulses initial phase difference, this can lead to the formation of short-lived two-pulse bound states. While these collisions are generally inelastic, singular phase values exist at which they are elastic. Finally, we detail the properties of Kruglov and Harvey's soliton solution under loss, verifying earlier predictions of perturbation theory and suggesting a convergence to the soliton solution of the standard nonlinear Schr\\\"odinger equation in the limit of large propagation distances.","sentences":["We study solitary wave solutions for the nonlinear Schr\\\"odinger equation perturbed by the effects of third-, and fourth-order dispersion, maintaining a wavenumber gap between the solitary waves and the propagation constant.","We numerically construct members of a family of such solitary waves, including Kruglov and Harvey's exact solution, using the spectral renormalization method and establish empirical relations between the pulse parameters.","A deeper insight into the properties of solitary waves and solitons can be obtained through collisions.","Therefore we perform pulse propagation simulations demonstrating different collision regimes.","Depending on the pulses initial phase difference, this can lead to the formation of short-lived two-pulse bound states.","While these collisions are generally inelastic, singular phase values exist at which they are elastic.","Finally, we detail the properties of Kruglov and Harvey's soliton solution under loss, verifying earlier predictions of perturbation theory and suggesting a convergence to the soliton solution of the standard nonlinear Schr\\\"odinger equation in the limit of large propagation distances."],"url":"http://arxiv.org/abs/2404.10449v1","category":"nlin.PS"}
{"created":"2024-04-16 10:31:06","title":"SparseDM: Toward Sparse Efficient Diffusion Models","abstract":"Diffusion models have been extensively used in data generation tasks and are recognized as one of the best generative models. However, their time-consuming deployment, long inference time, and requirements on large memory limit their application on mobile devices. In this paper, we propose a method based on the improved Straight-Through Estimator to improve the deployment efficiency of diffusion models. Specifically, we add sparse masks to the Convolution and Linear layers in a pre-trained diffusion model, then use design progressive sparsity for model training in the fine-tuning stage, and switch the inference mask on and off, which supports a flexible choice of sparsity during inference according to the FID and MACs requirements. Experiments on four datasets conducted on a state-of-the-art Transformer-based diffusion model demonstrate that our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on average. Under other MACs conditions, the FID is also lower than 1$\\sim$137 compared to other methods.","sentences":["Diffusion models have been extensively used in data generation tasks and are recognized as one of the best generative models.","However, their time-consuming deployment, long inference time, and requirements on large memory limit their application on mobile devices.","In this paper, we propose a method based on the improved Straight-Through Estimator to improve the deployment efficiency of diffusion models.","Specifically, we add sparse masks to the Convolution and Linear layers in a pre-trained diffusion model, then use design progressive sparsity for model training in the fine-tuning stage, and switch the inference mask on and off, which supports a flexible choice of sparsity during inference according to the FID and MACs requirements.","Experiments on four datasets conducted on a state-of-the-art Transformer-based diffusion model demonstrate that our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on average.","Under other MACs conditions, the FID is also lower than 1$\\sim$137 compared to other methods."],"url":"http://arxiv.org/abs/2404.10445v1","category":"cs.LG"}
{"created":"2024-04-16 10:30:48","title":"AGHINT: Attribute-Guided Representation Learning on Heterogeneous Information Networks with Transformer","abstract":"Recently, heterogeneous graph neural networks (HGNNs) have achieved impressive success in representation learning by capturing long-range dependencies and heterogeneity at the node level. However, few existing studies have delved into the utilization of node attributes in heterogeneous information networks (HINs). In this paper, we investigate the impact of inter-node attribute disparities on HGNNs performance within the benchmark task, i.e., node classification, and empirically find that typical models exhibit significant performance decline when classifying nodes whose attributes markedly differ from their neighbors. To alleviate this issue, we propose a novel Attribute-Guided heterogeneous Information Networks representation learning model with Transformer (AGHINT), which allows a more effective aggregation of neighbor node information under the guidance of attributes. Specifically, AGHINT transcends the constraints of the original graph structure by directly integrating higher-order similar neighbor features into the learning process and modifies the message-passing mechanism between nodes based on their attribute disparities. Extensive experimental results on three real-world heterogeneous graph benchmarks with target node attributes demonstrate that AGHINT outperforms the state-of-the-art.","sentences":["Recently, heterogeneous graph neural networks (HGNNs) have achieved impressive success in representation learning by capturing long-range dependencies and heterogeneity at the node level.","However, few existing studies have delved into the utilization of node attributes in heterogeneous information networks (HINs).","In this paper, we investigate the impact of inter-node attribute disparities on HGNNs performance within the benchmark task, i.e., node classification, and empirically find that typical models exhibit significant performance decline when classifying nodes whose attributes markedly differ from their neighbors.","To alleviate this issue, we propose a novel Attribute-Guided heterogeneous Information Networks representation learning model with Transformer (AGHINT), which allows a more effective aggregation of neighbor node information under the guidance of attributes.","Specifically, AGHINT transcends the constraints of the original graph structure by directly integrating higher-order similar neighbor features into the learning process and modifies the message-passing mechanism between nodes based on their attribute disparities.","Extensive experimental results on three real-world heterogeneous graph benchmarks with target node attributes demonstrate that AGHINT outperforms the state-of-the-art."],"url":"http://arxiv.org/abs/2404.10443v1","category":"cs.LG"}
{"created":"2024-04-16 10:10:19","title":"Language Proficiency and F0 Entrainment: A Study of L2 English Imitation in Italian, French, and Slovak Speakers","abstract":"This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART). Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances. Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination. However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment. This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation.","sentences":["This study explores F0 entrainment in second language (L2) English speech imitation during an Alternating Reading Task (ART).","Participants with Italian, French, and Slovak native languages imitated English utterances, and their F0 entrainment was quantified using the Dynamic Time Warping (DTW) distance between the parameterized F0 contours of the imitated utterances and those of the model utterances.","Results indicate a nuanced relationship between L2 English proficiency and entrainment: speakers with higher proficiency generally exhibit less entrainment in pitch variation and declination.","However, within dyads, the more proficient speakers demonstrate a greater ability to mimic pitch range, leading to increased entrainment.","This suggests that proficiency influences entrainment differently at individual and dyadic levels, highlighting the complex interplay between language skill and prosodic adaptation."],"url":"http://arxiv.org/abs/2404.10440v1","category":"cs.CL"}
{"created":"2024-04-16 10:04:38","title":"The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement","abstract":"Pose refinement is an interesting and practically relevant research direction. Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer. Existing approaches focus on learning features / scene representations for the pose refinement task. This involves training an implicit scene representation or learning features while optimizing a camera pose-based loss. A natural question is whether training specific features / representations is truly necessary or whether similar results can be already achieved with more generic features. In this work, we present a simple approach that combines pre-trained features with a particle filter and a renderable representation of the scene. Despite its simplicity, it achieves state-of-the-art results, demonstrating that one can easily build a pose refiner without the need for specific training. The code is at https://github.com/ga1i13o/mcloc_poseref","sentences":["Pose refinement is an interesting and practically relevant research direction.","Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer.","Existing approaches focus on learning features / scene representations for the pose refinement task.","This involves training an implicit scene representation or learning features while optimizing a camera pose-based loss.","A natural question is whether training specific features / representations is truly necessary or whether similar results can be already achieved with more generic features.","In this work, we present a simple approach that combines pre-trained features with a particle filter and a renderable representation of the scene.","Despite its simplicity, it achieves state-of-the-art results, demonstrating that one can easily build a pose refiner without the need for specific training.","The code is at https://github.com/ga1i13o/mcloc_poseref"],"url":"http://arxiv.org/abs/2404.10438v1","category":"cs.CV"}
{"created":"2024-04-16 10:04:20","title":"A note on generalized spherical maximal means","abstract":"The goal of this note is to provide an alternative proof of Theorem 1.1 (i) in \\cite{Liu2023arXiv}, that is, if $n\\geq 2$ and $M^{\\alpha}$ is bounded on $L^{p}(\\mathbb{R}^{n})$ for some $\\operatorname{Re}\\in \\mathbb{C}$ and $p\\geq 2$, then we have   \\begin{align*}   \\operatorname{Re} \\alpha\\geq \\max\\left\\{\\frac{1-n}{2}+\\frac{1}{p},\\frac{1-n}{p}\\right\\}.   \\end{align*}","sentences":["The goal of this note is to provide an alternative proof of Theorem 1.1 (i) in \\cite{Liu2023arXiv}, that is, if $n\\geq 2$ and $M^{\\alpha}$ is bounded on $L^{p}(\\mathbb{R}^{n})$ for some $\\operatorname{Re}\\in \\mathbb{C}$ and $p\\geq 2$, then we have   \\begin{align*}   \\operatorname{Re} \\alpha\\geq \\max\\left\\{\\frac{1-n}{2}+\\frac{1}{p},\\frac{1-n}{p}\\right\\}.   ","\\end{align*}"],"url":"http://arxiv.org/abs/2404.10437v1","category":"math.CA"}
{"created":"2024-04-16 10:02:36","title":"Tree Bandits for Generative Bayes","abstract":"In generative models with obscured likelihood, Approximate Bayesian Computation (ABC) is often the tool of last resort for inference. However, ABC demands many prior parameter trials to keep only a small fraction that passes an acceptance test. To accelerate ABC rejection sampling, this paper develops a self-aware framework that learns from past trials and errors. We apply recursive partitioning classifiers on the ABC lookup table to sequentially refine high-likelihood regions into boxes. Each box is regarded as an arm in a binary bandit problem treating ABC acceptance as a reward. Each arm has a proclivity for being chosen for the next ABC evaluation, depending on the prior distribution and past rejections. The method places more splits in those areas where the likelihood resides, shying away from low-probability regions destined for ABC rejections. We provide two versions: (1) ABC-Tree for posterior sampling, and (2) ABC-MAP for maximum a posteriori estimation. We demonstrate accurate ABC approximability at much lower simulation cost. We justify the use of our tree-based bandit algorithms with nearly optimal regret bounds. Finally, we successfully apply our approach to the problem of masked image classification using deep generative models.","sentences":["In generative models with obscured likelihood, Approximate Bayesian Computation (ABC) is often the tool of last resort for inference.","However, ABC demands many prior parameter trials to keep only a small fraction that passes an acceptance test.","To accelerate ABC rejection sampling, this paper develops a self-aware framework that learns from past trials and errors.","We apply recursive partitioning classifiers on the ABC lookup table to sequentially refine high-likelihood regions into boxes.","Each box is regarded as an arm in a binary bandit problem treating ABC acceptance as a reward.","Each arm has a proclivity for being chosen for the next ABC evaluation, depending on the prior distribution and past rejections.","The method places more splits in those areas where the likelihood resides, shying away from low-probability regions destined for ABC rejections.","We provide two versions: (1) ABC-Tree for posterior sampling, and (2) ABC-MAP for maximum a posteriori estimation.","We demonstrate accurate ABC approximability at much lower simulation cost.","We justify the use of our tree-based bandit algorithms with nearly optimal regret bounds.","Finally, we successfully apply our approach to the problem of masked image classification using deep generative models."],"url":"http://arxiv.org/abs/2404.10436v1","category":"cs.LG"}
{"created":"2024-04-16 09:56:08","title":"Explainable concept mappings of MRI: Revealing the mechanisms underlying deep learning-based brain disease classification","abstract":"Motivation. While recent studies show high accuracy in the classification of Alzheimer's disease using deep neural networks, the underlying learned concepts have not been investigated.   Goals. To systematically identify changes in brain regions through concepts learned by the deep neural network for model validation.   Approach. Using quantitative R2* maps we separated Alzheimer's patients (n=117) from normal controls (n=219) by using a convolutional neural network and systematically investigated the learned concepts using Concept Relevance Propagation and compared these results to a conventional region of interest-based analysis.   Results. In line with established histological findings and the region of interest-based analyses, highly relevant concepts were primarily found in and adjacent to the basal ganglia.   Impact. The identification of concepts learned by deep neural networks for disease classification enables validation of the models and could potentially improve reliability.","sentences":["Motivation.","While recent studies show high accuracy in the classification of Alzheimer's disease using deep neural networks, the underlying learned concepts have not been investigated.   ","Goals.","To systematically identify changes in brain regions through concepts learned by the deep neural network for model validation.   ","Approach.","Using quantitative R2* maps we separated Alzheimer's patients (n=117) from normal controls (n=219) by using a convolutional neural network and systematically investigated the learned concepts using Concept Relevance Propagation and compared these results to a conventional region of interest-based analysis.   ","Results.","In line with established histological findings and the region of interest-based analyses, highly relevant concepts were primarily found in and adjacent to the basal ganglia.   ","Impact.","The identification of concepts learned by deep neural networks for disease classification enables validation of the models and could potentially improve reliability."],"url":"http://arxiv.org/abs/2404.10433v1","category":"cs.CV"}
{"created":"2024-04-16 09:46:37","title":"MEEL: Multi-Modal Event Evolution Learning","abstract":"Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER is fundamental and underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism, yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans. Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution. In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction. We collect and curate a benchmark M-EV2 for MMER. Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs.","sentences":["Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities.","MMER is fundamental and underlies a wide broad of applications.","Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability.","The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios.","In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism, yielding advanced MMER ability.","Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios.","Subsequently, we employ ChatGPT to generate evolving graphs for these seed events.","We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans.","Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution.","In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction.","We collect and curate a benchmark M-EV2 for MMER.","Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs."],"url":"http://arxiv.org/abs/2404.10429v1","category":"cs.AI"}
{"created":"2024-04-16 09:46:24","title":"Zero-Sum Games for Volterra Integral Equations and Viscosity Solutions of Path-Dependent Hamilton-Jacobi Equations","abstract":"We consider a game, in which the dynamics is described by a non-linear Volterra integral equation of Hammerstein type with a weakly-singular kernel and the goals of the first and second players are, respectively, to minimize and maximize a given cost functional. We propose a way of how the dynamic programming principle can be formalized and the theory of generalized (viscosity) solutions of path-dependent Hamilton--Jacobi equations can be developed in order to prove the existence of the game value, obtain a characterization of the value functional, and construct players' optimal feedback strategies.","sentences":["We consider a game, in which the dynamics is described by a non-linear Volterra integral equation of Hammerstein type with a weakly-singular kernel and the goals of the first and second players are, respectively, to minimize and maximize a given cost functional.","We propose a way of how the dynamic programming principle can be formalized and the theory of generalized (viscosity) solutions of path-dependent Hamilton--Jacobi equations can be developed in order to prove the existence of the game value, obtain a characterization of the value functional, and construct players' optimal feedback strategies."],"url":"http://arxiv.org/abs/2404.10428v1","category":"math.OC"}
{"created":"2024-04-16 09:43:58","title":"Optimizing BioTac Simulation for Realistic Tactile Perception","abstract":"Tactile sensing presents a promising opportunity for enhancing the interaction capabilities of today's robots. BioTac is a commonly used tactile sensor that enables robots to perceive and respond to physical tactile stimuli. However, the sensor's non-linearity poses challenges in simulating its behavior. In this paper, we first investigate a BioTac simulation that uses temperature, force, and contact point positions to predict the sensor outputs. We show that training with BioTac temperature readings does not yield accurate sensor output predictions during deployment. Consequently, we tested three alternative models, i.e., an XGBoost regressor, a neural network, and a transformer encoder. We train these models without temperature readings and provide a detailed investigation of the window size of the input vectors. We demonstrate that we achieve statistically significant improvements over the baseline network. Furthermore, our results reveal that the XGBoost regressor and transformer outperform traditional feed-forward neural networks in this task. We make all our code and results available online on https://github.com/wzaielamri/Optimizing_BioTac_Simulation.","sentences":["Tactile sensing presents a promising opportunity for enhancing the interaction capabilities of today's robots.","BioTac is a commonly used tactile sensor that enables robots to perceive and respond to physical tactile stimuli.","However, the sensor's non-linearity poses challenges in simulating its behavior.","In this paper, we first investigate a BioTac simulation that uses temperature, force, and contact point positions to predict the sensor outputs.","We show that training with BioTac temperature readings does not yield accurate sensor output predictions during deployment.","Consequently, we tested three alternative models, i.e., an XGBoost regressor, a neural network, and a transformer encoder.","We train these models without temperature readings and provide a detailed investigation of the window size of the input vectors.","We demonstrate that we achieve statistically significant improvements over the baseline network.","Furthermore, our results reveal that the XGBoost regressor and transformer outperform traditional feed-forward neural networks in this task.","We make all our code and results available online on https://github.com/wzaielamri/Optimizing_BioTac_Simulation."],"url":"http://arxiv.org/abs/2404.10425v1","category":"cs.RO"}
{"created":"2024-04-16 09:43:56","title":"Symmetries of Quiver schemes","abstract":"We introduce reflection functors on quiver schemes in the sense of Hausel--Wong--Wyss, generalizing those on quiver varieties. Also we construct some isomorphisms between quiver schemes whose underlying quivers are different.","sentences":["We introduce reflection functors on quiver schemes in the sense of Hausel--Wong--Wyss, generalizing those on quiver varieties.","Also we construct some isomorphisms between quiver schemes whose underlying quivers are different."],"url":"http://arxiv.org/abs/2404.10424v1","category":"math.AG"}
{"created":"2024-04-16 09:42:03","title":"Integrating Discrete Sub-grid Filters with Discretization-Corrected Particle Strength Exchange Method for High Reynolds Number Flow Simulations","abstract":"We present a discrete filter for subgrid-scale (SGS) model, coupled with the discretization corrected particle strength exchange (DC-PSE) method for the simulation of three-dimensional viscous incompressible flow, at high Reynolds flows. The majority of turbulence modelling techniques, particularly in complex geometries, face significant computational challenges due to the difficulties in implementing 3-D convolution operations for asymmetric boundary conditions or curved domain boundaries. In this contribution Taylor expansion is used to define differential operators corresponding to the convolution filter, so that the transfer function remains very close to the unity of sizeable displacement in wave number, making the filter a good approximation to the convolution one. A discrete Gaussian filter, in both fourth and second-order forms, was evaluated with varying ratios of particle spacing to the cut-off length. The impact of the filter's order and the ratio's value is thoroughly examined and detailed in the study. Additionally, the Brinkman penalisation technique is employed to impose boundary conditions implicitly, allowing for efficient and accurate flow simulations around complex geometries without the need for modifying the numerical method or computational domain. The incompressible flow is governed by the the he Entropically Damped Artificial Compressibility equations allowing explicit simulation of the incompressible Navier-Stokes equations. The effectiveness of the proposed methodology is validated through several benchmark problems, including isotropic turbulence decay, and flow around four cylinders arranged in a square in-line configuration. These test cases demonstrate the method's accuracy in capturing the intricate flow structures characteristic of high Reynolds number flows, highlighting its applicability to industrial turbulence modeling.","sentences":["We present a discrete filter for subgrid-scale (SGS) model, coupled with the discretization corrected particle strength exchange (DC-PSE) method for the simulation of three-dimensional viscous incompressible flow, at high Reynolds flows.","The majority of turbulence modelling techniques, particularly in complex geometries, face significant computational challenges due to the difficulties in implementing 3-D convolution operations for asymmetric boundary conditions or curved domain boundaries.","In this contribution Taylor expansion is used to define differential operators corresponding to the convolution filter, so that the transfer function remains very close to the unity of sizeable displacement in wave number, making the filter a good approximation to the convolution one.","A discrete Gaussian filter, in both fourth and second-order forms, was evaluated with varying ratios of particle spacing to the cut-off length.","The impact of the filter's order and the ratio's value is thoroughly examined and detailed in the study.","Additionally, the Brinkman penalisation technique is employed to impose boundary conditions implicitly, allowing for efficient and accurate flow simulations around complex geometries without the need for modifying the numerical method or computational domain.","The incompressible flow is governed by the the he Entropically Damped Artificial Compressibility equations allowing explicit simulation of the incompressible Navier-Stokes equations.","The effectiveness of the proposed methodology is validated through several benchmark problems, including isotropic turbulence decay, and flow around four cylinders arranged in a square in-line configuration.","These test cases demonstrate the method's accuracy in capturing the intricate flow structures characteristic of high Reynolds number flows, highlighting its applicability to industrial turbulence modeling."],"url":"http://arxiv.org/abs/2404.10423v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 09:35:27","title":"MAD Speech: Measures of Acoustic Diversity of Speech","abstract":"Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech will be made publicly accessible.","sentences":["Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech.","However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics.","We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech.","We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise.","We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space.","Next, we build a series of datasets with a priori known diversity preferences for each facet.","Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines.","Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios.","MAD Speech will be made publicly accessible."],"url":"http://arxiv.org/abs/2404.10419v1","category":"eess.AS"}
{"created":"2024-04-16 09:34:10","title":"Gradient estimates for positive eigenfunctions of $ \\mathcal{L} $-operator on conformal solitons and its applications","abstract":"We prove a local gradient estimate for positive eigenfunctions of $ \\mathcal{L} $-operator on conformal solitons given by a general conformal vector field. As an application, we obtain a Liouville type theorem for $ \\mathcal{L} u = 0 $, which improves the one of Li--Sun (Acta Math. Sin. (Engl. Ser.), 37(11): 1768--1782, 2021.). We also consider applications where manifolds are special conformal solitons. Especially in the case of self-shrinkers, a better Liouville type theorem is obtained.","sentences":["We prove a local gradient estimate for positive eigenfunctions of $ \\mathcal{L} $-operator on conformal solitons given by a general conformal vector field.","As an application, we obtain a Liouville type theorem for $ \\mathcal{L} u = 0 $, which improves the one of Li--Sun (Acta Math.","Sin.","(Engl.","Ser.), 37(11): 1768--1782, 2021.).","We also consider applications where manifolds are special conformal solitons.","Especially in the case of self-shrinkers, a better Liouville type theorem is obtained."],"url":"http://arxiv.org/abs/2404.10417v1","category":"math.DG"}
{"created":"2024-04-16 09:33:07","title":"Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization","abstract":"Automatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS). Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries. To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process. Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries. Drawing on the insights, we propose a summary candidates fusion framework -- Disentangling Instructive information from Ranked candidates (DIR) for MDSS. Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality. Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder. These variables are further incorporated into the decoder to guide generation. We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation. More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability.","sentences":["Automatically condensing multiple topic-related scientific papers into a succinct and concise summary is referred to as Multi-Document Scientific Summarization (MDSS).","Currently, while commonly used abstractive MDSS methods can generate flexible and coherent summaries, the difficulty in handling global information and the lack of guidance during decoding still make it challenging to generate better summaries.","To alleviate these two shortcomings, this paper introduces summary candidates into MDSS, utilizing the global information of the document set and additional guidance from the summary candidates to guide the decoding process.","Our insights are twofold: Firstly, summary candidates can provide instructive information from both positive and negative perspectives, and secondly, selecting higher-quality candidates from multiple options contributes to producing better summaries.","Drawing on the insights, we propose a summary candidates fusion framework -- Disentangling Instructive information from Ranked candidates (DIR) for MDSS.","Specifically, DIR first uses a specialized pairwise comparison method towards multiple candidates to pick out those of higher quality.","Then DIR disentangles the instructive information of summary candidates into positive and negative latent variables with Conditional Variational Autoencoder.","These variables are further incorporated into the decoder to guide generation.","We evaluate our approach with three different types of Transformer-based models and three different types of candidates, and consistently observe noticeable performance improvements according to automatic and human evaluation.","More analyses further demonstrate the effectiveness of our model in handling global information and enhancing decoding controllability."],"url":"http://arxiv.org/abs/2404.10416v1","category":"cs.AI"}
{"created":"2024-04-16 09:23:36","title":"Time-Dependent Stable Operators","abstract":"We prove that every invertible generalized hyperbolic operator on a Banach space is time-dependent stable.","sentences":["We prove that every invertible generalized hyperbolic operator on a Banach space is time-dependent stable."],"url":"http://arxiv.org/abs/2404.10410v1","category":"math.DS"}
{"created":"2024-04-16 09:19:23","title":"Adversarial Identity Injection for Semantic Face Image Synthesis","abstract":"Nowadays, deep learning models have reached incredible performance in the task of image generation. Plenty of literature works address the task of face generation and editing, with human and automatic systems that struggle to distinguish what's real from generated. Whereas most systems reached excellent visual generation quality, they still face difficulties in preserving the identity of the starting input subject. Among all the explored techniques, Semantic Image Synthesis (SIS) methods, whose goal is to generate an image conditioned on a semantic segmentation mask, are the most promising, even though preserving the perceived identity of the input subject is not their main concern. Therefore, in this paper, we investigate the problem of identity preservation in face image generation and present an SIS architecture that exploits a cross-attention mechanism to merge identity, style, and semantic features to generate faces whose identities are as similar as possible to the input ones. Experimental results reveal that the proposed method is not only suitable for preserving the identity but is also effective in the face recognition adversarial attack, i.e. hiding a second identity in the generated faces.","sentences":["Nowadays, deep learning models have reached incredible performance in the task of image generation.","Plenty of literature works address the task of face generation and editing, with human and automatic systems that struggle to distinguish what's real from generated.","Whereas most systems reached excellent visual generation quality, they still face difficulties in preserving the identity of the starting input subject.","Among all the explored techniques, Semantic Image Synthesis (SIS) methods, whose goal is to generate an image conditioned on a semantic segmentation mask, are the most promising, even though preserving the perceived identity of the input subject is not their main concern.","Therefore, in this paper, we investigate the problem of identity preservation in face image generation and present an SIS architecture that exploits a cross-attention mechanism to merge identity, style, and semantic features to generate faces whose identities are as similar as possible to the input ones.","Experimental results reveal that the proposed method is not only suitable for preserving the identity but is also effective in the face recognition adversarial attack, i.e. hiding a second identity in the generated faces."],"url":"http://arxiv.org/abs/2404.10408v1","category":"cs.CV"}
{"created":"2024-04-16 09:12:16","title":"Integration of Self-Supervised BYOL in Semi-Supervised Medical Image Recognition","abstract":"Image recognition techniques heavily rely on abundant labeled data, particularly in medical contexts. Addressing the challenges associated with obtaining labeled data has led to the prominence of self-supervised learning and semi-supervised learning, especially in scenarios with limited annotated data. In this paper, we proposed an innovative approach by integrating self-supervised learning into semi-supervised models to enhance medical image recognition. Our methodology commences with pre-training on unlabeled data utilizing the BYOL method. Subsequently, we merge pseudo-labeled and labeled datasets to construct a neural network classifier, refining it through iterative fine-tuning. Experimental results on three different datasets demonstrate that our approach optimally leverages unlabeled data, outperforming existing methods in terms of accuracy for medical image recognition.","sentences":["Image recognition techniques heavily rely on abundant labeled data, particularly in medical contexts.","Addressing the challenges associated with obtaining labeled data has led to the prominence of self-supervised learning and semi-supervised learning, especially in scenarios with limited annotated data.","In this paper, we proposed an innovative approach by integrating self-supervised learning into semi-supervised models to enhance medical image recognition.","Our methodology commences with pre-training on unlabeled data utilizing the BYOL method.","Subsequently, we merge pseudo-labeled and labeled datasets to construct a neural network classifier, refining it through iterative fine-tuning.","Experimental results on three different datasets demonstrate that our approach optimally leverages unlabeled data, outperforming existing methods in terms of accuracy for medical image recognition."],"url":"http://arxiv.org/abs/2404.10405v1","category":"cs.CV"}
{"created":"2024-04-16 09:09:58","title":"Sisu: Decentralized Trustless Bridge For Full Ethereum Node","abstract":"In this paper, we present a detailed approach and implementation to prove Ethereum full node using recursive SNARK, distributed general GKR and Groth16. Our protocol's name is Sisu whose architecture is based on distributed Virgo in zkBridge with some major improvements. Besides proving signature aggregation, we provide solutions to 2 hard problems in proving Ethereum full node: 1) any public key is valid under previous beacon state and 2) all public keys are pairwise distinct. Our solution does not require worker-to-worker communication and therefore reduce total worker-to-worker network traffic from terabyte of data to zero compared to zkBridge. This makes our approach suitable for emerging distributed prover markets and more decentralized compared to zkBridge. Our design is highly parallelable and capable of running on GPU for most parts.","sentences":["In this paper, we present a detailed approach and implementation to prove Ethereum full node using recursive SNARK, distributed general GKR and Groth16.","Our protocol's name is Sisu whose architecture is based on distributed Virgo in zkBridge with some major improvements.","Besides proving signature aggregation, we provide solutions to 2 hard problems in proving Ethereum full node: 1) any public key is valid under previous beacon state and 2) all public keys are pairwise distinct.","Our solution does not require worker-to-worker communication and therefore reduce total worker-to-worker network traffic from terabyte of data to zero compared to zkBridge.","This makes our approach suitable for emerging distributed prover markets and more decentralized compared to zkBridge.","Our design is highly parallelable and capable of running on GPU for most parts."],"url":"http://arxiv.org/abs/2404.10404v1","category":"cs.CR"}
{"created":"2024-04-16 08:56:36","title":"Phase space analysis of the evolution of the early universe in Einstein-Cartan theory","abstract":"In this paper, we perform the phase space analysis to investigate the evolution of the early universe in Einstein-Cartan theory. By studying the stability of critical points in dynamical system, it is found that there exist two stable critical points which represent an expanding solution and an Einstein static solution respectively. After analyzing the phase diagram of the dynamical system, we find that there may exist a bouncing universe, an oscillating universe or an Einstein static universe in the early time of universe. In addition, by assuming that the early universe filled by the radiation with $\\omega= 1/3$ , the initial states of the early universe are Einstein static universe or oscillating universe. When the equation of state $\\omega$ decreases with time, the universe can exit from the initial state and evolve into an expanding phase.","sentences":["In this paper, we perform the phase space analysis to investigate the evolution of the early universe in Einstein-Cartan theory.","By studying the stability of critical points in dynamical system, it is found that there exist two stable critical points which represent an expanding solution and an Einstein static solution respectively.","After analyzing the phase diagram of the dynamical system, we find that there may exist a bouncing universe, an oscillating universe or an Einstein static universe in the early time of universe.","In addition, by assuming that the early universe filled by the radiation with $\\omega= 1/3$ , the initial states of the early universe are Einstein static universe or oscillating universe.","When the equation of state $\\omega$ decreases with time, the universe can exit from the initial state and evolve into an expanding phase."],"url":"http://arxiv.org/abs/2404.10400v1","category":"gr-qc"}
{"created":"2024-04-16 08:56:25","title":"FoundationGrasp: Generalizable Task-Oriented Grasping with Foundation Models","abstract":"Task-oriented grasping (TOG), which refers to the problem of synthesizing grasps on an object that are configurationally compatible with the downstream manipulation task, is the first milestone towards tool manipulation. Analogous to the activation of two brain regions responsible for semantic and geometric reasoning during cognitive processes, modeling the complex relationship between objects, tasks, and grasps requires rich prior knowledge about objects and tasks. Existing methods typically limit the prior knowledge to a closed-set scope and cannot support the generalization to novel objects and tasks out of the training set. To address such a limitation, we propose FoundationGrasp, a foundation model-based TOG framework that leverages the open-ended knowledge from foundation models to learn generalizable TOG skills. Comprehensive experiments are conducted on the contributed Language and Vision Augmented TaskGrasp (LaViA-TaskGrasp) dataset, demonstrating the superiority of FoudationGrasp over existing methods when generalizing to novel object instances, object classes, and tasks out of the training set. Furthermore, the effectiveness of FoudationGrasp is validated in real-robot grasping and manipulation experiments on a 7 DoF robotic arm. Our code, data, appendix, and video are publicly available at https://sites.google.com/view/foundationgrasp.","sentences":["Task-oriented grasping (TOG), which refers to the problem of synthesizing grasps on an object that are configurationally compatible with the downstream manipulation task, is the first milestone towards tool manipulation.","Analogous to the activation of two brain regions responsible for semantic and geometric reasoning during cognitive processes, modeling the complex relationship between objects, tasks, and grasps requires rich prior knowledge about objects and tasks.","Existing methods typically limit the prior knowledge to a closed-set scope and cannot support the generalization to novel objects and tasks out of the training set.","To address such a limitation, we propose FoundationGrasp, a foundation model-based TOG framework that leverages the open-ended knowledge from foundation models to learn generalizable TOG skills.","Comprehensive experiments are conducted on the contributed Language and Vision Augmented TaskGrasp (LaViA-TaskGrasp) dataset, demonstrating the superiority of FoudationGrasp over existing methods when generalizing to novel object instances, object classes, and tasks out of the training set.","Furthermore, the effectiveness of FoudationGrasp is validated in real-robot grasping and manipulation experiments on a 7 DoF robotic arm.","Our code, data, appendix, and video are publicly available at https://sites.google.com/view/foundationgrasp."],"url":"http://arxiv.org/abs/2404.10399v1","category":"cs.RO"}
{"created":"2024-04-16 08:56:00","title":"Problem of eigenvalues of stochastic Hamiltonian systems with boundary conditions and Markov chain","abstract":"In this paper, we study the eigenvalue problem of stochastic Hamiltonian system driven by Brownian motion and Markov chain with boundary conditions and time-dependent coefficients. For any dimensional case, the existence of the first eigenvalue is proven and the corresponding eigenfunctions are constructed by virtue of dual transformation and generalized Riccati equation system. Furthermore, we have more finely characterized the existence of all eigenvalues and constructed the related eigenfunctions for one-dimensional Hamiltonian system. Moreover, the increasing order of these eigenvalues have also been given.","sentences":["In this paper, we study the eigenvalue problem of stochastic Hamiltonian system driven by Brownian motion and Markov chain with boundary conditions and time-dependent coefficients.","For any dimensional case, the existence of the first eigenvalue is proven and the corresponding eigenfunctions are constructed by virtue of dual transformation and generalized Riccati equation system.","Furthermore, we have more finely characterized the existence of all eigenvalues and constructed the related eigenfunctions for one-dimensional Hamiltonian system.","Moreover, the increasing order of these eigenvalues have also been given."],"url":"http://arxiv.org/abs/2404.10398v1","category":"math.PR"}
{"created":"2024-04-16 08:53:25","title":"Spline-Interpolated Model Predictive Path Integral Control with Stein Variational Inference for Reactive Navigation","abstract":"This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD). The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach. The efficacy of MPPI is significantly influenced by the sampling noise. To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise. However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories. To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises. Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution. To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation. SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution. Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences. The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques.","sentences":["This paper presents a reactive navigation method that leverages a Model Predictive Path Integral (MPPI) control enhanced with spline interpolation for the control input sequence and Stein Variational Gradient Descent (SVGD).","The MPPI framework addresses a nonlinear optimization problem by determining an optimal sequence of control inputs through a sampling-based approach.","The efficacy of MPPI is significantly influenced by the sampling noise.","To rapidly identify routes that circumvent large and/or newly detected obstacles, it is essential to employ high levels of sampling noise.","However, such high noise levels result in jerky control input sequences, leading to non-smooth trajectories.","To mitigate this issue, we propose the integration of spline interpolation within the MPPI process, enabling the generation of smooth control input sequences despite the utilization of substantial sampling noises.","Nonetheless, the standard MPPI algorithm struggles in scenarios featuring multiple optimal or near-optimal solutions, such as environments with several viable obstacle avoidance paths, due to its assumption that the distribution over an optimal control input sequence can be closely approximated by a Gaussian distribution.","To address this limitation, we extend our method by incorporating SVGD into the MPPI framework with spline interpolation.","SVGD, rooted in the optimal transportation algorithm, possesses the unique ability to cluster samples around an optimal solution.","Consequently, our approach facilitates robust reactive navigation by swiftly identifying obstacle avoidance paths while maintaining the smoothness of the control input sequences.","The efficacy of our proposed method is validated on simulations with a quadrotor, demonstrating superior performance over existing baseline techniques."],"url":"http://arxiv.org/abs/2404.10395v1","category":"cs.RO"}
{"created":"2024-04-16 08:52:42","title":"Portrait3D: Text-Guided High-Quality 3D Portrait Generation Using Pyramid Representation and GANs Prior","abstract":"Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance. However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing. We present Portrait3D, a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues. To accomplish this, we train a 3D portrait generator, 3DPortraitGAN-Pyramid, as a robust prior. This generator is capable of producing 360{\\deg} canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process. To mitigate the \"grid-like\" artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel pyramid tri-grid 3D representation into 3DPortraitGAN-Pyramid. To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN-Pyramid's latent space. The resulting latent code is then used to synthesize a pyramid tri-grid. Beginning with the obtained pyramid tri-grid, we use score distillation sampling to distill the diffusion model's knowledge into the pyramid tri-grid. Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the pyramid tri-grid, effectively eliminating issues with unrealistic color and unnatural artifacts. Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt.","sentences":["Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance.","However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing.","We present Portrait3D, a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues.","To accomplish this, we train a 3D portrait generator, 3DPortraitGAN-Pyramid, as a robust prior.","This generator is capable of producing 360{\\deg} canonical 3D portraits, serving as a starting point for the subsequent diffusion-based generation process.","To mitigate the \"grid-like\" artifact caused by the high-frequency information in the feature-map-based 3D representation commonly used by most 3D-aware GANs, we integrate a novel pyramid tri-grid 3D representation into 3DPortraitGAN-Pyramid.","To generate 3D portraits from text, we first project a randomly generated image aligned with the given prompt into the pre-trained 3DPortraitGAN-Pyramid's latent space.","The resulting latent code is then used to synthesize a pyramid tri-grid.","Beginning with the obtained pyramid tri-grid, we use score distillation sampling to distill the diffusion model's knowledge into the pyramid tri-grid.","Following that, we utilize the diffusion model to refine the rendered images of the 3D portrait and then use these refined images as training data to further optimize the pyramid tri-grid, effectively eliminating issues with unrealistic color and unnatural artifacts.","Our experimental results show that Portrait3D can produce realistic, high-quality, and canonical 3D portraits that align with the prompt."],"url":"http://arxiv.org/abs/2404.10394v1","category":"cs.CV"}
{"created":"2024-04-16 08:48:46","title":"Offline Trajectory Generalization for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories. Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts. However, these methods suffer from (i) poor generalization to unseen states; and (ii) trivial improvement from low-qualified rollout simulation. In this paper, we propose offline trajectory generalization through world transformers for offline reinforcement learning (OTTO). Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward. Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data. Finally, we jointly use offline data with simulated data to train an offline RL algorithm. OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation. Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods.","sentences":["Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories.","Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts.","However, these methods suffer from (i) poor generalization to unseen states; and (ii) trivial improvement from low-qualified rollout simulation.","In this paper, we propose offline trajectory generalization through world transformers for offline reinforcement learning (OTTO).","Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward.","Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data.","Finally, we jointly use offline data with simulated data to train an offline RL algorithm.","OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation.","Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods."],"url":"http://arxiv.org/abs/2404.10393v1","category":"cs.LG"}
{"created":"2024-04-16 08:48:10","title":"Generating 6-D Trajectories for Omnidirectional Multirotor Aerial Vehicles in Cluttered Environments","abstract":"As fully-actuated systems, omnidirectional multirotor aerial vehicles (OMAVs) have more flexible maneuverability and advantages in aggressive flight in cluttered environments than traditional underactuated MAVs. %Due to the high dimensionality of configuration space, making the designed trajectory generation algorithm efficient is challenging. This paper aims to achieve safe flight of OMAVs in cluttered environments. Considering existing static obstacles, an efficient optimization-based framework is proposed to generate 6-D $SE(3)$ trajectories for OMAVs. Given the kinodynamic constraints and the 3D collision-free region represented by a series of intersecting convex polyhedra, the proposed method finally generates a safe and dynamically feasible 6-D trajectory. First, we parameterize the vehicle's attitude into a free 3D vector using stereographic projection to eliminate the constraints inherent in the $SO(3)$ manifold, while the complete $SE(3)$ trajectory is represented as a 6-D polynomial in time without inherent constraints. The vehicle's shape is modeled as a cuboid attached to the body frame to achieve whole-body collision evaluation. Then, we formulate the origin trajectory generation problem as a constrained optimization problem. The original constrained problem is finally transformed into an unconstrained one that can be solved efficiently. To verify the proposed framework's performance, simulations and real-world experiments based on a tilt-rotor hexarotor aerial vehicle are carried out.","sentences":["As fully-actuated systems, omnidirectional multirotor aerial vehicles (OMAVs) have more flexible maneuverability and advantages in aggressive flight in cluttered environments than traditional underactuated MAVs.","%Due to the high dimensionality of configuration space, making the designed trajectory generation algorithm efficient is challenging.","This paper aims to achieve safe flight of OMAVs in cluttered environments.","Considering existing static obstacles, an efficient optimization-based framework is proposed to generate 6-D $SE(3)$ trajectories for OMAVs.","Given the kinodynamic constraints and the 3D collision-free region represented by a series of intersecting convex polyhedra, the proposed method finally generates a safe and dynamically feasible 6-D trajectory.","First, we parameterize the vehicle's attitude into a free 3D vector using stereographic projection to eliminate the constraints inherent in the $SO(3)$ manifold, while the complete $SE(3)$ trajectory is represented as a 6-D polynomial in time without inherent constraints.","The vehicle's shape is modeled as a cuboid attached to the body frame to achieve whole-body collision evaluation.","Then, we formulate the origin trajectory generation problem as a constrained optimization problem.","The original constrained problem is finally transformed into an unconstrained one that can be solved efficiently.","To verify the proposed framework's performance, simulations and real-world experiments based on a tilt-rotor hexarotor aerial vehicle are carried out."],"url":"http://arxiv.org/abs/2404.10392v1","category":"cs.RO"}
{"created":"2024-04-16 08:39:29","title":"CNN-based explanation ensembling for dataset, representation and explanations evaluation","abstract":"Explainable Artificial Intelligence has gained significant attention due to the widespread use of complex deep learning models in high-stake domains such as medicine, finance, and autonomous cars. However, different explanations often present different aspects of the model's behavior. In this research manuscript, we explore the potential of ensembling explanations generated by deep classification models using convolutional model. Through experimentation and analysis, we aim to investigate the implications of combining explanations to uncover a more coherent and reliable patterns of the model's behavior, leading to the possibility of evaluating the representation learned by the model. With our method, we can uncover problems of under-representation of images in a certain class. Moreover, we discuss other side benefits like features' reduction by replacing the original image with its explanations resulting in the removal of some sensitive information. Through the use of carefully selected evaluation metrics from the Quantus library, we demonstrated the method's superior performance in terms of Localisation and Faithfulness, compared to individual explanations.","sentences":["Explainable Artificial Intelligence has gained significant attention due to the widespread use of complex deep learning models in high-stake domains such as medicine, finance, and autonomous cars.","However, different explanations often present different aspects of the model's behavior.","In this research manuscript, we explore the potential of ensembling explanations generated by deep classification models using convolutional model.","Through experimentation and analysis, we aim to investigate the implications of combining explanations to uncover a more coherent and reliable patterns of the model's behavior, leading to the possibility of evaluating the representation learned by the model.","With our method, we can uncover problems of under-representation of images in a certain class.","Moreover, we discuss other side benefits like features' reduction by replacing the original image with its explanations resulting in the removal of some sensitive information.","Through the use of carefully selected evaluation metrics from the Quantus library, we demonstrated the method's superior performance in terms of Localisation and Faithfulness, compared to individual explanations."],"url":"http://arxiv.org/abs/2404.10387v1","category":"cs.AI"}
{"created":"2024-04-16 08:37:36","title":"I/O in Machine Learning Applications on HPC Systems: A 360-degree Survey","abstract":"High-Performance Computing (HPC) systems excel in managing distributed workloads, and the growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference. In the past, research on HPC I/O focused on optimizing the underlying storage system for modeling and simulation applications and checkpointing the results, causing writes to be the dominant I/O operation. These applications typically access large portions of the data written by simulations or experiments. ML workloads, in contrast, perform small I/O reads spread across a large number of random files. This shift of I/O access patterns poses several challenges to HPC storage systems. In this paper, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024. We provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during ML training, explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature, and lastly, present gaps requiring further R&D. We seek to summarize the common practices used in accessing data by ML applications and expose research gaps that could spawn further R&D.","sentences":["High-Performance Computing (HPC) systems excel in managing distributed workloads, and the growing interest in Artificial Intelligence (AI) has resulted in a surge in demand for faster methods of Machine Learning (ML) model training and inference.","In the past, research on HPC I/O focused on optimizing the underlying storage system for modeling and simulation applications and checkpointing the results, causing writes to be the dominant I/O operation.","These applications typically access large portions of the data written by simulations or experiments.","ML workloads, in contrast, perform small I/O reads spread across a large number of random files.","This shift of I/O access patterns poses several challenges to HPC storage systems.","In this paper, we survey I/O in ML applications on HPC systems, and target literature within a 6-year time window from 2019 to 2024.","We provide an overview of the common phases of ML, review available profilers and benchmarks, examine the I/O patterns encountered during ML training, explore I/O optimizations utilized in modern ML frameworks and proposed in recent literature, and lastly, present gaps requiring further R&D.","We seek to summarize the common practices used in accessing data by ML applications and expose research gaps that could spawn further R&D."],"url":"http://arxiv.org/abs/2404.10386v1","category":"cs.DC"}
{"created":"2024-04-16 08:28:16","title":"Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering","abstract":"Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform surprisingly well and outperform human experts on many tasks. However, in many domain-specific evaluations, these LLMs often suffer from hallucination problems due to insufficient training of relevant corpus. Furthermore, fine-tuning large models may face problems such as the LLMs are not open source or the construction of high-quality domain instruction is difficult. Therefore, structured knowledge databases such as knowledge graph can better provide domain back- ground knowledge for LLMs and make full use of the reasoning and analysis capabilities of LLMs. In some previous works, LLM was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question. Especially for the question that require a multi-hop reasoning path, frequent calls to LLM will consume a lot of computing power. Moreover, when choosing the reasoning path, LLM will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps. In this paper, we integrated and optimized a pipeline for selecting reasoning paths from KG based on LLM, which can reduce the dependency on LLM. In addition, we propose a simple and effective subgraph retrieval method based on chain of thought (CoT) and page rank which can returns the paths most likely to contain the answer. We conduct experiments on three datasets: GenMedGPT-5k [14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using fewer LLM calls can achieve the same results as previous SOTAs models.","sentences":["Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform surprisingly well and outperform human experts on many tasks.","However, in many domain-specific evaluations, these LLMs often suffer from hallucination problems due to insufficient training of relevant corpus.","Furthermore, fine-tuning large models may face problems such as the LLMs are not open source or the construction of high-quality domain instruction is difficult.","Therefore, structured knowledge databases such as knowledge graph can better provide domain back- ground knowledge for LLMs and make full use of the reasoning and analysis capabilities of LLMs.","In some previous works, LLM was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question.","Especially for the question that require a multi-hop reasoning path, frequent calls to LLM will consume a lot of computing power.","Moreover, when choosing the reasoning path, LLM will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps.","In this paper, we integrated and optimized a pipeline for selecting reasoning paths from KG based on LLM, which can reduce the dependency on LLM.","In addition, we propose a simple and effective subgraph retrieval method based on chain of thought (CoT) and page rank which can returns the paths most likely to contain the answer.","We conduct experiments on three datasets: GenMedGPT-5k","[14], WebQuestions [2], and CMCQA","[21].","Finally, RoK can demonstrate that using fewer LLM calls can achieve the same results as previous SOTAs models."],"url":"http://arxiv.org/abs/2404.10384v1","category":"cs.CL"}
{"created":"2024-04-16 08:15:10","title":"Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data","abstract":"Synthetic data is gaining increasing relevance for training machine learning models. This is mainly motivated due to several factors such as the lack of real data and intra-class variability, time and errors produced in manual labeling, and in some cases privacy concerns, among others. This paper presents an overview of the 2nd edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at CVPR 2024. FRCSyn aims to investigate the use of synthetic data in face recognition to address current technological limitations, including data privacy concerns, demographic biases, generalization to novel scenarios, and performance constraints in challenging situations such as aging, pose variations, and occlusions. Unlike the 1st edition, in which synthetic data from DCFace and GANDiffFace methods was only allowed to train face recognition systems, in this 2nd edition we propose new sub-tasks that allow participants to explore novel face generative methods. The outcomes of the 2nd FRCSyn Challenge, along with the proposed experimental protocol and benchmarking contribute significantly to the application of synthetic data to face recognition.","sentences":["Synthetic data is gaining increasing relevance for training machine learning models.","This is mainly motivated due to several factors such as the lack of real data and intra-class variability, time and errors produced in manual labeling, and in some cases privacy concerns, among others.","This paper presents an overview of the 2nd edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at CVPR 2024.","FRCSyn aims to investigate the use of synthetic data in face recognition to address current technological limitations, including data privacy concerns, demographic biases, generalization to novel scenarios, and performance constraints in challenging situations such as aging, pose variations, and occlusions.","Unlike the 1st edition, in which synthetic data from DCFace and GANDiffFace methods was only allowed to train face recognition systems, in this 2nd edition we propose new sub-tasks that allow participants to explore novel face generative methods.","The outcomes of the 2nd FRCSyn Challenge, along with the proposed experimental protocol and benchmarking contribute significantly to the application of synthetic data to face recognition."],"url":"http://arxiv.org/abs/2404.10378v1","category":"cs.CV"}
{"created":"2024-04-16 08:12:56","title":"A Methodology of Cooperative Driving based on Microscopic Traffic Prediction","abstract":"We present a methodology of cooperative driving in vehicular traffic, in which for short-time traffic prediction rather than one of the statistical approaches of artificial intelligence (AI), we follow a qualitative different microscopic traffic prediction approach developed recently [Phys. Rev. E 106 (2022) 044307]. In the microscopic traffic prediction approach used for the planning of the subject vehicle trajectory, no learning algorithms of AI are applied; instead, microscopic traffic modeling based on the physics of vehicle motion is used. The presented methodology of cooperative driving is devoted to application cases in which microscopic traffic prediction without cooperative driving cannot lead to a successful vehicle control and trajectory planning. For the understanding of the physical features of the methodology of cooperative driving, a traffic city scenario has been numerically studied, in which a subject vehicle, which requires cooperative driving, is an automated vehicle. Based on microscopic traffic prediction, in the methodology first a cooperating vehicle(s) is found; then, motion requirements for the cooperating vehicle(s) and characteristics of automated vehicle control are predicted and used for vehicle motion; to update predicted characteristics of vehicle motion, calculations of the predictions of motion requirements for the cooperating vehicle and automated vehicle control are repeated for each next time instant at which new measured data for current microscopic traffic situation are available. With the use of microscopic traffic simulations, the evaluation of the applicability of this methodology is illustrated for a simple case of unsignalized city intersection, when the automated vehicle wants to turn right from a secondary road onto the priority road.","sentences":["We present a methodology of cooperative driving in vehicular traffic, in which for short-time traffic prediction rather than one of the statistical approaches of artificial intelligence (AI), we follow a qualitative different microscopic traffic prediction approach developed recently [Phys.","Rev. E 106 (2022) 044307].","In the microscopic traffic prediction approach used for the planning of the subject vehicle trajectory, no learning algorithms of AI are applied; instead, microscopic traffic modeling based on the physics of vehicle motion is used.","The presented methodology of cooperative driving is devoted to application cases in which microscopic traffic prediction without cooperative driving cannot lead to a successful vehicle control and trajectory planning.","For the understanding of the physical features of the methodology of cooperative driving, a traffic city scenario has been numerically studied, in which a subject vehicle, which requires cooperative driving, is an automated vehicle.","Based on microscopic traffic prediction, in the methodology first a cooperating vehicle(s) is found; then, motion requirements for the cooperating vehicle(s) and characteristics of automated vehicle control are predicted and used for vehicle motion; to update predicted characteristics of vehicle motion, calculations of the predictions of motion requirements for the cooperating vehicle and automated vehicle control are repeated for each next time instant at which new measured data for current microscopic traffic situation are available.","With the use of microscopic traffic simulations, the evaluation of the applicability of this methodology is illustrated for a simple case of unsignalized city intersection, when the automated vehicle wants to turn right from a secondary road onto the priority road."],"url":"http://arxiv.org/abs/2404.10375v1","category":"physics.soc-ph"}
{"created":"2024-04-16 08:12:03","title":"Enjeux normatifs des TICE de l'enseignement des langues dans le contexte arabo-berb{\u00e8}re","abstract":"E-learning is becoming a global phenomenon. Learning Arabic (or Arabic dialects), or learning one or several variants of Berber can be understood from a very local perspective (in the Maghreb for instance) or in the wider framework of the diaspora or even more broadly in a global world context (in case a Japanese or a Russian learns Arabic and Berber). Resources for distance learning must then be created and potentially used in any international cultural and linguistic context. This implies that the resources created for such perspective should cope with the general standards framework of the ISO / IEC JTC1SC36, and even beyond the scope of this standardization instance.","sentences":["E-learning is becoming a global phenomenon.","Learning Arabic (or Arabic dialects), or learning one or several variants of Berber can be understood from a very local perspective (in the Maghreb for instance) or in the wider framework of the diaspora or even more broadly in a global world context (in case a Japanese or a Russian learns Arabic and Berber).","Resources for distance learning must then be created and potentially used in any international cultural and linguistic context.","This implies that the resources created for such perspective should cope with the general standards framework of the ISO / IEC JTC1SC36, and even beyond the scope of this standardization instance."],"url":"http://arxiv.org/abs/2404.10374v1","category":"cs.CY"}
{"created":"2024-04-16 08:07:46","title":"Biological computations: limitations of attractor-based formalisms and the need for transients","abstract":"Living systems, from single cells to higher vertebrates, receive a continuous stream of non-stationary inputs that they sense, e.g., via cell surface receptors or sensory organs. Integrating these time-varying, multi-sensory, and often noisy information with memory using complex molecular or neuronal networks, they generate a variety of responses beyond simple stimulus-response association, including avoidance behavior, life-long-learning or social interactions. In a broad sense, these processes can be understood as a type of biological computation. Taking as a basis generic features of biological computations, such as real-time responsiveness or robustness and flexibility of the computation, we highlight the limitations of the current attractor-based framework for understanding computations in biological systems. We argue that frameworks based on transient dynamics away from attractors are better suited for the description of computations performed by neuronal and signaling networks. In particular, we discuss how quasi-stable transient dynamics from ghost states that emerge at criticality have a promising potential for developing an integrated framework of computations, that can help us understand how living system actively process information and learn from their continuously changing environment.","sentences":["Living systems, from single cells to higher vertebrates, receive a continuous stream of non-stationary inputs that they sense, e.g., via cell surface receptors or sensory organs.","Integrating these time-varying, multi-sensory, and often noisy information with memory using complex molecular or neuronal networks, they generate a variety of responses beyond simple stimulus-response association, including avoidance behavior, life-long-learning or social interactions.","In a broad sense, these processes can be understood as a type of biological computation.","Taking as a basis generic features of biological computations, such as real-time responsiveness or robustness and flexibility of the computation, we highlight the limitations of the current attractor-based framework for understanding computations in biological systems.","We argue that frameworks based on transient dynamics away from attractors are better suited for the description of computations performed by neuronal and signaling networks.","In particular, we discuss how quasi-stable transient dynamics from ghost states that emerge at criticality have a promising potential for developing an integrated framework of computations, that can help us understand how living system actively process information and learn from their continuously changing environment."],"url":"http://arxiv.org/abs/2404.10369v1","category":"q-bio.OT"}
{"created":"2024-04-16 07:55:56","title":"The lowest discriminant ideal of central extensions of Abelian groups","abstract":"In a previous joint paper with Wu and Yakimov, we gave an explicit description of the lowest discriminant ideal in a Cayley-Hamilton Hopf algebra (H,C,tr) of degree d over an algebraically closed field k, char k $\\notin[1, d]$ with basic identity fiber, i.e. all irreducible representations over the kernel of the counit of the central Hopf subalgebra C are one-dimensional. Using results developed in that paper, we compute relevant quantities associated with irreducible representations to explicitly describe the zero set of the lowest discriminant ideal in the group algebra of a central extension of the product of two arbitrary finitely generated Abelian groups by any finite Abelian group under some conditions. Over a fixed maximal ideal of C the representations are tensor products of representations each corresponding to a central extension of a subgroup isomorphic to the product of two cyclic groups of the same order. A description of the orbit of the identity, i.e. the kernel of the counit of C, under winding automorphisms is also given.","sentences":["In a previous joint paper with Wu and Yakimov, we gave an explicit description of the lowest discriminant ideal in a Cayley-Hamilton Hopf algebra (H,C,tr) of degree d over an algebraically closed field k, char k $\\notin[1, d]$ with basic identity fiber, i.e. all irreducible representations over the kernel of the counit of the central Hopf subalgebra C are one-dimensional.","Using results developed in that paper, we compute relevant quantities associated with irreducible representations to explicitly describe the zero set of the lowest discriminant ideal in the group algebra of a central extension of the product of two arbitrary finitely generated Abelian groups by any finite Abelian group under some conditions.","Over a fixed maximal ideal of C the representations are tensor products of representations each corresponding to a central extension of a subgroup isomorphic to the product of two cyclic groups of the same order.","A description of the orbit of the identity, i.e. the kernel of the counit of C, under winding automorphisms is also given."],"url":"http://arxiv.org/abs/2404.10366v1","category":"math.RT"}
{"created":"2024-04-16 07:55:34","title":"Learning Wireless Data Knowledge Graph for Green Intelligent Communications: Methodology and Experiments","abstract":"Intelligent communications have played a pivotal role in shaping the evolution of 6G networks. Native artificial intelligence (AI) within green communication systems must meet stringent real-time requirements. To achieve this, deploying lightweight and resource-efficient AI models is necessary. However, as wireless networks generate a multitude of data fields and indicators during operation, only a fraction of them imposes significant impact on the network AI models. Therefore, real-time intelligence of communication systems heavily relies on a small but critical set of the data that profoundly influences the performance of network AI models. These challenges underscore the need for innovative architectures and solutions. In this paper, we propose a solution, termed the pervasive multi-level (PML) native AI architecture, which integrates the concept of knowledge graph (KG) into the intelligent operational manipulations of mobile networks, resulting in the establishment of a wireless data KG. Leveraging the wireless data KG, we characterize the massive and complex data collected from wireless communication networks and analyze the relationships among various data fields. The obtained graph of data field relations enables the on-demand generation of minimal and effective datasets, referred to as feature datasets, tailored to specific application requirements. Consequently, this architecture not only enhances AI training, inference, and validation processes but also significantly reduces resource wastage and overhead for communication networks. To implement this architecture, we have developed a specific solution comprising a spatio-temporal heterogeneous graph attention neural network model (STREAM) as well as a feature dataset generation algorithm. Experiments are conducted to validate the effectiveness of the proposed architecture.","sentences":["Intelligent communications have played a pivotal role in shaping the evolution of 6G networks.","Native artificial intelligence (AI) within green communication systems must meet stringent real-time requirements.","To achieve this, deploying lightweight and resource-efficient AI models is necessary.","However, as wireless networks generate a multitude of data fields and indicators during operation, only a fraction of them imposes significant impact on the network AI models.","Therefore, real-time intelligence of communication systems heavily relies on a small but critical set of the data that profoundly influences the performance of network AI models.","These challenges underscore the need for innovative architectures and solutions.","In this paper, we propose a solution, termed the pervasive multi-level (PML) native AI architecture, which integrates the concept of knowledge graph (KG) into the intelligent operational manipulations of mobile networks, resulting in the establishment of a wireless data KG.","Leveraging the wireless data KG, we characterize the massive and complex data collected from wireless communication networks and analyze the relationships among various data fields.","The obtained graph of data field relations enables the on-demand generation of minimal and effective datasets, referred to as feature datasets, tailored to specific application requirements.","Consequently, this architecture not only enhances AI training, inference, and validation processes but also significantly reduces resource wastage and overhead for communication networks.","To implement this architecture, we have developed a specific solution comprising a spatio-temporal heterogeneous graph attention neural network model (STREAM) as well as a feature dataset generation algorithm.","Experiments are conducted to validate the effectiveness of the proposed architecture."],"url":"http://arxiv.org/abs/2404.10365v1","category":"cs.NI"}
{"created":"2024-04-16 07:55:33","title":"Quantum Gravity in Flat Spacetime","abstract":"Inspired by Einstein's Strong Principle of Equivalence we consider the effects of quantum mechanics to the gravity-like phenomena experienced by an observer in a uniformly accelerating motion in flat spacetime. Among other things, our model of quantum gravity, derived from the first principles, predicts the Unruh effect, and a discrete area spectrum for spacelike two-surfaces.","sentences":["Inspired by Einstein's Strong Principle of Equivalence we consider the effects of quantum mechanics to the gravity-like phenomena experienced by an observer in a uniformly accelerating motion in flat spacetime.","Among other things, our model of quantum gravity, derived from the first principles, predicts the Unruh effect, and a discrete area spectrum for spacelike two-surfaces."],"url":"http://arxiv.org/abs/2404.10364v1","category":"gr-qc"}
{"created":"2024-04-16 07:53:28","title":"3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers","abstract":"Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages. Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted. However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use.   In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle. Symbolic test generation also helps in distinguishing multiple plausible solutions. Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C.   We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified C code at a non-trivial scale. A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated, symbolic analysis is tractable.","sentences":["Improper parsing of attacker-controlled input is a leading source of software security vulnerabilities, especially when programmers transcribe informal format descriptions in RFCs into efficient parsing logic in low-level, memory unsafe languages.","Several researchers have proposed formal specification languages for data formats from which efficient code can be extracted.","However, distilling informal requirements into formal specifications is challenging and, despite their benefits, new, formal languages are hard for people to learn and use.   ","In this work, we present 3DGen, a framework that makes use of AI agents to transform mixed informal input, including natural language documents (i.e., RFCs) and example inputs into format specifications in a language called 3D. To support humans in understanding and trusting the generated specifications, 3DGen uses symbolic methods to also synthesize test inputs that can be validated against an external oracle.","Symbolic test generation also helps in distinguishing multiple plausible solutions.","Through a process of repeated refinement, 3DGen produces a 3D specification that conforms to a test suite, and which yields safe, efficient, provably correct, parsing code in C.   We have evaluated 3DGen on 20 Internet standard formats, demonstrating the potential for AI-agents to produce formally verified C code at a non-trivial scale.","A key enabler is the use of a domain-specific language to limit AI outputs to a class for which automated, symbolic analysis is tractable."],"url":"http://arxiv.org/abs/2404.10362v1","category":"cs.SE"}
{"created":"2024-04-16 07:52:53","title":"On Markov-dependent reflected autoregressive processes","abstract":"In this work, we study Markov-dependent reflected autoregressive processes. In queueing terms, such processes describe the workload just before a customer arrival, which makes obsolete a fraction of the work already present, and where the interarrival time and the service time depend on a common discrete time Markov chain. Among others, we consider the case where given the state of the underlying Markov chain, the interarrival time and the service time are independent, as well as the case where there is also additional dependence based on the Farlie-Gumbel-Morgenstern copula, the case where the service time depends on the waiting time of the customer, and the modulated shot-noise queue where server's speed is workload proportional. The Laplace-Stieltjes transform of the steady-state workload is derived via a recursive approach. Moreover, we further consider a Markov-modulated version of the reflected autoregressive process with more general structure and obtain the Laplace-Stieltjes transform of the transient distribution of the workload.","sentences":["In this work, we study Markov-dependent reflected autoregressive processes.","In queueing terms, such processes describe the workload just before a customer arrival, which makes obsolete a fraction of the work already present, and where the interarrival time and the service time depend on a common discrete time Markov chain.","Among others, we consider the case where given the state of the underlying Markov chain, the interarrival time and the service time are independent, as well as the case where there is also additional dependence based on the Farlie-Gumbel-Morgenstern copula, the case where the service time depends on the waiting time of the customer, and the modulated shot-noise queue where server's speed is workload proportional.","The Laplace-Stieltjes transform of the steady-state workload is derived via a recursive approach.","Moreover, we further consider a Markov-modulated version of the reflected autoregressive process with more general structure and obtain the Laplace-Stieltjes transform of the transient distribution of the workload."],"url":"http://arxiv.org/abs/2404.10361v1","category":"math.PR"}
{"created":"2024-04-16 07:46:55","title":"Improving Bracket Image Restoration and Enhancement with Flow-guided Alignment and Enhanced Feature Aggregation","abstract":"In this paper, we address the Bracket Image Restoration and Enhancement (BracketIRE) task using a novel framework, which requires restoring a high-quality high dynamic range (HDR) image from a sequence of noisy, blurred, and low dynamic range (LDR) multi-exposure RAW inputs. To overcome this challenge, we present the IREANet, which improves the multiple exposure alignment and aggregation with a Flow-guide Feature Alignment Module (FFAM) and an Enhanced Feature Aggregation Module (EFAM). Specifically, the proposed FFAM incorporates the inter-frame optical flow as guidance to facilitate the deformable alignment and spatial attention modules for better feature alignment. The EFAM further employs the proposed Enhanced Residual Block (ERB) as a foundational component, wherein a unidirectional recurrent network aggregates the aligned temporal features to better reconstruct the results. To improve model generalization and performance, we additionally employ the Bayer preserving augmentation (BayerAug) strategy to augment the multi-exposure RAW inputs. Our experimental evaluations demonstrate that the proposed IREANet shows state-of-the-art performance compared with previous methods.","sentences":["In this paper, we address the Bracket Image Restoration and Enhancement (BracketIRE) task using a novel framework, which requires restoring a high-quality high dynamic range (HDR) image from a sequence of noisy, blurred, and low dynamic range (LDR) multi-exposure RAW inputs.","To overcome this challenge, we present the IREANet, which improves the multiple exposure alignment and aggregation with a Flow-guide Feature Alignment Module (FFAM) and an Enhanced Feature Aggregation Module (EFAM).","Specifically, the proposed FFAM incorporates the inter-frame optical flow as guidance to facilitate the deformable alignment and spatial attention modules for better feature alignment.","The EFAM further employs the proposed Enhanced Residual Block (ERB) as a foundational component, wherein a unidirectional recurrent network aggregates the aligned temporal features to better reconstruct the results.","To improve model generalization and performance, we additionally employ the Bayer preserving augmentation (BayerAug) strategy to augment the multi-exposure RAW inputs.","Our experimental evaluations demonstrate that the proposed IREANet shows state-of-the-art performance compared with previous methods."],"url":"http://arxiv.org/abs/2404.10358v1","category":"cs.CV"}
{"created":"2024-04-16 07:44:52","title":"Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models","abstract":"Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications. To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential. However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules. This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks. To address this challenge, we propose Context Optimization with Multi-Knowledge Representation (CoKnow), a framework that enhances Prompt Learning for VLMs with rich contextual knowledge. To facilitate CoKnow during inference, we trained lightweight semantic knowledge mappers, which are capable of generating Multi-Knowledge Representation for an input image without requiring additional priors. Experimentally, We conducted extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods. We will make all resources open-source: https://github.com/EMZucas/CoKnow.","sentences":["Vision-Language Models (VLMs), such as CLIP, play a foundational role in various cross-modal applications.","To fully leverage VLMs' potential in adapting to downstream tasks, context optimization methods like Prompt Tuning are essential.","However, one key limitation is the lack of diversity in prompt templates, whether they are hand-crafted or learned through additional modules.","This limitation restricts the capabilities of pretrained VLMs and can result in incorrect predictions in downstream tasks.","To address this challenge, we propose Context Optimization with Multi-Knowledge Representation (CoKnow), a framework that enhances Prompt Learning for VLMs with rich contextual knowledge.","To facilitate CoKnow during inference, we trained lightweight semantic knowledge mappers, which are capable of generating Multi-Knowledge Representation for an input image without requiring additional priors.","Experimentally, We conducted extensive experiments on 11 publicly available datasets, demonstrating that CoKnow outperforms a series of previous methods.","We will make all resources open-source: https://github.com/EMZucas/CoKnow."],"url":"http://arxiv.org/abs/2404.10357v2","category":"cs.CV"}
{"created":"2024-04-16 07:44:08","title":"Generating Counterfactual Trajectories with Latent Diffusion Models for Concept Discovery","abstract":"Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction.","sentences":["Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine.","Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research.","The discovery of decision-relevant concepts from black box models is a particularly challenging task.","This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models.","In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset.","This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE).","Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space.","The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers.","Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient.","Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains.","CDCT represents a further step in this direction."],"url":"http://arxiv.org/abs/2404.10356v1","category":"cs.LG"}
{"created":"2024-04-16 07:42:55","title":"Physical formula enhanced multi-task learning for pharmacokinetics prediction","abstract":"Artificial intelligence (AI) technology has demonstrated remarkable potential in drug dis-covery, where pharmacokinetics plays a crucial role in determining the dosage, safety, and efficacy of new drugs. A major challenge for AI-driven drug discovery (AIDD) is the scarcity of high-quality data, which often requires extensive wet-lab work. A typical example of this is pharmacokinetic experiments. In this work, we develop a physical formula enhanced mul-ti-task learning (PEMAL) method that predicts four key parameters of pharmacokinetics simultaneously. By incorporating physical formulas into the multi-task framework, PEMAL facilitates effective knowledge sharing and target alignment among the pharmacokinetic parameters, thereby enhancing the accuracy of prediction. Our experiments reveal that PEMAL significantly lowers the data demand, compared to typical Graph Neural Networks. Moreover, we demonstrate that PEMAL enhances the robustness to noise, an advantage that conventional Neural Networks do not possess. Another advantage of PEMAL is its high flexibility, which can be potentially applied to other multi-task machine learning scenarios. Overall, our work illustrates the benefits and potential of using PEMAL in AIDD and other scenarios with data scarcity and noise.","sentences":["Artificial intelligence (AI) technology has demonstrated remarkable potential in drug dis-covery, where pharmacokinetics plays a crucial role in determining the dosage, safety, and efficacy of new drugs.","A major challenge for AI-driven drug discovery (AIDD) is the scarcity of high-quality data, which often requires extensive wet-lab work.","A typical example of this is pharmacokinetic experiments.","In this work, we develop a physical formula enhanced mul-ti-task learning (PEMAL) method that predicts four key parameters of pharmacokinetics simultaneously.","By incorporating physical formulas into the multi-task framework, PEMAL facilitates effective knowledge sharing and target alignment among the pharmacokinetic parameters, thereby enhancing the accuracy of prediction.","Our experiments reveal that PEMAL significantly lowers the data demand, compared to typical Graph Neural Networks.","Moreover, we demonstrate that PEMAL enhances the robustness to noise, an advantage that conventional Neural Networks do not possess.","Another advantage of PEMAL is its high flexibility, which can be potentially applied to other multi-task machine learning scenarios.","Overall, our work illustrates the benefits and potential of using PEMAL in AIDD and other scenarios with data scarcity and noise."],"url":"http://arxiv.org/abs/2404.10354v1","category":"q-bio.QM"}
{"created":"2024-04-16 07:40:32","title":"CanvasPic: An Interactive Tool for Freely Generating Facial Images Based on Spatial Layout","abstract":"In real-world usage, existing GAN image generation tools come up short due to their lack of intuitive interfaces and limited flexibility. To overcome these limitations, we developed CanvasPic, an innovative tool for flexible GAN image generation. Our tool introduces a novel 2D layout design that allows users to intuitively control image attributes based on real-world images. By interacting with the distances between images in the spatial layout, users are able to conveniently control the influence of each attribute on the target image and explore a wide range of generated results. Considering practical application scenarios, a user study involving 24 participants was conducted to compare our tool with existing tools in GAN image generation. The results of the study demonstrate that our tool significantly enhances the user experience, enabling more effective achievement of desired generative results.","sentences":["In real-world usage, existing GAN image generation tools come up short due to their lack of intuitive interfaces and limited flexibility.","To overcome these limitations, we developed CanvasPic, an innovative tool for flexible GAN image generation.","Our tool introduces a novel 2D layout design that allows users to intuitively control image attributes based on real-world images.","By interacting with the distances between images in the spatial layout, users are able to conveniently control the influence of each attribute on the target image and explore a wide range of generated results.","Considering practical application scenarios, a user study involving 24 participants was conducted to compare our tool with existing tools in GAN image generation.","The results of the study demonstrate that our tool significantly enhances the user experience, enabling more effective achievement of desired generative results."],"url":"http://arxiv.org/abs/2404.10352v1","category":"cs.HC"}
{"created":"2024-04-16 07:35:20","title":"On the Universality of Spatially Coupled LDPC Codes Over Intersymbol Interference Channels","abstract":"In this paper, we derive the exact input/output transfer functions of the optimal a-posteriori probability channel detector for a general ISI channel with erasures. Considering three channel impulse responses of different memory as an example, we compute the BP and MAP thresholds for regular spatially coupled LDPC codes with joint iterative detection and decoding. When we compare the results with the thresholds of ISI channels with Gaussian noise we observe an apparent inconsistency, i.e., a channel which performs better with erasures performs worse with AWGN. We show that this anomaly can be resolved by looking at the thresholds from an entropy perspective. We finally show that with spatial coupling we can achieve the symmetric information rates of different ISI channels using the same code.","sentences":["In this paper, we derive the exact input/output transfer functions of the optimal a-posteriori probability channel detector for a general ISI channel with erasures.","Considering three channel impulse responses of different memory as an example, we compute the BP and MAP thresholds for regular spatially coupled LDPC codes with joint iterative detection and decoding.","When we compare the results with the thresholds of ISI channels with Gaussian noise we observe an apparent inconsistency, i.e., a channel which performs better with erasures performs worse with AWGN.","We show that this anomaly can be resolved by looking at the thresholds from an entropy perspective.","We finally show that with spatial coupling we can achieve the symmetric information rates of different ISI channels using the same code."],"url":"http://arxiv.org/abs/2404.10348v1","category":"cs.IT"}
{"created":"2024-04-16 07:28:56","title":"Semi-parametric profile pseudolikelihood via local summary statistics for spatial point pattern intensity estimation","abstract":"Second-order statistics play a crucial role in analysing point processes. Previous research has specifically explored locally weighted second-order statistics for point processes, offering diagnostic tests in various spatial domains. However, there remains a need to improve inference for complex intensity functions, especially when the point process likelihood is intractable and in the presence of interactions among points. This paper addresses this gap by proposing a method that exploits local second-order characteristics to account for local dependencies in the fitting procedure. Our approach utilises the Papangelou conditional intensity function for general Gibbs processes, avoiding explicit assumptions about the degree of interaction and homogeneity. We provide simulation results and an application to real data to assess the proposed method's goodness-of-fit. Overall, this work contributes to advancing statistical techniques for point process analysis in the presence of spatial interactions.","sentences":["Second-order statistics play a crucial role in analysing point processes.","Previous research has specifically explored locally weighted second-order statistics for point processes, offering diagnostic tests in various spatial domains.","However, there remains a need to improve inference for complex intensity functions, especially when the point process likelihood is intractable and in the presence of interactions among points.","This paper addresses this gap by proposing a method that exploits local second-order characteristics to account for local dependencies in the fitting procedure.","Our approach utilises the Papangelou conditional intensity function for general Gibbs processes, avoiding explicit assumptions about the degree of interaction and homogeneity.","We provide simulation results and an application to real data to assess the proposed method's goodness-of-fit.","Overall, this work contributes to advancing statistical techniques for point process analysis in the presence of spatial interactions."],"url":"http://arxiv.org/abs/2404.10344v1","category":"stat.ME"}
{"created":"2024-04-16 07:22:54","title":"Study of the Balmer decrements for Galactic classical Be stars using the Himalayan Chandra Telescope of India","abstract":"In a recent study, Banerjee et al. (2021) produced an atlas of all major emission lines found in a large sample of 115 Galactic field Be stars using the 2-m Himalayan Chandra Telescope (HCT) facility located at Ladakh, India. This paper presents our further exploration of these stars to estimate the electron density in their discs. Our study using Balmer decrement values indicate that their discs are generally optically thick in nature with electron density (n_e) in their circumstellar envelopes (CEs) being in excess of 10^13 cm^-3 for around 65% of the stars. For another 19% stars, the average n_e in their discs probably range between 10^12 cm^-3 and 10^13 cm^-3. We noticed that the nature of the H{\\alpha} and H\\b{eta} line profiles might not influence the observed Balmer decrement values (i.e. D_34 and D_54) of the sample of stars. Interestingly, we also found that around 50% of the Be stars displaying D_34 greater than 2.7 are of earlier spectral types, i.e. within B0 -B3.","sentences":["In a recent study, Banerjee et al. (2021) produced an atlas of all major emission lines found in a large sample of 115 Galactic field Be stars using the 2-m Himalayan Chandra Telescope (HCT) facility located at Ladakh, India.","This paper presents our further exploration of these stars to estimate the electron density in their discs.","Our study using Balmer decrement values indicate that their discs are generally optically thick in nature with electron density (n_e) in their circumstellar envelopes (CEs) being in excess of 10^13","cm^-3 for around 65% of the stars.","For another 19% stars, the average n_e","in their discs probably range between 10^12","cm^-3 and 10^13 cm^-3.","We noticed that the nature of the H{\\alpha} and H\\b{eta} line profiles might not influence the observed Balmer decrement values (i.e. D_34 and D_54) of the sample of stars.","Interestingly, we also found that around 50% of the Be stars displaying D_34 greater than 2.7 are of earlier spectral types, i.e. within B0 -B3."],"url":"http://arxiv.org/abs/2404.10339v1","category":"astro-ph.SR"}
{"created":"2024-04-16 07:21:39","title":"Intriguing Properties of Positional Encoding in Time Series Forecasting","abstract":"Transformer-based methods have made significant progress in time series forecasting (TSF). They primarily handle two types of tokens, i.e., temporal tokens that contain all variables of the same timestamp, and variable tokens that contain all input time points for a specific variable. Transformer-based methods rely on positional encoding (PE) to mark tokens' positions, facilitating the model to perceive the correlation between tokens. However, in TSF, research on PE remains insufficient. To address this gap, we conduct experiments and uncover intriguing properties of existing PEs in TSF: (i) The positional information injected by PEs diminishes as the network depth increases; (ii) Enhancing positional information in deep networks is advantageous for improving the model's performance; (iii) PE based on the similarity between tokens can improve the model's performance. Motivated by these findings, we introduce two new PEs: Temporal Position Encoding (T-PE) for temporal tokens and Variable Positional Encoding (V-PE) for variable tokens. Both T-PE and V-PE incorporate geometric PE based on tokens' positions and semantic PE based on the similarity between tokens but using different calculations. To leverage both the PEs, we design a Transformer-based dual-branch framework named T2B-PE. It first calculates temporal tokens' correlation and variable tokens' correlation respectively and then fuses the dual-branch features through the gated unit. Extensive experiments demonstrate the superior robustness and effectiveness of T2B-PE. The code is available at: \\href{https://github.com/jlu-phyComputer/T2B-PE}{https://github.com/jlu-phyComputer/T2B-PE}.","sentences":["Transformer-based methods have made significant progress in time series forecasting (TSF).","They primarily handle two types of tokens, i.e., temporal tokens that contain all variables of the same timestamp, and variable tokens that contain all input time points for a specific variable.","Transformer-based methods rely on positional encoding (PE) to mark tokens' positions, facilitating the model to perceive the correlation between tokens.","However, in TSF, research on PE remains insufficient.","To address this gap, we conduct experiments and uncover intriguing properties of existing PEs in TSF: (i) The positional information injected by PEs diminishes as the network depth increases; (ii) Enhancing positional information in deep networks is advantageous for improving the model's performance; (iii) PE based on the similarity between tokens can improve the model's performance.","Motivated by these findings, we introduce two new PEs: Temporal Position Encoding (T-PE) for temporal tokens and Variable Positional Encoding (V-PE) for variable tokens.","Both T-PE and V-PE incorporate geometric PE based on tokens' positions and semantic PE based on the similarity between tokens but using different calculations.","To leverage both the PEs, we design a Transformer-based dual-branch framework named T2B-PE.","It first calculates temporal tokens' correlation and variable tokens' correlation respectively and then fuses the dual-branch features through the gated unit.","Extensive experiments demonstrate the superior robustness and effectiveness of T2B-PE.","The code is available at: \\href{https://github.com/jlu-phyComputer/T2B-PE}{https://github.com/jlu-phyComputer/T2B-PE}."],"url":"http://arxiv.org/abs/2404.10337v1","category":"cs.AI"}
{"created":"2024-04-16 07:19:52","title":"Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models","abstract":"Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner.","sentences":["Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs).","However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts.","Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure.","To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples.","Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability.","Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area.","Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples.","Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods.","Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner."],"url":"http://arxiv.org/abs/2404.10335v1","category":"cs.CV"}
{"created":"2024-04-16 07:14:32","title":"Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning","abstract":"Despite achieving outstanding performance on various cross-modal tasks, current large vision-language models (LVLMs) still suffer from hallucination issues, manifesting as inconsistencies between their generated responses and the corresponding images. Prior research has implicated that the low quality of instruction data, particularly the skewed balance between positive and negative samples, is a significant contributor to model hallucinations. Recently, researchers have proposed high-quality instruction datasets, such as LRV-Instruction, to mitigate model hallucination. Nonetheless, our investigation reveals that hallucinatory concepts from different LVLMs exhibit specificity, i.e. the distribution of hallucinatory concepts varies significantly across models. Existing datasets did not consider the hallucination specificity of different models in the design processes, thereby diminishing their efficacy in mitigating model hallucination. In this paper, we propose a targeted instruction data generation framework named DFTG that tailored to the hallucination specificity of different models. Concretely, DFTG consists of two stages: hallucination diagnosis, which extracts the necessary information from the model's responses and images for hallucination diagnosis; and targeted data generation, which generates targeted instruction data based on diagnostic results. The experimental results on hallucination benchmarks demonstrate that the targeted instruction data generated by our method are more effective in mitigating hallucinations compared to previous datasets.","sentences":["Despite achieving outstanding performance on various cross-modal tasks, current large vision-language models (LVLMs) still suffer from hallucination issues, manifesting as inconsistencies between their generated responses and the corresponding images.","Prior research has implicated that the low quality of instruction data, particularly the skewed balance between positive and negative samples, is a significant contributor to model hallucinations.","Recently, researchers have proposed high-quality instruction datasets, such as LRV-Instruction, to mitigate model hallucination.","Nonetheless, our investigation reveals that hallucinatory concepts from different LVLMs exhibit specificity, i.e. the distribution of hallucinatory concepts varies significantly across models.","Existing datasets did not consider the hallucination specificity of different models in the design processes, thereby diminishing their efficacy in mitigating model hallucination.","In this paper, we propose a targeted instruction data generation framework named DFTG that tailored to the hallucination specificity of different models.","Concretely, DFTG consists of two stages: hallucination diagnosis, which extracts the necessary information from the model's responses and images for hallucination diagnosis; and targeted data generation, which generates targeted instruction data based on diagnostic results.","The experimental results on hallucination benchmarks demonstrate that the targeted instruction data generated by our method are more effective in mitigating hallucinations compared to previous datasets."],"url":"http://arxiv.org/abs/2404.10332v1","category":"cs.CV"}
{"created":"2024-04-16 07:13:41","title":"A broadband vortex beam generation by reflective meta-surface based on metal double-slit resonant ring","abstract":"Recently, meta-surface(MS) has emerged as a promising alternative method for generating vortex waves. At the same time, MS also face the problem of narrow bandwidth, in order to obtain a board bandwidth, the MS unit cells structure become more and more complex, which will deduce many inconveniences to the preparation process of MS device. Therefore, we want to design a simple MS unit cell with a multi-frequency selection. In this paper, based on the principle of geometric phase, we design a simple reflective MS unit cell based on metal double-slit resonant ring. We elaborate on the resonance mechanism of the MS unit cell. Under the normal incidence of circularly polarized (CP) waves, the reflection coefficient of the same polarization was greater than 85%. By rotating the orientation angle of the resonator on the MS unit cell, the continuous 2pi phase coverage was satisfied in the frequency range of 0.52THz-1.1THz, and the relative bandwidth becomes 71.6%. Based on this, we construct a vortex generator by using a 15*15 MS unit array. The right-handed circularly polarized waves (RCP) and left-handed circularly polarized waves (LCP) are separately incident on MS with topological charges of l=1,2,3 under multiple resonant frequencies. The generated RCP vortex wave with topological charges of l=-1,-2,-3 and the generated LCP vortex wave with topological charges of l=1,2,3. The numerical simulation results exhibit our designed MS with multiple resonance outcomes can achieve a multi-broadband operation and generate a wide-band vortex beam. In addition, we also calculate the pattern purity. Through theoretical analysis and numerical simulation, we prove that our designed MS can generate a broadband vortex wave.","sentences":["Recently, meta-surface(MS) has emerged as a promising alternative method for generating vortex waves.","At the same time, MS also face the problem of narrow bandwidth, in order to obtain a board bandwidth, the MS unit cells structure become more and more complex, which will deduce many inconveniences to the preparation process of MS device.","Therefore, we want to design a simple MS unit cell with a multi-frequency selection.","In this paper, based on the principle of geometric phase, we design a simple reflective MS unit cell based on metal double-slit resonant ring.","We elaborate on the resonance mechanism of the MS unit cell.","Under the normal incidence of circularly polarized (CP) waves, the reflection coefficient of the same polarization was greater than 85%.","By rotating the orientation angle of the resonator on the MS unit cell, the continuous 2pi phase coverage was satisfied in the frequency range of 0.52THz-1.1THz, and the relative bandwidth becomes 71.6%.","Based on this, we construct a vortex generator by using a 15*15 MS unit array.","The right-handed circularly polarized waves (RCP) and left-handed circularly polarized waves (LCP) are separately incident on MS with topological charges of l=1,2,3 under multiple resonant frequencies.","The generated RCP vortex wave with topological charges of l=-1,-2,-3 and the generated LCP vortex wave with topological charges of l=1,2,3.","The numerical simulation results exhibit our designed MS with multiple resonance outcomes can achieve a multi-broadband operation and generate a wide-band vortex beam.","In addition, we also calculate the pattern purity.","Through theoretical analysis and numerical simulation, we prove that our designed MS can generate a broadband vortex wave."],"url":"http://arxiv.org/abs/2404.10330v1","category":"physics.optics"}
{"created":"2024-04-16 07:13:22","title":"Towards Complex Ontology Alignment using Large Language Models","abstract":"Ontology alignment, a critical process in the Semantic Web for detecting relationships between different ontologies, has traditionally focused on identifying so-called \"simple\" 1-to-1 relationships through class labels and properties comparison. The more practically useful exploration of more complex alignments remains a hard problem to automate, and as such is largely underexplored, i.e. in application practice it is usually done manually by ontology and domain experts. Recently, the surge in Natural Language Processing (NLP) capabilities, driven by advancements in Large Language Models (LLMs), presents new opportunities for enhancing ontology engineering practices, including ontology alignment tasks. This paper investigates the application of LLM technologies to tackle the complex ontology alignment challenge. Leveraging a prompt-based approach and integrating rich ontology content so-called modules our work constitutes a significant advance towards automating the complex alignment task.","sentences":["Ontology alignment, a critical process in the Semantic Web for detecting relationships between different ontologies, has traditionally focused on identifying so-called \"simple\" 1-to-1 relationships through class labels and properties comparison.","The more practically useful exploration of more complex alignments remains a hard problem to automate, and as such is largely underexplored, i.e. in application practice it is usually done manually by ontology and domain experts.","Recently, the surge in Natural Language Processing (NLP) capabilities, driven by advancements in Large Language Models (LLMs), presents new opportunities for enhancing ontology engineering practices, including ontology alignment tasks.","This paper investigates the application of LLM technologies to tackle the complex ontology alignment challenge.","Leveraging a prompt-based approach and integrating rich ontology content so-called modules our work constitutes a significant advance towards automating the complex alignment task."],"url":"http://arxiv.org/abs/2404.10329v1","category":"cs.AI"}
{"created":"2024-04-16 07:11:45","title":"Identification of Active Subfunctions in Finite-Max Minimisation via a Smooth Reformulation","abstract":"In this work, we consider a nonsmooth minimisation problem in which the objective function can be represented as the maximum of finitely many smooth ``subfunctions''. First, we study a smooth min-max reformulation of the problem. Due to this smoothness, the model provides enhanced capability of exploiting the structure of the problem, when compared to methods that attempt to tackle the nonsmooth problem directly. Then, we present several approaches to identify the set of active subfunctions at a minimiser, all within finitely many iterations of a first order method for solving the smooth model. As is well known, the problem can be equivalently rewritten in terms of these subfunctions, but a key challenge is to identify this set \\textit{a priori}. Such an identification is clearly beneficial in an algorithmic sense, since one can apply this knowledge to create an equivalent problem with lower complexity, thus facilitating generally faster convergence. Finally, numerical results comparing the accuracy of each of these approaches are presented, along with the effect they have on reducing the complexity of the original problem.","sentences":["In this work, we consider a nonsmooth minimisation problem in which the objective function can be represented as the maximum of finitely many smooth ``subfunctions''.","First, we study a smooth min-max reformulation of the problem.","Due to this smoothness, the model provides enhanced capability of exploiting the structure of the problem, when compared to methods that attempt to tackle the nonsmooth problem directly.","Then, we present several approaches to identify the set of active subfunctions at a minimiser, all within finitely many iterations of a first order method for solving the smooth model.","As is well known, the problem can be equivalently rewritten in terms of these subfunctions, but a key challenge is to identify this set \\textit{a priori}.","Such an identification is clearly beneficial in an algorithmic sense, since one can apply this knowledge to create an equivalent problem with lower complexity, thus facilitating generally faster convergence.","Finally, numerical results comparing the accuracy of each of these approaches are presented, along with the effect they have on reducing the complexity of the original problem."],"url":"http://arxiv.org/abs/2404.10326v1","category":"math.OC"}
{"created":"2024-04-16 07:07:40","title":"Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation","abstract":"Few-shot semantic segmentation (FSS) has achieved great success on segmenting objects of novel classes, supported by only a few annotated samples. However, existing FSS methods often underperform in the presence of domain shifts, especially when encountering new domain styles that are unseen during training. It is suboptimal to directly adapt or generalize the entire model to new domains in the few-shot scenario. Instead, our key idea is to adapt a small adapter for rectifying diverse target domain styles to the source domain. Consequently, the rectified target domain features can fittingly benefit from the well-optimized source domain segmentation model, which is intently trained on sufficient source domain data. Training domain-rectifying adapter requires sufficiently diverse target domains. We thus propose a novel local-global style perturbation method to simulate diverse potential target domains by perturbating the feature channel statistics of the individual images and collective statistics of the entire source domain, respectively. Additionally, we propose a cyclic domain alignment module to facilitate the adapter effectively rectifying domains using a reverse domain rectification supervision. The adapter is trained to rectify the image features from diverse synthesized target domains to align with the source domain. During testing on target domains, we start by rectifying the image features and then conduct few-shot segmentation on the domain-rectified features. Extensive experiments demonstrate the effectiveness of our method, achieving promising results on cross-domain few-shot semantic segmentation tasks. Our code is available at https://github.com/Matt-Su/DR-Adapter.","sentences":["Few-shot semantic segmentation (FSS) has achieved great success on segmenting objects of novel classes, supported by only a few annotated samples.","However, existing FSS methods often underperform in the presence of domain shifts, especially when encountering new domain styles that are unseen during training.","It is suboptimal to directly adapt or generalize the entire model to new domains in the few-shot scenario.","Instead, our key idea is to adapt a small adapter for rectifying diverse target domain styles to the source domain.","Consequently, the rectified target domain features can fittingly benefit from the well-optimized source domain segmentation model, which is intently trained on sufficient source domain data.","Training domain-rectifying adapter requires sufficiently diverse target domains.","We thus propose a novel local-global style perturbation method to simulate diverse potential target domains by perturbating the feature channel statistics of the individual images and collective statistics of the entire source domain, respectively.","Additionally, we propose a cyclic domain alignment module to facilitate the adapter effectively rectifying domains using a reverse domain rectification supervision.","The adapter is trained to rectify the image features from diverse synthesized target domains to align with the source domain.","During testing on target domains, we start by rectifying the image features and then conduct few-shot segmentation on the domain-rectified features.","Extensive experiments demonstrate the effectiveness of our method, achieving promising results on cross-domain few-shot semantic segmentation tasks.","Our code is available at https://github.com/Matt-Su/DR-Adapter."],"url":"http://arxiv.org/abs/2404.10322v1","category":"cs.CV"}
{"created":"2024-04-16 07:02:40","title":"CARE to Compare: A real-world dataset for anomaly detection in wind turbine data","abstract":"Anomaly detection plays a crucial role in the field of predictive maintenance for wind turbines, yet the comparison of different algorithms poses a difficult task because domain specific public datasets are scarce. Many comparisons of different approaches either use benchmarks composed of data from many different domains, inaccessible data or one of the few publicly available datasets which lack detailed information about the faults. Moreover, many publications highlight a couple of case studies where fault detection was successful. With this paper we publish a high quality dataset that contains data from 36 wind turbines across 3 different wind farms as well as the most detailed fault information of any public wind turbine dataset as far as we know. The new dataset contains 89 years worth of real-world operating data of wind turbines, distributed across 44 labeled time frames for anomalies that led up to faults, as well as 51 time series representing normal behavior. Additionally, the quality of training data is ensured by turbine-status-based labels for each data point. Furthermore, we propose a new scoring method, called CARE (Coverage, Accuracy, Reliability and Earliness), which takes advantage of the information depth that is present in the dataset to identify a good all-around anomaly detection model. This score considers the anomaly detection performance, the ability to recognize normal behavior properly and the capability to raise as few false alarms as possible while simultaneously detecting anomalies early.","sentences":["Anomaly detection plays a crucial role in the field of predictive maintenance for wind turbines, yet the comparison of different algorithms poses a difficult task because domain specific public datasets are scarce.","Many comparisons of different approaches either use benchmarks composed of data from many different domains, inaccessible data or one of the few publicly available datasets which lack detailed information about the faults.","Moreover, many publications highlight a couple of case studies where fault detection was successful.","With this paper we publish a high quality dataset that contains data from 36 wind turbines across 3 different wind farms as well as the most detailed fault information of any public wind turbine dataset as far as we know.","The new dataset contains 89 years worth of real-world operating data of wind turbines, distributed across 44 labeled time frames for anomalies that led up to faults, as well as 51 time series representing normal behavior.","Additionally, the quality of training data is ensured by turbine-status-based labels for each data point.","Furthermore, we propose a new scoring method, called CARE (Coverage, Accuracy, Reliability and Earliness), which takes advantage of the information depth that is present in the dataset to identify a good all-around anomaly detection model.","This score considers the anomaly detection performance, the ability to recognize normal behavior properly and the capability to raise as few false alarms as possible while simultaneously detecting anomalies early."],"url":"http://arxiv.org/abs/2404.10320v1","category":"cs.LG"}
{"created":"2024-04-16 06:58:30","title":"SRGS: Super-Resolution 3D Gaussian Splatting","abstract":"Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes will be released upon acceptance.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation.","This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering.","However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS).","To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space.","The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views.","The gradient accumulated from more viewpoints will facilitate the densification of primitives.","Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features.","In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives.","Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks & Temples.","Related codes will be released upon acceptance."],"url":"http://arxiv.org/abs/2404.10318v1","category":"cs.CV"}
{"created":"2024-04-16 06:55:45","title":"LLMs4OM: Matching Ontologies with Large Language Models","abstract":"Ontology Matching (OM), is a critical task in knowledge integration, where aligning heterogeneous ontologies facilitates data interoperability and knowledge sharing. Traditional OM systems often rely on expert knowledge or predictive models, with limited exploration of the potential of Large Language Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate the effectiveness of LLMs in OM tasks. This framework utilizes two modules for retrieval and matching, respectively, enhanced by zero-shot prompting across three ontology representations: concept, concept-parent, and concept-children. Through comprehensive evaluations using 20 OM datasets from various domains, we demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass the performance of traditional OM systems, particularly in complex matching scenarios. Our results highlight the potential of LLMs to significantly contribute to the field of OM.","sentences":["Ontology Matching (OM), is a critical task in knowledge integration, where aligning heterogeneous ontologies facilitates data interoperability and knowledge sharing.","Traditional OM systems often rely on expert knowledge or predictive models, with limited exploration of the potential of Large Language Models (LLMs).","We present the LLMs4OM framework, a novel approach to evaluate the effectiveness of LLMs in OM tasks.","This framework utilizes two modules for retrieval and matching, respectively, enhanced by zero-shot prompting across three ontology representations: concept, concept-parent, and concept-children.","Through comprehensive evaluations using 20 OM datasets from various domains, we demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass the performance of traditional OM systems, particularly in complex matching scenarios.","Our results highlight the potential of LLMs to significantly contribute to the field of OM."],"url":"http://arxiv.org/abs/2404.10317v1","category":"cs.AI"}
{"created":"2024-04-16 06:47:49","title":"Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience","abstract":"Large Language Models (LLMs) have exhibited remarkable performance across various downstream tasks, but they may generate inaccurate or false information with a confident tone. One of the possible solutions is to empower the LLM confidence expression capability, in which the confidence expressed can be well-aligned with the true probability of the generated answer being correct. However, leveraging the intrinsic ability of LLMs or the signals from the output logits of answers proves challenging in accurately capturing the response uncertainty in LLMs. Therefore, drawing inspiration from cognitive diagnostics, we propose a method of Learning from Past experience (LePe) to enhance the capability for confidence expression. Specifically, we first identify three key problems: (1) How to capture the inherent confidence of the LLM? (2) How to teach the LLM to express confidence? (3) How to evaluate the confidence expression of the LLM? Then we devise three stages in LePe to deal with these problems. Besides, to accurately capture the confidence of an LLM when constructing the training data, we design a complete pipeline including question preparation and answer sampling. We also conduct experiments using the Llama family of LLMs to verify the effectiveness of our proposed method on four datasets.","sentences":["Large Language Models (LLMs) have exhibited remarkable performance across various downstream tasks, but they may generate inaccurate or false information with a confident tone.","One of the possible solutions is to empower the LLM confidence expression capability, in which the confidence expressed can be well-aligned with the true probability of the generated answer being correct.","However, leveraging the intrinsic ability of LLMs or the signals from the output logits of answers proves challenging in accurately capturing the response uncertainty in LLMs.","Therefore, drawing inspiration from cognitive diagnostics, we propose a method of Learning from Past experience (LePe) to enhance the capability for confidence expression.","Specifically, we first identify three key problems: (1) How to capture the inherent confidence of the LLM?","(2) How to teach the LLM to express confidence?","(3) How to evaluate the confidence expression of the LLM?","Then we devise three stages in LePe to deal with these problems.","Besides, to accurately capture the confidence of an LLM when constructing the training data, we design a complete pipeline including question preparation and answer sampling.","We also conduct experiments using the Llama family of LLMs to verify the effectiveness of our proposed method on four datasets."],"url":"http://arxiv.org/abs/2404.10315v1","category":"cs.CL"}
{"created":"2024-04-16 06:40:51","title":"Awareness of uncertainty in classification using a multivariate model and multi-views","abstract":"One of the ways to make artificial intelligence more natural is to give it some room for doubt. Two main questions should be resolved in that way. First, how to train a model to estimate uncertainties of its own predictions? And then, what to do with the uncertain predictions if they appear? First, we proposed an uncertainty-aware negative log-likelihood loss for the case of N-dimensional multivariate normal distribution with spherical variance matrix to the solution of N-classes classification tasks. The loss is similar to the heteroscedastic regression loss. The proposed model regularizes uncertain predictions, and trains to calculate both the predictions and their uncertainty estimations. The model fits well with the label smoothing technique. Second, we expanded the limits of data augmentation at the training and test stages, and made the trained model to give multiple predictions for a given number of augmented versions of each test sample. Given the multi-view predictions together with their uncertainties and confidences, we proposed several methods to calculate final predictions, including mode values and bin counts with soft and hard weights. For the latter method, we formalized the model tuning task in the form of multimodal optimization with non-differentiable criteria of maximum accuracy, and applied particle swarm optimization to solve the tuning task. The proposed methodology was tested using CIFAR-10 dataset with clean and noisy labels and demonstrated good results in comparison with other uncertainty estimation methods related to sample selection, co-teaching, and label smoothing.","sentences":["One of the ways to make artificial intelligence more natural is to give it some room for doubt.","Two main questions should be resolved in that way.","First, how to train a model to estimate uncertainties of its own predictions?","And then, what to do with the uncertain predictions if they appear?","First, we proposed an uncertainty-aware negative log-likelihood loss for the case of N-dimensional multivariate normal distribution with spherical variance matrix to the solution of N-classes classification tasks.","The loss is similar to the heteroscedastic regression loss.","The proposed model regularizes uncertain predictions, and trains to calculate both the predictions and their uncertainty estimations.","The model fits well with the label smoothing technique.","Second, we expanded the limits of data augmentation at the training and test stages, and made the trained model to give multiple predictions for a given number of augmented versions of each test sample.","Given the multi-view predictions together with their uncertainties and confidences, we proposed several methods to calculate final predictions, including mode values and bin counts with soft and hard weights.","For the latter method, we formalized the model tuning task in the form of multimodal optimization with non-differentiable criteria of maximum accuracy, and applied particle swarm optimization to solve the tuning task.","The proposed methodology was tested using CIFAR-10 dataset with clean and noisy labels and demonstrated good results in comparison with other uncertainty estimation methods related to sample selection, co-teaching, and label smoothing."],"url":"http://arxiv.org/abs/2404.10314v1","category":"cs.CV"}
{"created":"2024-04-16 06:39:37","title":"OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model","abstract":"Omnidirectional images (ODIs) are commonly used in real-world visual tasks, and high-resolution ODIs help improve the performance of related visual tasks. Most existing super-resolution methods for ODIs use end-to-end learning strategies, resulting in inferior realness of generated images and a lack of effective out-of-domain generalization capabilities in training methods. Image generation methods represented by diffusion model provide strong priors for visual tasks and have been proven to be effectively applied to image restoration tasks. Leveraging the image priors of the Stable Diffusion (SD) model, we achieve omnidirectional image super-resolution with both fidelity and realness, dubbed as OmniSSR. Firstly, we transform the equirectangular projection (ERP) images into tangent projection (TP) images, whose distribution approximates the planar image domain. Then, we use SD to iteratively sample initial high-resolution results. At each denoising iteration, we further correct and update the initial results using the proposed Octadecaplex Tangent Information Interaction (OTII) and Gradient Decomposition (GD) technique to ensure better consistency. Finally, the TP images are transformed back to obtain the final high-resolution results. Our method is zero-shot, requiring no training or fine-tuning. Experiments of our method on two benchmark datasets demonstrate the effectiveness of our proposed method.","sentences":["Omnidirectional images (ODIs) are commonly used in real-world visual tasks, and high-resolution ODIs help improve the performance of related visual tasks.","Most existing super-resolution methods for ODIs use end-to-end learning strategies, resulting in inferior realness of generated images and a lack of effective out-of-domain generalization capabilities in training methods.","Image generation methods represented by diffusion model provide strong priors for visual tasks and have been proven to be effectively applied to image restoration tasks.","Leveraging the image priors of the Stable Diffusion (SD) model, we achieve omnidirectional image super-resolution with both fidelity and realness, dubbed as OmniSSR.","Firstly, we transform the equirectangular projection (ERP) images into tangent projection (TP) images, whose distribution approximates the planar image domain.","Then, we use SD to iteratively sample initial high-resolution results.","At each denoising iteration, we further correct and update the initial results using the proposed Octadecaplex Tangent Information Interaction (OTII) and Gradient Decomposition (GD) technique to ensure better consistency.","Finally, the TP images are transformed back to obtain the final high-resolution results.","Our method is zero-shot, requiring no training or fine-tuning.","Experiments of our method on two benchmark datasets demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2404.10312v1","category":"cs.CV"}
{"created":"2024-04-16 16:30:27","title":"Cross-Language Evolution of Divergent Collective Memory Around the Arab Spring","abstract":"The Arab Spring was a historic set of protests beginning in 2011 that toppled governments and led to major conflicts. Collective memories of events like these can vary significantly across social contexts in response to political, cultural, and linguistic factors. While Wikipedia plays an important role in documenting both historic and current events, little attention has been given to how Wikipedia articles, created in the aftermath of major events, continue to evolve over years or decades. Using the archived content of Arab Spring-related topics across the Arabic and English Wikipedias between 2011 and 2024, we define and evaluate multilingual measures of event salience, deliberation, contextualization, and consolidation of collective memory surrounding the Arab Spring. Our findings about the temporal evolution of the Wikipedia articles' content similarity across languages has implications for theorizing about online collective memory processes and evaluating linguistic models trained on these data.","sentences":["The Arab Spring was a historic set of protests beginning in 2011 that toppled governments and led to major conflicts.","Collective memories of events like these can vary significantly across social contexts in response to political, cultural, and linguistic factors.","While Wikipedia plays an important role in documenting both historic and current events, little attention has been given to how Wikipedia articles, created in the aftermath of major events, continue to evolve over years or decades.","Using the archived content of Arab Spring-related topics across the Arabic and English Wikipedias between 2011 and 2024, we define and evaluate multilingual measures of event salience, deliberation, contextualization, and consolidation of collective memory surrounding the Arab Spring.","Our findings about the temporal evolution of the Wikipedia articles' content similarity across languages has implications for theorizing about online collective memory processes and evaluating linguistic models trained on these data."],"url":"http://arxiv.org/abs/2404.10706v1","category":"cs.CY"}
{"created":"2024-04-16 16:16:40","title":"ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation","abstract":"We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a new outdoor large-scale aerial LiDAR dataset designed specifically for advancing research in point cloud semantic segmentation. As the most extensive and diverse collection of its kind to date, the dataset covers a total area of 10$km^2$ with close to 600 million points and features eleven distinct object categories. To guarantee the dataset's quality and utility, we have thoroughly curated the point labels through an internal team of experts, ensuring accuracy and consistency in semantic labeling. The dataset is engineered to move forward the fields of 3D urban modeling, scene understanding, and utility infrastructure management by presenting new challenges and potential applications. As a benchmark, we report qualitative and quantitative analysis of a voxel-based point cloud segmentation approach based on the Minkowski Engine.","sentences":["We introduce ECLAIR (Extended Classification of Lidar for AI Recognition), a new outdoor large-scale aerial LiDAR dataset designed specifically for advancing research in point cloud semantic segmentation.","As the most extensive and diverse collection of its kind to date, the dataset covers a total area of 10$km^2$ with close to 600 million points and features eleven distinct object categories.","To guarantee the dataset's quality and utility, we have thoroughly curated the point labels through an internal team of experts, ensuring accuracy and consistency in semantic labeling.","The dataset is engineered to move forward the fields of 3D urban modeling, scene understanding, and utility infrastructure management by presenting new challenges and potential applications.","As a benchmark, we report qualitative and quantitative analysis of a voxel-based point cloud segmentation approach based on the Minkowski Engine."],"url":"http://arxiv.org/abs/2404.10699v1","category":"cs.CV"}
{"created":"2024-04-16 14:16:43","title":"A Longitudinal Study of Child Wellbeing Assessment via Online Interactions with a Social Robots","abstract":"Socially Assistive Robots are studied in different Child-Robot Interaction settings. However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing. In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children. The children (N=40, 20 girls and 20 boys; 8-13 years) interacted with the Nao robot (30-45 mins) over three sessions, at least a week apart. Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety towards the robot, and the robot's abilities. We divided the participants into three wellbeing clusters (low, med and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analysed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants' gender. Our primary findings suggest that (I) online mediated-interactions with robots can be effective in assessing children's mental wellbeing over time, and (II) children's overall perception of the robot either improved or remained consistent across time. Supplementary exploratory analyses have also revealed that gender affected the children's wellbeing assessments as well as their perceptions of the robot.","sentences":["Socially Assistive Robots are studied in different Child-Robot Interaction settings.","However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing.","In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children.","The children (N=40, 20 girls and 20 boys; 8-13 years) interacted with the Nao robot (30-45 mins) over three sessions, at least a week apart.","Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety towards the robot, and the robot's abilities.","We divided the participants into three wellbeing clusters (low, med and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analysed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants' gender.","Our primary findings suggest that (I) online mediated-interactions with robots can be effective in assessing children's mental wellbeing over time, and (II) children's overall perception of the robot either improved or remained consistent across time.","Supplementary exploratory analyses have also revealed that gender affected the children's wellbeing assessments as well as their perceptions of the robot."],"url":"http://arxiv.org/abs/2404.10593v1","category":"cs.HC"}
{"created":"2024-04-16 13:05:23","title":"Benchmarking Machine Learning Applications on Heterogeneous Architecture using Reframe","abstract":"With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues. Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm. We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures.","sentences":["With the rapid increase in machine learning workloads performed on HPC systems, it is beneficial to regularly perform machine learning specific benchmarks to monitor performance and identify issues.","Furthermore, as part of the Edinburgh International Data Facility, EPCC currently hosts a wide range of machine learning accelerators including Nvidia GPUs, the Graphcore Bow Pod64 and Cerebras CS-2, which are managed via Kubernetes and Slurm.","We extended the Reframe framework to support the Kubernetes scheduler backend, and utilise Reframe to perform machine learning benchmarks, and we discuss the preliminary results collected and challenges involved in integrating Reframe across multiple platforms and architectures."],"url":"http://arxiv.org/abs/2404.10536v1","category":"cs.DC"}
{"created":"2024-04-16 11:40:55","title":"Efficient point-based simulation of four-way coupled particles in turbulence at high number density","abstract":"In many natural and industrial applications, turbulent flows encompass some form of dispersed particles. Although this type of multiphase turbulent flow is omnipresent, its numerical modeling has proven to be a remarkably challenging problem. Models that fully resolve the particle phase are computationally very expensive, strongly limiting the number of particles that can be considered in practice. This warrants the need for efficient reduced order modeling of the complex system of particles in turbulence that can handle high number densities of particles. Here, we present an efficient method for point-based simulation of particles in turbulence that are four-way coupled. In contrast with traditional one-way coupled simulations, where only the effect of the fluid phase on the particle phase is modeled, this method additionally captures the back-reaction of the particle phase on the fluid phase, as well as the interactions between particles themselves. We focus on the most challenging case of very light particles or bubbles, which show strong clustering in the high-vorticity regions of the fluid. This strong clustering poses numerical difficulties which are systematically treated in our work. Our method is valid in the limit of small particles with respect to the Kolmogorov scales of the flow and is able to handle very large number densities of particles. This methods paves the way for comprehensive studies of the collective effect of small particles in fluid turbulence for a multitude of applications.","sentences":["In many natural and industrial applications, turbulent flows encompass some form of dispersed particles.","Although this type of multiphase turbulent flow is omnipresent, its numerical modeling has proven to be a remarkably challenging problem.","Models that fully resolve the particle phase are computationally very expensive, strongly limiting the number of particles that can be considered in practice.","This warrants the need for efficient reduced order modeling of the complex system of particles in turbulence that can handle high number densities of particles.","Here, we present an efficient method for point-based simulation of particles in turbulence that are four-way coupled.","In contrast with traditional one-way coupled simulations, where only the effect of the fluid phase on the particle phase is modeled, this method additionally captures the back-reaction of the particle phase on the fluid phase, as well as the interactions between particles themselves.","We focus on the most challenging case of very light particles or bubbles, which show strong clustering in the high-vorticity regions of the fluid.","This strong clustering poses numerical difficulties which are systematically treated in our work.","Our method is valid in the limit of small particles with respect to the Kolmogorov scales of the flow and is able to handle very large number densities of particles.","This methods paves the way for comprehensive studies of the collective effect of small particles in fluid turbulence for a multitude of applications."],"url":"http://arxiv.org/abs/2404.10479v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 09:03:13","title":"A Phone-based Distributed Ambient Temperature Measurement System with An Efficient Label-free Automated Training Strategy","abstract":"Enhancing the energy efficiency of buildings significantly relies on monitoring indoor ambient temperature. The potential limitations of conventional temperature measurement techniques, together with the omnipresence of smartphones, have redirected researchers' attention towards the exploration of phone-based ambient temperature estimation technology. Nevertheless, numerous obstacles remain to be addressed in order to achieve a practical implementation of this technology. This study proposes a distributed phone-based ambient temperature estimation system which enables collaboration between multiple phones to accurately measure the ambient temperature in each small area of an indoor space. Besides, it offers a secure, efficient, and cost-effective training strategy to train a new estimation model for each newly added phone, eliminating the need for manual collection of labeled data. This innovative training strategy can yield a high-performing estimation model for a new phone with just 5 data points, requiring only a few iterations. Meanwhile, by crowdsourcing, our system automatically provides accurate inferred labels for all newly collected data. We also highlight the potential of integrating federated learning into our system to ensure privacy protection at the end of this study. We believe this study has the potential to advance the practical application of phone-based ambient temperature measurement, facilitating energy-saving efforts in buildings.","sentences":["Enhancing the energy efficiency of buildings significantly relies on monitoring indoor ambient temperature.","The potential limitations of conventional temperature measurement techniques, together with the omnipresence of smartphones, have redirected researchers' attention towards the exploration of phone-based ambient temperature estimation technology.","Nevertheless, numerous obstacles remain to be addressed in order to achieve a practical implementation of this technology.","This study proposes a distributed phone-based ambient temperature estimation system which enables collaboration between multiple phones to accurately measure the ambient temperature in each small area of an indoor space.","Besides, it offers a secure, efficient, and cost-effective training strategy to train a new estimation model for each newly added phone, eliminating the need for manual collection of labeled data.","This innovative training strategy can yield a high-performing estimation model for a new phone with just 5 data points, requiring only a few iterations.","Meanwhile, by crowdsourcing, our system automatically provides accurate inferred labels for all newly collected data.","We also highlight the potential of integrating federated learning into our system to ensure privacy protection at the end of this study.","We believe this study has the potential to advance the practical application of phone-based ambient temperature measurement, facilitating energy-saving efforts in buildings."],"url":"http://arxiv.org/abs/2404.10401v1","category":"cs.LG"}
{"created":"2024-04-16 07:39:54","title":"On the Use of Relative Validity Indices for Comparing Clustering Approaches","abstract":"Relative Validity Indices (RVIs) such as the Silhouette Width Criterion, Calinski-Harabasz and Davie's Bouldin indices are the most popular tools for evaluating and optimising applications of clustering. Their ability to rank collections of candidate partitions has been used to guide the selection of the number of clusters, and to compare partitions from different clustering algorithms. Beyond these more conventional tasks, many examples can be found in the literature where RVIs have been used to compare and select other aspects of clustering approaches such as data normalisation procedures, data representation methods, and distance measures. The authors are not aware of any studies that have attempted to establish the suitability of RVIs for such comparisons. Moreover, given the impact of these aspects on pairwise similarities, it is not even immediately obvious how RVIs should be implemented when comparing these aspects. In this study, we conducted experiments with seven common RVIs on over 2.7 million clustering partitions for both synthetic and real-world datasets, encompassing feature-vector and time-series data. Our findings suggest that RVIs are not well-suited to these unconventional tasks, and that conclusions drawn from such applications may be misleading. It is recommended that normalisation procedures, representation methods, and distance measures instead be selected using external validation on high quality labelled datasets or carefully designed outcome-oriented objective criteria, both of which should be informed by relevant domain knowledge and clustering aims.","sentences":["Relative Validity Indices (RVIs) such as the Silhouette Width Criterion, Calinski-Harabasz and Davie's Bouldin indices are the most popular tools for evaluating and optimising applications of clustering.","Their ability to rank collections of candidate partitions has been used to guide the selection of the number of clusters, and to compare partitions from different clustering algorithms.","Beyond these more conventional tasks, many examples can be found in the literature where RVIs have been used to compare and select other aspects of clustering approaches such as data normalisation procedures, data representation methods, and distance measures.","The authors are not aware of any studies that have attempted to establish the suitability of RVIs for such comparisons.","Moreover, given the impact of these aspects on pairwise similarities, it is not even immediately obvious how RVIs should be implemented when comparing these aspects.","In this study, we conducted experiments with seven common RVIs on over 2.7 million clustering partitions for both synthetic and real-world datasets, encompassing feature-vector and time-series data.","Our findings suggest that RVIs are not well-suited to these unconventional tasks, and that conclusions drawn from such applications may be misleading.","It is recommended that normalisation procedures, representation methods, and distance measures instead be selected using external validation on high quality labelled datasets or carefully designed outcome-oriented objective criteria, both of which should be informed by relevant domain knowledge and clustering aims."],"url":"http://arxiv.org/abs/2404.10351v1","category":"stat.ML"}
{"created":"2024-04-16 06:39:30","title":"Learning and Optimization for Price-based Demand Response of Electric Vehicle Charging","abstract":"In the context of charging electric vehicles (EVs), the price-based demand response (PBDR) is becoming increasingly significant for charging load management. Such response usually encourages cost-sensitive customers to adjust their energy demand in response to changes in price for financial incentives. Thus, to model and optimize EV charging, it is important for charging station operator to model the PBDR patterns of EV customers by precisely predicting charging demands given price signals. Then the operator refers to these demands to optimize charging station power allocation policy. The standard pipeline involves offline fitting of a PBDR function based on historical EV charging records, followed by applying estimated EV demands in downstream charging station operation optimization. In this work, we propose a new decision-focused end-to-end framework for PBDR modeling that combines prediction errors and downstream optimization cost errors in the model learning stage. We evaluate the effectiveness of our method on a simulation of charging station operation with synthetic PBDR patterns of EV customers, and experimental results demonstrate that this framework can provide a more reliable prediction model for the ultimate optimization process, leading to more effective optimization solutions in terms of cost savings and charging station operation objectives with only a few training samples.","sentences":["In the context of charging electric vehicles (EVs), the price-based demand response (PBDR) is becoming increasingly significant for charging load management.","Such response usually encourages cost-sensitive customers to adjust their energy demand in response to changes in price for financial incentives.","Thus, to model and optimize EV charging, it is important for charging station operator to model the PBDR patterns of EV customers by precisely predicting charging demands given price signals.","Then the operator refers to these demands to optimize charging station power allocation policy.","The standard pipeline involves offline fitting of a PBDR function based on historical EV charging records, followed by applying estimated EV demands in downstream charging station operation optimization.","In this work, we propose a new decision-focused end-to-end framework for PBDR modeling that combines prediction errors and downstream optimization cost errors in the model learning stage.","We evaluate the effectiveness of our method on a simulation of charging station operation with synthetic PBDR patterns of EV customers, and experimental results demonstrate that this framework can provide a more reliable prediction model for the ultimate optimization process, leading to more effective optimization solutions in terms of cost savings and charging station operation objectives with only a few training samples."],"url":"http://arxiv.org/abs/2404.10311v1","category":"eess.SY"}
{"created":"2024-04-16 06:34:08","title":"Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs","abstract":"Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. Previous works have explored architectural changes and modifications in positional encoding to relax the constraint, but they often require expensive training or do not address the computational demands of self-attention. In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable chunks. Each chunk is then processed collectively, employing a hierarchical strategy that merges adjacent chunks at progressive transformer layers. A token reduction technique precedes each merging, ensuring memory usage efficiency. We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions. Our experiments demonstrate the proposed method's superior performance and memory efficiency, enabling the broader use of LLMs in contexts requiring extended context. Code is available at https://github.com/alinlab/HOMER.","sentences":["Large language models (LLMs) have shown remarkable performance in various natural language processing tasks.","However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process.","Previous works have explored architectural changes and modifications in positional encoding to relax the constraint, but they often require expensive training or do not address the computational demands of self-attention.","In this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations.","HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable chunks.","Each chunk is then processed collectively, employing a hierarchical strategy that merges adjacent chunks at progressive transformer layers.","A token reduction technique precedes each merging, ensuring memory usage efficiency.","We also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions.","Our experiments demonstrate the proposed method's superior performance and memory efficiency, enabling the broader use of LLMs in contexts requiring extended context.","Code is available at https://github.com/alinlab/HOMER."],"url":"http://arxiv.org/abs/2404.10308v1","category":"cs.LG"}
{"created":"2024-04-16 06:33:08","title":"Learnable Prompt for Few-Shot Semantic Segmentation in Remote Sensing Domain","abstract":"Few-shot segmentation is a task to segment objects or regions of novel classes within an image given only a few annotated examples. In the generalized setting, the task extends to segment both the base and the novel classes. The main challenge is how to train the model such that the addition of novel classes does not hurt the base classes performance, also known as catastrophic forgetting. To mitigate this issue, we use SegGPT as our base model and train it on the base classes. Then, we use separate learnable prompts to handle predictions for each novel class. To handle various object sizes which typically present in remote sensing domain, we perform patch-based prediction. To address the discontinuities along patch boundaries, we propose a patch-and-stitch technique by re-framing the problem as an image inpainting task. During inference, we also utilize image similarity search over image embeddings for prompt selection and novel class filtering to reduce false positive predictions. Based on our experiments, our proposed method boosts the weighted mIoU of a simple fine-tuned SegGPT from 15.96 to 35.08 on the validation set of few-shot OpenEarthMap dataset given in the challenge.","sentences":["Few-shot segmentation is a task to segment objects or regions of novel classes within an image given only a few annotated examples.","In the generalized setting, the task extends to segment both the base and the novel classes.","The main challenge is how to train the model such that the addition of novel classes does not hurt the base classes performance, also known as catastrophic forgetting.","To mitigate this issue, we use SegGPT as our base model and train it on the base classes.","Then, we use separate learnable prompts to handle predictions for each novel class.","To handle various object sizes which typically present in remote sensing domain, we perform patch-based prediction.","To address the discontinuities along patch boundaries, we propose a patch-and-stitch technique by re-framing the problem as an image inpainting task.","During inference, we also utilize image similarity search over image embeddings for prompt selection and novel class filtering to reduce false positive predictions.","Based on our experiments, our proposed method boosts the weighted mIoU of a simple fine-tuned SegGPT from 15.96 to 35.08 on the validation set of few-shot OpenEarthMap dataset given in the challenge."],"url":"http://arxiv.org/abs/2404.10307v1","category":"cs.CV"}
{"created":"2024-04-16 05:56:41","title":"Clustering and Data Augmentation to Improve Accuracy of Sleep Assessment and Sleep Individuality Analysis","abstract":"Recently, growing health awareness, novel methods allow individuals to monitor sleep at home. Utilizing sleep sounds offers advantages over conventional methods like smartwatches, being non-intrusive, and capable of detecting various physiological activities. This study aims to construct a machine learning-based sleep assessment model providing evidence-based assessments, such as poor sleep due to frequent movement during sleep onset. Extracting sleep sound events, deriving latent representations using VAE, clustering with GMM, and training LSTM for subjective sleep assessment achieved a high accuracy of 94.8% in distinguishing sleep satisfaction. Moreover, TimeSHAP revealed differences in impactful sound event types and timings for different individuals.","sentences":["Recently, growing health awareness, novel methods allow individuals to monitor sleep at home.","Utilizing sleep sounds offers advantages over conventional methods like smartwatches, being non-intrusive, and capable of detecting various physiological activities.","This study aims to construct a machine learning-based sleep assessment model providing evidence-based assessments, such as poor sleep due to frequent movement during sleep onset.","Extracting sleep sound events, deriving latent representations using VAE, clustering with GMM, and training LSTM for subjective sleep assessment achieved a high accuracy of 94.8% in distinguishing sleep satisfaction.","Moreover, TimeSHAP revealed differences in impactful sound event types and timings for different individuals."],"url":"http://arxiv.org/abs/2404.10299v1","category":"cs.LG"}
{"created":"2024-04-16 05:45:52","title":"Future Language Modeling from Temporal Document History","abstract":"Predicting the future is of great interest across many aspects of human activity. Businesses are interested in future trends, traders are interested in future stock prices, and companies are highly interested in future technological breakthroughs. While there are many automated systems for predicting future numerical data, such as weather, stock prices, and demand for products, there is relatively little work in automatically predicting textual data. Humans are interested in textual data predictions because it is a natural format for our consumption, and experts routinely make predictions in a textual format (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015). However, there has been relatively little formalization of this general problem in the machine learning or natural language processing communities. To address this gap, we introduce the task of future language modeling: probabilistic modeling of texts in the future based on a temporal history of texts. To our knowledge, our work is the first work to formalize the task of predicting the future in this way. We show that it is indeed possible to build future language models that improve upon strong non-temporal language model baselines, opening the door to working on this important, and widely applicable problem.","sentences":["Predicting the future is of great interest across many aspects of human activity.","Businesses are interested in future trends, traders are interested in future stock prices, and companies are highly interested in future technological breakthroughs.","While there are many automated systems for predicting future numerical data, such as weather, stock prices, and demand for products, there is relatively little work in automatically predicting textual data.","Humans are interested in textual data predictions because it is a natural format for our consumption, and experts routinely make predictions in a textual format (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015).","However, there has been relatively little formalization of this general problem in the machine learning or natural language processing communities.","To address this gap, we introduce the task of future language modeling: probabilistic modeling of texts in the future based on a temporal history of texts.","To our knowledge, our work is the first work to formalize the task of predicting the future in this way.","We show that it is indeed possible to build future language models that improve upon strong non-temporal language model baselines, opening the door to working on this important, and widely applicable problem."],"url":"http://arxiv.org/abs/2404.10297v1","category":"cs.CL"}
{"created":"2024-04-16 05:40:30","title":"Engineering software 2.0 by interpolating neural networks: unifying training, solving, and calibration","abstract":"The evolution of artificial intelligence (AI) and neural network theories has revolutionized the way software is programmed, shifting from a hard-coded series of codes to a vast neural network. However, this transition in engineering software has faced challenges such as data scarcity, multi-modality of data, low model accuracy, and slow inference. Here, we propose a new network based on interpolation theories and tensor decomposition, the interpolating neural network (INN). Instead of interpolating training data, a common notion in computer science, INN interpolates interpolation points in the physical space whose coordinates and values are trainable. It can also extrapolate if the interpolation points reside outside of the range of training data and the interpolation functions have a larger support domain. INN features orders of magnitude fewer trainable parameters, faster training, a smaller memory footprint, and higher model accuracy compared to feed-forward neural networks (FFNN) or physics-informed neural networks (PINN). INN is poised to usher in Engineering Software 2.0, a unified neural network that spans various domains of space, time, parameters, and initial/boundary conditions. This has previously been computationally prohibitive due to the exponentially growing number of trainable parameters, easily exceeding the parameter size of ChatGPT, which is over 1 trillion. INN addresses this challenge by leveraging tensor decomposition and tensor product, with adaptable network architecture.","sentences":["The evolution of artificial intelligence (AI) and neural network theories has revolutionized the way software is programmed, shifting from a hard-coded series of codes to a vast neural network.","However, this transition in engineering software has faced challenges such as data scarcity, multi-modality of data, low model accuracy, and slow inference.","Here, we propose a new network based on interpolation theories and tensor decomposition, the interpolating neural network (INN).","Instead of interpolating training data, a common notion in computer science, INN interpolates interpolation points in the physical space whose coordinates and values are trainable.","It can also extrapolate if the interpolation points reside outside of the range of training data and the interpolation functions have a larger support domain.","INN features orders of magnitude fewer trainable parameters, faster training, a smaller memory footprint, and higher model accuracy compared to feed-forward neural networks (FFNN) or physics-informed neural networks (PINN).","INN is poised to usher in Engineering Software 2.0, a unified neural network that spans various domains of space, time, parameters, and initial/boundary conditions.","This has previously been computationally prohibitive due to the exponentially growing number of trainable parameters, easily exceeding the parameter size of ChatGPT, which is over 1 trillion.","INN addresses this challenge by leveraging tensor decomposition and tensor product, with adaptable network architecture."],"url":"http://arxiv.org/abs/2404.10296v1","category":"cs.LG"}
{"created":"2024-04-16 05:23:03","title":"The Dearth of the Author in AI-Supported Writing","abstract":"We diagnose and briefly discuss the dearth of the author: a condition that arises when AI-based creativity support tools for writing allow users to produce large amounts of text without making a commensurate number of creative decisions, resulting in output that is sparse in expressive intent. We argue that the dearth of the author helps to explain a number of recurring difficulties and anxieties around AI-based writing support tools, but that it also suggests an ambitious new goal for AI-based CSTs.","sentences":["We diagnose and briefly discuss the dearth of the author: a condition that arises when AI-based creativity support tools for writing allow users to produce large amounts of text without making a commensurate number of creative decisions, resulting in output that is sparse in expressive intent.","We argue that the dearth of the author helps to explain a number of recurring difficulties and anxieties around AI-based writing support tools, but that it also suggests an ambitious new goal for AI-based CSTs."],"url":"http://arxiv.org/abs/2404.10289v1","category":"cs.HC"}
{"created":"2024-04-16 04:52:41","title":"Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning","abstract":"Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set. In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation. In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively. In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits. To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives. The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks. We also verify that Tripod significantly improves upon its naive incarnation and that all three of its \"legs\" are necessary for best performance.","sentences":["Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set.","In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation.","In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively.","In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits.","To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives.","The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks.","We also verify that Tripod significantly improves upon its naive incarnation and that all three of its \"legs\" are necessary for best performance."],"url":"http://arxiv.org/abs/2404.10282v1","category":"cs.LG"}
{"created":"2024-04-16 04:49:35","title":"AI-Assisted Writing in Education: Ecosystem Risks and Mitigations","abstract":"While the excitement around the capabilities of technological advancements is giving rise to new AI-based writing assistants, the overarching ecosystem plays a crucial role in how they are adopted in educational practice. In this paper, we point to key ecological aspects for consideration. We draw insights from extensive research integrated with practice on a writing feedback tool over 9 years at a university, and we highlight potential risks when these are overlooked. It informs the design of educational writing support tools to be better aligned within broader contexts to balance innovation with practical impact.","sentences":["While the excitement around the capabilities of technological advancements is giving rise to new AI-based writing assistants, the overarching ecosystem plays a crucial role in how they are adopted in educational practice.","In this paper, we point to key ecological aspects for consideration.","We draw insights from extensive research integrated with practice on a writing feedback tool over 9 years at a university, and we highlight potential risks when these are overlooked.","It informs the design of educational writing support tools to be better aligned within broader contexts to balance innovation with practical impact."],"url":"http://arxiv.org/abs/2404.10281v1","category":"cs.HC"}
{"created":"2024-04-16 04:21:59","title":"OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a Gradient Based Learning","abstract":"This paper presents a novel approach to optimizing profit margins in non-life insurance markets through a gradient descent-based method, targeting three key objectives: 1) maximizing profit margins, 2) ensuring conversion rates, and 3) enforcing fairness criteria such as demographic parity (DP). Traditional pricing optimization, which heavily lean on linear and semi definite programming, encounter challenges in balancing profitability and fairness. These challenges become especially pronounced in situations that necessitate continuous rate adjustments and the incorporation of fairness criteria. Specifically, indirect Ratebook optimization, a widely-used method for new business price setting, relies on predictor models such as XGBoost or GLMs/GAMs to estimate on downstream individually optimized prices. However, this strategy is prone to sequential errors and struggles to effectively manage optimizations for continuous rate scenarios. In practice, to save time actuaries frequently opt for optimization within discrete intervals (e.g., range of [-20\\%, +20\\%] with fix increments) leading to approximate estimations. Moreover, to circumvent infeasible solutions they often use relaxed constraints leading to suboptimal pricing strategies. The reverse-engineered nature of traditional models complicates the enforcement of fairness and can lead to biased outcomes. Our method addresses these challenges by employing a direct optimization strategy in the continuous space of rates and by embedding fairness through an adversarial predictor model. This innovation not only reduces sequential errors and simplifies the complexities found in traditional models but also directly integrates fairness measures into the commercial premium calculation. We demonstrate improved margin performance and stronger enforcement of fairness highlighting the critical need to evolve existing pricing strategies.","sentences":["This paper presents a novel approach to optimizing profit margins in non-life insurance markets through a gradient descent-based method, targeting three key objectives: 1) maximizing profit margins, 2) ensuring conversion rates, and 3) enforcing fairness criteria such as demographic parity (DP).","Traditional pricing optimization, which heavily lean on linear and semi definite programming, encounter challenges in balancing profitability and fairness.","These challenges become especially pronounced in situations that necessitate continuous rate adjustments and the incorporation of fairness criteria.","Specifically, indirect Ratebook optimization, a widely-used method for new business price setting, relies on predictor models such as XGBoost or GLMs/GAMs to estimate on downstream individually optimized prices.","However, this strategy is prone to sequential errors and struggles to effectively manage optimizations for continuous rate scenarios.","In practice, to save time actuaries frequently opt for optimization within discrete intervals (e.g., range of [-20\\%, +20\\%] with fix increments) leading to approximate estimations.","Moreover, to circumvent infeasible solutions they often use relaxed constraints leading to suboptimal pricing strategies.","The reverse-engineered nature of traditional models complicates the enforcement of fairness and can lead to biased outcomes.","Our method addresses these challenges by employing a direct optimization strategy in the continuous space of rates and by embedding fairness through an adversarial predictor model.","This innovation not only reduces sequential errors and simplifies the complexities found in traditional models but also directly integrates fairness measures into the commercial premium calculation.","We demonstrate improved margin performance and stronger enforcement of fairness highlighting the critical need to evolve existing pricing strategies."],"url":"http://arxiv.org/abs/2404.10275v1","category":"cs.LG"}
{"created":"2024-04-16 04:17:17","title":"Sparse Attention Regression Network Based Soil Fertility Prediction With Ummaso","abstract":"The challenge of imbalanced soil nutrient datasets significantly hampers accurate predictions of soil fertility. To tackle this, a new method is suggested in this research, combining Uniform Manifold Approximation and Projection (UMAP) with Least Absolute Shrinkage and Selection Operator (LASSO). The main aim is to counter the impact of uneven data distribution and improve soil fertility models' predictive precision. The model introduced uses Sparse Attention Regression, effectively incorporating pertinent features from the imbalanced dataset. UMAP is utilized initially to reduce data complexity, unveiling hidden structures and important patterns. Following this, LASSO is applied to refine features and enhance the model's interpretability. The experimental outcomes highlight the effectiveness of the UMAP and LASSO hybrid approach. The proposed model achieves outstanding performance metrics, reaching a predictive accuracy of 98%, demonstrating its capability in accurate soil fertility predictions. Additionally, it showcases a Precision of 91.25%, indicating its adeptness in identifying fertile soil instances accurately. The Recall metric stands at 90.90%, emphasizing the model's ability to capture true positive cases effectively.","sentences":["The challenge of imbalanced soil nutrient datasets significantly hampers accurate predictions of soil fertility.","To tackle this, a new method is suggested in this research, combining Uniform Manifold Approximation and Projection (UMAP) with Least Absolute Shrinkage and Selection Operator (LASSO).","The main aim is to counter the impact of uneven data distribution and improve soil fertility models' predictive precision.","The model introduced uses Sparse Attention Regression, effectively incorporating pertinent features from the imbalanced dataset.","UMAP is utilized initially to reduce data complexity, unveiling hidden structures and important patterns.","Following this, LASSO is applied to refine features and enhance the model's interpretability.","The experimental outcomes highlight the effectiveness of the UMAP and LASSO hybrid approach.","The proposed model achieves outstanding performance metrics, reaching a predictive accuracy of 98%, demonstrating its capability in accurate soil fertility predictions.","Additionally, it showcases a Precision of 91.25%, indicating its adeptness in identifying fertile soil instances accurately.","The Recall metric stands at 90.90%, emphasizing the model's ability to capture true positive cases effectively."],"url":"http://arxiv.org/abs/2404.10274v1","category":"cs.AI"}
{"created":"2024-04-16 04:05:33","title":"Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using VDB Grid and Hierarchical Ray Traversal","abstract":"Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image. However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxels' emptiness in ray-tracing. In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning. First, we replace the dense grids with VDB grids to reduce the spatial redundancy. Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images.","sentences":["Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image.","However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxels' emptiness in ray-tracing.","In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning.","First, we replace the dense grids with VDB grids to reduce the spatial redundancy.","Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids.","Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images."],"url":"http://arxiv.org/abs/2404.10272v1","category":"cs.CV"}
{"created":"2024-04-16 03:59:33","title":"Social Choice for AI Alignment: Dealing with Diverse Human Feedback","abstract":"Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, so that, for example, they refuse to comply with requests for help with committing crimes or with producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans' expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about ''collective'' preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.","sentences":["Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, so that, for example, they refuse to comply with requests for help with committing crimes or with producing racist text.","One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans' expressed preferences over multiple outputs.","Another approach is constitutional AI, in which the input from humans is a list of high-level principles.","But how do we deal with potentially diverging input from humans?","How can we aggregate the input into consistent data about ''collective'' preferences or otherwise use it to make collective choices about model behavior?","In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023."],"url":"http://arxiv.org/abs/2404.10271v1","category":"cs.LG"}
{"created":"2024-04-16 03:45:45","title":"OneActor: Consistent Character Generation via Cluster-Conditioned Guidance","abstract":"Text-to-image diffusion models benefit artists with high-quality image generation. Yet its stochastic nature prevent artists from creating consistent images of the same character. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external data or require expensive tuning of the diffusion model. For this issue, we argue that a lightweight but intricate guidance is enough to function. Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor. We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster. To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference. This technique is later verified to significantly enhance the content diversity of generated images. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality. And our method is at least 4 times faster than tuning-based baselines. Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose. This property can serve as another promising tool for fine generation control.","sentences":["Text-to-image diffusion models benefit artists with high-quality image generation.","Yet its stochastic nature prevent artists from creating consistent images of the same character.","Existing methods try to tackle this challenge and generate consistent content in various ways.","However, they either depend on external data or require expensive tuning of the diffusion model.","For this issue, we argue that a lightweight but intricate guidance is enough to function.","Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor.","We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster.","To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference.","This technique is later verified to significantly enhance the content diversity of generated images.","Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality.","And our method is at least 4 times faster than tuning-based baselines.","Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose.","This property can serve as another promising tool for fine generation control."],"url":"http://arxiv.org/abs/2404.10267v1","category":"cs.CV"}
{"created":"2024-04-16 03:29:37","title":"HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights","abstract":"While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field. This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short. Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles. In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance. HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions. Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, surpassing AlphaFold-Multimer by several folds. HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version. Researchers can conveniently access and utilize this service for their development needs.","sentences":["While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field.","This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short.","Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles.","In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance.","HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions.","Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, surpassing AlphaFold-Multimer by several folds.","HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version.","Researchers can conveniently access and utilize this service for their development needs."],"url":"http://arxiv.org/abs/2404.10260v1","category":"q-bio.BM"}
{"created":"2024-04-16 03:26:43","title":"Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy","abstract":"The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events.","sentences":["The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion.","Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus.","On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances.","Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly.","In this work, we study the problem of discovering arguments associated with a specific theme.","We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging.","To demonstrate our approach, we apply our framework to contentious topics.","We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes.","Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events."],"url":"http://arxiv.org/abs/2404.10259v1","category":"cs.CL"}
{"created":"2024-04-16 03:18:27","title":"Privacy-Preserving Training-as-a-Service for On-Device Intelligence: Concept, Architectural Scheme, and Open Problems","abstract":"On-device intelligence (ODI) enables artificial intelligence (AI) applications to run on end devices, providing real-time and customized AI services without relying on remote servers. However, training models for on-device deployment face significant challenges due to the decentralized and privacy-sensitive nature of users' data, along with end-side constraints related to network connectivity, computation efficiency, etc. Existing training paradigms, such as cloud-based training, federated learning, and transfer learning, fail to sufficiently address these practical constraints that are prevalent for devices. To overcome these challenges, we propose Privacy-Preserving Training-as-a-Service (PTaaS), a novel service computing paradigm that provides privacy-friendly, customized AI model training for end devices. PTaaS outsources the core training process to remote and powerful cloud or edge servers, efficiently developing customized on-device models based on uploaded anonymous queries, ensuring data privacy while reducing the computation load on individual devices. We explore the definition, goals, and design principles of PTaaS, alongside emerging technologies that support the PTaaS paradigm. An architectural scheme for PTaaS is also presented, followed by a series of open problems that set the stage for future research directions in the field of PTaaS.","sentences":["On-device intelligence (ODI) enables artificial intelligence (AI) applications to run on end devices, providing real-time and customized AI services without relying on remote servers.","However, training models for on-device deployment face significant challenges due to the decentralized and privacy-sensitive nature of users' data, along with end-side constraints related to network connectivity, computation efficiency, etc.","Existing training paradigms, such as cloud-based training, federated learning, and transfer learning, fail to sufficiently address these practical constraints that are prevalent for devices.","To overcome these challenges, we propose Privacy-Preserving Training-as-a-Service (PTaaS), a novel service computing paradigm that provides privacy-friendly, customized AI model training for end devices.","PTaaS outsources the core training process to remote and powerful cloud or edge servers, efficiently developing customized on-device models based on uploaded anonymous queries, ensuring data privacy while reducing the computation load on individual devices.","We explore the definition, goals, and design principles of PTaaS, alongside emerging technologies that support the PTaaS paradigm.","An architectural scheme for PTaaS is also presented, followed by a series of open problems that set the stage for future research directions in the field of PTaaS."],"url":"http://arxiv.org/abs/2404.10255v1","category":"cs.LG"}
{"created":"2024-04-16 02:42:06","title":"Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology","abstract":"Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images. This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets. Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases. Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time. We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M). Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond.","sentences":["Featurizing microscopy images for use in biological research remains a significant challenge, especially for large-scale experiments spanning millions of images.","This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets.","Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement when recalling known biological relationships curated from public databases.","Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time.","We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M).","Our findings motivate continued research into scaling self-supervised learning on microscopy data in order to create powerful foundation models of cellular biology that have the potential to catalyze advancements in drug discovery and beyond."],"url":"http://arxiv.org/abs/2404.10242v1","category":"cs.CV"}
{"created":"2024-04-16 02:40:35","title":"Vision-and-Language Navigation via Causal Learning","abstract":"In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.","sentences":["In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments.","This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference.","By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations.","Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training.","Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches.","Code is available at https://github.com/CrystalSixone/VLN-GOAT."],"url":"http://arxiv.org/abs/2404.10241v1","category":"cs.CV"}
{"created":"2024-04-16 02:29:00","title":"Compressible and Searchable: AI-native Multi-Modal Retrieval System with Learned Image Compression","abstract":"The burgeoning volume of digital content across diverse modalities necessitates efficient storage and retrieval methods. Conventional approaches struggle to cope with the escalating complexity and scale of multimedia data. In this paper, we proposed framework addresses this challenge by fusing AI-native multi-modal search capabilities with neural image compression. First we analyze the intricate relationship between compressibility and searchability, recognizing the pivotal role each plays in the efficiency of storage and retrieval systems. Through the usage of simple adapter is to bridge the feature of Learned Image Compression(LIC) and Contrastive Language-Image Pretraining(CLIP) while retaining semantic fidelity and retrieval of multi-modal data. Experimental evaluations on Kodak datasets demonstrate the efficacy of our approach, showcasing significant enhancements in compression efficiency and search accuracy compared to existing methodologies. Our work marks a significant advancement towards scalable and efficient multi-modal search systems in the era of big data.","sentences":["The burgeoning volume of digital content across diverse modalities necessitates efficient storage and retrieval methods.","Conventional approaches struggle to cope with the escalating complexity and scale of multimedia data.","In this paper, we proposed framework addresses this challenge by fusing AI-native multi-modal search capabilities with neural image compression.","First we analyze the intricate relationship between compressibility and searchability, recognizing the pivotal role each plays in the efficiency of storage and retrieval systems.","Through the usage of simple adapter is to bridge the feature of Learned Image Compression(LIC) and Contrastive Language-Image Pretraining(CLIP) while retaining semantic fidelity and retrieval of multi-modal data.","Experimental evaluations on Kodak datasets demonstrate the efficacy of our approach, showcasing significant enhancements in compression efficiency and search accuracy compared to existing methodologies.","Our work marks a significant advancement towards scalable and efficient multi-modal search systems in the era of big data."],"url":"http://arxiv.org/abs/2404.10234v1","category":"cs.AI"}
{"created":"2024-04-16 02:11:46","title":"Find The Gap: Knowledge Base Reasoning For Visual Question Answering","abstract":"We analyze knowledge-based visual question answering, for which given a question, the models need to ground it into the visual modality and retrieve the relevant knowledge from a given large knowledge base (KB) to be able to answer. Our analysis has two folds, one based on designing neural architectures and training them from scratch, and another based on large pre-trained language models (LLMs). Our research questions are: 1) Can we effectively augment models by explicit supervised retrieval of the relevant KB information to solve the KB-VQA problem? 2) How do task-specific and LLM-based models perform in the integration of visual and external knowledge, and multi-hop reasoning over both sources of information? 3) Is the implicit knowledge of LLMs sufficient for KB-VQA and to what extent it can replace the explicit KB? Our results demonstrate the positive impact of empowering task-specific and LLM models with supervised external and visual knowledge retrieval models. Our findings show that though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop reasoning in comparison with our fine-tuned NN model even if the relevant information from both modalities is available to the model. Moreover, we observed that LLM models outperform the NN model for KB-related questions which confirms the effectiveness of implicit knowledge in LLMs however, they do not alleviate the need for external KB.","sentences":["We analyze knowledge-based visual question answering, for which given a question, the models need to ground it into the visual modality and retrieve the relevant knowledge from a given large knowledge base (KB) to be able to answer.","Our analysis has two folds, one based on designing neural architectures and training them from scratch, and another based on large pre-trained language models (LLMs).","Our research questions are: 1)","Can we effectively augment models by explicit supervised retrieval of the relevant KB information to solve the KB-VQA problem?","2) How do task-specific and LLM-based models perform in the integration of visual and external knowledge, and multi-hop reasoning over both sources of information?","3) Is the implicit knowledge of LLMs sufficient for KB-VQA and to what extent it can replace the explicit KB?","Our results demonstrate the positive impact of empowering task-specific and LLM models with supervised external and visual knowledge retrieval models.","Our findings show that though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop reasoning in comparison with our fine-tuned NN model even if the relevant information from both modalities is available to the model.","Moreover, we observed that LLM models outperform the NN model for KB-related questions which confirms the effectiveness of implicit knowledge in LLMs however, they do not alleviate the need for external KB."],"url":"http://arxiv.org/abs/2404.10226v1","category":"cs.AI"}
{"created":"2024-04-16 02:10:20","title":"Rethinking Software Engineering in the Foundation Model Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers","abstract":"The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. However, the current task-driven nature of these copilots falls short in addressing the broader goals and complexities inherent in software engineering (SE). In this paper, we propose a paradigm shift towards goal-driven AI-powered pair programmers that collaborate with human developers in a more holistic and context-aware manner. We envision AI pair programmers that are goal-driven, human partners, SE-aware, and self-learning. These AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making. We discuss the desired attributes of such AI pair programmers and outline key challenges that must be addressed to realize this vision. Ultimately, our work represents a shift from AI-augmented SE to AI-transformed SE by replacing code completion with a collaborative partnership between humans and AI that enhances both productivity and software quality.","sentences":["The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity.","However, the current task-driven nature of these copilots falls short in addressing the broader goals and complexities inherent in software engineering (SE).","In this paper, we propose a paradigm shift towards goal-driven AI-powered pair programmers that collaborate with human developers in a more holistic and context-aware manner.","We envision AI pair programmers that are goal-driven, human partners, SE-aware, and self-learning.","These AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making.","We discuss the desired attributes of such AI pair programmers and outline key challenges that must be addressed to realize this vision.","Ultimately, our work represents a shift from AI-augmented SE to AI-transformed SE by replacing code completion with a collaborative partnership between humans and AI that enhances both productivity and software quality."],"url":"http://arxiv.org/abs/2404.10225v1","category":"cs.SE"}
{"created":"2024-04-16 02:01:56","title":"Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V","abstract":"Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning. On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning. Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures. Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.","sentences":["Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback.","We present COME-robot, the first closed-loop framework utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.","We meticulously construct a library of action primitives for robot exploration, navigation, and manipulation, serving as callable execution modules for GPT-4V in task planning.","On top of these modules, GPT-4V serves as the brain that can accomplish multimodal reasoning, generate action policy with code, verify the task progress, and provide feedback for replanning.","Such design enables COME-robot to (i) actively perceive the environments, (ii) perform situated reasoning, and (iii) recover from failures.","Through comprehensive experiments involving 8 challenging real-world tabletop and manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~25%) compared to state-of-the-art baseline methods.","We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning."],"url":"http://arxiv.org/abs/2404.10220v1","category":"cs.RO"}
{"created":"2024-04-16 01:59:03","title":"Autonomous Implicit Indoor Scene Reconstruction with Frontier Exploration","abstract":"Implicit neural representations have demonstrated significant promise for 3D scene reconstruction. Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method. However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes. In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction. and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection. Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality. Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks. We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality.","sentences":["Implicit neural representations have demonstrated significant promise for 3D scene reconstruction.","Recent works have extended their applications to autonomous implicit reconstruction through the Next Best View (NBV) based method.","However, the NBV method cannot guarantee complete scene coverage and often necessitates extensive viewpoint sampling, particularly in complex scenes.","In the paper, we propose to 1) incorporate frontier-based exploration tasks for global coverage with implicit surface uncertainty-based reconstruction tasks to achieve high-quality reconstruction.","and 2) introduce a method to achieve implicit surface uncertainty using color uncertainty, which reduces the time needed for view selection.","Further with these two tasks, we propose an adaptive strategy for switching modes in view path planning, to reduce time and maintain superior reconstruction quality.","Our method exhibits the highest reconstruction quality among all planning methods and superior planning efficiency in methods involving reconstruction tasks.","We deploy our method on a UAV and the results show that our method can plan multi-task views and reconstruct a scene with high quality."],"url":"http://arxiv.org/abs/2404.10218v1","category":"cs.RO"}
{"created":"2024-04-16 01:45:18","title":"Anomaly Correction of Business Processes Using Transformer Autoencoder","abstract":"Event log records all events that occur during the execution of business processes, so detecting and correcting anomalies in event log can provide reliable guarantee for subsequent process analysis. The previous works mainly include next event prediction based methods and autoencoder-based methods. These methods cannot accurately and efficiently detect anomalies and correct anomalies at the same time, and they all rely on the set threshold to detect anomalies. To solve these problems, we propose a business process anomaly correction method based on Transformer autoencoder. By using self-attention mechanism and autoencoder structure, it can efficiently process event sequences of arbitrary length, and can directly output corrected business process instances, so that it can adapt to various scenarios. At the same time, the anomaly detection is transformed into a classification problem by means of selfsupervised learning, so that there is no need to set a specific threshold in anomaly detection. The experimental results on several real-life event logs show that the proposed method is superior to the previous methods in terms of anomaly detection accuracy and anomaly correction results while ensuring high running efficiency.","sentences":["Event log records all events that occur during the execution of business processes, so detecting and correcting anomalies in event log can provide reliable guarantee for subsequent process analysis.","The previous works mainly include next event prediction based methods and autoencoder-based methods.","These methods cannot accurately and efficiently detect anomalies and correct anomalies at the same time, and they all rely on the set threshold to detect anomalies.","To solve these problems, we propose a business process anomaly correction method based on Transformer autoencoder.","By using self-attention mechanism and autoencoder structure, it can efficiently process event sequences of arbitrary length, and can directly output corrected business process instances, so that it can adapt to various scenarios.","At the same time, the anomaly detection is transformed into a classification problem by means of selfsupervised learning, so that there is no need to set a specific threshold in anomaly detection.","The experimental results on several real-life event logs show that the proposed method is superior to the previous methods in terms of anomaly detection accuracy and anomaly correction results while ensuring high running efficiency."],"url":"http://arxiv.org/abs/2404.10211v1","category":"cs.LG"}
{"created":"2024-04-16 01:38:34","title":"Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models","abstract":"The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. The technologies of interacting with data particularly have an important entanglement with LLMs as efficient and intuitive data interactions are paramount. In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility. DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert. Its system design supports deployment across local, distributed, and cloud environments. Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL). The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs. Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily. The code of DB-GPT is available at Github(https://github.com/eosphoros-ai/DB-GPT) which already has over 10.7k stars.","sentences":["The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software.","The technologies of interacting with data particularly have an important entanglement with LLMs as efficient and intuitive data interactions are paramount.","In this paper, we present DB-GPT, a revolutionary and product-ready Python library that integrates LLMs into traditional data interaction tasks to enhance user experience and accessibility.","DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert.","Its system design supports deployment across local, distributed, and cloud environments.","Beyond handling basic data interaction tasks like Text-to-SQL with LLMs, it can handle complex tasks like generative data analysis through a Multi-Agents framework and the Agentic Workflow Expression Language (AWEL).","The Service-oriented Multi-model Management Framework (SMMF) ensures data privacy and security, enabling users to employ DB-GPT with private LLMs.","Additionally, DB-GPT offers a series of product-ready features designed to enable users to integrate DB-GPT within their product environments easily.","The code of DB-GPT is available at Github(https://github.com/eosphoros-ai/DB-GPT) which already has over 10.7k stars."],"url":"http://arxiv.org/abs/2404.10209v1","category":"cs.AI"}
{"created":"2024-04-16 01:12:20","title":"Research and Practice of Delivering Tabletop Exercises","abstract":"Tabletop exercises are used to train personnel in the efficient mitigation and resolution of incidents. They are applied in practice to support the preparedness of organizations and to highlight inefficient processes. Since tabletop exercises train competencies required in the workplace, they have been introduced into computing courses at universities as an innovation, especially within cybersecurity curricula. To help computing educators adopt this innovative method, we survey academic publications that deal with tabletop exercises. From 140 papers we identified and examined, we selected 14 papers for a detailed review. The results show that the existing research deals predominantly with exercises that follow a linear format and exercises that do not systematically collect data about trainees' learning. Computing education researchers can investigate novel approaches to instruction and assessment in the context of tabletop exercises to maximize the impact of this teaching method. Due to the relatively low number of published papers, the potential for future research is immense. Our review provides researchers, tool developers, and educators with an orientation in the area, a synthesis of trends, and implications for further work.","sentences":["Tabletop exercises are used to train personnel in the efficient mitigation and resolution of incidents.","They are applied in practice to support the preparedness of organizations and to highlight inefficient processes.","Since tabletop exercises train competencies required in the workplace, they have been introduced into computing courses at universities as an innovation, especially within cybersecurity curricula.","To help computing educators adopt this innovative method, we survey academic publications that deal with tabletop exercises.","From 140 papers we identified and examined, we selected 14 papers for a detailed review.","The results show that the existing research deals predominantly with exercises that follow a linear format and exercises that do not systematically collect data about trainees' learning.","Computing education researchers can investigate novel approaches to instruction and assessment in the context of tabletop exercises to maximize the impact of this teaching method.","Due to the relatively low number of published papers, the potential for future research is immense.","Our review provides researchers, tool developers, and educators with an orientation in the area, a synthesis of trends, and implications for further work."],"url":"http://arxiv.org/abs/2404.10206v1","category":"cs.CY"}
{"created":"2024-04-16 00:58:46","title":"Towards a Novel Perspective on Adversarial Examples Driven by Frequency","abstract":"Enhancing our understanding of adversarial examples is crucial for the secure application of machine learning models in real-world scenarios. A prevalent method for analyzing adversarial examples is through a frequency-based approach. However, existing research indicates that attacks designed to exploit low-frequency or high-frequency information can enhance attack performance, leading to an unclear relationship between adversarial perturbations and different frequency components. In this paper, we seek to demystify this relationship by exploring the characteristics of adversarial perturbations within the frequency domain. We employ wavelet packet decomposition for detailed frequency analysis of adversarial examples and conduct statistical examinations across various frequency bands. Intriguingly, our findings indicate that significant adversarial perturbations are present within the high-frequency components of low-frequency bands. Drawing on this insight, we propose a black-box adversarial attack algorithm based on combining different frequency bands. Experiments conducted on multiple datasets and models demonstrate that combining low-frequency bands and high-frequency components of low-frequency bands can significantly enhance attack efficiency. The average attack success rate reaches 99\\%, surpassing attacks that utilize a single frequency segment. Additionally, we introduce the normalized disturbance visibility index as a solution to the limitations of $L_2$ norm in assessing continuous and discrete perturbations.","sentences":["Enhancing our understanding of adversarial examples is crucial for the secure application of machine learning models in real-world scenarios.","A prevalent method for analyzing adversarial examples is through a frequency-based approach.","However, existing research indicates that attacks designed to exploit low-frequency or high-frequency information can enhance attack performance, leading to an unclear relationship between adversarial perturbations and different frequency components.","In this paper, we seek to demystify this relationship by exploring the characteristics of adversarial perturbations within the frequency domain.","We employ wavelet packet decomposition for detailed frequency analysis of adversarial examples and conduct statistical examinations across various frequency bands.","Intriguingly, our findings indicate that significant adversarial perturbations are present within the high-frequency components of low-frequency bands.","Drawing on this insight, we propose a black-box adversarial attack algorithm based on combining different frequency bands.","Experiments conducted on multiple datasets and models demonstrate that combining low-frequency bands and high-frequency components of low-frequency bands can significantly enhance attack efficiency.","The average attack success rate reaches 99\\%, surpassing attacks that utilize a single frequency segment.","Additionally, we introduce the normalized disturbance visibility index as a solution to the limitations of $L_2$ norm in assessing continuous and discrete perturbations."],"url":"http://arxiv.org/abs/2404.10202v1","category":"cs.LG"}
{"created":"2024-04-16 00:54:17","title":"TEL'M: Test and Evaluation of Language Models","abstract":"Language Models have demonstrated remarkable capabilities on some tasks while failing dramatically on others. The situation has generated considerable interest in understanding and comparing the capabilities of various Language Models (LMs) but those efforts have been largely ad hoc with results that are often little more than anecdotal. This is in stark contrast with testing and evaluation processes used in healthcare, radar signal processing, and other defense areas. In this paper, we describe Test and Evaluation of Language Models (TEL'M) as a principled approach for assessing the value of current and future LMs focused on high-value commercial, government and national security applications. We believe that this methodology could be applied to other Artificial Intelligence (AI) technologies as part of the larger goal of \"industrializing\" AI.","sentences":["Language Models have demonstrated remarkable capabilities on some tasks while failing dramatically on others.","The situation has generated considerable interest in understanding and comparing the capabilities of various Language Models (LMs) but those efforts have been largely ad hoc with results that are often little more than anecdotal.","This is in stark contrast with testing and evaluation processes used in healthcare, radar signal processing, and other defense areas.","In this paper, we describe Test and Evaluation of Language Models (TEL'M) as a principled approach for assessing the value of current and future LMs focused on high-value commercial, government and national security applications.","We believe that this methodology could be applied to other Artificial Intelligence (AI) technologies as part of the larger goal of \"industrializing\" AI."],"url":"http://arxiv.org/abs/2404.10200v1","category":"cs.AI"}
{"created":"2024-04-16 00:50:43","title":"CULTURE-GEN: Revealing Global Cultural Perception in Language Models through Natural Language Prompting","abstract":"As the utilization of large language models (LLMs) has proliferated worldwide, it is crucial for them to have adequate knowledge and fair representation for diverse global cultures. In this work, we uncover culture perceptions of three SOTA models on 110 countries and regions on 8 culture-related topics through culture-conditioned generations, and extract symbols from these generations that are associated to each culture by the LLM. We discover that culture-conditioned generation consist of linguistic \"markers\" that distinguish marginalized cultures apart from default cultures. We also discover that LLMs have an uneven degree of diversity in the culture symbols, and that cultures from different geographic regions have different presence in LLMs' culture-agnostic generation. Our findings promote further research in studying the knowledge and fairness of global culture perception in LLMs. Code and Data can be found in: https://github.com/huihanlhh/Culture-Gen/","sentences":["As the utilization of large language models (LLMs) has proliferated worldwide, it is crucial for them to have adequate knowledge and fair representation for diverse global cultures.","In this work, we uncover culture perceptions of three SOTA models on 110 countries and regions on 8 culture-related topics through culture-conditioned generations, and extract symbols from these generations that are associated to each culture by the LLM.","We discover that culture-conditioned generation consist of linguistic \"markers\" that distinguish marginalized cultures apart from default cultures.","We also discover that LLMs have an uneven degree of diversity in the culture symbols, and that cultures from different geographic regions have different presence in LLMs' culture-agnostic generation.","Our findings promote further research in studying the knowledge and fairness of global culture perception in LLMs.","Code and Data can be found in: https://github.com/huihanlhh/Culture-Gen/"],"url":"http://arxiv.org/abs/2404.10199v1","category":"cs.CL"}
{"created":"2024-04-16 00:43:03","title":"How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior","abstract":"Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.","sentences":["Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs).","However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error?","Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error?","To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree.","We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents.","As expected, providing the correct retrieved information fixes most model mistakes (94% accuracy).","However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger.","Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it.","These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents."],"url":"http://arxiv.org/abs/2404.10198v1","category":"cs.CL"}
{"created":"2024-04-16 00:04:46","title":"Smart Pilot Assignment for IoT in Massive MIMO Systems: A Path Towards Scalable IoT Infrastructure","abstract":"5G sets the foundation for an era of creativity with its faster speeds, increased data throughput, reduced latency, and enhanced IoT connectivity, all enabled by Massive MIMO (M-MIMO) technology. M-MIMO boosts network efficiency and enhances user experience by employing intelligent user scheduling. This paper presents a user scheduling scheme and pilot assignment strategy designed for IoT devices, emphasizing mitigating pilot contamination, a key obstacle to improving spectral efficiency (SE) and system scalability in M-MIMO networks. We utilize a user clustering-based pilot allocation scheme to boost IoT device scalability in M-MIMO systems. Additionally, our smart pilot allocation minimizes interference and enhances SE by treating pilot assignment as a graph coloring problem, optimizing it through integer linear programming (ILP). Recognizing the computational complexity of ILP, we introduced a binary search-based heuristic predicated on interference threshold to expedite the computation, while maintaining a near-optimal solution. The simulation results show a significant decrease in the required pilot overhead (about 17%), and substantial enhancement in SE (about 8-14%).","sentences":["5G sets the foundation for an era of creativity with its faster speeds, increased data throughput, reduced latency, and enhanced IoT connectivity, all enabled by Massive MIMO (M-MIMO) technology.","M-MIMO boosts network efficiency and enhances user experience by employing intelligent user scheduling.","This paper presents a user scheduling scheme and pilot assignment strategy designed for IoT devices, emphasizing mitigating pilot contamination, a key obstacle to improving spectral efficiency (SE) and system scalability in M-MIMO networks.","We utilize a user clustering-based pilot allocation scheme to boost IoT device scalability in M-MIMO systems.","Additionally, our smart pilot allocation minimizes interference and enhances SE by treating pilot assignment as a graph coloring problem, optimizing it through integer linear programming (ILP).","Recognizing the computational complexity of ILP, we introduced a binary search-based heuristic predicated on interference threshold to expedite the computation, while maintaining a near-optimal solution.","The simulation results show a significant decrease in the required pilot overhead (about 17%), and substantial enhancement in SE (about 8-14%)."],"url":"http://arxiv.org/abs/2404.10188v1","category":"cs.NI"}
{"created":"2024-04-15 23:34:01","title":"Errors in single pixel photography emerging from light collection limits by the bucket detector","abstract":"In single pixel photography an image is sampled with a programmable optical element like a digital micromirror array or a spatial light modulator that can project an orthogonal base. The light reflected or diffracted is collected by a lens and measured with a photodiode (bucket detector). In this work we demonstrate that single pixel photography that uses sampling bases with non-zero off-diagonal elements (i.e. Hadamard), can be susceptible to errors that emerge from the relative size of the bucket detector area compared with the spatial spread of the Fourier spectrum of the base element that has the the highest spatial frequency. Experiments with a spatial light modulator and simulations using a Hadamard basis show that if the bucket detector area is smaller than between $50-75\\%$ of the maximum area spanned by the projected spectrum of the measurement basis, the reconstructed photograph will exhibit cross-talk with the effective phase of the optical system. The phase can be encoded or errors can be introduced in the optical system to demonstrate this effect.","sentences":["In single pixel photography an image is sampled with a programmable optical element like a digital micromirror array or a spatial light modulator that can project an orthogonal base.","The light reflected or diffracted is collected by a lens and measured with a photodiode (bucket detector).","In this work we demonstrate that single pixel photography that uses sampling bases with non-zero off-diagonal elements (i.e. Hadamard), can be susceptible to errors that emerge from the relative size of the bucket detector area compared with the spatial spread of the Fourier spectrum of the base element that has the the highest spatial frequency.","Experiments with a spatial light modulator and simulations using a Hadamard basis show that if the bucket detector area is smaller than between $50-75\\%$ of the maximum area spanned by the projected spectrum of the measurement basis, the reconstructed photograph will exhibit cross-talk with the effective phase of the optical system.","The phase can be encoded or errors can be introduced in the optical system to demonstrate this effect."],"url":"http://arxiv.org/abs/2404.10182v1","category":"physics.optics"}
{"created":"2024-04-15 23:28:13","title":"Deferred NAM: Low-latency Top-K Context Injection via DeferredContext Encoding for Non-Streaming ASR","abstract":"Contextual biasing enables speech recognizers to transcribe important phrases in the speaker's context, such as contact names, even if they are rare in, or absent from, the training data. Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time components. Such biasers typically consist of a context encoder; followed by a context filter which narrows down the context to apply, improving per-step inference time; and, finally, context application via cross attention. Though much work has gone into optimizing per-frame performance, the context encoder is at least as important: recognition cannot begin before context encoding ends. Here, we show the lightweight phrase selection pass can be moved before context encoding, resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K phrases with a maximum pre-decoding delay under 33ms. With the addition of phrase- and wordpiece-level cross-entropy losses, our technique also achieves up to a 37.5% relative WER reduction over the baseline without the losses and lightweight phrase selection pass.","sentences":["Contextual biasing enables speech recognizers to transcribe important phrases in the speaker's context, such as contact names, even if they are rare in, or absent from, the training data.","Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time components.","Such biasers typically consist of a context encoder; followed by a context filter which narrows down the context to apply, improving per-step inference time; and, finally, context application via cross attention.","Though much work has gone into optimizing per-frame performance, the context encoder is at least as important: recognition cannot begin before context encoding ends.","Here, we show the lightweight phrase selection pass can be moved before context encoding, resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K phrases with a maximum pre-decoding delay under 33ms.","With the addition of phrase- and wordpiece-level cross-entropy losses, our technique also achieves up to a 37.5% relative WER reduction over the baseline without the losses and lightweight phrase selection pass."],"url":"http://arxiv.org/abs/2404.10180v1","category":"cs.CL"}
{"created":"2024-04-15 22:49:37","title":"High-Resolution Detection of Earth Structural Heterogeneities from Seismic Amplitudes using Convolutional Neural Networks with Attention layers","abstract":"Earth structural heterogeneities have a remarkable role in the petroleum economy for both exploration and production projects. Automatic detection of detailed structural heterogeneities is challenging when considering modern machine learning techniques like deep neural networks. Typically, these techniques can be an excellent tool for assisted interpretation of such heterogeneities, but it heavily depends on the amount of data to be trained.   We propose an efficient and cost-effective architecture for detecting seismic structural heterogeneities using Convolutional Neural Networks (CNNs) combined with Attention layers. The attention mechanism reduces costs and enhances accuracy, even in cases with relatively noisy data. Our model has half the parameters compared to the state-of-the-art, and it outperforms previous methods in terms of Intersection over Union (IoU) by 0.6% and precision by 0.4%. By leveraging synthetic data, we apply transfer learning to train and fine-tune the model, addressing the challenge of limited annotated data availability.","sentences":["Earth structural heterogeneities have a remarkable role in the petroleum economy for both exploration and production projects.","Automatic detection of detailed structural heterogeneities is challenging when considering modern machine learning techniques like deep neural networks.","Typically, these techniques can be an excellent tool for assisted interpretation of such heterogeneities, but it heavily depends on the amount of data to be trained.   ","We propose an efficient and cost-effective architecture for detecting seismic structural heterogeneities using Convolutional Neural Networks (CNNs) combined with Attention layers.","The attention mechanism reduces costs and enhances accuracy, even in cases with relatively noisy data.","Our model has half the parameters compared to the state-of-the-art, and it outperforms previous methods in terms of Intersection over Union (IoU) by 0.6% and precision by 0.4%.","By leveraging synthetic data, we apply transfer learning to train and fine-tune the model, addressing the challenge of limited annotated data availability."],"url":"http://arxiv.org/abs/2404.10170v1","category":"cs.CV"}
{"created":"2024-04-15 22:26:27","title":"EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning","abstract":"From a visual perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus. While existing models can accurately predict regions and objects that are likely to attract attention ``on average'', so far there is no scanpath model capable of predicting scanpaths for an individual. To close this gap, we introduce EyeFormer, which leverages a Transformer architecture as a policy network to guide a deep reinforcement learning algorithm that controls gaze locations. Our model has the unique capability of producing personalized predictions when given a few user scanpath samples. It can predict full scanpath information, including fixation positions and duration, across individuals and various stimulus types. Additionally, we demonstrate applications in GUI layout optimization driven by our model. Our software and models will be publicly available.","sentences":["From a visual perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus.","While existing models can accurately predict regions and objects that are likely to attract attention ``on average'', so far there is no scanpath model capable of predicting scanpaths for an individual.","To close this gap, we introduce EyeFormer, which leverages a Transformer architecture as a policy network to guide a deep reinforcement learning algorithm that controls gaze locations.","Our model has the unique capability of producing personalized predictions when given a few user scanpath samples.","It can predict full scanpath information, including fixation positions and duration, across individuals and various stimulus types.","Additionally, we demonstrate applications in GUI layout optimization driven by our model.","Our software and models will be publicly available."],"url":"http://arxiv.org/abs/2404.10163v1","category":"cs.CV"}
{"created":"2024-04-15 22:25:54","title":"Optimal Kernel Tuning Parameter Prediction using Deep Sequence Models","abstract":"GPU kernels have come to the forefront of computing due to their utility in varied fields, from high-performance computing to machine learning. A typical GPU compute kernel is invoked millions, if not billions of times in a typical application, which makes their performance highly critical. Due to the unknown nature of the optimization surface, an exhaustive search is required to discover the global optimum, which is infeasible due to the possible exponential number of parameter combinations. In this work, we propose a methodology that uses deep sequence-to-sequence models to predict the optimal tuning parameters governing compute kernels. This work considers the prediction of kernel parameters as a sequence to the sequence translation problem, borrowing models from the Natural Language Processing (NLP) domain. Parameters describing the input, output and weight tensors are considered as the input language to the model that emits the corresponding kernel parameters. In essence, the model translates the problem parameter language to kernel parameter language. The core contributions of this work are: a) Proposing that a sequence to sequence model can accurately learn the performance dynamics of a GPU compute kernel b) A novel network architecture which predicts the kernel tuning parameters for GPU kernels, c) A constrained beam search which incorporates the physical limits of the GPU hardware as well as other expert knowledge reducing the search space. The proposed algorithm can achieve more than 90% accuracy on various convolutional kernels in MIOpen, the AMD machine learning primitives library. As a result, the proposed technique can reduce the development time and compute resources required to tune unseen input configurations, resulting in shorter development cycles, reduced development costs, and better user experience.","sentences":["GPU kernels have come to the forefront of computing due to their utility in varied fields, from high-performance computing to machine learning.","A typical GPU compute kernel is invoked millions, if not billions of times in a typical application, which makes their performance highly critical.","Due to the unknown nature of the optimization surface, an exhaustive search is required to discover the global optimum, which is infeasible due to the possible exponential number of parameter combinations.","In this work, we propose a methodology that uses deep sequence-to-sequence models to predict the optimal tuning parameters governing compute kernels.","This work considers the prediction of kernel parameters as a sequence to the sequence translation problem, borrowing models from the Natural Language Processing (NLP) domain.","Parameters describing the input, output and weight tensors are considered as the input language to the model that emits the corresponding kernel parameters.","In essence, the model translates the problem parameter language to kernel parameter language.","The core contributions of this work are: a) Proposing that a sequence to sequence model can accurately learn the performance dynamics of a GPU compute kernel b)","A novel network architecture which predicts the kernel tuning parameters for GPU kernels, c) A constrained beam search which incorporates the physical limits of the GPU hardware as well as other expert knowledge reducing the search space.","The proposed algorithm can achieve more than 90% accuracy on various convolutional kernels in MIOpen, the AMD machine learning primitives library.","As a result, the proposed technique can reduce the development time and compute resources required to tune unseen input configurations, resulting in shorter development cycles, reduced development costs, and better user experience."],"url":"http://arxiv.org/abs/2404.10162v1","category":"cs.LG"}
{"created":"2024-04-15 22:18:50","title":"Deceiving to Enlighten: Coaxing LLMs to Self-Reflection for Enhanced Bias Detection and Mitigation","abstract":"Large Language Models (LLMs) embed complex biases and stereotypes that can lead to detrimental user experiences and societal consequences, often without conscious awareness from the models themselves. This paper emphasizes the importance of equipping LLMs with mechanisms for better self-reflection and bias recognition. Our experiments demonstrate that by informing LLMs that their generated content does not represent their own views and questioning them about bias, their capability to identify and address biases improves. This enhancement is attributed to the internal attention mechanisms and potential internal sensitivity policies of LLMs. Building upon these findings, we propose a novel method to diminish bias in LLM outputs. This involves engaging LLMs in multi-role scenarios acting as different roles where they are tasked for bias exposure, with a role of an impartial referee in the end of each loop of debate. A ranking scoring mechanism is employed to quantify bias levels, enabling more refined reflections and superior output quality. Comparative experimental results confirm that our method outperforms existing approaches in reducing bias, making it a valuable contribution to efforts towards more ethical AI systems.","sentences":["Large Language Models (LLMs) embed complex biases and stereotypes that can lead to detrimental user experiences and societal consequences, often without conscious awareness from the models themselves.","This paper emphasizes the importance of equipping LLMs with mechanisms for better self-reflection and bias recognition.","Our experiments demonstrate that by informing LLMs that their generated content does not represent their own views and questioning them about bias, their capability to identify and address biases improves.","This enhancement is attributed to the internal attention mechanisms and potential internal sensitivity policies of LLMs.","Building upon these findings, we propose a novel method to diminish bias in LLM outputs.","This involves engaging LLMs in multi-role scenarios acting as different roles where they are tasked for bias exposure, with a role of an impartial referee in the end of each loop of debate.","A ranking scoring mechanism is employed to quantify bias levels, enabling more refined reflections and superior output quality.","Comparative experimental results confirm that our method outperforms existing approaches in reducing bias, making it a valuable contribution to efforts towards more ethical AI systems."],"url":"http://arxiv.org/abs/2404.10160v1","category":"cs.AI"}
{"created":"2024-04-15 21:22:57","title":"Shaping Realities: Enhancing 3D Generative AI with Fabrication Constraints","abstract":"Generative AI tools are becoming more prevalent in 3D modeling, enabling users to manipulate or create new models with text or images as inputs. This makes it easier for users to rapidly customize and iterate on their 3D designs and explore new creative ideas. These methods focus on the aesthetic quality of the 3D models, refining them to look similar to the prompts provided by the user. However, when creating 3D models intended for fabrication, designers need to trade-off the aesthetic qualities of a 3D model with their intended physical properties. To be functional post-fabrication, 3D models have to satisfy structural constraints informed by physical principles. Currently, such requirements are not enforced by generative AI tools. This leads to the development of aesthetically appealing, but potentially non-functional 3D geometry, that would be hard to fabricate and use in the real world. This workshop paper highlights the limitations of generative AI tools in translating digital creations into the physical world and proposes new augmentations to generative AI tools for creating physically viable 3D models. We advocate for the development of tools that manipulate or generate 3D models by considering not only the aesthetic appearance but also using physical properties as constraints. This exploration seeks to bridge the gap between digital creativity and real-world applicability, extending the creative potential of generative AI into the tangible domain.","sentences":["Generative AI tools are becoming more prevalent in 3D modeling, enabling users to manipulate or create new models with text or images as inputs.","This makes it easier for users to rapidly customize and iterate on their 3D designs and explore new creative ideas.","These methods focus on the aesthetic quality of the 3D models, refining them to look similar to the prompts provided by the user.","However, when creating 3D models intended for fabrication, designers need to trade-off the aesthetic qualities of a 3D model with their intended physical properties.","To be functional post-fabrication, 3D models have to satisfy structural constraints informed by physical principles.","Currently, such requirements are not enforced by generative AI tools.","This leads to the development of aesthetically appealing, but potentially non-functional 3D geometry, that would be hard to fabricate and use in the real world.","This workshop paper highlights the limitations of generative AI tools in translating digital creations into the physical world and proposes new augmentations to generative AI tools for creating physically viable 3D models.","We advocate for the development of tools that manipulate or generate 3D models by considering not only the aesthetic appearance but also using physical properties as constraints.","This exploration seeks to bridge the gap between digital creativity and real-world applicability, extending the creative potential of generative AI into the tangible domain."],"url":"http://arxiv.org/abs/2404.10142v2","category":"cs.HC"}
{"created":"2024-04-15 21:02:48","title":"Language Model Cascades: Token-level uncertainty and beyond","abstract":"Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most \"easy\" instances, while a few \"hard\" instances are deferred to the large model. While the principles underpinning cascading are well-studied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples. To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.","sentences":["Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs.","Cascading offers a simple strategy to achieve more favorable cost-quality tradeoffs: here, a small model is invoked for most \"easy\" instances, while a few \"hard\" instances are deferred to the large model.","While the principles underpinning cascading are well-studied for classification tasks - with deferral based on predicted class uncertainty favored theoretically and practically - a similar understanding is lacking for generative LM tasks.","In this work, we initiate a systematic study of deferral rules for LM cascades.","We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty.","We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths.","This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across examples.","To mitigate this issue, we propose to exploit the richer token-level uncertainty information implicit in generative LMs.","We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties.","By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models.","We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff."],"url":"http://arxiv.org/abs/2404.10136v1","category":"cs.CL"}
{"created":"2024-04-15 21:01:31","title":"Using Long Short-term Memory (LSTM) to merge precipitation data over mountainous area in Sierra Nevada","abstract":"Obtaining reliable precipitation estimation with high resolutions in time and space is of great importance to hydrological studies. However, accurately estimating precipitation is a challenging task over high mountainous complex terrain. The three widely used precipitation measurement approaches, namely rainfall gauge, precipitation radars, and satellite-based precipitation sensors, have their own pros and cons in producing reliable precipitation products over complex areas. One way to decrease the detection error probability and improve data reliability is precipitation data merging. With the rapid advancements in computational capabilities and the escalating volume and diversity of earth observational data, Deep Learning (DL) models have gained considerable attention in geoscience. In this study, a deep learning technique, namely Long Short-term Memory (LSTM), was employed to merge a radar-based and a satellite-based Global Precipitation Measurement (GPM) precipitation product Integrated Multi-Satellite Retrievals for GPM (IMERG) precipitation product at hourly scale. The merged results are compared with the widely used reanalysis precipitation product, Multi-Radar Multi-Sensor (MRMS), and assessed against gauge observational data from the California Data Exchange Center (CDEC). The findings indicated that the LSTM-based merged precipitation notably underestimated gauge observations and, at times, failed to provide meaningful estimates, showing predominantly near-zero values. Relying solely on individual Quantitative Precipitation Estimates (QPEs) without additional meteorological input proved insufficient for generating reliable merged QPE. However, the merged results effectively captured the temporal trends of the observations, outperforming MRMS in this aspect. This suggested that incorporating bias correction techniques could potentially enhance the accuracy of the merged product.","sentences":["Obtaining reliable precipitation estimation with high resolutions in time and space is of great importance to hydrological studies.","However, accurately estimating precipitation is a challenging task over high mountainous complex terrain.","The three widely used precipitation measurement approaches, namely rainfall gauge, precipitation radars, and satellite-based precipitation sensors, have their own pros and cons in producing reliable precipitation products over complex areas.","One way to decrease the detection error probability and improve data reliability is precipitation data merging.","With the rapid advancements in computational capabilities and the escalating volume and diversity of earth observational data, Deep Learning (DL) models have gained considerable attention in geoscience.","In this study, a deep learning technique, namely Long Short-term Memory (LSTM), was employed to merge a radar-based and a satellite-based Global Precipitation Measurement (GPM) precipitation product Integrated Multi-Satellite Retrievals for GPM (IMERG) precipitation product at hourly scale.","The merged results are compared with the widely used reanalysis precipitation product, Multi-Radar Multi-Sensor (MRMS), and assessed against gauge observational data from the California Data Exchange Center (CDEC).","The findings indicated that the LSTM-based merged precipitation notably underestimated gauge observations and, at times, failed to provide meaningful estimates, showing predominantly near-zero values.","Relying solely on individual Quantitative Precipitation Estimates (QPEs) without additional meteorological input proved insufficient for generating reliable merged QPE.","However, the merged results effectively captured the temporal trends of the observations, outperforming MRMS in this aspect.","This suggested that incorporating bias correction techniques could potentially enhance the accuracy of the merged product."],"url":"http://arxiv.org/abs/2404.10135v1","category":"cs.LG"}
{"created":"2024-04-15 19:50:46","title":"From Predictive Algorithms to Automatic Generation of Anomalies","abstract":"Machine learning algorithms can find predictive signals that researchers fail to notice; yet they are notoriously hard-to-interpret. How can we extract theoretical insights from these black boxes? History provides a clue. Facing a similar problem -- how to extract theoretical insights from their intuitions -- researchers often turned to ``anomalies:'' constructed examples that highlight flaws in an existing theory and spur the development of new ones. Canonical examples include the Allais paradox and the Kahneman-Tversky choice experiments for expected utility theory. We suggest anomalies can extract theoretical insights from black box predictive algorithms. We develop procedures to automatically generate anomalies for an existing theory when given a predictive algorithm. We cast anomaly generation as an adversarial game between a theory and a falsifier, the solutions to which are anomalies: instances where the black box algorithm predicts - were we to collect data - we would likely observe violations of the theory. As an illustration, we generate anomalies for expected utility theory using a large, publicly available dataset on real lottery choices. Based on an estimated neural network that predicts lottery choices, our procedures recover known anomalies and discover new ones for expected utility theory. In incentivized experiments, subjects violate expected utility theory on these algorithmically generated anomalies; moreover, the violation rates are similar to observed rates for the Allais paradox and Common ratio effect.","sentences":["Machine learning algorithms can find predictive signals that researchers fail to notice; yet they are notoriously hard-to-interpret.","How can we extract theoretical insights from these black boxes?","History provides a clue.","Facing a similar problem -- how to extract theoretical insights from their intuitions -- researchers often turned to ``anomalies:'' constructed examples that highlight flaws in an existing theory and spur the development of new ones.","Canonical examples include the Allais paradox and the Kahneman-Tversky choice experiments for expected utility theory.","We suggest anomalies can extract theoretical insights from black box predictive algorithms.","We develop procedures to automatically generate anomalies for an existing theory when given a predictive algorithm.","We cast anomaly generation as an adversarial game between a theory and a falsifier, the solutions to which are anomalies: instances where the black box algorithm predicts - were we to collect data - we would likely observe violations of the theory.","As an illustration, we generate anomalies for expected utility theory using a large, publicly available dataset on real lottery choices.","Based on an estimated neural network that predicts lottery choices, our procedures recover known anomalies and discover new ones for expected utility theory.","In incentivized experiments, subjects violate expected utility theory on these algorithmically generated anomalies; moreover, the violation rates are similar to observed rates for the Allais paradox and Common ratio effect."],"url":"http://arxiv.org/abs/2404.10111v1","category":"econ.EM"}
{"created":"2024-04-15 19:45:07","title":"Communication-Efficient Hybrid Federated Learning for E-health with Horizontal and Vertical Data Partitioning","abstract":"E-health allows smart devices and medical institutions to collaboratively collect patients' data, which is trained by Artificial Intelligence (AI) technologies to help doctors make diagnosis. By allowing multiple devices to train models collaboratively, federated learning is a promising solution to address the communication and privacy issues in e-health. However, applying federated learning in e-health faces many challenges. First, medical data is both horizontally and vertically partitioned. Since single Horizontal Federated Learning (HFL) or Vertical Federated Learning (VFL) techniques cannot deal with both types of data partitioning, directly applying them may consume excessive communication cost due to transmitting a part of raw data when requiring high modeling accuracy. Second, a naive combination of HFL and VFL has limitations including low training efficiency, unsound convergence analysis, and lack of parameter tuning strategies. In this paper, we provide a thorough study on an effective integration of HFL and VFL, to achieve communication efficiency and overcome the above limitations when data is both horizontally and vertically partitioned. Specifically, we propose a hybrid federated learning framework with one intermediate result exchange and two aggregation phases. Based on this framework, we develop a Hybrid Stochastic Gradient Descent (HSGD) algorithm to train models. Then, we theoretically analyze the convergence upper bound of the proposed algorithm. Using the convergence results, we design adaptive strategies to adjust the training parameters and shrink the size of transmitted data. Experimental results validate that the proposed HSGD algorithm can achieve the desired accuracy while reducing communication cost, and they also verify the effectiveness of the adaptive strategies.","sentences":["E-health allows smart devices and medical institutions to collaboratively collect patients' data, which is trained by Artificial Intelligence (AI) technologies to help doctors make diagnosis.","By allowing multiple devices to train models collaboratively, federated learning is a promising solution to address the communication and privacy issues in e-health.","However, applying federated learning in e-health faces many challenges.","First, medical data is both horizontally and vertically partitioned.","Since single Horizontal Federated Learning (HFL) or Vertical Federated Learning (VFL) techniques cannot deal with both types of data partitioning, directly applying them may consume excessive communication cost due to transmitting a part of raw data when requiring high modeling accuracy.","Second, a naive combination of HFL and VFL has limitations including low training efficiency, unsound convergence analysis, and lack of parameter tuning strategies.","In this paper, we provide a thorough study on an effective integration of HFL and VFL, to achieve communication efficiency and overcome the above limitations when data is both horizontally and vertically partitioned.","Specifically, we propose a hybrid federated learning framework with one intermediate result exchange and two aggregation phases.","Based on this framework, we develop a Hybrid Stochastic Gradient Descent (HSGD) algorithm to train models.","Then, we theoretically analyze the convergence upper bound of the proposed algorithm.","Using the convergence results, we design adaptive strategies to adjust the training parameters and shrink the size of transmitted data.","Experimental results validate that the proposed HSGD algorithm can achieve the desired accuracy while reducing communication cost, and they also verify the effectiveness of the adaptive strategies."],"url":"http://arxiv.org/abs/2404.10110v1","category":"cs.LG"}
{"created":"2024-04-15 19:38:04","title":"Purple is the new green: biopigments and spectra of Earth-like purple worlds","abstract":"With more than 5500 detected exoplanets, the search for life is entering a new era. Using life on Earth as our guide, we look beyond green landscapes to expand our ability to detect signs of surface life on other worlds. While oxygenic photosynthesis gives rise to modern green landscapes, bacteriochlorophyll-based anoxygenic phototrophs can also colour their habitats and could dominate a much wider range of environments on Earth-like exoplanets. Here, we characterize the reflectance spectra of a collection of purple sulfur and purple non-sulfur bacteria from a variety of anoxic and oxic environments. We present models for Earth-like planets where purple bacteria dominate the surface and show the impact of their signatures on the reflectance spectra of terrestrial exoplanets. Our research provides a new resource to guide the detection of purple bacteria and improves our chances of detecting life on exoplanets with upcoming telescopes. Our biological pigment data base for purple bacteria and the high-resolution spectra of Earth-like planets, including ocean worlds, snowball planets, frozen worlds, and Earth analogues, are available online, providing a tool for modellers and observers to train retrieval algorithms, optimize search strategies, and inform models of Earth-like planets, where purple is the new green.","sentences":["With more than 5500 detected exoplanets, the search for life is entering a new era.","Using life on Earth as our guide, we look beyond green landscapes to expand our ability to detect signs of surface life on other worlds.","While oxygenic photosynthesis gives rise to modern green landscapes, bacteriochlorophyll-based anoxygenic phototrophs can also colour their habitats and could dominate a much wider range of environments on Earth-like exoplanets.","Here, we characterize the reflectance spectra of a collection of purple sulfur and purple non-sulfur bacteria from a variety of anoxic and oxic environments.","We present models for Earth-like planets where purple bacteria dominate the surface and show the impact of their signatures on the reflectance spectra of terrestrial exoplanets.","Our research provides a new resource to guide the detection of purple bacteria and improves our chances of detecting life on exoplanets with upcoming telescopes.","Our biological pigment data base for purple bacteria and the high-resolution spectra of Earth-like planets, including ocean worlds, snowball planets, frozen worlds, and Earth analogues, are available online, providing a tool for modellers and observers to train retrieval algorithms, optimize search strategies, and inform models of Earth-like planets, where purple is the new green."],"url":"http://arxiv.org/abs/2404.10105v1","category":"astro-ph.EP"}
{"created":"2024-04-15 19:19:56","title":"Chinchilla Scaling: A replication attempt","abstract":"Hoffmann et al. (2022) propose three methods for estimating a compute-optimal scaling law. We attempt to replicate their third estimation procedure, which involves fitting a parametric loss function to a reconstruction of data from their plots. We find that the reported estimates are inconsistent with their first two estimation methods, fail at fitting the extracted data, and report implausibly narrow confidence intervals--intervals this narrow would require over 600,000 experiments, while they likely only ran fewer than 500. In contrast, our rederivation of the scaling law using the third approach yields results that are compatible with the findings from the first two estimation procedures described by Hoffmann et al.","sentences":["Hoffmann et al.","(2022) propose three methods for estimating a compute-optimal scaling law.","We attempt to replicate their third estimation procedure, which involves fitting a parametric loss function to a reconstruction of data from their plots.","We find that the reported estimates are inconsistent with their first two estimation methods, fail at fitting the extracted data, and report implausibly narrow confidence intervals--intervals this narrow would require over 600,000 experiments, while they likely only ran fewer than 500.","In contrast, our rederivation of the scaling law using the third approach yields results that are compatible with the findings from the first two estimation procedures described by Hoffmann et al."],"url":"http://arxiv.org/abs/2404.10102v1","category":"cs.AI"}
{"created":"2024-04-15 19:08:48","title":"LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT Large Language Model","abstract":"A contract is a type of legal document commonly used in organizations. Contract review is an integral and repetitive process to avoid business risk and liability. Contract analysis requires the identification and classification of key provisions and paragraphs within an agreement. Identification and validation of contract clauses can be a time-consuming and challenging task demanding the services of trained and expensive lawyers, paralegals or other legal assistants. Classification of legal provisions in contracts using artificial intelligence and natural language processing is complex due to the requirement of domain-specialized legal language for model training and the scarcity of sufficient labeled data in the legal domain. Using general-purpose models is not effective in this context due to the use of specialized legal vocabulary in contracts which may not be recognized by a general model. To address this problem, we propose the use of a pre-trained large language model which is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a BERT transformer architecture model that we fine-tune to efficiently handle classification task for legal provisions. We conducted experiments to measure and compare metrics with current benchmark results. We found that LegalPro-BERT outperforms the previous benchmark used for comparison in this research.","sentences":["A contract is a type of legal document commonly used in organizations.","Contract review is an integral and repetitive process to avoid business risk and liability.","Contract analysis requires the identification and classification of key provisions and paragraphs within an agreement.","Identification and validation of contract clauses can be a time-consuming and challenging task demanding the services of trained and expensive lawyers, paralegals or other legal assistants.","Classification of legal provisions in contracts using artificial intelligence and natural language processing is complex due to the requirement of domain-specialized legal language for model training and the scarcity of sufficient labeled data in the legal domain.","Using general-purpose models is not effective in this context due to the use of specialized legal vocabulary in contracts which may not be recognized by a general model.","To address this problem, we propose the use of a pre-trained large language model which is subsequently calibrated on legal taxonomy.","We propose LegalPro-BERT, a BERT transformer architecture model that we fine-tune to efficiently handle classification task for legal provisions.","We conducted experiments to measure and compare metrics with current benchmark results.","We found that LegalPro-BERT outperforms the previous benchmark used for comparison in this research."],"url":"http://arxiv.org/abs/2404.10097v1","category":"cs.AI"}
{"created":"2024-04-15 19:06:58","title":"Vision Augmentation Prediction Autoencoder with Attention Design (VAPAAD)","abstract":"Recent advancements in sequence prediction have significantly improved the accuracy of video data interpretation; however, existing models often overlook the potential of attention-based mechanisms for next-frame prediction. This study introduces the Vision Augmentation Prediction Autoencoder with Attention Design (VAPAAD), an innovative approach that integrates attention mechanisms into sequence prediction, enabling nuanced analysis and understanding of temporal dynamics in video sequences. Utilizing the Moving MNIST dataset, we demonstrate VAPAAD's robust performance and superior handling of complex temporal data compared to traditional methods. VAPAAD combines data augmentation, ConvLSTM2D layers, and a custom-built self-attention mechanism to effectively focus on salient features within a sequence, enhancing predictive accuracy and context-aware analysis. This methodology not only adheres to human cognitive processes during video interpretation but also addresses limitations in conventional models, which often struggle with the variability inherent in video sequences. The experimental results confirm that VAPAAD outperforms existing models, especially in integrating attention mechanisms, which significantly improve predictive performance.","sentences":["Recent advancements in sequence prediction have significantly improved the accuracy of video data interpretation; however, existing models often overlook the potential of attention-based mechanisms for next-frame prediction.","This study introduces the Vision Augmentation Prediction Autoencoder with Attention Design (VAPAAD), an innovative approach that integrates attention mechanisms into sequence prediction, enabling nuanced analysis and understanding of temporal dynamics in video sequences.","Utilizing the Moving MNIST dataset, we demonstrate VAPAAD's robust performance and superior handling of complex temporal data compared to traditional methods.","VAPAAD combines data augmentation, ConvLSTM2D layers, and a custom-built self-attention mechanism to effectively focus on salient features within a sequence, enhancing predictive accuracy and context-aware analysis.","This methodology not only adheres to human cognitive processes during video interpretation but also addresses limitations in conventional models, which often struggle with the variability inherent in video sequences.","The experimental results confirm that VAPAAD outperforms existing models, especially in integrating attention mechanisms, which significantly improve predictive performance."],"url":"http://arxiv.org/abs/2404.10096v2","category":"cs.CV"}
{"created":"2024-04-15 18:58:39","title":"Empowering Federated Learning with Implicit Gossiping: Mitigating Connection Unreliability Amidst Unknown and Arbitrary Dynamics","abstract":"Federated learning is a popular distributed learning approach for training a machine learning model without disclosing raw data. It consists of a parameter server and a possibly large collection of clients (e.g., in cross-device federated learning) that may operate in congested and changing environments. In this paper, we study federated learning in the presence of stochastic and dynamic communication failures wherein the uplink between the parameter server and client $i$ is on with unknown probability $p_i^t$ in round $t$. Furthermore, we allow the dynamics of $p_i^t$ to be arbitrary.   We first demonstrate that when the $p_i^t$'s vary across clients, the most widely adopted federated learning algorithm, Federated Average (FedAvg), experiences significant bias. To address this observation, we propose Federated Postponed Broadcast (FedPBC), a simple variant of FedAvg. FedPBC differs from FedAvg in that the parameter server postpones broadcasting the global model till the end of each round. Despite uplink failures, we show that FedPBC converges to a stationary point of the original non-convex objective. On the technical front, postponing the global model broadcasts enables implicit gossiping among the clients with active links in round $t$. Despite the time-varying nature of $p_i^t$, we can bound the perturbation of the global model dynamics using techniques to control gossip-type information mixing errors. Extensive experiments have been conducted on real-world datasets over diversified unreliable uplink patterns to corroborate our analysis.","sentences":["Federated learning is a popular distributed learning approach for training a machine learning model without disclosing raw data.","It consists of a parameter server and a possibly large collection of clients (e.g., in cross-device federated learning) that may operate in congested and changing environments.","In this paper, we study federated learning in the presence of stochastic and dynamic communication failures wherein the uplink between the parameter server and client $i$ is on with unknown probability $p_i^t$ in round $t$. Furthermore, we allow the dynamics of $p_i^t$ to be arbitrary.   ","We first demonstrate that when the $p_i^t$'s vary across clients, the most widely adopted federated learning algorithm, Federated Average (FedAvg), experiences significant bias.","To address this observation, we propose Federated Postponed Broadcast (FedPBC), a simple variant of FedAvg.","FedPBC differs from FedAvg in that the parameter server postpones broadcasting the global model till the end of each round.","Despite uplink failures, we show that FedPBC converges to a stationary point of the original non-convex objective.","On the technical front, postponing the global model broadcasts enables implicit gossiping among the clients with active links in round $t$. Despite the time-varying nature of $p_i^t$, we can bound the perturbation of the global model dynamics using techniques to control gossip-type information mixing errors.","Extensive experiments have been conducted on real-world datasets over diversified unreliable uplink patterns to corroborate our analysis."],"url":"http://arxiv.org/abs/2404.10091v1","category":"cs.DC"}
{"created":"2024-04-15 18:32:52","title":"Low-Light Image Enhancement Framework for Improved Object Detection in Fisheye Lens Datasets","abstract":"This study addresses the evolving challenges in urban traffic monitoring detection systems based on fisheye lens cameras by proposing a framework that improves the efficacy and accuracy of these systems. In the context of urban infrastructure and transportation management, advanced traffic monitoring systems have become critical for managing the complexities of urbanization and increasing vehicle density. Traditional monitoring methods, which rely on static cameras with narrow fields of view, are ineffective in dynamic urban environments, necessitating the installation of multiple cameras, which raises costs. Fisheye lenses, which were recently introduced, provide wide and omnidirectional coverage in a single frame, making them a transformative solution. However, issues such as distorted views and blurriness arise, preventing accurate object detection on these images. Motivated by these challenges, this study proposes a novel approach that combines a ransformer-based image enhancement framework and ensemble learning technique to address these challenges and improve traffic monitoring accuracy, making significant contributions to the future of intelligent traffic management systems. Our proposed methodological framework won 5th place in the 2024 AI City Challenge, Track 4, with an F1 score of 0.5965 on experimental validation data. The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system. Our code is publicly available at https://github.com/daitranskku/AIC2024-TRACK4-TEAM15.","sentences":["This study addresses the evolving challenges in urban traffic monitoring detection systems based on fisheye lens cameras by proposing a framework that improves the efficacy and accuracy of these systems.","In the context of urban infrastructure and transportation management, advanced traffic monitoring systems have become critical for managing the complexities of urbanization and increasing vehicle density.","Traditional monitoring methods, which rely on static cameras with narrow fields of view, are ineffective in dynamic urban environments, necessitating the installation of multiple cameras, which raises costs.","Fisheye lenses, which were recently introduced, provide wide and omnidirectional coverage in a single frame, making them a transformative solution.","However, issues such as distorted views and blurriness arise, preventing accurate object detection on these images.","Motivated by these challenges, this study proposes a novel approach that combines a ransformer-based image enhancement framework and ensemble learning technique to address these challenges and improve traffic monitoring accuracy, making significant contributions to the future of intelligent traffic management systems.","Our proposed methodological framework won 5th place in the 2024 AI City Challenge, Track 4, with an F1 score of 0.5965 on experimental validation data.","The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system.","Our code is publicly available at https://github.com/daitranskku/AIC2024-TRACK4-TEAM15."],"url":"http://arxiv.org/abs/2404.10078v1","category":"cs.CV"}
{"created":"2024-04-15 18:21:20","title":"Dynamic Complex-Frequency Control of Grid-Forming Converters","abstract":"Complex droop control, alternatively known as dispatchable virtual oscillator control (dVOC), stands out for its unique capabilities in synchronization and voltage stabilization among existing control strategies for grid-forming converters. Complex droop control leverages the novel concept of ``complex frequency'', thereby establishing a coupled connection between active and reactive power inputs and frequency and rate-of-change-of voltage outputs. However, its reliance on static droop gains limits its ability to exhibit crucial dynamic response behaviors required in future power systems. To address this limitation, this paper introduces \\textit{dynamic complex-frequency control}, upgrading static droop gains with dynamic transfer functions to enhance the richness and flexibility in dynamic responses for frequency and voltage control. Unlike existing approaches, the complex-frequency control framework treats frequency and voltage dynamics collectively, ensuring small-signal stability for frequency synchronization and voltage stabilization simultaneously. The control framework is validated through detailed numerical case studies on the IEEE nine-bus system, also showcasing its applicability in multi-converter setups.","sentences":["Complex droop control, alternatively known as dispatchable virtual oscillator control (dVOC), stands out for its unique capabilities in synchronization and voltage stabilization among existing control strategies for grid-forming converters.","Complex droop control leverages the novel concept of ``complex frequency'', thereby establishing a coupled connection between active and reactive power inputs and frequency and rate-of-change-of voltage outputs.","However, its reliance on static droop gains limits its ability to exhibit crucial dynamic response behaviors required in future power systems.","To address this limitation, this paper introduces \\textit{dynamic complex-frequency control}, upgrading static droop gains with dynamic transfer functions to enhance the richness and flexibility in dynamic responses for frequency and voltage control.","Unlike existing approaches, the complex-frequency control framework treats frequency and voltage dynamics collectively, ensuring small-signal stability for frequency synchronization and voltage stabilization simultaneously.","The control framework is validated through detailed numerical case studies on the IEEE nine-bus system, also showcasing its applicability in multi-converter setups."],"url":"http://arxiv.org/abs/2404.10071v1","category":"eess.SY"}
{"created":"2024-04-15 18:00:30","title":"AIGeN: An Adversarial Approach for Instruction Generation in VLN","abstract":"In the last few years, the research interest in Vision-and-Language Navigation (VLN) has grown significantly. VLN is a challenging task that involves an agent following human instructions and navigating in a previously unknown environment to reach a specified goal. Recent work in literature focuses on different ways to augment the available datasets of instructions for improving navigation performance by exploiting synthetic training data. In this work, we propose AIGeN, a novel architecture inspired by Generative Adversarial Networks (GANs) that produces meaningful and well-formed synthetic instructions to improve navigation agents' performance. The model is composed of a Transformer decoder (GPT-2) and a Transformer encoder (BERT). During the training phase, the decoder generates sentences for a sequence of images describing the agent's path to a particular point while the encoder discriminates between real and fake instructions. Experimentally, we evaluate the quality of the generated instructions and perform extensive ablation studies. Additionally, we generate synthetic instructions for 217K trajectories using AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in the performance of an off-the-shelf VLN method. The validation analysis of our proposal is conducted on REVERIE and R2R and highlights the promising aspects of our proposal, achieving state-of-the-art performance.","sentences":["In the last few years, the research interest in Vision-and-Language Navigation (VLN) has grown significantly.","VLN is a challenging task that involves an agent following human instructions and navigating in a previously unknown environment to reach a specified goal.","Recent work in literature focuses on different ways to augment the available datasets of instructions for improving navigation performance by exploiting synthetic training data.","In this work, we propose AIGeN, a novel architecture inspired by Generative Adversarial Networks (GANs) that produces meaningful and well-formed synthetic instructions to improve navigation agents' performance.","The model is composed of a Transformer decoder (GPT-2) and a Transformer encoder (BERT).","During the training phase, the decoder generates sentences for a sequence of images describing the agent's path to a particular point while the encoder discriminates between real and fake instructions.","Experimentally, we evaluate the quality of the generated instructions and perform extensive ablation studies.","Additionally, we generate synthetic instructions for 217K trajectories using AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in the performance of an off-the-shelf VLN method.","The validation analysis of our proposal is conducted on REVERIE and R2R and highlights the promising aspects of our proposal, achieving state-of-the-art performance."],"url":"http://arxiv.org/abs/2404.10054v1","category":"cs.CV"}
{"created":"2024-04-15 17:59:57","title":"Taming Latent Diffusion Model for Neural Radiance Field Inpainting","abstract":"Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: https://hubert0527.github.io/MALD-NeRF","sentences":["Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images.","Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions.","One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry.","Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors.","These two problems are further reinforced with the use of pixel-distance losses.","To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training.","During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task.","Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes.","Project page: https://hubert0527.github.io/MALD-NeRF"],"url":"http://arxiv.org/abs/2404.09995v1","category":"cs.CV"}
{"created":"2024-04-15 17:59:50","title":"MMInA: Benchmarking Multihop Multimodal Internet Agents","abstract":"Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites. Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to autonomously extract multimodal information from web pages as observations; 2) Multihop web browsing. Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation. We propose a novel protocol for evaluating an agent's progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks of more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach replaying past action trajectories to reflect. Our method significantly improved both the single-hop and multihop web browsing abilities of agents. See our code and data at https://mmina.cliangyu.com","sentences":["Autonomous embodied agents live on an Internet of multimedia websites.","Can they hop around multimodal websites to complete complex user tasks?","Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites.","To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites.","Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks.","Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to autonomously extract multimodal information from web pages as observations; 2) Multihop web browsing.","Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.","We propose a novel protocol for evaluating an agent's progress in completing multihop tasks.","We experiment with both standalone (multimodal) language models and heuristic-based web agents.","Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents.","We identify that agents are more likely to fail on the early hops when solving tasks of more hops, which results in lower task success rates.","To address this issue, we propose a simple memory augmentation approach replaying past action trajectories to reflect.","Our method significantly improved both the single-hop and multihop web browsing abilities of agents.","See our code and data at https://mmina.cliangyu.com"],"url":"http://arxiv.org/abs/2404.09992v1","category":"cs.CV"}
{"created":"2024-04-15 17:59:31","title":"HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing","abstract":"This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing. In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. The project page is https://thefllood.github.io/HQEdit_web.","sentences":["This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits.","Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3.","To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing.","In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models.","For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data.","The project page is https://thefllood.github.io/HQEdit_web."],"url":"http://arxiv.org/abs/2404.09990v1","category":"cs.CV"}
{"created":"2024-04-15 17:57:34","title":"Hybrid Work meets Agile Software Development: A Systematic Mapping Study","abstract":"Hybrid work, a fusion of different work environments that allow employees to work in and outside their offices, represents a new frontier for agile researchers to explore. However, due to the nascent nature of the research phenomena, we are yet to achieve a good understanding of the research terrain formulated when hybrid work meets agile software development. This systematic mapping study, we aimed to provide a good understanding of this emerging research area. The systematic process we followed led to a collection of 12 primary studies, which is less than what we expected. All the papers are empirical studies, with most of them employing case studies as the research methodology. The people-centric nature of agile methods is yet to be adequately reflected in the studies in this area. Similarly, there is a lack of a richer understanding of hybrid work in terms of flexible work arrangements. Our mapping study identified various research opportunities that can be explored in future research.","sentences":["Hybrid work, a fusion of different work environments that allow employees to work in and outside their offices, represents a new frontier for agile researchers to explore.","However, due to the nascent nature of the research phenomena, we are yet to achieve a good understanding of the research terrain formulated when hybrid work meets agile software development.","This systematic mapping study, we aimed to provide a good understanding of this emerging research area.","The systematic process we followed led to a collection of 12 primary studies, which is less than what we expected.","All the papers are empirical studies, with most of them employing case studies as the research methodology.","The people-centric nature of agile methods is yet to be adequately reflected in the studies in this area.","Similarly, there is a lack of a richer understanding of hybrid work in terms of flexible work arrangements.","Our mapping study identified various research opportunities that can be explored in future research."],"url":"http://arxiv.org/abs/2404.09983v1","category":"cs.SE"}
{"created":"2024-04-15 17:57:30","title":"Memory Sharing for Large Language Model based Agents","abstract":"In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each \"memory\" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM","sentences":["In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries.","However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results.","Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process.","Each \"memory\" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents.","This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents.","Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions.","Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS.","The code and data are available at: https://github.com/GHupppp/MemorySharingLLM"],"url":"http://arxiv.org/abs/2404.09982v1","category":"cs.CL"}
{"created":"2024-04-15 17:45:36","title":"Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model","abstract":"ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours).","sentences":["ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses.","However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation.","First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden.","Second, ControlNet features for different frames might not effectively handle the temporal consistency.","To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos).","Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing.","In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen.","Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos.","We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control.","Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs.","With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours)."],"url":"http://arxiv.org/abs/2404.09967v1","category":"cs.CV"}
{"created":"2024-04-15 17:31:22","title":"Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization","abstract":"Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.","sentences":["Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life.","The generation of audio from text prompts is an important aspect of such processes in the music and film industry.","Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs.","These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt.","Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data.","As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from.","The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order.","We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics."],"url":"http://arxiv.org/abs/2404.09956v2","category":"cs.SD"}
{"created":"2024-04-15 17:15:18","title":"A Note on Loss Functions and Error Compounding in Model-based Reinforcement Learning","abstract":"This note clarifies some confusions (and perhaps throws out more) around model-based reinforcement learning and their theoretical understanding in the context of deep RL. Main topics of discussion are (1) how to reconcile model-based RL's bad empirical reputation on error compounding with its superior theoretical properties, and (2) the limitations of empirically popular losses. For the latter, concrete counterexamples for the \"MuZero loss\" are constructed to show that it not only fails in stochastic environments, but also suffers exponential sample complexity in deterministic environments when data provides sufficient coverage.","sentences":["This note clarifies some confusions (and perhaps throws out more) around model-based reinforcement learning and their theoretical understanding in the context of deep RL.","Main topics of discussion are (1) how to reconcile model-based RL's bad empirical reputation on error compounding with its superior theoretical properties, and (2) the limitations of empirically popular losses.","For the latter, concrete counterexamples for the \"MuZero loss\" are constructed to show that it not only fails in stochastic environments, but also suffers exponential sample complexity in deterministic environments when data provides sufficient coverage."],"url":"http://arxiv.org/abs/2404.09946v1","category":"cs.LG"}
{"created":"2024-04-15 17:09:53","title":"Evolving Interpretable Visual Classifiers with Large Language Models","abstract":"Multimodal pre-trained models, such as CLIP, are popular for zero-shot classification due to their open-vocabulary flexibility and high performance. However, vision-language models, which compute similarity scores between images and class labels, are largely black-box, with limited interpretability, risk for bias, and inability to discover new visual concepts not written down. Moreover, in practical settings, the vocabulary for class names and attributes of specialized concepts will not be known, preventing these methods from performing well on images uncommon in large-scale vision-language datasets. To address these limitations, we present a novel method that discovers interpretable yet discriminative sets of attributes for visual recognition. We introduce an evolutionary search algorithm that uses a large language model and its in-context learning abilities to iteratively mutate a concept bottleneck of attributes for classification. Our method produces state-of-the-art, interpretable fine-grained classifiers. We outperform the latest baselines by 18.4% on five fine-grained iNaturalist datasets and by 22.2% on two KikiBouba datasets, despite the baselines having access to privileged information about class names.","sentences":["Multimodal pre-trained models, such as CLIP, are popular for zero-shot classification due to their open-vocabulary flexibility and high performance.","However, vision-language models, which compute similarity scores between images and class labels, are largely black-box, with limited interpretability, risk for bias, and inability to discover new visual concepts not written down.","Moreover, in practical settings, the vocabulary for class names and attributes of specialized concepts will not be known, preventing these methods from performing well on images uncommon in large-scale vision-language datasets.","To address these limitations, we present a novel method that discovers interpretable yet discriminative sets of attributes for visual recognition.","We introduce an evolutionary search algorithm that uses a large language model and its in-context learning abilities to iteratively mutate a concept bottleneck of attributes for classification.","Our method produces state-of-the-art, interpretable fine-grained classifiers.","We outperform the latest baselines by 18.4% on five fine-grained iNaturalist datasets and by 22.2% on two KikiBouba datasets, despite the baselines having access to privileged information about class names."],"url":"http://arxiv.org/abs/2404.09941v1","category":"cs.CV"}
{"created":"2024-04-15 17:07:55","title":"A Survey on Deep Learning for Theorem Proving","abstract":"Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in mathematical language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; ii) a meticulous summary of available datasets and strategies for data generation; iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art; and iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, seeking to catalyze further research endeavors in this rapidly growing field.","sentences":["Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in mathematical language to rigorous derivations in formal systems.","In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving.","This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; ii) a meticulous summary of available datasets and strategies for data generation; iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art; and iv) a critical discussion on the persistent challenges and the promising avenues for future exploration.","Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, seeking to catalyze further research endeavors in this rapidly growing field."],"url":"http://arxiv.org/abs/2404.09939v1","category":"cs.AI"}
{"created":"2024-04-15 17:03:41","title":"Compression Represents Intelligence Linearly","abstract":"There is a belief that learning to compress well will lead to intelligence. Recently, language modeling has been shown to be equivalent to compression, which offers a compelling rationale for the success of large language models (LLMs): the development of more advanced language models is essentially enhancing compression which facilitates intelligence. Despite such appealing discussions, little empirical evidence is present for the interplay between compression and intelligence. In this work, we examine their relationship in the context of LLMs, treating LLMs as data compressors. Given the abstract concept of \"intelligence\", we adopt the average downstream benchmark scores as a surrogate, specifically targeting intelligence related to knowledge and commonsense, coding, and mathematical reasoning. Across 12 benchmarks, our study brings together 30 public LLMs that originate from diverse organizations. Remarkably, we find that LLMs' intelligence -- reflected by average benchmark scores -- almost linearly correlates with their ability to compress external text corpora. These results provide concrete evidence supporting the belief that superior compression indicates greater intelligence. Furthermore, our findings suggest that compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure that is linearly associated with the model capabilities. We open-source our compression datasets as well as our data collection pipelines to facilitate future researchers to assess compression properly.","sentences":["There is a belief that learning to compress well will lead to intelligence.","Recently, language modeling has been shown to be equivalent to compression, which offers a compelling rationale for the success of large language models (LLMs): the development of more advanced language models is essentially enhancing compression which facilitates intelligence.","Despite such appealing discussions, little empirical evidence is present for the interplay between compression and intelligence.","In this work, we examine their relationship in the context of LLMs, treating LLMs as data compressors.","Given the abstract concept of \"intelligence\", we adopt the average downstream benchmark scores as a surrogate, specifically targeting intelligence related to knowledge and commonsense, coding, and mathematical reasoning.","Across 12 benchmarks, our study brings together 30 public LLMs that originate from diverse organizations.","Remarkably, we find that LLMs' intelligence -- reflected by average benchmark scores -- almost linearly correlates with their ability to compress external text corpora.","These results provide concrete evidence supporting the belief that superior compression indicates greater intelligence.","Furthermore, our findings suggest that compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure that is linearly associated with the model capabilities.","We open-source our compression datasets as well as our data collection pipelines to facilitate future researchers to assess compression properly."],"url":"http://arxiv.org/abs/2404.09937v1","category":"cs.CL"}
{"created":"2024-04-15 16:58:28","title":"Foundational Challenges in Assuring Alignment and Safety of Large Language Models","abstract":"This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.","sentences":["This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs).","These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges.","Based on the identified challenges, we pose $200+$ concrete research questions."],"url":"http://arxiv.org/abs/2404.09932v1","category":"cs.LG"}
{"created":"2024-04-15 16:56:58","title":"Zero-shot detection of buildings in mobile LiDAR using Language Vision Model","abstract":"Recent advances have demonstrated that Language Vision Models (LVMs) surpass the existing State-of-the-Art (SOTA) in two-dimensional (2D) computer vision tasks, motivating attempts to apply LVMs to three-dimensional (3D) data. While LVMs are efficient and effective in addressing various downstream 2D vision tasks without training, they face significant challenges when it comes to point clouds, a representative format for representing 3D data. It is more difficult to extract features from 3D data and there are challenges due to large data sizes and the cost of the collection and labelling, resulting in a notably limited availability of datasets. Moreover, constructing LVMs for point clouds is even more challenging due to the requirements for large amounts of data and training time. To address these issues, our research aims to 1) apply the Grounded SAM through Spherical Projection to transfer 3D to 2D, and 2) experiment with synthetic data to evaluate its effectiveness in bridging the gap between synthetic and real-world data domains. Our approach exhibited high performance with an accuracy of 0.96, an IoU of 0.85, precision of 0.92, recall of 0.91, and an F1 score of 0.92, confirming its potential. However, challenges such as occlusion problems and pixel-level overlaps of multi-label points during spherical image generation remain to be addressed in future studies.","sentences":["Recent advances have demonstrated that Language Vision Models (LVMs) surpass the existing State-of-the-Art (SOTA) in two-dimensional (2D) computer vision tasks, motivating attempts to apply LVMs to three-dimensional (3D) data.","While LVMs are efficient and effective in addressing various downstream 2D vision tasks without training, they face significant challenges when it comes to point clouds, a representative format for representing 3D data.","It is more difficult to extract features from 3D data and there are challenges due to large data sizes and the cost of the collection and labelling, resulting in a notably limited availability of datasets.","Moreover, constructing LVMs for point clouds is even more challenging due to the requirements for large amounts of data and training time.","To address these issues, our research aims to 1) apply the Grounded SAM through Spherical Projection to transfer 3D to 2D, and 2) experiment with synthetic data to evaluate its effectiveness in bridging the gap between synthetic and real-world data domains.","Our approach exhibited high performance with an accuracy of 0.96, an IoU of 0.85, precision of 0.92, recall of 0.91, and an F1 score of 0.92, confirming its potential.","However, challenges such as occlusion problems and pixel-level overlaps of multi-label points during spherical image generation remain to be addressed in future studies."],"url":"http://arxiv.org/abs/2404.09931v1","category":"cs.CV"}
{"created":"2024-04-15 16:47:22","title":"Zero-shot Building Age Classification from Facade Image Using GPT-4","abstract":"A building's age of construction is crucial for supporting many geospatial applications. Much current research focuses on estimating building age from facade images using deep learning. However, building an accurate deep learning model requires a considerable amount of labelled training data, and the trained models often have geographical constraints. Recently, large pre-trained vision language models (VLMs) such as GPT-4 Vision, which demonstrate significant generalisation capabilities, have emerged as potential training-free tools for dealing with specific vision tasks, but their applicability and reliability for building information remain unexplored. In this study, a zero-shot building age classifier for facade images is developed using prompts that include logical instructions. Taking London as a test case, we introduce a new dataset, FI-London, comprising facade images and building age epochs. Although the training-free classifier achieved a modest accuracy of 39.69%, the mean absolute error of 0.85 decades indicates that the model can predict building age epochs successfully albeit with a small bias. The ensuing discussion reveals that the classifier struggles to predict the age of very old buildings and is challenged by fine-grained predictions within 2 decades. Overall, the classifier utilising GPT-4 Vision is capable of predicting the rough age epoch of a building from a single facade image without any training.","sentences":["A building's age of construction is crucial for supporting many geospatial applications.","Much current research focuses on estimating building age from facade images using deep learning.","However, building an accurate deep learning model requires a considerable amount of labelled training data, and the trained models often have geographical constraints.","Recently, large pre-trained vision language models (VLMs) such as GPT-4 Vision, which demonstrate significant generalisation capabilities, have emerged as potential training-free tools for dealing with specific vision tasks, but their applicability and reliability for building information remain unexplored.","In this study, a zero-shot building age classifier for facade images is developed using prompts that include logical instructions.","Taking London as a test case, we introduce a new dataset, FI-London, comprising facade images and building age epochs.","Although the training-free classifier achieved a modest accuracy of 39.69%, the mean absolute error of 0.85 decades indicates that the model can predict building age epochs successfully albeit with a small bias.","The ensuing discussion reveals that the classifier struggles to predict the age of very old buildings and is challenged by fine-grained predictions within 2 decades.","Overall, the classifier utilising GPT-4 Vision is capable of predicting the rough age epoch of a building from a single facade image without any training."],"url":"http://arxiv.org/abs/2404.09921v1","category":"cs.CV"}
{"created":"2024-04-15 16:46:17","title":"How fair are we? From conceptualization to automated assessment of fairness definitions","abstract":"Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems. With the increasing use of machine learning in software systems, researchers have been developing techniques to automatically assess the fairness of software systems. Nonetheless, a significant proportion of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders. To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment. Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation. Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions. In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE). We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study. Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover two additional application domains not addressed by currently available tools, i.e., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other Model-Driven Engineering-based approaches for fairness assessment.","sentences":["Fairness is a critical concept in ethics and social domains, but it is also a challenging property to engineer in software systems.","With the increasing use of machine learning in software systems, researchers have been developing techniques to automatically assess the fairness of software systems.","Nonetheless, a significant proportion of these techniques rely upon pre-established fairness definitions, metrics, and criteria, which may fail to encompass the wide-ranging needs and preferences of users and stakeholders.","To overcome this limitation, we propose a novel approach, called MODNESS, that enables users to customize and define their fairness concepts using a dedicated modeling environment.","Our approach guides the user through the definition of new fairness concepts also in emerging domains, and the specification and composition of metrics for its evaluation.","Ultimately, MODNESS generates the source code to implement fair assessment based on these custom definitions.","In addition, we elucidate the process we followed to collect and analyze relevant literature on fairness assessment in software engineering (SE).","We compare MODNESS with the selected approaches and evaluate how they support the distinguishing features identified by our study.","Our findings reveal that i) most of the current approaches do not support user-defined fairness concepts; ii) our approach can cover two additional application domains not addressed by currently available tools, i.e., mitigating bias in recommender systems for software engineering and Arduino software component recommendations; iii) MODNESS demonstrates the capability to overcome the limitations of the only two other Model-Driven Engineering-based approaches for fairness assessment."],"url":"http://arxiv.org/abs/2404.09919v1","category":"cs.SE"}
{"created":"2024-04-15 16:43:24","title":"Evaluating the Explainability of Attributes and Prototypes for a Medical Classification Model","abstract":"Due to the sensitive nature of medicine, it is particularly important and highly demanded that AI methods are explainable. This need has been recognised and there is great research interest in xAI solutions with medical applications. However, there is a lack of user-centred evaluation regarding the actual impact of the explanations. We evaluate attribute- and prototype-based explanations with the Proto-Caps model. This xAI model reasons the target classification with human-defined visual features of the target object in the form of scores and attribute-specific prototypes. The model thus provides a multimodal explanation that is intuitively understandable to humans thanks to predefined attributes. A user study involving six radiologists shows that the explanations are subjectivly perceived as helpful, as they reflect their decision-making process. The results of the model are considered a second opinion that radiologists can discuss using the model's explanations. However, it was shown that the inclusion and increased magnitude of model explanations objectively can increase confidence in the model's predictions when the model is incorrect. We can conclude that attribute scores and visual prototypes enhance confidence in the model. However, additional development and repeated user studies are needed to tailor the explanation to the respective use case.","sentences":["Due to the sensitive nature of medicine, it is particularly important and highly demanded that AI methods are explainable.","This need has been recognised and there is great research interest in xAI solutions with medical applications.","However, there is a lack of user-centred evaluation regarding the actual impact of the explanations.","We evaluate attribute- and prototype-based explanations with the Proto-Caps model.","This xAI model reasons the target classification with human-defined visual features of the target object in the form of scores and attribute-specific prototypes.","The model thus provides a multimodal explanation that is intuitively understandable to humans thanks to predefined attributes.","A user study involving six radiologists shows that the explanations are subjectivly perceived as helpful, as they reflect their decision-making process.","The results of the model are considered a second opinion that radiologists can discuss using the model's explanations.","However, it was shown that the inclusion and increased magnitude of model explanations objectively can increase confidence in the model's predictions when the model is incorrect.","We can conclude that attribute scores and visual prototypes enhance confidence in the model.","However, additional development and repeated user studies are needed to tailor the explanation to the respective use case."],"url":"http://arxiv.org/abs/2404.09917v1","category":"cs.CV"}
{"created":"2024-04-15 16:37:44","title":"Detecting AI Generated Text Based on NLP and Machine Learning Approaches","abstract":"Recent advances in natural language processing (NLP) may enable artificial intelligence (AI) models to generate writing that is identical to human written form in the future. This might have profound ethical, legal, and social repercussions. This study aims to address this problem by offering an accurate AI detector model that can differentiate between electronically produced text and human-written text. Our approach includes machine learning methods such as XGB Classifier, SVM, BERT architecture deep learning models. Furthermore, our results show that the BERT performs better than previous models in identifying information generated by AI from information provided by humans. Provide a comprehensive analysis of the current state of AI-generated text identification in our assessment of pertinent studies. Our testing yielded positive findings, showing that our strategy is successful, with the BERT emerging as the most probable answer. We analyze the research's societal implications, highlighting the possible advantages for various industries while addressing sustainability issues pertaining to morality and the environment. The XGB classifier and SVM give 0.84 and 0.81 accuracy in this article, respectively. The greatest accuracy in this research is provided by the BERT model, which provides 0.93% accuracy.","sentences":["Recent advances in natural language processing (NLP) may enable artificial intelligence (AI) models to generate writing that is identical to human written form in the future.","This might have profound ethical, legal, and social repercussions.","This study aims to address this problem by offering an accurate AI detector model that can differentiate between electronically produced text and human-written text.","Our approach includes machine learning methods such as XGB Classifier, SVM, BERT architecture deep learning models.","Furthermore, our results show that the BERT performs better than previous models in identifying information generated by AI from information provided by humans.","Provide a comprehensive analysis of the current state of AI-generated text identification in our assessment of pertinent studies.","Our testing yielded positive findings, showing that our strategy is successful, with the BERT emerging as the most probable answer.","We analyze the research's societal implications, highlighting the possible advantages for various industries while addressing sustainability issues pertaining to morality and the environment.","The XGB classifier and SVM give 0.84 and 0.81 accuracy in this article, respectively.","The greatest accuracy in this research is provided by the BERT model, which provides 0.93% accuracy."],"url":"http://arxiv.org/abs/2404.10032v1","category":"cs.LG"}
{"created":"2024-04-15 16:30:48","title":"Double-dome Unconventional Superconductivity in Twisted Trilayer Graphene","abstract":"Graphene moir\\'e systems are ideal environments for investigating complex phase diagrams and gaining fundamental insights into the mechanisms underlying exotic states of matter, as they permit controlled manipulation of electronic properties. Magic-angle twisted trilayer graphene (MATTG) has emerged as a key platform to explore moir\\'e superconductivity, owing to the robustness of its superconducting order and the displacement-field tunability of its energy bands. Recent measurements strongly suggest that superconductivity in MATTG is unconventional. Here, we report the first direct observation of double-dome superconductivity in MATTG. The temperature, magnetic field, and bias current dependence of the superconductivity of doped holes collectively show that it is significantly suppressed near moir\\'e filling $\\nu^* = -2.6$, leading to a double dome in the phase diagram within a finite window of the displacement field. The temperature dependence of the normal-state resistance and the $I-V$ curves straddling $\\nu^*$ are suggestive of a phase transition and the potentially distinct nature of superconductivity in the two domes. Hartree-Fock calculations incorporating mild strain yield an incommensurate Kekul\\'e spiral state whose effective spin polarization peaks in the regime where superconductivity is suppressed in experiments. This allows us to draw conclusions about the normal state as well as the unconventional nature of the superconducting order parameter.","sentences":["Graphene moir\\'e systems are ideal environments for investigating complex phase diagrams and gaining fundamental insights into the mechanisms underlying exotic states of matter, as they permit controlled manipulation of electronic properties.","Magic-angle twisted trilayer graphene (MATTG) has emerged as a key platform to explore moir\\'e superconductivity, owing to the robustness of its superconducting order and the displacement-field tunability of its energy bands.","Recent measurements strongly suggest that superconductivity in MATTG is unconventional.","Here, we report the first direct observation of double-dome superconductivity in MATTG.","The temperature, magnetic field, and bias current dependence of the superconductivity of doped holes collectively show that it is significantly suppressed near moir\\'e filling $\\nu^* = -2.6$, leading to a double dome in the phase diagram within a finite window of the displacement field.","The temperature dependence of the normal-state resistance and the $I-V$ curves straddling $\\nu^*$ are suggestive of a phase transition and the potentially distinct nature of superconductivity in the two domes.","Hartree-Fock calculations incorporating mild strain yield an incommensurate Kekul\\'e spiral state whose effective spin polarization peaks in the regime where superconductivity is suppressed in experiments.","This allows us to draw conclusions about the normal state as well as the unconventional nature of the superconducting order parameter."],"url":"http://arxiv.org/abs/2404.09909v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-15 16:17:22","title":"Priority aware grouping-based multihop routing scheme for RIS-assisted wireless networks","abstract":"Reconfigurable intelligent surfaces (RISs) is a novel communication technology that has been recognized and recently presented as a candidate for beyond fifth generation wireless communication technology. In this paper, we propose a priority aware user traffic dependent grouping based multihop routing scheme for a RIS-assisted millimeter wave (mmWave) device-to-device (D2D) communication network with spatially correlated channels. Specifically, the proposed scheme exploits the priority of the users (based on their respective delay constrained applications) and the aspect of spatial correlation in the narrowly spaced reflecting elements of the RISs. In this context, we establish a multihop connection for information transfer from one of the users to its desired receiver based on the other users in the neighbourhood, their respective traffic characteristics, and the already deployed RISs in the surroundings. Moreover, we also take into account the impact of considering practical discrete phase shifts at the RIS patches instead of its ideal continuous counterpart. Furthermore, we claim as well as demonstrate that the existing classic least remaining distance (LRD)-based approach is not always the optimal solution. Finally, numerical results demonstrate the advantages of the proposed strategy and that it significantly outperforms the existing benchmark schemes in terms of system performance metrics such as data throughput, energy consumption, as well as energy efficiency.","sentences":["Reconfigurable intelligent surfaces (RISs) is a novel communication technology that has been recognized and recently presented as a candidate for beyond fifth generation wireless communication technology.","In this paper, we propose a priority aware user traffic dependent grouping based multihop routing scheme for a RIS-assisted millimeter wave (mmWave) device-to-device (D2D) communication network with spatially correlated channels.","Specifically, the proposed scheme exploits the priority of the users (based on their respective delay constrained applications) and the aspect of spatial correlation in the narrowly spaced reflecting elements of the RISs.","In this context, we establish a multihop connection for information transfer from one of the users to its desired receiver based on the other users in the neighbourhood, their respective traffic characteristics, and the already deployed RISs in the surroundings.","Moreover, we also take into account the impact of considering practical discrete phase shifts at the RIS patches instead of its ideal continuous counterpart.","Furthermore, we claim as well as demonstrate that the existing classic least remaining distance (LRD)-based approach is not always the optimal solution.","Finally, numerical results demonstrate the advantages of the proposed strategy and that it significantly outperforms the existing benchmark schemes in terms of system performance metrics such as data throughput, energy consumption, as well as energy efficiency."],"url":"http://arxiv.org/abs/2404.09898v1","category":"eess.SY"}
{"created":"2024-04-15 16:16:59","title":"Progressive Knowledge Graph Completion","abstract":"Knowledge Graph Completion (KGC) has emerged as a promising solution to address the issue of incompleteness within Knowledge Graphs (KGs). Traditional KGC research primarily centers on triple classification and link prediction. Nevertheless, we contend that these tasks do not align well with real-world scenarios and merely serve as surrogate benchmarks. In this paper, we investigate three crucial processes relevant to real-world construction scenarios: (a) the verification process, which arises from the necessity and limitations of human verifiers; (b) the mining process, which identifies the most promising candidates for verification; and (c) the training process, which harnesses verified data for subsequent utilization; in order to achieve a transition toward more realistic challenges. By integrating these three processes, we introduce the Progressive Knowledge Graph Completion (PKGC) task, which simulates the gradual completion of KGs in real-world scenarios. Furthermore, to expedite PKGC processing, we propose two acceleration modules: Optimized Top-$k$ algorithm and Semantic Validity Filter. These modules significantly enhance the efficiency of the mining procedure. Our experiments demonstrate that performance in link prediction does not accurately reflect performance in PKGC. A more in-depth analysis reveals the key factors influencing the results and provides potential directions for future research.","sentences":["Knowledge Graph Completion (KGC) has emerged as a promising solution to address the issue of incompleteness within Knowledge Graphs (KGs).","Traditional KGC research primarily centers on triple classification and link prediction.","Nevertheless, we contend that these tasks do not align well with real-world scenarios and merely serve as surrogate benchmarks.","In this paper, we investigate three crucial processes relevant to real-world construction scenarios: (a) the verification process, which arises from the necessity and limitations of human verifiers; (b) the mining process, which identifies the most promising candidates for verification; and (c) the training process, which harnesses verified data for subsequent utilization; in order to achieve a transition toward more realistic challenges.","By integrating these three processes, we introduce the Progressive Knowledge Graph Completion (PKGC) task, which simulates the gradual completion of KGs in real-world scenarios.","Furthermore, to expedite PKGC processing, we propose two acceleration modules: Optimized Top-$k$ algorithm and Semantic Validity Filter.","These modules significantly enhance the efficiency of the mining procedure.","Our experiments demonstrate that performance in link prediction does not accurately reflect performance in PKGC.","A more in-depth analysis reveals the key factors influencing the results and provides potential directions for future research."],"url":"http://arxiv.org/abs/2404.09897v1","category":"cs.AI"}
{"created":"2024-04-15 15:55:01","title":"Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval","abstract":"Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found either in a single table or multiple tables identified through question decomposition or rewriting. However, neither of these approaches is sufficient, as many questions require retrieving multiple tables and joining them through a join plan that cannot be discerned from the user query itself. If the join plan is not considered in the retrieval stage, the subsequent steps of reasoning and answering based on those retrieved tables are likely to be incorrect. To address this problem, we introduce a method that uncovers useful join relations for any query and database during table retrieval. We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships. Our method outperforms the state-of-the-art approaches for table retrieval by up to 9.3% in F1 score and for end-to-end QA by up to 5.4% in accuracy.","sentences":["Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems.","Previous methods assume the answer to such a question can be found either in a single table or multiple tables identified through question decomposition or rewriting.","However, neither of these approaches is sufficient, as many questions require retrieving multiple tables and joining them through a join plan that cannot be discerned from the user query itself.","If the join plan is not considered in the retrieval stage, the subsequent steps of reasoning and answering based on those retrieved tables are likely to be incorrect.","To address this problem, we introduce a method that uncovers useful join relations for any query and database during table retrieval.","We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships.","Our method outperforms the state-of-the-art approaches for table retrieval by up to 9.3% in F1 score and for end-to-end QA by up to 5.4% in accuracy."],"url":"http://arxiv.org/abs/2404.09889v1","category":"cs.IR"}
{"created":"2024-04-15 15:47:08","title":"Synergising Human-like Responses and Machine Intelligence for Planning in Disaster Response","abstract":"In the rapidly changing environments of disaster response, planning and decision-making for autonomous agents involve complex and interdependent choices. Although recent advancements have improved traditional artificial intelligence (AI) approaches, they often struggle in such settings, particularly when applied to agents operating outside their well-defined training parameters. To address these challenges, we propose an attention-based cognitive architecture inspired by Dual Process Theory (DPT). This framework integrates, in an online fashion, rapid yet heuristic (human-like) responses (System 1) with the slow but optimized planning capabilities of machine intelligence (System 2). We illustrate how a supervisory controller can dynamically determine in real-time the engagement of either system to optimize mission objectives by assessing their performance across a number of distinct attributes. Evaluated for trajectory planning in dynamic environments, our framework demonstrates that this synergistic integration effectively manages complex tasks by optimizing multiple mission objectives.","sentences":["In the rapidly changing environments of disaster response, planning and decision-making for autonomous agents involve complex and interdependent choices.","Although recent advancements have improved traditional artificial intelligence (AI) approaches, they often struggle in such settings, particularly when applied to agents operating outside their well-defined training parameters.","To address these challenges, we propose an attention-based cognitive architecture inspired by Dual Process Theory (DPT).","This framework integrates, in an online fashion, rapid yet heuristic (human-like) responses (System 1) with the slow but optimized planning capabilities of machine intelligence (System 2).","We illustrate how a supervisory controller can dynamically determine in real-time the engagement of either system to optimize mission objectives by assessing their performance across a number of distinct attributes.","Evaluated for trajectory planning in dynamic environments, our framework demonstrates that this synergistic integration effectively manages complex tasks by optimizing multiple mission objectives."],"url":"http://arxiv.org/abs/2404.09877v1","category":"cs.AI"}
{"created":"2024-04-15 15:47:01","title":"The NICER data and a $\u03c3$-field dependent stiffness of the hadronic equation of state","abstract":"Analyses for the NICER data indicate that there is no significant variation of the compact star radii within the mass range of 1.4 to 2.0 solar masses. Yamamoto et al. [Phys. Rev. C 108, 035811 (2023)] concluded recently that ``this feature cannot be reproduced by the hadronic matter due to the softening of the equation of state (EoS) by hyperon mixing, suggesting the possible existence of quark phases in neutron-star interiors.'' Using a collection of 162 purely nucleonic, hyperonic, and quarkish EoSs from CompOSE database and some other works, we verify that hyperons indeed lead to a significant difference in radii of stars of 1.4 and 2.0 solar masses, which diminishes in the presence of quarks. We compare the shapes of the mass-radius curves and show that hyperons and quarks in the neutron star cores prefer a particular curve shape with backbending. It is argued that the shape {is controlled by the density dependence} of the nuclear symmetry energy. We draw attention to the existence of a class of purely hadronic relativistic mean-field EoSs with scalar-field dependent hadron masses and coupling constants that satisfy the known constraints on the EoSs including the analyses of the new NICER data and the above requirement of no significant variation of the neutron star radii.","sentences":["Analyses for the NICER data indicate that there is no significant variation of the compact star radii within the mass range of 1.4 to 2.0 solar masses.","Yamamoto et al.","[Phys. Rev. C 108, 035811 (2023)] concluded recently that ``this feature cannot be reproduced by the hadronic matter due to the softening of the equation of state (EoS) by hyperon mixing, suggesting the possible existence of quark phases in neutron-star interiors.''","Using a collection of 162 purely nucleonic, hyperonic, and quarkish EoSs from CompOSE database and some other works, we verify that hyperons indeed lead to a significant difference in radii of stars of 1.4 and 2.0 solar masses, which diminishes in the presence of quarks.","We compare the shapes of the mass-radius curves and show that hyperons and quarks in the neutron star cores prefer a particular curve shape with backbending.","It is argued that the shape {is controlled by the density dependence} of the nuclear symmetry energy.","We draw attention to the existence of a class of purely hadronic relativistic mean-field EoSs with scalar-field dependent hadron masses and coupling constants that satisfy the known constraints on the EoSs including the analyses of the new NICER data and the above requirement of no significant variation of the neutron star radii."],"url":"http://arxiv.org/abs/2404.09875v1","category":"nucl-th"}
{"created":"2024-04-15 15:33:29","title":"AI-Driven Statutory Reasoning via Software Engineering Methods","abstract":"The recent proliferation of generative artificial intelligence (GenAI) technologies such as pre-trained large language models (LLMs) has opened up new frontiers in computational law. An exciting area of development is the use of AI to automate the rule-based reasoning inherent in statutory and contract law. While this form of reasoning has long been studied using classical techniques of natural language processing (NLP) and formal logic, recent solutions increasingly make use of LLMs; though they are far from perfect.   The advent of GenAI has made it possible to treat many of these natural language documents essentially as programs that compute a result given some set of facts. As such, it should be possible to understand, debug, maintain, evolve, and fix these documents using well-studied techniques from the field of software engineering. This article introduces several concepts of automated software testing and program analysis that could potentially be useful in computational law when applied to AI-driven analysis of statutes and contracts.","sentences":["The recent proliferation of generative artificial intelligence (GenAI) technologies such as pre-trained large language models (LLMs) has opened up new frontiers in computational law.","An exciting area of development is the use of AI to automate the rule-based reasoning inherent in statutory and contract law.","While this form of reasoning has long been studied using classical techniques of natural language processing (NLP) and formal logic, recent solutions increasingly make use of LLMs; though they are far from perfect.   ","The advent of GenAI has made it possible to treat many of these natural language documents essentially as programs that compute a result given some set of facts.","As such, it should be possible to understand, debug, maintain, evolve, and fix these documents using well-studied techniques from the field of software engineering.","This article introduces several concepts of automated software testing and program analysis that could potentially be useful in computational law when applied to AI-driven analysis of statutes and contracts."],"url":"http://arxiv.org/abs/2404.09868v1","category":"cs.SE"}
{"created":"2024-04-15 15:23:34","title":"Quasar Microlensing Statistics and Flux-Ratio Anomalies in Lens Models","abstract":"Precise lens modeling is a critical step in time delay studies of multiply imaged quasars, which are key for measuring some important cosmological parameters (specially $H_0$). However, lens models (in particular those semi-automatically generated) often show discrepancies with the observed flux-ratios between the different quasar images. These flux-ratio anomalies are usually explained through differential effects between images (mainly microlensing) that alter the intrinsic magnification ratios predicted by the models. To check this hypothesis, we collect direct measurements of microlensing to obtain the histogram of microlensing magnifications. We compare this histogram with recently published model flux-ratio anomalies and conclude that they cannot be statistically explained by microlensing. The average value of the model anomalies ($0.74\\,$magnitudes) significantly exceeds the mean impact of microlensing ($0.33\\,$magnitudes). Moreover, the histogram of model anomalies presents a significant tail with high anomalies ($|\\Delta m| \\ge 0.7$ magnitudes) which is completely unexpected from the statistics of microlensing observations. Microlensing simulations neither predict the high mean nor the fat tail of the histogram of model anomalies. We perform several statistical tests which exclude that microlensing can explain the observed flux-ratio anomalies (although Kolmogorov-Smirnov, which is less sensitive to the tail of the distributions, is not always conclusive). Thus, microlensing cannot statistically explain the bulk of flux-ratio anomalies, and models may explore different alternatives to try to reduce them. In particular, we propose to complement photometric observations with accurate flux ratios of the broad emission lines obtained from integral field spectroscopy to check and, ideally, constrain lens models.","sentences":["Precise lens modeling is a critical step in time delay studies of multiply imaged quasars, which are key for measuring some important cosmological parameters (specially $H_0$).","However, lens models (in particular those semi-automatically generated) often show discrepancies with the observed flux-ratios between the different quasar images.","These flux-ratio anomalies are usually explained through differential effects between images (mainly microlensing) that alter the intrinsic magnification ratios predicted by the models.","To check this hypothesis, we collect direct measurements of microlensing to obtain the histogram of microlensing magnifications.","We compare this histogram with recently published model flux-ratio anomalies and conclude that they cannot be statistically explained by microlensing.","The average value of the model anomalies ($0.74\\,$magnitudes) significantly exceeds the mean impact of microlensing ($0.33\\,$magnitudes).","Moreover, the histogram of model anomalies presents a significant tail with high anomalies ($|\\Delta m| \\ge 0.7$ magnitudes) which is completely unexpected from the statistics of microlensing observations.","Microlensing simulations neither predict the high mean nor the fat tail of the histogram of model anomalies.","We perform several statistical tests which exclude that microlensing can explain the observed flux-ratio anomalies (although Kolmogorov-Smirnov, which is less sensitive to the tail of the distributions, is not always conclusive).","Thus, microlensing cannot statistically explain the bulk of flux-ratio anomalies, and models may explore different alternatives to try to reduce them.","In particular, we propose to complement photometric observations with accurate flux ratios of the broad emission lines obtained from integral field spectroscopy to check and, ideally, constrain lens models."],"url":"http://arxiv.org/abs/2404.09865v1","category":"astro-ph.CO"}
{"created":"2024-04-15 15:21:12","title":"Measuring a gravitomagentic effect with the triple pulsar PSR J0337+1715","abstract":"To the first post--Newtonian order, the orbital angular momentum of the fast--revolving inner binary of the triple system PSR J0337+1715, made of a millisecond pulsar and a white dwarf, induces an annular gravitomagnetic field which displaces the line of apsides of the slower orbit of the other, distant white dwarf by $-1.2$ milliarcseconds per year. The current accuracy in determining the periastron of the outer orbit is $63.9$ milliarcseconds after 1.38 years of data collection. By hypothesizing a constant rate of measurement of the pulsar's times of arrivals over the next 10 years, assumed equal to the present one, it can be argued that the periastron will be finally known to a $\\simeq 0.15$ milliarcseconds level, while its cumulative gravitomagnetic retrograde shift will be as large as $-12$ milliarcseconds. The competing post--Newtonian gravitolectric periastron advance due to the inner binary's masses, nominally amounting to $74.3$ milliarcseconds per year, can be presently modelled to an accuracy level as good as $\\simeq 0.04$ milliarcseconds per year. The mismodelling in the much larger Newtonian periastron rate due to the quadrupolar term of the multipolar expansion of the gravitational potential of a massive ring, whose nominal size for PSR J0337+1715 is $0.17$ degrees per year, might be reduced down to the $\\simeq 0.5$ milliarcseconds per year level over the next 10 years. Thus, a first measurement of such a novel form of gravitomagnetism, although challenging, may be somehow feasible in a not too distant future.","sentences":["To the first post--Newtonian order, the orbital angular momentum of the fast--revolving inner binary of the triple system PSR J0337+1715, made of a millisecond pulsar and a white dwarf, induces an annular gravitomagnetic field which displaces the line of apsides of the slower orbit of the other, distant white dwarf by $-1.2$ milliarcseconds per year.","The current accuracy in determining the periastron of the outer orbit is $63.9$ milliarcseconds after 1.38 years of data collection.","By hypothesizing a constant rate of measurement of the pulsar's times of arrivals over the next 10 years, assumed equal to the present one, it can be argued that the periastron will be finally known to a $\\simeq 0.15$ milliarcseconds level, while its cumulative gravitomagnetic retrograde shift will be as large as $-12$ milliarcseconds.","The competing post--Newtonian gravitolectric periastron advance due to the inner binary's masses, nominally amounting to $74.3$ milliarcseconds per year, can be presently modelled to an accuracy level as good as $\\simeq 0.04$ milliarcseconds per year.","The mismodelling in the much larger Newtonian periastron rate due to the quadrupolar term of the multipolar expansion of the gravitational potential of a massive ring, whose nominal size for PSR J0337+1715 is $0.17$ degrees per year, might be reduced down to the $\\simeq 0.5$ milliarcseconds per year level over the next 10 years.","Thus, a first measurement of such a novel form of gravitomagnetism, although challenging, may be somehow feasible in a not too distant future."],"url":"http://arxiv.org/abs/2404.09864v1","category":"gr-qc"}
{"created":"2024-04-15 15:12:53","title":"Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL","abstract":"Embodied visual tracking is to follow a target object in dynamic 3D environments using an agent's egocentric vision. This is a vital and challenging skill for embodied agents. However, existing methods suffer from inefficient training and poor generalization. In this paper, we propose a novel framework that combines visual foundation models (VFM) and offline reinforcement learning (offline RL) to empower embodied visual tracking. We use a pre-trained VFM, such as ``Tracking Anything\", to extract semantic segmentation masks with text prompts. We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online agent-environment interactions. To further improve the robustness and generalization of the policy network, we also introduce a mask re-targeting mechanism and a multi-level data collection strategy. In this way, we can train a robust tracker within an hour on a consumer-level GPU, e.g., Nvidia RTX 3090. Such efficiency is unprecedented for RL-based visual tracking methods. We evaluate our tracker on several high-fidelity environments with challenging situations, such as distraction and occlusion. The results show that our agent outperforms state-of-the-art methods in terms of sample efficiency, robustness to distractors, and generalization to unseen scenarios and targets. We also demonstrate the transferability of the learned tracker from the virtual world to real-world scenarios.","sentences":["Embodied visual tracking is to follow a target object in dynamic 3D environments using an agent's egocentric vision.","This is a vital and challenging skill for embodied agents.","However, existing methods suffer from inefficient training and poor generalization.","In this paper, we propose a novel framework that combines visual foundation models (VFM) and offline reinforcement learning (offline RL) to empower embodied visual tracking.","We use a pre-trained VFM, such as ``Tracking Anything\", to extract semantic segmentation masks with text prompts.","We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online agent-environment interactions.","To further improve the robustness and generalization of the policy network, we also introduce a mask re-targeting mechanism and a multi-level data collection strategy.","In this way, we can train a robust tracker within an hour on a consumer-level GPU, e.g., Nvidia RTX 3090.","Such efficiency is unprecedented for RL-based visual tracking methods.","We evaluate our tracker on several high-fidelity environments with challenging situations, such as distraction and occlusion.","The results show that our agent outperforms state-of-the-art methods in terms of sample efficiency, robustness to distractors, and generalization to unseen scenarios and targets.","We also demonstrate the transferability of the learned tracker from the virtual world to real-world scenarios."],"url":"http://arxiv.org/abs/2404.09857v1","category":"cs.CV"}
{"created":"2024-04-15 15:02:44","title":"Modeling the Lane-Change Reactions to Merging Vehicles for Highway On-Ramp Simulations","abstract":"Enhancing simulation environments to replicate real-world driver behavior is essential for developing Autonomous Vehicle technology. While some previous works have studied the yielding reaction of lag vehicles in response to a merging car at highway on-ramps, the possible lane-change reaction of the lag car has not been widely studied. In this work we aim to improve the simulation of the highway merge scenario by including the lane-change reaction in addition to yielding behavior of main-lane lag vehicles, and we evaluate two different models for their ability to capture this reactive lane-change behavior. To tune the payoff functions of these models, a novel naturalistic dataset was collected on U.S. highways that provided several hours of merge-specific data to learn the lane change behavior of U.S. drivers. To make sure that we are collecting a representative set of different U.S. highway geometries in our data, we surveyed 50,000 U.S. highway on-ramps and then selected eight representative sites. The data were collected using roadside-mounted lidar sensors to capture various merge driver interactions. The models were demonstrated to be configurable for both keep-straight and lane-change behavior. The models were finally integrated into a high-fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development.","sentences":["Enhancing simulation environments to replicate real-world driver behavior is essential for developing Autonomous Vehicle technology.","While some previous works have studied the yielding reaction of lag vehicles in response to a merging car at highway on-ramps, the possible lane-change reaction of the lag car has not been widely studied.","In this work we aim to improve the simulation of the highway merge scenario by including the lane-change reaction in addition to yielding behavior of main-lane lag vehicles, and we evaluate two different models for their ability to capture this reactive lane-change behavior.","To tune the payoff functions of these models, a novel naturalistic dataset was collected on U.S. highways that provided several hours of merge-specific data to learn the lane change behavior of U.S. drivers.","To make sure that we are collecting a representative set of different U.S. highway geometries in our data, we surveyed 50,000 U.S. highway on-ramps and then selected eight representative sites.","The data were collected using roadside-mounted lidar sensors to capture various merge driver interactions.","The models were demonstrated to be configurable for both keep-straight and lane-change behavior.","The models were finally integrated into a high-fidelity simulation environment and confirmed to have adequate computation time efficiency for use in large-scale simulations to support autonomous vehicle development."],"url":"http://arxiv.org/abs/2404.09851v1","category":"cs.RO"}
{"created":"2024-04-15 15:00:17","title":"HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge Representation","abstract":"In a hyper-relational knowledge graph (HKG), each fact is composed of a main triple associated with attribute-value qualifiers, which express additional factual knowledge. The hyper-relational knowledge graph completion (HKGC) task aims at inferring plausible missing links in a HKG. Most existing approaches to HKGC focus on enhancing the communication between qualifier pairs and main triples, while overlooking two important properties that emerge from the monotonicity of the hyper-relational graphs representation regime. Stage Reasoning allows for a two-step reasoning process, facilitating the integration of coarse-grained inference results derived solely from main triples and fine-grained inference results obtained from hyper-relational facts with qualifiers. In the initial stage, coarse-grained results provide an upper bound for correct predictions, which are subsequently refined in the fine-grained step. More generally, Qualifier Monotonicity implies that by attaching more qualifier pairs to a main triple, we may only narrow down the answer set, but never enlarge it. This paper proposes the HyperMono model for hyper-relational knowledge graph completion, which realizes stage reasoning and qualifier monotonicity. To implement qualifier monotonicity HyperMono resorts to cone embeddings. Experiments on three real-world datasets with three different scenario conditions demonstrate the strong performance of HyperMono when compared to the SoTA.","sentences":["In a hyper-relational knowledge graph (HKG), each fact is composed of a main triple associated with attribute-value qualifiers, which express additional factual knowledge.","The hyper-relational knowledge graph completion (HKGC) task aims at inferring plausible missing links in a HKG.","Most existing approaches to HKGC focus on enhancing the communication between qualifier pairs and main triples, while overlooking two important properties that emerge from the monotonicity of the hyper-relational graphs representation regime.","Stage Reasoning allows for a two-step reasoning process, facilitating the integration of coarse-grained inference results derived solely from main triples and fine-grained inference results obtained from hyper-relational facts with qualifiers.","In the initial stage, coarse-grained results provide an upper bound for correct predictions, which are subsequently refined in the fine-grained step.","More generally, Qualifier Monotonicity implies that by attaching more qualifier pairs to a main triple, we may only narrow down the answer set, but never enlarge it.","This paper proposes the HyperMono model for hyper-relational knowledge graph completion, which realizes stage reasoning and qualifier monotonicity.","To implement qualifier monotonicity HyperMono resorts to cone embeddings.","Experiments on three real-world datasets with three different scenario conditions demonstrate the strong performance of HyperMono when compared to the SoTA."],"url":"http://arxiv.org/abs/2404.09848v1","category":"cs.AI"}
{"created":"2024-04-15 14:55:43","title":"A Diffusion-based Data Generator for Training Object Recognition Models in Ultra-Range Distance","abstract":"Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot's camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot.","sentences":["Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks.","Some tasks require recognizing objects far from the robot's camera.","A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot.","However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples.","The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images.","In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes.","The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image.","We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish.","DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model.","More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data.","The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot."],"url":"http://arxiv.org/abs/2404.09846v1","category":"cs.CV"}
{"created":"2024-04-15 14:32:32","title":"Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video","abstract":"Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.","sentences":["Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes.","In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments.","At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects.","By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world.","We benchmark our system on both indoor and large-scale outdoor scenes.","We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top."],"url":"http://arxiv.org/abs/2404.09833v1","category":"cs.CV"}
{"created":"2024-04-15 14:28:33","title":"Negation Triplet Extraction with Syntactic Dependency and Semantic Consistency","abstract":"Previous works of negation understanding mainly focus on negation cue detection and scope resolution, without identifying negation subject which is also significant to the downstream tasks. In this paper, we propose a new negation triplet extraction (NTE) task which aims to extract negation subject along with negation cue and scope. To achieve NTE, we devise a novel Syntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is built based on a generative pretrained language model (PLM) {of Encoder-Decoder architecture} with a multi-task learning framework. Specifically, the given sentence's syntactic dependency tree is incorporated into the PLM's encoder to discover the correlations between the negation subject, cue and scope. Moreover, the semantic consistency between the sentence and the extracted triplet is ensured by an auxiliary task learning. Furthermore, we have constructed a high-quality Chinese dataset NegComment based on the users' reviews from the real-world platform of Meituan, upon which our evaluations show that SSENE achieves the best NTE performance compared to the baselines. Our ablation and case studies also demonstrate that incorporating the syntactic information helps the PLM's recognize the distant dependency between the subject and cue, and the auxiliary task learning is helpful to extract the negation triplets with more semantic consistency.","sentences":["Previous works of negation understanding mainly focus on negation cue detection and scope resolution, without identifying negation subject which is also significant to the downstream tasks.","In this paper, we propose a new negation triplet extraction (NTE) task which aims to extract negation subject along with negation cue and scope.","To achieve NTE, we devise a novel Syntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is built based on a generative pretrained language model (PLM) {of Encoder-Decoder architecture} with a multi-task learning framework.","Specifically, the given sentence's syntactic dependency tree is incorporated into the PLM's encoder to discover the correlations between the negation subject, cue and scope.","Moreover, the semantic consistency between the sentence and the extracted triplet is ensured by an auxiliary task learning.","Furthermore, we have constructed a high-quality Chinese dataset NegComment based on the users' reviews from the real-world platform of Meituan, upon which our evaluations show that SSENE achieves the best NTE performance compared to the baselines.","Our ablation and case studies also demonstrate that incorporating the syntactic information helps the PLM's recognize the distant dependency between the subject and cue, and the auxiliary task learning is helpful to extract the negation triplets with more semantic consistency."],"url":"http://arxiv.org/abs/2404.09830v1","category":"cs.CL"}
{"created":"2024-04-15 14:26:00","title":"Interaction as Explanation: A User Interaction-based Method for Explaining Image Classification Models","abstract":"In computer vision, explainable AI (xAI) methods seek to mitigate the 'black-box' problem by making the decision-making process of deep learning models more interpretable and transparent. Traditional xAI methods concentrate on visualizing input features that influence model predictions, providing insights primarily suited for experts. In this work, we present an interaction-based xAI method that enhances user comprehension of image classification models through their interaction. Thus, we developed a web-based prototype allowing users to modify images via painting and erasing, thereby observing changes in classification results. Our approach enables users to discern critical features influencing the model's decision-making process, aligning their mental models with the model's logic. Experiments conducted with five images demonstrate the potential of the method to reveal feature importance through user interaction. Our work contributes a novel perspective to xAI by centering on end-user engagement and understanding, paving the way for more intuitive and accessible explainability in AI systems.","sentences":["In computer vision, explainable AI (xAI) methods seek to mitigate the 'black-box' problem by making the decision-making process of deep learning models more interpretable and transparent.","Traditional xAI methods concentrate on visualizing input features that influence model predictions, providing insights primarily suited for experts.","In this work, we present an interaction-based xAI method that enhances user comprehension of image classification models through their interaction.","Thus, we developed a web-based prototype allowing users to modify images via painting and erasing, thereby observing changes in classification results.","Our approach enables users to discern critical features influencing the model's decision-making process, aligning their mental models with the model's logic.","Experiments conducted with five images demonstrate the potential of the method to reveal feature importance through user interaction.","Our work contributes a novel perspective to xAI by centering on end-user engagement and understanding, paving the way for more intuitive and accessible explainability in AI systems."],"url":"http://arxiv.org/abs/2404.09828v1","category":"cs.HC"}
{"created":"2024-04-15 14:22:51","title":"Mapping the anisotropic Galactic stellar halo with Blue Horizontal Branch stars","abstract":"We use Legacy Survey photometric data to probe the stellar halo in multiple directions of the sky using a probabilistic methodology to identify Blue Horizontal Branch (BHB) stars. The measured average radial density profile follows a double power law in the range $ 5 < r_{gc}/{\\rm kpc} < 120$, with a density break at $r_{gc}\\approx20$ kpc. This description, however, falls short, depending on the chosen line-of-sight, with some regions showing no signature of a break in the profile and a wide range of density slopes, e.g. outer slope $-5.5 \\lesssim \\alpha_{out} \\lesssim -4$, pointing towards a highly anisotropic stellar halo. This explains in part the wide range of density profiles reported in the literature owing to different tracers and sky coverage. Using our detailed 3-D stellar halo density map, we quantify the shape of the Pisces overdensity associated with the transient wake response of the Galaxy's (dark) halo to the Large Magellanic Cloud (LMC). Measured in the LMC's coordinate system, Pisces stands above the background, is 60 degrees long and 25 degrees wide and aligned with the LMC's orbit. This would correspond to a wake width of $\\sim 32$ kpc at $\\sim 70$ kpc. We do not find a statistically significant signature of the collective response in density as previously reported in the literature measured with K giant stars, despite our larger numbers. We release the catalogue constructed in this study with 95,446 possible BHB stars and their BHB probability.","sentences":["We use Legacy Survey photometric data to probe the stellar halo in multiple directions of the sky using a probabilistic methodology to identify Blue Horizontal Branch (BHB) stars.","The measured average radial density profile follows a double power law in the range $ 5 < r_{gc}/{\\rm kpc} < 120$, with a density break at $r_{gc}\\approx20$ kpc.","This description, however, falls short, depending on the chosen line-of-sight, with some regions showing no signature of a break in the profile and a wide range of density slopes, e.g. outer slope $-5.5 \\lesssim \\alpha_{out} \\lesssim -4$, pointing towards a highly anisotropic stellar halo.","This explains in part the wide range of density profiles reported in the literature owing to different tracers and sky coverage.","Using our detailed 3-D stellar halo density map, we quantify the shape of the Pisces overdensity associated with the transient wake response of the Galaxy's (dark) halo to the Large Magellanic Cloud (LMC).","Measured in the LMC's coordinate system, Pisces stands above the background, is 60 degrees long and 25 degrees wide and aligned with the LMC's orbit.","This would correspond to a wake width of $\\sim 32$ kpc at $\\sim 70$ kpc.","We do not find a statistically significant signature of the collective response in density as previously reported in the literature measured with K giant stars, despite our larger numbers.","We release the catalogue constructed in this study with 95,446 possible BHB stars and their BHB probability."],"url":"http://arxiv.org/abs/2404.09825v1","category":"astro-ph.GA"}
{"created":"2024-04-15 14:06:50","title":"Data-Driven Stability Assessment of Power Electronic Converters with Multi-Resolution Dynamic Mode Decomposition","abstract":"Harmonic instability occurs frequently in the power electronic converter system. This paper leverages multi-resolution dynamic mode decomposition (MR-DMD) as a data-driven diagnostic tool for the system stability of power electronic converters, not requiring complex modeling and detailed control information. By combining dynamic mode decomposition (DMD) with the multi-resolution analysis used in wavelet theory, dynamic modes and eigenvalues can be identified at different decomposition levels and time scales with the MR-DMD algorithm, thereby allowing for handling datasets with transient time behaviors, which is not achievable using conventional DMD. Further, the selection criteria for important parameters in MR-DMD are clearly defined through derivation, elucidating the reason for enabling it to extract eigenvalues within different frequency ranges. Finally, the analysis results are verified using the dataset collected from the experimental platform of a low-frequency oscillation scenario in electrified railways featuring a single-phase converter.","sentences":["Harmonic instability occurs frequently in the power electronic converter system.","This paper leverages multi-resolution dynamic mode decomposition (MR-DMD) as a data-driven diagnostic tool for the system stability of power electronic converters, not requiring complex modeling and detailed control information.","By combining dynamic mode decomposition (DMD) with the multi-resolution analysis used in wavelet theory, dynamic modes and eigenvalues can be identified at different decomposition levels and time scales with the MR-DMD algorithm, thereby allowing for handling datasets with transient time behaviors, which is not achievable using conventional DMD.","Further, the selection criteria for important parameters in MR-DMD are clearly defined through derivation, elucidating the reason for enabling it to extract eigenvalues within different frequency ranges.","Finally, the analysis results are verified using the dataset collected from the experimental platform of a low-frequency oscillation scenario in electrified railways featuring a single-phase converter."],"url":"http://arxiv.org/abs/2404.09808v1","category":"eess.SP"}
{"created":"2024-04-15 13:51:05","title":"Emergent Language Symbolic Autoencoder (ELSA) with Weak Supervision to Model Hierarchical Brain Networks","abstract":"Brain networks display a hierarchical organization, a complexity that poses a challenge for existing deep learning models, often structured as flat classifiers, leading to difficulties in interpretability and the 'black box' issue. To bridge this gap, we propose a novel architecture: a symbolic autoencoder informed by weak supervision and an Emergent Language (EL) framework. This model moves beyond traditional flat classifiers by producing hierarchical clusters and corresponding imagery, subsequently represented through symbolic sentences to improve the clinical interpretability of hierarchically organized data such as intrinsic brain networks, which can be characterized using resting-state fMRI images. Our innovation includes a generalized hierarchical loss function designed to ensure that both sentences and images accurately reflect the hierarchical structure of functional brain networks. This enables us to model functional brain networks from a broader perspective down to more granular details. Furthermore, we introduce a quantitative method to assess the hierarchical consistency of these symbolic representations. Our qualitative analyses show that our model successfully generates hierarchically organized, clinically interpretable images, a finding supported by our quantitative evaluations. We find that our best performing loss function leads to a hierarchical consistency of over 97% when identifying images corresponding to brain networks. This approach not only advances the interpretability of deep learning models in neuroimaging analysis but also represents a significant step towards modeling the intricate hierarchical nature of brain networks.","sentences":["Brain networks display a hierarchical organization, a complexity that poses a challenge for existing deep learning models, often structured as flat classifiers, leading to difficulties in interpretability and the 'black box' issue.","To bridge this gap, we propose a novel architecture: a symbolic autoencoder informed by weak supervision and an Emergent Language (EL) framework.","This model moves beyond traditional flat classifiers by producing hierarchical clusters and corresponding imagery, subsequently represented through symbolic sentences to improve the clinical interpretability of hierarchically organized data such as intrinsic brain networks, which can be characterized using resting-state fMRI images.","Our innovation includes a generalized hierarchical loss function designed to ensure that both sentences and images accurately reflect the hierarchical structure of functional brain networks.","This enables us to model functional brain networks from a broader perspective down to more granular details.","Furthermore, we introduce a quantitative method to assess the hierarchical consistency of these symbolic representations.","Our qualitative analyses show that our model successfully generates hierarchically organized, clinically interpretable images, a finding supported by our quantitative evaluations.","We find that our best performing loss function leads to a hierarchical consistency of over 97% when identifying images corresponding to brain networks.","This approach not only advances the interpretability of deep learning models in neuroimaging analysis but also represents a significant step towards modeling the intricate hierarchical nature of brain networks."],"url":"http://arxiv.org/abs/2404.10031v1","category":"q-bio.NC"}
{"created":"2024-04-15 13:48:25","title":"First Search for Light Fermionic Dark Matter Absorption on Electrons Using Germanium Detector in CDEX-10 Experiment","abstract":"We present the first results of the search for sub-MeV fermionic dark matter absorbed by electron targets of Germanium using the 205.4~kg$\\cdot$day data collected by the CDEX-10 experiment, with the analysis threshold of 160~eVee. No significant dark matter (DM) signals over the background are observed. Results are presented as limits on the cross section of DM--electron interaction. We present new constraints of cross section in the DM range of 0.1--10 keV/$c^2$ for vector and axial-vector interaction. The upper limit on the cross section is set to be $\\rm 5.5\\times10^{-46}~cm^2$ for vector interaction, and $\\rm 1.8\\times10^{-46}~cm^2$ for axial-vector interaction at DM mass of 5 keV/$c^2$.","sentences":["We present the first results of the search for sub-MeV fermionic dark matter absorbed by electron targets of Germanium using the 205.4~kg$\\cdot$day data collected by the CDEX-10 experiment, with the analysis threshold of 160~eVee.","No significant dark matter (DM) signals over the background are observed.","Results are presented as limits on the cross section of DM--electron interaction.","We present new constraints of cross section in the DM range of 0.1--10 keV/$c^2$ for vector and axial-vector interaction.","The upper limit on the cross section is set to be $\\rm 5.5\\times10^{-46}~cm^2$ for vector interaction, and $\\rm 1.8\\times10^{-46}~cm^2$ for axial-vector interaction at DM mass of 5 keV/$c^2$."],"url":"http://arxiv.org/abs/2404.09793v1","category":"hep-ex"}
{"created":"2024-04-15 13:45:48","title":"NTIRE 2024 Challenge on Image Super-Resolution ($\\times$4): Methods and Results","abstract":"This paper reviews the NTIRE 2024 challenge on image super-resolution ($\\times$4), highlighting the solutions proposed and the outcomes obtained. The challenge involves generating corresponding high-resolution (HR) images, magnified by a factor of four, from low-resolution (LR) inputs using prior information. The LR images originate from bicubic downsampling degradation. The aim of the challenge is to obtain designs/solutions with the most advanced SR performance, with no constraints on computational resources (e.g., model size and FLOPs) or training data. The track of this challenge assesses performance with the PSNR metric on the DIV2K testing dataset. The competition attracted 199 registrants, with 20 teams submitting valid entries. This collective endeavour not only pushes the boundaries of performance in single-image SR but also offers a comprehensive overview of current trends in this field.","sentences":["This paper reviews the NTIRE 2024 challenge on image super-resolution ($\\times$4), highlighting the solutions proposed and the outcomes obtained.","The challenge involves generating corresponding high-resolution (HR) images, magnified by a factor of four, from low-resolution (LR) inputs using prior information.","The LR images originate from bicubic downsampling degradation.","The aim of the challenge is to obtain designs/solutions with the most advanced SR performance, with no constraints on computational resources (e.g., model size and FLOPs) or training data.","The track of this challenge assesses performance with the PSNR metric on the DIV2K testing dataset.","The competition attracted 199 registrants, with 20 teams submitting valid entries.","This collective endeavour not only pushes the boundaries of performance in single-image SR but also offers a comprehensive overview of current trends in this field."],"url":"http://arxiv.org/abs/2404.09790v1","category":"cs.CV"}
{"created":"2024-04-15 13:44:01","title":"Shape Arithmetic Expressions: Advancing Scientific Discovery Beyond Closed-Form Equations","abstract":"Symbolic regression has excelled in uncovering equations from physics, chemistry, biology, and related disciplines. However, its effectiveness becomes less certain when applied to experimental data lacking inherent closed-form expressions. Empirically derived relationships, such as entire stress-strain curves, may defy concise closed-form representation, compelling us to explore more adaptive modeling approaches that balance flexibility with interpretability. In our pursuit, we turn to Generalized Additive Models (GAMs), a widely used class of models known for their versatility across various domains. Although GAMs can capture non-linear relationships between variables and targets, they cannot capture intricate feature interactions. In this work, we investigate both of these challenges and propose a novel class of models, Shape Arithmetic Expressions (SHAREs), that fuses GAM's flexible shape functions with the complex feature interactions found in mathematical expressions. SHAREs also provide a unifying framework for both of these approaches. We also design a set of rules for constructing SHAREs that guarantee transparency of the found expressions beyond the standard constraints based on the model's size.","sentences":["Symbolic regression has excelled in uncovering equations from physics, chemistry, biology, and related disciplines.","However, its effectiveness becomes less certain when applied to experimental data lacking inherent closed-form expressions.","Empirically derived relationships, such as entire stress-strain curves, may defy concise closed-form representation, compelling us to explore more adaptive modeling approaches that balance flexibility with interpretability.","In our pursuit, we turn to Generalized Additive Models (GAMs), a widely used class of models known for their versatility across various domains.","Although GAMs can capture non-linear relationships between variables and targets, they cannot capture intricate feature interactions.","In this work, we investigate both of these challenges and propose a novel class of models, Shape Arithmetic Expressions (SHAREs), that fuses GAM's flexible shape functions with the complex feature interactions found in mathematical expressions.","SHAREs also provide a unifying framework for both of these approaches.","We also design a set of rules for constructing SHAREs that guarantee transparency of the found expressions beyond the standard constraints based on the model's size."],"url":"http://arxiv.org/abs/2404.09788v1","category":"cs.LG"}
{"created":"2024-04-15 13:39:11","title":"ChainScience 2024, Conference Proceedings","abstract":"ChainScience 2024, the second edition of the interdisciplinary conference, brought together academics, practitioners, and industry experts to explore novel developments in the realm of distributed ledger technologies. The conference aimed to bridge diverse fields such as informatics, business, economics, finance, regulation, law, mathematics, physics, and complexity science. The papers presented in these conference proceedings address emerging topics such as AI/ML applications to blockchain, DLTs interoperability, decentralized financial services, and tokenomics, alongside ethical, societal, and governance aspects of blockchain and DLTs.   With a focus on promoting high-quality research and interdisciplinary collaboration, ChainScience24 aimed to unlock the collective potential of its diverse participants, embodying the ethos that the whole is greater than the sum of its parts.","sentences":["ChainScience 2024, the second edition of the interdisciplinary conference, brought together academics, practitioners, and industry experts to explore novel developments in the realm of distributed ledger technologies.","The conference aimed to bridge diverse fields such as informatics, business, economics, finance, regulation, law, mathematics, physics, and complexity science.","The papers presented in these conference proceedings address emerging topics such as AI/ML applications to blockchain, DLTs interoperability, decentralized financial services, and tokenomics, alongside ethical, societal, and governance aspects of blockchain and DLTs.   ","With a focus on promoting high-quality research and interdisciplinary collaboration, ChainScience24 aimed to unlock the collective potential of its diverse participants, embodying the ethos that the whole is greater than the sum of its parts."],"url":"http://arxiv.org/abs/2404.09782v2","category":"cs.DC"}
{"created":"2024-04-15 13:30:34","title":"The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning","abstract":"Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot learning performance. Few-shot learning aims to further enhance the transfer capability of CLIP by giving few images in each class, aka 'few shots'. Most existing methods either implicitly learn from the few shots by incorporating learnable prompts or adapters, or explicitly embed them in a cache model for inference. However, the narrow distribution of few shots often contains incomplete class information, leading to biased visual knowledge with high risk of misclassification. To tackle this problem, recent methods propose to supplement visual knowledge by generative models or extra databases, which can be costly and time-consuming. In this paper, we propose an Iterative Visual Knowledge CompLetion (KCL) method to complement visual knowledge by properly taking advantages of unlabeled samples without access to any auxiliary or synthetic data. Specifically, KCL first measures the similarities between unlabeled samples and each category. Then, the samples with top confidence to each category is selected and collected by a designed confidence criterion. Finally, the collected samples are treated as labeled ones and added to few shots to jointly re-estimate the remaining unlabeled ones. The above procedures will be repeated for a certain number of iterations with more and more samples being collected until convergence, ensuring a progressive and robust knowledge completion process. Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of KCL as a plug-and-play module under both few-shot and zero-shot learning settings. Code is available at https://github.com/Mark-Sky/KCL.","sentences":["Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot learning performance.","Few-shot learning aims to further enhance the transfer capability of CLIP by giving few images in each class, aka 'few shots'.","Most existing methods either implicitly learn from the few shots by incorporating learnable prompts or adapters, or explicitly embed them in a cache model for inference.","However, the narrow distribution of few shots often contains incomplete class information, leading to biased visual knowledge with high risk of misclassification.","To tackle this problem, recent methods propose to supplement visual knowledge by generative models or extra databases, which can be costly and time-consuming.","In this paper, we propose an Iterative Visual Knowledge CompLetion (KCL) method to complement visual knowledge by properly taking advantages of unlabeled samples without access to any auxiliary or synthetic data.","Specifically, KCL first measures the similarities between unlabeled samples and each category.","Then, the samples with top confidence to each category is selected and collected by a designed confidence criterion.","Finally, the collected samples are treated as labeled ones and added to few shots to jointly re-estimate the remaining unlabeled ones.","The above procedures will be repeated for a certain number of iterations with more and more samples being collected until convergence, ensuring a progressive and robust knowledge completion process.","Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of KCL as a plug-and-play module under both few-shot and zero-shot learning settings.","Code is available at https://github.com/Mark-Sky/KCL."],"url":"http://arxiv.org/abs/2404.09778v1","category":"cs.CV"}
{"created":"2024-04-15 13:06:32","title":"KG-CTG: Citation Generation through Knowledge Graph-guided Large Language Models","abstract":"Citation Text Generation (CTG) is a task in natural language processing (NLP) that aims to produce text that accurately cites or references a cited document within a source document. In CTG, the generated text draws upon contextual cues from both the source document and the cited paper, ensuring accurate and relevant citation information is provided. Previous work in the field of citation generation is mainly based on the text summarization of documents. Following this, this paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation. Also, we have shown the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers. To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language. Vicuna performs best for this task with 14.15 Meteor, 12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by including knowledge graphs.","sentences":["Citation Text Generation (CTG) is a task in natural language processing (NLP) that aims to produce text that accurately cites or references a cited document within a source document.","In CTG, the generated text draws upon contextual cues from both the source document and the cited paper, ensuring accurate and relevant citation information is provided.","Previous work in the field of citation generation is mainly based on the text summarization of documents.","Following this, this paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation.","Also, we have shown the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers.","To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language.","Vicuna performs best for this task with 14.15 Meteor, 12.88 Rouge-1, 1.52 Rouge-2, and","10.94 Rouge-L. Also, Alpaca performs best, and improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by including knowledge graphs."],"url":"http://arxiv.org/abs/2404.09763v1","category":"cs.CL"}
{"created":"2024-04-15 13:03:42","title":"Deep Learning-Based Segmentation of Tumors in PET/CT Volumes: Benchmark of Different Architectures and Training Strategies","abstract":"Cancer is one of the leading causes of death globally, and early diagnosis is crucial for patient survival. Deep learning algorithms have great potential for automatic cancer analysis. Artificial intelligence has achieved high performance in recognizing and segmenting single lesions. However, diagnosing multiple lesions remains a challenge. This study examines and compares various neural network architectures and training strategies for automatically segmentation of cancer lesions using PET/CT images from the head, neck, and whole body. The authors analyzed datasets from the AutoPET and HECKTOR challenges, exploring popular single-step segmentation architectures and presenting a two-step approach. The results indicate that the V-Net and nnU-Net models were the most effective for their respective datasets. The results for the HECKTOR dataset ranged from 0.75 to 0.76 for the aggregated Dice coefficient. Eliminating cancer-free cases from the AutoPET dataset was found to improve the performance of most models. In the case of AutoPET data, the average segmentation efficiency after training only on images containing cancer lesions increased from 0.55 to 0.66 for the classic Dice coefficient and from 0.65 to 0.73 for the aggregated Dice coefficient. The research demonstrates the potential of artificial intelligence in precise oncological diagnostics and may contribute to the development of more targeted and effective cancer assessment techniques.","sentences":["Cancer is one of the leading causes of death globally, and early diagnosis is crucial for patient survival.","Deep learning algorithms have great potential for automatic cancer analysis.","Artificial intelligence has achieved high performance in recognizing and segmenting single lesions.","However, diagnosing multiple lesions remains a challenge.","This study examines and compares various neural network architectures and training strategies for automatically segmentation of cancer lesions using PET/CT images from the head, neck, and whole body.","The authors analyzed datasets from the AutoPET and HECKTOR challenges, exploring popular single-step segmentation architectures and presenting a two-step approach.","The results indicate that the V-Net and nnU-Net models were the most effective for their respective datasets.","The results for the HECKTOR dataset ranged from 0.75 to 0.76 for the aggregated Dice coefficient.","Eliminating cancer-free cases from the AutoPET dataset was found to improve the performance of most models.","In the case of AutoPET data, the average segmentation efficiency after training only on images containing cancer lesions increased from 0.55 to 0.66 for the classic Dice coefficient and from 0.65 to 0.73 for the aggregated Dice coefficient.","The research demonstrates the potential of artificial intelligence in precise oncological diagnostics and may contribute to the development of more targeted and effective cancer assessment techniques."],"url":"http://arxiv.org/abs/2404.09761v1","category":"eess.IV"}
{"created":"2024-04-15 13:02:00","title":"Effective Reinforcement Learning Based on Structural Information Principles","abstract":"Although Reinforcement Learning (RL) algorithms acquire sequential behavioral patterns through interactions with the environment, their effectiveness in noisy and high-dimensional scenarios typically relies on specific structural priors. In this paper, we propose a novel and general Structural Information principles-based framework for effective Decision-Making, namely SIDM, approached from an information-theoretic perspective. This paper presents a specific unsupervised partitioning method that forms vertex communities in the state and action spaces based on their feature similarities. An aggregation function, which utilizes structural entropy as the vertex weight, is devised within each community to obtain its embedding, thereby facilitating hierarchical state and action abstractions. By extracting abstract elements from historical trajectories, a directed, weighted, homogeneous transition graph is constructed. The minimization of this graph's high-dimensional entropy leads to the generation of an optimal encoding tree. An innovative two-layer skill-based learning mechanism is introduced to compute the common path entropy of each state transition as its identified probability, thereby obviating the requirement for expert knowledge. Moreover, SIDM can be flexibly incorporated into various single-agent and multi-agent RL algorithms, enhancing their performance. Finally, extensive evaluations on challenging benchmarks demonstrate that, compared with SOTA baselines, our framework significantly and consistently improves the policy's quality, stability, and efficiency up to 32.70%, 88.26%, and 64.86%, respectively.","sentences":["Although Reinforcement Learning (RL) algorithms acquire sequential behavioral patterns through interactions with the environment, their effectiveness in noisy and high-dimensional scenarios typically relies on specific structural priors.","In this paper, we propose a novel and general Structural Information principles-based framework for effective Decision-Making, namely SIDM, approached from an information-theoretic perspective.","This paper presents a specific unsupervised partitioning method that forms vertex communities in the state and action spaces based on their feature similarities.","An aggregation function, which utilizes structural entropy as the vertex weight, is devised within each community to obtain its embedding, thereby facilitating hierarchical state and action abstractions.","By extracting abstract elements from historical trajectories, a directed, weighted, homogeneous transition graph is constructed.","The minimization of this graph's high-dimensional entropy leads to the generation of an optimal encoding tree.","An innovative two-layer skill-based learning mechanism is introduced to compute the common path entropy of each state transition as its identified probability, thereby obviating the requirement for expert knowledge.","Moreover, SIDM can be flexibly incorporated into various single-agent and multi-agent RL algorithms, enhancing their performance.","Finally, extensive evaluations on challenging benchmarks demonstrate that, compared with SOTA baselines, our framework significantly and consistently improves the policy's quality, stability, and efficiency up to 32.70%, 88.26%, and 64.86%, respectively."],"url":"http://arxiv.org/abs/2404.09760v1","category":"cs.LG"}
{"created":"2024-04-16 17:57:19","title":"Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes","abstract":"Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time.","However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians.","In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes.","Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work.","We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry.","Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's complexity.","Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis.","Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed."],"url":"http://arxiv.org/abs/2404.10772v1","category":"cs.CV"}
{"created":"2024-04-16 17:53:59","title":"Finite-dimensional approximations of push-forwards on locally analytic functionals and truncation of least-squares polynomials","abstract":"This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations. Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself. We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space. Moreover, we prove a rigorous convergence result with a convergence rate. As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution. One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward. Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation.","sentences":["This paper introduces a theoretical framework for investigating analytic maps from finite discrete data, elucidating mathematical machinery underlying the polynomial approximation with least-squares in multivariate situations.","Our approach is to consider the push-forward on the space of locally analytic functionals, instead of directly handling the analytic map itself.","We establish a methodology enabling appropriate finite-dimensional approximation of the push-forward from finite discrete data, through the theory of the Fourier--Borel transform and the Fock space.","Moreover, we prove a rigorous convergence result with a convergence rate.","As an application, we prove that it is not the least-squares polynomial, but the polynomial obtained by truncating its higher-degree terms, that approximates analytic functions and further allows for approximation beyond the support of the data distribution.","One advantage of our theory is that it enables us to apply linear algebraic operations to the finite-dimensional approximation of the push-forward.","Utilizing this, we prove the convergence of a method for approximating an analytic vector field from finite data of the flow map of an ordinary differential equation."],"url":"http://arxiv.org/abs/2404.10769v1","category":"math.NA"}
{"created":"2024-04-16 17:51:40","title":"Privacy Can Arise Endogenously in an Economic System with Learning Agents","abstract":"We study price-discrimination games between buyers and a seller where privacy arises endogenously--that is, utility maximization yields equilibrium strategies where privacy occurs naturally. In this game, buyers with a high valuation for a good have an incentive to keep their valuation private, lest the seller charge them a higher price. This yields an equilibrium where some buyers will send a signal that misrepresents their type with some probability; we refer to this as buyer-induced privacy. When the seller is able to publicly commit to providing a certain privacy level, we find that their equilibrium response is to commit to ignore buyers' signals with some positive probability; we refer to this as seller-induced privacy. We then turn our attention to a repeated interaction setting where the game parameters are unknown and the seller cannot credibly commit to a level of seller-induced privacy. In this setting, players must learn strategies based on information revealed in past rounds. We find that, even without commitment ability, seller-induced privacy arises as a result of reputation building. We characterize the resulting seller-induced privacy and seller's utility under no-regret and no-policy-regret learning algorithms and verify these results through simulations.","sentences":["We study price-discrimination games between buyers and a seller where privacy arises endogenously--that is, utility maximization yields equilibrium strategies where privacy occurs naturally.","In this game, buyers with a high valuation for a good have an incentive to keep their valuation private, lest the seller charge them a higher price.","This yields an equilibrium where some buyers will send a signal that misrepresents their type with some probability; we refer to this as buyer-induced privacy.","When the seller is able to publicly commit to providing a certain privacy level, we find that their equilibrium response is to commit to ignore buyers' signals with some positive probability; we refer to this as seller-induced privacy.","We then turn our attention to a repeated interaction setting where the game parameters are unknown and the seller cannot credibly commit to a level of seller-induced privacy.","In this setting, players must learn strategies based on information revealed in past rounds.","We find that, even without commitment ability, seller-induced privacy arises as a result of reputation building.","We characterize the resulting seller-induced privacy and seller's utility under no-regret and no-policy-regret learning algorithms and verify these results through simulations."],"url":"http://arxiv.org/abs/2404.10767v1","category":"cs.GT"}
{"created":"2024-04-16 17:50:09","title":"RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans","abstract":"Two-dimensional (2D) freehand ultrasonography is one of the most commonly used medical imaging modalities, particularly in obstetrics and gynaecology. However, it only captures 2D cross-sectional views of inherently 3D anatomies, losing valuable contextual information. As an alternative to requiring costly and complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans using machine learning. However this usually requires long computational time. Here, we propose RapidVol: a neural representation framework to speed up slice-to-volume ultrasound reconstruction. We use tensor-rank decomposition, to decompose the typical 3D volume into sets of tri-planes, and store those instead, as well as a small neural network. A set of 2D ultrasound scans, with their ground truth (or estimated) 3D position and orientation (pose) is all that is required to form a complete 3D reconstruction. Reconstructions are formed from real fetal brain scans, and then evaluated by requesting novel cross-sectional views. When compared to prior approaches based on fully implicit representation (e.g. neural radiance fields), our method is over 3x quicker, 46% more accurate, and if given inaccurate poses is more robust. Further speed-up is also possible by reconstructing from a structural prior rather than from scratch.","sentences":["Two-dimensional (2D) freehand ultrasonography is one of the most commonly used medical imaging modalities, particularly in obstetrics and gynaecology.","However, it only captures 2D cross-sectional views of inherently 3D anatomies, losing valuable contextual information.","As an alternative to requiring costly and complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans using machine learning.","However this usually requires long computational time.","Here, we propose RapidVol: a neural representation framework to speed up slice-to-volume ultrasound reconstruction.","We use tensor-rank decomposition, to decompose the typical 3D volume into sets of tri-planes, and store those instead, as well as a small neural network.","A set of 2D ultrasound scans, with their ground truth (or estimated) 3D position and orientation (pose) is all that is required to form a complete 3D reconstruction.","Reconstructions are formed from real fetal brain scans, and then evaluated by requesting novel cross-sectional views.","When compared to prior approaches based on fully implicit representation (e.g. neural radiance fields), our method is over 3x quicker, 46% more accurate, and if given inaccurate poses is more robust.","Further speed-up is also possible by reconstructing from a structural prior rather than from scratch."],"url":"http://arxiv.org/abs/2404.10766v1","category":"eess.IV"}
{"created":"2024-04-16 17:47:27","title":"Confidential Federated Computations","abstract":"Federated Learning and Analytics (FLA) have seen widespread adoption by technology platforms for processing sensitive on-device data. However, basic FLA systems have privacy limitations: they do not necessarily require anonymization mechanisms like differential privacy (DP), and provide limited protections against a potentially malicious service provider. Adding DP to a basic FLA system currently requires either adding excessive noise to each device's updates, or assuming an honest service provider that correctly implements the mechanism and only uses the privatized outputs. Secure multiparty computation (SMPC) -based oblivious aggregations can limit the service provider's access to individual user updates and improve DP tradeoffs, but the tradeoffs are still suboptimal, and they suffer from scalability challenges and susceptibility to Sybil attacks. This paper introduces a novel system architecture that leverages trusted execution environments (TEEs) and open-sourcing to both ensure confidentiality of server-side computations and provide externally verifiable privacy properties, bolstering the robustness and trustworthiness of private federated computations.","sentences":["Federated Learning and Analytics (FLA) have seen widespread adoption by technology platforms for processing sensitive on-device data.","However, basic FLA systems have privacy limitations: they do not necessarily require anonymization mechanisms like differential privacy (DP), and provide limited protections against a potentially malicious service provider.","Adding DP to a basic FLA system currently requires either adding excessive noise to each device's updates, or assuming an honest service provider that correctly implements the mechanism and only uses the privatized outputs.","Secure multiparty computation (SMPC) -based oblivious aggregations can limit the service provider's access to individual user updates and improve DP tradeoffs, but the tradeoffs are still suboptimal, and they suffer from scalability challenges and susceptibility to Sybil attacks.","This paper introduces a novel system architecture that leverages trusted execution environments (TEEs) and open-sourcing to both ensure confidentiality of server-side computations and provide externally verifiable privacy properties, bolstering the robustness and trustworthiness of private federated computations."],"url":"http://arxiv.org/abs/2404.10764v1","category":"cs.CR"}
{"created":"2024-04-16 17:41:17","title":"TorchSurv: A Lightweight Package for Deep Survival Analysis","abstract":"TorchSurv is a Python package that serves as a companion tool to perform deep survival modeling within the PyTorch environment. Unlike existing libraries that impose specific parametric forms, TorchSurv enables the use of custom PyTorch-based deep survival mod- els. With its lightweight design, minimal input requirements, full PyTorch backend, and freedom from restrictive survival model parameterizations, TorchSurv facilitates efficient deep survival model implementation and is particularly beneficial for high-dimensional and complex input data scenarios","sentences":["TorchSurv is a Python package that serves as a companion tool to perform deep survival modeling within the PyTorch environment.","Unlike existing libraries that impose specific parametric forms, TorchSurv enables the use of custom PyTorch-based deep survival mod- els.","With its lightweight design, minimal input requirements, full PyTorch backend, and freedom from restrictive survival model parameterizations, TorchSurv facilitates efficient deep survival model implementation and is particularly beneficial for high-dimensional and complex input data scenarios"],"url":"http://arxiv.org/abs/2404.10761v1","category":"cs.LG"}
{"created":"2024-04-16 17:34:39","title":"A High-Order Conservative Cut Finite Element Method for Problems in Time-Dependent Domains","abstract":"A mass-conservative high-order unfitted finite element method for convection-diffusion equations in evolving domains is proposed. The space-time method presented in [P. Hansbo, M. G. Larson, S. Zahedi, Comput. Methods Appl. Mech. Engrg. 307 (2016)] is extended to naturally achieve mass conservation by utilizing Reynold's transport theorem. Furthermore, by partitioning the time-dependent domain into macroelements, a more efficient stabilization procedure for the cut finite element method in time-dependent domains is presented. Numerical experiments illustrate that the method fulfills mass conservation, attains high-order convergence, and the condition number of the resulting system matrix is controlled while sparsity is increased. Problems in bulk domains as well as coupled bulk-surface problems are considered.","sentences":["A mass-conservative high-order unfitted finite element method for convection-diffusion equations in evolving domains is proposed.","The space-time method presented in [P. Hansbo, M. G. Larson, S. Zahedi, Comput.","Methods Appl.","Mech.","Engrg.","307 (2016)] is extended to naturally achieve mass conservation by utilizing Reynold's transport theorem.","Furthermore, by partitioning the time-dependent domain into macroelements, a more efficient stabilization procedure for the cut finite element method in time-dependent domains is presented.","Numerical experiments illustrate that the method fulfills mass conservation, attains high-order convergence, and the condition number of the resulting system matrix is controlled while sparsity is increased.","Problems in bulk domains as well as coupled bulk-surface problems are considered."],"url":"http://arxiv.org/abs/2404.10756v1","category":"math.NA"}
{"created":"2024-04-16 17:34:24","title":"A Systematic Survey of the Gemini Principles for Digital Twin Ontologies","abstract":"Ontologies are widely used for achieving interoperable Digital Twins (DTws), yet competing DTw definitions compound interoperability issues. Semantically linking these differing twins is feasible through ontologies and Cognitive Digital Twins (CDTws). However, it is often unclear how ontology use bolsters broader DTw advancements. This article presents a systematic survey following the PRISMA method, to explore the potential of ontologies to support DTws to meet the Centre for Digital Built Britain's Gemini Principles and aims to link progress in ontologies to this framework. The Gemini Principles focus on common DTw requirements, considering: Purpose for 1) Public Good, 2) Value Creation, and 3) Insight; Trustworthiness with sufficient 4) Security, 5) Openness, and 6) Quality; and appropriate Functionality of 7) Federation, 8) Curation, and 9) Evolution. This systematic literature review examines the role of ontologies in facilitating each principle. Existing research uses ontologies to solve DTw challenges within these principles, particularly by connecting DTws, optimising decisionmaking, and reasoning governance policies. Furthermore, analysing the sectoral distribution of literature found that research encompassing the crossover of ontologies, DTws and the Gemini Principles is emerging, and that most innovation is predominantly within manufacturing and built environment sectors. Critical gaps for researchers, industry practitioners, and policymakers are subsequently identified.","sentences":["Ontologies are widely used for achieving interoperable Digital Twins (DTws), yet competing DTw definitions compound interoperability issues.","Semantically linking these differing twins is feasible through ontologies and Cognitive Digital Twins (CDTws).","However, it is often unclear how ontology use bolsters broader DTw advancements.","This article presents a systematic survey following the PRISMA method, to explore the potential of ontologies to support DTws to meet the Centre for Digital Built Britain's Gemini Principles and aims to link progress in ontologies to this framework.","The Gemini Principles focus on common DTw requirements, considering: Purpose for 1) Public Good, 2) Value Creation, and 3) Insight; Trustworthiness with sufficient 4) Security, 5) Openness, and 6) Quality; and appropriate Functionality of 7) Federation, 8) Curation, and 9) Evolution.","This systematic literature review examines the role of ontologies in facilitating each principle.","Existing research uses ontologies to solve DTw challenges within these principles, particularly by connecting DTws, optimising decisionmaking, and reasoning governance policies.","Furthermore, analysing the sectoral distribution of literature found that research encompassing the crossover of ontologies, DTws and the Gemini Principles is emerging, and that most innovation is predominantly within manufacturing and built environment sectors.","Critical gaps for researchers, industry practitioners, and policymakers are subsequently identified."],"url":"http://arxiv.org/abs/2404.10754v1","category":"cs.ET"}
{"created":"2024-04-16 17:32:27","title":"EPOCHS III: Unbiased UV continuum slopes at 6.5<z<13 from combined PEARLS GTO and public JWST NIRCam imaging","abstract":"We present an analysis of rest-frame UV continuum slopes, $\\beta$, using a sample of 1011 galaxies at $6.5<z<13$ from the EPOCHS photometric sample collated from the GTO PEARLS and public ERS/GTO/GO (JADES, CEERS, NGDEEP, GLASS) JWST NIRCam imaging across $178.9~\\mathrm{arcmin}^2$ of unmasked blank sky. We correct our UV slopes for the photometric error coupling bias using $200,000$ power law SEDs for each $\\beta=\\{-1,-1.5,-2,-2.5,-3\\}$ in each field, finding biases as large as $\\Delta\\beta\\simeq-0.55$ for the lowest SNR galaxies in our sample. Additionally, we simulate the impact of rest-UV line emission (including Ly$\\alpha$) and damped Ly$\\alpha$ systems on our measured $\\beta$, finding biases as large as $0.5-0.6$ for the most extreme systems. We find a decreasing trend with redshift of $\\beta=-1.51\\pm0.08-(0.097\\pm0.010)\\times z$, with potential evidence for Pop.~III stars or top-heavy initial mass functions (IMFs) in a subsample of 68 $\\beta+\\sigma_{\\beta}<-2.8$ galaxies. At $z\\simeq11.5$, we measure an extremely blue $\\beta(M_{\\mathrm{UV}}=-19)=-2.73\\pm0.06$, deviating from simulations, indicative of low-metallicity galaxies with non-zero Lyman continuum escape fractions $f_{\\mathrm{esc, LyC}}\\gtrsim0$ and minimal dust content. The observed steepening of $\\mathrm{d}\\beta/\\mathrm{d}\\log_{10}(M_{\\star}/\\mathrm{M}_{\\odot})$ from $0.22\\pm0.02$ at $z=7$ to $0.81\\pm0.13$ at $z=11.5$ implies that dust produced in core-collapse supernovae (SNe) at early times may be ejected via outflows from low mass galaxies. We also observe a flatter $\\mathrm{d}\\beta/\\mathrm{d}M_{\\mathrm{UV}}=0.03\\pm0.02$ at $z=7$ and a shallower $\\mathrm{d}\\beta/\\mathrm{d}\\log_{10}(M_{\\star} / \\mathrm{M}_{\\odot})$ at $z<11$ than seen by HST, unveiling a new population of low mass, faint, galaxies reddened by dust produced in the stellar winds of asymptotic giant branch (AGB) stars or carbon-rich Wolf-Rayet binaries.","sentences":["We present an analysis of rest-frame UV continuum slopes, $\\beta$, using a sample of 1011 galaxies at $6.5<z<13$ from the EPOCHS photometric sample collated from the GTO PEARLS and public ERS/GTO/GO (JADES, CEERS, NGDEEP, GLASS) JWST NIRCam imaging across $178.9~\\mathrm{arcmin}^2$ of unmasked blank sky.","We correct our UV slopes for the photometric error coupling bias using $200,000$ power law SEDs for each $\\beta=\\{-1,-1.5,-2,-2.5,-3\\}$ in each field, finding biases as large as $\\Delta\\beta\\simeq-0.55$ for the lowest SNR galaxies in our sample.","Additionally, we simulate the impact of rest-UV line emission (including Ly$\\alpha$) and damped Ly$\\alpha$ systems on our measured $\\beta$, finding biases as large as $0.5-0.6$ for the most extreme systems.","We find a decreasing trend with redshift of $\\beta=-1.51\\pm0.08-(0.097\\pm0.010)\\times z$, with potential evidence for Pop.~III stars or top-heavy initial mass functions (IMFs) in a subsample of 68 $\\beta+\\sigma_{\\beta}<-2.8$ galaxies.","At $z\\simeq11.5$, we measure an extremely blue $\\beta(M_{\\mathrm{UV}}=-19)=-2.73\\pm0.06$, deviating from simulations, indicative of low-metallicity galaxies with non-zero Lyman continuum escape fractions $f_{\\mathrm{esc, LyC}}\\gtrsim0$ and minimal dust content.","The observed steepening of $\\mathrm{d}\\beta/\\mathrm{d}\\log_{10}(M_{\\star}/\\mathrm{M}_{\\odot})$ from $0.22\\pm0.02$ at $z=7$ to $0.81\\pm0.13$ at $z=11.5$ implies that dust produced in core-collapse supernovae (SNe) at early times may be ejected via outflows from low mass galaxies.","We also observe a flatter $\\mathrm{d}\\beta/\\mathrm{d}M_{\\mathrm{UV}}=0.03\\pm0.02$ at $z=7$ and a shallower $\\mathrm{d}\\beta/\\mathrm{d}\\log_{10}(M_{\\star} / \\mathrm{M}_{\\odot})$ at $z<11$ than seen by HST, unveiling a new population of low mass, faint, galaxies reddened by dust produced in the stellar winds of asymptotic giant branch (AGB) stars or carbon-rich Wolf-Rayet binaries."],"url":"http://arxiv.org/abs/2404.10751v1","category":"astro-ph.GA"}
{"created":"2024-04-16 17:24:55","title":"How Deduction Systems Can Help You To Verify Stability Properties","abstract":"Mathematical proofs are a cornerstone of control theory, and it is important to get them right. Deduction systems can help with this by mechanically checking the proofs. However, the structure and level of detail at which a proof is represented in a deduction system differ significantly from a proof read and written by mathematicians and engineers, hampering understanding and adoption of these systems.   This paper aims at helping to bridge the gap between machine-checked proofs and proofs in engineering and mathematics by presenting a machine-checked proof for stability using Lyapunov's theorem in a human-readable way. The structure of the proof is analyzed in detail, and potential benefits of such a proof are discussed, such as generalizability, reusability and increased trust in correctness.","sentences":["Mathematical proofs are a cornerstone of control theory, and it is important to get them right.","Deduction systems can help with this by mechanically checking the proofs.","However, the structure and level of detail at which a proof is represented in a deduction system differ significantly from a proof read and written by mathematicians and engineers, hampering understanding and adoption of these systems.   ","This paper aims at helping to bridge the gap between machine-checked proofs and proofs in engineering and mathematics by presenting a machine-checked proof for stability using Lyapunov's theorem in a human-readable way.","The structure of the proof is analyzed in detail, and potential benefits of such a proof are discussed, such as generalizability, reusability and increased trust in correctness."],"url":"http://arxiv.org/abs/2404.10747v1","category":"eess.SY"}
{"created":"2024-04-16 17:24:22","title":"Interpolation and differentiation of alchemical degrees of freedom in machine learning interatomic potentials","abstract":"Machine learning interatomic potentials (MLIPs) have become a workhorse of modern atomistic simulations, and recently published universal MLIPs, pre-trained on large datasets, have demonstrated remarkable accuracy and generalizability. However, the computational cost of MLIPs limits their applicability to chemically disordered systems requiring large simulation cells or to sample-intensive statistical methods. Here, we report the use of continuous and differentiable alchemical degrees of freedom in atomistic materials simulations, exploiting the fact that graph neural network MLIPs represent discrete elements as real-valued tensors. The proposed method introduces alchemical atoms with corresponding weights into the input graph, alongside modifications to the message-passing and readout mechanisms of MLIPs, and allows smooth interpolation between the compositional states of materials. The end-to-end differentiability of MLIPs enables efficient calculation of the gradient of energy with respect to the compositional weights. Leveraging these gradients, we propose methodologies for optimizing the composition of solid solutions towards target macroscopic properties and conducting alchemical free energy simulations to quantify the free energy of vacancy formation and composition changes. The approach offers an avenue for extending the capabilities of universal MLIPs in the modeling of compositional disorder and characterizing the phase stabilities of complex materials systems.","sentences":["Machine learning interatomic potentials (MLIPs) have become a workhorse of modern atomistic simulations, and recently published universal MLIPs, pre-trained on large datasets, have demonstrated remarkable accuracy and generalizability.","However, the computational cost of MLIPs limits their applicability to chemically disordered systems requiring large simulation cells or to sample-intensive statistical methods.","Here, we report the use of continuous and differentiable alchemical degrees of freedom in atomistic materials simulations, exploiting the fact that graph neural network MLIPs represent discrete elements as real-valued tensors.","The proposed method introduces alchemical atoms with corresponding weights into the input graph, alongside modifications to the message-passing and readout mechanisms of MLIPs, and allows smooth interpolation between the compositional states of materials.","The end-to-end differentiability of MLIPs enables efficient calculation of the gradient of energy with respect to the compositional weights.","Leveraging these gradients, we propose methodologies for optimizing the composition of solid solutions towards target macroscopic properties and conducting alchemical free energy simulations to quantify the free energy of vacancy formation and composition changes.","The approach offers an avenue for extending the capabilities of universal MLIPs in the modeling of compositional disorder and characterizing the phase stabilities of complex materials systems."],"url":"http://arxiv.org/abs/2404.10746v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 17:19:50","title":"Polynomial interacting particle systems and non-linear SPDEs","abstract":"Motivated by the robustness of the capital distribution curves, we study the behavior of a certain polynomial equity market model as the number of companies goes to infinity. More precisely, we extend volatility-stabilized market models introduced by Fernholz et al. by allowing for a common noise term such that the models remain polynomial. As the number of companies approaches infinity, we show that the limit of the empirical measure of the $N$-company system converges to the unique solution of a degenerate, non-linear SPDE. The obtained limit also has a representation as the conditional probability of the solution to a certain McKean-Vlasov SDE. Together with its conditional, this is again a polynomial process for which we can prove pathwise uniqueness as well as regularity properties for the marginal densities. We also provide conditional propagation of chaos results and numerical implementations of the particle system as well as its limiting equations.","sentences":["Motivated by the robustness of the capital distribution curves, we study the behavior of a certain polynomial equity market model as the number of companies goes to infinity.","More precisely, we extend volatility-stabilized market models introduced by Fernholz et al. by allowing for a common noise term such that the models remain polynomial.","As the number of companies approaches infinity, we show that the limit of the empirical measure of the $N$-company system converges to the unique solution of a degenerate, non-linear SPDE.","The obtained limit also has a representation as the conditional probability of the solution to a certain McKean-Vlasov SDE.","Together with its conditional, this is again a polynomial process for which we can prove pathwise uniqueness as well as regularity properties for the marginal densities.","We also provide conditional propagation of chaos results and numerical implementations of the particle system as well as its limiting equations."],"url":"http://arxiv.org/abs/2404.10744v1","category":"math.PR"}
{"created":"2024-04-16 17:19:49","title":"An angle rounding parameter initialization technique for ma-QAOA","abstract":"The multi-angle quantum approximate optimization algorithm (ma-QAOA) is a recently introduced algorithm that gives at least the same approximation ratio as the quantum approximate optimization algorithm (QAOA) and, in most cases, gives a significantly higher approximation ratio than QAOA. One drawback to ma-QAOA is that it uses significantly more classical parameters than QAOA, so the classical optimization component more complex. In this paper, we motivate a new parameter initialization strategy in which angles are initially randomly set to multiples of $\\pi/4$ between $-2\\pi$ and $2\\pi$ and this vector is used to seed one round of BFGS. We find that the parameter initialization strategy on four-vertex and eight-vertex data sets gives average approximation ratios of 0.931 and 0.894, respectively. This is comparable to the average approximation ratios of ma-QAOA where optimal parameters are found using BFGS with 1 random starting seed, which are 0.910 and 0.901 for the four-vertex and eight-vertex data sets.","sentences":["The multi-angle quantum approximate optimization algorithm (ma-QAOA) is a recently introduced algorithm that gives at least the same approximation ratio as the quantum approximate optimization algorithm (QAOA) and, in most cases, gives a significantly higher approximation ratio than QAOA.","One drawback to ma-QAOA is that it uses significantly more classical parameters than QAOA, so the classical optimization component more complex.","In this paper, we motivate a new parameter initialization strategy in which angles are initially randomly set to multiples of $\\pi/4$ between $-2\\pi$ and $2\\pi$ and this vector is used to seed one round of BFGS.","We find that the parameter initialization strategy on four-vertex and eight-vertex data sets gives average approximation ratios of 0.931 and 0.894, respectively.","This is comparable to the average approximation ratios of ma-QAOA where optimal parameters are found using BFGS with 1 random starting seed, which are 0.910 and 0.901 for the four-vertex and eight-vertex data sets."],"url":"http://arxiv.org/abs/2404.10743v1","category":"quant-ph"}
{"created":"2024-04-16 17:08:35","title":"An equivariant BGG correspondence and perfect complexes for extensions by $\\mathbb{Z}/2\\times \\mathbb{Z}/2$","abstract":"We provide an equivariant extension of Carlsson's BGG correspondence in characteristic two. As an application we classify perfect cochain complexes of $(\\mathbb{Z}/2\\times \\mathbb{Z}/2)\\rtimes Q$-representations with four-dimensional total homology for finite groups $Q$ of odd order. We deduce that cochain complexes of finite, free $A_4$-CW complexes with four-dimensional total homology are rigid: They are determined by the degrees of the nonzero homology groups.","sentences":["We provide an equivariant extension of Carlsson's BGG correspondence in characteristic two.","As an application we classify perfect cochain complexes of $(\\mathbb{Z}/2\\times \\mathbb{Z}/2)\\rtimes Q$-representations with four-dimensional total homology for finite groups $Q$ of odd order.","We deduce that cochain complexes of finite, free $A_4$-CW complexes with four-dimensional total homology are rigid: They are determined by the degrees of the nonzero homology groups."],"url":"http://arxiv.org/abs/2404.10735v1","category":"math.AC"}
{"created":"2024-04-16 17:06:40","title":"SPONGE: Open-Source Designs of Modular Articulated Soft Robots","abstract":"Soft-robot designs are manifold, but only a few are publicly available. Often, these are only briefly described in their publications. This complicates reproduction, and hinders the reproducibility and comparability of research results. If the designs were uniform and open source, validating researched methods on real benchmark systems would be possible. To address this, we present two variants of a soft pneumatic robot with antagonistic bellows as open source. Starting from a semi-modular design with multiple cables and tubes routed through the robot body, the transition to a fully modular robot with integrated microvalves and serial communication is highlighted. Modularity in terms of stackability, actuation, and communication is achieved, which is the crucial requirement for building soft robots with many degrees of freedom and high dexterity for real-world tasks. Both systems are compared regarding their respective advantages and disadvantages. The robots' functionality is demonstrated in experiments on airtightness, gravitational influence, position control with mean tracking errors of <3 deg, and long-term operation of cast and printed bellows. All soft- and hardware files required for reproduction are provided.","sentences":["Soft-robot designs are manifold, but only a few are publicly available.","Often, these are only briefly described in their publications.","This complicates reproduction, and hinders the reproducibility and comparability of research results.","If the designs were uniform and open source, validating researched methods on real benchmark systems would be possible.","To address this, we present two variants of a soft pneumatic robot with antagonistic bellows as open source.","Starting from a semi-modular design with multiple cables and tubes routed through the robot body, the transition to a fully modular robot with integrated microvalves and serial communication is highlighted.","Modularity in terms of stackability, actuation, and communication is achieved, which is the crucial requirement for building soft robots with many degrees of freedom and high dexterity for real-world tasks.","Both systems are compared regarding their respective advantages and disadvantages.","The robots' functionality is demonstrated in experiments on airtightness, gravitational influence, position control with mean tracking errors of <3 deg, and long-term operation of cast and printed bellows.","All soft- and hardware files required for reproduction are provided."],"url":"http://arxiv.org/abs/2404.10734v1","category":"cs.RO"}
{"created":"2024-04-16 17:02:05","title":"Minimal cellular resolutions of powers of matching field ideals","abstract":"We study a family of monomial ideals, called block diagonal matching field ideals, which arise as monomial Gr\\\"obner degenerations of determinantal ideals. Our focus is on the minimal free resolutions of these ideals and all of their powers. Initially, we establish their linear quotient property and compute their Betti numbers, illustrating that their minimal free resolution is supported on a regular CW complex. Our proof relies on the results of Herzog and Takayama, demonstrating that ideals with a linear quotient property have a minimal free resolution, and on the construction by Dochtermann and Mohammadi of cellular realizations of these resolutions. We begin by proving the linear quotient property for each power of such an ideal. Subsequently, we show that their corresponding decomposition map is regular, resulting in a minimal cellular resolution. Finally, we demonstrate that distinct decomposition maps lead to different cellular complexes with the same face numbers.","sentences":["We study a family of monomial ideals, called block diagonal matching field ideals, which arise as monomial Gr\\\"obner degenerations of determinantal ideals.","Our focus is on the minimal free resolutions of these ideals and all of their powers.","Initially, we establish their linear quotient property and compute their Betti numbers, illustrating that their minimal free resolution is supported on a regular CW complex.","Our proof relies on the results of Herzog and Takayama, demonstrating that ideals with a linear quotient property have a minimal free resolution, and on the construction by Dochtermann and Mohammadi of cellular realizations of these resolutions.","We begin by proving the linear quotient property for each power of such an ideal.","Subsequently, we show that their corresponding decomposition map is regular, resulting in a minimal cellular resolution.","Finally, we demonstrate that distinct decomposition maps lead to different cellular complexes with the same face numbers."],"url":"http://arxiv.org/abs/2404.10729v1","category":"math.AC"}
{"created":"2024-04-16 17:01:38","title":"Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning","abstract":"We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (\\textit{i.e.,} $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.","sentences":["We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL).","We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy respectively, which are flexible in design and easy to implement in practice.","For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication complexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes.","This is the first theoretical result for randomized exploration in cooperative MARL.","We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (\\textit{i.e.,} $N$-chain), a video game, and a real-world problem in energy systems.","Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models.","Additionally, we establish a connection between our unified framework and the practical application of federated learning."],"url":"http://arxiv.org/abs/2404.10728v1","category":"cs.LG"}
{"created":"2024-04-16 16:59:50","title":"Automatic re-calibration of quantum devices by reinforcement learning","abstract":"During their operation, due to shifts in environmental conditions, devices undergo various forms of detuning from their optimal settings. Typically, this is addressed through control loops, which monitor variables and the device performance, to maintain settings at their optimal values. Quantum devices are particularly challenging since their functionality relies on precisely tuning their parameters. At the same time, the detailed modeling of the environmental behavior is often computationally unaffordable, while a direct measure of the parameters defining the system state is costly and introduces extra noise in the mechanism. In this study, we investigate the application of reinforcement learning techniques to develop a model-free control loop for continuous recalibration of quantum device parameters. Furthermore, we explore the advantages of incorporating minimal environmental noise models. As an example, the application to numerical simulations of a Kennedy receiver-based long-distance quantum communication protocol is presented.","sentences":["During their operation, due to shifts in environmental conditions, devices undergo various forms of detuning from their optimal settings.","Typically, this is addressed through control loops, which monitor variables and the device performance, to maintain settings at their optimal values.","Quantum devices are particularly challenging since their functionality relies on precisely tuning their parameters.","At the same time, the detailed modeling of the environmental behavior is often computationally unaffordable, while a direct measure of the parameters defining the system state is costly and introduces extra noise in the mechanism.","In this study, we investigate the application of reinforcement learning techniques to develop a model-free control loop for continuous recalibration of quantum device parameters.","Furthermore, we explore the advantages of incorporating minimal environmental noise models.","As an example, the application to numerical simulations of a Kennedy receiver-based long-distance quantum communication protocol is presented."],"url":"http://arxiv.org/abs/2404.10726v1","category":"quant-ph"}
{"created":"2024-04-16 16:40:37","title":"Tetris with Few Piece Types","abstract":"We prove NP-hardness and #P-hardness of Tetris clearing (clearing an initial board using a given sequence of pieces) with the Super Rotation System (SRS), even when the pieces are limited to any two of the seven Tetris piece types. This result is the first advance on a question posed twenty years ago: which piece sets are easy vs. hard? All previous Tetris NP-hardness proofs used five of the seven piece types. We also prove ASP-completeness of Tetris clearing, using three piece types, as well as versions of 3-Partition and Numerical 3-Dimensional Matching where all input integers are distinct. Finally, we prove NP-hardness of Tetris survival and clearing under the \"hard drops only\" and \"20G\" modes, using two piece types, improving on a previous \"hard drops only\" result that used five piece types.","sentences":["We prove NP-hardness and #P-hardness of Tetris clearing (clearing an initial board using a given sequence of pieces) with the Super Rotation System (SRS), even when the pieces are limited to any two of the seven Tetris piece types.","This result is the first advance on a question posed twenty years ago: which piece sets are easy vs. hard?","All previous Tetris NP-hardness proofs used five of the seven piece types.","We also prove ASP-completeness of Tetris clearing, using three piece types, as well as versions of 3-Partition and Numerical 3-Dimensional Matching where all input integers are distinct.","Finally, we prove NP-hardness of Tetris survival and clearing under the \"hard drops only\" and \"20G\" modes, using two piece types, improving on a previous \"hard drops only\" result that used five piece types."],"url":"http://arxiv.org/abs/2404.10712v1","category":"cs.CC"}
{"created":"2024-04-16 16:36:51","title":"Photometric and Spectroscopic Analysis of V583 Lyrae, an Algol with a g-mode Pulsating Primary and Accretion Disk","abstract":"V583 Lyr is an extremely low mass ratio Algol-type binary with an orbital period of 11.2580 days. We determined an effective temperature of T_{eff1} = 9000 \\pm 350 K from newly observed spectra, which might be an underestimate due to binary mass transfer. The binary mass ratio q = 0.1 \\pm 0.004 and the orbital inclination i = 85.5{\\deg} are determined based on the assumption that the secondary fills its Roche lobe and rotates synchronously. The radial velocity curve is obtained from time series spectra, allowing for improved estimation of stellar masses and radii: M1 = 3.56 \\pm 0.5 Msun, R1 = 2.4 \\pm 0.2 Rsun; and M2 = 0.36 \\pm 0.02 Msun, R2= 6.9 \\pm 0.4 Rsun. The variations in the double-peaked H_{\\alpha} emission indicate the formation of a stable disk during mass transfer. V583 Lyr appears to be a post-mass-reversal system, according to the estimated mass transfer using O-C period analysis. Its orbital period is slowly increasing, from which the rate of mass accretion by the primary star is estimated to be dM1/dt = 3.384 \\times10^{-8} Msun/yr. The pulsation analysis was conducted on the residuals of the light curve. The primary component was found to be a g-mode pulsating star with 26 frequencies extracted lower than 9 d^{-1}. The frequency groups and rotational splitting properties of the g-mode were studied in detail. This study provides compelling evidence for an accretion disk surrounding the g-mode pulsating primary.","sentences":["V583 Lyr is an extremely low mass ratio Algol-type binary with an orbital period of 11.2580 days.","We determined an effective temperature of T_{eff1} = 9000 \\pm 350 K from newly observed spectra, which might be an underestimate due to binary mass transfer.","The binary mass ratio q = 0.1 \\pm 0.004 and the orbital inclination i = 85.5{\\deg} are determined based on the assumption that the secondary fills its Roche lobe and rotates synchronously.","The radial velocity curve is obtained from time series spectra, allowing for improved estimation of stellar masses and radii: M1 = 3.56 \\pm 0.5 Msun, R1 = 2.4 \\pm 0.2 Rsun; and M2 = 0.36 \\pm 0.02 Msun, R2= 6.9 \\pm 0.4 Rsun.","The variations in the double-peaked H_{\\alpha} emission indicate the formation of a stable disk during mass transfer.","V583 Lyr appears to be a post-mass-reversal system, according to the estimated mass transfer using O-C period analysis.","Its orbital period is slowly increasing, from which the rate of mass accretion by the primary star is estimated to be dM1/dt = 3.384 \\times10^{-8} Msun/yr.","The pulsation analysis was conducted on the residuals of the light curve.","The primary component was found to be a g-mode pulsating star with 26 frequencies extracted lower than 9 d^{-1}.","The frequency groups and rotational splitting properties of the g-mode were studied in detail.","This study provides compelling evidence for an accretion disk surrounding the g-mode pulsating primary."],"url":"http://arxiv.org/abs/2404.10711v1","category":"astro-ph.SR"}
{"created":"2024-04-16 16:16:00","title":"Drift estimation in stochastic flows using kernel integral operators","abstract":"A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise. The resulting motion can only be described as a random process instead of a solution curve. Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow. This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods. It presents a numerical procedure based on kernel integral operators, that computes this expectation. In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity.","sentences":["A stochastic differential equation (SDE) describes a motion in which a particle is governed simultaneously by the direction provided by a vector field / drift, and the scattering effects of white noise.","The resulting motion can only be described as a random process instead of a solution curve.","Due to the non-deterministic nature of this motion, the task of determining the drift from data is quite challenging, since the data does not directly represent the directional information of the flow.","This paper describes an interpretation of vector field as a conditional expectation, which makes its estimation feasible via kernel-integral methods.","It presents a numerical procedure based on kernel integral operators, that computes this expectation.","In addition, some techniques are presented which can overcome the challenge of dimensionality if the SDE's carry some structure enabling sparsity."],"url":"http://arxiv.org/abs/2404.10698v1","category":"math.DS"}
{"created":"2024-04-16 16:09:38","title":"Network architecture search of X-ray based scientific applications","abstract":"X-ray and electron diffraction-based microscopy use bragg peak detection and ptychography to perform 3-D imaging at an atomic resolution. Typically, these techniques are implemented using computationally complex tasks such as a Psuedo-Voigt function or solving a complex inverse problem. Recently, the use of deep neural networks has improved the existing state-of-the-art approaches. However, the design and development of the neural network models depends on time and labor intensive tuning of the model by application experts. To that end, we propose a hyperparameter (HPS) and neural architecture search (NAS) approach to automate the design and optimization of the neural network models for model size, energy consumption and throughput. We demonstrate the improved performance of the auto-tuned models when compared to the manually tuned BraggNN and PtychoNN benchmark. We study and demonstrate the importance of the exploring the search space of tunable hyperparameters in enhancing the performance of bragg peak detection and ptychographic reconstruction. Our NAS and HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection accuracy with a 87.57\\% reduction in model size, and (2) PtychoNN achieves a 16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size when compared to the baseline PtychoNN model. When inferred on the Orin-AGX platform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and 9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in energy consumption when compared to their respective baselines, when inferred in the Orin-AGX edge platform.","sentences":["X-ray and electron diffraction-based microscopy use bragg peak detection and ptychography to perform 3-D imaging at an atomic resolution.","Typically, these techniques are implemented using computationally complex tasks such as a Psuedo-Voigt function or solving a complex inverse problem.","Recently, the use of deep neural networks has improved the existing state-of-the-art approaches.","However, the design and development of the neural network models depends on time and labor intensive tuning of the model by application experts.","To that end, we propose a hyperparameter (HPS) and neural architecture search (NAS) approach to automate the design and optimization of the neural network models for model size, energy consumption and throughput.","We demonstrate the improved performance of the auto-tuned models when compared to the manually tuned BraggNN and PtychoNN benchmark.","We study and demonstrate the importance of the exploring the search space of tunable hyperparameters in enhancing the performance of bragg peak detection and ptychographic reconstruction.","Our NAS and HPS of (1) BraggNN achieves a 31.03\\% improvement in bragg peak detection accuracy with a 87.57\\% reduction in model size, and (2) PtychoNN","achieves a 16.77\\% improvement in model accuracy and a 12.82\\% reduction in model size when compared to the baseline PtychoNN model.","When inferred on the Orin-AGX platform, the optimized Braggnn and Ptychonn models demonstrate a 10.51\\% and 9.47\\% reduction in inference latency and a 44.18\\% and 15.34\\% reduction in energy consumption when compared to their respective baselines, when inferred in the Orin-AGX edge platform."],"url":"http://arxiv.org/abs/2404.10689v1","category":"cs.LG"}
{"created":"2024-04-16 16:07:16","title":"Invariant Kalman Filtering with Noise-Free Pseudo-Measurements","abstract":"In this paper, we focus on developing an Invariant Extended Kalman Filter (IEKF) for extended pose estimation for a noisy system with state equality constraints. We treat those constraints as noise-free pseudo-measurements. To this aim, we provide a formula for the Kalman gain in the limit of noise-free measurements and rank-deficient covariance matrix. We relate the constraints to group-theoretic properties and study the behavior of the IEKF in the presence of such noise-free measurements. We illustrate this perspective on the estimation of the motion of the load of an overhead crane, when a wireless inertial measurement unit is mounted on the hook.","sentences":["In this paper, we focus on developing an Invariant Extended Kalman Filter (IEKF) for extended pose estimation for a noisy system with state equality constraints.","We treat those constraints as noise-free pseudo-measurements.","To this aim, we provide a formula for the Kalman gain in the limit of noise-free measurements and rank-deficient covariance matrix.","We relate the constraints to group-theoretic properties and study the behavior of the IEKF in the presence of such noise-free measurements.","We illustrate this perspective on the estimation of the motion of the load of an overhead crane, when a wireless inertial measurement unit is mounted on the hook."],"url":"http://arxiv.org/abs/2404.10687v1","category":"eess.SY"}
{"created":"2024-04-16 16:04:11","title":"Driver Fatigue Prediction using Randomly Activated Neural Networks for Smart Ridesharing Platforms","abstract":"Drivers in ridesharing platforms exhibit cognitive atrophy and fatigue as they accept ride offers along the day, which can have a significant impact on the overall efficiency of the ridesharing platform. In contrast to the current literature which focuses primarily on modeling and learning driver's preferences across different ride offers, this paper proposes a novel Dynamic Discounted Satisficing (DDS) heuristic to model and predict driver's sequential ride decisions during a given shift. Based on DDS heuristic, a novel stochastic neural network with random activations is proposed to model DDS heuristic and predict the final decision made by a given driver. The presence of random activations in the network necessitated the development of a novel training algorithm called Sampling-Based Back Propagation Through Time (SBPTT), where gradients are computed for independent instances of neural networks (obtained via sampling the distribution of activation threshold) and aggregated to update the network parameters. Using both simulation experiments as well as on real Chicago taxi dataset, this paper demonstrates the improved performance of the proposed approach, when compared to state-of-the-art methods.","sentences":["Drivers in ridesharing platforms exhibit cognitive atrophy and fatigue as they accept ride offers along the day, which can have a significant impact on the overall efficiency of the ridesharing platform.","In contrast to the current literature which focuses primarily on modeling and learning driver's preferences across different ride offers, this paper proposes a novel Dynamic Discounted Satisficing (DDS) heuristic to model and predict driver's sequential ride decisions during a given shift.","Based on DDS heuristic, a novel stochastic neural network with random activations is proposed to model DDS heuristic and predict the final decision made by a given driver.","The presence of random activations in the network necessitated the development of a novel training algorithm called Sampling-Based Back Propagation Through Time (SBPTT), where gradients are computed for independent instances of neural networks (obtained via sampling the distribution of activation threshold) and aggregated to update the network parameters.","Using both simulation experiments as well as on real Chicago taxi dataset, this paper demonstrates the improved performance of the proposed approach, when compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10684v1","category":"cs.LG"}
{"created":"2024-04-16 15:58:40","title":"Extending the Tavis-Cummings model for molecular ensembles -- Exploring the effects of dipole self energies and static dipole moments","abstract":"Strong coupling of organic molecules to the vacuum field of a nanoscale cavity can be used to modify their chemical and physical properties. We extend the Tavis-Cummings model for molecular ensembles and show that the often neglected interaction terms arising from the static dipole moment and the dipole self-energy are essential for a correct description of the light-matter interaction in polaritonic chemistry. On the basis of a full quantum description, we simulate the excited-state dynamics and spectroscopy of MgH$^+$ molecules resonantly coupled to an optical cavity. We show that the inclusion of static dipole moments and the dipole self-energy is necessary to obtain a consistent model. We construct an efficient two-level system approach that reproduces the main features of the real molecular system and may be used to simulate larger molecular ensembles.","sentences":["Strong coupling of organic molecules to the vacuum field of a nanoscale cavity can be used to modify their chemical and physical properties.","We extend the Tavis-Cummings model for molecular ensembles and show that the often neglected interaction terms arising from the static dipole moment and the dipole self-energy are essential for a correct description of the light-matter interaction in polaritonic chemistry.","On the basis of a full quantum description, we simulate the excited-state dynamics and spectroscopy of MgH$^+$ molecules resonantly coupled to an optical cavity.","We show that the inclusion of static dipole moments and the dipole self-energy is necessary to obtain a consistent model.","We construct an efficient two-level system approach that reproduces the main features of the real molecular system and may be used to simulate larger molecular ensembles."],"url":"http://arxiv.org/abs/2404.10680v1","category":"physics.chem-ph"}
{"created":"2024-04-16 15:52:22","title":"Circuit-theoretic Joint Parameter-State Estimation -- Balancing Optimality and AC Feasibility","abstract":"AC State Estimation (ACSE) is widely recognized as a practical approach for determining the grid states in steady-state conditions. It serves as a fundamental analysis to ensure grid security and is a reference for market dispatch. As grid complexity increases with rapid electrification and decarbonization, there is a growing need for more accurate knowledge of the grid operating state. However, existing ACSE algorithms have technical gaps. Critically, current ACSE algorithms are susceptible to erroneous system parameters, which are assumed to be fixed in traditional approaches. In this paper, we build a novel circuit-theoretic joint parameter-state estimation algorithm to address this limitation. The innovative algorithm builds an analogous equivalent circuit of the grid with states and certain parameters unknown. It solves a circuit-constrained optimization to estimate the most likely grid states and parameters given a set of measurements. Further, it quantifies the goodness of the estimated output by formulating tight convex envelopes around the original non-convex problem to quantify the quality of estimates. We compare the various proposed approaches on systems with up to 2869 nodes while demonstrating a tradeoff between solution optimality and model fidelity.","sentences":["AC State Estimation (ACSE) is widely recognized as a practical approach for determining the grid states in steady-state conditions.","It serves as a fundamental analysis to ensure grid security and is a reference for market dispatch.","As grid complexity increases with rapid electrification and decarbonization, there is a growing need for more accurate knowledge of the grid operating state.","However, existing ACSE algorithms have technical gaps.","Critically, current ACSE algorithms are susceptible to erroneous system parameters, which are assumed to be fixed in traditional approaches.","In this paper, we build a novel circuit-theoretic joint parameter-state estimation algorithm to address this limitation.","The innovative algorithm builds an analogous equivalent circuit of the grid with states and certain parameters unknown.","It solves a circuit-constrained optimization to estimate the most likely grid states and parameters given a set of measurements.","Further, it quantifies the goodness of the estimated output by formulating tight convex envelopes around the original non-convex problem to quantify the quality of estimates.","We compare the various proposed approaches on systems with up to 2869 nodes while demonstrating a tradeoff between solution optimality and model fidelity."],"url":"http://arxiv.org/abs/2404.10676v1","category":"eess.SY"}
{"created":"2024-04-16 15:44:25","title":"Strings in metric spaces","abstract":"We introduce strings in metric spaces and define string complexes of metric spaces. We describe the class of 2-dimensional topological spaces which arise in this way from finite metric spaces.","sentences":["We introduce strings in metric spaces and define string complexes of metric spaces.","We describe the class of 2-dimensional topological spaces which arise in this way from finite metric spaces."],"url":"http://arxiv.org/abs/2404.10668v1","category":"math.MG"}
{"created":"2024-04-16 15:42:20","title":"Iterated Invariant Extended Kalman Filter (IIEKF)","abstract":"In this paper, we introduce the Iterated Invariant Extended Kalman Filter (IIEKF), which is an invariant extended Kalman filter (IEKF) where the updated state in the light of the latest measurement is defined as a maximum a posteriori (MAP) estimate. Under some compatibility requirements on the output map, we prove strong mathematical guarantees which echo those of the Kalman filter in the linear case. We apply the technique to two problems: solving a system of equations on a Lie group, and a problem of engineering interest, namely ego-localization of the hook of a crane. The latter serves as a benchmarking example, where the IIEKF favorably compares to other filters.","sentences":["In this paper, we introduce the Iterated Invariant Extended Kalman Filter (IIEKF), which is an invariant extended Kalman filter (IEKF) where the updated state in the light of the latest measurement is defined as a maximum a posteriori (MAP) estimate.","Under some compatibility requirements on the output map, we prove strong mathematical guarantees which echo those of the Kalman filter in the linear case.","We apply the technique to two problems: solving a system of equations on a Lie group, and a problem of engineering interest, namely ego-localization of the hook of a crane.","The latter serves as a benchmarking example, where the IIEKF favorably compares to other filters."],"url":"http://arxiv.org/abs/2404.10665v1","category":"eess.SY"}
{"created":"2024-04-16 15:40:18","title":"Assessing The Impact of CNN Auto Encoder-Based Image Denoising on Image Classification Tasks","abstract":"Images captured from the real world are often affected by different types of noise, which can significantly impact the performance of Computer Vision systems and the quality of visual data. This study presents a novel approach for defect detection in casting product noisy images, specifically focusing on submersible pump impellers. The methodology involves utilizing deep learning models such as VGG16, InceptionV3, and other models in both the spatial and frequency domains to identify noise types and defect status. The research process begins with preprocessing images, followed by applying denoising techniques tailored to specific noise categories. The goal is to enhance the accuracy and robustness of defect detection by integrating noise detection and denoising into the classification pipeline. The study achieved remarkable results using VGG16 for noise type classification in the frequency domain, achieving an accuracy of over 99%. Removal of salt and pepper noise resulted in an average SSIM of 87.9, while Gaussian noise removal had an average SSIM of 64.0, and periodic noise removal yielded an average SSIM of 81.6. This comprehensive approach showcases the effectiveness of the deep AutoEncoder model and median filter, for denoising strategies in real-world industrial applications. Finally, our study reports significant improvements in binary classification accuracy for defect detection compared to previous methods. For the VGG16 classifier, accuracy increased from 94.6% to 97.0%, demonstrating the effectiveness of the proposed noise detection and denoising approach. Similarly, for the InceptionV3 classifier, accuracy improved from 84.7% to 90.0%, further validating the benefits of integrating noise analysis into the classification pipeline.","sentences":["Images captured from the real world are often affected by different types of noise, which can significantly impact the performance of Computer Vision systems and the quality of visual data.","This study presents a novel approach for defect detection in casting product noisy images, specifically focusing on submersible pump impellers.","The methodology involves utilizing deep learning models such as VGG16, InceptionV3, and other models in both the spatial and frequency domains to identify noise types and defect status.","The research process begins with preprocessing images, followed by applying denoising techniques tailored to specific noise categories.","The goal is to enhance the accuracy and robustness of defect detection by integrating noise detection and denoising into the classification pipeline.","The study achieved remarkable results using VGG16 for noise type classification in the frequency domain, achieving an accuracy of over 99%.","Removal of salt and pepper noise resulted in an average SSIM of 87.9, while Gaussian noise removal had an average SSIM of 64.0, and periodic noise removal yielded an average SSIM of 81.6.","This comprehensive approach showcases the effectiveness of the deep AutoEncoder model and median filter, for denoising strategies in real-world industrial applications.","Finally, our study reports significant improvements in binary classification accuracy for defect detection compared to previous methods.","For the VGG16 classifier, accuracy increased from 94.6% to 97.0%, demonstrating the effectiveness of the proposed noise detection and denoising approach.","Similarly, for the InceptionV3 classifier, accuracy improved from 84.7% to 90.0%, further validating the benefits of integrating noise analysis into the classification pipeline."],"url":"http://arxiv.org/abs/2404.10664v1","category":"cs.CV"}
{"created":"2024-04-16 15:36:23","title":"Cybersecurity in the Quantum Era: Assessing the Impact of Quantum Computing on Infrastructure","abstract":"The emergence of quantum computing presents a double-edged sword for cybersecurity. While its immense power holds promise for advancements in various fields, it also threatens to crack the foundation of current encryption methods. This analysis explores the impact of quantum computing on critical infrastructure and cloud services, meticulously evaluating potential vulnerabilities across various layers, including applications, data, runtime, middleware, operating systems, virtualization, hardware, storage, and networks. We advocate for proactive security strategies and collaboration between sectors to develop and implement quantum-resistant cryptography. This crucial shift necessitates a comprehensive approach, and the paper introduces a tailored security blueprint encompassing nine critical infrastructure components. This blueprint strengthens each area's defenses against potential quantum-induced cyber threats. Our strategic vulnerability and risk assessment equips stakeholders with the knowledge to navigate the complex quantum threat landscape. This empowers them to make informed decisions about design, implementation, and policy formulation, ultimately bolstering the resilience of critical infrastructure. In essence, this analysis not only forecasts quantum threats but also offers a sophisticated, actionable framework for fortifying infrastructure and cloud environments against the multifaceted challenges of the quantum era. This proactive approach will ensure continued data security and a thriving digital landscape in the years to come","sentences":["The emergence of quantum computing presents a double-edged sword for cybersecurity.","While its immense power holds promise for advancements in various fields, it also threatens to crack the foundation of current encryption methods.","This analysis explores the impact of quantum computing on critical infrastructure and cloud services, meticulously evaluating potential vulnerabilities across various layers, including applications, data, runtime, middleware, operating systems, virtualization, hardware, storage, and networks.","We advocate for proactive security strategies and collaboration between sectors to develop and implement quantum-resistant cryptography.","This crucial shift necessitates a comprehensive approach, and the paper introduces a tailored security blueprint encompassing nine critical infrastructure components.","This blueprint strengthens each area's defenses against potential quantum-induced cyber threats.","Our strategic vulnerability and risk assessment equips stakeholders with the knowledge to navigate the complex quantum threat landscape.","This empowers them to make informed decisions about design, implementation, and policy formulation, ultimately bolstering the resilience of critical infrastructure.","In essence, this analysis not only forecasts quantum threats but also offers a sophisticated, actionable framework for fortifying infrastructure and cloud environments against the multifaceted challenges of the quantum era.","This proactive approach will ensure continued data security and a thriving digital landscape in the years to come"],"url":"http://arxiv.org/abs/2404.10659v1","category":"cs.CR"}
{"created":"2024-04-16 15:31:25","title":"Quantum Simulation of Open Quantum Dynamics via Non-Markovian Quantum State Diffusion","abstract":"Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources. Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment. Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation. Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements. To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM). Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior.","sentences":["Quantum simulation of non-Markovian open quantum dynamics is essential but challenging for standard quantum computers due to their non-Hermitian nature, leading to non-unitary evolution, and the limitations of available quantum resources.","Here we introduce a hybrid quantum-classical algorithm designed for simulating dissipative dynamics in system with non-Markovian environment.","Our approach includes formulating a non-Markovian Stochastic Schr\\\"odinger equation with complex frequency modes (cNMSSE) where the non-Markovianity is characterized by the mode excitation.","Following this, we utilize variational quantum simulation to capture the non-unitary evolution within the cNMSSE framework, leading to a substantial reduction in qubit requirements.","To demonstrate our approach, we investigated the spin-boson model and dynamic quantum phase transitions (DQPT) within transverse field Ising model (TFIM).","Significantly, our findings reveal the enhanced DQPT in TFIM due to non-Markovian behavior."],"url":"http://arxiv.org/abs/2404.10655v1","category":"quant-ph"}
{"created":"2024-04-16 15:22:29","title":"Navigating the Serious Game Design Landscape: A Comprehensive Reference Document","abstract":"Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions. Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients. As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex. Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge. Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches? This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions. We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices. Through an examination of literature reviews, it was observed that selected design decisions varied across studies. Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable.","sentences":["Within the evolving field of digital intervention, serious games emerge as promising tools for evidence-based interventions.","Research indicates that gamified therapy, whether employed independently or in conjunction with online psychoeducation or traditional programs, proves more efficacious in delivering care to patients.","As we navigate the intricate realm of serious game design, bridging the gap between therapeutic approaches and creative design proves complex.","Professionals in clinical and research roles demonstrate innovative thinking yet face challenges in executing engaging therapeutic serious games due to the lack of specialized design skills and knowledge.","Thus, a larger question remains: How might we aid and educate professionals in clinical and research roles the importance of game design to support their innovative therapeutic approaches?","This study examines potential solutions aimed at facilitating the integration of gamification design principles into clinical study protocols, a pivotal aspect for aligning therapeutic practices with captivating narratives in the pursuit of innovative interventions.","We propose two solutions, a flow chart framework for serious games or a comprehensive reference document encompassing gamification design principles and guidelines for best design practices.","Through an examination of literature reviews, it was observed that selected design decisions varied across studies.","Thus, we propose that the second solution, a comprehensive reference design guide, is more versatile and adaptable."],"url":"http://arxiv.org/abs/2404.10649v1","category":"cs.HC"}
{"created":"2024-04-16 15:17:23","title":"A Calibrated and Automated Simulator for Innovations in 5G","abstract":"The rise of 5G deployments has created the environment for many emerging technologies to flourish. Self-driving vehicles, Augmented and Virtual Reality, and remote operations are examples of applications that leverage 5G networks' support for extremely low latency, high bandwidth, and increased throughput. However, the complex architecture of 5G hinders innovation due to the lack of accessibility to testbeds or realistic simulators with adequate 5G functionalities. Also, configuring and managing simulators are complex and time consuming. Finally, the lack of adequate representative data hinders the data-driven designs in 5G campaigns. Thus, we calibrated a system-level open-source simulator, Simu5G, following 3GPP guidelines to enable faster innovation in the 5G domain. Furthermore, we developed an API for automatic simulator configuration without knowing the underlying architectural details. Finally, we demonstrate the usage of the calibrated and automated simulator by developing an ML-based anomaly detection in a 5G Radio Access Network (RAN).","sentences":["The rise of 5G deployments has created the environment for many emerging technologies to flourish.","Self-driving vehicles, Augmented and Virtual Reality, and remote operations are examples of applications that leverage 5G networks' support for extremely low latency, high bandwidth, and increased throughput.","However, the complex architecture of 5G hinders innovation due to the lack of accessibility to testbeds or realistic simulators with adequate 5G functionalities.","Also, configuring and managing simulators are complex and time consuming.","Finally, the lack of adequate representative data hinders the data-driven designs in 5G campaigns.","Thus, we calibrated a system-level open-source simulator, Simu5G, following 3GPP guidelines to enable faster innovation in the 5G domain.","Furthermore, we developed an API for automatic simulator configuration without knowing the underlying architectural details.","Finally, we demonstrate the usage of the calibrated and automated simulator by developing an ML-based anomaly detection in a 5G Radio Access Network (RAN)."],"url":"http://arxiv.org/abs/2404.10643v1","category":"cs.NI"}
{"created":"2024-04-16 15:15:59","title":"A Cloud Resources Portfolio Optimization Business Model - From Theory to Practice","abstract":"Cloud resources have become increasingly important, with many businesses using cloud solutions to supplement or outright replace their existing IT infrastructure. However, as there is a plethora of providers with varying products, services, and markets, it has become increasingly more challenging to keep track of the best solutions for each application. Cloud service intermediaries aim to alleviate this problem by offering services that help users meet their requirements.   This paper aims to lay the groundwork for developing a cloud portfolio management platform and its business model, defined via a business model canvas. Furthermore, a prototype of a platform is developed offering a cloud portfolio optimization service, using two algorithms developed in previous research to create suitable and well-utilized allocations for a customer's applications.","sentences":["Cloud resources have become increasingly important, with many businesses using cloud solutions to supplement or outright replace their existing IT infrastructure.","However, as there is a plethora of providers with varying products, services, and markets, it has become increasingly more challenging to keep track of the best solutions for each application.","Cloud service intermediaries aim to alleviate this problem by offering services that help users meet their requirements.   ","This paper aims to lay the groundwork for developing a cloud portfolio management platform and its business model, defined via a business model canvas.","Furthermore, a prototype of a platform is developed offering a cloud portfolio optimization service, using two algorithms developed in previous research to create suitable and well-utilized allocations for a customer's applications."],"url":"http://arxiv.org/abs/2404.10641v1","category":"cs.DC"}
{"created":"2024-04-16 14:56:50","title":"A spin-refrigerated cavity quantum electrodynamic sensor","abstract":"Quantum sensors based on solid-state defects, in particular nitrogen-vacancy (NV) centers in diamond, enable precise measurement of magnetic fields, temperature, rotation, and electric fields. However, the sensitivity of leading NV spin ensemble sensors remains far from the intrinsic spin-projection noise limit. Here we move towards this quantum limit of performance by introducing (i) a cavity quantum electrodynamic (cQED) hybrid system operating in the strong coupling regime, which enables high readout fidelity of an NV ensemble using microwave homodyne detection; (ii) a comprehensive nonlinear model of the cQED sensor operation, including NV ensemble inhomogeneity and optical polarization; and (iii) ``spin refrigeration'' where the optically-polarized spin ensemble sharply reduces the ambient-temperature microwave thermal noise, resulting in enhanced sensitivity. Applying these advances to magnetometry, we demonstrate a broadband sensitivity of 580 fT/$\\sqrt{\\mathrm{Hz}}$ around 15 kHz in ambient conditions. We then discuss the implications of this model for design of future magnetometers, including devices approaching 12 fT/$\\sqrt{\\mathrm{Hz}}$ sensitivity. Applications of these techniques extend to the fields of gyroscope and clock technologies.","sentences":["Quantum sensors based on solid-state defects, in particular nitrogen-vacancy (NV) centers in diamond, enable precise measurement of magnetic fields, temperature, rotation, and electric fields.","However, the sensitivity of leading NV spin ensemble sensors remains far from the intrinsic spin-projection noise limit.","Here we move towards this quantum limit of performance by introducing (i) a cavity quantum electrodynamic (cQED) hybrid system operating in the strong coupling regime, which enables high readout fidelity of an NV ensemble using microwave homodyne detection; (ii) a comprehensive nonlinear model of the cQED sensor operation, including NV ensemble inhomogeneity and optical polarization; and (iii) ``spin refrigeration'' where the optically-polarized spin ensemble sharply reduces the ambient-temperature microwave thermal noise, resulting in enhanced sensitivity.","Applying these advances to magnetometry, we demonstrate a broadband sensitivity of 580 fT/$\\sqrt{\\mathrm{Hz}}$ around 15 kHz in ambient conditions.","We then discuss the implications of this model for design of future magnetometers, including devices approaching 12 fT/$\\sqrt{\\mathrm{Hz}}$ sensitivity.","Applications of these techniques extend to the fields of gyroscope and clock technologies."],"url":"http://arxiv.org/abs/2404.10628v1","category":"quant-ph"}
{"created":"2024-04-16 14:42:50","title":"Scatter-Gather DMA Performance Analysis within an SoC-based Control System for Trapped-Ion Quantum Computing","abstract":"Scatter-gather dynamic-memory-access (SG-DMA) is utilized in applications that require high bandwidth and low latency data transfers between memory and peripherals, where data blocks, described using buffer descriptors (BDs), are distributed throughout the memory system. The data transfer organization and requirements of a Trapped-Ion Quantum Computer (TIQC) possess characteristics similar to those targeted by SG-DMA. In particular, the ion qubits in a TIQC are manipulated by applying control sequences consisting primarily of modulated laser pulses. These optical pulses are defined by parameters that are (re)configured by the electrical control system. Variations in the operating environment and equipment make it necessary to create and run a wide range of control sequence permutations, which can be well represented as BD regions distributed across the main memory. In this paper, we experimentally evaluate the latency and throughput of SG-DMA on Xilinx radiofrequency SoC (RFSoC) devices under a variety of BD and payload sizes as a means of determining the benefits and limitations of an RFSoC system architecture for TIQC applications.","sentences":["Scatter-gather dynamic-memory-access (SG-DMA) is utilized in applications that require high bandwidth and low latency data transfers between memory and peripherals, where data blocks, described using buffer descriptors (BDs), are distributed throughout the memory system.","The data transfer organization and requirements of a Trapped-Ion Quantum Computer (TIQC) possess characteristics similar to those targeted by SG-DMA.","In particular, the ion qubits in a TIQC are manipulated by applying control sequences consisting primarily of modulated laser pulses.","These optical pulses are defined by parameters that are (re)configured by the electrical control system.","Variations in the operating environment and equipment make it necessary to create and run a wide range of control sequence permutations, which can be well represented as BD regions distributed across the main memory.","In this paper, we experimentally evaluate the latency and throughput of SG-DMA on Xilinx radiofrequency SoC (RFSoC) devices under a variety of BD and payload sizes as a means of determining the benefits and limitations of an RFSoC system architecture for TIQC applications."],"url":"http://arxiv.org/abs/2404.10619v1","category":"quant-ph"}
{"created":"2024-04-16 14:38:40","title":"Linear Stability Analysis of Relativistic Magnetized Jets: The Minimalist Approach","abstract":"A minimalist approach to the linear stability problem in fluid dynamics is developed that ensures efficiency by utilizing only the essential elements required to find the eigenvalues for given boundary conditions. It is shown that the problem is equivalent to a single first-order ordinary differential equation, and that studying the argument of the unknown complex function in the eigenvalue space is sufficient to find the dispersion relation. The method is applied to a model for relativistic magnetized astrophysical jets.","sentences":["A minimalist approach to the linear stability problem in fluid dynamics is developed that ensures efficiency by utilizing only the essential elements required to find the eigenvalues for given boundary conditions.","It is shown that the problem is equivalent to a single first-order ordinary differential equation, and that studying the argument of the unknown complex function in the eigenvalue space is sufficient to find the dispersion relation.","The method is applied to a model for relativistic magnetized astrophysical jets."],"url":"http://arxiv.org/abs/2404.10613v1","category":"astro-ph.HE"}
{"created":"2024-04-16 14:36:34","title":"Swirling due to misaligned perception-dependent motility","abstract":"A system of particles with motility variable in terms of a vision-type of perception is here investigated by a combination of Langevin dynamics simulations in two-dimensional systems and an analytical approach based on conservation law principles. Persistent swirling with predetermined direction is here induced by differentiating the self-propulsion direction and the perception cone axis. Clusters can have a fluid-like center with a rotating outer layer, or display a solid-like rotation driven by the outer layer activity. Discontinuous motility with misaligned perception might therefore serve as a powerful self-organization strategy in micro-robots.","sentences":["A system of particles with motility variable in terms of a vision-type of perception is here investigated by a combination of Langevin dynamics simulations in two-dimensional systems and an analytical approach based on conservation law principles.","Persistent swirling with predetermined direction is here induced by differentiating the self-propulsion direction and the perception cone axis.","Clusters can have a fluid-like center with a rotating outer layer, or display a solid-like rotation driven by the outer layer activity.","Discontinuous motility with misaligned perception might therefore serve as a powerful self-organization strategy in micro-robots."],"url":"http://arxiv.org/abs/2404.10608v1","category":"cond-mat.soft"}
{"created":"2024-04-16 14:32:47","title":"UAV Trajectory Optimization for Sensing Exploiting Target Location Distribution Map","abstract":"In this paper, we study the trajectory optimization of a cellular-connected unmanned aerial vehicle (UAV) which aims to sense the location of a target while maintaining satisfactory communication quality with the ground base stations (GBSs). In contrast to most existing works which assumed the target's location is known, we focus on a more challenging scenario where the exact location of the target to be sensed is unknown and random, while its distribution is known a priori and stored in a novel target location distribution map. Based on this map, the probability for the UAV to successfully sense the target can be expressed as a function of the UAV's trajectory. We aim to optimize the UAV's trajectory between two pre-determined locations to maximize the overall sensing probability during its flight, subject to a GBS-UAV communication quality constraint at each time instant and a maximum mission completion time constraint. Despite the non-convexity and NP-hardness of this problem, we devise three high-quality suboptimal solutions tailored for it with polynomial complexity. Numerical results show that our proposed designs outperform various benchmark schemes.","sentences":["In this paper, we study the trajectory optimization of a cellular-connected unmanned aerial vehicle (UAV) which aims to sense the location of a target while maintaining satisfactory communication quality with the ground base stations (GBSs).","In contrast to most existing works which assumed the target's location is known, we focus on a more challenging scenario where the exact location of the target to be sensed is unknown and random, while its distribution is known a priori and stored in a novel target location distribution map.","Based on this map, the probability for the UAV to successfully sense the target can be expressed as a function of the UAV's trajectory.","We aim to optimize the UAV's trajectory between two pre-determined locations to maximize the overall sensing probability during its flight, subject to a GBS-UAV communication quality constraint at each time instant and a maximum mission completion time constraint.","Despite the non-convexity and NP-hardness of this problem, we devise three high-quality suboptimal solutions tailored for it with polynomial complexity.","Numerical results show that our proposed designs outperform various benchmark schemes."],"url":"http://arxiv.org/abs/2404.10605v1","category":"cs.IT"}
{"created":"2024-04-16 14:28:08","title":"Exploring Post Quantum Cryptography with Quantum Key Distribution for Sustainable Mobile Network Architecture Design","abstract":"The proliferation of mobile networks and their increasing importance to modern life, combined with the emerging threat of quantum computing, present new challenges and opportunities for cybersecurity. This paper addresses the complexity of protecting these critical infrastructures against future quantum attacks while considering operational sustainability. We begin with an overview of the current landscape, identify the main vulnerabilities in mobile networks, and evaluate existing security solutions with new post-quantum cryptography (PQC) methods. We then present a quantum-secure architecture with PQC and Quantum Key Distribution (QKD) tailored explicitly for sustainable mobile networks and illustrate its applicability with several use cases that emphasize the need for advanced protection measures in this new era. In addition, a comprehensive analysis of PQC algorithm families is presented, focusing on their suitability for integration in mobile environments, with particular attention to the trade-offs between energy consumption and security improvements. Finally, recommendations for strengthening mobile networks against quantum threats are provided through a detailed examination of current challenges and opportunities.","sentences":["The proliferation of mobile networks and their increasing importance to modern life, combined with the emerging threat of quantum computing, present new challenges and opportunities for cybersecurity.","This paper addresses the complexity of protecting these critical infrastructures against future quantum attacks while considering operational sustainability.","We begin with an overview of the current landscape, identify the main vulnerabilities in mobile networks, and evaluate existing security solutions with new post-quantum cryptography (PQC) methods.","We then present a quantum-secure architecture with PQC and Quantum Key Distribution (QKD) tailored explicitly for sustainable mobile networks and illustrate its applicability with several use cases that emphasize the need for advanced protection measures in this new era.","In addition, a comprehensive analysis of PQC algorithm families is presented, focusing on their suitability for integration in mobile environments, with particular attention to the trade-offs between energy consumption and security improvements.","Finally, recommendations for strengthening mobile networks against quantum threats are provided through a detailed examination of current challenges and opportunities."],"url":"http://arxiv.org/abs/2404.10602v1","category":"cs.CR"}
{"created":"2024-04-16 14:26:55","title":"Intra-operative tumour margin evaluation in breast-conserving surgery with deep learning","abstract":"A positive margin may result in an increased risk of local recurrences after breast retention surgery for any malignant tumour. In order to reduce the number of positive margins would offer surgeon real-time intra-operative information on the presence of positive resection margins. This study aims to design an intra-operative tumour margin evaluation scheme by using specimen mammography in breast-conserving surgery. Total of 30 cases were evaluated and compared with the manually determined contours by experienced physicians and pathology report. The proposed method utilizes image thresholding to extract regions of interest and then performs a deep learning model, i.e. SegNet, to segment tumour tissue. The margin width of normal tissues surrounding it is evaluated as the result. The desired size of margin around the tumor was set for 10 mm. The smallest average difference to manual sketched margin (6.53 mm +- 5.84). In the all case, the SegNet architecture was utilized to obtain tissue specimen boundary and tumor contour, respectively. The simulation results indicated that this technology is helpful in discriminating positive from negative margins in the intra-operative setting. The aim of proposed scheme was a potential procedure in the intra-operative measurement system. The experimental results reveal that deep learning techniques can draw results that are consistent with pathology reports.","sentences":["A positive margin may result in an increased risk of local recurrences after breast retention surgery for any malignant tumour.","In order to reduce the number of positive margins would offer surgeon real-time intra-operative information on the presence of positive resection margins.","This study aims to design an intra-operative tumour margin evaluation scheme by using specimen mammography in breast-conserving surgery.","Total of 30 cases were evaluated and compared with the manually determined contours by experienced physicians and pathology report.","The proposed method utilizes image thresholding to extract regions of interest and then performs a deep learning model, i.e. SegNet, to segment tumour tissue.","The margin width of normal tissues surrounding it is evaluated as the result.","The desired size of margin around the tumor was set for 10 mm.","The smallest average difference to manual sketched margin (6.53 mm +- 5.84).","In the all case, the SegNet architecture was utilized to obtain tissue specimen boundary and tumor contour, respectively.","The simulation results indicated that this technology is helpful in discriminating positive from negative margins in the intra-operative setting.","The aim of proposed scheme was a potential procedure in the intra-operative measurement system.","The experimental results reveal that deep learning techniques can draw results that are consistent with pathology reports."],"url":"http://arxiv.org/abs/2404.10600v1","category":"cs.CV"}
{"created":"2024-04-16 14:26:39","title":"Towards free-response paradigm: a theory on decision-making in spiking neural networks","abstract":"The energy-efficient and brain-like information processing abilities of Spiking Neural Networks (SNNs) have attracted considerable attention, establishing them as a crucial element of brain-inspired computing. One prevalent challenge encountered by SNNs is the trade-off between inference speed and accuracy, which requires sufficient time to achieve the desired level of performance. Drawing inspiration from animal behavior experiments that demonstrate a connection between decision-making reaction times, task complexity, and confidence levels, this study seeks to apply these insights to SNNs. The focus is on understanding how SNNs make inferences, with a particular emphasis on untangling the interplay between signal and noise in decision-making processes. The proposed theoretical framework introduces a new optimization objective for SNN training, highlighting the importance of not only the accuracy of decisions but also the development of predictive confidence through learning from past experiences. Experimental results demonstrate that SNNs trained according to this framework exhibit improved confidence expression, leading to better decision-making outcomes. In addition, a strategy is introduced for efficient decision-making during inference, which allows SNNs to complete tasks more quickly and can use stopping times as indicators of decision confidence. By integrating neuroscience insights with neuromorphic computing, this study opens up new possibilities to explore the capabilities of SNNs and advance their application in complex decision-making scenarios.","sentences":["The energy-efficient and brain-like information processing abilities of Spiking Neural Networks (SNNs) have attracted considerable attention, establishing them as a crucial element of brain-inspired computing.","One prevalent challenge encountered by SNNs is the trade-off between inference speed and accuracy, which requires sufficient time to achieve the desired level of performance.","Drawing inspiration from animal behavior experiments that demonstrate a connection between decision-making reaction times, task complexity, and confidence levels, this study seeks to apply these insights to SNNs.","The focus is on understanding how SNNs make inferences, with a particular emphasis on untangling the interplay between signal and noise in decision-making processes.","The proposed theoretical framework introduces a new optimization objective for SNN training, highlighting the importance of not only the accuracy of decisions but also the development of predictive confidence through learning from past experiences.","Experimental results demonstrate that SNNs trained according to this framework exhibit improved confidence expression, leading to better decision-making outcomes.","In addition, a strategy is introduced for efficient decision-making during inference, which allows SNNs to complete tasks more quickly and can use stopping times as indicators of decision confidence.","By integrating neuroscience insights with neuromorphic computing, this study opens up new possibilities to explore the capabilities of SNNs and advance their application in complex decision-making scenarios."],"url":"http://arxiv.org/abs/2404.10599v1","category":"cs.NE"}
{"created":"2024-04-16 14:23:19","title":"Resilient-By-Design Framework for MIMO-OFDM Communications under Smart Jamming","abstract":"Native jamming mitigation is essential for addressing security and resilience in future 6G wireless networks. In this paper a resilient-by-design framework for effective anti-jamming in MIMO-OFDM wireless communications is introduced. A novel approach that integrates information from wireless sensing services to develop anti-jamming strategies, which do not rely on any prior information or assumptions on the adversary's concrete setup, is explored. To this end, a method that replaces conventional approaches to noise covariance estimation in anti-jamming with a surrogate covariance model is proposed, which instead incorporates sensing information on the jamming signal's directions-of-arrival (DoAs) to provide an effective approximation of the true jamming strategy. The study further focuses on integrating this novel, sensing-assisted approach into the joint optimization of beamforming, user scheduling and power allocation for a multi-user MIMO-OFDM uplink setting. Despite the NP-hard nature of this optimization problem, it can be effectively solved using an iterative water-filling approach. In order to assess the effectiveness of the proposed sensing-assisted jamming mitigation, the corresponding worst-case jamming strategy is investigated, which aims to minimize the total user sum-rate. Experimental simulations eventually affirm the robustness of our approach against both worst-case and barrage jamming, demonstrating its potential to address a wide range of jamming scenarios. Since such an integration of sensing-assisted information is directly implemented on the physical layer, resilience is incorporated preemptively by-design.","sentences":["Native jamming mitigation is essential for addressing security and resilience in future 6G wireless networks.","In this paper a resilient-by-design framework for effective anti-jamming in MIMO-OFDM wireless communications is introduced.","A novel approach that integrates information from wireless sensing services to develop anti-jamming strategies, which do not rely on any prior information or assumptions on the adversary's concrete setup, is explored.","To this end, a method that replaces conventional approaches to noise covariance estimation in anti-jamming with a surrogate covariance model is proposed, which instead incorporates sensing information on the jamming signal's directions-of-arrival (DoAs) to provide an effective approximation of the true jamming strategy.","The study further focuses on integrating this novel, sensing-assisted approach into the joint optimization of beamforming, user scheduling and power allocation for a multi-user MIMO-OFDM uplink setting.","Despite the NP-hard nature of this optimization problem, it can be effectively solved using an iterative water-filling approach.","In order to assess the effectiveness of the proposed sensing-assisted jamming mitigation, the corresponding worst-case jamming strategy is investigated, which aims to minimize the total user sum-rate.","Experimental simulations eventually affirm the robustness of our approach against both worst-case and barrage jamming, demonstrating its potential to address a wide range of jamming scenarios.","Since such an integration of sensing-assisted information is directly implemented on the physical layer, resilience is incorporated preemptively by-design."],"url":"http://arxiv.org/abs/2404.10598v1","category":"cs.IT"}
{"created":"2024-04-16 14:02:17","title":"Vivo : une approche multimodale de la synthese concatenative par corpus dans le cadre d'une oeuvre audiovisuelle immersive","abstract":"Which visual descriptors are suitable for multi-modal interaction and how to integrate them via real-time video data analysis into a corpus-based concatenative synthesis sound system.","sentences":["Which visual descriptors are suitable for multi-modal interaction and how to integrate them via real-time video data analysis into a corpus-based concatenative synthesis sound system."],"url":"http://arxiv.org/abs/2404.10578v1","category":"cs.SD"}
{"created":"2024-04-16 13:45:44","title":"Tracing the evolutionary pathways of dust and cold gas in high-z quiescent galaxies with SIMBA","abstract":"Recent discoveries of copious amounts of dust in quiescent galaxies (QGs) at high redshifts ($z\\gtrsim 1-2$) challenge the conventional view that these objects have poor interstellar medium (ISM) in proportion to their stellar mass. We use the SIMBA cosmological simulation to explore the evolution of dust and cold gas content in QGs in relation to the quenching processes affecting them. We track the changes in the ISM dust abundance across the evolutionary history of QGs identified at $0 \\lesssim z \\lesssim2$ in the field and cluster environments. The QGs quench via diverse pathways, both rapid and slow, and exhibit a wide range of times elapsed between the quenching event and cold gas removal (from $\\sim650$ Myr to $\\sim8$ Gyr). We find that quenching modes attributed to the feedback from active galactic nuclei (AGN) do not affect dust and cold gas within the same timescales. Remarkably, QGs may replenish their dust content in the quenched phase primarily due to internal processes and marginally by external factors such as minor mergers. The key mechanism for re-formation of dust is prolonged grain growth on gas-phase metals, it is effective within $\\sim100$ Myr after the quenching event, and rapidly increases the dust-to-gas mass ratio in QGs above the standard values ($\\delta_{\\rm DGR}\\gtrsim1/100$). As a result, despite heavily depleted cold gas reservoirs, roughly half of QGs maintain little evolution in their ISM dust with stellar age within the first 2 Gyr following the quenching. Overall, we predict that relatively dusty QGs ($M_{\\rm dust}/M_{\\star}\\gtrsim10^{-3}-10^{-4}$) arise from both fast and slow quenchers, and are prevalent in systems of intermediate and low stellar masses ($9<\\log(M_{\\star}/M_{\\odot})<10.5$). This prediction poses an immediate quest for observational synergy between e.g., James Webb Space Telescope (JWST) and the Atacama Large Millimeter Array (ALMA).","sentences":["Recent discoveries of copious amounts of dust in quiescent galaxies (QGs) at high redshifts ($z\\gtrsim 1-2$) challenge the conventional view that these objects have poor interstellar medium (ISM) in proportion to their stellar mass.","We use the SIMBA cosmological simulation to explore the evolution of dust and cold gas content in QGs in relation to the quenching processes affecting them.","We track the changes in the ISM dust abundance across the evolutionary history of QGs identified at $0 \\lesssim z \\lesssim2$ in the field and cluster environments.","The QGs quench via diverse pathways, both rapid and slow, and exhibit a wide range of times elapsed between the quenching event and cold gas removal (from $\\sim650$ Myr to $\\sim8$ Gyr).","We find that quenching modes attributed to the feedback from active galactic nuclei (AGN) do not affect dust and cold gas within the same timescales.","Remarkably, QGs may replenish their dust content in the quenched phase primarily due to internal processes and marginally by external factors such as minor mergers.","The key mechanism for re-formation of dust is prolonged grain growth on gas-phase metals, it is effective within $\\sim100$ Myr after the quenching event, and rapidly increases the dust-to-gas mass ratio in QGs above the standard values ($\\delta_{\\rm DGR}\\gtrsim1/100$).","As a result, despite heavily depleted cold gas reservoirs, roughly half of QGs maintain little evolution in their ISM dust with stellar age within the first 2 Gyr following the quenching.","Overall, we predict that relatively dusty QGs ($M_{\\rm dust}/M_{\\star}\\gtrsim10^{-3}-10^{-4}$) arise from both fast and slow quenchers, and are prevalent in systems of intermediate and low stellar masses ($9<\\log(M_{\\star}/M_{\\odot})<10.5$).","This prediction poses an immediate quest for observational synergy between e.g., James Webb Space Telescope (JWST) and the Atacama Large Millimeter Array (ALMA)."],"url":"http://arxiv.org/abs/2404.10568v1","category":"astro-ph.GA"}
{"created":"2024-04-16 13:26:12","title":"Quantum Mechanics of Human Perception, Behaviour and Decision-Making: A Do-It-Yourself Model Kit for Modelling Optical Illusions and Opinion Formation in Social Networks","abstract":"On the surface, behavioural science and physics seem to be two disparate fields of research. However, a closer examination of problems solved by them reveals that they are uniquely related to one another. Exemplified by the theories of quantum mind, cognition and decision-making, this unique relationship serves as the topic of this chapter. Surveying the current academic journal papers and scholarly monographs, we present an alternative vision of the role of quantum mechanics in the modern studies of human perception, behaviour and decision-making. To that end, we mostly aim to answer the 'how' question, deliberately avoiding complex mathematical concepts but developing a technically simple computational code that the readers can modify to design their own quantum-inspired models. We also present several practical examples of the application of the computation code and outline several plausible scenarios, where quantum models based on the proposed do-it-yourself model kit can help understand the differences between the behaviour of individuals and social groups.","sentences":["On the surface, behavioural science and physics seem to be two disparate fields of research.","However, a closer examination of problems solved by them reveals that they are uniquely related to one another.","Exemplified by the theories of quantum mind, cognition and decision-making, this unique relationship serves as the topic of this chapter.","Surveying the current academic journal papers and scholarly monographs, we present an alternative vision of the role of quantum mechanics in the modern studies of human perception, behaviour and decision-making.","To that end, we mostly aim to answer the 'how' question, deliberately avoiding complex mathematical concepts but developing a technically simple computational code that the readers can modify to design their own quantum-inspired models.","We also present several practical examples of the application of the computation code and outline several plausible scenarios, where quantum models based on the proposed do-it-yourself model kit can help understand the differences between the behaviour of individuals and social groups."],"url":"http://arxiv.org/abs/2404.10554v1","category":"physics.soc-ph"}
{"created":"2024-04-16 13:03:08","title":"Stellar angular momentum of intermediate redshift galaxies in MUSE surveys","abstract":"We quantify the stellar rotation of galaxies by computing the $\\lambda_{R}$ parameter, a proxy for the stellar angular momentum in a sample of 106 galaxies with redshift 0.1 $<$ z $<$ 0.8 and stellar masses from $\\sim$10$^{7.5}$ to 10$^{11.8}$ M$_{\\odot}$. The sample is located in the CANDELS/GOODS-S and COSMOS fields, and it was observed by various MUSE surveys. We create stellar velocity and velocity dispersion maps using a full-spectrum fitting technique, covering spatially $\\sim$2$R_{e}$ for the galaxies. We study the impact of the atmospheric seeing on the spin parameter and apply corrections when pertinent. Through the analysis of the $\\lambda_{R}-\\epsilon$ diagram, we notice that the fraction of round and massive galaxies increases with redshift. We lack galaxies with $\\lambda_{R}$ < 0.1 in the sample and we find only one potential, but uncertain, low-mass slow rotator at z $\\sim0.3$. Moreover, we do not see an evident evolution or trend in the stellar angular momentum with redshift. We characterize the sample environment using two indicators: a local estimator based on the Voronoi tesselation method, and a global estimator derived by the use of the Friends-of-Friends algorithm. We find no correlation between the environment and $\\lambda_{R}$ given that we are not probing dense regions or massive galaxy structures. We also analyze the kinematic maps of the sample finding that about 40$\\%$ of galaxies are consistent with being regular rotators, having rotating stellar discs with flat velocity dispersion maps, while $\\sim20\\%$ of galaxies have complex velocity maps and can be identified as non-regular rotators in spite of their $\\lambda_{R}$ values. For the remaining galaxies the classification is uncertain. As we lack galaxies with $\\lambda_{R}$< 0.1, we are not able to identify when galaxies become slow rotators within the surveyed environments, area and redshift range.","sentences":["We quantify the stellar rotation of galaxies by computing the $\\lambda_{R}$ parameter, a proxy for the stellar angular momentum in a sample of 106 galaxies with redshift 0.1 $<$ z $<$ 0.8 and stellar masses from $\\sim$10$^{7.5}$ to 10$^{11.8}$ M$_{\\odot}$.","The sample is located in the CANDELS/GOODS-S and COSMOS fields, and it was observed by various MUSE surveys.","We create stellar velocity and velocity dispersion maps using a full-spectrum fitting technique, covering spatially $\\sim$2$R_{e}$ for the galaxies.","We study the impact of the atmospheric seeing on the spin parameter and apply corrections when pertinent.","Through the analysis of the $\\lambda_{R}-\\epsilon$ diagram, we notice that the fraction of round and massive galaxies increases with redshift.","We lack galaxies with $\\lambda_{R}$ < 0.1 in the sample and we find only one potential, but uncertain, low-mass slow rotator at z $\\sim0.3$.","Moreover, we do not see an evident evolution or trend in the stellar angular momentum with redshift.","We characterize the sample environment using two indicators: a local estimator based on the Voronoi tesselation method, and a global estimator derived by the use of the Friends-of-Friends algorithm.","We find no correlation between the environment and $\\lambda_{R}$ given that we are not probing dense regions or massive galaxy structures.","We also analyze the kinematic maps of the sample finding that about 40$\\%$ of galaxies are consistent with being regular rotators, having rotating stellar discs with flat velocity dispersion maps, while $\\sim20\\%$ of galaxies have complex velocity maps and can be identified as non-regular rotators in spite of their $\\lambda_{R}$ values.","For the remaining galaxies the classification is uncertain.","As we lack galaxies with $\\lambda_{R}$< 0.1, we are not able to identify when galaxies become slow rotators within the surveyed environments, area and redshift range."],"url":"http://arxiv.org/abs/2404.10533v1","category":"astro-ph.GA"}
{"created":"2024-04-16 12:57:16","title":"JCGM 101-compliant uncertainty evaluation using virtual experiments","abstract":"Virtual experiments (VEs), a modern tool in metrology, can be used to help perform an uncertainty evaluation for the measurand. Current guidelines in metrology do not cover the many possibilities to incorporate VEs into an uncertainty evaluation, and it is often difficult to assess if the intended use of a VE complies with said guidelines. In recent work, it was shown that a VE can be used in conjunction with real measurement data and a Monte Carlo procedure to produce equal results to a supplement of the Guide to the Expression of Uncertainty in Measurement. However, this was shown only for linear measurement models. In this work, we extend this Monte Carlo approach to a common class of non-linear measurement models and more complex VEs, providing a reference approach for suitable uncertainty evaluations involving VEs. Numerical examples are given to show that the theoretical derivations hold in a practical scenario.","sentences":["Virtual experiments (VEs), a modern tool in metrology, can be used to help perform an uncertainty evaluation for the measurand.","Current guidelines in metrology do not cover the many possibilities to incorporate VEs into an uncertainty evaluation, and it is often difficult to assess if the intended use of a VE complies with said guidelines.","In recent work, it was shown that a VE can be used in conjunction with real measurement data and a Monte Carlo procedure to produce equal results to a supplement of the Guide to the Expression of Uncertainty in Measurement.","However, this was shown only for linear measurement models.","In this work, we extend this Monte Carlo approach to a common class of non-linear measurement models and more complex VEs, providing a reference approach for suitable uncertainty evaluations involving VEs.","Numerical examples are given to show that the theoretical derivations hold in a practical scenario."],"url":"http://arxiv.org/abs/2404.10530v1","category":"stat.AP"}
{"created":"2024-04-16 12:53:22","title":"Breaking of Time Translation Symmetry and Ergodicity, and Entropy decrease in a Continuous Time Crystal Driven by Nonreciprocal Optical Forces","abstract":"Nonreciprocal nonequilibrium process are attracting growing interest in sociology, animal behaviour, chemistry, and nanotechnology, and may have played a role in the origin of life. It is less widely recognized, however, that in open systems light can induce nonreciprocal predator-prey like forces between nanoparticles. Such forces provide access to the continuous time crystal state of matter, which has been demonstrated in a plasmonic metamaterial array of nanowires wherein light triggers a spontaneous mobilization transition to the robust oscillatory state, breaking time translation symmetry. Here, we report on the first experimental study of the transient dynamics of light-induced mobilization and demobilization in a time crystal. By analysing time resolved phase trajectories of the system of nanowires, we show that the mobilization transition is accompanied by breaking of continuous time translation symmetry and ergodicity, and a decrease in the entropy of motion. This insight into the transient dynamics of a nonreciprocity-driven time crystal is relevant to optical timetronics, an information and communications technology paradigm relying on the unique functionalities of time crystals, and applications of the interacting nanowire oscillator platform to modelling a wide range of nonreciprocal processes from many-body dynamics to the early stages of matter-to-life transitions.","sentences":["Nonreciprocal nonequilibrium process are attracting growing interest in sociology, animal behaviour, chemistry, and nanotechnology, and may have played a role in the origin of life.","It is less widely recognized, however, that in open systems light can induce nonreciprocal predator-prey like forces between nanoparticles.","Such forces provide access to the continuous time crystal state of matter, which has been demonstrated in a plasmonic metamaterial array of nanowires wherein light triggers a spontaneous mobilization transition to the robust oscillatory state, breaking time translation symmetry.","Here, we report on the first experimental study of the transient dynamics of light-induced mobilization and demobilization in a time crystal.","By analysing time resolved phase trajectories of the system of nanowires, we show that the mobilization transition is accompanied by breaking of continuous time translation symmetry and ergodicity, and a decrease in the entropy of motion.","This insight into the transient dynamics of a nonreciprocity-driven time crystal is relevant to optical timetronics, an information and communications technology paradigm relying on the unique functionalities of time crystals, and applications of the interacting nanowire oscillator platform to modelling a wide range of nonreciprocal processes from many-body dynamics to the early stages of matter-to-life transitions."],"url":"http://arxiv.org/abs/2404.10525v1","category":"physics.optics"}
{"created":"2024-04-16 12:48:54","title":"Capturing the Macroscopic Behaviour of Molecular Dynamics with Membership Functions","abstract":"Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis. While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges. This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves. We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition. Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data. In addition, we present a novel method for extracting transition paths based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the mu-opioid receptor. With this approach we provide a new perspective on analyzing the macroscopic behaviour of Markov systems.","sentences":["Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis.","While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges.","This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves.","We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition.","Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data.","In addition, we present a novel method for extracting transition paths based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the mu-opioid receptor.","With this approach we provide a new perspective on analyzing the macroscopic behaviour of Markov systems."],"url":"http://arxiv.org/abs/2404.10523v1","category":"physics.chem-ph"}
{"created":"2024-04-16 12:44:02","title":"A Game-Theoretic Approach for PMU Deployment Against False Data Injection Attacks","abstract":"Phasor Measurement Units (PMUs) are used in the measurement, control and protection of power grids. However, deploying PMUs at every bus in a power system is prohibitively expensive, necessitating partial PMU placement that can ensure system observability with minimal units. One consequence of this economic approach is increased system vulnerability to False Data Injection Attacks (FDIAs). This paper proposes a zero-sum game-based approach to strategically place an additional PMU (following the initial optimal PMU deployment that ensures full observability) to bolster robustness against FDIAs by introducing redundancy in attack-susceptible areas. To compute the Nash equilibrium (NE) solution, we leverage a reinforcement learning algorithm that mitigates the need for complete knowledge of the opponent's actions. The proposed PMU deployment algorithm increases the detection rate of FDIA by 36% compared to benchmark algorithms.","sentences":["Phasor Measurement Units (PMUs) are used in the measurement, control and protection of power grids.","However, deploying PMUs at every bus in a power system is prohibitively expensive, necessitating partial PMU placement that can ensure system observability with minimal units.","One consequence of this economic approach is increased system vulnerability to False Data Injection Attacks (FDIAs).","This paper proposes a zero-sum game-based approach to strategically place an additional PMU (following the initial optimal PMU deployment that ensures full observability) to bolster robustness against FDIAs by introducing redundancy in attack-susceptible areas.","To compute the Nash equilibrium (NE) solution, we leverage a reinforcement learning algorithm that mitigates the need for complete knowledge of the opponent's actions.","The proposed PMU deployment algorithm increases the detection rate of FDIA by 36% compared to benchmark algorithms."],"url":"http://arxiv.org/abs/2404.10520v1","category":"eess.SY"}
{"created":"2024-04-16 12:39:14","title":"Exact descriptional complexity of determinization of input-driven pushdown automata","abstract":"The number of states and stack symbols needed to determinize nondeterministic input-driven pushdown automata (NIDPDA) working over a fixed alphabet is determined precisely. It is proved that in the worst case exactly 2^{n^2} states are needed to determinize an n-state NIDPDA, and the proof uses witness automata with a stack alphabet \\Gamma = {0,1} working on strings over a 4-symbol input alphabet (Only an asymptotic lower bound was known before in the case of a fixed alphabet). Also, the impact of NIDPDA determinization on the size of stack alphabet is determined precisely for the first time: it is proved that s(2^{n^2}-1) stack symbols are necessary in the worst case to determinize an n-state NIDPDA working over an input alphabet of size s+5 with s left brackets (The previous lower bound was only asymptotic in the number of states and did not depend on the number of left brackets).","sentences":["The number of states and stack symbols needed to determinize nondeterministic input-driven pushdown automata (NIDPDA) working over a fixed alphabet is determined precisely.","It is proved that in the worst case exactly 2^{n^2} states are needed to determinize an n-state NIDPDA, and the proof uses witness automata with a stack alphabet \\Gamma = {0,1} working on strings over a 4-symbol input alphabet (Only an asymptotic lower bound was known before in the case of a fixed alphabet).","Also, the impact of NIDPDA determinization on the size of stack alphabet is determined precisely for the first time: it is proved that s(2^{n^2}-1) stack symbols are necessary in the worst case to determinize an n-state NIDPDA working over an input alphabet of size s+5 with s left brackets (The previous lower bound was only asymptotic in the number of states and did not depend on the number of left brackets)."],"url":"http://arxiv.org/abs/2404.10516v1","category":"cs.FL"}
{"created":"2024-04-16 12:32:55","title":"Balancing-based model reduction for switched descriptor systems","abstract":"We present a novel certified model order reduction (MOR) algorithm for switched descriptor systems applicable to large-scale systems. Our algorithm combines the idea of [Hossain \\& Trenn, Technical report, 2023] to reformulate the switched descriptor system as a switched ordinary differential equation with jumps and an extension of the balanced truncation for switched ODE from [Pontes Duff et al., IEEE Trans.~Automat.~Control, 2020]. Besides being the first MOR method for switched descriptor systems applicable to the large-scale setting, we give a detailed numerical analysis by incorporating the error in the computation of the system Gramians in the a-priori error bound for the output of the reduced system. In more detail, we demonstrate, theoretically and numerically, that the standard error bound is not applicable, and a certificate must account for the numerical approximation errors.","sentences":["We present a novel certified model order reduction (MOR) algorithm for switched descriptor systems applicable to large-scale systems.","Our algorithm combines the idea of [Hossain \\& Trenn, Technical report, 2023] to reformulate the switched descriptor system as a switched ordinary differential equation with jumps and an extension of the balanced truncation for switched ODE from [Pontes Duff et al., IEEE Trans.~Automat.~Control, 2020].","Besides being the first MOR method for switched descriptor systems applicable to the large-scale setting, we give a detailed numerical analysis by incorporating the error in the computation of the system Gramians in the a-priori error bound for the output of the reduced system.","In more detail, we demonstrate, theoretically and numerically, that the standard error bound is not applicable, and a certificate must account for the numerical approximation errors."],"url":"http://arxiv.org/abs/2404.10511v1","category":"math.NA"}
{"created":"2024-04-16 12:29:18","title":"Dependability in Embedded Systems: A Survey of Fault Tolerance Methods and Software-Based Mitigation Techniques","abstract":"Fault tolerance is a critical aspect of modern computing systems, ensuring correct functionality in the presence of faults. This paper presents a comprehensive survey of fault tolerance methods and software-based mitigation techniques in embedded systems. The focus is on real-time embedded systems, considering their resource constraints and the increasing interconnectivity of computing systems in commercial and industrial applications. The survey covers various fault-tolerance methods, including hardware, software, and hybrid redundancy. Particular emphasis is given to software faults, acknowledging their significance as a leading cause of system failures. Moreover, the paper explores the challenges posed by soft errors in modern computing systems. The survey concludes by emphasizing the need for continued research and development in fault-tolerance methods, specifically in the context of real-time embedded systems, and highlights the potential for extending fault-tolerance approaches to diverse computing environments.","sentences":["Fault tolerance is a critical aspect of modern computing systems, ensuring correct functionality in the presence of faults.","This paper presents a comprehensive survey of fault tolerance methods and software-based mitigation techniques in embedded systems.","The focus is on real-time embedded systems, considering their resource constraints and the increasing interconnectivity of computing systems in commercial and industrial applications.","The survey covers various fault-tolerance methods, including hardware, software, and hybrid redundancy.","Particular emphasis is given to software faults, acknowledging their significance as a leading cause of system failures.","Moreover, the paper explores the challenges posed by soft errors in modern computing systems.","The survey concludes by emphasizing the need for continued research and development in fault-tolerance methods, specifically in the context of real-time embedded systems, and highlights the potential for extending fault-tolerance approaches to diverse computing environments."],"url":"http://arxiv.org/abs/2404.10509v1","category":"eess.SY"}
{"created":"2024-04-16 12:24:55","title":"Restoring Connectivity in Vascular Segmentation using a Learned Post-Processing Model","abstract":"Accurate segmentation of vascular networks is essential for computer-aided tools designed to address cardiovascular diseases. Despite more than thirty years of research, it remains a challenge to obtain vascular segmentation results that preserve the connectivity of the underlying vascular network. Yet connectivity is one of the key feature of these tools. In this work, we propose a post-processing algorithm aiming to reconnect vascular structures that have been disconnected by a segmentation algorithm. Connectivity being a complex property to model explicity, we propose to learn this geometric feature either through synthetic data or annotations of the application of interest. The resulting post-processing model can be used on the output of any supervised or unsupervised vascular segmentation algorithm. We show that this post-processing effectively restores the connectivity of vascular networks both in 2D and 3D images, leading to improved overall segmentation results.","sentences":["Accurate segmentation of vascular networks is essential for computer-aided tools designed to address cardiovascular diseases.","Despite more than thirty years of research, it remains a challenge to obtain vascular segmentation results that preserve the connectivity of the underlying vascular network.","Yet connectivity is one of the key feature of these tools.","In this work, we propose a post-processing algorithm aiming to reconnect vascular structures that have been disconnected by a segmentation algorithm.","Connectivity being a complex property to model explicity, we propose to learn this geometric feature either through synthetic data or annotations of the application of interest.","The resulting post-processing model can be used on the output of any supervised or unsupervised vascular segmentation algorithm.","We show that this post-processing effectively restores the connectivity of vascular networks both in 2D and 3D images, leading to improved overall segmentation results."],"url":"http://arxiv.org/abs/2404.10506v1","category":"eess.IV"}
{"created":"2024-04-16 12:22:13","title":"Existence and multiplicity of blow-up profiles for a quasilinear diffusion equation with source","abstract":"We classify radially symmetric self-similar profiles presenting finite time blow-up to the quasilinear diffusion equation with weighted source $$ u_t=\\Delta u^m+|x|^{\\sigma}u^p, $$ posed for $(x,t)\\in\\real^N\\times(0,T)$, $T>0$, in dimension $N\\geq1$ and in the range of exponents $-2<\\sigma<\\infty$, $1<m<p<p_s(\\sigma)$, where $$ p_s(\\sigma)=\\left\\{\\begin{array}{ll}\\frac{m(N+2\\sigma+2)}{N-2}, & N\\geq3,\\\\ +\\infty, & N\\in\\{1,2\\},\\end{array}\\right. $$ is the renowned Sobolev critical exponent. The most interesting result is the \\emph{multiplicity of two different types} of self-similar profiles for $p$ sufficiently close to $m$ and $\\sigma$ sufficiently close to zero in dimension $N\\geq2$, including \\emph{dead-core profiles}. For $\\sigma=0$, this answers in dimension $N\\geq2$ a question still left open in \\cite[Section IV.1.4, pp. 195-196]{S4}, where only multiplicity in dimension $N=1$ had been established. Besides this result, we also prove that, for any $\\sigma\\in(-2,0)$, $N\\geq1$ and $m<p<p_s(\\sigma)$ \\emph{existence} of at least a self-similar blow-up profile is granted. In strong contrast with the previous results, given any $N\\geq1$, $\\sigma\\geq\\sigma^*=(mN+2)/(m-1)$ and $p\\in(m,p_s(\\sigma))$, \\emph{non-existence} of any radially symmetric self-similar profile is proved.","sentences":["We classify radially symmetric self-similar profiles presenting finite time blow-up to the quasilinear diffusion equation with weighted source $$ u_t=\\Delta u^m+|x|^{\\sigma}u^p, $$ posed for $(x,t)\\in\\real^N\\times(0,T)$, $T>0$, in dimension $N\\geq1$ and in the range of exponents $-2<\\sigma<\\infty$, $1<m<p<p_s(\\sigma)$, where $$ p_s(\\sigma)=\\left\\{\\begin{array}{ll}\\frac{m(N+2\\sigma+2)}{N-2}, & N\\geq3,\\\\","+\\infty, & N\\in\\{1,2\\},\\end{array}\\right.","$$ is the renowned Sobolev critical exponent.","The most interesting result is the \\emph{multiplicity of two different types} of self-similar profiles for $p$ sufficiently close to $m$ and $\\sigma$ sufficiently close to zero in dimension $N\\geq2$, including \\emph{dead-core profiles}.","For $\\sigma=0$, this answers in dimension $N\\geq2$ a question still left open in \\cite[Section IV.1.4, pp.","195-196]{S4}, where only multiplicity in dimension $N=1$ had been established.","Besides this result, we also prove that, for any $\\sigma\\in(-2,0)$, $N\\geq1$ and $m<p<p_s(\\sigma)$ \\emph{existence} of at least a self-similar blow-up profile is granted.","In strong contrast with the previous results, given any $N\\geq1$, $\\sigma\\geq\\sigma^*=(mN+2)/(m-1)$ and $p\\in(m,p_s(\\sigma))$, \\emph{non-existence} of any radially symmetric self-similar profile is proved."],"url":"http://arxiv.org/abs/2404.10504v1","category":"math.AP"}
{"created":"2024-04-16 12:02:53","title":"Reconfigurable spin-wave platform based on interplay between nanodots and waveguide in hybrid magnonic crystal","abstract":"We present a hybrid magnonic crystal composed of a chain of nanodots with strong perpendicular magnetic anisotropy and Dzyaloshinskii-Moriya interaction, positioned above a permalloy waveguide. The study examines two different magnetization states in the nanodots: a single-domain state and an egg-shaped skyrmion state. Due to the dipolar coupling between the dot and the waveguide, a strongly bound hybrid magnetization texture is formed in the system. Our numerical results show complex spin-wave spectra, combining the effects of periodicity, magnetization texture, and hybridization of the propagating waves in the waveguide with the dot/skyrmion modes. The systems are characterized by different band gap sizes. For the skyrmion state, the azimuthal modes confined to the skyrmion domain wall lead to the formation of flat bands at low frequencies, while at higher frequencies we identify among them modes interacting with the propagating waves, which can introduce additional non-Bragg band gaps, as well as isolated modes leading to the formation of bound states. On the other hand, the system with a single-domain state in nanodots offers a wide range of frequencies where the spin waves are predominantly in the waveguide. Thus, the study shows that the proposed hybrid magnonic crystals have many distinct functionalities, highlighting their reconfigurable potential, magnon-magnon couplings, mode localization, and bound states overlapping with the propagating waves. This opens up potential applications in analog and quantum magnonics, spin-wave filtering, and the establishment of magnonic neural networks.","sentences":["We present a hybrid magnonic crystal composed of a chain of nanodots with strong perpendicular magnetic anisotropy and Dzyaloshinskii-Moriya interaction, positioned above a permalloy waveguide.","The study examines two different magnetization states in the nanodots: a single-domain state and an egg-shaped skyrmion state.","Due to the dipolar coupling between the dot and the waveguide, a strongly bound hybrid magnetization texture is formed in the system.","Our numerical results show complex spin-wave spectra, combining the effects of periodicity, magnetization texture, and hybridization of the propagating waves in the waveguide with the dot/skyrmion modes.","The systems are characterized by different band gap sizes.","For the skyrmion state, the azimuthal modes confined to the skyrmion domain wall lead to the formation of flat bands at low frequencies, while at higher frequencies we identify among them modes interacting with the propagating waves, which can introduce additional non-Bragg band gaps, as well as isolated modes leading to the formation of bound states.","On the other hand, the system with a single-domain state in nanodots offers a wide range of frequencies where the spin waves are predominantly in the waveguide.","Thus, the study shows that the proposed hybrid magnonic crystals have many distinct functionalities, highlighting their reconfigurable potential, magnon-magnon couplings, mode localization, and bound states overlapping with the propagating waves.","This opens up potential applications in analog and quantum magnonics, spin-wave filtering, and the establishment of magnonic neural networks."],"url":"http://arxiv.org/abs/2404.10493v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-16 11:57:03","title":"Teaching Chinese Sign Language with Feedback in Mixed Reality","abstract":"Traditional sign language teaching methods face challenges such as limited feedback and diverse learning scenarios. Although 2D resources lack real-time feedback, classroom teaching is constrained by a scarcity of teacher. Methods based on VR and AR have relatively primitive interaction feedback mechanisms. This study proposes an innovative teaching model that uses real-time monocular vision and mixed reality technology. First, we introduce an improved hand-posture reconstruction method to achieve sign language semantic retention and real-time feedback. Second, a ternary system evaluation algorithm is proposed for a comprehensive assessment, maintaining good consistency with experts in sign language. Furthermore, we use mixed reality technology to construct a scenario-based 3D sign language classroom and explore the user experience of scenario teaching. Overall, this paper presents a novel teaching method that provides an immersive learning experience, advanced posture reconstruction, and precise feedback, achieving positive feedback on user experience and learning effectiveness.","sentences":["Traditional sign language teaching methods face challenges such as limited feedback and diverse learning scenarios.","Although 2D resources lack real-time feedback, classroom teaching is constrained by a scarcity of teacher.","Methods based on VR and AR have relatively primitive interaction feedback mechanisms.","This study proposes an innovative teaching model that uses real-time monocular vision and mixed reality technology.","First, we introduce an improved hand-posture reconstruction method to achieve sign language semantic retention and real-time feedback.","Second, a ternary system evaluation algorithm is proposed for a comprehensive assessment, maintaining good consistency with experts in sign language.","Furthermore, we use mixed reality technology to construct a scenario-based 3D sign language classroom and explore the user experience of scenario teaching.","Overall, this paper presents a novel teaching method that provides an immersive learning experience, advanced posture reconstruction, and precise feedback, achieving positive feedback on user experience and learning effectiveness."],"url":"http://arxiv.org/abs/2404.10490v1","category":"cs.CV"}
{"created":"2024-04-16 11:56:39","title":"Computation of the solution for the 2D acoustic pulse propagation","abstract":"We consider the 2D acoustic system with the Gaussian pulse as the initial data. This case was proposed at the first Workshop on benchmark problems in computational aeroacoustics, and it is commonly used for the verification of numerical methods. We construct an efficient algorithm to evaluate the exact solution for a given time t and distance r. For a precision eps, it takes c*ln(1/eps) operations (the evaluation of a Bessel function counts as one operation) where c does not depend on t and r. This becomes possible by using three different integral representations and an asymptotic series depending on t and r.","sentences":["We consider the 2D acoustic system with the Gaussian pulse as the initial data.","This case was proposed at the first Workshop on benchmark problems in computational aeroacoustics, and it is commonly used for the verification of numerical methods.","We construct an efficient algorithm to evaluate the exact solution for a given time t and distance r.","For a precision eps, it takes c*ln(1/eps) operations (the evaluation of a Bessel function counts as one operation) where c does not depend on t and r. This becomes possible by using three different integral representations and an asymptotic series depending on t and r."],"url":"http://arxiv.org/abs/2404.10489v1","category":"math.NA"}
{"created":"2024-04-16 11:50:19","title":"Discovery of a dormant 33 solar-mass black hole in pre-release Gaia astrometry","abstract":"Gravitational waves from black-hole merging events have revealed a population of extra-galactic BHs residing in short-period binaries with masses that are higher than expected based on most stellar evolution models - and also higher than known stellar-origin black holes in our Galaxy. It has been proposed that those high-mass BHs are the remnants of massive metal-poor stars. Gaia astrometry is expected to uncover many Galactic wide-binary systems containing dormant BHs, which may not have been detected before. The study of this population will provide new information on the BH-mass distribution in binaries and shed light on their formation mechanisms and progenitors. As part of the validation efforts in preparation for the fourth Gaia data release (DR4), we analysed the preliminary astrometric binary solutions, obtained by the Gaia Non-Single Star pipeline, to verify their significance and to minimise false-detection rates in high-mass-function orbital solutions. The astrometric binary solution of one source, Gaia BH3, implies the presence of a 32.70 \\pm 0.82 M\\odot BH in a binary system with a period of 11.6 yr. Gaia radial velocities independently validate the astrometric orbit. Broad-band photometric and spectroscopic data show that the visible component is an old, very metal-poor giant of the Galactic halo, at a distance of 590 pc. The BH in the Gaia BH3 system is more massive than any other Galactic stellar-origin BH known thus far. The low metallicity of the star companion supports the scenario that metal-poor massive stars are progenitors of the high-mass BHs detected by gravitational-wave telescopes. The Galactic orbit of the system and its metallicity indicate that it might belong to the Sequoia halo substructure. Alternatively, and more plausibly, it could belong to the ED-2 stream, which likely originated from a globular cluster that had been disrupted by the Milky Way.","sentences":["Gravitational waves from black-hole merging events have revealed a population of extra-galactic BHs residing in short-period binaries with masses that are higher than expected based on most stellar evolution models - and also higher than known stellar-origin black holes in our Galaxy.","It has been proposed that those high-mass BHs are the remnants of massive metal-poor stars.","Gaia astrometry is expected to uncover many Galactic wide-binary systems containing dormant BHs, which may not have been detected before.","The study of this population will provide new information on the BH-mass distribution in binaries and shed light on their formation mechanisms and progenitors.","As part of the validation efforts in preparation for the fourth Gaia data release (DR4), we analysed the preliminary astrometric binary solutions, obtained by the Gaia Non-Single Star pipeline, to verify their significance and to minimise false-detection rates in high-mass-function orbital solutions.","The astrometric binary solution of one source, Gaia BH3, implies the presence of a 32.70 \\pm 0.82 M\\odot BH in a binary system with a period of 11.6 yr.","Gaia radial velocities independently validate the astrometric orbit.","Broad-band photometric and spectroscopic data show that the visible component is an old, very metal-poor giant of the Galactic halo, at a distance of 590 pc.","The BH in the Gaia BH3 system is more massive than any other Galactic stellar-origin BH known thus far.","The low metallicity of the star companion supports the scenario that metal-poor massive stars are progenitors of the high-mass BHs detected by gravitational-wave telescopes.","The Galactic orbit of the system and its metallicity indicate that it might belong to the Sequoia halo substructure.","Alternatively, and more plausibly, it could belong to the ED-2 stream, which likely originated from a globular cluster that had been disrupted by the Milky Way."],"url":"http://arxiv.org/abs/2404.10486v1","category":"astro-ph.GA"}
{"created":"2024-04-16 11:42:19","title":"Primary Decomposition of Symmetric Ideals","abstract":"We propose an effective method for primary decomposition of symmetric ideals. Let $K[X]=K[x_1,\\ldots,x_n]$ be the $n$-valuables polynomial ring over a field $K$ and $\\mathfrak{S}_n$ the symmetric group of order $n$. We consider the canonical action of $\\mathfrak{S}_n$ on $K[X]$ i.e. $\\sigma(f(x_1,\\ldots,x_n))=f(x_{\\sigma(1)},\\ldots,x_{\\sigma(n)})$ for $\\sigma\\in \\mathfrak{S}_n$. For an ideal $I$ of $K[X]$, $I$ is called {\\em symmetric} if $\\sigma(I)=I$ for any $\\sigma\\in \\mathfrak{S}_n$. For a minimal primary decomposition $I=Q_1\\cap \\cdots \\cap Q_r$ of a symmetric ideal $I$, $\\sigma(I)=\\sigma (Q_1)\\cap \\cdots \\cap \\sigma(Q_r)$ is a minimal primary decomposition of $I$ for any $\\sigma\\in \\mathfrak{S}_n$. We utilize this property to compute a full primary decomposition of $I$ efficiently from partial primary components. We investigate the effectiveness of our algorithm by implementing it in the computer algebra system Risa/Asir.","sentences":["We propose an effective method for primary decomposition of symmetric ideals.","Let $K[X]=K[x_1,\\ldots,x_n]$ be the $n$-valuables polynomial ring over a field $K$ and $\\mathfrak{S}_n$ the symmetric group of order $n$. We consider the canonical action of $\\mathfrak{S}_n$ on $K[X]$ i.e. $\\sigma(f(x_1,\\ldots,x_n))=f(x_{\\sigma(1)},\\ldots,x_{\\sigma(n)})$ for $\\sigma\\in \\mathfrak{S}_n$. For an ideal $I$ of $K[X]$, $I$ is called {\\em symmetric} if $\\sigma(I)=I$ for any $\\sigma\\in \\mathfrak{S}_n$. For a minimal primary decomposition $I=Q_1\\cap \\cdots \\cap Q_r$ of a symmetric ideal $I$, $\\sigma(I)=\\sigma (Q_1)\\cap \\cdots \\cap \\sigma(Q_r)$ is a minimal primary decomposition of $I$ for any $\\sigma\\in \\mathfrak{S}_n$. We utilize this property to compute a full primary decomposition of $I$ efficiently from partial primary components.","We investigate the effectiveness of our algorithm by implementing it in the computer algebra system Risa/Asir."],"url":"http://arxiv.org/abs/2404.10482v1","category":"math.AC"}
{"created":"2024-04-16 11:29:43","title":"Toward a Realistic Benchmark for Out-of-Distribution Detection","abstract":"Deep neural networks are increasingly used in a wide range of technologies and services, but remain highly susceptible to out-of-distribution (OOD) samples, that is, drawn from a different distribution than the original training set. A common approach to address this issue is to endow deep neural networks with the ability to detect OOD samples. Several benchmarks have been proposed to design and validate OOD detection techniques. However, many of them are based on far-OOD samples drawn from very different distributions, and thus lack the complexity needed to capture the nuances of real-world scenarios. In this work, we introduce a comprehensive benchmark for OOD detection, based on ImageNet and Places365, that assigns individual classes as in-distribution or out-of-distribution depending on the semantic similarity with the training set. Several techniques can be used to determine which classes should be considered in-distribution, yielding benchmarks with varying properties. Experimental results on different OOD detection techniques show how their measured efficacy depends on the selected benchmark and how confidence-based techniques may outperform classifier-based ones on near-OOD samples.","sentences":["Deep neural networks are increasingly used in a wide range of technologies and services, but remain highly susceptible to out-of-distribution (OOD) samples, that is, drawn from a different distribution than the original training set.","A common approach to address this issue is to endow deep neural networks with the ability to detect OOD samples.","Several benchmarks have been proposed to design and validate OOD detection techniques.","However, many of them are based on far-OOD samples drawn from very different distributions, and thus lack the complexity needed to capture the nuances of real-world scenarios.","In this work, we introduce a comprehensive benchmark for OOD detection, based on ImageNet and Places365, that assigns individual classes as in-distribution or out-of-distribution depending on the semantic similarity with the training set.","Several techniques can be used to determine which classes should be considered in-distribution, yielding benchmarks with varying properties.","Experimental results on different OOD detection techniques show how their measured efficacy depends on the selected benchmark and how confidence-based techniques may outperform classifier-based ones on near-OOD samples."],"url":"http://arxiv.org/abs/2404.10474v1","category":"cs.LG"}
{"created":"2024-04-16 11:15:23","title":"Reversible optical isolators and quasi-circulators using a magneto-optical Fabry-P\u00e9rot cavity","abstract":"Nonreciprocal optical devices are essential for laser protection, modern optical communication and quantum information processing by enforcing one-way light propagation. The conventional Faraday magneto-optical nonreciprocal devices rely on a strong magnetic field, which is provided by a permanent magnet. As a result, the isolation direction of such devices is fixed and severely restricts their applications in quantum networks.In this work, we experimentally demonstrate the simultaneous one-way transmission and unidirectional reflection by using a magneto-optical Fabry-P\\'{e}rot cavity and a magnetic field strength of $50~\\milli\\tesla$. An optical isolator and a three-port quasi-circulator are realized based on this nonreciprocal cavity system. The isolator achieves an isolation ratio of up to $22~\\deci\\bel$ and an averaged insertion loss down to $0.97~\\deci\\bel$. The quasi-circulator is realized with a fidelity exceeding $99\\%$ and an overall survival probability of $89.9\\%$, corresponding to an insertion loss of $\\sim 0.46~\\deci\\bel$. The magnetic field is provided by an electromagnetic coil, thereby allowing for reversing the light circulating path. The reversible quasi-circulator paves the way for building reconfigurable quantum networks.","sentences":["Nonreciprocal optical devices are essential for laser protection, modern optical communication and quantum information processing by enforcing one-way light propagation.","The conventional Faraday magneto-optical nonreciprocal devices rely on a strong magnetic field, which is provided by a permanent magnet.","As a result, the isolation direction of such devices is fixed and severely restricts their applications in quantum networks.","In this work, we experimentally demonstrate the simultaneous one-way transmission and unidirectional reflection by using a magneto-optical Fabry-P\\'{e}rot cavity and a magnetic field strength of $50~\\milli\\tesla$. An optical isolator and a three-port quasi-circulator are realized based on this nonreciprocal cavity system.","The isolator achieves an isolation ratio of up to $22~\\deci\\bel$ and an averaged insertion loss down to $0.97~\\deci\\bel$. The quasi-circulator is realized with a fidelity exceeding $99\\%$ and an overall survival probability of $89.9\\%$, corresponding to an insertion loss of $\\sim 0.46~\\deci\\bel$.","The magnetic field is provided by an electromagnetic coil, thereby allowing for reversing the light circulating path.","The reversible quasi-circulator paves the way for building reconfigurable quantum networks."],"url":"http://arxiv.org/abs/2404.10470v1","category":"physics.optics"}
{"created":"2024-04-16 11:14:33","title":"How quickly can you pack short paths? Engineering a search-tree algorithm for disjoint s-t paths of bounded length","abstract":"We study the Short Path Packing problem which asks, given a graph $G$, integers $k$ and $\\ell$, and vertices $s$ and $t$, whether there exist $k$ pairwise internally vertex-disjoint $s$-$t$ paths of length at most $\\ell$. The problem has been proven to be NP-hard and fixed-parameter tractable parameterized by $k$ and $\\ell$. Most previous research on this problem has been theoretical with limited practical implemetations. We present an exact FPT-algorithm based on a search-tree approach in combination with greedy localization. While its worst case runtime complexity of $(k\\cdot \\ell^2)^{k\\cdot \\ell}\\cdot n^{O(1)}$ is larger than the state of the art, the nature of search-tree algorithms allows for a broad range of potential optimizations. We exploit this potential by presenting techniques for input preprocessing, early detection of trivial and infeasible instances, and strategic selection of promising subproblems. Those approaches were implemented and heavily tested on a large dataset of diverse graphs. The results show that our heuristic improvements are very effective and that for the majority of instances, we can achieve fast runtimes.","sentences":["We study the Short Path Packing problem which asks, given a graph $G$, integers $k$ and $\\ell$, and vertices $s$ and $t$, whether there exist $k$ pairwise internally vertex-disjoint $s$-$t$ paths of length at most $\\ell$. The problem has been proven to be NP-hard and fixed-parameter tractable parameterized by $k$ and $\\ell$. Most previous research on this problem has been theoretical with limited practical implemetations.","We present an exact FPT-algorithm based on a search-tree approach in combination with greedy localization.","While its worst case runtime complexity of $(k\\cdot \\ell^2)^{k\\cdot \\ell}\\cdot n^{O(1)}$ is larger than the state of the art, the nature of search-tree algorithms allows for a broad range of potential optimizations.","We exploit this potential by presenting techniques for input preprocessing, early detection of trivial and infeasible instances, and strategic selection of promising subproblems.","Those approaches were implemented and heavily tested on a large dataset of diverse graphs.","The results show that our heuristic improvements are very effective and that for the majority of instances, we can achieve fast runtimes."],"url":"http://arxiv.org/abs/2404.10469v1","category":"cs.DS"}
{"created":"2024-04-16 11:09:35","title":"Formulations of the continuous set-covering problem on networks: a comparative study","abstract":"We study the continuous set covering problem on networks and propose several new MILP formulations and valid inequalities. In contrast to state-of-the-art formulations, the new formulations only use edges to index installed points, and the formulation sizes are smaller. The covering conditions can be represented as multivariate piecewise linear concave constraints, which we formulate as disjunctive systems. We propose three MILP formulations based on indicator constraint, big-M, and disjunctive programming techniques for modeling the disjunctive system. Finally, we give a classification of new and old formulations, and conduct experiments to compare them computationally.","sentences":["We study the continuous set covering problem on networks and propose several new MILP formulations and valid inequalities.","In contrast to state-of-the-art formulations, the new formulations only use edges to index installed points, and the formulation sizes are smaller.","The covering conditions can be represented as multivariate piecewise linear concave constraints, which we formulate as disjunctive systems.","We propose three MILP formulations based on indicator constraint, big-M, and disjunctive programming techniques for modeling the disjunctive system.","Finally, we give a classification of new and old formulations, and conduct experiments to compare them computationally."],"url":"http://arxiv.org/abs/2404.10467v1","category":"math.OC"}
{"created":"2024-04-16 11:00:46","title":"A Theory of Quantum Jumps","abstract":"Using the principles of the ETH - Approach to Quantum Mechanics we study fluorescence and the phenomenon of ``quantum jumps'' in idealized models of atoms coupled to the quantized electromagnetic field. In a limiting regime where the orbital motion of the atoms is neglected and the velocity of light tends to infinity we derive explicit non-linear stochastic differential equations describing the effective time evolution of states of individual atoms. These equations give rise to a measure on state-trajectories with quantum jumps which is a quantum-mechanical analogue of the Wiener measure of Brownian motion. Our results amount to a derivation of the fundamental randomness in the quantum-mechanical description of microscopic systems from basic principles in the context of some simple models.","sentences":["Using the principles of the ETH - Approach to Quantum Mechanics we study fluorescence and the phenomenon of ``quantum jumps'' in idealized models of atoms coupled to the quantized electromagnetic field.","In a limiting regime where the orbital motion of the atoms is neglected and the velocity of light tends to infinity we derive explicit non-linear stochastic differential equations describing the effective time evolution of states of individual atoms.","These equations give rise to a measure on state-trajectories with quantum jumps which is a quantum-mechanical analogue of the Wiener measure of Brownian motion.","Our results amount to a derivation of the fundamental randomness in the quantum-mechanical description of microscopic systems from basic principles in the context of some simple models."],"url":"http://arxiv.org/abs/2404.10460v1","category":"quant-ph"}
{"created":"2024-04-16 10:58:57","title":"The promises and challenges of many-body quantum technologies: a focus on quantum engines","abstract":"Can many-body systems be beneficial to designing quantum technologies? We address this question by examining quantum engines, where recent studies indicate potential benefits through the harnessing of many-body effects, such as divergences close to phase transitions. However, open questions remain regarding their real-world applications.","sentences":["Can many-body systems be beneficial to designing quantum technologies?","We address this question by examining quantum engines, where recent studies indicate potential benefits through the harnessing of many-body effects, such as divergences close to phase transitions.","However, open questions remain regarding their real-world applications."],"url":"http://arxiv.org/abs/2404.10459v1","category":"quant-ph"}
{"created":"2024-04-16 10:41:00","title":"Ultrahigh Stability of O-Sublattice in $\u03b2$-Ga$_2$O$_3$","abstract":"Recently reported remarkably high radiation tolerance of $\\gamma$/$\\beta$-Ga$_2$O$_3$ double-polymorphic structure brings this ultrawide bandgap semiconductor to the frontiers of power electronics applications that are able to operate in challenging environments. Understanding the mechanism of radiation tolerance is crucial for further material modification and tailoring of the desired properties. In this study, we employ machine-learning-enhanced atomistic simulations to assess the stability of both the gallium (Ga) and oxygen (O) sublattices under various levels of damage. Our study uncovers the remarkable resilience and stability of the O-sublattice, attributing this property to the strong tendency of recovery of the O defects, especially within the stronger disordered regions. Interestingly, we observe the opposite behavior of the Ga defects that display enhanced stability in the same regions of increased disorder. Moreover, we observe that highly defective $\\beta$-Ga$_2$O$_3$ is able to transform into $\\gamma$-Ga$_2$O$_3$ upon annealing due to preserved lattice organization of the O-sublattice. This result clearly manifests that the ultrahigh stability of the O-sublattice provides the backbone for the exceptional radiation tolerance of the $\\gamma$/$\\beta$ double-polymorphic structure. These computational insights closely align with experimental observations, opening avenues for further exploration of polymorphism in Ga$_2$O$_3$ and potentially in analogous polymorphic families spanning a broad range of diverse materials of complex polymorphic nature.","sentences":["Recently reported remarkably high radiation tolerance of $\\gamma$/$\\beta$-Ga$_2$O$_3$ double-polymorphic structure brings this ultrawide bandgap semiconductor to the frontiers of power electronics applications that are able to operate in challenging environments.","Understanding the mechanism of radiation tolerance is crucial for further material modification and tailoring of the desired properties.","In this study, we employ machine-learning-enhanced atomistic simulations to assess the stability of both the gallium (Ga) and oxygen (O) sublattices under various levels of damage.","Our study uncovers the remarkable resilience and stability of the O-sublattice, attributing this property to the strong tendency of recovery of the O defects, especially within the stronger disordered regions.","Interestingly, we observe the opposite behavior of the Ga defects that display enhanced stability in the same regions of increased disorder.","Moreover, we observe that highly defective $\\beta$-Ga$_2$O$_3$ is able to transform into $\\gamma$-Ga$_2$O$_3$ upon annealing due to preserved lattice organization of the O-sublattice.","This result clearly manifests that the ultrahigh stability of the O-sublattice provides the backbone for the exceptional radiation tolerance of the $\\gamma$/$\\beta$ double-polymorphic structure.","These computational insights closely align with experimental observations, opening avenues for further exploration of polymorphism in Ga$_2$O$_3$ and potentially in analogous polymorphic families spanning a broad range of diverse materials of complex polymorphic nature."],"url":"http://arxiv.org/abs/2404.10451v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 10:31:24","title":"Watching Grass Grow: Long-term Visual Navigation and Mission Planning for Autonomous Biodiversity Monitoring","abstract":"We describe a challenging robotics deployment in a complex ecosystem to monitor a rich plant community. The study site is dominated by dynamic grassland vegetation and is thus visually ambiguous and liable to drastic appearance change over the course of a day and especially through the growing season. This dynamism and complexity in appearance seriously impact the stability of the robotics platform, as localisation is a foundational part of that control loop, and so routes must be carefully taught and retaught until autonomy is robust and repeatable. Our system is demonstrated over a 6-week period monitoring the response of grass species to experimental climate change manipulations. We also discuss the applicability of our pipeline to monitor biodiversity in other complex natural settings.","sentences":["We describe a challenging robotics deployment in a complex ecosystem to monitor a rich plant community.","The study site is dominated by dynamic grassland vegetation and is thus visually ambiguous and liable to drastic appearance change over the course of a day and especially through the growing season.","This dynamism and complexity in appearance seriously impact the stability of the robotics platform, as localisation is a foundational part of that control loop, and so routes must be carefully taught and retaught until autonomy is robust and repeatable.","Our system is demonstrated over a 6-week period monitoring the response of grass species to experimental climate change manipulations.","We also discuss the applicability of our pipeline to monitor biodiversity in other complex natural settings."],"url":"http://arxiv.org/abs/2404.10446v1","category":"cs.RO"}
{"created":"2024-04-16 10:04:47","title":"Decentralized Control for Heterogeneous Battery Energy Storage System","abstract":"Battery energy storage systems (BESSs) are essential for stable power supply in renewable energy systems that can operate in all weather. Future BESSs will be massive and pluggable with several heterogeneous batteries. In this paper, a novel decentralized control method for a heterogeneous BESS is proposed, in which each battery autonomously operates based on its characteristics. First, a control method that uses only one broadcast signal for each type of battery is proposed. Second, the asymptotic stability of the tracking error is proved. Third, numerical simulations confirm that the proposed control method has robust tracking performance of the total electric power to the demanded power when some batteries fail and are detached from the system. Last, in order to suppress degradation of battery, equalization of the state of charge is achieved for each type of battery without communication among the batteries.","sentences":["Battery energy storage systems (BESSs) are essential for stable power supply in renewable energy systems that can operate in all weather.","Future BESSs will be massive and pluggable with several heterogeneous batteries.","In this paper, a novel decentralized control method for a heterogeneous BESS is proposed, in which each battery autonomously operates based on its characteristics.","First, a control method that uses only one broadcast signal for each type of battery is proposed.","Second, the asymptotic stability of the tracking error is proved.","Third, numerical simulations confirm that the proposed control method has robust tracking performance of the total electric power to the demanded power when some batteries fail and are detached from the system.","Last, in order to suppress degradation of battery, equalization of the state of charge is achieved for each type of battery without communication among the batteries."],"url":"http://arxiv.org/abs/2404.10439v1","category":"eess.SY"}
{"created":"2024-04-16 09:52:26","title":"Analysis of a Navier-Stokes phase-field crystal system","abstract":"We consider an evolution system modeling a flow of colloidal particles which are suspended in an incompressible fluid and accounts for colloidal crystallization. The system consists of the Navier-Stokes equations for the volume averaged velocity coupled with the so-called Phase-Field Crystal equation for the density deviation. Considering this system in a periodic domain and assuming that the viscosity as well as the mobility depend on the density deviation, we first prove the existence of a weak solution in dimension three. Then, in dimension two, we establish the existence of a (unique) strong solution.","sentences":["We consider an evolution system modeling a flow of colloidal particles which are suspended in an incompressible fluid and accounts for colloidal crystallization.","The system consists of the Navier-Stokes equations for the volume averaged velocity coupled with the so-called Phase-Field Crystal equation for the density deviation.","Considering this system in a periodic domain and assuming that the viscosity as well as the mobility depend on the density deviation, we first prove the existence of a weak solution in dimension three.","Then, in dimension two, we establish the existence of a (unique) strong solution."],"url":"http://arxiv.org/abs/2404.10431v1","category":"math.AP"}
{"created":"2024-04-16 09:46:16","title":"Effect of Systematic Uncertainties on Density and Temperature Estimates in Coronae of Capella","abstract":"We estimate the coronal density of Capella using the O VII and Fe XVII line systems in the soft X-ray regime that have been observed over the course of the Chandra mission. Our analysis combines measures of error due to uncertainty in the underlying atomic data with statistical errors in the Chandra data to derive meaningful overall uncertainties on the plasma density of the coronae of Capella. We consider two Bayesian frameworks. First, the so-called pragmatic-Bayesian approach considers the atomic data and their uncertainties as fully specified and uncorrectable. The fully-Bayesian approach, on the other hand, allows the observed spectral data to update the atomic data and their uncertainties, thereby reducing the overall errors on the inferred parameters. To incorporate atomic data uncertainties, we obtain a set of atomic data replicates, the distribution of which captures their uncertainty. A principal component analysis of these replicates allows us to represent the atomic uncertainty with a lower-dimensional multivariate Gaussian distribution. A $t$-distribution approximation of the uncertainties of a subset of plasma parameters including a priori temperature information, obtained from the temperature-sensitive-only Fe XVII spectral line analysis, is carried forward into the density- and temperature-sensitive O VII spectral line analysis. Markov Chain Monte Carlo based model fitting is implemented including Multi-step Monte Carlo Gibbs Sampler and Hamiltonian Monte Carlo. Our analysis recovers an isothermally approximated coronal plasma temperature of $\\approx$5 MK and a coronal plasma density of $\\approx$10$^{10}$ cm$^{-3}$, with uncertainties of 0.1 and 0.2 dex respectively.","sentences":["We estimate the coronal density of Capella using the O VII and Fe XVII line systems in the soft X-ray regime that have been observed over the course of the Chandra mission.","Our analysis combines measures of error due to uncertainty in the underlying atomic data with statistical errors in the Chandra data to derive meaningful overall uncertainties on the plasma density of the coronae of Capella.","We consider two Bayesian frameworks.","First, the so-called pragmatic-Bayesian approach considers the atomic data and their uncertainties as fully specified and uncorrectable.","The fully-Bayesian approach, on the other hand, allows the observed spectral data to update the atomic data and their uncertainties, thereby reducing the overall errors on the inferred parameters.","To incorporate atomic data uncertainties, we obtain a set of atomic data replicates, the distribution of which captures their uncertainty.","A principal component analysis of these replicates allows us to represent the atomic uncertainty with a lower-dimensional multivariate Gaussian distribution.","A $t$-distribution approximation of the uncertainties of a subset of plasma parameters including a priori temperature information, obtained from the temperature-sensitive-only Fe XVII spectral line analysis, is carried forward into the density- and temperature-sensitive O VII spectral line analysis.","Markov Chain Monte Carlo based model fitting is implemented including Multi-step Monte Carlo Gibbs Sampler and Hamiltonian Monte Carlo.","Our analysis recovers an isothermally approximated coronal plasma temperature of $\\approx$5 MK and a coronal plasma density of $\\approx$10$^{10}$ cm$^{-3}$, with uncertainties of 0.1 and 0.2 dex respectively."],"url":"http://arxiv.org/abs/2404.10427v1","category":"astro-ph.SR"}
{"created":"2024-04-16 09:40:10","title":"Lipshitzian Vector Fields, Upper Gradients And Distributional Derivatives","abstract":"We prove that given a locally integrable function $f$ on an open set of an Euclidean space the distributional derivative $Xf$ with respect to a locally Lipshitzian vector field $X$ is locally integrable if, and only if, the function $f$ admits a locally integrable upper gradient along the vector field $X$; in this case $Xf$ coincides with the Lie derivative $L_X f$ and $|Xf|$ is the least upper gradient of the function $f$. Applications to systems of locally Lipshitzian vector fields are given.","sentences":["We prove that given a locally integrable function $f$ on an open set of an Euclidean space the distributional derivative $Xf$ with respect to a locally Lipshitzian vector field $X$ is locally integrable if, and only if, the function $f$ admits a locally integrable upper gradient along the vector field $X$; in this case $Xf$ coincides with the Lie derivative $L_X f$ and $|Xf|$ is the least upper gradient of the function $f$. Applications to systems of locally Lipshitzian vector fields are given."],"url":"http://arxiv.org/abs/2404.10422v1","category":"math.MG"}
{"created":"2024-04-16 09:38:40","title":"Concurrency Model of BDI Programming Frameworks: Why Should We Control It?","abstract":"We provide a taxonomy of concurrency models for BDI frameworks, elicited by analysing state-of-the-art technologies, and aimed at helping both BDI designers and developers in making informed decisions. Comparison among BDI technologies w.r.t. concurrency models reveals heterogeneous support, and low customisability.","sentences":["We provide a taxonomy of concurrency models for BDI frameworks, elicited by analysing state-of-the-art technologies, and aimed at helping both BDI designers and developers in making informed decisions.","Comparison among BDI technologies w.r.t. concurrency models reveals heterogeneous support, and low customisability."],"url":"http://arxiv.org/abs/2404.10421v1","category":"cs.MA"}
{"created":"2024-04-16 09:34:49","title":"An upper bound on the number of relevant variables for Boolean functions on the Hamming graph","abstract":"The spectrum of a complex-valued function $f$ on $\\mathbb{Z}_{q}^n$ is the set $\\{|u|:u\\in \\mathbb{Z}_q^n~\\mathrm{and}~\\widehat{f}(u)\\neq 0\\}$, where $|u|$ is the Hamming weight of $u$ and $\\widehat{f}$ is the Fourier transform of $f$. Let $1\\leq d'\\leq d\\leq n$. In this work, we study Boolean functions on $\\mathbb{Z}_{q}^n$, $q\\geq 3$, whose spectrum is a subset of $\\{0\\}\\cup \\{d',\\ldots,d\\}$. We prove that such functions have at most $\\frac{d}{2}\\cdot \\frac{q^{d+d'}}{2^{d'}(q-1)^{d'}}$ relevant variables for $d'+d\\leq n+1$. In particular, we prove that any Boolean function of degree $d$ on $\\mathbb{Z}_{q}^n$, $q\\geq 3$, has at most $\\frac{dq^{d+1}}{4(q-1)}$ relevant variables. We also show that any equitable 2-partition of the Hamming graph $H(n,q)$, $q\\geq 3$, associated with the eigenvalue $n(q-1)-qd$ has at most $\\frac{d}{2}\\cdot \\frac{q^{2d}}{2^d(q-1)^{d}}$ relevant variables for $d\\leq \\frac{n+1}{2}$.","sentences":["The spectrum of a complex-valued function $f$ on $\\mathbb{Z}_{q}^n$ is the set $\\{|u|:u\\in \\mathbb{Z}_q^n~\\mathrm{and}~\\widehat{f}(u)\\neq 0\\}$, where $|u|$ is the Hamming weight of $u$ and $\\widehat{f}$ is the Fourier transform of $f$. Let $1\\leq d'\\leq d\\leq n$.","In this work, we study Boolean functions on $\\mathbb{Z}_{q}^n$, $q\\geq 3$, whose spectrum is a subset of $\\{0\\}\\cup \\{d',\\ldots,d\\}$. We prove that such functions have at most $\\frac{d}{2}\\cdot \\frac{q^{d+d'}}{2^{d'}(q-1)^{d'}}$ relevant variables for $d'+d\\leq n+1$.","In particular, we prove that any Boolean function of degree $d$ on $\\mathbb{Z}_{q}^n$, $q\\geq 3$, has at most $\\frac{dq^{d+1}}{4(q-1)}$ relevant variables.","We also show that any equitable 2-partition of the Hamming graph $H(n,q)$, $q\\geq 3$, associated with the eigenvalue $n(q-1)-qd$ has at most $\\frac{d}{2}\\cdot \\frac{q^{2d}}{2^d(q-1)^{d}}$ relevant variables for $d\\leq \\frac{n+1}{2}$."],"url":"http://arxiv.org/abs/2404.10418v1","category":"math.CO"}
{"created":"2024-04-16 09:32:44","title":"A Comprehensive Study on A Tapered Paul Trap: From Design to Potential Applications","abstract":"We present a tapered Paul trap whose radio frequency electrodes are inclined to the symmetric axis of the endcap electrodes, resulting in a funnel-shaped trapping potential. With this configuration, a charged particle confined in this trap has its radial degrees of freedom coupled to that of the axial direction. The same design was successfully used to experimentally realize a single-atom heat engine, and with this setup amplification of zeptonewton forces was implemented. In this paper, we show the design, implementation, and characterization of such an ion trap in detail. This system offers a high level of control over the ion's motion. Its novel features promise applications in the field of quantum thermodynamics, quantum sensing, and quantum information.","sentences":["We present a tapered Paul trap whose radio frequency electrodes are inclined to the symmetric axis of the endcap electrodes, resulting in a funnel-shaped trapping potential.","With this configuration, a charged particle confined in this trap has its radial degrees of freedom coupled to that of the axial direction.","The same design was successfully used to experimentally realize a single-atom heat engine, and with this setup amplification of zeptonewton forces was implemented.","In this paper, we show the design, implementation, and characterization of such an ion trap in detail.","This system offers a high level of control over the ion's motion.","Its novel features promise applications in the field of quantum thermodynamics, quantum sensing, and quantum information."],"url":"http://arxiv.org/abs/2404.10415v1","category":"quant-ph"}
{"created":"2024-04-16 09:31:19","title":"VDTuner: Automated Performance Tuning for Vector Data Management Systems","abstract":"Vector data management systems (VDMSs) have become an indispensable cornerstone in large-scale information retrieval and machine learning systems like large language models. To enhance the efficiency and flexibility of similarity search, VDMS exposes many tunable index parameters and system parameters for users to specify. However, due to the inherent characteristics of VDMS, automatic performance tuning for VDMS faces several critical challenges, which cannot be well addressed by the existing auto-tuning methods. In this paper, we introduce VDTuner, a learning-based automatic performance tuning framework for VDMS, leveraging multi-objective Bayesian optimization. VDTuner overcomes the challenges associated with VDMS by efficiently exploring a complex multi-dimensional parameter space without requiring any prior knowledge. Moreover, it is able to achieve a good balance between search speed and recall rate, delivering an optimal configuration. Extensive evaluations demonstrate that VDTuner can markedly improve VDMS performance (14.12% in search speed and 186.38% in recall rate) compared with default setting, and is more efficient compared with state-of-the-art baselines (up to 3.57 times faster in terms of tuning time). In addition, VDTuner is scalable to specific user preference and cost-aware optimization objective. VDTuner is available online at https://github.com/tiannuo-yang/VDTuner.","sentences":["Vector data management systems (VDMSs) have become an indispensable cornerstone in large-scale information retrieval and machine learning systems like large language models.","To enhance the efficiency and flexibility of similarity search, VDMS exposes many tunable index parameters and system parameters for users to specify.","However, due to the inherent characteristics of VDMS, automatic performance tuning for VDMS faces several critical challenges, which cannot be well addressed by the existing auto-tuning methods.","In this paper, we introduce VDTuner, a learning-based automatic performance tuning framework for VDMS, leveraging multi-objective Bayesian optimization.","VDTuner overcomes the challenges associated with VDMS by efficiently exploring a complex multi-dimensional parameter space without requiring any prior knowledge.","Moreover, it is able to achieve a good balance between search speed and recall rate, delivering an optimal configuration.","Extensive evaluations demonstrate that VDTuner can markedly improve VDMS performance (14.12% in search speed and 186.38% in recall rate) compared with default setting, and is more efficient compared with state-of-the-art baselines (up to 3.57 times faster in terms of tuning time).","In addition, VDTuner is scalable to specific user preference and cost-aware optimization objective.","VDTuner is available online at https://github.com/tiannuo-yang/VDTuner."],"url":"http://arxiv.org/abs/2404.10413v1","category":"cs.DB"}
{"created":"2024-04-16 09:28:54","title":"Camera clustering for scalable stream-based active distillation","abstract":"We present a scalable framework designed to craft efficient lightweight models for video object detection utilizing self-training and knowledge distillation techniques. We scrutinize methodologies for the ideal selection of training images from video streams and the efficacy of model sharing across numerous cameras. By advocating for a camera clustering methodology, we aim to diminish the requisite number of models for training while augmenting the distillation dataset. The findings affirm that proper camera clustering notably amplifies the accuracy of distilled models, eclipsing the methodologies that employ distinct models for each camera or a universal model trained on the aggregate camera data.","sentences":["We present a scalable framework designed to craft efficient lightweight models for video object detection utilizing self-training and knowledge distillation techniques.","We scrutinize methodologies for the ideal selection of training images from video streams and the efficacy of model sharing across numerous cameras.","By advocating for a camera clustering methodology, we aim to diminish the requisite number of models for training while augmenting the distillation dataset.","The findings affirm that proper camera clustering notably amplifies the accuracy of distilled models, eclipsing the methodologies that employ distinct models for each camera or a universal model trained on the aggregate camera data."],"url":"http://arxiv.org/abs/2404.10411v1","category":"cs.CV"}
{"created":"2024-04-16 09:22:17","title":"Efficient Electrochemical CO2 Reduction Reaction over Cu-decorated Biphenylene","abstract":"Developing efficient electrocatalysts for CO$_2$ reduction into value-added products is crucial for the green economy. Inspired by the recent synthesis of Biphenylene (BPH), we have systematically investigated pristine, defective, and Cu-decorated BPH as an electrocatalyst for the CO$_2$ reduction reactions (CRR). Our first-principles calculations show the CO$_2$ molecules weakly interact with the pristine BPH surface while defective BPH facilitates the CO$_2$ adsorption with a binding energy ($E_b$) of -3.22 eV, indicating the detrimental process for the CRR on the surface of both systems. Furthermore, we have investigated the binding energy and kinetic stability of Cu-decorated BPH as a single-atom-catalyst (SAC). The molecular dynamics simulations confirm the kinetic stability, revealing that the Cu-atom avoids agglomeration under low metal dispersal conditions. The CO$_2$ molecule gets adsorbed horizontally on the Cu-BPH surface with $E_b$ of -0.52 eV. The CRR mechanism is investigated using two pathways beginning with two different initial intermediate states, formate ($\\mathrm{^*OCOH}$) and the carboxylic ($\\mathrm{^*COOH}$) pathways. The formate pathway confirms the conversion of $\\mathrm{^*OCOH}$ to $\\mathrm{^*HCOOH}$ with the rate-limiting potential ($U_L$) of 0.57 eV for the production of HCOOH, while for the carboxylic pathway, the conversion of $\\mathrm{^*COH}$ to $\\mathrm{^*CHOH}$ has $U_L$ of 0.49 eV for the production of CH$_3$OH. We have also investigated the effect of protons using charged hydrogen pseudopotential, which hints towards the possible formation of CH$_3$OH as fuel. Our findings propose Cu-BPH as an efficient single-atom catalyst for CO$_2$ conversion compared to the well-known Cu metal.","sentences":["Developing efficient electrocatalysts for CO$_2$ reduction into value-added products is crucial for the green economy.","Inspired by the recent synthesis of Biphenylene (BPH), we have systematically investigated pristine, defective, and Cu-decorated BPH as an electrocatalyst for the CO$_2$ reduction reactions (CRR).","Our first-principles calculations show the CO$_2$ molecules weakly interact with the pristine BPH surface while defective BPH facilitates the CO$_2$ adsorption with a binding energy ($E_b$) of -3.22 eV, indicating the detrimental process for the CRR on the surface of both systems.","Furthermore, we have investigated the binding energy and kinetic stability of Cu-decorated BPH as a single-atom-catalyst (SAC).","The molecular dynamics simulations confirm the kinetic stability, revealing that the Cu-atom avoids agglomeration under low metal dispersal conditions.","The CO$_2$ molecule gets adsorbed horizontally on the Cu-BPH surface with $E_b$ of -0.52 eV. The CRR mechanism is investigated using two pathways beginning with two different initial intermediate states, formate ($\\mathrm{^*OCOH}$) and the carboxylic ($\\mathrm{^*COOH}$) pathways.","The formate pathway confirms the conversion of $\\mathrm{^*OCOH}$ to $\\mathrm{^*HCOOH}$ with the rate-limiting potential ($U_L$) of 0.57 eV for the production of HCOOH, while for the carboxylic pathway, the conversion of $\\mathrm{^*COH}$ to $\\mathrm{^*CHOH}$ has $U_L$ of 0.49 eV for the production of CH$_3$OH.","We have also investigated the effect of protons using charged hydrogen pseudopotential, which hints towards the possible formation of CH$_3$OH as fuel.","Our findings propose Cu-BPH as an efficient single-atom catalyst for CO$_2$ conversion compared to the well-known Cu metal."],"url":"http://arxiv.org/abs/2404.10409v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 09:16:36","title":"Dispersionless version of the multicomponent KP hierarchy revisited","abstract":"We revisit dispersionless version of the multicomponent KP hierarchy considered previously by Takasaki and Takebe. In contrast to their study, we do not fix any distinguished component treating all of them on equal footing. We obtain nonlinear equations for dispersionless tau-function (the F-function) and represent them using the trigonometric parametrization. In this trigonometric uniformization the equations considerably simplify and acquire a nice form.","sentences":["We revisit dispersionless version of the multicomponent KP hierarchy considered previously by Takasaki and Takebe.","In contrast to their study, we do not fix any distinguished component treating all of them on equal footing.","We obtain nonlinear equations for dispersionless tau-function (the F-function) and represent them using the trigonometric parametrization.","In this trigonometric uniformization the equations considerably simplify and acquire a nice form."],"url":"http://arxiv.org/abs/2404.10406v1","category":"nlin.SI"}
{"created":"2024-04-16 08:55:46","title":"On the external concurrency of current BDI frameworks for MAS","abstract":"The execution of Belief-Desire-Intention (BDI) agents in a Multi-Agent System (MAS) can be practically implemented on top of low-level concurrency mechanisms that impact on efficiency, determinism, and reproducibility. We argue that developers should specify the MAS behaviour independently of the execution model, and choose or configure the concurrency model later on, according to their target domain's specific needs, leaving the MAS specification unaffected. We identify patterns for mapping the agent execution over the underlying concurrency abstractions, and investigate which concurrency models are supported by some of the most commonly used BDI platforms. Although most frameworks support multiple concurrency models, we find that they tend to hide them under the hood, making them opaque to the developer, and effectively limiting the possibility of fine-tuning the MAS.","sentences":["The execution of Belief-Desire-Intention (BDI) agents in a Multi-Agent System (MAS) can be practically implemented on top of low-level concurrency mechanisms that impact on efficiency, determinism, and reproducibility.","We argue that developers should specify the MAS behaviour independently of the execution model, and choose or configure the concurrency model later on, according to their target domain's specific needs, leaving the MAS specification unaffected.","We identify patterns for mapping the agent execution over the underlying concurrency abstractions, and investigate which concurrency models are supported by some of the most commonly used BDI platforms.","Although most frameworks support multiple concurrency models, we find that they tend to hide them under the hood, making them opaque to the developer, and effectively limiting the possibility of fine-tuning the MAS."],"url":"http://arxiv.org/abs/2404.10397v1","category":"cs.MA"}
{"created":"2024-04-16 08:54:29","title":"Efficient evaluation of Bernstein-B\u00e9zier coefficients of B-spline basis functions over one knot span","abstract":"New differential-recurrence relations for B-spline basis functions are given. Using these relations, a recursive method for finding the Bernstein-B\\'{e}zier coefficients of B-spline basis functions over a single knot span is proposed. The algorithm works for any knot sequence which guarantees that all B-spline functions are at least $C^0$-continuous. It has good numerical behavior and has an asymptotically optimal computational complexity.","sentences":["New differential-recurrence relations for B-spline basis functions are given.","Using these relations, a recursive method for finding the Bernstein-B\\'{e}zier coefficients of B-spline basis functions over a single knot span is proposed.","The algorithm works for any knot sequence which guarantees that all B-spline functions are at least $C^0$-continuous.","It has good numerical behavior and has an asymptotically optimal computational complexity."],"url":"http://arxiv.org/abs/2404.10396v1","category":"math.NA"}
{"created":"2024-04-16 08:44:39","title":"Complexity and algorithms for Arc-Kayles and Non-Disconnecting Arc-Kayles","abstract":"Arc-Kayles is a game where two players alternate removing two adjacent vertices until no move is left. Introduced in 1978, its computational complexity is still open. More recently, subtraction games, where the players cannot disconnect the graph while removing vertices, were introduced. In particular, Arc-Kayles admits a non-disconnecting variant that is a subtraction game. We study the computational complexity of subtraction games on graphs, proving that they are PSPACE-complete even on very structured graph classes (split, bipartite of any even girth). We prove that Non-Disconnecting Arc-Kayles can be solved in polynomial-time on unicyclic graphs, clique trees, and subclasses of threshold graphs. We also show that a sufficient condition for a second player-win on Arc-Kayles is equivalent to the graph isomorphism problem.","sentences":["Arc-Kayles is a game where two players alternate removing two adjacent vertices until no move is left.","Introduced in 1978, its computational complexity is still open.","More recently, subtraction games, where the players cannot disconnect the graph while removing vertices, were introduced.","In particular, Arc-Kayles admits a non-disconnecting variant that is a subtraction game.","We study the computational complexity of subtraction games on graphs, proving that they are PSPACE-complete even on very structured graph classes (split, bipartite of any even girth).","We prove that Non-Disconnecting Arc-Kayles can be solved in polynomial-time on unicyclic graphs, clique trees, and subclasses of threshold graphs.","We also show that a sufficient condition for a second player-win on Arc-Kayles is equivalent to the graph isomorphism problem."],"url":"http://arxiv.org/abs/2404.10390v1","category":"math.CO"}
{"created":"2024-04-16 08:44:34","title":"Paving the Way to Hybrid Quantum-Classical Scientific Workflows","abstract":"The increasing growth of data volume, and the consequent explosion in demand for computational power, are affecting scientific computing, as shown by the rise of extreme data scientific workflows. As the need for computing power increases, quantum computing has been proposed as a way to deliver it. It may provide significant theoretical speedups for many scientific applications (i.e., molecular dynamics, quantum chemistry, combinatorial optimization, and machine learning). Therefore, integrating quantum computers into the computing continuum constitutes a promising way to speed up scientific computation. However, the scientific computing community still lacks the necessary tools and expertise to fully harness the power of quantum computers in the execution of complex applications such as scientific workflows. In this work, we describe the main characteristics of quantum computing and its main benefits for scientific applications, then we formalize hybrid quantum-classic workflows, explore how to identify quantum components and map them onto resources. We demonstrate concepts on a real use case and define a software architecture for a hybrid workflow management system.","sentences":["The increasing growth of data volume, and the consequent explosion in demand for computational power, are affecting scientific computing, as shown by the rise of extreme data scientific workflows.","As the need for computing power increases, quantum computing has been proposed as a way to deliver it.","It may provide significant theoretical speedups for many scientific applications (i.e., molecular dynamics, quantum chemistry, combinatorial optimization, and machine learning).","Therefore, integrating quantum computers into the computing continuum constitutes a promising way to speed up scientific computation.","However, the scientific computing community still lacks the necessary tools and expertise to fully harness the power of quantum computers in the execution of complex applications such as scientific workflows.","In this work, we describe the main characteristics of quantum computing and its main benefits for scientific applications, then we formalize hybrid quantum-classic workflows, explore how to identify quantum components and map them onto resources.","We demonstrate concepts on a real use case and define a software architecture for a hybrid workflow management system."],"url":"http://arxiv.org/abs/2404.10389v1","category":"cs.ET"}
{"created":"2024-04-16 08:43:45","title":"Worst-Case Riemannian Optimization with Uncertain Target Steering Vector for Slow-Time Transmit Sequence of Cognitive Radar","abstract":"Optimization of slow-time transmit sequence endows cognitive radar with the ability to suppress strong clutter in the range-Doppler domain. However, in practice, inaccurate target velocity information or random phase error would induce uncertainty about the actual target steering vector, which would in turn severely deteriorate the the performance of the slow-time matched filter. In order to solve this problem, we propose a new optimization method for slow-time transmit sequence design. The proposed method transforms the original non-convex optimization with an uncertain target steering vector into a two-step worst-case optimization problem. For each sub-problem, we develop a corresponding trust-region Riemannian optimization algorithm. By iteratively solving the two sub-problems, a sub-optimal solution can be reached without accurate information about the target steering vector. Furthermore, the convergence property of the proposed algorithms has been analyzed and detailed proof of the convergence is given. Unlike the traditional waveform optimization method, the proposed method is designed to work with an uncertain target steering vector and therefore, is more robust in practical radar systems. Numerical simulation results in different scenarios verify the effectiveness of the proposed method in suppressing the clutter and show its advantages in terms of the output signal-to-clutter plus noise ratio (SCNR) over traditional methods.","sentences":["Optimization of slow-time transmit sequence endows cognitive radar with the ability to suppress strong clutter in the range-Doppler domain.","However, in practice, inaccurate target velocity information or random phase error would induce uncertainty about the actual target steering vector, which would in turn severely deteriorate the the performance of the slow-time matched filter.","In order to solve this problem, we propose a new optimization method for slow-time transmit sequence design.","The proposed method transforms the original non-convex optimization with an uncertain target steering vector into a two-step worst-case optimization problem.","For each sub-problem, we develop a corresponding trust-region Riemannian optimization algorithm.","By iteratively solving the two sub-problems, a sub-optimal solution can be reached without accurate information about the target steering vector.","Furthermore, the convergence property of the proposed algorithms has been analyzed and detailed proof of the convergence is given.","Unlike the traditional waveform optimization method, the proposed method is designed to work with an uncertain target steering vector and therefore, is more robust in practical radar systems.","Numerical simulation results in different scenarios verify the effectiveness of the proposed method in suppressing the clutter and show its advantages in terms of the output signal-to-clutter plus noise ratio (SCNR) over traditional methods."],"url":"http://arxiv.org/abs/2404.10388v1","category":"eess.SP"}
{"created":"2024-04-16 08:24:43","title":"Nonlinearity-enhanced quantum sensing in Stark probes","abstract":"Stark systems in which a linear gradient field is applied across a many-body system have recently been harnessed for quantum sensing. Here, we explore sensing capacity of Stark models, in both single-particle and many-body interacting systems, for estimating the strength of both linear and nonlinear Stark fields. The problem naturally lies in the context of multi-parameter estimation. We determine the phase diagram of the system in terms of both linear and nonlinear gradient fields showing how the extended phase turns into a localized one as the Stark fields increase. We also characterize the properties of the phase transition, including critical exponents, through a comprehesive finite-size scaling analysis. Interestingly, our results show that the estimation of both the linear and the nonlinear fields can achieve super-Heisenberg scaling. In fact, the scaling exponent of the sensing precision is directly proportional to the nonlinearity exponent which shows that nonlinearity enhances the estimation precision. Finally, we show that even after considering the cost of the preparation time the sensing precision still reveals super-Heisenberg scaling.","sentences":["Stark systems in which a linear gradient field is applied across a many-body system have recently been harnessed for quantum sensing.","Here, we explore sensing capacity of Stark models, in both single-particle and many-body interacting systems, for estimating the strength of both linear and nonlinear Stark fields.","The problem naturally lies in the context of multi-parameter estimation.","We determine the phase diagram of the system in terms of both linear and nonlinear gradient fields showing how the extended phase turns into a localized one as the Stark fields increase.","We also characterize the properties of the phase transition, including critical exponents, through a comprehesive finite-size scaling analysis.","Interestingly, our results show that the estimation of both the linear and the nonlinear fields can achieve super-Heisenberg scaling.","In fact, the scaling exponent of the sensing precision is directly proportional to the nonlinearity exponent which shows that nonlinearity enhances the estimation precision.","Finally, we show that even after considering the cost of the preparation time the sensing precision still reveals super-Heisenberg scaling."],"url":"http://arxiv.org/abs/2404.10382v1","category":"quant-ph"}
{"created":"2024-04-16 08:21:30","title":"PSPACE-Hard 2D Super Mario Games: Thirteen Doors","abstract":"We prove PSPACE-hardness for fifteen games in the Super Mario Bros. 2D platforming video game series. Previously, only the original Super Mario Bros. was known to be PSPACE-hard (FUN 2016), though several of the games we study were known to be NP-hard (FUN 2014). Our reductions build door gadgets with open, close, and traverse traversals, in each case using mechanics unique to the game. While some of our door constructions are similar to those from FUN 2016, those for Super Mario Bros. 2, Super Mario Land 2, Super Mario World 2, and the New Super Mario Bros. series are quite different; notably, the Super Mario Bros. 2 door is extremely difficult. Doors remain elusive for just two 2D Mario games (Super Mario Land and Super Mario Run); we prove that these games are at least NP-hard.","sentences":["We prove PSPACE-hardness for fifteen games in the Super Mario Bros. 2D platforming video game series.","Previously, only the original Super Mario Bros. was known to be PSPACE-hard (FUN 2016), though several of the games we study were known to be NP-hard (FUN 2014).","Our reductions build door gadgets with open, close, and traverse traversals, in each case using mechanics unique to the game.","While some of our door constructions are similar to those from FUN 2016, those for Super Mario Bros. 2, Super Mario Land 2, Super Mario World 2, and the New Super Mario Bros. series are quite different; notably, the Super Mario Bros. 2 door is extremely difficult.","Doors remain elusive for just two 2D Mario games (Super Mario Land and Super Mario Run); we prove that these games are at least NP-hard."],"url":"http://arxiv.org/abs/2404.10380v1","category":"cs.CC"}
{"created":"2024-04-16 08:13:13","title":"Hunting DeFi Vulnerabilities via Context-Sensitive Concolic Verification","abstract":"Decentralized finance (DeFi) is revolutionizing the traditional centralized finance paradigm with its attractive features such as high availability, transparency, and tamper-proofing. However, attacks targeting DeFi services have severely damaged the DeFi market, as evidenced by our investigation of 80 real-world DeFi incidents from 2017 to 2022. Existing methods, based on symbolic execution, model checking, semantic analysis, and fuzzing, fall short in identifying the most DeFi vulnerability types. To address the deficiency, we propose Context-Sensitive Concolic Verification (CSCV), a method of automating the DeFi vulnerability finding based on user-defined properties formulated in temporal logic. CSCV builds and optimizes contexts to guide verification processes that dynamically construct context-carrying transition systems in tandem with concolic executions. Furthermore, we demonstrate the effectiveness of CSCV through experiments on real-world DeFi services and qualitative comparison. The experiment results show that our CSCV prototype successfully detects 76.25% of the vulnerabilities from the investigated incidents with an average time of 253.06 seconds.","sentences":["Decentralized finance (DeFi) is revolutionizing the traditional centralized finance paradigm with its attractive features such as high availability, transparency, and tamper-proofing.","However, attacks targeting DeFi services have severely damaged the DeFi market, as evidenced by our investigation of 80 real-world DeFi incidents from 2017 to 2022.","Existing methods, based on symbolic execution, model checking, semantic analysis, and fuzzing, fall short in identifying the most DeFi vulnerability types.","To address the deficiency, we propose Context-Sensitive Concolic Verification (CSCV), a method of automating the DeFi vulnerability finding based on user-defined properties formulated in temporal logic.","CSCV builds and optimizes contexts to guide verification processes that dynamically construct context-carrying transition systems in tandem with concolic executions.","Furthermore, we demonstrate the effectiveness of CSCV through experiments on real-world DeFi services and qualitative comparison.","The experiment results show that our CSCV prototype successfully detects 76.25% of the vulnerabilities from the investigated incidents with an average time of 253.06 seconds."],"url":"http://arxiv.org/abs/2404.10376v1","category":"cs.SE"}
{"created":"2024-04-16 08:11:27","title":"A newly developed multi-kilo-channel high-speed and precision waveform digitization system for neutrino experiments","abstract":"The Jinping Neutrino Experiment(JNE), conducted within the China Jinping Underground Laboratory, aims to detect and analyze of solar neutrinos, geo-neutrinos, and supernova neutrinos. A one-ton prototype will soon be in commision with an upgrade from 30 channels to 60 channels, which will increase the data bandwidth by one to two orders of magnitude and exceed the capacity of the current CAEN DAQ system. Additionally, enhancing the performance and flexibility of JNE DAQ system is crucial. This paper presents the design of a new Tsinghua DAQ system for the JNE and its performance and stability. The new Tsinghua DAQ(THDAQ) system for JNE is based on the cPCI protocol and demonstrates powerful performance improvements: ADC ENOB of the THDAQ system approximately exceeds 9.8-bit, marking a 14% improvement over the CAEN DAQ system; The maximum clock deviation within a single chassis is 85.6 ps, satisfying sub-nanosecond synchronization criteria; Each DAQ board features two QSFP+ optical ports with 82.5Gbps transmission capability, while the PCIe board supports a transmission rate of 100.2 Gbps. In addition, comparative experiments between the two systems were also tested in detail. The analysis results of waveform and charge spectrum prove the high stability of the THDAQ system. This provides a foundation for the 60-channel and 4000-channel DAQ systems.","sentences":["The Jinping Neutrino Experiment(JNE), conducted within the China Jinping Underground Laboratory, aims to detect and analyze of solar neutrinos, geo-neutrinos, and supernova neutrinos.","A one-ton prototype will soon be in commision with an upgrade from 30 channels to 60 channels, which will increase the data bandwidth by one to two orders of magnitude and exceed the capacity of the current CAEN DAQ system.","Additionally, enhancing the performance and flexibility of JNE DAQ system is crucial.","This paper presents the design of a new Tsinghua DAQ system for the JNE and its performance and stability.","The new Tsinghua DAQ(THDAQ) system for JNE is based on the cPCI protocol and demonstrates powerful performance improvements: ADC ENOB of the THDAQ system approximately exceeds 9.8-bit, marking a 14% improvement over the CAEN DAQ system; The maximum clock deviation within a single chassis is 85.6 ps, satisfying sub-nanosecond synchronization criteria; Each DAQ board features two QSFP+ optical ports with 82.5Gbps transmission capability, while the PCIe board supports a transmission rate of 100.2 Gbps.","In addition, comparative experiments between the two systems were also tested in detail.","The analysis results of waveform and charge spectrum prove the high stability of the THDAQ system.","This provides a foundation for the 60-channel and 4000-channel DAQ systems."],"url":"http://arxiv.org/abs/2404.10373v1","category":"physics.ins-det"}
{"created":"2024-04-16 07:50:15","title":"Stampede Alert Clustering Algorithmic System Based on Tiny-Scale Strengthened DETR","abstract":"A novel crowd stampede detection and prediction algorithm based on Deformable DETR is proposed to address the challenges of detecting a large number of small targets and target occlusion in crowded airport and train station environments. In terms of model design, the algorithm incorporates a multi-scale feature fusion module to enlarge the receptive field and enhance the detection capability of small targets. Furthermore, the deformable attention mechanism is improved to reduce missed detections and false alarms for critical targets. Additionally, a new algorithm is innovatively introduced for stampede event prediction and visualization. Experimental evaluations on the PKX-LHR dataset demonstrate that the enhanced algorithm achieves a 34% performance in small target detection accuracy while maintaining the original detection speed.","sentences":["A novel crowd stampede detection and prediction algorithm based on Deformable DETR is proposed to address the challenges of detecting a large number of small targets and target occlusion in crowded airport and train station environments.","In terms of model design, the algorithm incorporates a multi-scale feature fusion module to enlarge the receptive field and enhance the detection capability of small targets.","Furthermore, the deformable attention mechanism is improved to reduce missed detections and false alarms for critical targets.","Additionally, a new algorithm is innovatively introduced for stampede event prediction and visualization.","Experimental evaluations on the PKX-LHR dataset demonstrate that the enhanced algorithm achieves a 34% performance in small target detection accuracy while maintaining the original detection speed."],"url":"http://arxiv.org/abs/2404.10359v1","category":"cs.SI"}
{"created":"2024-04-16 07:43:01","title":"AERO: Adaptive Erase Operation for Improving Lifetime and Performance of Modern NAND Flash-Based SSDs","abstract":"This work investigates a new erase scheme in NAND flash memory to improve the lifetime and performance of modern solid-state drives (SSDs). In NAND flash memory, an erase operation applies a high voltage (e.g., > 20 V) to flash cells for a long time (e.g., > 3.5 ms), which degrades cell endurance and potentially delays user I/O requests. While a large body of prior work has proposed various techniques to mitigate the negative impact of erase operations, no work has yet investigated how erase latency should be set to fully exploit the potential of NAND flash memory; most existing techniques use a fixed latency for every erase operation which is set to cover the worst-case operating conditions. To address this, we propose AERO (Adaptive ERase Operation), a new erase scheme that dynamically adjusts erase latency to be just long enough for reliably erasing target cells, depending on the cells' current erase characteristics. AERO accurately predicts such near-optimal erase latency based on the number of fail bits during an erase operation. To maximize its benefits, we further optimize AERO in two aspects. First, at the beginning of an erase operation, AERO attempts to erase the cells for a short time (e.g., 1 ms), which enables AERO to always obtain the number of fail bits necessary to accurately predict the near-optimal erase latency. Second, AERO aggressively yet safely reduces erase latency by leveraging a large reliability margin present in modern SSDs. We demonstrate the feasibility and reliability of AERO using 160 real 3D NAND flash chips, showing that it enhances SSD lifetime over the conventional erase scheme by 43% without change to existing NAND flash chips. Our system-level evaluation using eleven real-world workloads shows that an AERO-enabled SSD reduces read tail latency by 34% on average over a state-of-the-art technique.","sentences":["This work investigates a new erase scheme in NAND flash memory to improve the lifetime and performance of modern solid-state drives (SSDs).","In NAND flash memory, an erase operation applies a high voltage (e.g., > 20 V) to flash cells for a long time (e.g., > 3.5 ms), which degrades cell endurance and potentially delays user I/O requests.","While a large body of prior work has proposed various techniques to mitigate the negative impact of erase operations, no work has yet investigated how erase latency should be set to fully exploit the potential of NAND flash memory; most existing techniques use a fixed latency for every erase operation which is set to cover the worst-case operating conditions.","To address this, we propose AERO (Adaptive ERase Operation), a new erase scheme that dynamically adjusts erase latency to be just long enough for reliably erasing target cells, depending on the cells' current erase characteristics.","AERO accurately predicts such near-optimal erase latency based on the number of fail bits during an erase operation.","To maximize its benefits, we further optimize AERO in two aspects.","First, at the beginning of an erase operation, AERO attempts to erase the cells for a short time (e.g., 1 ms), which enables AERO to always obtain the number of fail bits necessary to accurately predict the near-optimal erase latency.","Second, AERO aggressively yet safely reduces erase latency by leveraging a large reliability margin present in modern SSDs.","We demonstrate the feasibility and reliability of AERO using 160 real 3D NAND flash chips, showing that it enhances SSD lifetime over the conventional erase scheme by 43% without change to existing NAND flash chips.","Our system-level evaluation using eleven real-world workloads shows that an AERO-enabled SSD reduces read tail latency by 34% on average over a state-of-the-art technique."],"url":"http://arxiv.org/abs/2404.10355v1","category":"cs.AR"}
{"created":"2024-04-16 07:39:52","title":"Optimal complexity solution of space-time finite element systems for state-based parabolic distributed optimal control problems","abstract":"We consider a distributed optimal control problem subject to a parabolic evolution equation as constraint. The control will be considered in the energy norm of the anisotropic Sobolev space $[H_{0;,0}^{1,1/2}(Q)]^\\ast$, such that the state equation of the partial differential equation defines an isomorphism onto $H^{1,1/2}_{0;0,}(Q)$. Thus, we can eliminate the control from the tracking type functional to be minimized, to derive the optimality system in order to determine the state. Since the appearing operator induces an equivalent norm in $H_{0;0,}^{1,1/2}(Q)$, we will replace it by a computable realization of the anisotropic Sobolev norm, using a modified Hilbert transformation. We are then able to link the cost or regularization parameter $\\varrho>0$ to the distance of the state and the desired target, solely depending on the regularity of the target. For a conforming space-time finite element discretization, this behavior carries over to the discrete setting, leading to an optimal choice $\\varrho = h_x^2$ of the regularization parameter $\\varrho$ to the spatial finite element mesh size $h_x$. Using a space-time tensor product mesh, error estimates for the distance of the computable state to the desired target are derived. The main advantage of this new approach is, that applying sparse factorization techniques, a solver of optimal, i.e., almost linear, complexity is proposed and analyzed. The theoretical results are complemented by numerical examples, including discontinuous and less regular targets. Moreover, this approach can be applied also to optimal control problems subject to non-linear state equations.","sentences":["We consider a distributed optimal control problem subject to a parabolic evolution equation as constraint.","The control will be considered in the energy norm of the anisotropic Sobolev space $","[H_{0;,0}^{1,1/2}(Q)]^\\ast$, such that the state equation of the partial differential equation defines an isomorphism onto $H^{1,1/2}_{0;0,}(Q)$. Thus, we can eliminate the control from the tracking type functional to be minimized, to derive the optimality system in order to determine the state.","Since the appearing operator induces an equivalent norm in $H_{0;0,}^{1,1/2}(Q)$, we will replace it by a computable realization of the anisotropic Sobolev norm, using a modified Hilbert transformation.","We are then able to link the cost or regularization parameter $\\varrho>0$ to the distance of the state and the desired target, solely depending on the regularity of the target.","For a conforming space-time finite element discretization, this behavior carries over to the discrete setting, leading to an optimal choice $\\varrho = h_x^2$ of the regularization parameter $\\varrho$ to the spatial finite element mesh size $h_x$. Using a space-time tensor product mesh, error estimates for the distance of the computable state to the desired target are derived.","The main advantage of this new approach is, that applying sparse factorization techniques, a solver of optimal, i.e., almost linear, complexity is proposed and analyzed.","The theoretical results are complemented by numerical examples, including discontinuous and less regular targets.","Moreover, this approach can be applied also to optimal control problems subject to non-linear state equations."],"url":"http://arxiv.org/abs/2404.10350v1","category":"math.NA"}
{"created":"2024-04-16 07:29:50","title":"1D solutions for compressible two-phase flows in a heated and cooled duct: mechanical equilibrium","abstract":"Analytical/quasi-analytical solutions are proposed for a steady, compressible, two-phase flow in mechanical equilibrium in a rectilinear duct subjected to heating followed by cooling. The flow is driven by the pressure ratio between a variable outlet pressure and an upstream tank. A critical pressure ratio distinguishes subsonic and supersonic outlet regimes: the article proposes a methodology to determine the full flow behaviour, as a function of pressure ratio and heat-flux distribution. Going forward, these analytical reference solutions will help validate numerical codes for more complex industrial applications. Specific results are studied for a mixture of liquid water and water vapour.","sentences":["Analytical/quasi-analytical solutions are proposed for a steady, compressible, two-phase flow in mechanical equilibrium in a rectilinear duct subjected to heating followed by cooling.","The flow is driven by the pressure ratio between a variable outlet pressure and an upstream tank.","A critical pressure ratio distinguishes subsonic and supersonic outlet regimes: the article proposes a methodology to determine the full flow behaviour, as a function of pressure ratio and heat-flux distribution.","Going forward, these analytical reference solutions will help validate numerical codes for more complex industrial applications.","Specific results are studied for a mixture of liquid water and water vapour."],"url":"http://arxiv.org/abs/2404.10345v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 07:25:17","title":"Referring Flexible Image Restoration","abstract":"In reality, images often exhibit multiple degradations, such as rain and fog at night (triple degradations). However, in many cases, individuals may not want to remove all degradations, for instance, a blurry lens revealing a beautiful snowy landscape (double degradations). In such scenarios, people may only desire to deblur. These situations and requirements shed light on a new challenge in image restoration, where a model must perceive and remove specific degradation types specified by human commands in images with multiple degradations. We term this task Referring Flexible Image Restoration (RFIR). To address this, we first construct a large-scale synthetic dataset called RFIR, comprising 153,423 samples with the degraded image, text prompt for specific degradation removal and restored image. RFIR consists of five basic degradation types: blur, rain, haze, low light and snow while six main sub-categories are included for varying degrees of degradation removal. To tackle the challenge, we propose a novel transformer-based multi-task model named TransRFIR, which simultaneously perceives degradation types in the degraded image and removes specific degradation upon text prompt. TransRFIR is based on two devised attention modules, Multi-Head Agent Self-Attention (MHASA) and Multi-Head Agent Cross Attention (MHACA), where MHASA and MHACA introduce the agent token and reach the linear complexity, achieving lower computation cost than vanilla self-attention and cross-attention and obtaining competitive performances. Our TransRFIR achieves state-of-the-art performances compared with other counterparts and is proven as an effective architecture for image restoration. We release our project at https://github.com/GuanRunwei/FIR-CP.","sentences":["In reality, images often exhibit multiple degradations, such as rain and fog at night (triple degradations).","However, in many cases, individuals may not want to remove all degradations, for instance, a blurry lens revealing a beautiful snowy landscape (double degradations).","In such scenarios, people may only desire to deblur.","These situations and requirements shed light on a new challenge in image restoration, where a model must perceive and remove specific degradation types specified by human commands in images with multiple degradations.","We term this task Referring Flexible Image Restoration (RFIR).","To address this, we first construct a large-scale synthetic dataset called RFIR, comprising 153,423 samples with the degraded image, text prompt for specific degradation removal and restored image.","RFIR consists of five basic degradation types: blur, rain, haze, low light and snow while six main sub-categories are included for varying degrees of degradation removal.","To tackle the challenge, we propose a novel transformer-based multi-task model named TransRFIR, which simultaneously perceives degradation types in the degraded image and removes specific degradation upon text prompt.","TransRFIR is based on two devised attention modules, Multi-Head Agent Self-Attention (MHASA) and Multi-Head Agent Cross Attention (MHACA), where MHASA and MHACA introduce the agent token and reach the linear complexity, achieving lower computation cost than vanilla self-attention and cross-attention and obtaining competitive performances.","Our TransRFIR achieves state-of-the-art performances compared with other counterparts and is proven as an effective architecture for image restoration.","We release our project at https://github.com/GuanRunwei/FIR-CP."],"url":"http://arxiv.org/abs/2404.10342v1","category":"cs.CV"}
{"created":"2024-04-16 07:22:05","title":"Dimension reduction in quantum sampling of stochastic processes","abstract":"Quantum technologies offer a promising route to the efficient sampling and analysis of stochastic processes, with potential applications across the sciences. Such quantum advantages rely on the preparation of a quantum sample state of the stochastic process, which requires a memory system to propagate correlations between the past and future of the process. Here, we introduce a method of lossy quantum dimension reduction that allows this memory to be compressed, not just beyond classical limits, but also beyond current state-of-the-art quantum stochastic sampling approaches. We investigate the trade-off between the saving in memory resources from this compression, and the distortion it introduces. We show that our approach can be highly effective in low distortion compression of both Markovian and strongly non-Markovian processes alike. We further discuss the application of our results to quantum stochastic modelling more broadly.","sentences":["Quantum technologies offer a promising route to the efficient sampling and analysis of stochastic processes, with potential applications across the sciences.","Such quantum advantages rely on the preparation of a quantum sample state of the stochastic process, which requires a memory system to propagate correlations between the past and future of the process.","Here, we introduce a method of lossy quantum dimension reduction that allows this memory to be compressed, not just beyond classical limits, but also beyond current state-of-the-art quantum stochastic sampling approaches.","We investigate the trade-off between the saving in memory resources from this compression, and the distortion it introduces.","We show that our approach can be highly effective in low distortion compression of both Markovian and strongly non-Markovian processes alike.","We further discuss the application of our results to quantum stochastic modelling more broadly."],"url":"http://arxiv.org/abs/2404.10338v1","category":"quant-ph"}
{"created":"2024-04-16 07:19:25","title":"Spin Hall Nano-Oscillator Empirical Electrical Model for Optimal On-chip Detector Design","abstract":"As nascent nonlinear oscillators, nano-constriction spin Hall nano-oscillators (SHNOs) represent a promising potential for integration into more complicated systems such as neural networks, magnetic field sensors, and radio frequency (RF) signal classification, their tunable high-frequency operating regime, easy synchronization, and CMOS compatibility can streamline the process. To implement SHNOs in any of these networks, the electrical features of a single device are needed before designing the signal detection CMOS circuitry. This study centers on presenting an empirical electrical model of the SHNO based on a comprehensive characterization of the output impedance of a single SHNO, and its available output power in the range of 2-10 GHz at various bias currents.","sentences":["As nascent nonlinear oscillators, nano-constriction spin Hall nano-oscillators (SHNOs) represent a promising potential for integration into more complicated systems such as neural networks, magnetic field sensors, and radio frequency (RF) signal classification, their tunable high-frequency operating regime, easy synchronization, and CMOS compatibility can streamline the process.","To implement SHNOs in any of these networks, the electrical features of a single device are needed before designing the signal detection CMOS circuitry.","This study centers on presenting an empirical electrical model of the SHNO based on a comprehensive characterization of the output impedance of a single SHNO, and its available output power in the range of 2-10 GHz at various bias currents."],"url":"http://arxiv.org/abs/2404.10334v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-16 07:08:04","title":"Graph neural network-based surrogate modelling for real-time hydraulic prediction of urban drainage networks","abstract":"Physics-based models are computationally time-consuming and infeasible for real-time scenarios of urban drainage networks, and a surrogate model is needed to accelerate the online predictive modelling. Fully-connected neural networks (NNs) are potential surrogate models, but may suffer from low interpretability and efficiency in fitting complex targets. Owing to the state-of-the-art modelling power of graph neural networks (GNNs) and their match with urban drainage networks in the graph structure, this work proposes a GNN-based surrogate of the flow routing model for the hydraulic prediction problem of drainage networks, which regards recent hydraulic states as initial conditions, and future runoff and control policy as boundary conditions. To incorporate hydraulic constraints and physical relationships into drainage modelling, physics-guided mechanisms are designed on top of the surrogate model to restrict the prediction variables with flow balance and flooding occurrence constraints. According to case results in a stormwater network, the GNN-based model is more cost-effective with better hydraulic prediction accuracy than the NN-based model after equal training epochs, and the designed mechanisms further limit prediction errors with interpretable domain knowledge. As the model structure adheres to the flow routing mechanisms and hydraulic constraints in urban drainage networks, it provides an interpretable and effective solution for data-driven surrogate modelling. Simultaneously, the surrogate model accelerates the predictive modelling of urban drainage networks for real-time use compared with the physics-based model.","sentences":["Physics-based models are computationally time-consuming and infeasible for real-time scenarios of urban drainage networks, and a surrogate model is needed to accelerate the online predictive modelling.","Fully-connected neural networks (NNs) are potential surrogate models, but may suffer from low interpretability and efficiency in fitting complex targets.","Owing to the state-of-the-art modelling power of graph neural networks (GNNs) and their match with urban drainage networks in the graph structure, this work proposes a GNN-based surrogate of the flow routing model for the hydraulic prediction problem of drainage networks, which regards recent hydraulic states as initial conditions, and future runoff and control policy as boundary conditions.","To incorporate hydraulic constraints and physical relationships into drainage modelling, physics-guided mechanisms are designed on top of the surrogate model to restrict the prediction variables with flow balance and flooding occurrence constraints.","According to case results in a stormwater network, the GNN-based model is more cost-effective with better hydraulic prediction accuracy than the NN-based model after equal training epochs, and the designed mechanisms further limit prediction errors with interpretable domain knowledge.","As the model structure adheres to the flow routing mechanisms and hydraulic constraints in urban drainage networks, it provides an interpretable and effective solution for data-driven surrogate modelling.","Simultaneously, the surrogate model accelerates the predictive modelling of urban drainage networks for real-time use compared with the physics-based model."],"url":"http://arxiv.org/abs/2404.10324v1","category":"cs.LG"}
{"created":"2024-04-16 07:05:16","title":"Cluster-based Graph Collaborative Filtering","abstract":"Graph Convolution Networks (GCNs) have significantly succeeded in learning user and item representations for recommendation systems. The core of their efficacy is the ability to explicitly exploit the collaborative signals from both the first- and high-order neighboring nodes. However, most existing GCN-based methods overlook the multiple interests of users while performing high-order graph convolution. Thus, the noisy information from unreliable neighbor nodes (e.g., users with dissimilar interests) negatively impacts the representation learning of the target node. Additionally, conducting graph convolution operations without differentiating high-order neighbors suffers the over-smoothing issue when stacking more layers, resulting in performance degradation. In this paper, we aim to capture more valuable information from high-order neighboring nodes while avoiding noise for better representation learning of the target node. To achieve this goal, we propose a novel GCN-based recommendation model, termed Cluster-based Graph Collaborative Filtering (ClusterGCF). This model performs high-order graph convolution on cluster-specific graphs, which are constructed by capturing the multiple interests of users and identifying the common interests among them. Specifically, we design an unsupervised and optimizable soft node clustering approach to classify user and item nodes into multiple clusters. Based on the soft node clustering results and the topology of the user-item interaction graph, we assign the nodes with probabilities for different clusters to construct the cluster-specific graphs. To evaluate the effectiveness of ClusterGCF, we conducted extensive experiments on four publicly available datasets. Experimental results demonstrate that our model can significantly improve recommendation performance.","sentences":["Graph Convolution Networks (GCNs) have significantly succeeded in learning user and item representations for recommendation systems.","The core of their efficacy is the ability to explicitly exploit the collaborative signals from both the first- and high-order neighboring nodes.","However, most existing GCN-based methods overlook the multiple interests of users while performing high-order graph convolution.","Thus, the noisy information from unreliable neighbor nodes (e.g., users with dissimilar interests) negatively impacts the representation learning of the target node.","Additionally, conducting graph convolution operations without differentiating high-order neighbors suffers the over-smoothing issue when stacking more layers, resulting in performance degradation.","In this paper, we aim to capture more valuable information from high-order neighboring nodes while avoiding noise for better representation learning of the target node.","To achieve this goal, we propose a novel GCN-based recommendation model, termed Cluster-based Graph Collaborative Filtering (ClusterGCF).","This model performs high-order graph convolution on cluster-specific graphs, which are constructed by capturing the multiple interests of users and identifying the common interests among them.","Specifically, we design an unsupervised and optimizable soft node clustering approach to classify user and item nodes into multiple clusters.","Based on the soft node clustering results and the topology of the user-item interaction graph, we assign the nodes with probabilities for different clusters to construct the cluster-specific graphs.","To evaluate the effectiveness of ClusterGCF, we conducted extensive experiments on four publicly available datasets.","Experimental results demonstrate that our model can significantly improve recommendation performance."],"url":"http://arxiv.org/abs/2404.10321v1","category":"cs.IR"}
{"created":"2024-04-16 06:40:43","title":"Metamaterial-inspired Wearable Pad for Enhancing EM Coupling with Biological Tissues","abstract":"Wearable, implantable, and ingestible antennas are continuously evolving in biomedical applications, as they are crucial components in devices used for monitoring and controlling physiological parameters. This work presents an experimentally validated wearable pad which can improve transmission of electromagnetic waves into the human body. This metamaterial-inspired matching pad, which is based on small metallic loops encased in a thin dielectric layer, is mechanically stable, flexible, and passive. As such, the pad can serve as a coupling medium for microwave medical systems and implantable device communication. Operating in the 2.4-2.5 GHz range, the pad demonstrates significant improvement in signal penetration levels (and, hence, depth) into a biological tissue. The study presents design methodology, simulation studies, in-lab development, and experimental characterization of this pad, which can offer a practical solution for enhanced communication and functionality in various medical diagnostic systems.","sentences":["Wearable, implantable, and ingestible antennas are continuously evolving in biomedical applications, as they are crucial components in devices used for monitoring and controlling physiological parameters.","This work presents an experimentally validated wearable pad which can improve transmission of electromagnetic waves into the human body.","This metamaterial-inspired matching pad, which is based on small metallic loops encased in a thin dielectric layer, is mechanically stable, flexible, and passive.","As such, the pad can serve as a coupling medium for microwave medical systems and implantable device communication.","Operating in the 2.4-2.5 GHz range, the pad demonstrates significant improvement in signal penetration levels (and, hence, depth) into a biological tissue.","The study presents design methodology, simulation studies, in-lab development, and experimental characterization of this pad, which can offer a practical solution for enhanced communication and functionality in various medical diagnostic systems."],"url":"http://arxiv.org/abs/2404.10313v1","category":"eess.SY"}
{"created":"2024-04-16 06:37:19","title":"Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A Deep Learning Approach","abstract":"Several therapy routines require deep breathing exercises as a key component and patients undergoing such therapies must perform these exercises regularly. Assessing the outcome of a therapy and tailoring its course necessitates monitoring a patient's compliance with the therapy. While therapy compliance monitoring is routine in a clinical environment, it is challenging to do in an at-home setting. This is so because a home setting lacks access to specialized equipment and skilled professionals needed to effectively monitor the performance of a therapy routine by a patient. For some types of therapies, these challenges can be addressed with the use of consumer-grade hardware, such as earphones and smartphones, as practical solutions. To accurately monitor breathing exercises using wireless earphones, this paper proposes a framework that has the potential for assessing a patient's compliance with an at-home therapy. The proposed system performs real-time detection of breathing phases and channels with high accuracy by processing a $\\mathbf{500}$ ms audio signal through two convolutional neural networks. The first network, called a channel classifier, distinguishes between nasal and oral breathing, and a pause. The second network, called a phase classifier, determines whether the audio segment is from inhalation or exhalation. According to $k$-fold cross-validation, the channel and phase classifiers achieved a maximum F1 score of $\\mathbf{97.99\\%}$ and $\\mathbf{89.46\\%}$, respectively. The results demonstrate the potential of using commodity earphones for real-time breathing channel and phase detection for breathing therapy compliance monitoring.","sentences":["Several therapy routines require deep breathing exercises as a key component and patients undergoing such therapies must perform these exercises regularly.","Assessing the outcome of a therapy and tailoring its course necessitates monitoring a patient's compliance with the therapy.","While therapy compliance monitoring is routine in a clinical environment, it is challenging to do in an at-home setting.","This is so because a home setting lacks access to specialized equipment and skilled professionals needed to effectively monitor the performance of a therapy routine by a patient.","For some types of therapies, these challenges can be addressed with the use of consumer-grade hardware, such as earphones and smartphones, as practical solutions.","To accurately monitor breathing exercises using wireless earphones, this paper proposes a framework that has the potential for assessing a patient's compliance with an at-home therapy.","The proposed system performs real-time detection of breathing phases and channels with high accuracy by processing a $\\mathbf{500}$ ms audio signal through two convolutional neural networks.","The first network, called a channel classifier, distinguishes between nasal and oral breathing, and a pause.","The second network, called a phase classifier, determines whether the audio segment is from inhalation or exhalation.","According to $k$-fold cross-validation, the channel and phase classifiers achieved a maximum F1 score of $\\mathbf{97.99\\%}$ and $\\mathbf{89.46\\%}$, respectively.","The results demonstrate the potential of using commodity earphones for real-time breathing channel and phase detection for breathing therapy compliance monitoring."],"url":"http://arxiv.org/abs/2404.10310v1","category":"eess.AS"}
{"created":"2024-04-16 06:34:49","title":"Non-perturbative correction to thermodynamics of conformally dressed 3D black hole","abstract":"We extend the study of corrected thermodynamics for the 3D black holes conformally coupled to scalar field up to non-perturbative level. We calculate the exponential correction to entropy arises due to the microstate counting for quantum states on the boundary. This exponential correction in entropy attributes to the other thermodynamical quantities also. We study the stability and phase transition for this system of black hole under the influence of non-perturbative correction. We also discuss the quantum work associated with exponential corrected entropy. Finally, we justify the results from the view point of thermodynamic geometry.","sentences":["We extend the study of corrected thermodynamics for the 3D black holes conformally coupled to scalar field up to non-perturbative level.","We calculate the exponential correction to entropy arises due to the microstate counting for quantum states on the boundary.","This exponential correction in entropy attributes to the other thermodynamical quantities also.","We study the stability and phase transition for this system of black hole under the influence of non-perturbative correction.","We also discuss the quantum work associated with exponential corrected entropy.","Finally, we justify the results from the view point of thermodynamic geometry."],"url":"http://arxiv.org/abs/2404.10309v1","category":"gr-qc"}
{"created":"2024-04-16 06:24:53","title":"TC-OCR: TableCraft OCR for Efficient Detection & Recognition of Table Structure & Content","abstract":"The automatic recognition of tabular data in document images presents a significant challenge due to the diverse range of table styles and complex structures. Tables offer valuable content representation, enhancing the predictive capabilities of various systems such as search engines and Knowledge Graphs. Addressing the two main problems, namely table detection (TD) and table structure recognition (TSR), has traditionally been approached independently. In this research, we propose an end-to-end pipeline that integrates deep learning models, including DETR, CascadeTabNet, and PP OCR v2, to achieve comprehensive image-based table recognition. This integrated approach effectively handles diverse table styles, complex structures, and image distortions, resulting in improved accuracy and efficiency compared to existing methods like Table Transformers. Our system achieves simultaneous table detection (TD), table structure recognition (TSR), and table content recognition (TCR), preserving table structures and accurately extracting tabular data from document images. The integration of multiple models addresses the intricacies of table recognition, making our approach a promising solution for image-based table understanding, data extraction, and information retrieval applications. Our proposed approach achieves an IOU of 0.96 and an OCR Accuracy of 78%, showcasing a remarkable improvement of approximately 25% in the OCR Accuracy compared to the previous Table Transformer approach.","sentences":["The automatic recognition of tabular data in document images presents a significant challenge due to the diverse range of table styles and complex structures.","Tables offer valuable content representation, enhancing the predictive capabilities of various systems such as search engines and Knowledge Graphs.","Addressing the two main problems, namely table detection (TD) and table structure recognition (TSR), has traditionally been approached independently.","In this research, we propose an end-to-end pipeline that integrates deep learning models, including DETR, CascadeTabNet, and PP OCR v2, to achieve comprehensive image-based table recognition.","This integrated approach effectively handles diverse table styles, complex structures, and image distortions, resulting in improved accuracy and efficiency compared to existing methods like Table Transformers.","Our system achieves simultaneous table detection (TD), table structure recognition (TSR), and table content recognition (TCR), preserving table structures and accurately extracting tabular data from document images.","The integration of multiple models addresses the intricacies of table recognition, making our approach a promising solution for image-based table understanding, data extraction, and information retrieval applications.","Our proposed approach achieves an IOU of 0.96 and an OCR Accuracy of 78%, showcasing a remarkable improvement of approximately 25% in the OCR Accuracy compared to the previous Table Transformer approach."],"url":"http://arxiv.org/abs/2404.10305v1","category":"cs.CV"}
{"created":"2024-04-16 06:20:06","title":"LLM-Powered Test Case Generation for Detecting Tricky Bugs","abstract":"Conventional automated test generation tools struggle to generate test oracles and tricky bug-revealing test inputs. Large Language Models (LLMs) can be prompted to produce test inputs and oracles for a program directly, but the precision of the tests can be very low for complex scenarios (only 6.3% based on our experiments). To fill this gap, this paper proposes AID, which combines LLMs with differential testing to generate fault-revealing test inputs and oracles targeting plausibly correct programs (i.e., programs that have passed all the existing tests). In particular, AID selects test inputs that yield diverse outputs on a set of program variants generated by LLMs, then constructs the test oracle based on the outputs. We evaluate AID on two large-scale datasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three state-of-the-art baselines. The evaluation results show that the recall, precision, and F1 score of AID outperform the state-of-the-art by up to 1.80x, 2.65x, and 1.66x, respectively.","sentences":["Conventional automated test generation tools struggle to generate test oracles and tricky bug-revealing test inputs.","Large Language Models (LLMs) can be prompted to produce test inputs and oracles for a program directly, but the precision of the tests can be very low for complex scenarios (only 6.3% based on our experiments).","To fill this gap, this paper proposes AID, which combines LLMs with differential testing to generate fault-revealing test inputs and oracles targeting plausibly correct programs (i.e., programs that have passed all the existing tests).","In particular, AID selects test inputs that yield diverse outputs on a set of program variants generated by LLMs, then constructs the test oracle based on the outputs.","We evaluate AID on two large-scale datasets with tricky bugs: TrickyBugs and EvalPlus, and compare it with three state-of-the-art baselines.","The evaluation results show that the recall, precision, and F1 score of AID outperform the state-of-the-art by up to 1.80x, 2.65x, and 1.66x, respectively."],"url":"http://arxiv.org/abs/2404.10304v1","category":"cs.SE"}
{"created":"2024-04-16 06:10:38","title":"General Relativistic Approach to the Vis-viva Equation on Schwarzschild Metric","abstract":"A modification to the vis-viva equation that accounts for general relativistic effects is introduced to enhance the accuracy of predictions of orbital motion and precession. The updated equation reduces to the traditional vis-viva equation under Newtonian conditions and is a more accurate tool for astrodynamics than the traditional equation. Preliminary simulation results demonstrate the application potential of the modified vis-viva equation for more complex n-body systems. Spherical symmetry is assumed in this approach; however, this limitation could be removed in future research. This study is a pivotal step toward bridging classical and relativistic mechanics and thus makes an important contribution to the field of celestial dynamics.","sentences":["A modification to the vis-viva equation that accounts for general relativistic effects is introduced to enhance the accuracy of predictions of orbital motion and precession.","The updated equation reduces to the traditional vis-viva equation under Newtonian conditions and is a more accurate tool for astrodynamics than the traditional equation.","Preliminary simulation results demonstrate the application potential of the modified vis-viva equation for more complex n-body systems.","Spherical symmetry is assumed in this approach; however, this limitation could be removed in future research.","This study is a pivotal step toward bridging classical and relativistic mechanics and thus makes an important contribution to the field of celestial dynamics."],"url":"http://arxiv.org/abs/2404.10302v1","category":"gr-qc"}
{"created":"2024-04-16 05:33:49","title":"Topological Fukaya category of tagged arcs","abstract":"A tagged arc on a surface is introduced by Fomin, Shapiro, and Thurston to study cluster theory on marked surfaces. Given a tagged arc system on a graded marked surface, we define its $\\mathbb{Z}$-graded $\\mathcal{A}_\\infty$-category, generalizing the construction of Haiden, Katzarkov, and Kontsevich for arc systems. When a tagged arc system arises from a non-trivial involution on a marked surface, we show that this $\\mathcal{A}_\\infty$-category is quasi-isomorphic to the invariant part of the topological Fukaya category under the involution. In particular, this identifies tagged arcs with non-geometric idempotents of Fukaya category.","sentences":["A tagged arc on a surface is introduced by Fomin, Shapiro, and Thurston to study cluster theory on marked surfaces.","Given a tagged arc system on a graded marked surface, we define its $\\mathbb{Z}$-graded $\\mathcal{A}_\\infty$-category, generalizing the construction of Haiden, Katzarkov, and Kontsevich for arc systems.","When a tagged arc system arises from a non-trivial involution on a marked surface, we show that this $\\mathcal{A}_\\infty$-category is quasi-isomorphic to the invariant part of the topological Fukaya category under the involution.","In particular, this identifies tagged arcs with non-geometric idempotents of Fukaya category."],"url":"http://arxiv.org/abs/2404.10294v1","category":"math.SG"}
{"created":"2024-04-16 05:20:30","title":"A novel scheme for modelling dissipation or thermalization in open quantum systems","abstract":"In this letter, we introduce a novel method for investigating dissipation or thermalization in an open quantum system. In this method, the quantum system is coupled linearly with a copy of itself or with another system described by a finite number of bosonic operators. The time-dependent coupling functions play a fundamental role in this scheme. To demonstrate the efficacy and significance of this method, we apply it to examine several important and ubiquitous open quantum systems. Firstly, we investigate a quantum oscillator in the presence of a thermal bath at the inverse temperature $\\beta$, obtaining the reduced density matrix, the Husimi distribution function, and the quantum heat distribution function accurately. The results are consistent with existing literature by appropriate choices for the time-dependent coupling function. To illustrate the generalizability of this method to systems interacting with multiple thermal baths, we study the interaction of a quantum oscillator with two thermal baths at different temperatures and obtain compatible results. Subsequently, we analyze a two-level atom with energy or phase dissipation and derive the spontaneous emission and the pure dephasing processes consistently using the new method. Finally, we investigate Markovian and non-Markovian processes in a dissipative two-level atom and observe that these processes depend on the coupling strength $g_0$, and the non-Markovian property increases with an increase in $g_0$.","sentences":["In this letter, we introduce a novel method for investigating dissipation or thermalization in an open quantum system.","In this method, the quantum system is coupled linearly with a copy of itself or with another system described by a finite number of bosonic operators.","The time-dependent coupling functions play a fundamental role in this scheme.","To demonstrate the efficacy and significance of this method, we apply it to examine several important and ubiquitous open quantum systems.","Firstly, we investigate a quantum oscillator in the presence of a thermal bath at the inverse temperature $\\beta$, obtaining the reduced density matrix, the Husimi distribution function, and the quantum heat distribution function accurately.","The results are consistent with existing literature by appropriate choices for the time-dependent coupling function.","To illustrate the generalizability of this method to systems interacting with multiple thermal baths, we study the interaction of a quantum oscillator with two thermal baths at different temperatures and obtain compatible results.","Subsequently, we analyze a two-level atom with energy or phase dissipation and derive the spontaneous emission and the pure dephasing processes consistently using the new method.","Finally, we investigate Markovian and non-Markovian processes in a dissipative two-level atom and observe that these processes depend on the coupling strength $g_0$, and the non-Markovian property increases with an increase in $g_0$."],"url":"http://arxiv.org/abs/2404.10286v1","category":"quant-ph"}
{"created":"2024-04-16 05:04:31","title":"Two system transformation data-driven algorithms for linear quadratic mean-field games","abstract":"This paper studies a class of continuous-time linear quadratic (LQ) mean-field game problems. We develop two system transformation data-driven algorithms to approximate the decentralized strategies of the LQ mean-field games. The main feature of the obtained data-driven algorithms is that they eliminate the requirement on all system matrices. First, we transform the original stochastic system into an ordinary differential equation (ODE). Subsequently, we construct some Kronecker product-based matrices by the input/state data of the ODE. By virtue of these matrices, we implement a model-based policy iteration (PI) algorithm and a model-based value iteration (VI) algorithm in a data-driven fashion. In addition, we also demonstrate the convergence of these two data-driven algorithms under some mild conditions. Finally, we illustrate the practicality of our algorithms via two numerical examples.","sentences":["This paper studies a class of continuous-time linear quadratic (LQ) mean-field game problems.","We develop two system transformation data-driven algorithms to approximate the decentralized strategies of the LQ mean-field games.","The main feature of the obtained data-driven algorithms is that they eliminate the requirement on all system matrices.","First, we transform the original stochastic system into an ordinary differential equation (ODE).","Subsequently, we construct some Kronecker product-based matrices by the input/state data of the ODE.","By virtue of these matrices, we implement a model-based policy iteration (PI) algorithm and a model-based value iteration (VI) algorithm in a data-driven fashion.","In addition, we also demonstrate the convergence of these two data-driven algorithms under some mild conditions.","Finally, we illustrate the practicality of our algorithms via two numerical examples."],"url":"http://arxiv.org/abs/2404.10285v1","category":"math.OC"}
{"created":"2024-04-16 04:35:05","title":"The semi-classical saddles in three-dimensional gravity via holography and mini-superspace approach","abstract":"We determine the complex geometries dual to the semi-classical saddles in three-dimensional gravity with positive or negative cosmological constant. We examine the semi-classical saddles in Liouville field theory and interpret them in terms of gravity theory. For this, we describe the gravity theory by Chern Simons theory and classify the possible saddles based on homotopy group argument. We further realize the semi-classical saddles using the mini-superspace model of quantum gravity and explicitly determine the integral contour. In the case of positive cosmological constant, we recovered the geometry used for no-boundary proposal of Hartle and Hawking. In the case of negative cosmological constant, the geometry can be identified with Euclidean anti-de Sitter space attached with imaginary radius spheres. The geometry should be unphysical and several arguments on this issue are provided. Partial results were already presented in our earlier letter, and more detailed derivations and explanations on the results are given along with additional results. In particular, we reproduce the classical Liouville action from the Chern-Simons formulation of dual gravity theory.","sentences":["We determine the complex geometries dual to the semi-classical saddles in three-dimensional gravity with positive or negative cosmological constant.","We examine the semi-classical saddles in Liouville field theory and interpret them in terms of gravity theory.","For this, we describe the gravity theory by Chern Simons theory and classify the possible saddles based on homotopy group argument.","We further realize the semi-classical saddles using the mini-superspace model of quantum gravity and explicitly determine the integral contour.","In the case of positive cosmological constant, we recovered the geometry used for no-boundary proposal of Hartle and Hawking.","In the case of negative cosmological constant, the geometry can be identified with Euclidean anti-de Sitter space attached with imaginary radius spheres.","The geometry should be unphysical and several arguments on this issue are provided.","Partial results were already presented in our earlier letter, and more detailed derivations and explanations on the results are given along with additional results.","In particular, we reproduce the classical Liouville action from the Chern-Simons formulation of dual gravity theory."],"url":"http://arxiv.org/abs/2404.10277v1","category":"hep-th"}
{"created":"2024-04-16 04:06:53","title":"Algebraically primitive invariant subvarieties with quadratic field of definition","abstract":"We show that the only algebraically primitive invariant subvarieties of strata of translation surfaces with quadratic field of definition are the decagon, Weierstrass curves, and eigenform loci in genus two and the rank two example in the minimal stratum of genus four translation surfaces discovered by Eskin-McMullen-Mukamel-Wright.","sentences":["We show that the only algebraically primitive invariant subvarieties of strata of translation surfaces with quadratic field of definition are the decagon, Weierstrass curves, and eigenform loci in genus two and the rank two example in the minimal stratum of genus four translation surfaces discovered by Eskin-McMullen-Mukamel-Wright."],"url":"http://arxiv.org/abs/2404.10273v1","category":"math.DS"}
{"created":"2024-04-16 03:37:25","title":"Irreducible components in Hochschild cohomology of flag varieties","abstract":"Let $G$ be a simple, simply-connected complex algebraic group with Lie algebra $\\mathfrak{g}$, and $G/B$ the associated complete flag variety. The Hochschild cohomology $HH^\\bullet(G/B)$ is a geometric invariant of the flag variety related to its generalized deformation theory and has the structure of a $\\mathfrak{g}$-module. We study this invariant via representation-theoretic methods; in particular, we give a complete list of irreducible subrepresentations in $HH^\\bullet(G/B)$ when $G=SL_n(\\mathbb{C})$ or is of exceptional type (and conjecturally for all types) along with nontrivial lower bounds on their multiplicities. These results follow from a conjecture due to Kostant on the structure of the tensor product representation $V(\\rho) \\otimes V(\\rho)$.","sentences":["Let $G$ be a simple, simply-connected complex algebraic group with Lie algebra $\\mathfrak{g}$, and $G/B$ the associated complete flag variety.","The Hochschild cohomology $HH^\\bullet(G/B)$ is a geometric invariant of the flag variety related to its generalized deformation theory and has the structure of a $\\mathfrak{g}$-module.","We study this invariant via representation-theoretic methods; in particular, we give a complete list of irreducible subrepresentations in $HH^\\bullet(G/B)$ when $G=SL_n(\\mathbb{C})$ or is of exceptional type (and conjecturally for all types) along with nontrivial lower bounds on their multiplicities.","These results follow from a conjecture due to Kostant on the structure of the tensor product representation $V(\\rho) \\otimes V(\\rho)$."],"url":"http://arxiv.org/abs/2404.10266v1","category":"math.RT"}
{"created":"2024-04-16 03:34:55","title":"Realization of Planar Optical Tweezer (2D-LOT) Powered by Light Sheet","abstract":"We report the realization of the first planar optical tweezer trap system by a sheet of light. To visualize the trapping of the target object (dielectric bead or live cell) in a plane, an orthogonal widefield detection is employed. The planar / two-dimensional lightsheet optical tweezer (2D-LOT) sub-system is realized in an inverted microscopy mode with illumination from the bottom. A 1064 nm laser (power $\\sim 500 mW$) is expanded and directed to a combination of cylindrical lens and high NA objective lens to generate a tightly-focused diffraction-limited light sheet. The object to be trapped is injected in the specimen chamber (consists of two coverslips placed at a distance of $\\approx 1~mm$) using a syringe. The illumination of trap-laser light is along Z-direction (with coverslip along XZ-plane) whereas, the detection is achieved perpendicular to the coverslip (along Y-axis). The orthogonal detection is employed to directly visualize the trapping in a plane. The characterization of system PSF estimates the size of light sheet trap PSF to be, $2073.84 ~\\mu m^2$ which defines the active trap region / area. Beads are tracked on their way to the trap region for determining the trap stiffness along Z and X i.e, $k_z = 1.13 \\pm 0.034 ~pN/\\mu m$ and $k_x = 0.74 \\pm 0.021 ~pN/ \\mu m$. Results (image and video) show real-time trapping of dielectric beads in the trap zone (2D plane) generated by the light sheet. The beads can be seen getting trapped from all directions in the XZ-plane. Prolonged exposure to the light sheet builds up a 2D array of beads in the trap zone. Similar experiments on live NIH3T3 cells show cells trapped in the 2D trap. The potential of the planar trap lies in its ability to confine objects in two dimensions, thereby opening new kinds of experiments in biophysics, atomic physics, and optical physics.","sentences":["We report the realization of the first planar optical tweezer trap system by a sheet of light.","To visualize the trapping of the target object (dielectric bead or live cell) in a plane, an orthogonal widefield detection is employed.","The planar / two-dimensional lightsheet optical tweezer (2D-LOT) sub-system is realized in an inverted microscopy mode with illumination from the bottom.","A 1064 nm laser (power $\\sim 500 mW$) is expanded and directed to a combination of cylindrical lens and high NA objective lens to generate a tightly-focused diffraction-limited light sheet.","The object to be trapped is injected in the specimen chamber (consists of two coverslips placed at a distance of $\\approx 1~mm$) using a syringe.","The illumination of trap-laser light is along Z-direction (with coverslip along XZ-plane) whereas, the detection is achieved perpendicular to the coverslip (along Y-axis).","The orthogonal detection is employed to directly visualize the trapping in a plane.","The characterization of system PSF estimates the size of light sheet trap PSF to be, $2073.84 ~\\mu m^2$ which defines the active trap region / area.","Beads are tracked on their way to the trap region for determining the trap stiffness along Z and X i.e, $k_z = 1.13 \\pm 0.034 ~pN/\\mu m$ and $k_x = 0.74 \\pm 0.021 ~pN/ \\mu m$. Results (image and video) show real-time trapping of dielectric beads in the trap zone (2D plane) generated by the light sheet.","The beads can be seen getting trapped from all directions in the XZ-plane.","Prolonged exposure to the light sheet builds up a 2D array of beads in the trap zone.","Similar experiments on live NIH3T3 cells show cells trapped in the 2D trap.","The potential of the planar trap lies in its ability to confine objects in two dimensions, thereby opening new kinds of experiments in biophysics, atomic physics, and optical physics."],"url":"http://arxiv.org/abs/2404.10265v1","category":"physics.optics"}
{"created":"2024-04-16 03:34:39","title":"Calibration of the Cryogenic Measurement System of a Resonant Haloscope Cavity","abstract":"Possible light bosonic dark matter interactions with the Standard Model photon have been searched by microwave resonant cavities. In this paper, we demonstrate the cryogenic readout system calibration of a 7.138 GHz copper cavity with a loaded quality factor $Q_l=10^4$, operated at 22 mK temperature based on a dilution refrigerator. Our readout system consists of High Electron Mobility Transistors as cryogenic amplifiers at 4 K, plus room-temperature amplifiers and a spectrum analyzer for signal power detection. We test the system with a superconducting two-level system as a single-photon source in the microwave frequency regime and report an overall 95.6 dB system gain and -71.4 dB attenuation in the cavity's input channel. The effective noise temperature of the measurement system is 7.5 K.","sentences":["Possible light bosonic dark matter interactions with the Standard Model photon have been searched by microwave resonant cavities.","In this paper, we demonstrate the cryogenic readout system calibration of a 7.138 GHz copper cavity with a loaded quality factor $Q_l=10^4$, operated at 22 mK temperature based on a dilution refrigerator.","Our readout system consists of High Electron Mobility Transistors as cryogenic amplifiers at 4 K, plus room-temperature amplifiers and a spectrum analyzer for signal power detection.","We test the system with a superconducting two-level system as a single-photon source in the microwave frequency regime and report an overall 95.6 dB system gain and -71.4 dB attenuation in the cavity's input channel.","The effective noise temperature of the measurement system is 7.5 K."],"url":"http://arxiv.org/abs/2404.10264v1","category":"hep-ex"}
{"created":"2024-04-16 03:34:35","title":"PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network","abstract":"Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD). Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation. Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity. In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks. After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph. Then, four graph attention layers are applied to learn the relationships among agents and lanes. In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM). Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections. In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs. We conduct validation experiments on two downstream tasks, i.e., trajectory prediction in urban scenario, and intention recognition in highway scenario, to verify the generalized ability and understanding ability. Results show that compared with the baselines, PreGSU achieves better accuracy on both tasks, indicating the potential to be generalized to various scenes and targets. Ablation study shows the effectiveness of pre-train task design.","sentences":["Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD).","Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation.","Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity.","In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks.","After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph.","Then, four graph attention layers are applied to learn the relationships among agents and lanes.","In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM).","Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections.","In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs.","We conduct validation experiments on two downstream tasks, i.e., trajectory prediction in urban scenario, and intention recognition in highway scenario, to verify the generalized ability and understanding ability.","Results show that compared with the baselines, PreGSU achieves better accuracy on both tasks, indicating the potential to be generalized to various scenes and targets.","Ablation study shows the effectiveness of pre-train task design."],"url":"http://arxiv.org/abs/2404.10263v1","category":"cs.CV"}
{"created":"2024-04-16 03:25:08","title":"Dynamical Model of Rotation and Orbital Coupling for Deimos","abstract":"This paper introduces a novel dynamical model, building upon the existing dynamical model for Deimos in the current numerical ephemerides, which only encompasses the simple libration effects of Deimos. The study comprehensively incorporates the rotational dynamics of Deimos influenced by the torque exerted by the major celestial bodies (Mars, the Sun) in the solar system within the inertial space. Consequently, a full dynamical model is formulated to account for the complete coupling between the rotation and orbit of Deimos. Simultaneously, employing precision orbit determination methods used for artificial satellites, we develop an adjustment model for fitting data to the complete model. The 12-order Adams--Bashforth--Moulton (ABM) integration algorithm is employed to synchronously integrate the 12 state variables of the full model to obtain the orbit of Deimos. Numerical simulation results indicate that the full dynamical model and adjustment model are stable and reliable. Compared to the simple model, the polar axis of Deimos in the inertial space exhibits a more complex oscillation in the full model. This work further advances the current dynamical model for Deimos and establishes the foundational model for the generation of a new set of precise numerical ephemerides for Deimos.","sentences":["This paper introduces a novel dynamical model, building upon the existing dynamical model for Deimos in the current numerical ephemerides, which only encompasses the simple libration effects of Deimos.","The study comprehensively incorporates the rotational dynamics of Deimos influenced by the torque exerted by the major celestial bodies (Mars, the Sun) in the solar system within the inertial space.","Consequently, a full dynamical model is formulated to account for the complete coupling between the rotation and orbit of Deimos.","Simultaneously, employing precision orbit determination methods used for artificial satellites, we develop an adjustment model for fitting data to the complete model.","The 12-order Adams--Bashforth--Moulton (ABM) integration algorithm is employed to synchronously integrate the 12 state variables of the full model to obtain the orbit of Deimos.","Numerical simulation results indicate that the full dynamical model and adjustment model are stable and reliable.","Compared to the simple model, the polar axis of Deimos in the inertial space exhibits a more complex oscillation in the full model.","This work further advances the current dynamical model for Deimos and establishes the foundational model for the generation of a new set of precise numerical ephemerides for Deimos."],"url":"http://arxiv.org/abs/2404.10257v1","category":"astro-ph.EP"}
{"created":"2024-04-16 17:23:19","title":"Settling Constant Regrets in Linear Markov Decision Processes","abstract":"We study the constant regret guarantees in reinforcement learning (RL). Our objective is to design an algorithm that incurs only finite regret over infinite episodes with high probability. We introduce an algorithm, Cert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where both the transition kernel and the reward function can be approximated by some linear function up to misspecification level $\\zeta$. At the core of Cert-LSVI-UCB is an innovative certified estimator, which facilitates a fine-grained concentration analysis for multi-phase value-targeted regression, enabling us to establish an instance-dependent regret bound that is constant w.r.t. the number of episodes. Specifically, we demonstrate that for an MDP characterized by a minimal suboptimality gap $\\Delta$, Cert-LSVI-UCB has a cumulative regret of $\\tilde{\\mathcal{O}}(d^3H^5/\\Delta)$ with high probability, provided that the misspecification level $\\zeta$ is below $\\tilde{\\mathcal{O}}(\\Delta / (\\sqrt{d}H^2))$. Remarkably, this regret bound remains constant relative to the number of episodes $K$. To the best of our knowledge, Cert-LSVI-UCB is the first algorithm to achieve a constant, instance-dependent, high-probability regret bound in RL with linear function approximation for infinite runs without relying on prior distribution assumptions. This not only highlights the robustness of Cert-LSVI-UCB to model misspecification but also introduces novel algorithmic designs and analytical techniques of independent interest.","sentences":["We study the constant regret guarantees in reinforcement learning (RL).","Our objective is to design an algorithm that incurs only finite regret over infinite episodes with high probability.","We introduce an algorithm, Cert-LSVI-UCB, for misspecified linear Markov decision processes (MDPs) where both the transition kernel and the reward function can be approximated by some linear function up to misspecification level $\\zeta$. At the core of Cert-LSVI-UCB is an innovative certified estimator, which facilitates a fine-grained concentration analysis for multi-phase value-targeted regression, enabling us to establish an instance-dependent regret bound that is constant w.r.t.","the number of episodes.","Specifically, we demonstrate that for an MDP characterized by a minimal suboptimality gap $\\Delta$, Cert-LSVI-UCB has a cumulative regret of $\\tilde{\\mathcal{O}}(d^3H^5/\\Delta)$ with high probability, provided that the misspecification level $\\zeta$ is below $\\tilde{\\mathcal{O}}(\\Delta / (\\sqrt{d}H^2))$. Remarkably, this regret bound remains constant relative to the number of episodes $K$. To the best of our knowledge, Cert-LSVI-UCB is the first algorithm to achieve a constant, instance-dependent, high-probability regret bound in RL with linear function approximation for infinite runs without relying on prior distribution assumptions.","This not only highlights the robustness of Cert-LSVI-UCB to model misspecification but also introduces novel algorithmic designs and analytical techniques of independent interest."],"url":"http://arxiv.org/abs/2404.10745v1","category":"cs.LG"}
{"created":"2024-04-16 17:04:32","title":"Attention-Aware Visualization: Tracking and Responding to User Perception Over Time","abstract":"We propose the notion of Attention-Aware Visualizations (AAVs) that track the user's perception of a visual representation over time and feed this information back to the visualization. Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user's attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user's gaze on a visualization and its parts; (2) tracking the user's attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user's gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention.","sentences":["We propose the notion of Attention-Aware Visualizations (AAVs) that track the user's perception of a visual representation over time and feed this information back to the visualization.","Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user's attention: for example, by highlighting data the user has not yet seen.","We can separate the approach into three components: (1) measuring the user's gaze on a visualization and its parts; (2) tracking the user's attention over time; and (3) reactively modifying the visual representation based on the current attention metric.","In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user's gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization.","Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response.","We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention."],"url":"http://arxiv.org/abs/2404.10732v1","category":"cs.HC"}
{"created":"2024-04-16 15:59:17","title":"Characterization of the multimode nature of single-photon sources based on spontaneous parametric down conversion","abstract":"Single-photon sources are necessary components for many prospective quantum technologies. One candidate for a single-photon source is spontaneous parametric down conversion combined with a heralding photon detection. The heralded light pulse from such a source, is typically treated as single-mode, this treatment, however, is incomplete. We develop a full multimode description based on the exact Bogoliubov treatment of the down conversion process. We then provide a perturbative and effective treatment, which illustrates the most important physical mechanisms and permits analytical estimates of the success probability and purity of single-photon states under practical heralding conditions, both without relying on the precise detection time of the heralding photon and when accepting photons only in a narrow window around the time of the detection. This permits us to characterize the emitted light under three different assumptions for the pump pulse. For spontaneous parametric down conversion with a very short pump pulse, we find the single-mode description to be accurate, while for longer pump pulses and continuous pumping, a multimode description is necessary. Our findings can be used to guide the design of quantum information protocols based on heralded single-photon sources, as their performance may depend on the multimode nature of the sources.","sentences":["Single-photon sources are necessary components for many prospective quantum technologies.","One candidate for a single-photon source is spontaneous parametric down conversion combined with a heralding photon detection.","The heralded light pulse from such a source, is typically treated as single-mode, this treatment, however, is incomplete.","We develop a full multimode description based on the exact Bogoliubov treatment of the down conversion process.","We then provide a perturbative and effective treatment, which illustrates the most important physical mechanisms and permits analytical estimates of the success probability and purity of single-photon states under practical heralding conditions, both without relying on the precise detection time of the heralding photon and when accepting photons only in a narrow window around the time of the detection.","This permits us to characterize the emitted light under three different assumptions for the pump pulse.","For spontaneous parametric down conversion with a very short pump pulse, we find the single-mode description to be accurate, while for longer pump pulses and continuous pumping, a multimode description is necessary.","Our findings can be used to guide the design of quantum information protocols based on heralded single-photon sources, as their performance may depend on the multimode nature of the sources."],"url":"http://arxiv.org/abs/2404.10682v1","category":"quant-ph"}
{"created":"2024-04-16 15:49:09","title":"Climbing to the Top of the ATLAS 13 TeV data","abstract":"The large amount of data recorded with the ATLAS detector at the Large Hadron Collider, corresponding to 140 fb$^{-1}$ of $pp$ collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV, has brought our knowledge of the top quark to the next level. The measurement of the top$-$antitop quark pair-production cross-section has reached a precision of 1.8% and the cross-section was measured differentially up to several TeV in several observables including the top-quark transverse momentum and top-quark-pair invariant mass. Single-top-quark production was studied in all production modes. Rare production processes where the top quark is associated with a vector boson, and four-top-quark production, have become accessible and precision measurements of several of these processes have reached cross-section uncertainties of around 10% or smaller. Innovative measurements of the top-quark mass and properties have also emerged, including the observation of quantum entanglement in the top-quark sector and tests of lepton-flavour universality using top-quark decays. Searches for flavour-changing neutral currents in the top-quark sector have been significantly improved, reaching branching-ratio exclusion limits ranging from $10^{-3}$ to $10^{-5}$. Many of these analyses have been used to set limits on Wilson coefficients within the effective field theory framework.","sentences":["The large amount of data recorded with the ATLAS detector at the Large Hadron Collider, corresponding to 140 fb$^{-1}$ of $pp$ collisions at a centre-of-mass energy of $\\sqrt{s}=13$ TeV, has brought our knowledge of the top quark to the next level.","The measurement of the top$-$antitop quark pair-production cross-section has reached a precision of 1.8% and the cross-section was measured differentially up to several TeV in several observables including the top-quark transverse momentum and top-quark-pair invariant mass.","Single-top-quark production was studied in all production modes.","Rare production processes where the top quark is associated with a vector boson, and four-top-quark production, have become accessible and precision measurements of several of these processes have reached cross-section uncertainties of around 10% or smaller.","Innovative measurements of the top-quark mass and properties have also emerged, including the observation of quantum entanglement in the top-quark sector and tests of lepton-flavour universality using top-quark decays.","Searches for flavour-changing neutral currents in the top-quark sector have been significantly improved, reaching branching-ratio exclusion limits ranging from $10^{-3}$ to $10^{-5}$. Many of these analyses have been used to set limits on Wilson coefficients within the effective field theory framework."],"url":"http://arxiv.org/abs/2404.10674v1","category":"hep-ex"}
{"created":"2024-04-16 15:30:20","title":"Context-Free Languages of String Diagrams","abstract":"We introduce context-free languages of morphisms in monoidal categories, extending recent work on the categorification of context-free languages, and regular languages of string diagrams. Context-free languages of string diagrams include classical context-free languages of words, trees, and hypergraphs, when instantiated over appropriate monoidal categories. Using a contour-splicing adjunction, we prove a representation theorem for context-free languages of string diagrams: every such language arises as the image under a monoidal functor of a regular language of string diagrams.","sentences":["We introduce context-free languages of morphisms in monoidal categories, extending recent work on the categorification of context-free languages, and regular languages of string diagrams.","Context-free languages of string diagrams include classical context-free languages of words, trees, and hypergraphs, when instantiated over appropriate monoidal categories.","Using a contour-splicing adjunction, we prove a representation theorem for context-free languages of string diagrams: every such language arises as the image under a monoidal functor of a regular language of string diagrams."],"url":"http://arxiv.org/abs/2404.10653v1","category":"cs.FL"}
{"created":"2024-04-16 15:00:12","title":"Weighting methods for truncation by death in cluster-randomized trials","abstract":"Patient-centered outcomes, such as quality of life and length of hospital stay, are the focus in a wide array of clinical studies. However, participants in randomized trials for elderly or critically and severely ill patient populations may have truncated or undefined non-mortality outcomes if they do not survive through the measurement time point. To address truncation by death, the survivor average causal effect (SACE) has been proposed as a causally interpretable subgroup treatment effect defined under the principal stratification framework. However, the majority of methods for estimating SACE have been developed in the context of individually-randomized trials. Only limited discussions have been centered around cluster-randomized trials (CRTs), where methods typically involve strong distributional assumptions for outcome modeling. In this paper, we propose two weighting methods to estimate SACE in CRTs that obviate the need for potentially complicated outcome distribution modeling. We establish the requisite assumptions that address latent clustering effects to enable point identification of SACE, and we provide computationally-efficient asymptotic variance estimators for each weighting estimator. In simulations, we evaluate our weighting estimators, demonstrating their finite-sample operating characteristics and robustness to certain departures from the identification assumptions. We illustrate our methods using data from a CRT to assess the impact of a sedation protocol on mechanical ventilation among children with acute respiratory failure.","sentences":["Patient-centered outcomes, such as quality of life and length of hospital stay, are the focus in a wide array of clinical studies.","However, participants in randomized trials for elderly or critically and severely ill patient populations may have truncated or undefined non-mortality outcomes","if they do not survive through the measurement time point",".","To address truncation by death, the survivor average causal effect (SACE) has been proposed as a causally interpretable subgroup treatment effect defined under the principal stratification framework.","However, the majority of methods for estimating SACE have been developed in the context of individually-randomized trials.","Only limited discussions have been centered around cluster-randomized trials (CRTs), where methods typically involve strong distributional assumptions for outcome modeling.","In this paper, we propose two weighting methods to estimate SACE in CRTs that obviate the need for potentially complicated outcome distribution modeling.","We establish the requisite assumptions that address latent clustering effects to enable point identification of SACE, and we provide computationally-efficient asymptotic variance estimators for each weighting estimator.","In simulations, we evaluate our weighting estimators, demonstrating their finite-sample operating characteristics and robustness to certain departures from the identification assumptions.","We illustrate our methods using data from a CRT to assess the impact of a sedation protocol on mechanical ventilation among children with acute respiratory failure."],"url":"http://arxiv.org/abs/2404.10629v1","category":"stat.ME"}
{"created":"2024-04-16 14:41:06","title":"Phase diagram of the quantum spin-1/2 Heisenberg-$\u0393$ model on a frustrated zigzag chain","abstract":"We investigate the quantum spin-1/2 zigzag chain with frustrated $J_1$-$J_2$ Heisenberg interactions, incorporating additional off-diagonal exchange interactions known as the $\\Gamma$-term, both with and without an applied magnetic field. Based on the density-matrix renormalization group calculation, we map out the ground state phase diagram that shows a variety of magnetic and nonmagnetic phases including multicritical points and several exactly solvable points. Upon introducing a finite $\\Gamma$-term, we observe the persistent dimer singlet state of the $J_1$-$J_2$ Heisenberg model, sustaining a nonzero spin gap, while also giving rise to a new gapless branch of nonmagnetic excitation. This gapless mode induces robust nematic fluctuations manifesting in its long-ranged correlations, however, the true condensation does not occur until the $\\Gamma$-term becomes comparable with the Heisenberg term. There, the nonmagnetic phases transform into Ising-type ferromagnetic or antiferromagnetic long-range orders that arise from the $\\Gamma$-term spontaneously selecting magnetic easy axes. Its orientations dictate the type of magnetic order under geometric frustration effects as predicted by Landau's mean-field theory. These theoretical findings provide insights into the exotic low-temperature phase observed in YbCuS$_2$, characterized by gapless excitations and seemingly nonmagnetic behavior accompanied by incommensurate correlations.","sentences":["We investigate the quantum spin-1/2 zigzag chain with frustrated $J_1$-$J_2$ Heisenberg interactions, incorporating additional off-diagonal exchange interactions known as the $\\Gamma$-term, both with and without an applied magnetic field.","Based on the density-matrix renormalization group calculation, we map out the ground state phase diagram that shows a variety of magnetic and nonmagnetic phases including multicritical points and several exactly solvable points.","Upon introducing a finite $\\Gamma$-term, we observe the persistent dimer singlet state of the $J_1$-$J_2$ Heisenberg model, sustaining a nonzero spin gap, while also giving rise to a new gapless branch of nonmagnetic excitation.","This gapless mode induces robust nematic fluctuations manifesting in its long-ranged correlations, however, the true condensation does not occur until the $\\Gamma$-term becomes comparable with the Heisenberg term.","There, the nonmagnetic phases transform into Ising-type ferromagnetic or antiferromagnetic long-range orders that arise from the $\\Gamma$-term spontaneously selecting magnetic easy axes.","Its orientations dictate the type of magnetic order under geometric frustration effects as predicted by Landau's mean-field theory.","These theoretical findings provide insights into the exotic low-temperature phase observed in YbCuS$_2$, characterized by gapless excitations and seemingly nonmagnetic behavior accompanied by incommensurate correlations."],"url":"http://arxiv.org/abs/2404.10615v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 14:21:24","title":"Formation of GW230529 from Isolated Binary Evolution and Its Electromagnetic Counterparts","abstract":"In this {\\em Letter}, we explore the formation of the mass-gap black hole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event, i.e., GW230529, from the isolated binary evolution channel, and study potential signatures of its electromagnetic signals. By adopting the `delayed' supernova prescription and reasonable model realizations, our population synthesis simulation results can simultaneously match the inferred event rate densities of GW230529-like mgBHNS and total BHNS mergers, as well as the population distribution of the BH mass in BHNS mergers reported by the LIGO-Virgo-KAGRA Collaboration. Thus, we conclude that the recently-discovered mgBHNS merger, GW230529, can be explained through the isolated binary evolution channel. Considering the equation of states of AP4 and DD2, the probabilities that GW230529 can make tidal disruption are $12.8\\%$ and $63.2\\%$, respectively. If GW230529 is a disrupted event, the associated kilonova is predicted to have an apparent magnitude of $\\sim23-24\\,{\\rm{mag}}$, and hence, can be detected by the present survey projects and LSST. Since GW230529 could be an off-axis event inferred from the GW observation, its associated gamma-ray burst (GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting the lack of GRB observations. The detection of GW230529 confirms the existence of mgBHNS mergers formed through the isolated binary evolution channel, suggesting that BHNS mergers are still likely to be multimessenger sources that emit GWs, GRBs, and kilonovae. Although mgBHNS mergers account for $\\sim60\\%$ cosmological BHNS population, we find that $\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS mergers.","sentences":["In this {\\em Letter}, we explore the formation of the mass-gap black hole-neutron star (mgBHNS) merger detected in gravitational wave (GW) event, i.e., GW230529, from the isolated binary evolution channel, and study potential signatures of its electromagnetic signals.","By adopting the `delayed' supernova prescription and reasonable model realizations, our population synthesis simulation results can simultaneously match the inferred event rate densities of GW230529-like mgBHNS and total BHNS mergers, as well as the population distribution of the BH mass in BHNS mergers reported by the LIGO-Virgo-KAGRA Collaboration.","Thus, we conclude that the recently-discovered mgBHNS merger, GW230529, can be explained through the isolated binary evolution channel.","Considering the equation of states of AP4 and DD2, the probabilities that GW230529 can make tidal disruption are $12.8\\%$ and $63.2\\%$, respectively.","If GW230529 is a disrupted event, the associated kilonova is predicted to have an apparent magnitude of $\\sim23-24\\,{\\rm{mag}}$, and hence, can be detected by the present survey projects and LSST.","Since GW230529 could be an off-axis event inferred from the GW observation, its associated gamma-ray burst (GRB) might be too dim to be observed by $\\gamma$-ray detectors, interpreting the lack of GRB observations.","The detection of GW230529 confirms the existence of mgBHNS mergers formed through the isolated binary evolution channel, suggesting that BHNS mergers are still likely to be multimessenger sources that emit GWs, GRBs, and kilonovae.","Although mgBHNS mergers account for $\\sim60\\%$ cosmological BHNS population, we find that $\\gtrsim90\\%$ disrupted BHNS mergers are expected to originate from mgBHNS mergers."],"url":"http://arxiv.org/abs/2404.10596v1","category":"astro-ph.HE"}
{"created":"2024-04-16 14:20:30","title":"Nonparametric Isotropy Test for Spatial Point Processes using Random Rotations","abstract":"In spatial statistics, point processes are often assumed to be isotropic meaning that their distribution is invariant under rotations. Statistical tests for the null hypothesis of isotropy found in the literature are based either on asymptotics or on Monte Carlo simulation of a parametric null model. Here, we present a nonparametric test based on resampling the Fry points of the observed point pattern. Empirical levels and powers of the test are investigated in a simulation study for four point process models with anisotropy induced by different mechanisms. Finally, a real data set is tested for isotropy.","sentences":["In spatial statistics, point processes are often assumed to be isotropic meaning that their distribution is invariant under rotations.","Statistical tests for the null hypothesis of isotropy found in the literature are based either on asymptotics or on Monte Carlo simulation of a parametric null model.","Here, we present a nonparametric test based on resampling the Fry points of the observed point pattern.","Empirical levels and powers of the test are investigated in a simulation study for four point process models with anisotropy induced by different mechanisms.","Finally, a real data set is tested for isotropy."],"url":"http://arxiv.org/abs/2404.10594v1","category":"stat.ME"}
{"created":"2024-04-16 14:00:10","title":"Observations of Fan-Spine Topology by Solar Orbiter/EUI: Rotational Motions and Indications of Alfv\u00e9n Waves","abstract":"Torsional Alfv\\'en waves do not produce any intensity variation and are, therefore, challenging to observe with imaging instruments. Previously, Alfv\\'en wave observations were reported throughout all the layers of the solar atmosphere using spectral imaging. We present an observation of a torsional Alfv\\'en wave detected in an inverted y-shape structure observed with the HRIEUV telescope of the EUI instrument onboard Solar Orbiter in its 174 \\r{A} channel. The feature consists of two footpoints connected through short loops and a spine with a length of 30 Mm originating from one of the footpoints. In the current work, we also make use of the simultaneous observations from two other instruments onboard Solar Orbiter. The first one is PHI that is used to derive the magnetic configuration of the observed feature. The second one is SPICE that provided observations of intensity maps in different lines including Ne VIII and C III lines. We also address the issues of the SPICE point spread function and its influence on the Doppler maps via performed forward modeling analysis. The difference movie shows clear signatures of propagating rotational motions in the spine. Doppler maps obtained with SPICE show strong signal in the spine region. Under the assumption that the recovered point spread function is mostly correct, synthesized raster images confirm that this signal is predominantly physical. We conclude that the presented observations are compatible with an interpretation of either propagating torsional Alfv\\'en waves in a low coronal structure or untwisting of a flux rope. This is the first time we see signatures of propagating torsional motion in corona as observed by the three instruments onboard Solar Orbiter.","sentences":["Torsional Alfv\\'en waves do not produce any intensity variation and are, therefore, challenging to observe with imaging instruments.","Previously, Alfv\\'en wave observations were reported throughout all the layers of the solar atmosphere using spectral imaging.","We present an observation of a torsional Alfv\\'en wave detected in an inverted y-shape structure observed with the HRIEUV telescope of the EUI instrument onboard Solar Orbiter in its 174 \\r{A} channel.","The feature consists of two footpoints connected through short loops and a spine with a length of 30 Mm originating from one of the footpoints.","In the current work, we also make use of the simultaneous observations from two other instruments onboard Solar Orbiter.","The first one is PHI that is used to derive the magnetic configuration of the observed feature.","The second one is SPICE that provided observations of intensity maps in different lines including Ne VIII and C III lines.","We also address the issues of the SPICE point spread function and its influence on the Doppler maps via performed forward modeling analysis.","The difference movie shows clear signatures of propagating rotational motions in the spine.","Doppler maps obtained with SPICE show strong signal in the spine region.","Under the assumption that the recovered point spread function is mostly correct, synthesized raster images confirm that this signal is predominantly physical.","We conclude that the presented observations are compatible with an interpretation of either propagating torsional Alfv\\'en waves in a low coronal structure or untwisting of a flux rope.","This is the first time we see signatures of propagating torsional motion in corona as observed by the three instruments onboard Solar Orbiter."],"url":"http://arxiv.org/abs/2404.10577v1","category":"astro-ph.SR"}
{"created":"2024-04-16 13:47:19","title":"PAKT: Perspectivized Argumentation Knowledge Graph and Tool for Deliberation Analysis (with Supplementary Materials)","abstract":"Deliberative processes play a vital role in shaping opinions, decisions and policies in our society. In contrast to persuasive debates, deliberation aims to foster understanding of conflicting perspectives among interested parties. The exchange of arguments in deliberation serves to elucidate viewpoints, to raise awareness of conflicting interests, and to finally converge on a resolution. To better understand and analyze the underlying processes of deliberation, we propose PAKT, a Perspectivized Argumentation Knowledge Graph and Tool. The graph structures the argumentative space across diverse topics, where arguments i) are divided into premises and conclusions, ii) are annotated for stances, framings and their underlying values and iii) are connected to background knowledge. We show how to construct PAKT and conduct case studies on the obtained multifaceted argumentation graph. Our findings show the analytical potential offered by our framework, highlighting the capability to go beyond individual arguments and to reveal structural patterns in the way participants and stakeholders argue in a debate. The overarching goal of our work is to facilitate constructive discourse and informed decision making as a special form of argumentation. We offer public access to PAKT and its rich capabilities to support analytics, visualizaton, navigation and efficient search, for diverse forms of argumentation.","sentences":["Deliberative processes play a vital role in shaping opinions, decisions and policies in our society.","In contrast to persuasive debates, deliberation aims to foster understanding of conflicting perspectives among interested parties.","The exchange of arguments in deliberation serves to elucidate viewpoints, to raise awareness of conflicting interests, and to finally converge on a resolution.","To better understand and analyze the underlying processes of deliberation, we propose PAKT, a Perspectivized Argumentation Knowledge Graph and Tool.","The graph structures the argumentative space across diverse topics, where arguments i) are divided into premises and conclusions, ii) are annotated for stances, framings and their underlying values and iii) are connected to background knowledge.","We show how to construct PAKT and conduct case studies on the obtained multifaceted argumentation graph.","Our findings show the analytical potential offered by our framework, highlighting the capability to go beyond individual arguments and to reveal structural patterns in the way participants and stakeholders argue in a debate.","The overarching goal of our work is to facilitate constructive discourse and informed decision making as a special form of argumentation.","We offer public access to PAKT and its rich capabilities to support analytics, visualizaton, navigation and efficient search, for diverse forms of argumentation."],"url":"http://arxiv.org/abs/2404.10570v1","category":"cs.CY"}
{"created":"2024-04-16 13:23:05","title":"Progressive polymer deformation induced by polar activity and the influence of inertia","abstract":"Polar activity is shown here to induce a progressive local deformation of linear polymer chains, making a clear distinction between head and tail, while the overall chain conformation gets more compact. This breakdown of self-similarity, provoked by the accumulated tension on the polymer backbone induced by the activity, is shown to occur both in the absence and presence of inertia, although it is stronger in the latter case. Other properties like the relaxation time and diffusion are also largely influenced by the presence of a polar activity.","sentences":["Polar activity is shown here to induce a progressive local deformation of linear polymer chains, making a clear distinction between head and tail, while the overall chain conformation gets more compact.","This breakdown of self-similarity, provoked by the accumulated tension on the polymer backbone induced by the activity, is shown to occur both in the absence and presence of inertia, although it is stronger in the latter case.","Other properties like the relaxation time and diffusion are also largely influenced by the presence of a polar activity."],"url":"http://arxiv.org/abs/2404.10553v1","category":"cond-mat.soft"}
{"created":"2024-04-16 13:16:41","title":"A/B testing under Interference with Partial Network Information","abstract":"A/B tests are often required to be conducted on subjects that might have social connections. For e.g., experiments on social media, or medical and social interventions to control the spread of an epidemic. In such settings, the SUTVA assumption for randomized-controlled trials is violated due to network interference, or spill-over effects, as treatments to group A can potentially also affect the control group B. When the underlying social network is known exactly, prior works have demonstrated how to conduct A/B tests adequately to estimate the global average treatment effect (GATE). However, in practice, it is often impossible to obtain knowledge about the exact underlying network. In this paper, we present UNITE: a novel estimator that relax this assumption and can identify GATE while only relying on knowledge of the superset of neighbors for any subject in the graph. Through theoretical analysis and extensive experiments, we show that the proposed approach performs better in comparison to standard estimators.","sentences":["A/B tests are often required to be conducted on subjects that might have social connections.","For e.g., experiments on social media, or medical and social interventions to control the spread of an epidemic.","In such settings, the SUTVA assumption for randomized-controlled trials is violated due to network interference, or spill-over effects, as treatments to group A can potentially also affect the control group B.","When the underlying social network is known exactly, prior works have demonstrated how to conduct A/B tests adequately to estimate the global average treatment effect (GATE).","However, in practice, it is often impossible to obtain knowledge about the exact underlying network.","In this paper, we present UNITE: a novel estimator that relax this assumption and can identify GATE while only relying on knowledge of the superset of neighbors for any subject in the graph.","Through theoretical analysis and extensive experiments, we show that the proposed approach performs better in comparison to standard estimators."],"url":"http://arxiv.org/abs/2404.10547v1","category":"cs.LG"}
{"created":"2024-04-16 12:04:04","title":"Assumption-Lean Quantile Regression","abstract":"Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure. Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic. In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model. We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified. This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning. Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods. The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight.","sentences":["Quantile regression is a powerful tool for detecting exposure-outcome associations given covariates across different parts of the outcome's distribution, but has two major limitations when the aim is to infer the effect of an exposure.","Firstly, the exposure coefficient estimator may not converge to a meaningful quantity when the model is misspecified, and secondly, variable selection methods may induce bias and excess uncertainty, rendering inferences biased and overly optimistic.","In this paper, we address these issues via partially linear quantile regression models which parametrize the conditional association of interest, but do not restrict the association with other covariates in the model.","We propose consistent estimators for the unknown model parameter by mapping it onto a nonparametric main effect estimand that captures the (conditional) association of interest even when the quantile model is misspecified.","This estimand is estimated using the efficient influence function under the nonparametric model, allowing for the incorporation of data-adaptive procedures such as variable selection and machine learning.","Our approach provides a flexible and reliable method for detecting associations that is robust to model misspecification and excess uncertainty induced by variable selection methods.","The proposal is illustrated using simulation studies and data on annual health care costs associated with excess body weight."],"url":"http://arxiv.org/abs/2404.10495v1","category":"stat.ME"}
{"created":"2024-04-16 11:44:12","title":"AbsGS: Recovering Fine Details for 3D Gaussian Splatting","abstract":"3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: https://ty424.github.io/AbsGS.github.io/","sentences":["3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance.","However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images.","The underlying reason for the flaw has still been under-explored.","In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting.","To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification.","Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting.","We evaluate our proposed method on various challenging datasets.","The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption.","Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods.","We will open source our codes upon formal publication.","Our project page is available at: https://ty424.github.io/AbsGS.github.io/"],"url":"http://arxiv.org/abs/2404.10484v1","category":"cs.CV"}
{"created":"2024-04-16 11:43:26","title":"Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty","abstract":"The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Carlo Dropout model with kernel modelling. Our model is designed to enhance reliability on small medical datasets, a crucial barrier to the wider adoption of AI in healthcare. This model leverages existing language models for improved effectiveness and seamlessly integrates with current workflows. We demonstrate significant improvements in reliability, even with limited data, offering a promising step towards building trust in AI-driven medical predictions and unlocking its potential to improve patient care.","sentences":["The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability.","This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Carlo Dropout model with kernel modelling.","Our model is designed to enhance reliability on small medical datasets, a crucial barrier to the wider adoption of AI in healthcare.","This model leverages existing language models for improved effectiveness and seamlessly integrates with current workflows.","We demonstrate significant improvements in reliability, even with limited data, offering a promising step towards building trust in AI-driven medical predictions and unlocking its potential to improve patient care."],"url":"http://arxiv.org/abs/2404.10483v1","category":"cs.LG"}
{"created":"2024-04-16 11:42:06","title":"BayesJudge: Bayesian Kernel Language Modelling with Confidence Uncertainty in Legal Judgment Prediction","abstract":"Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout. Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score. Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks. We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%. By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty.","sentences":["Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications.","While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial.","We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout.","Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score.","Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks.","We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%.","By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty."],"url":"http://arxiv.org/abs/2404.10481v1","category":"cs.LG"}
{"created":"2024-04-16 11:15:41","title":"Advances in Atom Interferometry and their Impacts on the Performance of Quantum Accelerometers On-board Future Satellite Gravity Missions","abstract":"Recent advances in cold atom interferometry have cleared the path for space applications of quantum inertial sensors, whose level of stability is expected to increase dramatically with the longer interrogation times accessible in space. In this study, a comprehensive in-orbit model is developed for a Mach-Zehnder-type cold-atom accelerometer. Performance tests are realized under different assumptions, and the impact of various sources of errors on instrument stability is evaluated. Current and future advances for space-based atom interferometry are discussed, and their impact on the performance of quantum sensors on-board satellite gravity missions is investigated in three different scenarios: state-of-the-art scenario, near-future (between the next 5 and 10 years) and far-future scenarios (between the next 10 to 20 years). We show that one can achieve a sensitivity level close to 5E-10 with the current state-of-the-art technology. We also estimate that in the near and far-future, atom interferometry in space is expected to achieve sensitivity levels of 1E-11 and 1E-12, respectively. A roadmap for improvements in atom interferometry is provided that would maximize the performance of future CAI accelerometers, considering their technical capabilities. Finally, the possibility and challenges of having ultra-sensitive atom interferometry in space for future space missions are discussed.","sentences":["Recent advances in cold atom interferometry have cleared the path for space applications of quantum inertial sensors, whose level of stability is expected to increase dramatically with the longer interrogation times accessible in space.","In this study, a comprehensive in-orbit model is developed for a Mach-Zehnder-type cold-atom accelerometer.","Performance tests are realized under different assumptions, and the impact of various sources of errors on instrument stability is evaluated.","Current and future advances for space-based atom interferometry are discussed, and their impact on the performance of quantum sensors on-board satellite gravity missions is investigated in three different scenarios: state-of-the-art scenario, near-future (between the next 5 and 10 years) and far-future scenarios (between the next 10 to 20 years).","We show that one can achieve a sensitivity level close to 5E-10 with the current state-of-the-art technology.","We also estimate that in the near and far-future, atom interferometry in space is expected to achieve sensitivity levels of 1E-11 and 1E-12, respectively.","A roadmap for improvements in atom interferometry is provided that would maximize the performance of future CAI accelerometers, considering their technical capabilities.","Finally, the possibility and challenges of having ultra-sensitive atom interferometry in space for future space missions are discussed."],"url":"http://arxiv.org/abs/2404.10471v1","category":"physics.ins-det"}
{"created":"2024-04-16 10:52:30","title":"Berkeley Cardinals and Vop\u011bnka's Principle","abstract":"We introduce \"$n$-choiceless\" supercompact and extendible cardinals in Zermelo-Fraenkel set theory without the Axiom of Choice. We prove relations between these cardinals and Vop\\v{e}nka's Principle similar to those of Bagaria's work in his papers \"$C^{(n)}$-Cardinals\" and \"More on the Preservation of Large Cardinals Under Class Forcing.\" We use these relations to characterize Berkeley cardinals in terms of a restricted form of Vop\\v{e}nka's Principle. Finally, we establish the equiconsistency of the \"$n$-choiceless\" extendible cardinals with their original counterparts, and study the consistency strength of other relevant theories.","sentences":["We introduce \"$n$-choiceless\" supercompact and extendible cardinals in Zermelo-Fraenkel set theory without the Axiom of Choice.","We prove relations between these cardinals and Vop\\v{e}nka's Principle similar to those of Bagaria's work in his papers \"$C^{(n)}$-Cardinals\" and \"More on the Preservation of Large Cardinals Under Class Forcing.\"","We use these relations to characterize Berkeley cardinals in terms of a restricted form of Vop\\v{e}nka's Principle.","Finally, we establish the equiconsistency of the \"$n$-choiceless\" extendible cardinals with their original counterparts, and study the consistency strength of other relevant theories."],"url":"http://arxiv.org/abs/2404.10455v1","category":"math.LO"}
{"created":"2024-04-16 10:27:56","title":"Fundamentals of a Null Field Method-Surface Equivalence Principle Approach for Scattering by Dielectric Cylinders","abstract":"The null-field method (NFM) and the method of auxiliary sources (MAS) have been both used extensively for the numerical solution of boundary-value problems arising in diverse applications involving propagation and scattering of waves. It has been shown that, under certain conditions, the applicabil- ity of MAS may be restricted by issues concerning the divergence of the auxiliary currents, manifested by the appearance of exponentially large os- cillations. In this work, we combine the NFM with the surface equivalence principle (SEP) and investigate analytically the convergence properties of the combined NFM-SEP with reference to the problem of (internal or external) line-source excitation of a dielectric cylinder. Our main purpose is to prove that (contrary to the MAS) the discrete NFM-SEP currents, when prop- erly normalized, always converge to the corresponding continuous current densities, and thus no divergence and oscillations phenomena appear. The theoretical analysis of the NFM-SEP is accompanied by detailed comparisons with the MAS as well as with representative numerical results illustrating the conclusions.","sentences":["The null-field method (NFM) and the method of auxiliary sources (MAS) have been both used extensively for the numerical solution of boundary-value problems arising in diverse applications involving propagation and scattering of waves.","It has been shown that, under certain conditions, the applicabil- ity of MAS may be restricted by issues concerning the divergence of the auxiliary currents, manifested by the appearance of exponentially large os- cillations.","In this work, we combine the NFM with the surface equivalence principle (SEP) and investigate analytically the convergence properties of the combined NFM-SEP with reference to the problem of (internal or external) line-source excitation of a dielectric cylinder.","Our main purpose is to prove that (contrary to the MAS) the discrete NFM-SEP currents, when prop- erly normalized, always converge to the corresponding continuous current densities, and thus no divergence and oscillations phenomena appear.","The theoretical analysis of the NFM-SEP is accompanied by detailed comparisons with the MAS as well as with representative numerical results illustrating the conclusions."],"url":"http://arxiv.org/abs/2404.10442v1","category":"math.NA"}
{"created":"2024-04-16 09:57:30","title":"Observation of thermal microwave photons with a Josephson junction detector","abstract":"When measuring electromagnetic radiation of frequency $f$, the most sensitive detector is the one that counts the single quanta of energy $h f$. Single photon detectors (SPDs) were demonstrated from $\\gamma$-rays to infrared wavelengths, and extending this range down to the microwaves is the focus of intense research. The energy of $10\\,\\mathrm{GHz}$ microwave photon, about $40\\,\\mathrm{\\mu eV}$ or $7\\, \\mathrm{yJ},$ is enough to force a superconducting Josephson junction into its resistive state, making it suitable to be used as a sensor. In this work, we use an underdamped Josephson junction to detect single thermal photons stochastically emitted by a microwave copper cavity at millikelvin temperatures. After characterizing the source and detector, we vary the temperature of the resonant cavity and measure the increased photon rate. The device shows an efficiency up to 40% and a dark count rate of $0.1\\,\\mathrm{Hz}$ in a bandwidth of several gigahertz. To confirm the thermal nature of the emitted photons we verify their super-Poissonian statistics, which is also a signature of quantum chaos. We discuss detector application in the scope of Dark Matter Axion searches, and note its importance for quantum information, metrology and fundamental physics.","sentences":["When measuring electromagnetic radiation of frequency $f$, the most sensitive detector is the one that counts the single quanta of energy $h f$.","Single photon detectors (SPDs) were demonstrated from $\\gamma$-rays to infrared wavelengths, and extending this range down to the microwaves is the focus of intense research.","The energy of $10\\,\\mathrm{GHz}$ microwave photon, about $40\\,\\mathrm{\\mu eV}$ or $7\\, \\mathrm{yJ},$ is enough to force a superconducting Josephson junction into its resistive state, making it suitable to be used as a sensor.","In this work, we use an underdamped Josephson junction to detect single thermal photons stochastically emitted by a microwave copper cavity at millikelvin temperatures.","After characterizing the source and detector, we vary the temperature of the resonant cavity and measure the increased photon rate.","The device shows an efficiency up to 40% and a dark count rate of $0.1\\,\\mathrm{Hz}$ in a bandwidth of several gigahertz.","To confirm the thermal nature of the emitted photons we verify their super-Poissonian statistics, which is also a signature of quantum chaos.","We discuss detector application in the scope of Dark Matter Axion searches, and note its importance for quantum information, metrology and fundamental physics."],"url":"http://arxiv.org/abs/2404.10434v1","category":"quant-ph"}
{"created":"2024-04-16 09:47:53","title":"A stochastic discrete slip approach to microplasticity: Application to submicron W pillars","abstract":"A stochastic discrete slip approach is proposed to model plastic deformation in submicron domains. The model is applied to the study of submicron pillar ($D~\\leq~1\\mu m$) compression experiments on tungsten (W), a prototypical metal for applications under extreme conditions. Slip events are geometrically resolved in the specimen and considered as eigenstrain fields producing a displacement jump across a slip plane. This novel method includes several aspects of utmost importance to small-scale plasticity, i.e. source truncation effects, surface nucleation effects, starvation effects, slip localization and an inherently stochastic response. Implementation on an FFT-spectral solver results in an efficient computational 3-D framework. Simulations of submicron W pillars ($D~\\leq~1\\mu m$) under compression show that the method is capable of capturing salient features of sub-micron scale plasticity. These include the natural competition between pre-existing dislocations and surface nucleation of new dislocations. Our results predict distinctive flow stress power-law dependence exponents as well as a size-dependence of the strain-rate sensitivity exponent. The results are thoroughly compared with experimental literature.","sentences":["A stochastic discrete slip approach is proposed to model plastic deformation in submicron domains.","The model is applied to the study of submicron pillar ($D~\\leq~1\\mu m$) compression experiments on tungsten (W), a prototypical metal for applications under extreme conditions.","Slip events are geometrically resolved in the specimen and considered as eigenstrain fields producing a displacement jump across a slip plane.","This novel method includes several aspects of utmost importance to small-scale plasticity, i.e. source truncation effects, surface nucleation effects, starvation effects, slip localization and an inherently stochastic response.","Implementation on an FFT-spectral solver results in an efficient computational 3-D framework.","Simulations of submicron W pillars ($D~\\leq~1\\mu m$) under compression show that the method is capable of capturing salient features of sub-micron scale plasticity.","These include the natural competition between pre-existing dislocations and surface nucleation of new dislocations.","Our results predict distinctive flow stress power-law dependence exponents as well as a size-dependence of the strain-rate sensitivity exponent.","The results are thoroughly compared with experimental literature."],"url":"http://arxiv.org/abs/2404.10430v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 09:37:41","title":"AudioProtoPNet: An interpretable deep learning model for bird sound classification","abstract":"Recently, scientists have proposed several deep learning models to monitor the diversity of bird species. These models can detect bird species with high accuracy by analyzing acoustic signals. However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process. For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools. In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture. Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data. Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions.","sentences":["Recently, scientists have proposed several deep learning models to monitor the diversity of bird species.","These models can detect bird species with high accuracy by analyzing acoustic signals.","However, traditional deep learning algorithms are black-box models that provide no insight into their decision-making process.","For domain experts, such as ornithologists, it is crucial that these models are not only efficient, but also interpretable in order to be used as assistive tools.","In this study, we present an adaption of the Prototypical Part Network (ProtoPNet) for audio classification that provides inherent interpretability through its model architecture.","Our approach is based on a ConvNeXt backbone architecture for feature extraction and learns prototypical patterns for each bird species using spectrograms of the training data.","Classification of new data is done by comparison with these prototypes in latent space, which simultaneously serve as easily understandable explanations for the model's decisions."],"url":"http://arxiv.org/abs/2404.10420v1","category":"cs.LG"}
{"created":"2024-04-16 09:31:28","title":"Supercoiled ring polymers under shear flow","abstract":"We apply monomer-resolved computer simulations of supercoiled ring polymers under shear, taking full account of the hydrodynamic interactions, accompanied, in parallel, by simulations in which these are switched off. The combination of bending and torsional rigidities inherent in these polymers, in conjunction with hydrodynamics, has a profound impact on their flow properties. In contrast to their flexible counterparts, which dramatcially deform and inflate under shear [Liebetreu et al., Commun. Mater. 1, 4 (2020)], supercoiled rings undergo only weak changes in their overall shape and they display both a reduced propensity to tumbling (at fixed Weissenberg number) and a much stronger orientational resistance with respect to their flexible counterparts. In the presence of hydrodynamic interactions, the coupling of the polymer to solvent flow is capable of bringing about a topological transformation of writhe to twist at strong shear upon conservation of the overall linking number.","sentences":["We apply monomer-resolved computer simulations of supercoiled ring polymers under shear, taking full account of the hydrodynamic interactions, accompanied, in parallel, by simulations in which these are switched off.","The combination of bending and torsional rigidities inherent in these polymers, in conjunction with hydrodynamics, has a profound impact on their flow properties.","In contrast to their flexible counterparts, which dramatcially deform and inflate under shear [Liebetreu et al., Commun.","Mater.","1, 4 (2020)], supercoiled rings undergo only weak changes in their overall shape and they display both a reduced propensity to tumbling (at fixed Weissenberg number) and a much stronger orientational resistance with respect to their flexible counterparts.","In the presence of hydrodynamic interactions, the coupling of the polymer to solvent flow is capable of bringing about a topological transformation of writhe to twist at strong shear upon conservation of the overall linking number."],"url":"http://arxiv.org/abs/2404.10414v1","category":"cond-mat.soft"}
{"created":"2024-04-16 08:08:47","title":"Know Yourself Better: Diverse Discriminative Feature Learning Improves Open Set Recognition","abstract":"Open set recognition (OSR) is a critical aspect of machine learning, addressing the challenge of detecting novel classes during inference. Within the realm of deep learning, neural classifiers trained on a closed set of data typically struggle to identify novel classes, leading to erroneous predictions. To address this issue, various heuristic methods have been proposed, allowing models to express uncertainty by stating \"I don't know.\" However, a gap in the literature remains, as there has been limited exploration of the underlying mechanisms of these methods. In this paper, we conduct an analysis of open set recognition methods, focusing on the aspect of feature diversity. Our research reveals a significant correlation between learning diverse discriminative features and enhancing OSR performance. Building on this insight, we propose a novel OSR approach that leverages the advantages of feature diversity. The efficacy of our method is substantiated through rigorous evaluation on a standard OSR testbench, demonstrating a substantial improvement over state-of-the-art methods.","sentences":["Open set recognition (OSR) is a critical aspect of machine learning, addressing the challenge of detecting novel classes during inference.","Within the realm of deep learning, neural classifiers trained on a closed set of data typically struggle to identify novel classes, leading to erroneous predictions.","To address this issue, various heuristic methods have been proposed, allowing models to express uncertainty by stating \"I don't know.\"","However, a gap in the literature remains, as there has been limited exploration of the underlying mechanisms of these methods.","In this paper, we conduct an analysis of open set recognition methods, focusing on the aspect of feature diversity.","Our research reveals a significant correlation between learning diverse discriminative features and enhancing OSR performance.","Building on this insight, we propose a novel OSR approach that leverages the advantages of feature diversity.","The efficacy of our method is substantiated through rigorous evaluation on a standard OSR testbench, demonstrating a substantial improvement over state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10370v1","category":"cs.CV"}
{"created":"2024-04-16 07:30:11","title":"Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards","abstract":"Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs). However, acquiring human-authored rationales or augmenting rationales from proprietary models is costly and not scalable. In this paper, we study the problem of whether LLMs could self-improve their reasoning capabilities. To this end, we propose Self-Explore, where the LLM is tasked to explore the first wrong step (i.e., the first pit) within the rationale and use such signals as fine-grained rewards for further improvement. On the GSM8K and MATH test set, Self-Explore achieves 11.57% and 2.89% improvement on average across three LLMs compared to supervised fine-tuning (SFT). Our code is available at https://github.com/hbin0701/Self-Explore.","sentences":["Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective at improving the reasoning capabilities of large language models (LLMs).","However, acquiring human-authored rationales or augmenting rationales from proprietary models is costly and not scalable.","In this paper, we study the problem of whether LLMs could self-improve their reasoning capabilities.","To this end, we propose Self-Explore, where the LLM is tasked to explore the first wrong step (i.e., the first pit) within the rationale and use such signals as fine-grained rewards for further improvement.","On the GSM8K and MATH test set, Self-Explore achieves 11.57% and 2.89% improvement on average across three LLMs compared to supervised fine-tuning (SFT).","Our code is available at https://github.com/hbin0701/Self-Explore."],"url":"http://arxiv.org/abs/2404.10346v1","category":"cs.CL"}
{"created":"2024-04-16 07:21:03","title":"Lifetime predictions for virgin and recycled high-density polyethylene under creep conditions","abstract":"Recycling has become a predominant subject in industry and science due to a rising concern for the environment driven by high production volume of plastics. Replacement of virgin polymers with their recycled analogs is not always possible because recycled polymers cannot met the same property profiles as their virgin counterparts. To avoid deterioration of the mechanical properties, it is proposed to replace a virgin polymer with a recycled polymer of another grade whose characteristics (measured in tensile tests) are close to those of the virgin material. This approach opens a way for the use of recycled polymers in short-term application, but its suitability for long-term applications has not yet been assessed. A thorough experimental investigation is conducted of the mechanical response of virgin high-density polyethylene (HDPE) used for insulation of pipes and recycled HDPE manufactured from post-consumer plastic waste (their stiffness, strength and elongation to break adopt similar values). A model is presented in viscoelastoplasticity of semicrystalline polymers. Its parameters are determined by matching experimental data in short-term relaxation and creep tests. The lifetime of virgin and recycled HDPE under creep conditions is evaluated by means of numerical simulation. It is shown that the stress-time to failure diagrams for virgin and recycled HDPE practically coincide.","sentences":["Recycling has become a predominant subject in industry and science due to a rising concern for the environment driven by high production volume of plastics.","Replacement of virgin polymers with their recycled analogs is not always possible because recycled polymers cannot met the same property profiles as their virgin counterparts.","To avoid deterioration of the mechanical properties, it is proposed to replace a virgin polymer with a recycled polymer of another grade whose characteristics (measured in tensile tests) are close to those of the virgin material.","This approach opens a way for the use of recycled polymers in short-term application, but its suitability for long-term applications has not yet been assessed.","A thorough experimental investigation is conducted of the mechanical response of virgin high-density polyethylene (HDPE) used for insulation of pipes and recycled HDPE manufactured from post-consumer plastic waste (their stiffness, strength and elongation to break adopt similar values).","A model is presented in viscoelastoplasticity of semicrystalline polymers.","Its parameters are determined by matching experimental data in short-term relaxation and creep tests.","The lifetime of virgin and recycled HDPE under creep conditions is evaluated by means of numerical simulation.","It is shown that the stress-time to failure diagrams for virgin and recycled HDPE practically coincide."],"url":"http://arxiv.org/abs/2404.10336v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 07:07:41","title":"Investigations on Projection-Based Reduced Order Model Development for Rotating Detonation Engine","abstract":"The current study aims to evaluate and investigate the development of projection-based reduced-order models (ROMs) for efficient and accurate RDE simulations. Specifically, we focus on assessing the projection-based ROM construction utilizing three different approaches: the linear static basis, nonlinear quadratic basis, and an adaptive model order reduction (MOR) formulation. First, an ~\\textit{a priori} analysis is performed to evaluate the effectiveness of the linear static and nonlinear quadratic bases in representing the detonation-wave dynamics. The~\\textit{a priori} analysis reveals that compared to the linear basis, the nonlinear quadratic basis provides significantly improved representation of detonation-wave dynamics within the training regime. However, it exhibits limited capabilities in representing the dynamics beyond the training regime, either in the future state or under a different operating parameter (i.e., inlet velocity). Second, the investigations proceed to the adaptive MOR formulation, which constructs an \\textit{online} adaptive ROM with a small amount of offline training data. It is demonstrated that the adaptive ROM can provide significantly enhanced predictive capabilities in modeling the RDE dynamics in the future state, and subject to parametric variations. More importantly, the adaptive ROM is shown to be capable of capturing the initial transience in establishing the detonation wave.","sentences":["The current study aims to evaluate and investigate the development of projection-based reduced-order models (ROMs) for efficient and accurate RDE simulations.","Specifically, we focus on assessing the projection-based ROM construction utilizing three different approaches: the linear static basis, nonlinear quadratic basis, and an adaptive model order reduction (MOR) formulation.","First, an ~\\textit{a priori} analysis is performed to evaluate the effectiveness of the linear static and nonlinear quadratic bases in representing the detonation-wave dynamics.","The~\\textit{a","priori} analysis reveals that compared to the linear basis, the nonlinear quadratic basis provides significantly improved representation of detonation-wave dynamics within the training regime.","However, it exhibits limited capabilities in representing the dynamics beyond the training regime, either in the future state or under a different operating parameter (i.e., inlet velocity).","Second, the investigations proceed to the adaptive MOR formulation, which constructs an \\textit{online} adaptive ROM with a small amount of offline training data.","It is demonstrated that the adaptive ROM can provide significantly enhanced predictive capabilities in modeling the RDE dynamics in the future state, and subject to parametric variations.","More importantly, the adaptive ROM is shown to be capable of capturing the initial transience in establishing the detonation wave."],"url":"http://arxiv.org/abs/2404.10323v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 06:59:26","title":"Application of Deep Learning Methods to Processing of Noisy Medical Video Data","abstract":"Cells count become a challenging problem when the cells move in a continuous stream, and their boundaries are difficult for visual detection. To resolve this problem we modified the training and decision making processes using curriculum learning and multi-view predictions techniques, respectively.","sentences":["Cells count become a challenging problem when the cells move in a continuous stream, and their boundaries are difficult for visual detection.","To resolve this problem we modified the training and decision making processes using curriculum learning and multi-view predictions techniques, respectively."],"url":"http://arxiv.org/abs/2404.10319v1","category":"cs.CV"}
{"created":"2024-04-16 05:36:39","title":"ControlMTR: Control-Guided Motion Transformer with Scene-Compliant Intention Points for Feasible Motion Prediction","abstract":"The ability to accurately predict feasible multimodal future trajectories of surrounding traffic participants is crucial for behavior planning in autonomous vehicles. The Motion Transformer (MTR), a state-of-the-art motion prediction method, alleviated mode collapse and instability during training and enhanced overall prediction performance by replacing conventional dense future endpoints with a small set of fixed prior motion intention points. However, the fixed prior intention points make the MTR multi-modal prediction distribution over-scattered and infeasible in many scenarios. In this paper, we propose the ControlMTR framework to tackle the aforementioned issues by generating scene-compliant intention points and additionally predicting driving control commands, which are then converted into trajectories by a simple kinematic model with soft constraints. These control-generated trajectories will guide the directly predicted trajectories by an auxiliary loss function. Together with our proposed scene-compliant intention points, they can effectively restrict the prediction distribution within the road boundaries and suppress infeasible off-road predictions while enhancing prediction performance. Remarkably, without resorting to additional model ensemble techniques, our method surpasses the baseline MTR model across all performance metrics, achieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in MissRate. Our approach notably results in a 41.85% reduction in the cross-boundary rate of the MTR, effectively ensuring that the prediction distribution is confined within the drivable area.","sentences":["The ability to accurately predict feasible multimodal future trajectories of surrounding traffic participants is crucial for behavior planning in autonomous vehicles.","The Motion Transformer (MTR), a state-of-the-art motion prediction method, alleviated mode collapse and instability during training and enhanced overall prediction performance by replacing conventional dense future endpoints with a small set of fixed prior motion intention points.","However, the fixed prior intention points make the MTR multi-modal prediction distribution over-scattered and infeasible in many scenarios.","In this paper, we propose the ControlMTR framework to tackle the aforementioned issues by generating scene-compliant intention points and additionally predicting driving control commands, which are then converted into trajectories by a simple kinematic model with soft constraints.","These control-generated trajectories will guide the directly predicted trajectories by an auxiliary loss function.","Together with our proposed scene-compliant intention points, they can effectively restrict the prediction distribution within the road boundaries and suppress infeasible off-road predictions while enhancing prediction performance.","Remarkably, without resorting to additional model ensemble techniques, our method surpasses the baseline MTR model across all performance metrics, achieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in MissRate.","Our approach notably results in a 41.85% reduction in the cross-boundary rate of the MTR, effectively ensuring that the prediction distribution is confined within the drivable area."],"url":"http://arxiv.org/abs/2404.10295v1","category":"cs.RO"}
{"created":"2024-04-16 05:29:14","title":"From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for Efficient Text-based Person Search","abstract":"In text-based person search endeavors, data generation has emerged as a prevailing practice, addressing concerns over privacy preservation and the arduous task of manual annotation. Although the number of synthesized data can be infinite in theory, the scientific conundrum persists that how much generated data optimally fuels subsequent model training. We observe that only a subset of the data in these constructed datasets plays a decisive role. Therefore, we introduce a new Filtering-WoRA paradigm, which contains a filtering algorithm to identify this crucial data subset and WoRA (Weighted Low-Rank Adaptation) learning strategy for light fine-tuning. The filtering algorithm is based on the cross-modality relevance to remove the lots of coarse matching synthesis pairs. As the number of data decreases, we do not need to fine-tune the entire model. Therefore, we propose a WoRA learning strategy to efficiently update a minimal portion of model parameters. WoRA streamlines the learning process, enabling heightened efficiency in extracting knowledge from fewer, yet potent, data instances. Extensive experimentation validates the efficacy of pretraining, where our model achieves advanced and efficient retrieval performance on challenging real-world benchmarks. Notably, on the CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing model training time by 19.82%.","sentences":["In text-based person search endeavors, data generation has emerged as a prevailing practice, addressing concerns over privacy preservation and the arduous task of manual annotation.","Although the number of synthesized data can be infinite in theory, the scientific conundrum persists that how much generated data optimally fuels subsequent model training.","We observe that only a subset of the data in these constructed datasets plays a decisive role.","Therefore, we introduce a new Filtering-WoRA paradigm, which contains a filtering algorithm to identify this crucial data subset and WoRA (Weighted Low-Rank Adaptation) learning strategy for light fine-tuning.","The filtering algorithm is based on the cross-modality relevance to remove the lots of coarse matching synthesis pairs.","As the number of data decreases, we do not need to fine-tune the entire model.","Therefore, we propose a WoRA learning strategy to efficiently update a minimal portion of model parameters.","WoRA streamlines the learning process, enabling heightened efficiency in extracting knowledge from fewer, yet potent, data instances.","Extensive experimentation validates the efficacy of pretraining, where our model achieves advanced and efficient retrieval performance on challenging real-world benchmarks.","Notably, on the CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing model training time by 19.82%."],"url":"http://arxiv.org/abs/2404.10292v1","category":"cs.CV"}
{"created":"2024-04-16 05:28:07","title":"NeuroMorphix: A Novel Brain MRI Asymmetry-specific Feature Construction Approach For Seizure Recurrence Prediction","abstract":"Seizure recurrence is an important concern after an initial unprovoked seizure; without drug treatment, it occurs within 2 years in 40-50% of cases. The decision to treat currently relies on predictors of seizure recurrence risk that are inaccurate, resulting in unnecessary, possibly harmful, treatment in some patients and potentially preventable seizures in others. Because of the link between brain lesions and seizure recurrence, we developed a recurrence prediction tool using machine learning and clinical 3T brain MRI. We developed NeuroMorphix, a feature construction approach based on MRI brain anatomy. Each of seven NeuroMorphix features measures the absolute or relative difference between corresponding regions in each cerebral hemisphere. FreeSurfer was used to segment brain regions and to generate values for morphometric parameters (8 for each cortical region and 5 for each subcortical region). The parameters were then mapped to whole brain NeuroMorphix features, yielding a total of 91 features per subject. Features were generated for a first seizure patient cohort (n = 169) categorised into seizure recurrence and non-recurrence subgroups. State-of-the-art classification algorithms were trained and tested using NeuroMorphix features to predict seizure recurrence. Classification models using the top 5 features, ranked by sequential forward selection, demonstrated excellent performance in predicting seizure recurrence, with area under the ROC curve of 88-93%, accuracy of 83-89%, and F1 score of 83-90%. Highly ranked features aligned with structural alterations known to be associated with epilepsy. This study highlights the potential for targeted, data-driven approaches to aid clinical decision-making in brain disorders.","sentences":["Seizure recurrence is an important concern after an initial unprovoked seizure; without drug treatment, it occurs within 2 years in 40-50% of cases.","The decision to treat currently relies on predictors of seizure recurrence risk that are inaccurate, resulting in unnecessary, possibly harmful, treatment in some patients and potentially preventable seizures in others.","Because of the link between brain lesions and seizure recurrence, we developed a recurrence prediction tool using machine learning and clinical 3T brain MRI.","We developed NeuroMorphix, a feature construction approach based on MRI brain anatomy.","Each of seven NeuroMorphix features measures the absolute or relative difference between corresponding regions in each cerebral hemisphere.","FreeSurfer was used to segment brain regions and to generate values for morphometric parameters (8 for each cortical region and 5 for each subcortical region).","The parameters were then mapped to whole brain NeuroMorphix features, yielding a total of 91 features per subject.","Features were generated for a first seizure patient cohort (n = 169) categorised into seizure recurrence and non-recurrence subgroups.","State-of-the-art classification algorithms were trained and tested using NeuroMorphix features to predict seizure recurrence.","Classification models using the top 5 features, ranked by sequential forward selection, demonstrated excellent performance in predicting seizure recurrence, with area under the ROC curve of 88-93%, accuracy of 83-89%, and F1 score of 83-90%.","Highly ranked features aligned with structural alterations known to be associated with epilepsy.","This study highlights the potential for targeted, data-driven approaches to aid clinical decision-making in brain disorders."],"url":"http://arxiv.org/abs/2404.10290v1","category":"eess.IV"}
{"created":"2024-04-16 03:31:28","title":"Lighter, Better, Faster Multi-Source Domain Adaptation with Gaussian Mixture Models and Optimal Transport","abstract":"In this paper, we tackle Multi-Source Domain Adaptation (MSDA), a task in transfer learning where one adapts multiple heterogeneous, labeled source probability measures towards a different, unlabeled target measure. We propose a novel framework for MSDA, based on Optimal Transport (OT) and Gaussian Mixture Models (GMMs). Our framework has two key advantages. First, OT between GMMs can be solved efficiently via linear programming. Second, it provides a convenient model for supervised learning, especially classification, as components in the GMM can be associated with existing classes. Based on the GMM-OT problem, we propose a novel technique for calculating barycenters of GMMs. Based on this novel algorithm, we propose two new strategies for MSDA: GMM-WBT and GMM-DaDiL. We empirically evaluate our proposed methods on four benchmarks in image classification and fault diagnosis, showing that we improve over the prior art while being faster and involving fewer parameters.","sentences":["In this paper, we tackle Multi-Source Domain Adaptation (MSDA), a task in transfer learning where one adapts multiple heterogeneous, labeled source probability measures towards a different, unlabeled target measure.","We propose a novel framework for MSDA, based on Optimal Transport (OT) and Gaussian Mixture Models (GMMs).","Our framework has two key advantages.","First, OT between GMMs can be solved efficiently via linear programming.","Second, it provides a convenient model for supervised learning, especially classification, as components in the GMM can be associated with existing classes.","Based on the GMM-OT problem, we propose a novel technique for calculating barycenters of GMMs.","Based on this novel algorithm, we propose two new strategies for MSDA: GMM-WBT and GMM-DaDiL. We empirically evaluate our proposed methods on four benchmarks in image classification and fault diagnosis, showing that we improve over the prior art while being faster and involving fewer parameters."],"url":"http://arxiv.org/abs/2404.10261v1","category":"stat.ML"}
{"created":"2024-04-16 03:25:43","title":"CO-oPS: A Mobile App for Community Oversight of Privacy and Security","abstract":"Smartphone users install numerous mobile apps that require access to different information from their devices. Much of this information is very sensitive, and users often struggle to manage these accesses due to their lack of tech expertise and knowledge regarding mobile privacy. Thus, they often seek help from others to make decisions regarding their mobile privacy and security. We embedded these social processes in a mobile app titled \"CO-oPS'' (\"Community Oversight for Privacy and Security\"). CO-oPS allows trusted community members to review one another's apps installed and permissions granted to those apps. Community members can provide feedback to one another regarding their privacy behaviors. Users are also allowed to hide some of their mobile apps that they do not like others to see, ensuring their personal privacy.","sentences":["Smartphone users install numerous mobile apps that require access to different information from their devices.","Much of this information is very sensitive, and users often struggle to manage these accesses due to their lack of tech expertise and knowledge regarding mobile privacy.","Thus, they often seek help from others to make decisions regarding their mobile privacy and security.","We embedded these social processes in a mobile app titled \"CO-oPS'' (\"Community Oversight for Privacy and Security\").","CO-oPS allows trusted community members to review one another's apps installed and permissions granted to those apps.","Community members can provide feedback to one another regarding their privacy behaviors.","Users are also allowed to hide some of their mobile apps that they do not like others to see, ensuring their personal privacy."],"url":"http://arxiv.org/abs/2404.10258v1","category":"cs.HC"}
{"created":"2024-04-16 03:09:40","title":"Kilometer-Level Coupled Modeling Using 40 Million Cores: An Eight-Year Journey of Model Development","abstract":"With current and future leading systems adopting heterogeneous architectures, adapting existing models for heterogeneous supercomputers is of urgent need for improving model resolution and reducing modeling uncertainty. This paper presents our three-week effort on porting a complex earth system model, CESM 2.2, to a 40-million-core Sunway supercomputer. Taking a non-intrusive approach that tries to minimizes manual code modifications, our project tries to achieve both improvement of performance and consistency of the model code. By using a hierarchical grid system and an OpenMP-based offloading toolkit, our porting and parallelization effort covers over 80% of the code, and achieves a simulation speed of 340 SDPD (simulated days per day) for 5-km atmosphere, 265 SDPD for 3-km ocean, and 222 SDPD for a coupled model, thus making multi-year or even multi-decadal experiments at such high resolution possible.","sentences":["With current and future leading systems adopting heterogeneous architectures, adapting existing models for heterogeneous supercomputers is of urgent need for improving model resolution and reducing modeling uncertainty.","This paper presents our three-week effort on porting a complex earth system model, CESM 2.2, to a 40-million-core Sunway supercomputer.","Taking a non-intrusive approach that tries to minimizes manual code modifications, our project tries to achieve both improvement of performance and consistency of the model code.","By using a hierarchical grid system and an OpenMP-based offloading toolkit, our porting and parallelization effort covers over 80% of the code, and achieves a simulation speed of 340 SDPD (simulated days per day) for 5-km atmosphere, 265 SDPD for 3-km ocean, and 222 SDPD for a coupled model, thus making multi-year or even multi-decadal experiments at such high resolution possible."],"url":"http://arxiv.org/abs/2404.10253v1","category":"cs.DC"}
{"created":"2024-04-16 03:08:02","title":"Learning from Offline and Online Experiences: A Hybrid Adaptive Operator Selection Framework","abstract":"In many practical applications, usually, similar optimisation problems or scenarios repeatedly appear. Learning from previous problem-solving experiences can help adjust algorithm components of meta-heuristics, e.g., adaptively selecting promising search operators, to achieve better optimisation performance. However, those experiences obtained from previously solved problems, namely offline experiences, may sometimes provide misleading perceptions when solving a new problem, if the characteristics of previous problems and the new one are relatively different. Learning from online experiences obtained during the ongoing problem-solving process is more instructive but highly restricted by limited computational resources. This paper focuses on the effective combination of offline and online experiences. A novel hybrid framework that learns to dynamically and adaptively select promising search operators is proposed. Two adaptive operator selection modules with complementary paradigms cooperate in the framework to learn from offline and online experiences and make decisions. An adaptive decision policy is maintained to balance the use of those two modules in an online manner. Extensive experiments on 170 widely studied real-value benchmark optimisation problems and a benchmark set with 34 instances for combinatorial optimisation show that the proposed hybrid framework outperforms the state-of-the-art methods. Ablation study verifies the effectiveness of each component of the framework.","sentences":["In many practical applications, usually, similar optimisation problems or scenarios repeatedly appear.","Learning from previous problem-solving experiences can help adjust algorithm components of meta-heuristics, e.g., adaptively selecting promising search operators, to achieve better optimisation performance.","However, those experiences obtained from previously solved problems, namely offline experiences, may sometimes provide misleading perceptions when solving a new problem, if the characteristics of previous problems and the new one are relatively different.","Learning from online experiences obtained during the ongoing problem-solving process is more instructive but highly restricted by limited computational resources.","This paper focuses on the effective combination of offline and online experiences.","A novel hybrid framework that learns to dynamically and adaptively select promising search operators is proposed.","Two adaptive operator selection modules with complementary paradigms cooperate in the framework to learn from offline and online experiences and make decisions.","An adaptive decision policy is maintained to balance the use of those two modules in an online manner.","Extensive experiments on 170 widely studied real-value benchmark optimisation problems and a benchmark set with 34 instances for combinatorial optimisation show that the proposed hybrid framework outperforms the state-of-the-art methods.","Ablation study verifies the effectiveness of each component of the framework."],"url":"http://arxiv.org/abs/2404.10252v1","category":"cs.NE"}
{"created":"2024-04-16 02:53:42","title":"All meromorphic solutions of Fermat-type functional equations","abstract":"In this paper, by making use of properties of elliptic functions, we describe meromorphic solutions of Fermat-type functional equations $f(z)^{n}+f(L(z))^{m}=1$ over the complex plane $\\mathbb{C}$, where $L(z)$ is a nonconstant entire function, $m$ and $n$ are two positive integers. As applications, we also consider meromorphic solutions of Fermat-type difference and $q$-difference equations.","sentences":["In this paper, by making use of properties of elliptic functions, we describe meromorphic solutions of Fermat-type functional equations $f(z)^{n}+f(L(z))^{m}=1$ over the complex plane $\\mathbb{C}$, where $L(z)$ is a nonconstant entire function, $m$ and $n$ are two positive integers.","As applications, we also consider meromorphic solutions of Fermat-type difference and $q$-difference equations."],"url":"http://arxiv.org/abs/2404.10248v1","category":"math.CV"}
{"created":"2024-04-16 02:47:47","title":"A 25-micron single photon sensitive kinetic inductance detector","abstract":"We report measurements characterizing the performance of a kinetic inductance detector array designed for a wavelength of 25 microns and very low optical background level suitable for applications such as a far-infrared instrument on a cryogenically cooled space telescope. In a pulse counting mode of operation at low optical flux, the detectors can resolve individual 25-micron photons. In an integrating mode, the detectors remain photon noise limited over more than six orders of magnitude in absorbed power from 70 zW to 200 fW, with a limiting NEP of 4.6 x 10^-20 W/rtHz at 1 Hz. In addition, the detectors are highly stable with flat power spectra under optical load down to 1 mHz. Operational parameters of the detector are determined including the efficiency of conversion of the incident optical power into quasiparticles in the aluminum absorbing element and the quasiparticle self-recombination constant.","sentences":["We report measurements characterizing the performance of a kinetic inductance detector array designed for a wavelength of 25 microns and very low optical background level suitable for applications such as a far-infrared instrument on a cryogenically cooled space telescope.","In a pulse counting mode of operation at low optical flux, the detectors can resolve individual 25-micron photons.","In an integrating mode, the detectors remain photon noise limited over more than six orders of magnitude in absorbed power from 70 zW to 200 fW, with a limiting NEP of 4.6 x 10^-20","W/rtHz at 1 Hz.","In addition, the detectors are highly stable with flat power spectra under optical load down to 1 mHz.","Operational parameters of the detector are determined including the efficiency of conversion of the incident optical power into quasiparticles in the aluminum absorbing element and the quasiparticle self-recombination constant."],"url":"http://arxiv.org/abs/2404.10246v1","category":"astro-ph.IM"}
{"created":"2024-04-16 02:46:07","title":"An adaptive Euler-Maruyama scheme for SDDEs","abstract":"This paper proposes an adaptive numerical method for stochastic delay differential equations (SDDEs) with a non-global Lipschitz drift term and a non-constant delay, building upon the work of Wei Fang and others. The method adapts the step size based on the growth of the drift term. Differing slightly from the conventional Euler-Maruyama format, this paper addresses the estimation of the delay term by substituting it with the numerically obtained solution closest to the left endpoint.This approach overcomes the challenge of numerical nodes not falling within the nodes after subtracting the delay. The paper proves the convergence of the numerical method for a class of non-global Lipschitz continuous SDDEs under the assumption that the step size function satisfies certain conditions.","sentences":["This paper proposes an adaptive numerical method for stochastic delay differential equations (SDDEs) with a non-global Lipschitz drift term and a non-constant delay, building upon the work of Wei Fang and others.","The method adapts the step size based on the growth of the drift term.","Differing slightly from the conventional Euler-Maruyama format, this paper addresses the estimation of the delay term by substituting it with the numerically obtained solution closest to the left endpoint.","This approach overcomes the challenge of numerical nodes not falling within the nodes after subtracting the delay.","The paper proves the convergence of the numerical method for a class of non-global Lipschitz continuous SDDEs under the assumption that the step size function satisfies certain conditions."],"url":"http://arxiv.org/abs/2404.10244v1","category":"math.NA"}
{"created":"2024-04-16 02:36:21","title":"Disturbance Rejection-Guarded Learning for Vibration Suppression of Two-Inertia Systems","abstract":"Model uncertainty presents significant challenges in vibration suppression of multi-inertia systems, as these systems often rely on inaccurate nominal mathematical models due to system identification errors or unmodeled dynamics. An observer, such as an extended state observer (ESO), can estimate the discrepancy between the inaccurate nominal model and the true model, thus improving control performance via disturbance rejection. The conventional observer design is memoryless in the sense that once its estimated disturbance is obtained and sent to the controller, the datum is discarded. In this research, we propose a seamless integration of ESO and machine learning. On one hand, the machine learning model attempts to model the disturbance. With the assistance of prior information about the disturbance, the observer is expected to achieve faster convergence in disturbance estimation. On the other hand, machine learning benefits from an additional assurance layer provided by the ESO, as any imperfections in the machine learning model can be compensated for by the ESO. We validated the effectiveness of this novel learning-for-control paradigm through simulation and physical tests on two-inertial motion control systems used for vibration studies.","sentences":["Model uncertainty presents significant challenges in vibration suppression of multi-inertia systems, as these systems often rely on inaccurate nominal mathematical models due to system identification errors or unmodeled dynamics.","An observer, such as an extended state observer (ESO), can estimate the discrepancy between the inaccurate nominal model and the true model, thus improving control performance via disturbance rejection.","The conventional observer design is memoryless in the sense that once its estimated disturbance is obtained and sent to the controller, the datum is discarded.","In this research, we propose a seamless integration of ESO and machine learning.","On one hand, the machine learning model attempts to model the disturbance.","With the assistance of prior information about the disturbance, the observer is expected to achieve faster convergence in disturbance estimation.","On the other hand, machine learning benefits from an additional assurance layer provided by the ESO, as any imperfections in the machine learning model can be compensated for by the ESO.","We validated the effectiveness of this novel learning-for-control paradigm through simulation and physical tests on two-inertial motion control systems used for vibration studies."],"url":"http://arxiv.org/abs/2404.10240v1","category":"eess.SY"}
{"created":"2024-04-16 02:36:07","title":"Diffusion assisted image reconstruction in optoacoustic tomography","abstract":"In this paper we consider the problem of acoustic inversion in the context of the optoacoustic tomography image reconstruction problem. By leveraging the ability of the recently proposed diffusion models for image generative tasks among others, we devise an image reconstruction architecture based on a conditional diffusion process. The scheme makes use of an initial image reconstruction, which is preprocessed by an autoencoder to generate an adequate representation. This representation is used as conditional information in a generative diffusion process. Although the computational requirements for training and implementing the architecture are not low, several design choices discussed in the work were made to keep them manageable. Numerical results show that the conditional information allows to properly bias the parameters of the diffusion model to improve the quality of the initial reconstructed image, eliminating artifacts or even reconstructing finer details of the ground-truth image that are not recoverable by the initial image reconstruction method. We also tested the proposal under experimental conditions and the obtained results were in line with those corresponding to the numerical simulations. Improvements in image quality up to 17 % in terms of peak signal-to-noise ratio were observed.","sentences":["In this paper we consider the problem of acoustic inversion in the context of the optoacoustic tomography image reconstruction problem.","By leveraging the ability of the recently proposed diffusion models for image generative tasks among others, we devise an image reconstruction architecture based on a conditional diffusion process.","The scheme makes use of an initial image reconstruction, which is preprocessed by an autoencoder to generate an adequate representation.","This representation is used as conditional information in a generative diffusion process.","Although the computational requirements for training and implementing the architecture are not low, several design choices discussed in the work were made to keep them manageable.","Numerical results show that the conditional information allows to properly bias the parameters of the diffusion model to improve the quality of the initial reconstructed image, eliminating artifacts or even reconstructing finer details of the ground-truth image that are not recoverable by the initial image reconstruction method.","We also tested the proposal under experimental conditions and the obtained results were in line with those corresponding to the numerical simulations.","Improvements in image quality up to 17 % in terms of peak signal-to-noise ratio were observed."],"url":"http://arxiv.org/abs/2404.10239v1","category":"eess.IV"}
{"created":"2024-04-16 02:32:56","title":"Integrated Sensing and Communication for Edge Inference with End-to-End Multi-View Fusion","abstract":"Integrated sensing and communication (ISAC) is a promising solution to accelerate edge inference via the dual use of wireless signals. However, this paradigm needs to minimize the inference error and latency under ISAC co-functionality interference, for which the existing ISAC or edge resource allocation algorithms become inefficient, as they ignore the inter-dependency between low-level ISAC designs and high-level inference services. This letter proposes an inference-oriented ISAC (IO-ISAC) scheme, which minimizes upper bounds on end-to-end inference error and latency using multi-objective optimization. The key to our approach is to derive a multi-view inference model that accounts for both the number of observations and the angles of observations, by integrating a half-voting fusion rule and an angle-aware sensing model. Simulation results show that the proposed IO-ISAC outperforms other benchmarks in terms of both accuracy and latency.","sentences":["Integrated sensing and communication (ISAC) is a promising solution to accelerate edge inference via the dual use of wireless signals.","However, this paradigm needs to minimize the inference error and latency under ISAC co-functionality interference, for which the existing ISAC or edge resource allocation algorithms become inefficient, as they ignore the inter-dependency between low-level ISAC designs and high-level inference services.","This letter proposes an inference-oriented ISAC (IO-ISAC) scheme, which minimizes upper bounds on end-to-end inference error and latency using multi-objective optimization.","The key to our approach is to derive a multi-view inference model that accounts for both the number of observations and the angles of observations, by integrating a half-voting fusion rule and an angle-aware sensing model.","Simulation results show that the proposed IO-ISAC outperforms other benchmarks in terms of both accuracy and latency."],"url":"http://arxiv.org/abs/2404.10235v1","category":"eess.SP"}
{"created":"2024-04-16 01:37:50","title":"Forecasting Tech Sector Market Downturns based on Macroeconomic Indicators","abstract":"Predicting stock price movements is a pivotal element of investment strategy, providing insights into potential trends and market volatility. This study specifically examines the predictive capacity of historical stock prices and technical indicators within the Global Industry Classification Standard (GICS) Information Technology Sector, focusing on companies established before 1980. We aim to identify patterns that precede significant, non-transient downturns - defined as declines exceeding 10% from peak values. Utilizing a combination of machine learning techniques, including multiple regression analysis, logistic regression, we analyze an enriched dataset comprising both macroeconomic indicators and market data. Our findings suggest that certain clusters of technical indicators, when combined with broader economic signals, offer predictive insights into forthcoming sector-specific downturns. This research not only enhances our understanding of the factors driving market dynamics in the tech sector but also provides portfolio managers and investors with a sophisticated tool for anticipating and mitigating potential losses from market downturns. Through a rigorous validation process, we demonstrate the robustness of our models, contributing to the field of financial analytics by offering a novel approach to predicting market downturns with significant implications for investment strategies and economic policy planning.","sentences":["Predicting stock price movements is a pivotal element of investment strategy, providing insights into potential trends and market volatility.","This study specifically examines the predictive capacity of historical stock prices and technical indicators within the Global Industry Classification Standard (GICS) Information Technology Sector, focusing on companies established before 1980.","We aim to identify patterns that precede significant, non-transient downturns - defined as declines exceeding 10% from peak values.","Utilizing a combination of machine learning techniques, including multiple regression analysis, logistic regression, we analyze an enriched dataset comprising both macroeconomic indicators and market data.","Our findings suggest that certain clusters of technical indicators, when combined with broader economic signals, offer predictive insights into forthcoming sector-specific downturns.","This research not only enhances our understanding of the factors driving market dynamics in the tech sector but also provides portfolio managers and investors with a sophisticated tool for anticipating and mitigating potential losses from market downturns.","Through a rigorous validation process, we demonstrate the robustness of our models, contributing to the field of financial analytics by offering a novel approach to predicting market downturns with significant implications for investment strategies and economic policy planning."],"url":"http://arxiv.org/abs/2404.10208v1","category":"cs.CE"}
{"created":"2024-04-16 01:10:09","title":"The Impact of Machine Learning on Society: An Analysis of Current Trends and Future Implications","abstract":"The Machine learning (ML) is a rapidly evolving field of technology that has the potential to greatly impact society in a variety of ways. However, there are also concerns about the potential negative effects of ML on society, such as job displacement and privacy issues. This research aimed to conduct a comprehensive analysis of the current and future impact of ML on society. The research included a thorough literature review, case studies, and surveys to gather data on the economic impact of ML, ethical and privacy implications, and public perceptions of the technology. The survey was conducted on 150 respondents from different areas. The case studies conducted were on the impact of ML on healthcare, finance, transportation, and manufacturing. The findings of this research revealed that the majority of respondents have a moderate level of familiarity with the concept of ML, believe that it has the potential to benefit society, and think that society should prioritize the development and use of ML. Based on these findings, it was recommended that more research is conducted on the impact of ML on society, stronger regulations and laws to protect the privacy and rights of individuals when it comes to ML should be developed, transparency and accountability in ML decision-making processes should be increased, and public education and awareness about ML should be enhanced.","sentences":["The Machine learning (ML) is a rapidly evolving field of technology that has the potential to greatly impact society in a variety of ways.","However, there are also concerns about the potential negative effects of ML on society, such as job displacement and privacy issues.","This research aimed to conduct a comprehensive analysis of the current and future impact of ML on society.","The research included a thorough literature review, case studies, and surveys to gather data on the economic impact of ML, ethical and privacy implications, and public perceptions of the technology.","The survey was conducted on 150 respondents from different areas.","The case studies conducted were on the impact of ML on healthcare, finance, transportation, and manufacturing.","The findings of this research revealed that the majority of respondents have a moderate level of familiarity with the concept of ML, believe that it has the potential to benefit society, and think that society should prioritize the development and use of ML.","Based on these findings, it was recommended that more research is conducted on the impact of ML on society, stronger regulations and laws to protect the privacy and rights of individuals when it comes to ML should be developed, transparency and accountability in ML decision-making processes should be increased, and public education and awareness about ML should be enhanced."],"url":"http://arxiv.org/abs/2404.10204v1","category":"cs.CY"}
{"created":"2024-04-16 00:33:21","title":"Novel Method to Estimate Kinetic Microparameters from Dynamic Whole-Body Imaging in Regular-Axial Field-of-View PET Scanners","abstract":"For whole-body (WB) kinetic modeling based on a typical PET scanner, a multipass multibed scanning protocol is necessary because of the limited axial field of view. Such a protocol introduces loss of early dynamics of the time-activity curve (TAC) and sparsity in TAC measurements, inducing uncertainty in parameter estimation when using prevalent least squares estimation (LSE) (i.e., common standard) especially for kinetic microparameters. We developed and investigated a method to estimate microparameters enabling parametric imaging, by focusing on general image qualities, overall visibility, and tumor detectability. Our method, denoted parameter combination-driven estimation (PCDE), has two distinctive characteristics: 1) improved probability of having one-on-one mapping between early and late dynamics in TACs (the former missing from typical protocols) at the cost of the precision of the estimated parameter, and 2) utilization of multiple aspects of TAC in selection of best fits. To compare the general image quality of the two methods, we plotted tradeoff curves for noise and bias. We also evaluated the impact of different iteration numbers of the OSEM reconstruction algorithm on the tradeoff curves. In addition, for overall visibility, the overall signal-to-noise ratio (SNR) and spatial noise were calculated and compared. Furthermore, the contrast-to-noise ratio (CNR) and relative error of the tumor-to-background ratio were calculated. Furthermore, we implemented and tested the proposed method on patient datasets to further verify clinical applicability. Overall, improved general image quality was verified in microparametric images (i.e., reduction in overall NRMSE). The overall visibility and tumor detectability were also improved. For our patient study, improved overall visibility and tumor detectability were demonstrated in micoparametric images.","sentences":["For whole-body (WB) kinetic modeling based on a typical PET scanner, a multipass multibed scanning protocol is necessary because of the limited axial field of view.","Such a protocol introduces loss of early dynamics of the time-activity curve (TAC) and sparsity in TAC measurements, inducing uncertainty in parameter estimation when using prevalent least squares estimation (LSE) (i.e., common standard) especially for kinetic microparameters.","We developed and investigated a method to estimate microparameters enabling parametric imaging, by focusing on general image qualities, overall visibility, and tumor detectability.","Our method, denoted parameter combination-driven estimation (PCDE), has two distinctive characteristics: 1) improved probability of having one-on-one mapping between early and late dynamics in TACs (the former missing from typical protocols) at the cost of the precision of the estimated parameter, and 2) utilization of multiple aspects of TAC in selection of best fits.","To compare the general image quality of the two methods, we plotted tradeoff curves for noise and bias.","We also evaluated the impact of different iteration numbers of the OSEM reconstruction algorithm on the tradeoff curves.","In addition, for overall visibility, the overall signal-to-noise ratio (SNR) and spatial noise were calculated and compared.","Furthermore, the contrast-to-noise ratio (CNR) and relative error of the tumor-to-background ratio were calculated.","Furthermore, we implemented and tested the proposed method on patient datasets to further verify clinical applicability.","Overall, improved general image quality was verified in microparametric images (i.e., reduction in overall NRMSE).","The overall visibility and tumor detectability were also improved.","For our patient study, improved overall visibility and tumor detectability were demonstrated in micoparametric images."],"url":"http://arxiv.org/abs/2404.10197v1","category":"physics.med-ph"}
{"created":"2024-04-16 00:29:04","title":"Strong Markov dissipation in driven-dissipative quantum systems","abstract":"The Lindblad equation, which describes Markovian quantum dynamics under dissipation, is usually derived under the weak system-bath coupling assumption. Strong system-bath coupling often leads to non-Markov evolution. The singular-coupling limit is known as an exception: it yields a Lindblad equation with an arbitrary strength of dissipation. However, the singular-coupling limit requires high-temperature limit of the bath, and hence the system ends up in a trivial infinite-temperature state, which is not desirable in the context of quantum control. In this work, it is shown that we can derive a Markovian Lindblad equation for an arbitrary strength of the system-bath coupling by considering a new scaling limit that is called the singular-driving limit, which combines the singular-coupling limit and fast periodic driving. In contrast to the standard singular-coupling limit, an interplay between dissipation and periodic driving results in a nontrivial steady state.","sentences":["The Lindblad equation, which describes Markovian quantum dynamics under dissipation, is usually derived under the weak system-bath coupling assumption.","Strong system-bath coupling often leads to non-Markov evolution.","The singular-coupling limit is known as an exception: it yields a Lindblad equation with an arbitrary strength of dissipation.","However, the singular-coupling limit requires high-temperature limit of the bath, and hence the system ends up in a trivial infinite-temperature state, which is not desirable in the context of quantum control.","In this work, it is shown that we can derive a Markovian Lindblad equation for an arbitrary strength of the system-bath coupling by considering a new scaling limit that is called the singular-driving limit, which combines the singular-coupling limit and fast periodic driving.","In contrast to the standard singular-coupling limit, an interplay between dissipation and periodic driving results in a nontrivial steady state."],"url":"http://arxiv.org/abs/2404.10195v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-16 00:28:26","title":"Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering","abstract":"The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of a model, require retraining a model or study only unimodal models. However, the most powerful models (e.g. GPT-4) are typically only available as black boxes with inaccessible internals, are not retrainable by end-users, and are frequently used for multimodal tasks. We study the possibility of selective prediction for vision-language models in a realistic, black-box setting. We propose using the principle of \\textit{neighborhood consistency} to identify unreliable responses from a black-box vision-language model in question answering tasks. We hypothesize that given only a visual question and model response, the consistency of the model's responses over the neighborhood of a visual question will indicate reliability. It is impossible to directly sample neighbors in feature space in a black-box setting. Instead, we show that it is possible to use a smaller proxy model to approximately sample from the neighborhood. We find that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model.","sentences":["The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts.","Existing approaches to selective prediction typically require access to the internals of a model, require retraining a model or study only unimodal models.","However, the most powerful models (e.g. GPT-4) are typically only available as black boxes with inaccessible internals, are not retrainable by end-users, and are frequently used for multimodal tasks.","We study the possibility of selective prediction for vision-language models in a realistic, black-box setting.","We propose using the principle of \\textit{neighborhood consistency} to identify unreliable responses from a black-box vision-language model in question answering tasks.","We hypothesize that given only a visual question and model response, the consistency of the model's responses over the neighborhood of a visual question will indicate reliability.","It is impossible to directly sample neighbors in feature space in a black-box setting.","Instead, we show that it is possible to use a smaller proxy model to approximately sample from the neighborhood.","We find that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model."],"url":"http://arxiv.org/abs/2404.10193v1","category":"cs.CV"}
{"created":"2024-04-16 00:17:20","title":"Minimizing effects of the Kalman gain on Posterior covariance Eigenvalues, the characteristic polynomial and symmetric polynomials of Eigenvalues","abstract":"The Kalman gain is commonly derived as the minimizer of the trace of theposterior covariance. It is known that it also minimizes the determinant of the posterior covariance. I will show that it also minimizes the smallest Eigenvalue $\\lambda_1$ and the chracteristic polynomial on $(-\\infty,\\lambda_1)$ and is critical point to all symmetric polynomials of the Eigenvalues, minimizing some. This expands the range of uncertainty measures for which the Kalman Filter is optimal.","sentences":["The Kalman gain is commonly derived as the minimizer of the trace of theposterior covariance.","It is known that it also minimizes the determinant of the posterior covariance.","I will show that it also minimizes the smallest Eigenvalue $\\lambda_1$ and the chracteristic polynomial on $(-\\infty,\\lambda_1)$ and is critical point to all symmetric polynomials of the Eigenvalues, minimizing some.","This expands the range of uncertainty measures for which the Kalman Filter is optimal."],"url":"http://arxiv.org/abs/2404.10191v1","category":"math.ST"}
{"created":"2024-04-16 00:15:33","title":"Holographic Renormalization Group and Stress Tensor Operators","abstract":"The holographic duality conjectures a relation between strongly coupled quantum systems and quantum gravity in higher-dimensional spacetimes. Gravitational theories in two and three dimensions are meaningful examples for classical and quantum exploration due to their unique characteristics, notably the absence of propagating bulk degrees of freedom and the presence of only boundary degrees of freedom, distinguishing them from higher-dimensional counterparts. These gravitational theories exhibit complex interactions when the bulk spacetime has a finite size, regulated by Zamolodchikov's double-trace irrelevant $T\\overline{T}$ operator.   This thesis aims to gain a holographic understanding of $\\mathrm{AdS}_3$ and JT gravity under the influence of the $T\\overline{T}$ deformation. Under a finite radial cutoff, these theories exhibit perturbative behavior that implies the emergence of the Nambu-Goto action for the corresponding boundary graviton action. We also conducted semi-classical calculations of observables related to finite-cutoff gravity and its dual $T\\overline{T}$-deformed CFT description, including correlation functions involving stress tensors and gravitational Wilson lines, along with an analysis of their supersymmetric extensions. Additionally, we explored the implications of general stress tensor deformations within field-theoretic and holographic settings.   This thesis integrates previously adapted publications while also pioneering new ground, notably exploring the definition of a quantum $T\\overline{T}$ operator beyond two dimensions with $\\frac{1}{N}$ corrections, investigating quantum-corrected higher point correlators for a planar boundary, and offering insights into a two-dimensional spherical boundary at a finite cutoff. Furthermore, throughout the thesis, we show more details in calculations at various points.","sentences":["The holographic duality conjectures a relation between strongly coupled quantum systems and quantum gravity in higher-dimensional spacetimes.","Gravitational theories in two and three dimensions are meaningful examples for classical and quantum exploration due to their unique characteristics, notably the absence of propagating bulk degrees of freedom and the presence of only boundary degrees of freedom, distinguishing them from higher-dimensional counterparts.","These gravitational theories exhibit complex interactions when the bulk spacetime has a finite size, regulated by Zamolodchikov's double-trace irrelevant $T\\overline{T}$ operator.   ","This thesis aims to gain a holographic understanding of $\\mathrm{AdS}_3$ and JT gravity under the influence of the $T\\overline{T}$ deformation.","Under a finite radial cutoff, these theories exhibit perturbative behavior that implies the emergence of the Nambu-Goto action for the corresponding boundary graviton action.","We also conducted semi-classical calculations of observables related to finite-cutoff gravity and its dual $T\\overline{T}$-deformed CFT description, including correlation functions involving stress tensors and gravitational Wilson lines, along with an analysis of their supersymmetric extensions.","Additionally, we explored the implications of general stress tensor deformations within field-theoretic and holographic settings.   ","This thesis integrates previously adapted publications while also pioneering new ground, notably exploring the definition of a quantum $T\\overline{T}$ operator beyond two dimensions with $\\frac{1}{N}$ corrections, investigating quantum-corrected higher point correlators for a planar boundary, and offering insights into a two-dimensional spherical boundary at a finite cutoff.","Furthermore, throughout the thesis, we show more details in calculations at various points."],"url":"http://arxiv.org/abs/2404.10190v1","category":"hep-th"}
{"created":"2024-04-15 23:56:03","title":"SoK (or SoLK?): On the Quantitative Study of Sociodemographic Factors and Computer Security Behaviors","abstract":"Researchers are increasingly exploring how gender, culture, and other sociodemographic factors correlate with user computer security and privacy behaviors. To more holistically understand relationships between these factors and behaviors, we make two contributions. First, we broadly survey existing scholarship on sociodemographics and secure behavior (151 papers) before conducting a focused literature review of 47 papers to synthesize what is currently known and identify open questions for future research. Second, by incorporating contemporary social and critical theories, we establish guidelines for future studies of sociodemographic factors and security behaviors that address how to overcome common pitfalls. We present a case study to demonstrate our guidelines in action, at-scale, that conduct a measurement study of the relationships between sociodemographics and de-identified, aggregated log data of security and privacy behaviors among 16,829 users on Facebook across 16 countries. Through these contributions, we position our work as a systemization of a lack of knowledge (SoLK). Overall, we find contradictory results and vast unknowns about how identity shapes security behavior. Through our guidelines and discussion, we chart new directions to more deeply examine how and why sociodemographic factors affect security behaviors.","sentences":["Researchers are increasingly exploring how gender, culture, and other sociodemographic factors correlate with user computer security and privacy behaviors.","To more holistically understand relationships between these factors and behaviors, we make two contributions.","First, we broadly survey existing scholarship on sociodemographics and secure behavior (151 papers) before conducting a focused literature review of 47 papers to synthesize what is currently known and identify open questions for future research.","Second, by incorporating contemporary social and critical theories, we establish guidelines for future studies of sociodemographic factors and security behaviors that address how to overcome common pitfalls.","We present a case study to demonstrate our guidelines in action, at-scale, that conduct a measurement study of the relationships between sociodemographics and de-identified, aggregated log data of security and privacy behaviors among 16,829 users on Facebook across 16 countries.","Through these contributions, we position our work as a systemization of a lack of knowledge (SoLK).","Overall, we find contradictory results and vast unknowns about how identity shapes security behavior.","Through our guidelines and discussion, we chart new directions to more deeply examine how and why sociodemographic factors affect security behaviors."],"url":"http://arxiv.org/abs/2404.10187v1","category":"cs.CR"}
{"created":"2024-04-15 23:54:56","title":"General theory for longitudinal nonreciprocal charge transport","abstract":"The longitudinal nonreciprocal charge transport (NCT) in crystalline materials is a highly non-trivial phenomenon, motivating the design of next generation two-terminal rectification devices (e.g., semiconductor diodes beyond PN junctions). The practical application of such devices is built upon crystalline materials whose longitudinal NCT occurs at room temperature and under low magnetic field. However, materials of this type are rather rare and elusive, and theory guiding the discovery of these materials is lacking. Here, we develop such a theory within the framework of semiclassical Boltzmann transport theory. By symmetry analysis, we classify the complete 122 magnetic point groups with respect to the longitudinal NCT phenomenon. The symmetry-adapted Hamiltonian analysis further uncovers a previously overlooked mechanism for this phenomenon. Our theory guides the first-principles prediction of longitudinal NCT in multiferroic \\epsilon-Fe2O3 semiconductor that possibly occurs at room temperature, without the application of external magnetic field. These findings advance our fundamental understandings of longitudinal NCT in crystalline materials, and aid the corresponding materials discoveries.","sentences":["The longitudinal nonreciprocal charge transport (NCT) in crystalline materials is a highly non-trivial phenomenon, motivating the design of next generation two-terminal rectification devices (e.g., semiconductor diodes beyond PN junctions).","The practical application of such devices is built upon crystalline materials whose longitudinal NCT occurs at room temperature and under low magnetic field.","However, materials of this type are rather rare and elusive, and theory guiding the discovery of these materials is lacking.","Here, we develop such a theory within the framework of semiclassical Boltzmann transport theory.","By symmetry analysis, we classify the complete 122 magnetic point groups with respect to the longitudinal NCT phenomenon.","The symmetry-adapted Hamiltonian analysis further uncovers a previously overlooked mechanism for this phenomenon.","Our theory guides the first-principles prediction of longitudinal NCT in multiferroic \\epsilon-Fe2O3 semiconductor that possibly occurs at room temperature, without the application of external magnetic field.","These findings advance our fundamental understandings of longitudinal NCT in crystalline materials, and aid the corresponding materials discoveries."],"url":"http://arxiv.org/abs/2404.10186v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 22:50:42","title":"Numerical Attributes Learning for Cardiac Failure Diagnostic from Clinical Narratives - A LESA-CamemBERT-bio Approach","abstract":"Medical records created by healthcare professionals upon patient admission are rich in details critical for diagnosis. Yet, their potential is not fully realized because of obstacles such as complex medical language, inadequate comprehension of medical numerical data by state-of-the-art Large Language Models (LLMs), and the limitations imposed by small annotated training datasets. This research aims to classify numerical values extracted from medical documents across seven distinct physiological categories, employing CamemBERT-bio. Previous studies suggested that transformer-based models might not perform as well as traditional NLP models in such tasks. To enhance CamemBERT-bio's performances, we introduce two main innovations: integrating keyword embeddings into the model and adopting a number-agnostic strategy by excluding all numerical data from the text. The implementation of label embedding techniques refines the attention mechanisms, while the technique of using a `numerical-blind' dataset aims to bolster context-centric learning. Another key component of our research is determining the criticality of extracted numerical data. To achieve this, we utilized a simple approach that involves verifying if the value falls within the established standard ranges Our findings are encouraging, showing substantial improvements in the effectiveness of CamemBERT-bio, surpassing conventional methods with an F1 score of 0.89. This represents an over 20\\% increase over the 0.73 $F_1$ score of traditional approaches and an over 9\\% increase over the 0.82 $F_1$ score of state-of-the-art approaches.","sentences":["Medical records created by healthcare professionals upon patient admission are rich in details critical for diagnosis.","Yet, their potential is not fully realized because of obstacles such as complex medical language, inadequate comprehension of medical numerical data by state-of-the-art Large Language Models (LLMs), and the limitations imposed by small annotated training datasets.","This research aims to classify numerical values extracted from medical documents across seven distinct physiological categories, employing CamemBERT-bio.","Previous studies suggested that transformer-based models might not perform as well as traditional NLP models in such tasks.","To enhance CamemBERT-bio's performances, we introduce two main innovations: integrating keyword embeddings into the model and adopting a number-agnostic strategy by excluding all numerical data from the text.","The implementation of label embedding techniques refines the attention mechanisms, while the technique of using a `numerical-blind' dataset aims to bolster context-centric learning.","Another key component of our research is determining the criticality of extracted numerical data.","To achieve this, we utilized a simple approach that involves verifying if the value falls within the established standard ranges Our findings are encouraging, showing substantial improvements in the effectiveness of CamemBERT-bio, surpassing conventional methods with an F1 score of 0.89.","This represents an over 20\\% increase over the 0.73 $F_1$ score of traditional approaches and an over 9\\% increase over the 0.82 $F_1$ score of state-of-the-art approaches."],"url":"http://arxiv.org/abs/2404.10171v1","category":"eess.SP"}
{"created":"2024-04-15 22:32:50","title":"Self-Supervised Learning Featuring Small-Scale Image Dataset for Treatable Retinal Diseases Classification","abstract":"Automated medical diagnosis through image-based neural networks has increased in popularity and matured over years. Nevertheless, it is confined by the scarcity of medical images and the expensive labor annotation costs. Self-Supervised Learning (SSL) is an good alternative to Transfer Learning (TL) and is suitable for imbalanced image datasets. In this study, we assess four pretrained SSL models and two TL models in treatable retinal diseases classification using small-scale Optical Coherence Tomography (OCT) images ranging from 125 to 4000 with balanced or imbalanced distribution for training. The proposed SSL model achieves the state-of-art accuracy of 98.84% using only 4,000 training images. Our results suggest the SSL models provide superior performance under both the balanced and imbalanced training scenarios. The SSL model with MoCo-v2 scheme has consistent good performance under the imbalanced scenario and, especially, surpasses the other models when the training set is less than 500 images.","sentences":["Automated medical diagnosis through image-based neural networks has increased in popularity and matured over years.","Nevertheless, it is confined by the scarcity of medical images and the expensive labor annotation costs.","Self-Supervised Learning (SSL) is an good alternative to Transfer Learning (TL) and is suitable for imbalanced image datasets.","In this study, we assess four pretrained SSL models and two TL models in treatable retinal diseases classification using small-scale Optical Coherence Tomography (OCT) images ranging from 125 to 4000 with balanced or imbalanced distribution for training.","The proposed SSL model achieves the state-of-art accuracy of 98.84% using only 4,000 training images.","Our results suggest the SSL models provide superior performance under both the balanced and imbalanced training scenarios.","The SSL model with MoCo-v2 scheme has consistent good performance under the imbalanced scenario and, especially, surpasses the other models when the training set is less than 500 images."],"url":"http://arxiv.org/abs/2404.10166v1","category":"cs.CV"}
{"created":"2024-04-15 22:02:58","title":"Quality Assessment of Prompts Used in Code Generation","abstract":"Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code-generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models possibly have data contamination issues.","sentences":["Large Language Models (LLMs) are gaining popularity among software engineers.","A crucial aspect of developing effective code-generation LLMs is to evaluate these models using a robust benchmark.","Evaluation benchmarks with quality issues can provide a false sense of performance.","In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models.","To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them.","We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance.","We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness.","We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model.","These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style.","Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation.","We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models possibly have data contamination issues."],"url":"http://arxiv.org/abs/2404.10155v1","category":"cs.SE"}
{"created":"2024-04-15 21:42:20","title":"TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition","abstract":"Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.","sentences":["Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data.","Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length.","In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task.","In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input.","Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications.","Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%.","Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%.","These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt).","TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance."],"url":"http://arxiv.org/abs/2404.10150v1","category":"cs.CL"}
{"created":"2024-04-15 21:35:25","title":"Node Similarities under Random Projections: Limits and Pathological Cases","abstract":"Random Projections have been widely used to generate embeddings for various graph tasks due to their computational efficiency. The majority of applications have been justified through the Johnson-Lindenstrauss Lemma. In this paper, we take a step further and investigate how well dot product and cosine similarity are preserved by Random Projections. Our analysis provides new theoretical results, identifies pathological cases, and tests them with numerical experiments. We find that, for nodes of lower or higher degrees, the method produces especially unreliable embeddings for the dot product, regardless of whether the adjacency or the (normalized version) transition is used. With respect to the statistical noise introduced by Random Projections, we show that cosine similarity produces remarkably more precise approximations.","sentences":["Random Projections have been widely used to generate embeddings for various graph tasks due to their computational efficiency.","The majority of applications have been justified through the Johnson-Lindenstrauss Lemma.","In this paper, we take a step further and investigate how well dot product and cosine similarity are preserved by Random Projections.","Our analysis provides new theoretical results, identifies pathological cases, and tests them with numerical experiments.","We find that, for nodes of lower or higher degrees, the method produces especially unreliable embeddings for the dot product, regardless of whether the adjacency or the (normalized version) transition is used.","With respect to the statistical noise introduced by Random Projections, we show that cosine similarity produces remarkably more precise approximations."],"url":"http://arxiv.org/abs/2404.10148v1","category":"cs.SI"}
{"created":"2024-04-15 21:28:43","title":"Nonnegative Ricci curvature, splitting at infinity, and first Betti number rigidity","abstract":"We study the rigidity problems for open (complete and noncompact) $n$-manifolds with nonnegative Ricci curvature. We prove that if an asymptotic cone of $M$ properly contains a Euclidean $\\mathbb{R}^{k-1}$, then the first Betti number of $M$ is at most $n-k$; moreover, if equality holds, then $M$ is flat. Next, we study the geometry of the orbit $\\Gamma\\tilde{p}$, where $\\Gamma=\\pi_1(M,p)$ acts on the universal cover $(\\widetilde{M},\\tilde{p})$. Under a similar asymptotic condition, we prove a geometric rigidity in terms of the growth order of $\\Gamma\\tilde{p}$. We also give the first example of a manifold $M$ of $\\mathrm{Ric}>0$ and $\\pi_1(M)=\\mathbb{Z}$ but with a varying orbit growth order.","sentences":["We study the rigidity problems for open (complete and noncompact) $n$-manifolds with nonnegative Ricci curvature.","We prove that if an asymptotic cone of $M$ properly contains a Euclidean $\\mathbb{R}^{k-1}$, then the first Betti number of $M$ is at most $n-k$; moreover, if equality holds, then $M$ is flat.","Next, we study the geometry of the orbit $\\Gamma\\tilde{p}$, where $\\Gamma=\\pi_1(M,p)$ acts on the universal cover $(\\widetilde{M},\\tilde{p})$. Under a similar asymptotic condition, we prove a geometric rigidity in terms of the growth order of $\\Gamma\\tilde{p}$. We also give the first example of a manifold $M$ of $\\mathrm{Ric}>0$ and $\\pi_1(M)=\\mathbb{Z}$ but with a varying orbit growth order."],"url":"http://arxiv.org/abs/2404.10145v1","category":"math.DG"}
{"created":"2024-04-15 21:27:03","title":"Grain boundary metastability controls irradiation resistance in nanocrystalline metals","abstract":"Grain boundaries (GBs) in polycrystalline materials are powerful sinks for irradiation defects. While standard theories assume that the sink efficiency of a grain boundary is defined solely by its character before irradiation, recent evidence conclusively shows that the irradiation sink efficiency is a highly dynamic property controlled by the intrinsic metastability of GBs under far-from-equilibrium irradiation conditions. In this paper, we reveal that the denuded (i.e., defect-free) zone, typically the signature of a strong sink, can collapse as irradiation damage accumulates. We propose a radiation damage evolution model that captures this behavior based on the emergence of a series of irradiation defect-enabled metastable GB microstate changes that dynamically alter the ability of the GB to absorb further damage. We show that these microstate changes control further defect absorption and give rise to the formation of a defect network that manifests itself as a net Nye-tensor signal detectable via lattice curvature experiments.","sentences":["Grain boundaries (GBs) in polycrystalline materials are powerful sinks for irradiation defects.","While standard theories assume that the sink efficiency of a grain boundary is defined solely by its character before irradiation, recent evidence conclusively shows that the irradiation sink efficiency is a highly dynamic property controlled by the intrinsic metastability of GBs under far-from-equilibrium irradiation conditions.","In this paper, we reveal that the denuded (i.e., defect-free) zone, typically the signature of a strong sink, can collapse as irradiation damage accumulates.","We propose a radiation damage evolution model that captures this behavior based on the emergence of a series of irradiation defect-enabled metastable GB microstate changes that dynamically alter the ability of the GB to absorb further damage.","We show that these microstate changes control further defect absorption and give rise to the formation of a defect network that manifests itself as a net Nye-tensor signal detectable via lattice curvature experiments."],"url":"http://arxiv.org/abs/2404.10144v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 21:19:10","title":"ANCHOR: LLM-driven News Subject Conditioning for Text-to-Image Synthesis","abstract":"Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing synthesized image quality, but current datasets evaluate model performance only on descriptive, instruction-based prompts. Real-world news image captions take a more pragmatic approach, providing high-level situational and Named-Entity (NE) information and limited physical object descriptions, making them abstractive. To evaluate the ability of T2I models to capture intended subjects from news captions, we introduce the Abstractive News Captions with High-level cOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5 different news media organizations. With Large Language Models (LLM) achieving success in language and commonsense reasoning tasks, we explore the ability of different LLMs to identify and understand key subjects from abstractive captions. Our proposed method Subject-Aware Finetuning (SAFE), selects and enhances the representation of key subjects in synthesized images by leveraging LLM-generated subject weights. It also adapts to the domain distribution of news images and captions through custom Domain Fine-tuning, outperforming current T2I baselines on ANCHOR. By launching the ANCHOR dataset, we hope to motivate research in furthering the Natural Language Understanding (NLU) capabilities of T2I models.","sentences":["Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing synthesized image quality, but current datasets evaluate model performance only on descriptive, instruction-based prompts.","Real-world news image captions take a more pragmatic approach, providing high-level situational and Named-Entity (NE) information and limited physical object descriptions, making them abstractive.","To evaluate the ability of T2I models to capture intended subjects from news captions, we introduce the Abstractive News Captions with High-level cOntext Representation (ANCHOR) dataset, containing 70K+ samples sourced from 5 different news media organizations.","With Large Language Models (LLM) achieving success in language and commonsense reasoning tasks, we explore the ability of different LLMs to identify and understand key subjects from abstractive captions.","Our proposed method Subject-Aware Finetuning (SAFE), selects and enhances the representation of key subjects in synthesized images by leveraging LLM-generated subject weights.","It also adapts to the domain distribution of news images and captions through custom Domain Fine-tuning, outperforming current T2I baselines on ANCHOR.","By launching the ANCHOR dataset, we hope to motivate research in furthering the Natural Language Understanding (NLU) capabilities of T2I models."],"url":"http://arxiv.org/abs/2404.10141v1","category":"cs.CV"}
{"created":"2024-04-15 21:03:38","title":"Bayesian Networks for Variational System Identification","abstract":"This paper details how the Bayesian-network structure of the posterior distribution of state-space models can be exploited to build improved parameterizations for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution-smoother; each resulting in a different parameter estimation method. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization methods, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using and embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and be work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.","sentences":["This paper details how the Bayesian-network structure of the posterior distribution of state-space models can be exploited to build improved parameterizations for system identification using variational inference.","Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution-smoother; each resulting in a different parameter estimation method.","In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods.","Furthermore, when applied to mini-batches in conjunction with stochastic optimization methods, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets.","For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using and embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs).","The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and be work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control."],"url":"http://arxiv.org/abs/2404.10137v1","category":"stat.AP"}
{"created":"2024-04-15 20:21:57","title":"Where are the N-Koszul algebras of finite global dimension?","abstract":"The class of $N$-Koszul graded algebras of finite global dimension has gained lots of attention in recent years, especially in the study of Artin-Schelter regular algebras. While structurally rich and concrete, the only known examples of such algebras are either when $N = 2$, i.e. the algebra is Koszul, or when $N = 3$. Under a mild Hilbert series assumption, we rule out the existence of $N$-Koszul graded algebras of finite global dimension for $N$ not prime. Furthermore, we establish strong restrictions on the global dimension of such algebras. This suggests that perhaps the existence of 3-Koszul algebras with finite global dimension and `nice' Hilbert series is an anomaly.","sentences":["The class of $N$-Koszul graded algebras of finite global dimension has gained lots of attention in recent years, especially in the study of Artin-Schelter regular algebras.","While structurally rich and concrete, the only known examples of such algebras are either when $N = 2$, i.e. the algebra is Koszul, or when $N = 3$. Under a mild Hilbert series assumption, we rule out the existence of $N$-Koszul graded algebras of finite global dimension for $N$ not prime.","Furthermore, we establish strong restrictions on the global dimension of such algebras.","This suggests that perhaps the existence of 3-Koszul algebras with finite global dimension and `nice' Hilbert series is an anomaly."],"url":"http://arxiv.org/abs/2404.10125v1","category":"math.RA"}
{"created":"2024-04-15 20:21:05","title":"Epistemic Uncertainty Quantification For Pre-trained Neural Network","abstract":"Epistemic uncertainty quantification (UQ) identifies where models lack knowledge. Traditional UQ methods, often based on Bayesian neural networks, are not suitable for pre-trained non-Bayesian models. Our study addresses quantifying epistemic uncertainty for any pre-trained model, which does not need the original training data or model modifications and can ensure broad applicability regardless of network architectures or training techniques. Specifically, we propose a gradient-based approach to assess epistemic uncertainty, analyzing the gradients of outputs relative to model parameters, and thereby indicating necessary model adjustments to accurately represent the inputs. We first explore theoretical guarantees of gradient-based methods for epistemic UQ, questioning the view that this uncertainty is only calculable through differences between multiple models. We further improve gradient-driven UQ by using class-specific weights for integrating gradients and emphasizing distinct contributions from neural network layers. Additionally, we enhance UQ accuracy by combining gradient and perturbation methods to refine the gradients. We evaluate our approach on out-of-distribution detection, uncertainty calibration, and active learning, demonstrating its superiority over current state-of-the-art UQ methods for pre-trained models.","sentences":["Epistemic uncertainty quantification (UQ) identifies where models lack knowledge.","Traditional UQ methods, often based on Bayesian neural networks, are not suitable for pre-trained non-Bayesian models.","Our study addresses quantifying epistemic uncertainty for any pre-trained model, which does not need the original training data or model modifications and can ensure broad applicability regardless of network architectures or training techniques.","Specifically, we propose a gradient-based approach to assess epistemic uncertainty, analyzing the gradients of outputs relative to model parameters, and thereby indicating necessary model adjustments to accurately represent the inputs.","We first explore theoretical guarantees of gradient-based methods for epistemic UQ, questioning the view that this uncertainty is only calculable through differences between multiple models.","We further improve gradient-driven UQ by using class-specific weights for integrating gradients and emphasizing distinct contributions from neural network layers.","Additionally, we enhance UQ accuracy by combining gradient and perturbation methods to refine the gradients.","We evaluate our approach on out-of-distribution detection, uncertainty calibration, and active learning, demonstrating its superiority over current state-of-the-art UQ methods for pre-trained models."],"url":"http://arxiv.org/abs/2404.10124v1","category":"cs.LG"}
{"created":"2024-04-15 20:19:18","title":"Online Estimation via Offline Estimation: An Information-Theoretic Framework","abstract":"$ $The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (\"offline estimation\"), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (\"online estimation\"). Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion? We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream. Our main results settle the statistical and computational complexity of online estimation in this framework.   $\\bullet$ Statistical complexity. We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.   $\\bullet$ Computational complexity. We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.   Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making.","sentences":["$ $The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (\"offline estimation\"), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (\"online estimation\").","Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion?","We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream.","Our main results settle the statistical and computational complexity of online estimation in this framework.   ","$\\bullet$ Statistical complexity.","We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.   ","$\\bullet$ Computational complexity.","We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.   ","Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making."],"url":"http://arxiv.org/abs/2404.10122v1","category":"stat.ML"}
{"created":"2024-04-15 20:19:14","title":"Convergence of reputations under indirect reciprocity","abstract":"Previous research has shown how indirect reciprocity can promote cooperation through evolutionary game theoretic models. Most work in this field assumes a separation of time-scales: individuals' reputations equilibrate at a fast time scale for given frequencies of strategies while the strategies change slowly according to the replicator dynamics. Much of the previous research has focused on the behaviour and stability of equilibria for the replicator dynamics. Here we focus on the underlying reputational dynamics that occur on a fast time scale. We describe reputational dynamics as systems of differential equations and conduct stability analyses on their equilibria. We prove that reputations converge to a unique equilibrium for each of the five standard norms whether assessments are public or private. These results confirm a crucial but previously unconfirmed assumption underlying the theory of indirect reciprocity for the most studied set of norms.","sentences":["Previous research has shown how indirect reciprocity can promote cooperation through evolutionary game theoretic models.","Most work in this field assumes a separation of time-scales: individuals' reputations equilibrate at a fast time scale for given frequencies of strategies while the strategies change slowly according to the replicator dynamics.","Much of the previous research has focused on the behaviour and stability of equilibria for the replicator dynamics.","Here we focus on the underlying reputational dynamics that occur on a fast time scale.","We describe reputational dynamics as systems of differential equations and conduct stability analyses on their equilibria.","We prove that reputations converge to a unique equilibrium for each of the five standard norms whether assessments are public or private.","These results confirm a crucial but previously unconfirmed assumption underlying the theory of indirect reciprocity for the most studied set of norms."],"url":"http://arxiv.org/abs/2404.10121v1","category":"q-bio.PE"}
{"created":"2024-04-15 20:13:42","title":"Stochastic Thermodynamics at the Quantum-Classical Boundary: A Self-Consistent Framework Based on Adiabatic-Response Theory","abstract":"Microscopic thermal machines promise to play an important role in future quantum technologies. Making such devices widely applicable will require effective strategies to channel their output into easily accessible storage systems like classical degrees of freedom. Here, we develop a self-consistent theoretical framework that makes it possible to model such quantum-classical hybrid devices in a thermodynamically consistent manner. Our approach is based on the assumption that the quantum part of the device is subject to strong decoherence and dissipation induced by a thermal reservoir. Due to the ensuing separation of time scales between slowly evolving classical and fast relaxing quantum degrees of freedom, the dynamics of the hybrid system can be described by means of adiabatic-response theory. We show that, upon including fluctuations in a minimally consistent way, the resulting equations of motion can be equipped with a first and second law, both on the ensemble level and on the level of individual trajectories of the classical part of the system, where thermodynamic quantities like heat and work become stochastic variables. As an application of our theory, we work out a physically transparent model of a quantum-classical hybrid engine, whose working system consists of a chain of Rydberg atoms, which is confined in an optical cavity and driven by periodic temperature variations. We show by means of numerical simulations that the engine can sustain periodic oscillations of a movable mirror, which acts as a classical load, against external friction and extract the full distributions of input heat and output work. By making the statistics of thermodynamic processes in quantum-classical hybrid systems accessible without the need to further specify a measurement protocol, our work contributes towards bridging the long-standing gap between classical and quantum stochastic thermodynamics.","sentences":["Microscopic thermal machines promise to play an important role in future quantum technologies.","Making such devices widely applicable will require effective strategies to channel their output into easily accessible storage systems like classical degrees of freedom.","Here, we develop a self-consistent theoretical framework that makes it possible to model such quantum-classical hybrid devices in a thermodynamically consistent manner.","Our approach is based on the assumption that the quantum part of the device is subject to strong decoherence and dissipation induced by a thermal reservoir.","Due to the ensuing separation of time scales between slowly evolving classical and fast relaxing quantum degrees of freedom, the dynamics of the hybrid system can be described by means of adiabatic-response theory.","We show that, upon including fluctuations in a minimally consistent way, the resulting equations of motion can be equipped with a first and second law, both on the ensemble level and on the level of individual trajectories of the classical part of the system, where thermodynamic quantities like heat and work become stochastic variables.","As an application of our theory, we work out a physically transparent model of a quantum-classical hybrid engine, whose working system consists of a chain of Rydberg atoms, which is confined in an optical cavity and driven by periodic temperature variations.","We show by means of numerical simulations that the engine can sustain periodic oscillations of a movable mirror, which acts as a classical load, against external friction and extract the full distributions of input heat and output work.","By making the statistics of thermodynamic processes in quantum-classical hybrid systems accessible without the need to further specify a measurement protocol, our work contributes towards bridging the long-standing gap between classical and quantum stochastic thermodynamics."],"url":"http://arxiv.org/abs/2404.10118v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-15 19:43:16","title":"GeoAI Reproducibility and Replicability: a computational and spatial perspective","abstract":"GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner. While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed. This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives. We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability). Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success. We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes. We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors. The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research.","sentences":["GeoAI has emerged as an exciting interdisciplinary research area that combines spatial theories and data with cutting-edge AI models to address geospatial problems in a novel, data-driven manner.","While GeoAI research has flourished in the GIScience literature, its reproducibility and replicability (R&R), fundamental principles that determine the reusability, reliability, and scientific rigor of research findings, have rarely been discussed.","This paper aims to provide an in-depth analysis of this topic from both computational and spatial perspectives.","We first categorize the major goals for reproducing GeoAI research, namely, validation (repeatability), learning and adapting the method for solving a similar or new problem (reproducibility), and examining the generalizability of the research findings (replicability).","Each of these goals requires different levels of understanding of GeoAI, as well as different methods to ensure its success.","We then discuss the factors that may cause the lack of R&R in GeoAI research, with an emphasis on (1) the selection and use of training data; (2) the uncertainty that resides in the GeoAI model design, training, deployment, and inference processes; and more importantly (3) the inherent spatial heterogeneity of geospatial data and processes.","We use a deep learning-based image analysis task as an example to demonstrate the results' uncertainty and spatial variance caused by different factors.","The findings reiterate the importance of knowledge sharing, as well as the generation of a \"replicability map\" that incorporates spatial autocorrelation and spatial heterogeneity into consideration in quantifying the spatial replicability of GeoAI research."],"url":"http://arxiv.org/abs/2404.10108v1","category":"cs.CV"}
{"created":"2024-04-15 19:15:32","title":"Feature selection in linear SVMs via hard cardinality constraint: a scalable SDP decomposition approach","abstract":"In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to a fully explainable selection model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel SDP relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed SDPs. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.","sentences":["In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to a fully explainable selection model.","The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time.","To handle the hard problem, we first introduce two mixed-integer formulations for which novel SDP relaxations are proposed.","Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable.","To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution.","Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed SDPs.","Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach."],"url":"http://arxiv.org/abs/2404.10099v1","category":"math.OC"}
{"created":"2024-04-15 19:13:56","title":"Widths and rigidity of unconditional sets and random vectors","abstract":"We prove that any unconditional set in $\\mathbb{R}^N$ that is invariant under cyclic shifts of coordinates is rigid in $\\ell_p^N$, $1\\le p\\le 2$, i.e. it can not be well approximated by linear spaces of dimension essentially smaller than $N$. We apply the approach of E.D. Gluskin to the setting of average Kolmogorov widths of unconditional random vectors.   This paper continues the study of the rigidity initiated by the first author. This research is related to the notion of matrix rigidity developed within Complexity Theory.","sentences":["We prove that any unconditional set in $\\mathbb{R}^N$ that is invariant under cyclic shifts of coordinates is rigid in $\\ell_p^N$, $1\\le p\\le 2$, i.e. it can not be well approximated by linear spaces of dimension essentially smaller than $N$. We apply the approach of E.D. Gluskin to the setting of average Kolmogorov widths of unconditional random vectors.   ","This paper continues the study of the rigidity initiated by the first author.","This research is related to the notion of matrix rigidity developed within Complexity Theory."],"url":"http://arxiv.org/abs/2404.10098v1","category":"math.FA"}
{"created":"2024-04-15 19:00:38","title":"Active pattern formation emergent from single-species nonreciprocity","abstract":"Nonreciprocal forces violating Newton's third law are common in a plethora of nonequilibrium situations ranging from predator-prey systems to the swarming of birds and effective colloidal interactions under flow. While many recent studies have focused on two species with nonreciprocal coupling, much less is examined for the basic single-component system breaking the actio and reactio equality of force within the same species. Here, we systematically derive the fundamental field theory of single-species nonreciprocal interactions from microscopic dynamics, leading to a generic framework termed Active Model N (N denoting nonreciprocity). We explore the rich dynamics of pattern formation in this intrinsic nonreciprocal system and the emergence of self-traveling states with persistent variation and flowing of active branched patterns. One particular new characteristic pattern is an interwoven self-knitting \"yarn\" structure with a unique feature of simultaneous development of micro- and bulk phase separations. The growth dynamics of a \"ball-of-wool\" active droplet towards these self-knitted yarn or branched states exhibits a crossover between different scaling behaviors. The mechanism underlying this distinct class of active phase separation is attributed to the interplay between force nonreciprocity and competition. Our predictions can be applied to various biological and artificial active matter systems controlled by single-species nonreciprocity.","sentences":["Nonreciprocal forces violating Newton's third law are common in a plethora of nonequilibrium situations ranging from predator-prey systems to the swarming of birds and effective colloidal interactions under flow.","While many recent studies have focused on two species with nonreciprocal coupling, much less is examined for the basic single-component system breaking the actio and reactio equality of force within the same species.","Here, we systematically derive the fundamental field theory of single-species nonreciprocal interactions from microscopic dynamics, leading to a generic framework termed Active Model N (N denoting nonreciprocity).","We explore the rich dynamics of pattern formation in this intrinsic nonreciprocal system and the emergence of self-traveling states with persistent variation and flowing of active branched patterns.","One particular new characteristic pattern is an interwoven self-knitting \"yarn\" structure with a unique feature of simultaneous development of micro- and bulk phase separations.","The growth dynamics of a \"ball-of-wool\" active droplet towards these self-knitted yarn or branched states exhibits a crossover between different scaling behaviors.","The mechanism underlying this distinct class of active phase separation is attributed to the interplay between force nonreciprocity and competition.","Our predictions can be applied to various biological and artificial active matter systems controlled by single-species nonreciprocity."],"url":"http://arxiv.org/abs/2404.10093v1","category":"cond-mat.soft"}
{"created":"2024-04-15 18:52:30","title":"Quantum Risk Analysis of Financial Derivatives","abstract":"We introduce two quantum algorithms to compute the Value at Risk (VaR) and Conditional Value at Risk (CVaR) of financial derivatives using quantum computers: the first by applying existing ideas from quantum risk analysis to derivative pricing, and the second based on a novel approach using Quantum Signal Processing (QSP). Previous work in the literature has shown that quantum advantage is possible in the context of individual derivative pricing and that advantage can be leveraged in a straightforward manner in the estimation of the VaR and CVaR. The algorithms we introduce in this work aim to provide an additional advantage by encoding the derivative price over multiple market scenarios in superposition and computing the desired values by applying appropriate transformations to the quantum system. We perform complexity and error analysis of both algorithms, and show that while the two algorithms have the same asymptotic scaling the QSP-based approach requires significantly fewer quantum resources for the same target accuracy. Additionally, by numerically simulating both quantum and classical VaR algorithms, we demonstrate that the quantum algorithm can extract additional advantage from a quantum computer compared to individual derivative pricing. Specifically, we show that under certain conditions VaR estimation can lower the latest published estimates of the logical clock rate required for quantum advantage in derivative pricing by up to $\\sim 30$x. In light of these results, we are encouraged that our formulation of derivative pricing in the QSP framework may be further leveraged for quantum advantage in other relevant financial applications, and that quantum computers could be harnessed more efficiently by considering problems in the financial sector at a higher level.","sentences":["We introduce two quantum algorithms to compute the Value at Risk (VaR) and Conditional Value at Risk (CVaR) of financial derivatives using quantum computers: the first by applying existing ideas from quantum risk analysis to derivative pricing, and the second based on a novel approach using Quantum Signal Processing (QSP).","Previous work in the literature has shown that quantum advantage is possible in the context of individual derivative pricing and that advantage can be leveraged in a straightforward manner in the estimation of the VaR and CVaR.","The algorithms we introduce in this work aim to provide an additional advantage by encoding the derivative price over multiple market scenarios in superposition and computing the desired values by applying appropriate transformations to the quantum system.","We perform complexity and error analysis of both algorithms, and show that while the two algorithms have the same asymptotic scaling the QSP-based approach requires significantly fewer quantum resources for the same target accuracy.","Additionally, by numerically simulating both quantum and classical VaR algorithms, we demonstrate that the quantum algorithm can extract additional advantage from a quantum computer compared to individual derivative pricing.","Specifically, we show that under certain conditions VaR estimation can lower the latest published estimates of the logical clock rate required for quantum advantage in derivative pricing by up to $\\sim 30$x.","In light of these results, we are encouraged that our formulation of derivative pricing in the QSP framework may be further leveraged for quantum advantage in other relevant financial applications, and that quantum computers could be harnessed more efficiently by considering problems in the financial sector at a higher level."],"url":"http://arxiv.org/abs/2404.10088v1","category":"quant-ph"}
{"created":"2024-04-15 18:50:44","title":"cuFastTuckerPlus: A Stochastic Parallel Sparse FastTucker Decomposition Using GPU Tensor Cores","abstract":"Sparse tensors are prevalent in real-world applications, often characterized by their large-scale, high-order, and high-dimensional nature. Directly handling raw tensors is impractical due to the significant memory and computational overhead involved. The current mainstream approach involves compressing or decomposing the original tensor. One popular tensor decomposition algorithm is the Tucker decomposition. However, existing state-of-the-art algorithms for large-scale Tucker decomposition typically relax the original optimization problem into multiple convex optimization problems to ensure polynomial convergence. Unfortunately, these algorithms tend to converge slowly. In contrast, tensor decomposition exhibits a simple optimization landscape, making local search algorithms capable of converging to a global (approximate) optimum much faster. In this paper, we propose the FastTuckerPlus algorithm, which decomposes the original optimization problem into two non-convex optimization problems and solves them alternately using the Stochastic Gradient Descent method. Furthermore, we introduce cuFastTuckerPlus, a fine-grained parallel algorithm designed for GPU platforms, leveraging the performance of tensor cores. This algorithm minimizes memory access overhead and computational costs, surpassing the state-of-the-art algorithms. Our experimental results demonstrate that our method achieves a speedup of $3X$ to $5X$ compared to state-of-the-art algorithms.","sentences":["Sparse tensors are prevalent in real-world applications, often characterized by their large-scale, high-order, and high-dimensional nature.","Directly handling raw tensors is impractical due to the significant memory and computational overhead involved.","The current mainstream approach involves compressing or decomposing the original tensor.","One popular tensor decomposition algorithm is the Tucker decomposition.","However, existing state-of-the-art algorithms for large-scale Tucker decomposition typically relax the original optimization problem into multiple convex optimization problems to ensure polynomial convergence.","Unfortunately, these algorithms tend to converge slowly.","In contrast, tensor decomposition exhibits a simple optimization landscape, making local search algorithms capable of converging to a global (approximate) optimum much faster.","In this paper, we propose the FastTuckerPlus algorithm, which decomposes the original optimization problem into two non-convex optimization problems and solves them alternately using the Stochastic Gradient Descent method.","Furthermore, we introduce cuFastTuckerPlus, a fine-grained parallel algorithm designed for GPU platforms, leveraging the performance of tensor cores.","This algorithm minimizes memory access overhead and computational costs, surpassing the state-of-the-art algorithms.","Our experimental results demonstrate that our method achieves a speedup of $3X$ to $5X$ compared to state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2404.10087v1","category":"cs.DC"}
{"created":"2024-04-15 18:48:27","title":"The Average Spectrum Norm and Near-Optimal Tensor Completion","abstract":"We introduce a new tensor norm, the average spectrum norm, to study sample complexity of tensor completion problems based on the canonical polyadic decomposition (CPD). Properties of the average spectrum norm and its dual norm are investigated, demonstrating their utility for low-rank tensor recovery analysis. Our novel approach significantly reduces the provable sample rate for CPD-based noisy tensor completion, providing the best bounds to date on the number of observed noisy entries required to produce an arbitrarily accurate estimate of an underlying mean value tensor. Under Poisson and Bernoulli multivariate distributions, we show that an $N$-way CPD rank-$R$ parametric tensor $\\boldsymbol{\\mathscr{M}}\\in\\mathbb{R}^{I\\times \\cdots\\times I}$ generating noisy observations can be approximated by large likelihood estimators from $\\mathcal{O}(IR^2\\log^{N+2}(I))$ revealed entries. Furthermore, under nonnegative and orthogonal versions of the CPD we improve the result to depend linearly on the rank, achieving the near-optimal rate $\\mathcal{O}(IR\\log^{N+2}(I))$.","sentences":["We introduce a new tensor norm, the average spectrum norm, to study sample complexity of tensor completion problems based on the canonical polyadic decomposition (CPD).","Properties of the average spectrum norm and its dual norm are investigated, demonstrating their utility for low-rank tensor recovery analysis.","Our novel approach significantly reduces the provable sample rate for CPD-based noisy tensor completion, providing the best bounds to date on the number of observed noisy entries required to produce an arbitrarily accurate estimate of an underlying mean value tensor.","Under Poisson and Bernoulli multivariate distributions, we show that an $N$-way CPD rank-$R$ parametric tensor $\\boldsymbol{\\mathscr{M}}\\in\\mathbb{R}^{I\\times \\cdots\\times I}$ generating noisy observations can be approximated by large likelihood estimators from $\\mathcal{O}(IR^2\\log^{N+2}(I))$ revealed entries.","Furthermore, under nonnegative and orthogonal versions of the CPD we improve the result to depend linearly on the rank, achieving the near-optimal rate $\\mathcal{O}(IR\\log^{N+2}(I))$."],"url":"http://arxiv.org/abs/2404.10085v1","category":"cs.IT"}
{"created":"2024-04-15 18:45:49","title":"Onsager's \"Ideal Turbulence\" Theory","abstract":"Lars Onsager in 1945-1949 made an exact analysis of the high Reynolds-number limit for individual turbulent flow realizations modeled by incompressible Navier-Stokes equations, motivated by experimental observations that dissipation of kinetic energy does not vanish. I review here developments spurred by his key idea, that such flows are well-described by distributional or \"weak\" solutions of ideal Euler equations. 1/3 H\\\"older singularities of the velocity field were predicted by Onsager and since observed. His theory describes turbulent energy cascade without probabilistic assumptions and yields a local, deterministic version of the Kolmogorov 4/5th law. The approach is closely related to renormalization group methods in physics and envisages \"conservation-law anomalies\", as discovered later in quantum field theory. There are also deep connections with Large-Eddy Simulation modeling. More recently, dissipative Euler solutions of the type conjectured by Onsager have been constructed and his 1/3 H\\\"older singularity proved to be the sharp threshold for anomalous dissipation. This progress has been achieved by an unexpected connection with work of John Nash on isometric embeddings of low regularity or \"convex integration\" techniques. The dissipative Euler solutions yielded by this method are wildly non-unique for fixed initial data, suggesting \"spontaneously stochastic\" behavior of high-Reynolds number solutions. I focus in particular on applications to wall-bounded turbulence, leading to novel concepts of spatial cascades of momentum, energy and vorticity to or from the wall as deterministic, space-time local phenomena. This theory thus makes testable predictions and offers new perspectives on Large-Eddy Simulation in presence of solid walls.","sentences":["Lars Onsager in 1945-1949 made an exact analysis of the high Reynolds-number limit for individual turbulent flow realizations modeled by incompressible Navier-Stokes equations, motivated by experimental observations that dissipation of kinetic energy does not vanish.","I review here developments spurred by his key idea, that such flows are well-described by distributional or \"weak\" solutions of ideal Euler equations.","1/3 H\\\"older singularities of the velocity field were predicted by Onsager and since observed.","His theory describes turbulent energy cascade without probabilistic assumptions and yields a local, deterministic version of the Kolmogorov 4/5th law.","The approach is closely related to renormalization group methods in physics and envisages \"conservation-law anomalies\", as discovered later in quantum field theory.","There are also deep connections with Large-Eddy Simulation modeling.","More recently, dissipative Euler solutions of the type conjectured by Onsager have been constructed and his 1/3 H\\\"older singularity proved to be the sharp threshold for anomalous dissipation.","This progress has been achieved by an unexpected connection with work of John Nash on isometric embeddings of low regularity or \"convex integration\" techniques.","The dissipative Euler solutions yielded by this method are wildly non-unique for fixed initial data, suggesting \"spontaneously stochastic\" behavior of high-Reynolds number solutions.","I focus in particular on applications to wall-bounded turbulence, leading to novel concepts of spatial cascades of momentum, energy and vorticity to or from the wall as deterministic, space-time local phenomena.","This theory thus makes testable predictions and offers new perspectives on Large-Eddy Simulation in presence of solid walls."],"url":"http://arxiv.org/abs/2404.10084v1","category":"physics.flu-dyn"}
{"created":"2024-04-15 18:28:10","title":"Field-Programmable Gate Array Architecture for Deep Learning: Survey & Future Directions","abstract":"Deep learning (DL) is becoming the cornerstone of numerous applications both in datacenters and at the edge. Specialized hardware is often necessary to meet the performance requirements of state-of-the-art DL models, but the rapid pace of change in DL models and the wide variety of systems integrating DL make it impossible to create custom computer chips for all but the largest markets. Field-programmable gate arrays (FPGAs) present a unique blend of reprogrammability and direct hardware execution that make them suitable for accelerating DL inference. They offer the ability to customize processing pipelines and memory hierarchies to achieve lower latency and higher energy efficiency compared to general-purpose CPUs and GPUs, at a fraction of the development time and cost of custom chips. Their diverse high-speed IOs also enable directly interfacing the FPGA to the network and/or a variety of external sensors, making them suitable for both datacenter and edge use cases. As DL has become an ever more important workload, FPGA architectures are evolving to enable higher DL performance. In this article, we survey both academic and industrial FPGA architecture enhancements for DL. First, we give a brief introduction on the basics of FPGA architecture and how its components lead to strengths and weaknesses for DL applications. Next, we discuss different styles of DL inference accelerators on FPGA, ranging from model-specific dataflow styles to software-programmable overlay styles. We survey DL-specific enhancements to traditional FPGA building blocks such as logic blocks, arithmetic circuitry, and on-chip memories, as well as new in-fabric DL-specialized blocks for accelerating tensor computations. Finally, we discuss hybrid devices that combine processors and coarse-grained accelerator blocks with FPGA-like interconnect and networks-on-chip, and highlight promising future research directions.","sentences":["Deep learning (DL) is becoming the cornerstone of numerous applications both in datacenters and at the edge.","Specialized hardware is often necessary to meet the performance requirements of state-of-the-art DL models, but the rapid pace of change in DL models and the wide variety of systems integrating DL make it impossible to create custom computer chips for all but the largest markets.","Field-programmable gate arrays (FPGAs) present a unique blend of reprogrammability and direct hardware execution that make them suitable for accelerating DL inference.","They offer the ability to customize processing pipelines and memory hierarchies to achieve lower latency and higher energy efficiency compared to general-purpose CPUs and GPUs, at a fraction of the development time and cost of custom chips.","Their diverse high-speed IOs also enable directly interfacing the FPGA to the network and/or a variety of external sensors, making them suitable for both datacenter and edge use cases.","As DL has become an ever more important workload, FPGA architectures are evolving to enable higher DL performance.","In this article, we survey both academic and industrial FPGA architecture enhancements for DL.","First, we give a brief introduction on the basics of FPGA architecture and how its components lead to strengths and weaknesses for DL applications.","Next, we discuss different styles of DL inference accelerators on FPGA, ranging from model-specific dataflow styles to software-programmable overlay styles.","We survey DL-specific enhancements to traditional FPGA building blocks such as logic blocks, arithmetic circuitry, and on-chip memories, as well as new in-fabric DL-specialized blocks for accelerating tensor computations.","Finally, we discuss hybrid devices that combine processors and coarse-grained accelerator blocks with FPGA-like interconnect and networks-on-chip, and highlight promising future research directions."],"url":"http://arxiv.org/abs/2404.10076v1","category":"cs.AR"}
{"created":"2024-04-15 18:26:03","title":"Explainable Light-Weight Deep Learning Pipeline for Improved Drought Stres","abstract":"Early identification of drought stress in crops is vital for implementing effective mitigation measures and reducing yield loss. Non-invasive imaging techniques hold immense potential by capturing subtle physiological changes in plants under water deficit. Sensor based imaging data serves as a rich source of information for machine learning and deep learning algorithms, facilitating further analysis aimed at identifying drought stress. While these approaches yield favorable results, real-time field applications requires algorithms specifically designed for the complexities of natural agricultural conditions. Our work proposes a novel deep learning framework for classifying drought stress in potato crops captured by UAVs in natural settings. The novelty lies in the synergistic combination of a pretrained network with carefully designed custom layers. This architecture leverages feature extraction capabilities of the pre-trained network while the custom layers enable targeted dimensionality reduction and enhanced regularization, ultimately leading to improved performance. A key innovation of our work involves the integration of Gradient-Class Activation Mapping (Grad-CAM), an explainability technique. Grad-CAM sheds light on the internal workings of the deep learning model, typically referred to as a black box. By visualizing the focus areas of the model within the images, Grad-CAM fosters interpretability and builds trust in the decision-making process of the model. Our proposed framework achieves superior performance, particularly with the DenseNet121 pre-trained network, reaching a precision of 98% to identify the stressed class with an overall accuracy of 90%. Comparative analysis of existing state-of-the-art object detection algorithms reveals the superiority of our approach in significantly higher precision and accuracy.","sentences":["Early identification of drought stress in crops is vital for implementing effective mitigation measures and reducing yield loss.","Non-invasive imaging techniques hold immense potential by capturing subtle physiological changes in plants under water deficit.","Sensor based imaging data serves as a rich source of information for machine learning and deep learning algorithms, facilitating further analysis aimed at identifying drought stress.","While these approaches yield favorable results, real-time field applications requires algorithms specifically designed for the complexities of natural agricultural conditions.","Our work proposes a novel deep learning framework for classifying drought stress in potato crops captured by UAVs in natural settings.","The novelty lies in the synergistic combination of a pretrained network with carefully designed custom layers.","This architecture leverages feature extraction capabilities of the pre-trained network while the custom layers enable targeted dimensionality reduction and enhanced regularization, ultimately leading to improved performance.","A key innovation of our work involves the integration of Gradient-Class Activation Mapping (Grad-CAM), an explainability technique.","Grad-CAM sheds light on the internal workings of the deep learning model, typically referred to as a black box.","By visualizing the focus areas of the model within the images, Grad-CAM fosters interpretability and builds trust in the decision-making process of the model.","Our proposed framework achieves superior performance, particularly with the DenseNet121 pre-trained network, reaching a precision of 98% to identify the stressed class with an overall accuracy of 90%.","Comparative analysis of existing state-of-the-art object detection algorithms reveals the superiority of our approach in significantly higher precision and accuracy."],"url":"http://arxiv.org/abs/2404.10073v1","category":"cs.CV"}
{"created":"2024-04-15 18:08:46","title":"The Feasibility of Constrained Reinforcement Learning Algorithms: A Tutorial Study","abstract":"Satisfying safety constraints is a priority concern when solving optimal control problems (OCPs). Due to the existence of infeasibility phenomenon, where a constraint-satisfying solution cannot be found, it is necessary to identify a feasible region before implementing a policy. Existing feasibility theories built for model predictive control (MPC) only consider the feasibility of optimal policy. However, reinforcement learning (RL), as another important control method, solves the optimal policy in an iterative manner, which comes with a series of non-optimal intermediate policies. Feasibility analysis of these non-optimal policies is also necessary for iteratively improving constraint satisfaction; but that is not available under existing MPC feasibility theories. This paper proposes a feasibility theory that applies to both MPC and RL by filling in the missing part of feasibility analysis for an arbitrary policy. The basis of our theory is to decouple policy solving and implementation into two temporal domains: virtual-time domain and real-time domain. This allows us to separately define initial and endless, state and policy feasibility, and their corresponding feasible regions. Based on these definitions, we analyze the containment relationships between different feasible regions, which enables us to describe the feasible region of an arbitrary policy. We further provide virtual-time constraint design rules along with a practical design tool called feasibility function that helps to achieve the maximum feasible region. We review most of existing constraint formulations and point out that they are essentially applications of feasibility functions in different forms. We demonstrate our feasibility theory by visualizing different feasible regions under both MPC and RL policies in an emergency braking control task.","sentences":["Satisfying safety constraints is a priority concern when solving optimal control problems (OCPs).","Due to the existence of infeasibility phenomenon, where a constraint-satisfying solution cannot be found, it is necessary to identify a feasible region before implementing a policy.","Existing feasibility theories built for model predictive control (MPC) only consider the feasibility of optimal policy.","However, reinforcement learning (RL), as another important control method, solves the optimal policy in an iterative manner, which comes with a series of non-optimal intermediate policies.","Feasibility analysis of these non-optimal policies is also necessary for iteratively improving constraint satisfaction; but that is not available under existing MPC feasibility theories.","This paper proposes a feasibility theory that applies to both MPC and RL by filling in the missing part of feasibility analysis for an arbitrary policy.","The basis of our theory is to decouple policy solving and implementation into two temporal domains: virtual-time domain and real-time domain.","This allows us to separately define initial and endless, state and policy feasibility, and their corresponding feasible regions.","Based on these definitions, we analyze the containment relationships between different feasible regions, which enables us to describe the feasible region of an arbitrary policy.","We further provide virtual-time constraint design rules along with a practical design tool called feasibility function that helps to achieve the maximum feasible region.","We review most of existing constraint formulations and point out that they are essentially applications of feasibility functions in different forms.","We demonstrate our feasibility theory by visualizing different feasible regions under both MPC and RL policies in an emergency braking control task."],"url":"http://arxiv.org/abs/2404.10064v1","category":"eess.SY"}
{"created":"2024-04-15 18:02:53","title":"The outflowing ionised gas of I Zw 1 observed by HST COS","abstract":"We present an analysis of the HST COS spectrum of IZw1 aiming to probe the absorbing medium associated with the active galactic nucleus (AGN). We fitted the emission spectrum and performed spectral analysis of the identified absorption features to derive the corresponding ionic column densities and covering fractions of the associated outflows. We employed photoionisation modelling to constrain the total column density and the ionisation parameter of four detected kinematic components. By investigating the implications of the results together with the observed kinematic properties of both emission and absorption features, we derived constraints on the structure and geometry of the absorbing medium in the AGN environment. We find and characterise absorption line systems from outflowing ionised gas in four distinct kinematic components, located at -60, -280, -1950, and -2900 km/s with respect to the source rest frame. While the two slower outflows are consistent with a full covering of the underlying radiation source, the well-constrained doublet line ratios of the faster two, higher column density, outflows suggest partial covering, with a covering fraction of C_f~0.4. The faster outflows show also line-locking in the NV doublet, a signature of acceleration via line absorption. This makes IZw1 possibly the closest object that shows evidence for hosting line-driven winds. The observed -1950 km/s absorption is likely due to the same gas as an X-ray warm absorber. Furthermore, the behaviour in UV and X-ray bands implies that this outflow has a clumpy structure. We find that the highly asymmetric broad emission lines in IZw1, indicative of a collimated, outflowing broad line region, are covered by the absorbing gas. Finally, the strongest UV--X-ray absorber may be connected to some of the blueshifted line emission, indicative of a more spatially extended structure of this ionised medium.","sentences":["We present an analysis of the HST COS spectrum of IZw1 aiming to probe the absorbing medium associated with the active galactic nucleus (AGN).","We fitted the emission spectrum and performed spectral analysis of the identified absorption features to derive the corresponding ionic column densities and covering fractions of the associated outflows.","We employed photoionisation modelling to constrain the total column density and the ionisation parameter of four detected kinematic components.","By investigating the implications of the results together with the observed kinematic properties of both emission and absorption features, we derived constraints on the structure and geometry of the absorbing medium in the AGN environment.","We find and characterise absorption line systems from outflowing ionised gas in four distinct kinematic components, located at -60, -280, -1950, and -2900 km/s with respect to the source rest frame.","While the two slower outflows are consistent with a full covering of the underlying radiation source, the well-constrained doublet line ratios of the faster two, higher column density, outflows suggest partial covering, with a covering fraction of C_f~0.4.","The faster outflows show also line-locking in the NV doublet, a signature of acceleration via line absorption.","This makes IZw1 possibly the closest object that shows evidence for hosting line-driven winds.","The observed -1950 km/s absorption is likely due to the same gas as an X-ray warm absorber.","Furthermore, the behaviour in UV and X-ray bands implies that this outflow has a clumpy structure.","We find that the highly asymmetric broad emission lines in IZw1, indicative of a collimated, outflowing broad line region, are covered by the absorbing gas.","Finally, the strongest UV--X-ray absorber may be connected to some of the blueshifted line emission, indicative of a more spatially extended structure of this ionised medium."],"url":"http://arxiv.org/abs/2404.10060v1","category":"astro-ph.GA"}
{"created":"2024-04-15 18:02:35","title":"GHOST Commissioning Science Results III: Characterizing an iron-poor damped Lyman $\u03b1$ system","abstract":"The Gemini High-resolution Optical SpecTrograph (GHOST) is a new echelle spectrograph available on the Gemini-South telescope as of Semester 2024A. We present the first high resolution spectrum of the quasar J1449-1227 (redshift z_em=3.27) using data taken during the commissioning of GHOST. The observed quasar hosts an intervening iron-poor ([Fe/H] = -2.5) damped Lyman alpha (DLA) system at redshift z=2.904. Taking advantage of the high spectral resolving power of GHOST (R~55000), we are able to accurately model the metal absorption lines of the metal-poor DLA and find a supersolar [Si/Fe], suggesting the DLA gas is in an early stage of chemical enrichment. Using simple ionization models, we find that the large range in the C IV/Si IV column density ratio of individual components within the DLA's high ionization absorption profile can be reproduced by several metal-poor Lyman limit systems surrounding the low-ionization gas of the DLA. It is possible that this metal-poor DLA resides within a complex system of metal-poor galaxies or filaments with inflowing gas. The high spectral resolution, wavelength coverage and sensitivity of GHOST makes it an ideal spectrograph for characterizing the chemistry and kinematics of quasar absorption lines.","sentences":["The Gemini High-resolution Optical SpecTrograph (GHOST) is a new echelle spectrograph available on the Gemini-South telescope as of Semester 2024A. We present the first high resolution spectrum of the quasar J1449-1227 (redshift z_em=3.27) using data taken during the commissioning of GHOST.","The observed quasar hosts an intervening iron-poor ([Fe/H] = -2.5) damped Lyman alpha (DLA) system at redshift z=2.904.","Taking advantage of the high spectral resolving power of GHOST (R~55000), we are able to accurately model the metal absorption lines of the metal-poor DLA and find a supersolar [Si/Fe], suggesting the DLA gas is in an early stage of chemical enrichment.","Using simple ionization models, we find that the large range in the C IV/Si IV column density ratio of individual components within the DLA's high ionization absorption profile can be reproduced by several metal-poor Lyman limit systems surrounding the low-ionization gas of the DLA.","It is possible that this metal-poor DLA resides within a complex system of metal-poor galaxies or filaments with inflowing gas.","The high spectral resolution, wavelength coverage and sensitivity of GHOST makes it an ideal spectrograph for characterizing the chemistry and kinematics of quasar absorption lines."],"url":"http://arxiv.org/abs/2404.10058v1","category":"astro-ph.GA"}
{"created":"2024-04-15 18:01:13","title":"Universal distributions of overlaps from unitary dynamics in generic quantum many-body systems","abstract":"We study the preparation of a quantum state using a circuit of depth $t$ from a factorized state of $N$ sites. We argue that in the appropriate scaling limit of large $t$ and $N$, the overlap between states evolved under generic many-body chaotic dynamics belongs to a family of universal distribution that generalizes the celebrated Porter-Thomas distribution. This is a consequence of a mapping in the space of replicas to a model of dilute domain walls. Our result provides a rare example in which analysis at an arbitrary number of replicas is possible, giving rise to the complete overlap distribution. Our general picture is derived and corroborated by the exact solution of the random phase model and of an emergent random matrix model given by the Ginibre ensemble. Finally, numerical simulations of two distinct random circuits show excellent agreement, thereby demonstrating universality.","sentences":["We study the preparation of a quantum state using a circuit of depth $t$ from a factorized state of $N$ sites.","We argue that in the appropriate scaling limit of large $t$ and $N$, the overlap between states evolved under generic many-body chaotic dynamics belongs to a family of universal distribution that generalizes the celebrated Porter-Thomas distribution.","This is a consequence of a mapping in the space of replicas to a model of dilute domain walls.","Our result provides a rare example in which analysis at an arbitrary number of replicas is possible, giving rise to the complete overlap distribution.","Our general picture is derived and corroborated by the exact solution of the random phase model and of an emergent random matrix model given by the Ginibre ensemble.","Finally, numerical simulations of two distinct random circuits show excellent agreement, thereby demonstrating universality."],"url":"http://arxiv.org/abs/2404.10057v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-15 18:01:02","title":"A critical study of the Monte Carlo replica method","abstract":"We present a detailed mathematical study of the Monte Carlo replica method as applied in the global fitting literature from the high-energy physics theory community. For the first time, we provide a rigorous derivation of the parameter distributions implied by the method, and show that, whilst they agree with Bayesian posteriors for linear models, they disagree otherwise. We proceed to numerically quantify the disagreement between the Monte Carlo replica method and the Bayesian method in the context of two phenomenologically relevant scenarios: fits of the SMEFT Wilson coefficients, and fits of PDFs (albeit in a toy scenario). In both scenarios, we find that uncertainty estimates of the quantities of interest are discrepant between the two approaches when non-linearity is relevant. Our findings motivate future investigation of Bayesian methodologies for global PDF fits, especially in the context of simultaneous determination of PDFs and SMEFT Wilson coefficients.","sentences":["We present a detailed mathematical study of the Monte Carlo replica method as applied in the global fitting literature from the high-energy physics theory community.","For the first time, we provide a rigorous derivation of the parameter distributions implied by the method, and show that, whilst they agree with Bayesian posteriors for linear models, they disagree otherwise.","We proceed to numerically quantify the disagreement between the Monte Carlo replica method and the Bayesian method in the context of two phenomenologically relevant scenarios: fits of the SMEFT Wilson coefficients, and fits of PDFs (albeit in a toy scenario).","In both scenarios, we find that uncertainty estimates of the quantities of interest are discrepant between the two approaches when non-linearity is relevant.","Our findings motivate future investigation of Bayesian methodologies for global PDF fits, especially in the context of simultaneous determination of PDFs and SMEFT Wilson coefficients."],"url":"http://arxiv.org/abs/2404.10056v1","category":"hep-ph"}
{"created":"2024-04-15 18:00:18","title":"Landau-Zener without a Qubit: Unveiling Multiphoton Interference, Synthetic Floquet Dimensions, and Dissipative Quantum Chaos","abstract":"Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interference emerges when the parameters of a $\\textit{qubit}$ are periodically modulated across an avoided level crossing. Here, we investigate the occurrence of the LZSM phenomenon in nonlinear multilevel bosonic systems, where the interference pattern is determined by multiple energy levels and cannot be described by a level crossing between only two states. We fabricate two superconducting resonators made of flux-tunable Josephson junction arrays. The first device is very weakly nonlinear (the nonlinearity is smaller than the photon-loss rate) and, when a weak driving field is applied, it behaves as a linear resonator, yet shows the same LZSM interference as in a two-level system. Notably, here the interference originates from multiple avoided level crossings of the harmonic ladder. When subjected to a stronger drive, nonlinear effects start playing a role, and the interference pattern departs from the one observed in two-level systems. We demonstrate that, when two or more LZSM interference peaks merge, dissipative quantum chaos emerges. In the second device, where the nonlinearity surpasses the photon-loss rate, we observe additional LZSM interference peaks due to Kerr multiphoton resonances. When described under the light of the Floquet theory, these resonances can be interpreted as synthetic modes of an array of coupled cavities. We derive a simple effective model highlighting the essential features of the entirety of these phenomena. As the control of LZSM in qubit systems led to the implementation of fast protocols for characterization and state preparation, our findings pave the way to better control of nonlinear resonators, with implications for diverse quantum technological platforms.","sentences":["Landau-Zener-St\\\"uckelberg-Majorana (LZSM) interference emerges when the parameters of a $\\textit{qubit}$ are periodically modulated across an avoided level crossing.","Here, we investigate the occurrence of the LZSM phenomenon in nonlinear multilevel bosonic systems, where the interference pattern is determined by multiple energy levels and cannot be described by a level crossing between only two states.","We fabricate two superconducting resonators made of flux-tunable Josephson junction arrays.","The first device is very weakly nonlinear (the nonlinearity is smaller than the photon-loss rate) and, when a weak driving field is applied, it behaves as a linear resonator, yet shows the same LZSM interference as in a two-level system.","Notably, here the interference originates from multiple avoided level crossings of the harmonic ladder.","When subjected to a stronger drive, nonlinear effects start playing a role, and the interference pattern departs from the one observed in two-level systems.","We demonstrate that, when two or more LZSM interference peaks merge, dissipative quantum chaos emerges.","In the second device, where the nonlinearity surpasses the photon-loss rate, we observe additional LZSM interference peaks due to Kerr multiphoton resonances.","When described under the light of the Floquet theory, these resonances can be interpreted as synthetic modes of an array of coupled cavities.","We derive a simple effective model highlighting the essential features of the entirety of these phenomena.","As the control of LZSM in qubit systems led to the implementation of fast protocols for characterization and state preparation, our findings pave the way to better control of nonlinear resonators, with implications for diverse quantum technological platforms."],"url":"http://arxiv.org/abs/2404.10051v1","category":"quant-ph"}
{"created":"2024-04-15 18:00:05","title":"Classifying binary black holes from Population III stars with the Einstein Telescope: a machine-learning approach","abstract":"Third-generation (3G) gravitational-wave (GW) detectors like the Einstein Telescope (ET) will observe binary black hole (BBH) mergers at redshifts up to $z\\sim 100$. However, unequivocal determination of the origin of high-redshift sources will remain uncertain, due to the low signal-to-noise ratio (SNR) and poor estimate of their luminosity distance. This study proposes a machine learning approach to infer the origins of high-redshift BBHs, specifically differentiating those arising from Population III (Pop. III) stars - likely the first progenitors of stellar-born BBH mergers in the Universe - and those originated from Population I-II (Pop. I-II) stars. We have considered a wide range of state-of-the-art models encompassing current uncertainties on Pop. III BBH mergers. We then estimate parameter errors of detected sources with ET using the Fisher-information-matrix formalism, followed by classification using XGBoost, a machine learning algorithm based on decision trees. For a set of mock observed BBHs, we provide the probability that they belong to the Pop. III class while considering the parameter errors of each source. In our fiducial model, we accurately identify ~10% of detected BBHs originating from Pop. III stars with > 90% precision. Our study demonstrates how machine learning enables to achieve some pivotal aspects of ET science case by exploring the origin of individual high-redshift GW observations. We set the basis for further studies, which will integrate additional simulated populations and account for population modeling uncertainties","sentences":["Third-generation (3G) gravitational-wave (GW) detectors like the Einstein Telescope (ET) will observe binary black hole (BBH) mergers at redshifts up to $z\\sim 100$. However, unequivocal determination of the origin of high-redshift sources will remain uncertain, due to the low signal-to-noise ratio (SNR) and poor estimate of their luminosity distance.","This study proposes a machine learning approach to infer the origins of high-redshift BBHs, specifically differentiating those arising from Population III (Pop. III) stars - likely the first progenitors of stellar-born BBH mergers in the Universe - and those originated from Population I-II (Pop. I-II) stars.","We have considered a wide range of state-of-the-art models encompassing current uncertainties on Pop.","III BBH mergers.","We then estimate parameter errors of detected sources with ET using the Fisher-information-matrix formalism, followed by classification using XGBoost, a machine learning algorithm based on decision trees.","For a set of mock observed BBHs, we provide the probability that they belong to the Pop.","III class while considering the parameter errors of each source.","In our fiducial model, we accurately identify ~10% of detected BBHs originating from Pop.","III stars with > 90% precision.","Our study demonstrates how machine learning enables to achieve some pivotal aspects of ET science case by exploring the origin of individual high-redshift GW observations.","We set the basis for further studies, which will integrate additional simulated populations and account for population modeling uncertainties"],"url":"http://arxiv.org/abs/2404.10048v1","category":"astro-ph.HE"}
{"created":"2024-04-15 18:00:03","title":"Theoretical predictions for $b\\to s \u03bc^+ \u03bc^-$ decays","abstract":"I review the state of the art of the theoretical calculations for decays mediated by $b\\to s \\ell^+ \\ell^-$ transitions, for $\\ell=e,\\mu$. I focus on the predictions of observables in $B\\to K \\mu^+\\mu^-$, $B\\to K^* \\mu^+\\mu^-$, and $B_s\\to \\phi \\mu^+\\mu^-$ decays, as many of these predictions are in tension with the corresponding experimental measurements. I also briefly discuss the $\\Lambda_b\\to \\Lambda \\mu^+\\mu^-$ decay and present a new calculation for this channel. Special emphasis is placed on the non-local contributions, as they are the largest systematic uncertainties in these decays. The current theoretical calculations for $b\\to s \\mu^+ \\mu^-$ decays are not able to explain the tensions with the experimental measurements.","sentences":["I review the state of the art of the theoretical calculations for decays mediated by $b\\to s \\ell^+ \\ell^-$ transitions, for $\\ell=e,\\mu$. I focus on the predictions of observables in $B\\to K \\mu^+\\mu^-$, $B\\to K^* \\mu^+\\mu^-$, and $B_s\\to \\phi \\mu^+\\mu^-$ decays, as many of these predictions are in tension with the corresponding experimental measurements.","I also briefly discuss the $\\Lambda_b\\to \\Lambda \\mu^+\\mu^-$ decay and present a new calculation for this channel.","Special emphasis is placed on the non-local contributions, as they are the largest systematic uncertainties in these decays.","The current theoretical calculations for $b\\to s \\mu^+ \\mu^-$ decays are not able to explain the tensions with the experimental measurements."],"url":"http://arxiv.org/abs/2404.10043v1","category":"hep-ph"}
{"created":"2024-04-15 18:00:02","title":"JT gravity from non-Abelian T-duality","abstract":"We study the geometries obtained by performing super non-Abelian T-duality of the Principal Chiral Model on OSp$(1|2)$. While the initial model represents an appropriate 3D supergravity background, interpretable as the superspace version of AdS$_{3}$, the T-dual model fails solving the 3D supergravity torsion constraints. We argue that this has to do with a factorisation pattern taking place under dualisation: the dual 3D geometry can be rewritten as the supersymmetric version of AdS$_{2}$, satisfying the supergravity constraints, fibered over what we interpret as the superspace equivalent of the standard bosonic line. We discuss an interesting connection between T-duals of generic Principal Chiral Models and Poisson sigma models. We exploit it to show that in a suitable limit the dual action studied in this work gives rise to JT (super)gravity.","sentences":["We study the geometries obtained by performing super non-Abelian T-duality of the Principal Chiral Model on OSp$(1|2)$.","While the initial model represents an appropriate 3D supergravity background, interpretable as the superspace version of AdS$_{3}$, the T-dual model fails solving the 3D supergravity torsion constraints.","We argue that this has to do with a factorisation pattern taking place under dualisation: the dual 3D geometry can be rewritten as the supersymmetric version of AdS$_{2}$, satisfying the supergravity constraints, fibered over what we interpret as the superspace equivalent of the standard bosonic line.","We discuss an interesting connection between T-duals of generic Principal Chiral Models and Poisson sigma models.","We exploit it to show that in a suitable limit the dual action studied in this work gives rise to JT (super)gravity."],"url":"http://arxiv.org/abs/2404.10041v1","category":"hep-th"}
{"created":"2024-04-15 18:00:01","title":"Neutrino and Gamma-Ray Signatures of Inelastic Dark Matter Annihilating outside Neutron Stars","abstract":"We present a new inelastic dark matter search: neutron stars in dark matter-rich environments capture inelastic dark matter which, for interstate mass splittings between about $45 - 285 \\ \\rm MeV$, will annihilate away before becoming fully trapped inside the object. This means a sizable fraction of the dark matter particles can annihilate while being outside the neutron star, producing neutron star-focused gamma-rays and neutrinos. We analyze this effect for the first time and target the neutron star population in the Galactic Center, where the large dark matter and neutron star content makes this signal most significant. Depending on the assumed neutron star and dark matter distributions, we set constraints on the dark matter-nucleon inelastic cross-section using existing H.E.S.S. observations. We also forecast the sensitivity of upcoming gamma-ray and neutrino telescopes to this signal, which can reach inelastic cross-sections as low as $\\sim 2 \\times 10^{-47} \\ \\rm cm^2$.","sentences":["We present a new inelastic dark matter search: neutron stars in dark matter-rich environments capture inelastic dark matter which, for interstate mass splittings between about $45 - 285 \\ \\rm MeV$, will annihilate away before becoming fully trapped inside the object.","This means a sizable fraction of the dark matter particles can annihilate while being outside the neutron star, producing neutron star-focused gamma-rays and neutrinos.","We analyze this effect for the first time and target the neutron star population in the Galactic Center, where the large dark matter and neutron star content makes this signal most significant.","Depending on the assumed neutron star and dark matter distributions, we set constraints on the dark matter-nucleon inelastic cross-section using existing H.E.S.S. observations.","We also forecast the sensitivity of upcoming gamma-ray and neutrino telescopes to this signal, which can reach inelastic cross-sections as low as $\\sim 2 \\times 10^{-47} \\ \\rm cm^2$."],"url":"http://arxiv.org/abs/2404.10039v1","category":"hep-ph"}
{"created":"2024-04-15 18:00:01","title":"Anatomy of an ionized bubble: NIRCam grism spectroscopy of the $z=6.6$ double-peaked Lyman-$\u03b1$ emitter COLA1 and its environment","abstract":"The increasingly neutral intergalactic gas at $z>6$ impacts the Lyman-$\\alpha$ flux observed from galaxies. One luminous galaxy, COLA1, stands out because of its unique double-peaked Ly$\\alpha$ line at $z=6.6$, unseen in any simulation of reionization. Here we present JWST/NIRCam wide-field slitless spectroscopy in a 21 arcmin$^2$ field centered on COLA1. We find 141 galaxies spectroscopically-selected through the [OIII]($\\lambda4969,5008$) doublet at $5.35<z<6.95$, with 40 of these sources showing H$\\beta$. For COLA1 we additionally detect [OIII]$_{4363}$ and H$\\gamma$. We measure a systemic redshift of $z=6.5917$ for COLA1, confirming the double-peak nature of the Ly$\\alpha$ profile. This implies that it resides in a highly ionized bubble and that it is leaking ionizing photons with a high escape fraction $f_{\\rm esc}{\\rm (LyC)}=20$-$50$%, making it a prime laboratory to study Lyman continuum escape in the Epoch of Reionization. COLA1 shows all the signs of a prolific ionizer with a Ly$\\alpha$ escape fraction of $81\\pm5\\%$, Balmer decrement indicating no dust, a steep UV slope ($\\beta_{\\rm UV}=-3.2\\pm 0.4$), and a star-formation surface density $\\gtrsim 10\\times$ that of typical galaxies at similar redshift. We detect 5 galaxies in COLA1's close environment ($\\Delta z<0.02$). Exploiting the high spectroscopic completeness inherent to grism surveys, and using mock simulations that mimic the selection function, we show the that number of detected companions is very typical for a similarly UV-bright ($M_{\\rm{UV}}\\sim-21.3$) galaxy; that is, the ionized bubble around COLA1 is unlikely due to an excessively large over-density. Instead, the measured ionizing properties suggest that COLA1 by itself might be powering the bubble required to explain its double-peaked Ly$\\alpha$ profile ($R_{\\rm ion}\\approx0.7$ pMpc), with minor contribution from detected neighbours ($-17.5>M_{\\rm UV}>-19.5$).","sentences":["The increasingly neutral intergalactic gas at $z>6$ impacts the Lyman-$\\alpha$ flux observed from galaxies.","One luminous galaxy, COLA1, stands out because of its unique double-peaked Ly$\\alpha$ line at $z=6.6$, unseen in any simulation of reionization.","Here we present JWST/NIRCam wide-field slitless spectroscopy in a 21 arcmin$^2$ field centered on COLA1.","We find 141 galaxies spectroscopically-selected through the [OIII]($\\lambda4969,5008$) doublet at $5.35<z<6.95$, with 40 of these sources showing H$\\beta$. For COLA1 we additionally detect [OIII]$_{4363}$ and","H$\\gamma$. We measure a systemic redshift of $z=6.5917$ for COLA1, confirming the double-peak nature of the Ly$\\alpha$ profile.","This implies that it resides in a highly ionized bubble and that it is leaking ionizing photons with a high escape fraction $f_{\\rm esc}{\\rm (LyC)}=20$-$50$%, making it a prime laboratory to study Lyman continuum escape in the Epoch of Reionization.","COLA1 shows all the signs of a prolific ionizer with a Ly$\\alpha$ escape fraction of $81\\pm5\\%$, Balmer decrement indicating no dust, a steep UV slope ($\\beta_{\\rm UV}=-3.2\\pm 0.4$), and a star-formation surface density $\\gtrsim 10\\times$ that of typical galaxies at similar redshift.","We detect 5 galaxies in COLA1's close environment ($\\Delta z<0.02$).","Exploiting the high spectroscopic completeness inherent to grism surveys, and using mock simulations that mimic the selection function, we show the that number of detected companions is very typical for a similarly UV-bright ($M_{\\rm{UV}}\\sim-21.3$) galaxy; that is, the ionized bubble around COLA1 is unlikely due to an excessively large over-density.","Instead, the measured ionizing properties suggest that COLA1 by itself might be powering the bubble required to explain its double-peaked Ly$\\alpha$ profile ($R_{\\rm ion}\\approx0.7$ pMpc), with minor contribution from detected neighbours ($-17.5>M_{\\rm UV}>-19.5$)."],"url":"http://arxiv.org/abs/2404.10040v1","category":"astro-ph.GA"}
{"created":"2024-04-15 17:58:40","title":"Thermal conversion of ultrathin nickel hydroxide for wide bandgap 2D nickel oxides","abstract":"Wide bandgap (WBG) semiconductors (Eg >2.0 eV) are integral to the advancement of next generation electronics, optoelectronics, and power industries, owing to their capability for high temperature operation, high breakdown voltage and efficient light emission. Enhanced power efficiency and functional performance can be attained through miniaturization, specifically via the integration of device fabrication into two-dimensional (2D) structure enabled by WBG 2D semiconductors. However, as an essential subgroup of WBG semiconductors, 2D transition metal oxides (TMOs) remain largely underexplored in terms of physical properties and applications in 2D opto-electronic devices, primarily due to the scarcity of sufficiently large 2D crystals. Thus, our goal is to develop synthesis pathways for 2D TMOs possessing large crystal domain (e.g. >10 nm), expanding the 2D TMOs family and providing insights for future engineering of 2D TMOs. Here, we demonstrate the synthesis of WBG 2D nickel oxide (NiO) (Eg > 2.7 eV) thermally converted from 2D nickel hydroxide (Ni(OH)2) with the lateral domain size larger than 10 um. Moreover, the conversion process is investigated using various microscopic techniques such as atomic force microscopy (AFM), Raman spectroscopy, transmission electron microscopy (TEM) and X-ray photoelectron spectroscopy (XPS), providing significant insights on the morphology and structure variation under different oxidative conditions. The electronic structure of the converted NixOy is further investigated using multiple soft X-ray spectroscopies, such as X-ray absorption (XAS) and emission spectroscopies (XES).","sentences":["Wide bandgap (WBG) semiconductors (Eg >2.0 eV) are integral to the advancement of next generation electronics, optoelectronics, and power industries, owing to their capability for high temperature operation, high breakdown voltage and efficient light emission.","Enhanced power efficiency and functional performance can be attained through miniaturization, specifically via the integration of device fabrication into two-dimensional (2D) structure enabled by WBG 2D semiconductors.","However, as an essential subgroup of WBG semiconductors, 2D transition metal oxides (TMOs) remain largely underexplored in terms of physical properties and applications in 2D opto-electronic devices, primarily due to the scarcity of sufficiently large 2D crystals.","Thus, our goal is to develop synthesis pathways for 2D TMOs possessing large crystal domain (e.g. >10 nm), expanding the 2D TMOs family and providing insights for future engineering of 2D TMOs.","Here, we demonstrate the synthesis of WBG 2D nickel oxide (NiO) (Eg > 2.7 eV) thermally converted from 2D nickel hydroxide (Ni(OH)2) with the lateral domain size larger than 10 um.","Moreover, the conversion process is investigated using various microscopic techniques such as atomic force microscopy (AFM), Raman spectroscopy, transmission electron microscopy (TEM) and X-ray photoelectron spectroscopy (XPS), providing significant insights on the morphology and structure variation under different oxidative conditions.","The electronic structure of the converted NixOy is further investigated using multiple soft X-ray spectroscopies, such as X-ray absorption (XAS) and emission spectroscopies (XES)."],"url":"http://arxiv.org/abs/2404.09986v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-15 17:46:45","title":"Reconstructing classes of 3D FRI signals from sampled tomographic projections at unknown angles","abstract":"Traditional sampling schemes often assume that the sampling locations are known. Motivated by the recent bioimaging technique known as cryogenic electron microscopy (cryoEM), we consider the problem of reconstructing an unknown 3D structure from samples of its 2D tomographic projections at unknown angles. We focus on 3D convex bilevel polyhedra and 3D point sources and show that the exact estimation of these 3D structures and of the projection angles can be achieved up to an orthogonal transformation. Moreover, we are able to show that the minimum number of projections needed to achieve perfect reconstruction is independent of the complexity of the signal model. By using the divergence theorem, we are able to retrieve the projected vertices of the polyhedron from the sampled tomographic projections, and then we show how to retrieve the 3D object and the projection angles from this information. The proof of our theorem is constructive and leads to a robust reconstruction algorithm, which we validate under various conditions. Finally, we apply aspects of the proposed framework to calibration of X-ray computed tomography (CT) data.","sentences":["Traditional sampling schemes often assume that the sampling locations are known.","Motivated by the recent bioimaging technique known as cryogenic electron microscopy (cryoEM), we consider the problem of reconstructing an unknown 3D structure from samples of its 2D tomographic projections at unknown angles.","We focus on 3D convex bilevel polyhedra and 3D point sources and show that the exact estimation of these 3D structures and of the projection angles can be achieved up to an orthogonal transformation.","Moreover, we are able to show that the minimum number of projections needed to achieve perfect reconstruction is independent of the complexity of the signal model.","By using the divergence theorem, we are able to retrieve the projected vertices of the polyhedron from the sampled tomographic projections, and then we show how to retrieve the 3D object and the projection angles from this information.","The proof of our theorem is constructive and leads to a robust reconstruction algorithm, which we validate under various conditions.","Finally, we apply aspects of the proposed framework to calibration of X-ray computed tomography (CT) data."],"url":"http://arxiv.org/abs/2404.09969v1","category":"eess.SP"}
{"created":"2024-04-15 17:38:34","title":"Pseudo P-values for Assessing Covariate Balance in a Finite Study Population with Application to the California Sugar Sweetened Beverage Tax Study","abstract":"Assessing covariate balance (CB) is a common practice in various types of evaluation studies. Two-sample descriptive statistics, such as the standardized mean difference, have been widely applied in the scientific literature to assess the goodness of CB. Studies in health policy, health services research, built and social environment research, and many other fields often involve a finite number of units that may be subject to different treatment levels. Our case study, the California Sugar Sweetened Beverage (SSB) Tax Study, include 332 study cities in the state of California, among which individual cities may elect to levy a city-wide excise tax on SSB sales. Evaluating the balance of covariates between study cities with and without the tax policy is essential for assessing the effects of the policy on health outcomes of interest. In this paper, we introduce the novel concepts of the pseudo p-value and the standardized pseudo p-value, which are descriptive statistics to assess the overall goodness of CB between study arms in a finite study population. While not meant as a hypothesis test, the pseudo p-values bear superficial similarity to the classic p-value, which makes them easy to apply and interpret in applications. We discuss some theoretical properties of the pseudo p-values and present an algorithm to calculate them. We report a numerical simulation study to demonstrate their performance. We apply the pseudo p-values to the California SSB Tax study to assess the balance of city-level characteristics between the two study arms.","sentences":["Assessing covariate balance (CB) is a common practice in various types of evaluation studies.","Two-sample descriptive statistics, such as the standardized mean difference, have been widely applied in the scientific literature to assess the goodness of CB.","Studies in health policy, health services research, built and social environment research, and many other fields often involve a finite number of units that may be subject to different treatment levels.","Our case study, the California Sugar Sweetened Beverage (SSB) Tax Study, include 332 study cities in the state of California, among which individual cities may elect to levy a city-wide excise tax on SSB sales.","Evaluating the balance of covariates between study cities with and without the tax policy is essential for assessing the effects of the policy on health outcomes of interest.","In this paper, we introduce the novel concepts of the pseudo p-value and the standardized pseudo p-value, which are descriptive statistics to assess the overall goodness of CB between study arms in a finite study population.","While not meant as a hypothesis test, the pseudo p-values bear superficial similarity to the classic p-value, which makes them easy to apply and interpret in applications.","We discuss some theoretical properties of the pseudo p-values and present an algorithm to calculate them.","We report a numerical simulation study to demonstrate their performance.","We apply the pseudo p-values to the California SSB Tax study to assess the balance of city-level characteristics between the two study arms."],"url":"http://arxiv.org/abs/2404.09960v1","category":"stat.ME"}
{"created":"2024-04-15 17:22:49","title":"Scalable photonic diffractive generators through sampling noises from scattering medium","abstract":"Photonic computing, with potentials of high parallelism, low latency and high energy efficiency, have gained progressive interest at the forefront of neural network (NN) accelerators. However, most existing photonic computing accelerators concentrate on discriminative NNs. Large-scale generative photonic computing machines remain largely unexplored, partly due to poor data accessibility, accuracy and hardware feasibility. Here, we harness random light scattering in disordered media as a native noise source and leverage large-scale diffractive optical computing to generate images from above noise, thereby achieving hardware consistency by solely pursuing the spatial parallelism of light. To realize experimental data accessibility, we design two encoding strategies between images and optical noise latent space that effectively solves the training problem. Furthermore, we utilize advanced photonic NN architectures including cascaded and parallel configurations of diffraction layers to enhance the image generation performance. Our results show that the photonic generator is capable of producing clear and meaningful synthesized images across several standard public datasets. As a photonic generative machine, this work makes an important contribution to photonic computing and paves the way for more sophisticated applications such as real world data augmentation and multi modal generation.","sentences":["Photonic computing, with potentials of high parallelism, low latency and high energy efficiency, have gained progressive interest at the forefront of neural network (NN) accelerators.","However, most existing photonic computing accelerators concentrate on discriminative NNs.","Large-scale generative photonic computing machines remain largely unexplored, partly due to poor data accessibility, accuracy and hardware feasibility.","Here, we harness random light scattering in disordered media as a native noise source and leverage large-scale diffractive optical computing to generate images from above noise, thereby achieving hardware consistency by solely pursuing the spatial parallelism of light.","To realize experimental data accessibility, we design two encoding strategies between images and optical noise latent space that effectively solves the training problem.","Furthermore, we utilize advanced photonic NN architectures including cascaded and parallel configurations of diffraction layers to enhance the image generation performance.","Our results show that the photonic generator is capable of producing clear and meaningful synthesized images across several standard public datasets.","As a photonic generative machine, this work makes an important contribution to photonic computing and paves the way for more sophisticated applications such as real world data augmentation and multi modal generation."],"url":"http://arxiv.org/abs/2404.09948v1","category":"physics.optics"}
{"created":"2024-04-15 17:21:23","title":"Bounding seed loss from isolated habitat patches","abstract":"Dispersal of propagules (seeds, spores) from a geographically isolated population into an uninhabitable matrix can threaten population persistence if it prevents new growth from keeping pace with mortality. Quantifying propagule loss can thus inform restoration and conservation of vulnerable populations in fragmented landscapes. To model propagule loss in detail, one can integrate dispersal kernels (probabilistic descriptions of dispersal) and plant densities. However, one might lack the detailed spatial information and computational tools needed by such integral models. Here we derive two upper bounds on the probability of propagule loss--one assuming rotational symmetry of dispersal and the other not--that require only habitat area, habitat perimeter, and the mean dispersal distance of a propagule. We compare the bounds to simulations of integral models for the population of Asclepias syriaca (common milkweed) at McKnight Prairie--a 13.7 hectare reserve surrounded by agricultural fields in Goodhue County, Minnesota--and identify conditions under which the bounds closely estimate propagule loss.","sentences":["Dispersal of propagules (seeds, spores) from a geographically isolated population into an uninhabitable matrix can threaten population persistence if it prevents new growth from keeping pace with mortality.","Quantifying propagule loss can thus inform restoration and conservation of vulnerable populations in fragmented landscapes.","To model propagule loss in detail, one can integrate dispersal kernels (probabilistic descriptions of dispersal) and plant densities.","However, one might lack the detailed spatial information and computational tools needed by such integral models.","Here we derive two upper bounds on the probability of propagule loss--one assuming rotational symmetry of dispersal and the other not--that require only habitat area, habitat perimeter, and the mean dispersal distance of a propagule.","We compare the bounds to simulations of integral models for the population of Asclepias syriaca (common milkweed) at McKnight Prairie--a 13.7 hectare reserve surrounded by agricultural fields in Goodhue County, Minnesota--and identify conditions under which the bounds closely estimate propagule loss."],"url":"http://arxiv.org/abs/2404.09947v1","category":"q-bio.PE"}
{"created":"2024-04-15 17:16:29","title":"A Systematic Overview of Single-Cell Transcriptomics Databases, their Use cases, and Limitations","abstract":"Rapid advancements in high-throughput single-cell RNA-seq (scRNA-seq) technologies and experimental protocols have led to the generation of vast amounts of genomic data that populates several online databases and repositories. Here, we systematically examined large-scale scRNA-seq databases, categorizing them based on their scope and purpose such as general, tissue-specific databases, disease-specific databases, cancer-focused databases, and cell type-focused databases. Next, we discuss the technical and methodological challenges associated with curating large-scale scRNA-seq databases, along with current computational solutions. We argue that understanding scRNA-seq databases, including their limitations and assumptions, is crucial for effectively utilizing this data to make robust discoveries and identify novel biological insights. Furthermore, we propose that bridging the gap between computational and wet lab scientists through user-friendly web-based platforms is needed for democratizing access to single-cell data. These platforms would facilitate interdisciplinary research, enabling researchers from various disciplines to collaborate effectively. This review underscores the importance of leveraging computational approaches to unravel the complexities of single-cell data and offers a promising direction for future research in the field.","sentences":["Rapid advancements in high-throughput single-cell RNA-seq (scRNA-seq) technologies and experimental protocols have led to the generation of vast amounts of genomic data that populates several online databases and repositories.","Here, we systematically examined large-scale scRNA-seq databases, categorizing them based on their scope and purpose such as general, tissue-specific databases, disease-specific databases, cancer-focused databases, and cell type-focused databases.","Next, we discuss the technical and methodological challenges associated with curating large-scale scRNA-seq databases, along with current computational solutions.","We argue that understanding scRNA-seq databases, including their limitations and assumptions, is crucial for effectively utilizing this data to make robust discoveries and identify novel biological insights.","Furthermore, we propose that bridging the gap between computational and wet lab scientists through user-friendly web-based platforms is needed for democratizing access to single-cell data.","These platforms would facilitate interdisciplinary research, enabling researchers from various disciplines to collaborate effectively.","This review underscores the importance of leveraging computational approaches to unravel the complexities of single-cell data and offers a promising direction for future research in the field."],"url":"http://arxiv.org/abs/2404.10545v1","category":"q-bio.GN"}
{"created":"2024-04-15 17:14:20","title":"Witt vector affine Springer fibers","abstract":"We establish dimension formulas for the Witt vector affine Springer fibers associated to a reductive group over a mixed characteristic local field, under the assumption that the group is essentially tamely ramified and the residue characteristic is not bad. Besides the discriminant valuations that show up in classical works on the usual affine Springer fibers, our formula also involves the Artin conductors and the Kottwitz invariants of the relevant conjugacy classes.","sentences":["We establish dimension formulas for the Witt vector affine Springer fibers associated to a reductive group over a mixed characteristic local field, under the assumption that the group is essentially tamely ramified and the residue characteristic is not bad.","Besides the discriminant valuations that show up in classical works on the usual affine Springer fibers, our formula also involves the Artin conductors and the Kottwitz invariants of the relevant conjugacy classes."],"url":"http://arxiv.org/abs/2404.09945v1","category":"math.AG"}
{"created":"2024-04-15 17:12:55","title":"Novel Joint Estimation and Decoding Metrics for Short-Block length Transmission Systems","abstract":"This paper presents Bit-Interleaved Coded Modulation metrics for joint estimation detection using training or reference signal transmission strategies for short to long block length channels. We show that it is possible to enhance the performance and sensitivity through joint detection-estimation compared to standard receivers, especially when the channel state information is unknown and the density of the training dimensions is low. The performance analysis makes use of a full 5G transmitter and receiver chains for both Polar and LDPC coded transmissions paired with BPSK/QPSK modulation schemes. We consider transmissions where reference signals are interleaved with data and both are transmitted over a small number of OFDM symbols so that near-perfect channel estimation cannot be achieved. This is particularly adapted to mini-slot transmissions for ultra-reliable, low-latency communications (URLLC) or for short packet random access use cases. We characterize the performance for up to eight receiving antennas in order to determine the performance gain offered by the proposed BICM detection in realistic base station receiver scenarios. Our findings demonstrate that when the detection windows used in the metric units is on the order of four modulated symbols the proposed BICM metrics can be used to achieve detection performance that is close to that of a coherent receiver with perfect channel state information for both polar and LDPC coded configurations. Furthermore, we show that for transmissions with low DMRS density, a good trade-off can be achieved in terms of additional coding gain and improved channel estimation quality by adaptive DMRS power adjustment.","sentences":["This paper presents Bit-Interleaved Coded Modulation metrics for joint estimation detection using training or reference signal transmission strategies for short to long block length channels.","We show that it is possible to enhance the performance and sensitivity through joint detection-estimation compared to standard receivers, especially when the channel state information is unknown and the density of the training dimensions is low.","The performance analysis makes use of a full 5G transmitter and receiver chains for both Polar and LDPC coded transmissions paired with BPSK/QPSK modulation schemes.","We consider transmissions where reference signals are interleaved with data and both are transmitted over a small number of OFDM symbols so that near-perfect channel estimation cannot be achieved.","This is particularly adapted to mini-slot transmissions for ultra-reliable, low-latency communications (URLLC) or for short packet random access use cases.","We characterize the performance for up to eight receiving antennas in order to determine the performance gain offered by the proposed BICM detection in realistic base station receiver scenarios.","Our findings demonstrate that when the detection windows used in the metric units is on the order of four modulated symbols the proposed BICM metrics can be used to achieve detection performance that is close to that of a coherent receiver with perfect channel state information for both polar and LDPC coded configurations.","Furthermore, we show that for transmissions with low DMRS density, a good trade-off can be achieved in terms of additional coding gain and improved channel estimation quality by adaptive DMRS power adjustment."],"url":"http://arxiv.org/abs/2404.09943v1","category":"cs.IT"}
{"created":"2024-04-15 17:11:25","title":"Knowledge-enhanced Visual-Language Pretraining for Computational Pathology","abstract":"In this paper, we consider the problem of visual representation learning for computational pathology, by exploiting large-scale image-text pairs gathered from public resources, along with the domain specific knowledge in pathology. Specifically, we make the following contributions: (i) We curate a pathology knowledge tree that consists of 50,470 informative attributes for 4,718 diseases requiring pathology diagnosis from 32 human tissues. To our knowledge, this is the first comprehensive structured pathology knowledge base; (ii) We develop a knowledge-enhanced visual-language pretraining approach, where we first project pathology-specific knowledge into latent embedding space via language model, and use it to guide the visual representation learning; (iii) We conduct thorough experiments to validate the effectiveness of our proposed components, demonstrating significant performance improvement on various downstream tasks, including cross-modal retrieval, zero-shot classification on pathology patches, and zero-shot tumor subtyping on whole slide images (WSIs). All codes, models and the pathology knowledge tree will be released to the research community","sentences":["In this paper, we consider the problem of visual representation learning for computational pathology, by exploiting large-scale image-text pairs gathered from public resources, along with the domain specific knowledge in pathology.","Specifically, we make the following contributions: (i) We curate a pathology knowledge tree that consists of 50,470 informative attributes for 4,718 diseases requiring pathology diagnosis from 32 human tissues.","To our knowledge, this is the first comprehensive structured pathology knowledge base; (ii) We develop a knowledge-enhanced visual-language pretraining approach, where we first project pathology-specific knowledge into latent embedding space via language model, and use it to guide the visual representation learning; (iii) We conduct thorough experiments to validate the effectiveness of our proposed components, demonstrating significant performance improvement on various downstream tasks, including cross-modal retrieval, zero-shot classification on pathology patches, and zero-shot tumor subtyping on whole slide images (WSIs).","All codes, models and the pathology knowledge tree will be released to the research community"],"url":"http://arxiv.org/abs/2404.09942v1","category":"cs.CV"}
{"created":"2024-04-15 17:08:53","title":"eMotion-GAN: A Motion-based GAN for Photorealistic and Facial Expression Preserving Frontal View Synthesis","abstract":"Many existing facial expression recognition (FER) systems encounter substantial performance degradation when faced with variations in head pose. Numerous frontalization methods have been proposed to enhance these systems' performance under such conditions. However, they often introduce undesirable deformations, rendering them less suitable for precise facial expression analysis. In this paper, we present eMotion-GAN, a novel deep learning approach designed for frontal view synthesis while preserving facial expressions within the motion domain. Considering the motion induced by head variation as noise and the motion induced by facial expression as the relevant information, our model is trained to filter out the noisy motion in order to retain only the motion related to facial expression. The filtered motion is then mapped onto a neutral frontal face to generate the corresponding expressive frontal face. We conducted extensive evaluations using several widely recognized dynamic FER datasets, which encompass sequences exhibiting various degrees of head pose variations in both intensity and orientation. Our results demonstrate the effectiveness of our approach in significantly reducing the FER performance gap between frontal and non-frontal faces. Specifically, we achieved a FER improvement of up to +5\\% for small pose variations and up to +20\\% improvement for larger pose variations. Code available at \\url{https://github.com/o-ikne/eMotion-GAN.git}.","sentences":["Many existing facial expression recognition (FER) systems encounter substantial performance degradation when faced with variations in head pose.","Numerous frontalization methods have been proposed to enhance these systems' performance under such conditions.","However, they often introduce undesirable deformations, rendering them less suitable for precise facial expression analysis.","In this paper, we present eMotion-GAN, a novel deep learning approach designed for frontal view synthesis while preserving facial expressions within the motion domain.","Considering the motion induced by head variation as noise and the motion induced by facial expression as the relevant information, our model is trained to filter out the noisy motion in order to retain only the motion related to facial expression.","The filtered motion is then mapped onto a neutral frontal face to generate the corresponding expressive frontal face.","We conducted extensive evaluations using several widely recognized dynamic FER datasets, which encompass sequences exhibiting various degrees of head pose variations in both intensity and orientation.","Our results demonstrate the effectiveness of our approach in significantly reducing the FER performance gap between frontal and non-frontal faces.","Specifically, we achieved a FER improvement of up to +5\\% for small pose variations and up to +20\\% improvement for larger pose variations.","Code available at \\url{https://github.com/o-ikne/eMotion-GAN.git}."],"url":"http://arxiv.org/abs/2404.09940v1","category":"cs.CV"}
{"created":"2024-04-15 17:05:04","title":"Testing for homogeneity of several functional variables via multiple maximum variance discrepancy","abstract":"This paper adresses the problem of testing for the equality of $k$ probability distributions on Hilbert spaces, with $k\\geqslant 2$. We introduce a generalization of the maximum variance discrepancy called multiple maximum variance discrepancy (MMVD). Then, a consistent estimator of this measure is proposed as test statistic, and its asymptotic distribution under the null hypothesis is derived. A simulation study comparing the proposed test with existing ones is provided","sentences":["This paper adresses the problem of testing for the equality of $k$ probability distributions on Hilbert spaces, with $k\\geqslant 2$. We introduce a generalization of the maximum variance discrepancy called multiple maximum variance discrepancy (MMVD).","Then, a consistent estimator of this measure is proposed as test statistic, and its asymptotic distribution under the null hypothesis is derived.","A simulation study comparing the proposed test with existing ones is provided"],"url":"http://arxiv.org/abs/2404.09938v1","category":"math.ST"}
{"created":"2024-04-15 17:03:14","title":"Accelerated black holes in (2+1) dimensions: Quasinormal modes and Stability","abstract":"We investigate the propagation of a scalar field in a $(2+1)$-dimensional accelerated black hole, recently revisited in \\cite{Arenas_Henriquez_2022}. We briefly describe the minimally-coupled configuration as rendering a trivial scalar perturbation with a rescale of the field mass. On the contrary, the free scalar field propagation presents an intricate dynamic, whose equation may be reduced through the use of Israel junction conditions and a non-trivial ansatz. In this case, using two different methods we calculate the quasinormal modes of the solution also obtaining unstable field profiles delivered by the linear scalar perturbation to the background geometry. We scrutinize the parameter space of angular eigenvalue of the field and accelerations under which such instabilities happen.","sentences":["We investigate the propagation of a scalar field in a $(2+1)$-dimensional accelerated black hole, recently revisited in \\cite{Arenas_Henriquez_2022}.","We briefly describe the minimally-coupled configuration as rendering a trivial scalar perturbation with a rescale of the field mass.","On the contrary, the free scalar field propagation presents an intricate dynamic, whose equation may be reduced through the use of Israel junction conditions and a non-trivial ansatz.","In this case, using two different methods we calculate the quasinormal modes of the solution also obtaining unstable field profiles delivered by the linear scalar perturbation to the background geometry.","We scrutinize the parameter space of angular eigenvalue of the field and accelerations under which such instabilities happen."],"url":"http://arxiv.org/abs/2404.09936v1","category":"gr-qc"}
{"created":"2024-04-16 16:51:27","title":"GazeHTA: End-to-end Gaze Target Detection with Head-Target Association","abstract":"We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets.","sentences":["We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at.","Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets.","In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image.","GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets.","Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets."],"url":"http://arxiv.org/abs/2404.10718v1","category":"cs.CV"}
{"created":"2024-04-16 15:35:08","title":"Partial Differential Equations on Low-Dimensional Structures","abstract":"This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures. The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem. The main technical result is proving the closedness of the low-dimensional second-order operator. To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted. An alternative direction of study is presented to extend the class of accessible initial data. Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma. The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems. We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator.","sentences":["This thesis pertains to the study of elliptic and parabolic partial differential equations on \"thin\" structures.","The first main objective is to establish the strong and weak low-dimensional counterparts of the parabolic Neumann problem.","The main technical result is proving the closedness of the low-dimensional second-order operator.","To construct a semigroup, a variant of Magyar of the Hille-Yosida Theorem for non-invertible operators is adapted.","An alternative direction of study is presented to extend the class of accessible initial data.","Weak-type parabolic problems are defined, and the existence of solutions is obtained by the application of the Lions version of the Lax-Milgram Lemma.","The second aspect of the thesis is to examine the higher regularity of weak solutions to low-dimensional elliptic problems.","We prove that for any weak low-dimensional elliptic solution, some Cosserat vector field exists as a witness of the membership of the solution to the domain of the second-order operator."],"url":"http://arxiv.org/abs/2404.10657v1","category":"math.AP"}
{"created":"2024-04-16 15:14:45","title":"Adapting SAM for Surgical Instrument Tracking and Segmentation in Endoscopic Submucosal Dissection Videos","abstract":"The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures. However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process. To tackle this, we propose a novel framework for surgical instrument segmentation and tracking. Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video. Our method adopts a two-stage approach to efficiently segment videos. Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset. The fine-tuned SAM model is applied to segment the initial frames of the video accurately. Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence. This workflow enables us to precisely segment and track objects within the video. Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery.","sentences":["The precise tracking and segmentation of surgical instruments have led to a remarkable enhancement in the efficiency of surgical procedures.","However, the challenge lies in achieving accurate segmentation of surgical instruments while minimizing the need for manual annotation and reducing the time required for the segmentation process.","To tackle this, we propose a novel framework for surgical instrument segmentation and tracking.","Specifically, with a tiny subset of frames for segmentation, we ensure accurate segmentation across the entire surgical video.","Our method adopts a two-stage approach to efficiently segment videos.","Initially, we utilize the Segment-Anything (SAM) model, which has been fine-tuned using the Low-Rank Adaptation (LoRA) on the EndoVis17 Dataset.","The fine-tuned SAM model is applied to segment the initial frames of the video accurately.","Subsequently, we deploy the XMem++ tracking algorithm to follow the annotated frames, thereby facilitating the segmentation of the entire video sequence.","This workflow enables us to precisely segment and track objects within the video.","Through extensive evaluation of the in-distribution dataset (EndoVis17) and the out-of-distribution datasets (EndoVis18 \\& the endoscopic submucosal dissection surgery (ESD) dataset), our framework demonstrates exceptional accuracy and robustness, thus showcasing its potential to advance the automated robotic-assisted surgery."],"url":"http://arxiv.org/abs/2404.10640v1","category":"eess.IV"}
{"created":"2024-04-16 15:10:36","title":"On Homomorphism Indistinguishability and Hypertree Depth","abstract":"$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs. It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting. It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$. This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs. The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012). To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs.","sentences":["$GC^k$ is a logic introduced by Scheidt and Schweikardt (2023) to express properties of hypergraphs.","It is similar to first-order logic with counting quantifiers ($C$) adapted to the hypergraph setting.","It has distinct sets of variables for vertices and for hyperedges and requires vertex variables to be guarded by hyperedge variables on every quantification.   ","We prove that two hypergraphs $G$, $H$ satisfy the same sentences in the logic $GC^k$ with guard depth at most $k$ if, and only if, they are homomorphism indistinguishable over the class of hypergraphs of strict hypertree depth at most $k$.","This lifts the analogous result for tree depth $\\leq k$ and sentences of first-order logic with counting quantifiers of quantifier rank at most $k$ due to Grohe (2020) from graphs to hypergraphs.","The guard depth of a formula is the quantifier rank with respect to hyperedge variables, and strict hypertree depth is a restriction of hypertree depth as defined by Adler, Gaven\\v{c}iak and Klimo\\v{s}ov\\'a (2012).","To justify this restriction, we show that for every $H$, the strict hypertree depth of $H$ is at most 1 larger than its hypertree depth, and we give additional evidence that strict hypertree depth can be viewed as a reasonable generalisation of tree depth for hypergraphs."],"url":"http://arxiv.org/abs/2404.10637v1","category":"cs.LO"}
{"created":"2024-04-16 14:52:15","title":"Exploring selective image matching methods for zero-shot and few-sample unsupervised domain adaptation of urban canopy prediction","abstract":"We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning. Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning. We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning. These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model. The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively.","sentences":["We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning.","Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning.","We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning.","These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model.","The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively."],"url":"http://arxiv.org/abs/2404.10626v1","category":"cs.CV"}
{"created":"2024-04-16 13:11:02","title":"Characterizing Polkadot's Transactions Ecosystem: methodology, tools, and insights","abstract":"The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology. However, these projects are usually distributed, with a weak feedback schemes. Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes. Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk). To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability. However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far. The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions. In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading. These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics.","sentences":["The growth potential of a crypto(currency) project can be measured by the use cases spurred by the underlying technology.","However, these projects are usually distributed, with a weak feedback schemes.","Hence, a metric that is widely used as a proxy for their healthiness is the number of transactions and related volumes.","Nevertheless, such a metric can be subject to manipulation (the crypto market being an unregulated one magnifies such a risk).","To address the cited gap we design a comprehensive methodology to process large cryptocurrency transaction graphs that, after clustering user addresses of interest, derives a compact representation of the network that highlights clusters interactions.   ","To show the viability of our solution, we bring forward a use case centered on Polkadot, which has gained significant attention in the digital currency landscape due to its pioneering approach to interoperability and scalability.","However, little is known about how many and to what extent its wide range of enabled use cases have been adopted by end-users so far.","The answer to this type of question means mapping Polkadot (or any analyzed crypto project) on a palette that ranges from a thriving ecosystem to a speculative coin without compelling use cases.   ","Our findings demonstrate that crypto exchanges exert considerable influence on the Polkadot network, owning nearly 40% of all addresses in the ledger and absorbing at least 80% of all transactions.","In addition, the high volume of inter-exchange transactions (> 20%) underscores the strong interconnections among just a couple of prominent exchanges, prompting further investigations into the behavior of these actors to uncover potential unethical activities, such as wash trading.","These results, while characterized by a high level of scalability and adaptability, are at the same time immune from the drawbacks of currently used metrics."],"url":"http://arxiv.org/abs/2404.10543v1","category":"cs.CR"}
{"created":"2024-04-16 07:59:53","title":"Robust Performance Over Changing Intersymbol Interference Channels by Spatial Coupling","abstract":"We show that spatially coupled low-density parity- check (LDPC) codes yield robust performance over changing intersymbol interfere (ISI) channels with optimal and suboptimal detectors. We compare the performance with classical LDPC code design which involves optimizing the degree distribution for a given (known) channel. We demonstrate that these classical schemes, despite working very good when designed for a given channel, can perform poorly if the channel is exchanged. With spatially coupled LDPC codes, however, we get performances close to the symmetric information rates with just a single code, without the need to know the channel and adapt to it at the transmitter. We also investigate threshold saturation with the linear minimum mean square error (LMMSE) detector and show that with spatial coupling its performance can get remarkably close to that of an optimal detector for regular LDPC codes.","sentences":["We show that spatially coupled low-density parity- check (LDPC) codes yield robust performance over changing intersymbol interfere (ISI) channels with optimal and suboptimal detectors.","We compare the performance with classical LDPC code design which involves optimizing the degree distribution for a given (known) channel.","We demonstrate that these classical schemes, despite working very good when designed for a given channel, can perform poorly if the channel is exchanged.","With spatially coupled LDPC codes, however, we get performances close to the symmetric information rates with just a single code, without the need to know the channel and adapt to it at the transmitter.","We also investigate threshold saturation with the linear minimum mean square error (LMMSE) detector and show that with spatial coupling its performance can get remarkably close to that of an optimal detector for regular LDPC codes."],"url":"http://arxiv.org/abs/2404.10367v1","category":"cs.IT"}
{"created":"2024-04-16 07:50:28","title":"Numerical study of the Gross-Pitaevskii equation on a two-dimensional ring and vortex nucleation","abstract":"We consider the Gross-Pitaevskii equation with a confining ring potential with a Gaussian profile. By introducing a rotating sinusoidal perturbation, we numerically highlight the nucleation of quantum vortices in a particular regime throughout the dynamics. Numerical computations are made via a Strang splitting time integration and a two-point flux approximation Finite Volume scheme based on a particular admissible triangulation. We also develop numerical algorithms for vortex tracking adapted to our finite volume framework.","sentences":["We consider the Gross-Pitaevskii equation with a confining ring potential with a Gaussian profile.","By introducing a rotating sinusoidal perturbation, we numerically highlight the nucleation of quantum vortices in a particular regime throughout the dynamics.","Numerical computations are made via a Strang splitting time integration and a two-point flux approximation Finite Volume scheme based on a particular admissible triangulation.","We also develop numerical algorithms for vortex tracking adapted to our finite volume framework."],"url":"http://arxiv.org/abs/2404.10360v1","category":"math.NA"}
{"created":"2024-04-16 07:11:48","title":"Exact and Efficient Unlearning for Large Language Model-based Recommendation","abstract":"The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec) customizes Large Language Models (LLMs) through parameter-efficient fine-tuning (PEFT) using recommenda- tion data. The inclusion of user data in LLMs raises privacy concerns. To protect users, the unlearning process in LLMRec, specifically removing unusable data (e.g., historical behaviors) from established LLMRec models, becomes crucial. However, existing unlearning methods are insufficient for the unique characteristics of LLM- Rec, mainly due to high computational costs or incomplete data erasure. In this study, we introduce the Adapter Partition and Ag- gregation (APA) framework for exact and efficient unlearning while maintaining recommendation performance. APA achieves this by establishing distinct adapters for partitioned training data shards and retraining only the adapters impacted by unusable data for un- learning. To preserve recommendation performance and mitigate considerable inference costs, APA employs parameter-level adapter aggregation with sample-adaptive attention for individual testing samples. Extensive experiments substantiate the effectiveness and efficiency of our proposed framework","sentences":["The evolving paradigm of Large Language Model-based Recom- mendation (LLMRec) customizes Large Language Models (LLMs) through parameter-efficient fine-tuning (PEFT) using recommenda- tion data.","The inclusion of user data in LLMs raises privacy concerns.","To protect users, the unlearning process in LLMRec, specifically removing unusable data (e.g., historical behaviors) from established LLMRec models, becomes crucial.","However, existing unlearning methods are insufficient for the unique characteristics of LLM- Rec, mainly due to high computational costs or incomplete data erasure.","In this study, we introduce the Adapter Partition and Ag- gregation (APA) framework for exact and efficient unlearning while maintaining recommendation performance.","APA achieves this by establishing distinct adapters for partitioned training data shards and retraining only the adapters impacted by unusable data for un- learning.","To preserve recommendation performance and mitigate considerable inference costs, APA employs parameter-level adapter aggregation with sample-adaptive attention for individual testing samples.","Extensive experiments substantiate the effectiveness and efficiency of our proposed framework"],"url":"http://arxiv.org/abs/2404.10327v1","category":"cs.IR"}
{"created":"2024-04-16 05:20:53","title":"Inducing spectral gaps for the cohomological Laplacians of $\\operatorname{SL}_n(\\mathbb{Z})$ and $\\operatorname{SAut}(F_n)$","abstract":"The technique of inducing spectral gaps for cohomological Laplacians in degree zero was used by Kaluba, Kielak and Nowak to prove property (T) for $\\operatorname{SAut}(F_n)$ and $\\operatorname{SL}_n(\\mathbb{Z})$. In this paper, we adapt this technique to Laplacians in degree one. This allows to provide a lower bound for the cohomological Laplacian in degree one for $\\operatorname{SL}_n(\\mathbb{Z})$ for every unitary representation. In particular, one gets in that way an alternative proof of property (T) for $\\operatorname{SL}_n(\\mathbb{Z})$ whenever $n\\geq 3$.","sentences":["The technique of inducing spectral gaps for cohomological Laplacians in degree zero was used by Kaluba, Kielak and Nowak to prove property (T) for $\\operatorname{SAut}(F_n)$ and $\\operatorname{SL}_n(\\mathbb{Z})$. In this paper, we adapt this technique to Laplacians in degree one.","This allows to provide a lower bound for the cohomological Laplacian in degree one for $\\operatorname{SL}_n(\\mathbb{Z})$ for every unitary representation.","In particular, one gets in that way an alternative proof of property (T) for $\\operatorname{SL}_n(\\mathbb{Z})$ whenever $n\\geq 3$."],"url":"http://arxiv.org/abs/2404.10287v1","category":"math.GR"}
{"created":"2024-04-16 02:25:12","title":"Urban Water Sprinkler Routing: A Multi-Depot Mixed Capacitated Arc Routing Problem Incorporating Real-Time Demands","abstract":"Fugitive road dust (FRD), as one of the major pollutants in the city, poses great harm to the environment and the physical health of citizens. A common countermeasure adopted by government agencies is employing on-road water trucks (sprinklers) to spray water (sprinkle) on urban streets to reduce the FRD. Currently, the traveling routes of sprinklers are usually planned based on drivers' experience, which may lead low operation efficiency and could not respond to the real-time sprinkling demands. To address these issues, this study formulates the routes planning of sprinklers as a multi-depot mixed capacitated arc routing problem with real-time demands with the aim of minimizing the sprinklers' travel distance. We develop an improved adaptive large neighborhood search (ALNS) algorithm that incorporates a tabu-list and a perturbation mechanism to solve this problem. Furthermore, a problem-specific acceleration mechanism is designed to reduce unnecessary search domains to improve the efficiency of the algorithm. Empirical experiments are conducted based on various scenarios and the results demonstrate that the proposed algorithm generates solutions that are superior or at least comparable to the solutions generated by the traditional ALNS algorithm but with significantly lower computation time. Sensitivity analysis is conducted to explore the effects of relevant parameters on the results. This study is the first to incorporate real-time FRD pollution information, gathered through multiple data sources via IoT technology, into urban sprinkling operations, extending the traditional CARP from a tactical planning to a real-time operational environment. A real-world implementation case is also presented.","sentences":["Fugitive road dust (FRD), as one of the major pollutants in the city, poses great harm to the environment and the physical health of citizens.","A common countermeasure adopted by government agencies is employing on-road water trucks (sprinklers) to spray water (sprinkle) on urban streets to reduce the FRD.","Currently, the traveling routes of sprinklers are usually planned based on drivers' experience, which may lead low operation efficiency and could not respond to the real-time sprinkling demands.","To address these issues, this study formulates the routes planning of sprinklers as a multi-depot mixed capacitated arc routing problem with real-time demands with the aim of minimizing the sprinklers' travel distance.","We develop an improved adaptive large neighborhood search (ALNS) algorithm that incorporates a tabu-list and a perturbation mechanism to solve this problem.","Furthermore, a problem-specific acceleration mechanism is designed to reduce unnecessary search domains to improve the efficiency of the algorithm.","Empirical experiments are conducted based on various scenarios and the results demonstrate that the proposed algorithm generates solutions that are superior or at least comparable to the solutions generated by the traditional ALNS algorithm but with significantly lower computation time.","Sensitivity analysis is conducted to explore the effects of relevant parameters on the results.","This study is the first to incorporate real-time FRD pollution information, gathered through multiple data sources via IoT technology, into urban sprinkling operations, extending the traditional CARP from a tactical planning to a real-time operational environment.","A real-world implementation case is also presented."],"url":"http://arxiv.org/abs/2404.10230v1","category":"math.OC"}
{"created":"2024-04-15 22:13:35","title":"Salient Object-Aware Background Generation using Text-Guided Diffusion Models","abstract":"Generating background scenes for salient objects plays a crucial role across various domains including creative design and e-commerce, as it enhances the presentation and context of subjects by integrating them into tailored environments. Background generation can be framed as a task of text-conditioned outpainting, where the goal is to extend image content beyond a salient object's boundaries on a blank background. Although popular diffusion models for text-guided inpainting can also be used for outpainting by mask inversion, they are trained to fill in missing parts of an image rather than to place an object into a scene. Consequently, when used for background creation, inpainting models frequently extend the salient object's boundaries and thereby change the object's identity, which is a phenomenon we call \"object expansion.\" This paper introduces a model for adapting inpainting diffusion models to the salient object outpainting task using Stable Diffusion and ControlNet architectures. We present a series of qualitative and quantitative results across models and datasets, including a newly proposed metric to measure object expansion that does not require any human labeling. Compared to Stable Diffusion 2.0 Inpainting, our proposed approach reduces object expansion by 3.6x on average with no degradation in standard visual metrics across multiple datasets.","sentences":["Generating background scenes for salient objects plays a crucial role across various domains including creative design and e-commerce, as it enhances the presentation and context of subjects by integrating them into tailored environments.","Background generation can be framed as a task of text-conditioned outpainting, where the goal is to extend image content beyond a salient object's boundaries on a blank background.","Although popular diffusion models for text-guided inpainting can also be used for outpainting by mask inversion, they are trained to fill in missing parts of an image rather than to place an object into a scene.","Consequently, when used for background creation, inpainting models frequently extend the salient object's boundaries and thereby change the object's identity, which is a phenomenon we call \"object expansion.\"","This paper introduces a model for adapting inpainting diffusion models to the salient object outpainting task using Stable Diffusion and ControlNet architectures.","We present a series of qualitative and quantitative results across models and datasets, including a newly proposed metric to measure object expansion that does not require any human labeling.","Compared to Stable Diffusion 2.0 Inpainting, our proposed approach reduces object expansion by 3.6x on average with no degradation in standard visual metrics across multiple datasets."],"url":"http://arxiv.org/abs/2404.10157v1","category":"cs.CV"}
{"created":"2024-04-15 21:30:50","title":"Cross-Modal Self-Training: Aligning Images and Pointclouds to Learn Classification without Labels","abstract":"Large-scale vision 2D vision language models, such as CLIP can be aligned with a 3D encoder to learn generalizable (open-vocabulary) 3D vision models. However, current methods require supervised pre-training for such alignment, and the performance of such 3D zero-shot models remains sub-optimal for real-world adaptation. In this work, we propose an optimization framework: Cross-MoST: Cross-Modal Self-Training, to improve the label-free classification performance of a zero-shot 3D vision model by simply leveraging unlabeled 3D data and their accompanying 2D views. We propose a student-teacher framework to simultaneously process 2D views and 3D point clouds and generate joint pseudo labels to train a classifier and guide cross-model feature alignment. Thereby we demonstrate that 2D vision language models such as CLIP can be used to complement 3D representation learning to improve classification performance without the need for expensive class annotations. Using synthetic and real-world 3D datasets, we further demonstrate that Cross-MoST enables efficient cross-modal knowledge exchange resulting in both image and point cloud modalities learning from each other's rich representations.","sentences":["Large-scale vision 2D vision language models, such as CLIP can be aligned with a 3D encoder to learn generalizable (open-vocabulary) 3D vision models.","However, current methods require supervised pre-training for such alignment, and the performance of such 3D zero-shot models remains sub-optimal for real-world adaptation.","In this work, we propose an optimization framework: Cross-MoST: Cross-Modal Self-Training, to improve the label-free classification performance of a zero-shot 3D vision model by simply leveraging unlabeled 3D data and their accompanying 2D views.","We propose a student-teacher framework to simultaneously process 2D views and 3D point clouds and generate joint pseudo labels to train a classifier and guide cross-model feature alignment.","Thereby we demonstrate that 2D vision language models such as CLIP can be used to complement 3D representation learning to improve classification performance without the need for expensive class annotations.","Using synthetic and real-world 3D datasets, we further demonstrate that Cross-MoST enables efficient cross-modal knowledge exchange resulting in both image and point cloud modalities learning from each other's rich representations."],"url":"http://arxiv.org/abs/2404.10146v1","category":"cs.CV"}
{"created":"2024-04-15 20:03:58","title":"PRODIS - a speech database and a phoneme-based language model for the study of predictability effects in Polish","abstract":"We present a speech database and a phoneme-level language model of Polish. The database and model are designed for the analysis of prosodic and discourse factors and their impact on acoustic parameters in interaction with predictability effects. The database is also the first large, publicly available Polish speech corpus of excellent acoustic quality that can be used for phonetic analysis and training of multi-speaker speech technology systems. The speech in the database is processed in a pipeline that achieves a 90% degree of automation. It incorporates state-of-the-art, freely available tools enabling database expansion or adaptation to additional languages.","sentences":["We present a speech database and a phoneme-level language model of Polish.","The database and model are designed for the analysis of prosodic and discourse factors and their impact on acoustic parameters in interaction with predictability effects.","The database is also the first large, publicly available Polish speech corpus of excellent acoustic quality that can be used for phonetic analysis and training of multi-speaker speech technology systems.","The speech in the database is processed in a pipeline that achieves a 90% degree of automation.","It incorporates state-of-the-art, freely available tools enabling database expansion or adaptation to additional languages."],"url":"http://arxiv.org/abs/2404.10112v1","category":"cs.CL"}
{"created":"2024-04-15 18:26:07","title":"Nano-welding of quantum spin-$1/2$ chains at minimal dissipation","abstract":"We consider the optimal control of switching on a coupling term between two quantum many-body systems. Specifically, we (i) quantify the energetic cost of establishing a weak junction between two quantum spin-$1/2$ chains in finite time $\\tau$ and (ii) identify the energetically optimal protocol to realize it. For linear driving protocols, we find that for long times the excess (irreversible) work scales as $\\tau^{-\\eta}$, where $\\eta=1, 2$ or a nonuniversal number depending on the phase of the chains. Interestingly, increasing a $J_z$ anisotropy in the chains suppresses the excess work thus promoting quasi-adiabaticity. The general optimal control problem is solved, employing a Chebyshev ansatz. We find that the optimal control protocol is intimately sensitive to the chain phases.","sentences":["We consider the optimal control of switching on a coupling term between two quantum many-body systems.","Specifically, we (i) quantify the energetic cost of establishing a weak junction between two quantum spin-$1/2$ chains in finite time $\\tau$ and (ii) identify the energetically optimal protocol to realize it.","For linear driving protocols, we find that for long times the excess (irreversible) work scales as $\\tau^{-\\eta}$, where $\\eta=1, 2$ or a nonuniversal number depending on the phase of the chains.","Interestingly, increasing a $J_z$ anisotropy in the chains suppresses the excess work thus promoting quasi-adiabaticity.","The general optimal control problem is solved, employing a Chebyshev ansatz.","We find that the optimal control protocol is intimately sensitive to the chain phases."],"url":"http://arxiv.org/abs/2404.10074v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 18:11:30","title":"Enhanced Low-Complexity Receiver Design for Short Block Transmission Systems","abstract":"This paper presents a comprehensive analysis and performance enhancement of short block length channel detection incorporating training information. The current communication systems' short block length channel detection typically consists of least squares channel estimation followed by quasi-coherent detection. By investigating the receiver structure, specifically the estimator-correlator, we show that the non-coherent term, often disregarded in conventional detection metrics, results in significant losses in performance and sensitivity in typical operating regimes of 5G and 6G systems. A comparison with the fully non-coherent receiver in multi-antenna configurations reveals substantial losses in low spectral efficiency operating areas. Additionally, we demonstrate that by employing an adaptive DMRS-data power adjustment, it is possible to reduce the performance loss gap, which is amenable to a more sensitive quasi-coherent receiver. However, both of the aforementioned ML detection strategies can result in substantial computational complexity when processing long bit-length codes. We propose an approach to tackle this challenge by introducing the principle of block or segment coding using First-Order RM Codes, which is amenable to low-cost decoding through block-based fast Hadamard transforms. The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance. Additionally, by incorporating an adaptive DMRS-data power adjustment technique, we are able to bridge/reduce the performance gap with respect to the conventional maximum likelihood receiver and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads.","sentences":["This paper presents a comprehensive analysis and performance enhancement of short block length channel detection incorporating training information.","The current communication systems' short block length channel detection typically consists of least squares channel estimation followed by quasi-coherent detection.","By investigating the receiver structure, specifically the estimator-correlator, we show that the non-coherent term, often disregarded in conventional detection metrics, results in significant losses in performance and sensitivity in typical operating regimes of 5G and 6G systems.","A comparison with the fully non-coherent receiver in multi-antenna configurations reveals substantial losses in low spectral efficiency operating areas.","Additionally, we demonstrate that by employing an adaptive DMRS-data power adjustment, it is possible to reduce the performance loss gap, which is amenable to a more sensitive quasi-coherent receiver.","However, both of the aforementioned ML detection strategies can result in substantial computational complexity when processing long bit-length codes.","We propose an approach to tackle this challenge by introducing the principle of block or segment coding using First-Order RM Codes, which is amenable to low-cost decoding through block-based fast Hadamard transforms.","The Block-based FHT has demonstrated to be cost-efficient with regards to decoding time, as it evolves from quadric to quasi-linear complexity with a manageable decline in performance.","Additionally, by incorporating an adaptive DMRS-data power adjustment technique, we are able to bridge/reduce the performance gap with respect to the conventional maximum likelihood receiver and attain high sensitivity, leading to a good trade-off between performance and complexity to efficiently handle small payloads."],"url":"http://arxiv.org/abs/2404.10065v1","category":"cs.IT"}
{"created":"2024-04-15 17:55:43","title":"Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers","abstract":"Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models. Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks. However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets. This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model. To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks. In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously. Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets. We perform experiments on four unconditional image generation datasets. We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task.","sentences":["Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models.","Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks.","However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets.","This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model.","To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks.","In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset.","As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously.","Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets.","We perform experiments on four unconditional image generation datasets.","We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task."],"url":"http://arxiv.org/abs/2404.09976v1","category":"cs.CV"}
{"created":"2024-04-15 17:39:44","title":"Invariant Subspace Decomposition","abstract":"We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.","sentences":["We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time.","For this to be feasible, assumptions on how the conditional distribution changes over time are required.","Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible.","In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component.","As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively.","We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization.","Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure."],"url":"http://arxiv.org/abs/2404.09962v1","category":"stat.ML"}
{"created":"2024-04-15 17:27:00","title":"Classification Tree-based Active Learning: A Wrapper Approach","abstract":"Supervised machine learning often requires large training sets to train accurate models, yet obtaining large amounts of labeled data is not always feasible. Hence, it becomes crucial to explore active learning methods for reducing the size of training sets while maintaining high accuracy. The aim is to select the optimal subset of data for labeling from an initial unlabeled set, ensuring precise prediction of outcomes. However, conventional active learning approaches are comparable to classical random sampling. This paper proposes a wrapper active learning method for classification, organizing the sampling process into a tree structure, that improves state-of-the-art algorithms. A classification tree constructed on an initial set of labeled samples is considered to decompose the space into low-entropy regions. Input-space based criteria are used thereafter to sub-sample from these regions, the total number of points to be labeled being decomposed into each region. This adaptation proves to be a significant enhancement over existing active learning methods. Through experiments conducted on various benchmark data sets, the paper demonstrates the efficacy of the proposed framework by being effective in constructing accurate classification models, even when provided with a severely restricted labeled data set.","sentences":["Supervised machine learning often requires large training sets to train accurate models, yet obtaining large amounts of labeled data is not always feasible.","Hence, it becomes crucial to explore active learning methods for reducing the size of training sets while maintaining high accuracy.","The aim is to select the optimal subset of data for labeling from an initial unlabeled set, ensuring precise prediction of outcomes.","However, conventional active learning approaches are comparable to classical random sampling.","This paper proposes a wrapper active learning method for classification, organizing the sampling process into a tree structure, that improves state-of-the-art algorithms.","A classification tree constructed on an initial set of labeled samples is considered to decompose the space into low-entropy regions.","Input-space based criteria are used thereafter to sub-sample from these regions, the total number of points to be labeled being decomposed into each region.","This adaptation proves to be a significant enhancement over existing active learning methods.","Through experiments conducted on various benchmark data sets, the paper demonstrates the efficacy of the proposed framework by being effective in constructing accurate classification models, even when provided with a severely restricted labeled data set."],"url":"http://arxiv.org/abs/2404.09953v1","category":"cs.LG"}
{"created":"2024-04-15 17:24:57","title":"Unifying Global and Local Scene Entities Modelling for Precise Action Spotting","abstract":"Sports videos pose complex challenges, including cluttered backgrounds, camera angle changes, small action-representing objects, and imbalanced action class distribution. Existing methods for detecting actions in sports videos heavily rely on global features, utilizing a backbone network as a black box that encompasses the entire spatial frame. However, these approaches tend to overlook the nuances of the scene and struggle with detecting actions that occupy a small portion of the frame. In particular, they face difficulties when dealing with action classes involving small objects, such as balls or yellow/red cards in soccer, which only occupy a fraction of the screen space. To address these challenges, we introduce a novel approach that analyzes and models scene entities using an adaptive attention mechanism. Particularly, our model disentangles the scene content into the global environment feature and local relevant scene entities feature. To efficiently extract environmental features while considering temporal information with less computational cost, we propose the use of a 2D backbone network with a time-shift mechanism. To accurately capture relevant scene entities, we employ a Vision-Language model in conjunction with the adaptive attention mechanism. Our model has demonstrated outstanding performance, securing the 1st place in the SoccerNet-v2 Action Spotting, FineDiving, and FineGym challenge with a substantial performance improvement of 1.6, 2.0, and 1.3 points in avg-mAP compared to the runner-up methods. Furthermore, our approach offers interpretability capabilities in contrast to other deep learning models, which are often designed as black boxes. Our code and models are released at: https://github.com/Fsoft-AIC/unifying-global-local-feature.","sentences":["Sports videos pose complex challenges, including cluttered backgrounds, camera angle changes, small action-representing objects, and imbalanced action class distribution.","Existing methods for detecting actions in sports videos heavily rely on global features, utilizing a backbone network as a black box that encompasses the entire spatial frame.","However, these approaches tend to overlook the nuances of the scene and struggle with detecting actions that occupy a small portion of the frame.","In particular, they face difficulties when dealing with action classes involving small objects, such as balls or yellow/red cards in soccer, which only occupy a fraction of the screen space.","To address these challenges, we introduce a novel approach that analyzes and models scene entities using an adaptive attention mechanism.","Particularly, our model disentangles the scene content into the global environment feature and local relevant scene entities feature.","To efficiently extract environmental features while considering temporal information with less computational cost, we propose the use of a 2D backbone network with a time-shift mechanism.","To accurately capture relevant scene entities, we employ a Vision-Language model in conjunction with the adaptive attention mechanism.","Our model has demonstrated outstanding performance, securing the 1st place in the SoccerNet-v2 Action Spotting, FineDiving, and FineGym challenge with a substantial performance improvement of 1.6, 2.0, and 1.3 points in avg-mAP compared to the runner-up methods.","Furthermore, our approach offers interpretability capabilities in contrast to other deep learning models, which are often designed as black boxes.","Our code and models are released at: https://github.com/Fsoft-AIC/unifying-global-local-feature."],"url":"http://arxiv.org/abs/2404.09951v1","category":"cs.CV"}
{"created":"2024-04-15 16:29:56","title":"An adaptive hierarchical ensemble Kalman filter with reduced basis models","abstract":"The use of model order reduction techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community. Methods such as the multi-fidelity ensemble Kalman filter (MF-EnKF) and the multi-level ensemble Kalman filter (ML-EnKF) are recognized as state-of-the-art techniques. However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient. In our work, we investigate the use of {\\it{adaptive}} reduced basis techniques in which the approximation space is modified online based on the information that is extracted from a limited number of full order solutions and that is carried by the past models. This allows to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multi-fidelity and multi-level methods.","sentences":["The use of model order reduction techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community.","Methods such as the multi-fidelity ensemble Kalman filter (MF-EnKF) and the multi-level ensemble Kalman filter (ML-EnKF) are recognized as state-of-the-art techniques.","However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient.","In our work, we investigate the use of {\\it{adaptive}} reduced basis techniques in which the approximation space is modified online based on the information that is extracted from a limited number of full order solutions and that is carried by the past models.","This allows to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multi-fidelity and multi-level methods."],"url":"http://arxiv.org/abs/2404.09907v1","category":"math.NA"}
{"created":"2024-04-15 16:28:51","title":"Quality of Experience Oriented Cross-layer Optimization for Real-time XR Video Transmission","abstract":"Extended reality (XR) is one of the most important applications of beyond 5G and 6G networks. Real-time XR video transmission presents challenges in terms of data rate and delay. In particular, the frame-by-frame transmission mode of XR video makes real-time XR video very sensitive to dynamic network environments. To improve the users' quality of experience (QoE), we design a cross-layer transmission framework for real-time XR video. The proposed framework allows the simple information exchange between the base station (BS) and the XR server, which assists in adaptive bitrate and wireless resource scheduling. We utilize the cross-layer information to formulate the problem of maximizing user QoE by finding the optimal scheduling and bitrate adjustment strategies. To address the issue of mismatched time scales between two strategies, we decouple the original problem and solve them individually using a multi-agent-based approach. Specifically, we propose the multi-step Deep Q-network (MS-DQN) algorithm to obtain a frame-priority-based wireless resource scheduling strategy and then propose the Transformer-based Proximal Policy Optimization (TPPO) algorithm for video bitrate adaptation. The experimental results show that the TPPO+MS-DQN algorithm proposed in this study can improve the QoE by 3.6% to 37.8%. More specifically, the proposed MS-DQN algorithm enhances the transmission quality by 49.9%-80.2%.","sentences":["Extended reality (XR) is one of the most important applications of beyond 5G and 6G networks.","Real-time XR video transmission presents challenges in terms of data rate and delay.","In particular, the frame-by-frame transmission mode of XR video makes real-time XR video very sensitive to dynamic network environments.","To improve the users' quality of experience (QoE), we design a cross-layer transmission framework for real-time XR video.","The proposed framework allows the simple information exchange between the base station (BS) and the XR server, which assists in adaptive bitrate and wireless resource scheduling.","We utilize the cross-layer information to formulate the problem of maximizing user QoE by finding the optimal scheduling and bitrate adjustment strategies.","To address the issue of mismatched time scales between two strategies, we decouple the original problem and solve them individually using a multi-agent-based approach.","Specifically, we propose the multi-step Deep Q-network (MS-DQN) algorithm to obtain a frame-priority-based wireless resource scheduling strategy and then propose the Transformer-based Proximal Policy Optimization (TPPO) algorithm for video bitrate adaptation.","The experimental results show that the TPPO+MS-DQN algorithm proposed in this study can improve the QoE by 3.6% to 37.8%.","More specifically, the proposed MS-DQN algorithm enhances the transmission quality by 49.9%-80.2%."],"url":"http://arxiv.org/abs/2404.09905v1","category":"cs.NI"}
{"created":"2024-04-15 15:30:12","title":"Reimagining Self-Adaptation in the Age of Large Language Models","abstract":"Modern software systems are subjected to various types of uncertainties arising from context, environment, etc. To this end, self-adaptation techniques have been sought out as potential solutions. Although recent advances in self-adaptation through the use of ML techniques have demonstrated promising results, the capabilities are limited by constraints imposed by the ML techniques, such as the need for training samples, the ability to generalize, etc. Recent advancements in Generative AI (GenAI) open up new possibilities as it is trained on massive amounts of data, potentially enabling the interpretation of uncertainties and synthesis of adaptation strategies. In this context, this paper presents a vision for using GenAI, particularly Large Language Models (LLMs), to enhance the effectiveness and efficiency of architectural adaptation. Drawing parallels with human operators, we propose that LLMs can autonomously generate similar, context-sensitive adaptation strategies through its advanced natural language processing capabilities. This method allows software systems to understand their operational state and implement adaptations that align with their architectural requirements and environmental changes. By integrating LLMs into the self-adaptive system architecture, we facilitate nuanced decision-making that mirrors human-like adaptive reasoning. A case study with the SWIM exemplar system provides promising results, indicating that LLMs can potentially handle different adaptation scenarios. Our findings suggest that GenAI has significant potential to improve software systems' dynamic adaptability and resilience.","sentences":["Modern software systems are subjected to various types of uncertainties arising from context, environment, etc.","To this end, self-adaptation techniques have been sought out as potential solutions.","Although recent advances in self-adaptation through the use of ML techniques have demonstrated promising results, the capabilities are limited by constraints imposed by the ML techniques, such as the need for training samples, the ability to generalize, etc.","Recent advancements in Generative AI (GenAI) open up new possibilities as it is trained on massive amounts of data, potentially enabling the interpretation of uncertainties and synthesis of adaptation strategies.","In this context, this paper presents a vision for using GenAI, particularly Large Language Models (LLMs), to enhance the effectiveness and efficiency of architectural adaptation.","Drawing parallels with human operators, we propose that LLMs can autonomously generate similar, context-sensitive adaptation strategies through its advanced natural language processing capabilities.","This method allows software systems to understand their operational state and implement adaptations that align with their architectural requirements and environmental changes.","By integrating LLMs into the self-adaptive system architecture, we facilitate nuanced decision-making that mirrors human-like adaptive reasoning.","A case study with the SWIM exemplar system provides promising results, indicating that LLMs can potentially handle different adaptation scenarios.","Our findings suggest that GenAI has significant potential to improve software systems' dynamic adaptability and resilience."],"url":"http://arxiv.org/abs/2404.09866v1","category":"cs.SE"}
{"created":"2024-04-15 15:16:28","title":"Thermodynamic constraints on kinetic perturbations of homogeneous driven diffusions","abstract":"We analyze the static response to kinetic perturbations of nonequilibrium steady states that can be modeled as diffusions. We demonstrate that kinetic response is purely a nonequilibirum effect, measuring the degree to which the Fluctuation-Dissipation Theorem is violated out of equilibrium. For driven diffusions in a flat landscape, we further demonstrate that such response is constrained by the strength of the nonequilibrium driving via quantitative inequalities.","sentences":["We analyze the static response to kinetic perturbations of nonequilibrium steady states that can be modeled as diffusions.","We demonstrate that kinetic response is purely a nonequilibirum effect, measuring the degree to which the Fluctuation-Dissipation Theorem is violated out of equilibrium.","For driven diffusions in a flat landscape, we further demonstrate that such response is constrained by the strength of the nonequilibrium driving via quantitative inequalities."],"url":"http://arxiv.org/abs/2404.09860v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-15 15:12:41","title":"Matching Hadronization and Perturbative Evolution: The Cluster Model in Light of Infrared Shower Cutoff Dependence","abstract":"In the context of Monte Carlo (MC) generators with parton showers that have next-to-leading-logarithmic (NLL) precision, the cutoff $Q_0$ terminating the shower evolution should be viewed as an infrared factorization scale so that parameters or non-perturbative effects of the MC generator may have a field theoretic interpretation with a controllable scheme dependence. This implies that the generator's parton level should be carefully defined within QCD perturbation theory with subleading order precision. Furthermore, it entails that the shower cut $Q_0$ is not treated as one of the generator's tuning parameters, but that the tuning can be carried out reliably for a range of $Q_0$ values and that the hadron level description is $Q_0$-invariant. This in turn imposes non-trival constraints on the behavior of the generator's hadronization model, so that its parameters can adapt accordingly when the $Q_0$ value is changed. We investigate these features using the angular ordered parton shower and the cluster hadronization model implemented in the Herwig~7.2 MC generator focusing in particular on the $e^+e^-$ 2-jettiness distribution, where the shower is known to be NLL precise and where QCD factorization imposes stringent constraints on the hadronization corrections. We show that the Herwig default cluster hadronization model does not exhibit these features or consistency with QCD factorization with a satisfying precision. We design a modification of the cluster hadronization model, where some dynamical parton shower aspects are added that are missing in the default model. For this novel dynamical cluster hadronization model these features and consistency with QCD factorization are realized much more accurately.","sentences":["In the context of Monte Carlo (MC) generators with parton showers that have next-to-leading-logarithmic (NLL) precision, the cutoff $Q_0$ terminating the shower evolution should be viewed as an infrared factorization scale so that parameters or non-perturbative effects of the MC generator may have a field theoretic interpretation with a controllable scheme dependence.","This implies that the generator's parton level should be carefully defined within QCD perturbation theory with subleading order precision.","Furthermore, it entails that the shower cut $Q_0$ is not treated as one of the generator's tuning parameters, but that the tuning can be carried out reliably for a range of $Q_0$ values and that the hadron level description is $Q_0$-invariant.","This in turn imposes non-trival constraints on the behavior of the generator's hadronization model, so that its parameters can adapt accordingly when the $Q_0$ value is changed.","We investigate these features using the angular ordered parton shower and the cluster hadronization model implemented in the Herwig~7.2 MC generator focusing in particular on the $e^+e^-$ 2-jettiness distribution, where the shower is known to be NLL precise and where QCD factorization imposes stringent constraints on the hadronization corrections.","We show that the Herwig default cluster hadronization model does not exhibit these features or consistency with QCD factorization with a satisfying precision.","We design a modification of the cluster hadronization model, where some dynamical parton shower aspects are added that are missing in the default model.","For this novel dynamical cluster hadronization model these features and consistency with QCD factorization are realized much more accurately."],"url":"http://arxiv.org/abs/2404.09856v1","category":"hep-ph"}
{"created":"2024-04-15 14:52:02","title":"STMixer: A One-Stage Sparse Action Detector","abstract":"Traditional video action detectors typically adopt the two-stage pipeline, where a person detector is first employed to generate actor boxes and then 3D RoIAlign is used to extract actor-specific features for classification. This detection paradigm requires multi-stage training and inference, and the feature sampling is constrained inside the box, failing to effectively leverage richer context information outside. Recently, a few query-based action detectors have been proposed to predict action instances in an end-to-end manner. However, they still lack adaptability in feature sampling and decoding, thus suffering from the issues of inferior performance or slower convergence. In this paper, we propose two core designs for a more flexible one-stage sparse action detector. First, we present a query-based adaptive feature sampling module, which endows the detector with the flexibility of mining a group of discriminative features from the entire spatio-temporal domain. Second, we devise a decoupled feature mixing module, which dynamically attends to and mixes video features along the spatial and temporal dimensions respectively for better feature decoding. Based on these designs, we instantiate two detection pipelines, that is, STMixer-K for keyframe action detection and STMixer-T for action tubelet detection. Without bells and whistles, our STMixer detectors obtain state-of-the-art results on five challenging spatio-temporal action detection benchmarks for keyframe action detection or action tube detection.","sentences":["Traditional video action detectors typically adopt the two-stage pipeline, where a person detector is first employed to generate actor boxes and then 3D RoIAlign is used to extract actor-specific features for classification.","This detection paradigm requires multi-stage training and inference, and the feature sampling is constrained inside the box, failing to effectively leverage richer context information outside.","Recently, a few query-based action detectors have been proposed to predict action instances in an end-to-end manner.","However, they still lack adaptability in feature sampling and decoding, thus suffering from the issues of inferior performance or slower convergence.","In this paper, we propose two core designs for a more flexible one-stage sparse action detector.","First, we present a query-based adaptive feature sampling module, which endows the detector with the flexibility of mining a group of discriminative features from the entire spatio-temporal domain.","Second, we devise a decoupled feature mixing module, which dynamically attends to and mixes video features along the spatial and temporal dimensions respectively for better feature decoding.","Based on these designs, we instantiate two detection pipelines, that is, STMixer-K for keyframe action detection and STMixer-T for action tubelet detection.","Without bells and whistles, our STMixer detectors obtain state-of-the-art results on five challenging spatio-temporal action detection benchmarks for keyframe action detection or action tube detection."],"url":"http://arxiv.org/abs/2404.09842v1","category":"cs.CV"}
{"created":"2024-04-15 14:14:05","title":"FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity","abstract":"The interest in federated learning has surged in recent research due to its unique ability to train a global model using privacy-secured information held locally on each client. This paper pays particular attention to the issue of client-side model heterogeneity, a pervasive challenge in the practical implementation of FL that escalates its complexity. Assuming a scenario where each client possesses varied memory storage, processing capabilities and network bandwidth - a phenomenon referred to as system heterogeneity - there is a pressing need to customize a unique model for each client. In response to this, we present an effective and adaptable federated framework FedP3, representing Federated Personalized and Privacy-friendly network Pruning, tailored for model heterogeneity scenarios. Our proposed methodology can incorporate and adapt well-established techniques to its specific instances. We offer a theoretical interpretation of FedP3 and its locally differential-private variant, DP-FedP3, and theoretically validate their efficiencies.","sentences":["The interest in federated learning has surged in recent research due to its unique ability to train a global model using privacy-secured information held locally on each client.","This paper pays particular attention to the issue of client-side model heterogeneity, a pervasive challenge in the practical implementation of FL that escalates its complexity.","Assuming a scenario where each client possesses varied memory storage, processing capabilities and network bandwidth - a phenomenon referred to as system heterogeneity - there is a pressing need to customize a unique model for each client.","In response to this, we present an effective and adaptable federated framework FedP3, representing Federated Personalized and Privacy-friendly network Pruning, tailored for model heterogeneity scenarios.","Our proposed methodology can incorporate and adapt well-established techniques to its specific instances.","We offer a theoretical interpretation of FedP3 and its locally differential-private variant, DP-FedP3, and theoretically validate their efficiencies."],"url":"http://arxiv.org/abs/2404.09816v1","category":"cs.LG"}
{"created":"2024-04-15 13:58:22","title":"The Performance of Sequential Deep Learning Models in Detecting Phishing Websites Using Contextual Features of URLs","abstract":"Cyber attacks continue to pose significant threats to individuals and organizations, stealing sensitive data such as personally identifiable information, financial information, and login credentials. Hence, detecting malicious websites before they cause any harm is critical to preventing fraud and monetary loss. To address the increasing number of phishing attacks, protective mechanisms must be highly responsive, adaptive, and scalable. Fortunately, advances in the field of machine learning, coupled with access to vast amounts of data, have led to the adoption of various deep learning models for timely detection of these cyber crimes. This study focuses on the detection of phishing websites using deep learning models such as Multi-Head Attention, Temporal Convolutional Network (TCN), BI-LSTM, and LSTM where URLs of the phishing websites are treated as a sequence. The results demonstrate that Multi-Head Attention and BI-LSTM model outperform some other deep learning-based algorithms such as TCN and LSTM in producing better precision, recall, and F1-scores.","sentences":["Cyber attacks continue to pose significant threats to individuals and organizations, stealing sensitive data such as personally identifiable information, financial information, and login credentials.","Hence, detecting malicious websites before they cause any harm is critical to preventing fraud and monetary loss.","To address the increasing number of phishing attacks, protective mechanisms must be highly responsive, adaptive, and scalable.","Fortunately, advances in the field of machine learning, coupled with access to vast amounts of data, have led to the adoption of various deep learning models for timely detection of these cyber crimes.","This study focuses on the detection of phishing websites using deep learning models such as Multi-Head Attention, Temporal Convolutional Network (TCN), BI-LSTM, and LSTM where URLs of the phishing websites are treated as a sequence.","The results demonstrate that Multi-Head Attention and BI-LSTM model outperform some other deep learning-based algorithms such as TCN and LSTM in producing better precision, recall, and F1-scores."],"url":"http://arxiv.org/abs/2404.09802v1","category":"cs.CR"}
{"created":"2024-04-16 16:53:15","title":"Dependence of spicule properties on magnetic field - results from Magnetohydrodynamics simulations","abstract":"Solar spicules are plasma jets observed in the interface region between the visible solar surface and the corona. At any given time, there are millions of spicules present all over the Sun. While various models attempt to elucidate their origin and characteristics, here, we consider the one driven by the magneto-convection undulations. The radiative magneto-hydrodynamical (rMHD) equations are solved using PENCIL CODE with a spatial resolution of 16 km using various magnetic field strengths. The obtained rMHD simulation data are investigated to unveil the various trends in spicular properties as function of the applied magnetic fields. The important outcome of this study is the finding of a consistent reduction in both the number density and the maximum height reached by spicules as magnetic field strength increases. We also use parabolic fitting on time-distance curves of spicules that are taller than $75^\\mathrm{th}$ percentile in the distribution, in order to find a relation between deceleration of the spicule tip and the magnetic field strength. Our results offer insights into the response of solar spicules to magnetic field strength.","sentences":["Solar spicules are plasma jets observed in the interface region between the visible solar surface and the corona.","At any given time, there are millions of spicules present all over the Sun.","While various models attempt to elucidate their origin and characteristics, here, we consider the one driven by the magneto-convection undulations.","The radiative magneto-hydrodynamical (rMHD) equations are solved using PENCIL CODE with a spatial resolution of 16 km using various magnetic field strengths.","The obtained rMHD simulation data are investigated to unveil the various trends in spicular properties as function of the applied magnetic fields.","The important outcome of this study is the finding of a consistent reduction in both the number density and the maximum height reached by spicules as magnetic field strength increases.","We also use parabolic fitting on time-distance curves of spicules that are taller than $75^\\mathrm{th}$ percentile in the distribution, in order to find a relation between deceleration of the spicule tip and the magnetic field strength.","Our results offer insights into the response of solar spicules to magnetic field strength."],"url":"http://arxiv.org/abs/2404.10720v1","category":"astro-ph.SR"}
{"created":"2024-04-16 15:22:30","title":"Space Regularity of Evolution Equations Driven by Rough Paths","abstract":"In this paper, we consider the linear evolution equation $dy(t)=Ay(t)dt+Gy(t)dx(t)$, where $A$ is a closed operator, associated to a semigroup, with good smoothing effects in a Banach space $E$, $x$ is a nonsmooth path, which is $\\eta$-H\\\"older continuous for some $\\eta\\in (1/3,1/2]$, and $G$ is a non-smoothing linear operator on $E$. We prove that the Cauchy problem associated with the previous equation admits a unique mild solution and we also show that the solution increases the regularity of the initial datum as soon as time evolves. Then, we show that the mild solution is also an integral solution and this allows us to prove a It\\^o formula.","sentences":["In this paper, we consider the linear evolution equation $dy(t)=Ay(t)dt+Gy(t)dx(t)$, where $A$ is a closed operator, associated to a semigroup, with good smoothing effects in a Banach space $E$, $x$ is a nonsmooth path, which is $\\eta$-H\\\"older continuous for some $\\eta\\in (1/3,1/2]$, and $G$ is a non-smoothing linear operator on $E$. We prove that the Cauchy problem associated with the previous equation admits a unique mild solution and we also show that the solution increases the regularity of the initial datum as soon as time evolves.","Then, we show that the mild solution is also an integral solution and this allows us to prove a It\\^o formula."],"url":"http://arxiv.org/abs/2404.10650v1","category":"math.AP"}
{"created":"2024-04-16 14:43:33","title":"PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction","abstract":"We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects from images using interpretable shape programs. In comparison to traditional CAD model retrieval methods, the use of shape programs for 3D reconstruction allows for reasoning about the semantic properties of reconstructed objects, editing, low memory footprint, etc. However, the utilization of shape programs for 3D scene understanding has been largely neglected in past works. As our main contribution, we enable gradient-based optimization by introducing a module that translates shape programs designed in Blender, for example, into efficient PyTorch code. We also provide a method that relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search (MCTS) to jointly optimize discrete and continuous parameters of shape programs and reconstruct 3D objects for input scenes. In our experiments, we apply our algorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our results against CAD model retrieval-based reconstructions. Our experiments indicate that our reconstructions match well the input scenes while enabling semantic reasoning about reconstructed objects.","sentences":["We propose PyTorchGeoNodes, a differentiable module for reconstructing 3D objects from images using interpretable shape programs.","In comparison to traditional CAD model retrieval methods, the use of shape programs for 3D reconstruction allows for reasoning about the semantic properties of reconstructed objects, editing, low memory footprint, etc.","However, the utilization of shape programs for 3D scene understanding has been largely neglected in past works.","As our main contribution, we enable gradient-based optimization by introducing a module that translates shape programs designed in Blender, for example, into efficient PyTorch code.","We also provide a method that relies on PyTorchGeoNodes and is inspired by Monte Carlo Tree Search (MCTS) to jointly optimize discrete and continuous parameters of shape programs and reconstruct 3D objects for input scenes.","In our experiments, we apply our algorithm to reconstruct 3D objects in the ScanNet dataset and evaluate our results against CAD model retrieval-based reconstructions.","Our experiments indicate that our reconstructions match well the input scenes while enabling semantic reasoning about reconstructed objects."],"url":"http://arxiv.org/abs/2404.10620v1","category":"cs.CV"}
{"created":"2024-04-16 14:36:14","title":"Transitive Lie algebroids and \\textbf{Q}-manifolds","abstract":"We introduce the notion of \\textbf{Q}-principal bundle, which is the appropriate version of principal fibre bundles in the setting of R. Barre's \\textbf{Q}-manifolds. As an application, we prove that every transitive Lie algebroid arises from the Atiyah sequence of a certain \\textbf{Q}-principal bundle, and we give the interpretation of this result in terms of groupoids.","sentences":["We introduce the notion of \\textbf{Q}-principal bundle, which is the appropriate version of principal fibre bundles in the setting of R. Barre's \\textbf{Q}-manifolds.","As an application, we prove that every transitive Lie algebroid arises from the Atiyah sequence of a certain \\textbf{Q}-principal bundle, and we give the interpretation of this result in terms of groupoids."],"url":"http://arxiv.org/abs/2404.10607v1","category":"math.DG"}
{"created":"2024-04-16 14:11:26","title":"Prediction of Nuclear Charge Density Distribution with Feedback Neural Network","abstract":"The nuclear charge density distribution plays an important role in nuclear physics and atomic physics. As one of the most frequently used models to obtain charge density distribution, the two-parameter fermi (2pF) model has been widely applied in both nuclear physics and atomic physics. Currently, the feedforward neural network has been employed to study the available 2pF model parameters for 86 nuclei, and it is found that by introducing A^{1/3} into the input parameter of the neural network, the accuracy and precision of the parameter learning effect are improved. Furthermore, the average result of multiple predictions is more reliable than the best result of a single prediction, and there is no significant difference between the average result of density value and of parameter value for the average charge density distribution. In addition, 2pF parameters of 284 (near) stable nuclei are also predicted in this work, which provides a reference for the experiment.","sentences":["The nuclear charge density distribution plays an important role in nuclear physics and atomic physics.","As one of the most frequently used models to obtain charge density distribution, the two-parameter fermi (2pF) model has been widely applied in both nuclear physics and atomic physics.","Currently, the feedforward neural network has been employed to study the available 2pF model parameters for 86 nuclei, and it is found that by introducing A^{1/3} into the input parameter of the neural network, the accuracy and precision of the parameter learning effect are improved.","Furthermore, the average result of multiple predictions is more reliable than the best result of a single prediction, and there is no significant difference between the average result of density value and of parameter value for the average charge density distribution.","In addition, 2pF parameters of 284 (near) stable nuclei are also predicted in this work, which provides a reference for the experiment."],"url":"http://arxiv.org/abs/2404.10585v1","category":"nucl-th"}
{"created":"2024-04-16 13:18:02","title":"Classification of Prostate Cancer in 3D Magnetic Resonance Imaging Data based on Convolutional Neural Networks","abstract":"Prostate cancer is a commonly diagnosed cancerous disease among men world-wide. Even with modern technology such as multi-parametric magnetic resonance tomography and guided biopsies, the process for diagnosing prostate cancer remains time consuming and requires highly trained professionals. In this paper, different convolutional neural networks (CNN) are evaluated on their abilities to reliably classify whether an MRI sequence contains malignant lesions. Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image data are trained and evaluated. The models are trained using different data augmentation techniques, learning rates, and optimizers. The data is taken from a private dataset, provided by Cantonal Hospital Aarau. The best result was achieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC ROC score of 0.6214.","sentences":["Prostate cancer is a commonly diagnosed cancerous disease among men world-wide.","Even with modern technology such as multi-parametric magnetic resonance tomography and guided biopsies, the process for diagnosing prostate cancer remains time consuming and requires highly trained professionals.","In this paper, different convolutional neural networks (CNN) are evaluated on their abilities to reliably classify whether an MRI sequence contains malignant lesions.","Implementations of a ResNet, a ConvNet and a ConvNeXt for 3D image data are trained and evaluated.","The models are trained using different data augmentation techniques, learning rates, and optimizers.","The data is taken from a private dataset, provided by Cantonal Hospital Aarau.","The best result was achieved by a ResNet3D, yielding an average precision score of 0.4583 and AUC ROC score of 0.6214."],"url":"http://arxiv.org/abs/2404.10548v1","category":"eess.IV"}
{"created":"2024-04-16 12:48:08","title":"Finiteness of the number of irreducible $\u03bb$-quiddities over a finite commutative and unitary ring","abstract":"$\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes. The study of these solutions involves in particular the use of a notion of irreducibility. The main objective of this text is to show that there is a finite number of irreducible $\\lambda$-quiddities over a finite unitary commutative ring and to obtain in this case an upper bound of the maximal size of them.","sentences":["$\\lambda$-quiddities of size $n$ are $n$-tuples of elements of a fixed set, solutions of a matrix equation appearing in the study of Coxeter's friezes.","The study of these solutions involves in particular the use of a notion of irreducibility.","The main objective of this text is to show that there is a finite number of irreducible $\\lambda$-quiddities over a finite unitary commutative ring and to obtain in this case an upper bound of the maximal size of them."],"url":"http://arxiv.org/abs/2404.10521v1","category":"math.CO"}
{"created":"2024-04-16 12:43:46","title":"Optical absorption in tilted geometries as an indirect measure of longitudinal plasma waves in layered cuprates","abstract":"Electromagnetic waves propagating in a layered superconductor with arbitrary momentum with respect to the main crystallographic directions display an unavoidable mixing between longitudinal and transverse degrees of freedom. Here we show that this basic physical mechanism explains the emergence of a well-defined absorption peak in the in-plane optical conductivity for light propagating at small tilting angles with respect to the stacking direction in layered cuprates. More specifically, we show that this peak, often interpreted as a spurious leakage of the $c$-axis Josephson plasmon, is instead a signature of the true longitudinal plasma mode occurring at larger momenta. By combining a classical approach based on Maxwell's equations with a full quantum derivation of the plasma modes based on the modelling of the superconducting phase degrees of freedom, we provide an analytical expression for the absorption peak as a function of the tilting angle and light polarization. We suggest that an all-optical measurement in tilted geometry can be used as an alternative way to access plasma-wave dispersion, usually measured by means of large-momenta scattering techniques like RIXS or EELS.","sentences":["Electromagnetic waves propagating in a layered superconductor with arbitrary momentum with respect to the main crystallographic directions display an unavoidable mixing between longitudinal and transverse degrees of freedom.","Here we show that this basic physical mechanism explains the emergence of a well-defined absorption peak in the in-plane optical conductivity for light propagating at small tilting angles with respect to the stacking direction in layered cuprates.","More specifically, we show that this peak, often interpreted as a spurious leakage of the $c$-axis Josephson plasmon, is instead a signature of the true longitudinal plasma mode occurring at larger momenta.","By combining a classical approach based on Maxwell's equations with a full quantum derivation of the plasma modes based on the modelling of the superconducting phase degrees of freedom, we provide an analytical expression for the absorption peak as a function of the tilting angle and light polarization.","We suggest that an all-optical measurement in tilted geometry can be used as an alternative way to access plasma-wave dispersion, usually measured by means of large-momenta scattering techniques like RIXS or EELS."],"url":"http://arxiv.org/abs/2404.10519v1","category":"cond-mat.supr-con"}
{"created":"2024-04-16 12:40:29","title":"Magnetic Evolution of an Active Region Producing Successive Flares and Confined Eruptions","abstract":"We analyze the magnetic evolution of solar active region (AR) NOAA 11476 that, between 9 and 10 May 2012, produced a series of surge-type eruptions accompanied by GOES X-ray class M flares. Using force-free models of the AR coronal structure and observations in several wavelengths, in previous works we studied the detailed evolution of those eruptions, relating them to the characteristic magnetic topology of the AR and reconstructing the involved reconnection scheme. We found that the eruptions were due to the ejection of minifilaments, which were recurrently ejected and reformed at the polarity inversion line of a bipole that emerged in the middle of the positive main AR magnetic polarity. The bipole was observed to rotate for several tens of hours before the events. In this article we analyze, for the full AR and the rotating bipole, the evolution of a series of magnetic parameters computed using the Helioseismic and Magnetic Imager (HMI) vector magnetograms. We combine this analysis with estimations of the injection of magnetic energy and helicity obtained using the Differential Affine Velocity Estimator for Vector Magnetograms (DAVE4VM) method that determines, from vector magnetograms, the affine velocity field constrained by the induction equation. From our results, we conclude that the bipole rotation was the main driver that provided the magnetic energy and helicity involved in the minifilament destabilizations and ejections. The results also suggest that the observed rotation is probably due to the emergence of a kinked magnetic flux rope with negative writhe helicity.","sentences":["We analyze the magnetic evolution of solar active region (AR) NOAA 11476 that, between 9 and 10 May 2012, produced a series of surge-type eruptions accompanied by GOES X-ray class M flares.","Using force-free models of the AR coronal structure and observations in several wavelengths, in previous works we studied the detailed evolution of those eruptions, relating them to the characteristic magnetic topology of the AR and reconstructing the involved reconnection scheme.","We found that the eruptions were due to the ejection of minifilaments, which were recurrently ejected and reformed at the polarity inversion line of a bipole that emerged in the middle of the positive main AR magnetic polarity.","The bipole was observed to rotate for several tens of hours before the events.","In this article we analyze, for the full AR and the rotating bipole, the evolution of a series of magnetic parameters computed using the Helioseismic and Magnetic Imager (HMI) vector magnetograms.","We combine this analysis with estimations of the injection of magnetic energy and helicity obtained using the Differential Affine Velocity Estimator for Vector Magnetograms (DAVE4VM) method that determines, from vector magnetograms, the affine velocity field constrained by the induction equation.","From our results, we conclude that the bipole rotation was the main driver that provided the magnetic energy and helicity involved in the minifilament destabilizations and ejections.","The results also suggest that the observed rotation is probably due to the emergence of a kinked magnetic flux rope with negative writhe helicity."],"url":"http://arxiv.org/abs/2404.10517v1","category":"astro-ph.SR"}
{"created":"2024-04-16 12:38:03","title":"An Enhanced Differential Grouping Method for Large-Scale Overlapping Problems","abstract":"Large-scale overlapping problems are prevalent in practical engineering applications, and the optimization challenge is significantly amplified due to the existence of shared variables. Decomposition-based cooperative coevolution (CC) algorithms have demonstrated promising performance in addressing large-scale overlapping problems. However, current CC frameworks designed for overlapping problems rely on grouping methods for the identification of overlapping problem structures and the current grouping methods for large-scale overlapping problems fail to consider both accuracy and efficiency simultaneously. In this article, we propose a two-stage enhanced grouping method for large-scale overlapping problems, called OEDG, which achieves accurate grouping while significantly reducing computational resource consumption. In the first stage, OEDG employs a grouping method based on the finite differences principle to identify all subcomponents and shared variables. In the second stage, we propose two grouping refinement methods, called subcomponent union detection (SUD) and subcomponent detection (SD), to enhance and refine the grouping results. SUD examines the information of the subcomponents and shared variables obtained in the previous stage, and SD corrects inaccurate grouping results. To better verify the performance of the proposed OEDG, we propose a series of novel benchmarks that consider various properties of large-scale overlapping problems, including the topology structure, overlapping degree, and separability. Extensive experimental results demonstrate that OEDG is capable of accurately grouping different types of large-scale overlapping problems while consuming fewer computational resources. Finally, we empirically verify that the proposed OEDG can effectively improve the optimization performance of diverse large-scale overlapping problems.","sentences":["Large-scale overlapping problems are prevalent in practical engineering applications, and the optimization challenge is significantly amplified due to the existence of shared variables.","Decomposition-based cooperative coevolution (CC) algorithms have demonstrated promising performance in addressing large-scale overlapping problems.","However, current CC frameworks designed for overlapping problems rely on grouping methods for the identification of overlapping problem structures and the current grouping methods for large-scale overlapping problems fail to consider both accuracy and efficiency simultaneously.","In this article, we propose a two-stage enhanced grouping method for large-scale overlapping problems, called OEDG, which achieves accurate grouping while significantly reducing computational resource consumption.","In the first stage, OEDG employs a grouping method based on the finite differences principle to identify all subcomponents and shared variables.","In the second stage, we propose two grouping refinement methods, called subcomponent union detection (SUD) and subcomponent detection (SD), to enhance and refine the grouping results.","SUD examines the information of the subcomponents and shared variables obtained in the previous stage, and SD corrects inaccurate grouping results.","To better verify the performance of the proposed OEDG, we propose a series of novel benchmarks that consider various properties of large-scale overlapping problems, including the topology structure, overlapping degree, and separability.","Extensive experimental results demonstrate that OEDG is capable of accurately grouping different types of large-scale overlapping problems while consuming fewer computational resources.","Finally, we empirically verify that the proposed OEDG can effectively improve the optimization performance of diverse large-scale overlapping problems."],"url":"http://arxiv.org/abs/2404.10515v1","category":"cs.NE"}
{"created":"2024-04-16 11:39:40","title":"In-depth analysis of solar models with high-metallicity abundances and updated opacity tables","abstract":"Due to the high quality constraints available for the Sun, we can carry out combined analyses using neutrino, spectroscopic and helioseismic observations. Such studies lay the ground for future improvements of key physical components of solar and stellar models, such as the equation of state, radiative opacities or prescriptions for macroscopic transport processes of chemicals which are then used to study other stars in the Universe. We study the existing degeneracies in solar models using the recent high-metallicity spectroscopic abundances by comparing them to helioseismic and neutrino data and discuss how their properties are impacted by changes in various physical ingredients. We carry out a detailed study of solar models computed with a high-metallicity composition from the literature based on averaged-3D models that was claimed to solve the solar problem. The properties of the solar models are significantly affected by using the recent OPLIB opacities and the inclusion of macroscopic transport. The properties of the standard solar models computed using the OPAL opacities are similar to those using the OP opacities. We show that a modifying the temperature gradient just below the base of the convective zone is required to erase the discrepancies in solar models, particularly in the presence of macroscopic mixing. This can be simulated by a local increase of opacity of a few percent. We conclude that the existing degeneracies and issues in solar modelling are not erased by an increase in the solar metallicity in contradiction to was suggested in recent papers. Therefore, standard solar models cannot be used as an argument for a high metallicity composition. While further work is required to improve solar models, we note that direct helioseismic inversions indicate a low metallicity in the convective envelope, in agreement with spectroscopic analyses based on full 3D models.","sentences":["Due to the high quality constraints available for the Sun, we can carry out combined analyses using neutrino, spectroscopic and helioseismic observations.","Such studies lay the ground for future improvements of key physical components of solar and stellar models, such as the equation of state, radiative opacities or prescriptions for macroscopic transport processes of chemicals which are then used to study other stars in the Universe.","We study the existing degeneracies in solar models using the recent high-metallicity spectroscopic abundances by comparing them to helioseismic and neutrino data and discuss how their properties are impacted by changes in various physical ingredients.","We carry out a detailed study of solar models computed with a high-metallicity composition from the literature based on averaged-3D models that was claimed to solve the solar problem.","The properties of the solar models are significantly affected by using the recent OPLIB opacities and the inclusion of macroscopic transport.","The properties of the standard solar models computed using the OPAL opacities are similar to those using the OP opacities.","We show that a modifying the temperature gradient just below the base of the convective zone is required to erase the discrepancies in solar models, particularly in the presence of macroscopic mixing.","This can be simulated by a local increase of opacity of a few percent.","We conclude that the existing degeneracies and issues in solar modelling are not erased by an increase in the solar metallicity in contradiction to was suggested in recent papers.","Therefore, standard solar models cannot be used as an argument for a high metallicity composition.","While further work is required to improve solar models, we note that direct helioseismic inversions indicate a low metallicity in the convective envelope, in agreement with spectroscopic analyses based on full 3D models."],"url":"http://arxiv.org/abs/2404.10478v1","category":"astro-ph.SR"}
{"created":"2024-04-16 11:27:46","title":"Friction and heat transfer in forced air convection with variable physical properties","abstract":"We establish a theoretical framework for predicting friction and heat transfer coefficients in variable-properties forced air convection. Drawing from concepts in high-speed wall turbulence, which also involves significant temperature, viscosity, and density variations, we utilize the mean momentum balance and mean thermal balance equations to develop integral transformations that account for the impact of variable fluid properties. These transformations are then applied inversely to predict the friction and heat transfer coefficients, leveraging the universality of passive scalars transport theory. Our proposed approach is validated using a comprehensive dataset from direct numerical simulations, covering both heating and cooling conditions up to a friction Reynolds number of approximately $\\Rey_\\tau\\approx 3200$. The predicted friction and heat transfer coefficients closely match DNS data with an accuracy margin of 1-2\\%, representing a significant improvement over the current state of the art.","sentences":["We establish a theoretical framework for predicting friction and heat transfer coefficients in variable-properties forced air convection.","Drawing from concepts in high-speed wall turbulence, which also involves significant temperature, viscosity, and density variations, we utilize the mean momentum balance and mean thermal balance equations to develop integral transformations that account for the impact of variable fluid properties.","These transformations are then applied inversely to predict the friction and heat transfer coefficients, leveraging the universality of passive scalars transport theory.","Our proposed approach is validated using a comprehensive dataset from direct numerical simulations, covering both heating and cooling conditions up to a friction Reynolds number of approximately $\\Rey_\\tau\\approx 3200$.","The predicted friction and heat transfer coefficients closely match DNS data with an accuracy margin of 1-2\\%, representing a significant improvement over the current state of the art."],"url":"http://arxiv.org/abs/2404.10473v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 11:25:00","title":"Machine Learning Based Optimization Workflow for Tuning Numerical Settings of Differential Equation Solvers for Boundary Value Problems","abstract":"Several numerical differential equation solvers have been employed effectively over the years as an alternative to analytical solvers to quickly and conveniently solve differential equations. One category of these is boundary value solvers, which are used to solve real-world problems formulated as differential equations with boundary conditions. These solvers require certain numerical settings to solve the differential equations that affect their solvability and performance. A systematic fine-tuning of these settings is required to obtain the desired solution and performance. Currently, these settings are either selected by trial and error or require domain expertise. In this paper, we propose a machine learning-based optimization workflow for fine-tuning the numerical settings to reduce the time and domain expertise required in the process. In the evaluation section, we discuss the scalability, stability, and reliability of the proposed workflow. We demonstrate our workflow on a numerical boundary value problem solver.","sentences":["Several numerical differential equation solvers have been employed effectively over the years as an alternative to analytical solvers to quickly and conveniently solve differential equations.","One category of these is boundary value solvers, which are used to solve real-world problems formulated as differential equations with boundary conditions.","These solvers require certain numerical settings to solve the differential equations that affect their solvability and performance.","A systematic fine-tuning of these settings is required to obtain the desired solution and performance.","Currently, these settings are either selected by trial and error or require domain expertise.","In this paper, we propose a machine learning-based optimization workflow for fine-tuning the numerical settings to reduce the time and domain expertise required in the process.","In the evaluation section, we discuss the scalability, stability, and reliability of the proposed workflow.","We demonstrate our workflow on a numerical boundary value problem solver."],"url":"http://arxiv.org/abs/2404.10472v1","category":"math.NA"}
{"created":"2024-04-16 10:39:25","title":"Graph Neural Networks for Protein-Protein Interactions - A Short Survey","abstract":"Protein-protein interactions (PPIs) play key roles in a broad range of biological processes. Numerous strategies have been proposed for predicting PPIs, and among them, graph-based methods have demonstrated promising outcomes owing to the inherent graph structure of PPI networks. This paper reviews various graph-based methodologies, and discusses their applications in PPI prediction. We classify these approaches into two primary groups based on their model structures. The first category employs Graph Neural Networks (GNN) or Graph Convolutional Networks (GCN), while the second category utilizes Graph Attention Networks (GAT), Graph Auto-Encoders and Graph-BERT. We highlight the distinctive methodologies of each approach in managing the graph-structured data inherent in PPI networks and anticipate future research directions in this domain.","sentences":["Protein-protein interactions (PPIs) play key roles in a broad range of biological processes.","Numerous strategies have been proposed for predicting PPIs, and among them, graph-based methods have demonstrated promising outcomes owing to the inherent graph structure of PPI networks.","This paper reviews various graph-based methodologies, and discusses their applications in PPI prediction.","We classify these approaches into two primary groups based on their model structures.","The first category employs Graph Neural Networks (GNN) or Graph Convolutional Networks (GCN), while the second category utilizes Graph Attention Networks (GAT), Graph Auto-Encoders and Graph-BERT.","We highlight the distinctive methodologies of each approach in managing the graph-structured data inherent in PPI networks and anticipate future research directions in this domain."],"url":"http://arxiv.org/abs/2404.10450v1","category":"cs.LG"}
{"created":"2024-04-16 09:08:59","title":"Determining the order of time and spatial fractional derivatives","abstract":"The paper considers the initial-boundary value problem for equation $D^\\rho_t u(x,t)+ (-\\Delta)^\\sigma u(x,t)=0$, $\\rho\\in (0,1)$, $\\sigma>0$, in an N-dimensional domain $\\Omega$ with a homogeneous Dirichlet condition. The fractional derivative is taken in the sense of Caputo. The main goal of the work is to solve the inverse problem of simultaneously determining two parameters: the order of the fractional derivative $\\rho$ and the degree of the Laplace operator $\\sigma$. A new formulation and solution method for this inverse problem are proposed. It is proved that in the new formulation the solution to the inverse problem exists and is unique for an arbitrary initial function from the class $L_2(\\Omega)$. Note that in previously known works, only the uniqueness of the solution to the inverse problem was proved and the initial function was required to be sufficiently smooth and non-negative.","sentences":["The paper considers the initial-boundary value problem for equation $D^\\rho_t u(x,t)+ (-\\Delta)^\\sigma u(x,t)=0$, $\\rho\\in (0,1)$, $\\sigma>0$, in an N-dimensional domain $\\Omega$ with a homogeneous Dirichlet condition.","The fractional derivative is taken in the sense of Caputo.","The main goal of the work is to solve the inverse problem of simultaneously determining two parameters: the order of the fractional derivative $\\rho$ and the degree of the Laplace operator $\\sigma$.","A new formulation and solution method for this inverse problem are proposed.","It is proved that in the new formulation the solution to the inverse problem exists and is unique for an arbitrary initial function from the class $L_2(\\Omega)$. Note that in previously known works, only the uniqueness of the solution to the inverse problem was proved and the initial function was required to be sufficiently smooth and non-negative."],"url":"http://arxiv.org/abs/2404.10403v1","category":"math.AP"}
{"created":"2024-04-16 08:45:44","title":"Convergence rate of the spectral difference method on regular triangular meshes","abstract":"We consider the spectral difference method based on the p-th order Raviart~-- Thomas space (p=1,2,3) on regular triangular meshes for the scalar transport equation. The solution converges with the order p if the transport velocity is parallel to a family of mesh edges and with the order p+1 otherwise. We prove this fact for p=1 and show it for p=1,2,3 in numerical experiments.","sentences":["We consider the spectral difference method based on the p-th order Raviart~-- Thomas space (p=1,2,3) on regular triangular meshes for the scalar transport equation.","The solution converges with the order p if the transport velocity is parallel to a family of mesh edges and with the order p+1 otherwise.","We prove this fact for p=1 and show it for p=1,2,3 in numerical experiments."],"url":"http://arxiv.org/abs/2404.10391v1","category":"math.NA"}
{"created":"2024-04-16 07:41:29","title":"Rethinking the Graph Polynomial Filter via Positive and Negative Coupling Analysis","abstract":"Recently, the optimization of polynomial filters within Spectral Graph Neural Networks (GNNs) has emerged as a prominent research focus. Existing spectral GNNs mainly emphasize polynomial properties in filter design, introducing computational overhead and neglecting the integration of crucial graph structure information. We argue that incorporating graph information into basis construction can enhance understanding of polynomial basis, and further facilitate simplified polynomial filter design. Motivated by this, we first propose a Positive and Negative Coupling Analysis (PNCA) framework, where the concepts of positive and negative activation are defined and their respective and mixed effects are analysed. Then, we explore PNCA from the message propagation perspective, revealing the subtle information hidden in the activation process. Subsequently, PNCA is used to analyze the mainstream polynomial filters, and a novel simple basis that decouples the positive and negative activation and fully utilizes graph structure information is designed. Finally, a simple GNN (called GSCNet) is proposed based on the new basis. Experimental results on the benchmark datasets for node classification verify that our GSCNet obtains better or comparable results compared with existing state-of-the-art GNNs while demanding relatively less computational time.","sentences":["Recently, the optimization of polynomial filters within Spectral Graph Neural Networks (GNNs) has emerged as a prominent research focus.","Existing spectral GNNs mainly emphasize polynomial properties in filter design, introducing computational overhead and neglecting the integration of crucial graph structure information.","We argue that incorporating graph information into basis construction can enhance understanding of polynomial basis, and further facilitate simplified polynomial filter design.","Motivated by this, we first propose a Positive and Negative Coupling Analysis (PNCA) framework, where the concepts of positive and negative activation are defined and their respective and mixed effects are analysed.","Then, we explore PNCA from the message propagation perspective, revealing the subtle information hidden in the activation process.","Subsequently, PNCA is used to analyze the mainstream polynomial filters, and a novel simple basis that decouples the positive and negative activation and fully utilizes graph structure information is designed.","Finally, a simple GNN (called GSCNet) is proposed based on the new basis.","Experimental results on the benchmark datasets for node classification verify that our GSCNet obtains better or comparable results compared with existing state-of-the-art GNNs while demanding relatively less computational time."],"url":"http://arxiv.org/abs/2404.10353v1","category":"cs.LG"}
{"created":"2024-04-16 07:22:59","title":"Axion star condensation around primordial black holes and microlensing limits","abstract":"We present novel findings concerning the parameter space of axion stars, extended object forming in dense dark matter environments through gravitational condensation. We emphasize their formation within the dense minihalos that potentially surround primordial black holes and in axion miniclusters. Our study investigates the relation between the radius and mass of an axion star in these dense surroundings, revealing distinct morphological characteristics compared to isolated scenarios. We explore the implications of these results when applied to gravitational microlensing from extended objects and gravitational wave detection from compact binaries, leading to insights on the observational constraints from such ``halo'' axion stars. We provide a constraint on the fraction of the galactic population of axion stars from their contribution to the microlensing events from the EROS-2 survey, using the numerical resolution of the Schr\\\"odinger-Poisson equation.","sentences":["We present novel findings concerning the parameter space of axion stars, extended object forming in dense dark matter environments through gravitational condensation.","We emphasize their formation within the dense minihalos that potentially surround primordial black holes and in axion miniclusters.","Our study investigates the relation between the radius and mass of an axion star in these dense surroundings, revealing distinct morphological characteristics compared to isolated scenarios.","We explore the implications of these results when applied to gravitational microlensing from extended objects and gravitational wave detection from compact binaries, leading to insights on the observational constraints from such ``halo'' axion stars.","We provide a constraint on the fraction of the galactic population of axion stars from their contribution to the microlensing events from the EROS-2 survey, using the numerical resolution of the Schr\\\"odinger-Poisson equation."],"url":"http://arxiv.org/abs/2404.10340v1","category":"hep-ph"}
{"created":"2024-04-16 06:12:18","title":"Upscaling-based modified deep bed filtration model to match hyper-exponential retention","abstract":"Modelling of colloidal and nano-suspension transport in porous media has garnered significant attention due to the prevalence of these processes in many engineering applications. A number of experimental studies have reported retention profiles after coreflooding that are hyper-exponential, a feature that the traditional models for deep bed filtration are unable to capture. The aim of this work is the development of a model for binary particle transport which can account for hyper-exponential retention profiles (HERPs). Averaging of the binary model results in a non-linear dependence of the capture rate on the suspended particle concentration. The averaged model demonstrates how nonlinear capture behaviour arises due to particle heterogeneity even when individual particle populations filter traditionally. The model establishes that nonlinear capture behaviour, including the prediction of HERPs, is possible even for dilute particle suspensions given that particle heterogeneity is significant. An exact solution of the colloidal transport equations is presented for any non-linear suspension function and any step-wise-constant injected concentration. The binary suspension function cannot be written explicitly, and so we present asymptotic expansions for several limiting cases where the suspension function can be expressed directly. The asymptotic expansions show good agreement with the binary model, and correctly capture the HERPs calculated with the exact solution. Several laboratory tests exhibiting HERPs have been matched by the traditional, binary, and asymptotic expansion models. The traditional model significantly deviates from the laboratory data due to its inability to capture the hyper-exponential behaviour, while both the binary and asymptotic models capture this behaviour well","sentences":["Modelling of colloidal and nano-suspension transport in porous media has garnered significant attention due to the prevalence of these processes in many engineering applications.","A number of experimental studies have reported retention profiles after coreflooding that are hyper-exponential, a feature that the traditional models for deep bed filtration are unable to capture.","The aim of this work is the development of a model for binary particle transport which can account for hyper-exponential retention profiles (HERPs).","Averaging of the binary model results in a non-linear dependence of the capture rate on the suspended particle concentration.","The averaged model demonstrates how nonlinear capture behaviour arises due to particle heterogeneity even when individual particle populations filter traditionally.","The model establishes that nonlinear capture behaviour, including the prediction of HERPs, is possible even for dilute particle suspensions given that particle heterogeneity is significant.","An exact solution of the colloidal transport equations is presented for any non-linear suspension function and any step-wise-constant injected concentration.","The binary suspension function cannot be written explicitly, and so we present asymptotic expansions for several limiting cases where the suspension function can be expressed directly.","The asymptotic expansions show good agreement with the binary model, and correctly capture the HERPs calculated with the exact solution.","Several laboratory tests exhibiting HERPs have been matched by the traditional, binary, and asymptotic expansion models.","The traditional model significantly deviates from the laboratory data due to its inability to capture the hyper-exponential behaviour, while both the binary and asymptotic models capture this behaviour well"],"url":"http://arxiv.org/abs/2404.10303v1","category":"physics.geo-ph"}
{"created":"2024-04-16 05:54:01","title":"Anisotropic Gauss curvature flow of complete non-compact graphs","abstract":"In this paper, we consider the anisotropic $\\alpha$-Gauss curvature flow for complete noncompact convex hypersurfaces in the Euclidean space with the anisotropy determined by a smooth closed uniformly convex Wulff shape. We show that for all positive power $\\alpha>0$, if the initial hypersurface is complete noncompact and locally uniformly convex, then the solution of the flow exists for all positive time.","sentences":["In this paper, we consider the anisotropic $\\alpha$-Gauss curvature flow for complete noncompact convex hypersurfaces in the Euclidean space with the anisotropy determined by a smooth closed uniformly convex Wulff shape.","We show that for all positive power $\\alpha>0$, if the initial hypersurface is complete noncompact and locally uniformly convex, then the solution of the flow exists for all positive time."],"url":"http://arxiv.org/abs/2404.10298v1","category":"math.DG"}
{"created":"2024-04-16 05:30:55","title":"Orbital Angular Momentum Beam assisted High-Order Harmonic Generation in Semiconductor Materials","abstract":"We investigate the use of light beams carrying orbital angular momentum (OAM) in the context of high harmonic generation (HHG) within semiconductor crystals. Our contribution deals with the transfer and conservation of OAM in the strong-field regime, from the driving laser field to the generated harmonics. To this end, in this work, we combine the semiconductor Bloch equations with the thin slab model to simulate the generation of high-order harmonics in semiconductor media and to compute the features of the far-field harmonics. We demonstrate that this theoretical approach is capable of satisfactorily reproduce previously published experimental features of the generated harmonics in ZnO driven by a Laguerre-Gauss beam. Our research not only deepens the understanding of light-solid interactions but also heralds the dawn of bright, structured XUV coherent radiation sources with unparalleled potential across diverse technological areas, paving the way for enhanced functionalities in fields such as microscopy, spectroscopy, and optical communication.","sentences":["We investigate the use of light beams carrying orbital angular momentum (OAM) in the context of high harmonic generation (HHG) within semiconductor crystals.","Our contribution deals with the transfer and conservation of OAM in the strong-field regime, from the driving laser field to the generated harmonics.","To this end, in this work, we combine the semiconductor Bloch equations with the thin slab model to simulate the generation of high-order harmonics in semiconductor media and to compute the features of the far-field harmonics.","We demonstrate that this theoretical approach is capable of satisfactorily reproduce previously published experimental features of the generated harmonics in ZnO driven by a Laguerre-Gauss beam.","Our research not only deepens the understanding of light-solid interactions but also heralds the dawn of bright, structured XUV coherent radiation sources with unparalleled potential across diverse technological areas, paving the way for enhanced functionalities in fields such as microscopy, spectroscopy, and optical communication."],"url":"http://arxiv.org/abs/2404.10293v1","category":"physics.optics"}
{"created":"2024-04-16 04:44:16","title":"EucliDreamer: Fast and High-Quality Texturing for 3D Models with Depth-Conditioned Stable Diffusion","abstract":"We present EucliDreamer, a simple and effective method to generate textures for 3D models given text prompts and meshes. The texture is parametrized as an implicit function on the 3D surface, which is optimized with the Score Distillation Sampling (SDS) process and differentiable rendering. To generate high-quality textures, we leverage a depth-conditioned Stable Diffusion model guided by the depth image rendered from the mesh. We test our approach on 3D models in Objaverse and conducted a user study, which shows its superior quality compared to existing texturing methods like Text2Tex. In addition, our method converges 2 times faster than DreamFusion. Through text prompting, textures of diverse art styles can be produced. We hope Euclidreamer proides a viable solution to automate a labor-intensive stage in 3D content creation.","sentences":["We present EucliDreamer, a simple and effective method to generate textures for 3D models given text prompts and meshes.","The texture is parametrized as an implicit function on the 3D surface, which is optimized with the Score Distillation Sampling (SDS) process and differentiable rendering.","To generate high-quality textures, we leverage a depth-conditioned Stable Diffusion model guided by the depth image rendered from the mesh.","We test our approach on 3D models in Objaverse and conducted a user study, which shows its superior quality compared to existing texturing methods like Text2Tex.","In addition, our method converges 2 times faster than DreamFusion.","Through text prompting, textures of diverse art styles can be produced.","We hope Euclidreamer proides a viable solution to automate a labor-intensive stage in 3D content creation."],"url":"http://arxiv.org/abs/2404.10279v1","category":"cs.CV"}
{"created":"2024-04-16 04:31:20","title":"CMC hypersurface with finite index in hyperbolic space $\\mathbb{H}^4$","abstract":"In this paper, we prove that there are no complete noncompact constant mean curvature hypersurfaces with the mean curvature $H>1$ and finite index in hyperbolic space $\\mathbb{H}^4$. This improves the previous best assumption, namely $H>\\frac{64}{63}$, to the optimal case. A more general nonexistence result can be proved in a 4-dimensional Riemannian manifold with certain curvature conditions. The proof relies on the harmonic function theory developed by Li-Tam-Wang and the $\\mu$-bubble initially introduced by Gromov and further developed by Chodosh-Li-Stryker in the context of stable minimal hypersurfaces.","sentences":["In this paper, we prove that there are no complete noncompact constant mean curvature hypersurfaces with the mean curvature $H>1$ and finite index in hyperbolic space $\\mathbb{H}^4$. This improves the previous best assumption, namely $H>\\frac{64}{63}$, to the optimal case.","A more general nonexistence result can be proved in a 4-dimensional Riemannian manifold with certain curvature conditions.","The proof relies on the harmonic function theory developed by Li-Tam-Wang and the $\\mu$-bubble initially introduced by Gromov and further developed by Chodosh-Li-Stryker in the context of stable minimal hypersurfaces."],"url":"http://arxiv.org/abs/2404.10276v1","category":"math.DG"}
{"created":"2024-04-16 02:33:19","title":"Dark photon - dark energy stationary axisymmetric black holes","abstract":"Using Ernst formalism, stationary axisymmetric black hole solution in Einstein-dark matter-dark energy gravity has been elaborated. The dark sector was chosen as dark photon concept, where an auxiliary U(1)-gauge field coupled to ordinary Maxwell one was introduced, while dark energy was modelled by the existence of positive cosmological constant. Refining our studies to the case of vanishing cosmological constant, the uniqueness theorem for the black hole in question has been proved.","sentences":["Using Ernst formalism, stationary axisymmetric black hole solution in Einstein-dark matter-dark energy gravity has been elaborated.","The dark sector was chosen as dark photon concept, where an auxiliary U(1)-gauge field coupled to ordinary Maxwell one was introduced, while dark energy was modelled by the existence of positive cosmological constant.","Refining our studies to the case of vanishing cosmological constant, the uniqueness theorem for the black hole in question has been proved."],"url":"http://arxiv.org/abs/2404.10236v1","category":"gr-qc"}
{"created":"2024-04-16 02:18:30","title":"Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural Networks","abstract":"The high volume and rapid evolution of content on social media present major challenges for studying the stance of social media users. In this work, we develop a two stage stance labeling method that utilizes the user-hashtag bipartite graph and the user-user interaction graph. In the first stage, a simple and efficient heuristic for stance labeling uses the user-hashtag bipartite graph to iteratively update the stance association of user and hashtag nodes via a label propagation mechanism. This set of soft labels is then integrated with the user-user interaction graph to train a graph neural network (GNN) model using semi-supervised learning. We evaluate this method on two large-scale datasets containing tweets related to climate change from June 2021 to June 2022 and gun control from January 2022 to January 2023. Experiments demonstrate that our user-hashtag heuristic and the semi-supervised GNN method outperform zero-shot stance labeling using LLMs such as GPT4. Further analysis illustrates how the stance labeling information and interaction graph can be used for evaluating the polarization of social media interactions on divisive issues such as climate change and gun control.","sentences":["The high volume and rapid evolution of content on social media present major challenges for studying the stance of social media users.","In this work, we develop a two stage stance labeling method that utilizes the user-hashtag bipartite graph and the user-user interaction graph.","In the first stage, a simple and efficient heuristic for stance labeling uses the user-hashtag bipartite graph to iteratively update the stance association of user and hashtag nodes via a label propagation mechanism.","This set of soft labels is then integrated with the user-user interaction graph to train a graph neural network (GNN) model using semi-supervised learning.","We evaluate this method on two large-scale datasets containing tweets related to climate change from June 2021 to June 2022 and gun control from January 2022 to January 2023.","Experiments demonstrate that our user-hashtag heuristic and the semi-supervised GNN method outperform zero-shot stance labeling using LLMs such as GPT4.","Further analysis illustrates how the stance labeling information and interaction graph can be used for evaluating the polarization of social media interactions on divisive issues such as climate change and gun control."],"url":"http://arxiv.org/abs/2404.10228v1","category":"cs.LG"}
{"created":"2024-04-16 02:09:12","title":"Prethermalization in aperiodically driven classical spin systems","abstract":"Periodically driven classical many-body systems can host a rich zoo of prethermal dynamical phases. In this work, we extend the paradigm of classical prethermalization to aperiodically driven systems. We establish the existence of a long-lived prethermal regime in spin systems subjected to random multipolar drives (RMDs). We demonstrate that the thermalization time scales as $(1/T)^{2n+2}$, where $n$ is the multipolar order and $T$ is the intrinsic time-scale associated with the drive. In the $n \\rightarrow \\infty$ limit, the drive becomes quasi-periodic and the thermalization time becomes exponentially long ($\\sim \\exp(\\beta/T)$). We further establish the robustness of prethermalization by demonstrating that these thermalization time scaling laws hold for a wide range of initial state energy densities. Intriguingly, the thermalization process in these classical systems is parametrically slower than their quantum counterparts, thereby highlighting important differences between classical and quantum prethermalization. Finally, we propose a protocol to harness this classical prethermalization to realize time rondeau crystals.","sentences":["Periodically driven classical many-body systems can host a rich zoo of prethermal dynamical phases.","In this work, we extend the paradigm of classical prethermalization to aperiodically driven systems.","We establish the existence of a long-lived prethermal regime in spin systems subjected to random multipolar drives (RMDs).","We demonstrate that the thermalization time scales as $(1/T)^{2n+2}$, where $n$ is the multipolar order and $T$ is the intrinsic time-scale associated with the drive.","In the $n \\rightarrow \\infty$ limit, the drive becomes quasi-periodic and the thermalization time becomes exponentially long ($\\sim \\exp(\\beta/T)$).","We further establish the robustness of prethermalization by demonstrating that these thermalization time scaling laws hold for a wide range of initial state energy densities.","Intriguingly, the thermalization process in these classical systems is parametrically slower than their quantum counterparts, thereby highlighting important differences between classical and quantum prethermalization.","Finally, we propose a protocol to harness this classical prethermalization to realize time rondeau crystals."],"url":"http://arxiv.org/abs/2404.10224v1","category":"quant-ph"}
{"created":"2024-04-16 02:02:33","title":"On $\u03c4$-preconditioners for a quasi-compact difference scheme to Riesz fractional diffusion equations with variable coefficients","abstract":"In the present study, we consider the numerical method for Toeplitz-like linear systems arising from the $d$-dimensional Riesz space fractional diffusion equations (RSFDEs). We apply the Crank-Nicolson (CN) technique to discretize the temporal derivative and apply a quasi-compact finite difference method to discretize the Riesz space fractional derivatives. For the $d$-dimensional problem, the corresponding coefficient matrix is the sum of a product of a (block) tridiagonal matrix multiplying a diagonal matrix and a $d$-level Toeplitz matrix. We develop a sine transform based preconditioner to accelerate the convergence of the GMRES method. Theoretical analyses show that the upper bound of relative residual norm of the preconditioned GMRES method with the proposed preconditioner is mesh-independent, which leads to a linear convergence rate. Numerical results are presented to confirm the theoretical results regarding the preconditioned matrix and to illustrate the efficiency of the proposed preconditioner.","sentences":["In the present study, we consider the numerical method for Toeplitz-like linear systems arising from the $d$-dimensional Riesz space fractional diffusion equations (RSFDEs).","We apply the Crank-Nicolson (CN) technique to discretize the temporal derivative and apply a quasi-compact finite difference method to discretize the Riesz space fractional derivatives.","For the $d$-dimensional problem, the corresponding coefficient matrix is the sum of a product of a (block) tridiagonal matrix multiplying a diagonal matrix and a $d$-level Toeplitz matrix.","We develop a sine transform based preconditioner to accelerate the convergence of the GMRES method.","Theoretical analyses show that the upper bound of relative residual norm of the preconditioned GMRES method with the proposed preconditioner is mesh-independent, which leads to a linear convergence rate.","Numerical results are presented to confirm the theoretical results regarding the preconditioned matrix and to illustrate the efficiency of the proposed preconditioner."],"url":"http://arxiv.org/abs/2404.10221v1","category":"math.NA"}
{"created":"2024-04-16 01:41:22","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition","abstract":"In recent years, skeleton-based action recognition, leveraging multimodal Graph Convolutional Networks (GCN), has achieved remarkable results. However, due to their deep structure and reliance on continuous floating-point operations, GCN-based methods are energy-intensive. To address this issue, we propose an innovative Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation (MK-SGN). By merging the energy efficiency of Spiking Neural Network (SNN) with the graph representation capability of GCN, the proposed MK-SGN reduces energy consumption while maintaining recognition accuracy. Firstly, we convert GCN into Spiking Graph Convolutional Network (SGN) and construct a foundational Base-SGN for skeleton-based action recognition, establishing a new benchmark and paving the way for future research exploration. Secondly, we further propose a Spiking Multimodal Fusion module (SMF), leveraging mutual information to process multimodal data more efficiently. Additionally, we introduce a spiking attention mechanism and design a Spatio Graph Convolution module with a Spatial Global Spiking Attention mechanism (SA-SGC), enhancing feature learning capability. Furthermore, we delve into knowledge distillation methods from multimodal GCN to SGN and propose a novel, integrated method that simultaneously focuses on both intermediate layer distillation and soft label distillation to improve the performance of SGN. On two challenging datasets for skeleton-based action recognition, MK-SGN outperforms the state-of-the-art GCN-like frameworks in reducing computational load and energy consumption. In contrast, typical GCN methods typically consume more than 35mJ per action sample, while MK-SGN reduces energy consumption by more than 98%.","sentences":["In recent years, skeleton-based action recognition, leveraging multimodal Graph Convolutional Networks (GCN), has achieved remarkable results.","However, due to their deep structure and reliance on continuous floating-point operations, GCN-based methods are energy-intensive.","To address this issue, we propose an innovative Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation (MK-SGN).","By merging the energy efficiency of Spiking Neural Network (SNN) with the graph representation capability of GCN, the proposed MK-SGN reduces energy consumption while maintaining recognition accuracy.","Firstly, we convert GCN into Spiking Graph Convolutional Network (SGN) and construct a foundational Base-SGN for skeleton-based action recognition, establishing a new benchmark and paving the way for future research exploration.","Secondly, we further propose a Spiking Multimodal Fusion module (SMF), leveraging mutual information to process multimodal data more efficiently.","Additionally, we introduce a spiking attention mechanism and design a Spatio Graph Convolution module with a Spatial Global Spiking Attention mechanism (SA-SGC), enhancing feature learning capability.","Furthermore, we delve into knowledge distillation methods from multimodal GCN to SGN and propose a novel, integrated method that simultaneously focuses on both intermediate layer distillation and soft label distillation to improve the performance of SGN.","On two challenging datasets for skeleton-based action recognition, MK-SGN outperforms the state-of-the-art GCN-like frameworks in reducing computational load and energy consumption.","In contrast, typical GCN methods typically consume more than 35mJ per action sample, while MK-SGN reduces energy consumption by more than 98%."],"url":"http://arxiv.org/abs/2404.10210v1","category":"cs.CV"}
{"created":"2024-04-16 00:29:07","title":"Yang-Mills-Higgs Equations on Spherical Orbifolds","abstract":"In this paper we study (static) solutions of the rank 2 Yang-Mills-Higgs equations on the Riemann sphere, with concical singularities, that bifurcate from constant curvature connections. We focus attention on the case where there are exactly four such singularities. This study brings together ideas from the gauge theory of constant curvature connections on vector bundles over singular Riemann surfaces with the Riemann-Hilbert analysis of classical Fuchsian ODEs.","sentences":["In this paper we study (static) solutions of the rank 2 Yang-Mills-Higgs equations on the Riemann sphere, with concical singularities, that bifurcate from constant curvature connections.","We focus attention on the case where there are exactly four such singularities.","This study brings together ideas from the gauge theory of constant curvature connections on vector bundles over singular Riemann surfaces with the Riemann-Hilbert analysis of classical Fuchsian ODEs."],"url":"http://arxiv.org/abs/2404.10196v1","category":"math-ph"}
{"created":"2024-04-16 00:11:32","title":"Communicating skyrmions as the main mechanism underlying skyrmionium (meta)stability in quasi-two-dimensional chiral magnets","abstract":"We re-examine the internal structure of skyrmioniums stabilized in quasi-two-dimensional chiral magnets with easy-axis uniaxial anisotropy. Skyrmioniums are particle-like states of two nested skyrmions with opposite polarities contributing to zero topological charge. The physical principles of skyrmionium stability are drawn from both the analytical analysis with a trial function and from numerical simulations within the framework of micromagnetism. We deduce that the radii of the internal skyrmion with the positive polarity and the ring-shaped external skyrmion with the negative polarity are mutually dependent, which constitutes the paradigm of communicating skyrmions. For large central skyrmions, the skyrmionium transforms into a narrow circular domain wall, whereas for small internal radii, the ring expands, which occurs at the verge of collapsing into an ordinary isolated skyrmion. We show that skyrmioniums may form lattices of two varieties depending on the polarity of the internal skyrmion. At the phase diagram (magnetic field)-(uniaxial anisotropy), both skyrmionium lattices share the same area with one-dimensional spiral states and remain metastable solutions for the whole range of control parameters. By expanding at the critical line, skyrmionium lattices do not release isolated skyrmioniums. Isolated skyrmioniums of just one type exist apart from the corresponding lattice in a narrow field region restricted by the critical line of expansion from below and by the line of collapse above.","sentences":["We re-examine the internal structure of skyrmioniums stabilized in quasi-two-dimensional chiral magnets with easy-axis uniaxial anisotropy.","Skyrmioniums are particle-like states of two nested skyrmions with opposite polarities contributing to zero topological charge.","The physical principles of skyrmionium stability are drawn from both the analytical analysis with a trial function and from numerical simulations within the framework of micromagnetism.","We deduce that the radii of the internal skyrmion with the positive polarity and the ring-shaped external skyrmion with the negative polarity are mutually dependent, which constitutes the paradigm of communicating skyrmions.","For large central skyrmions, the skyrmionium transforms into a narrow circular domain wall, whereas for small internal radii, the ring expands, which occurs at the verge of collapsing into an ordinary isolated skyrmion.","We show that skyrmioniums may form lattices of two varieties depending on the polarity of the internal skyrmion.","At the phase diagram (magnetic field)-(uniaxial anisotropy), both skyrmionium lattices share the same area with one-dimensional spiral states and remain metastable solutions for the whole range of control parameters.","By expanding at the critical line, skyrmionium lattices do not release isolated skyrmioniums.","Isolated skyrmioniums of just one type exist apart from the corresponding lattice in a narrow field region restricted by the critical line of expansion from below and by the line of collapse above."],"url":"http://arxiv.org/abs/2404.10189v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-15 23:23:31","title":"CryoMAE: Few-Shot Cryo-EM Particle Picking with Masked Autoencoders","abstract":"Cryo-electron microscopy (cryo-EM) emerges as a pivotal technology for determining the architecture of cells, viruses, and protein assemblies at near-atomic resolution. Traditional particle picking, a key step in cryo-EM, struggles with manual effort and automated methods' sensitivity to low signal-to-noise ratio (SNR) and varied particle orientations. Furthermore, existing neural network (NN)-based approaches often require extensive labeled datasets, limiting their practicality. To overcome these obstacles, we introduce cryoMAE, a novel approach based on few-shot learning that harnesses the capabilities of Masked Autoencoders (MAE) to enable efficient selection of single particles in cryo-EM images. Contrary to conventional NN-based techniques, cryoMAE requires only a minimal set of positive particle images for training yet demonstrates high performance in particle detection. Furthermore, the implementation of a self-cross similarity loss ensures distinct features for particle and background regions, thereby enhancing the discrimination capability of cryoMAE. Experiments on large-scale cryo-EM datasets show that cryoMAE outperforms existing state-of-the-art (SOTA) methods, improving 3D reconstruction resolution by up to 22.4%.","sentences":["Cryo-electron microscopy (cryo-EM) emerges as a pivotal technology for determining the architecture of cells, viruses, and protein assemblies at near-atomic resolution.","Traditional particle picking, a key step in cryo-EM, struggles with manual effort and automated methods' sensitivity to low signal-to-noise ratio (SNR) and varied particle orientations.","Furthermore, existing neural network (NN)-based approaches often require extensive labeled datasets, limiting their practicality.","To overcome these obstacles, we introduce cryoMAE, a novel approach based on few-shot learning that harnesses the capabilities of Masked Autoencoders (MAE) to enable efficient selection of single particles in cryo-EM images.","Contrary to conventional NN-based techniques, cryoMAE requires only a minimal set of positive particle images for training yet demonstrates high performance in particle detection.","Furthermore, the implementation of a self-cross similarity loss ensures distinct features for particle and background regions, thereby enhancing the discrimination capability of cryoMAE.","Experiments on large-scale cryo-EM datasets show that cryoMAE outperforms existing state-of-the-art (SOTA) methods, improving 3D reconstruction resolution by up to 22.4%."],"url":"http://arxiv.org/abs/2404.10178v1","category":"q-bio.BM"}
{"created":"2024-04-15 23:07:57","title":"Multi-objective evolutionary GAN for tabular data synthesis","abstract":"Synthetic data has a key role to play in data sharing by statistical agencies and other generators of statistical data products. Generative Adversarial Networks (GANs), typically applied to image synthesis, are also a promising method for tabular data synthesis. However, there are unique challenges in tabular data compared to images, eg tabular data may contain both continuous and discrete variables and conditional sampling, and, critically, the data should possess high utility and low disclosure risk (the risk of re-identifying a population unit or learning something new about them), providing an opportunity for multi-objective (MO) optimization. Inspired by MO GANs for images, this paper proposes a smart MO evolutionary conditional tabular GAN (SMOE-CTGAN). This approach models conditional synthetic data by applying conditional vectors in training, and uses concepts from MO optimisation to balance disclosure risk against utility. Our results indicate that SMOE-CTGAN is able to discover synthetic datasets with different risk and utility levels for multiple national census datasets. We also find a sweet spot in the early stage of training where a competitive utility and extremely low risk are achieved, by using an Improvement Score. The full code can be downloaded from https://github.com/HuskyNian/SMO\\_EGAN\\_pytorch.","sentences":["Synthetic data has a key role to play in data sharing by statistical agencies and other generators of statistical data products.","Generative Adversarial Networks (GANs), typically applied to image synthesis, are also a promising method for tabular data synthesis.","However, there are unique challenges in tabular data compared to images, eg tabular data may contain both continuous and discrete variables and conditional sampling, and, critically, the data should possess high utility and low disclosure risk (the risk of re-identifying a population unit or learning something new about them), providing an opportunity for multi-objective (MO) optimization.","Inspired by MO GANs for images, this paper proposes a smart MO evolutionary conditional tabular GAN (SMOE-CTGAN).","This approach models conditional synthetic data by applying conditional vectors in training, and uses concepts from MO optimisation to balance disclosure risk against utility.","Our results indicate that SMOE-CTGAN is able to discover synthetic datasets with different risk and utility levels for multiple national census datasets.","We also find a sweet spot in the early stage of training where a competitive utility and extremely low risk are achieved, by using an Improvement Score.","The full code can be downloaded from https://github.com/HuskyNian/SMO\\_EGAN\\_pytorch."],"url":"http://arxiv.org/abs/2404.10176v1","category":"cs.LG"}
{"created":"2024-04-15 23:01:59","title":"Forensic Iris Image-Based Post-Mortem Interval Estimation","abstract":"Post-mortem iris recognition is an emerging application of iris-based human identification in a forensic setup. One factor that may be useful in conditioning iris recognition methods is the tissue decomposition level, which is correlated with the post-mortem interval (PMI), i.g., the number of hours that have elapsed since death. PMI, however, is not always available, and its precise estimation remains one of the core challenges in forensic examination. This paper presents the first known to us method of PMI estimation directly from forensic iris images. To assess the feasibility of the iris-based PMI estimation, convolutional neural networks-based models (VGG19, DenseNet121, ResNet152, and Inception_v3) were trained to predict the PMI from (a) near-infrared (NIR), (b) visible (RGB), and (c) multispectral forensic iris images. Models were evaluated following a 10-fold cross-validation in (S1) sample-disjoint, (S2) subject-disjoint, and (S3) cross-dataset scenarios. We found that using the multispectral data offers a spectacularly low mean absolute error (MAE) of approximately 3.5 hours in scenario (S1), a bit worse MAE of approximately 17.5 hours in scenario (S2), and an MAE of approximately 69.0 hours of in the scenario (S3). This suggests that if the environmental conditions are favorable (e.g., bodies are kept in low temperatures), forensic iris images provide features that are indicative of the PMI and can be automatically estimated. The source codes and model weights are made available with the paper.","sentences":["Post-mortem iris recognition is an emerging application of iris-based human identification in a forensic setup.","One factor that may be useful in conditioning iris recognition methods is the tissue decomposition level, which is correlated with the post-mortem interval (PMI), i.g., the number of hours that have elapsed since death.","PMI, however, is not always available, and its precise estimation remains one of the core challenges in forensic examination.","This paper presents the first known to us method of PMI estimation directly from forensic iris images.","To assess the feasibility of the iris-based PMI estimation, convolutional neural networks-based models (VGG19, DenseNet121, ResNet152, and Inception_v3) were trained to predict the PMI from (a) near-infrared (NIR), (b) visible (RGB), and (c) multispectral forensic iris images.","Models were evaluated following a 10-fold cross-validation in (S1) sample-disjoint, (S2) subject-disjoint, and (S3) cross-dataset scenarios.","We found that using the multispectral data offers a spectacularly low mean absolute error (MAE) of approximately 3.5 hours in scenario (S1), a bit worse MAE of approximately 17.5 hours in scenario (S2), and an MAE of approximately 69.0 hours of in the scenario (S3).","This suggests that if the environmental conditions are favorable (e.g., bodies are kept in low temperatures), forensic iris images provide features that are indicative of the PMI and can be automatically estimated.","The source codes and model weights are made available with the paper."],"url":"http://arxiv.org/abs/2404.10172v1","category":"cs.CV"}
{"created":"2024-04-15 22:27:58","title":"A solvable non-unitary fermionic long-range model with extended symmetry","abstract":"We define and study a long-range version of the XX model, arising as the free-fermion point of the XXZ-type Haldane-Shastry (HS) chain. It has a simple realisation via non-unitary fermions, based on the free-fermion Temperley-Lieb algebra, and may also be viewed as an alternating $\\mathfrak{gl}(1|1)$ spin chain. Even and odd length behave very differently; we focus on odd length. The model is integrable, and we explicitly identify two commuting hamiltonians. While non-unitary, their spectrum is real by PT-symmetry. One hamiltonian is chiral and quadratic in fermions, while the other is parity-invariant and quartic. Their one-particle spectra have two linear branches, realising a massless relativistic dispersion on the lattice. The appropriate fermionic modes arise from 'quasi-translation' symmetry, which replaces ordinary translation symmetry. The model exhibits exclusion statistics, like for the isotropic HS chain, with even more 'extended symmetry' and larger degeneracies.","sentences":["We define and study a long-range version of the XX model, arising as the free-fermion point of the XXZ-type Haldane-Shastry (HS) chain.","It has a simple realisation via non-unitary fermions, based on the free-fermion Temperley-Lieb algebra, and may also be viewed as an alternating $\\mathfrak{gl}(1|1)$ spin chain.","Even and odd length behave very differently; we focus on odd length.","The model is integrable, and we explicitly identify two commuting hamiltonians.","While non-unitary, their spectrum is real by PT-symmetry.","One hamiltonian is chiral and quadratic in fermions, while the other is parity-invariant and quartic.","Their one-particle spectra have two linear branches, realising a massless relativistic dispersion on the lattice.","The appropriate fermionic modes arise from 'quasi-translation' symmetry, which replaces ordinary translation symmetry.","The model exhibits exclusion statistics, like for the isotropic HS chain, with even more 'extended symmetry' and larger degeneracies."],"url":"http://arxiv.org/abs/2404.10164v1","category":"cond-mat.str-el"}
{"created":"2024-04-15 22:15:28","title":"New Asymptotic Preserving, Hybrid Discontinuous Galerkin Methods the Radiation Transport Equation with Isotropic Scattering and Diffusive Scaling","abstract":"Discontinuous Galerkin (DG) methods are widely adopted to discretize the radiation transport equation (RTE) with diffusive scalings. One of the most important advantages of the DG methods for RTE is their asymptotic preserving (AP) property, in the sense that they preserve the diffusive limits of the equation in the discrete setting, without requiring excessive refinement of the discretization. However, compared to finite element methods or finite volume methods, the employment of DG methods also increases the number of unknowns, which requires more memory and computational time to solve the problems. In this paper, when the spherical harmonic method is applied for the angular discretization, we perform an asymptotic analysis which shows that to retain the uniform convergence, it is only necessary to employ non-constant elements for the degree zero moment only in the DG spatial discretization. Based on this observation, we propose a heterogeneous DG method that employs polynomial spaces of different degrees for the degree zero and remaining moments respectively. To improve the convergence order, we further develop a spherical harmonics hybrid DG finite volume method, which preserves the AP property and convergence rate while tremendously reducing the number of unknowns. Numerical examples are provided to illustrate the effectiveness and accuracy of the proposed scheme.","sentences":["Discontinuous Galerkin (DG) methods are widely adopted to discretize the radiation transport equation (RTE) with diffusive scalings.","One of the most important advantages of the DG methods for RTE is their asymptotic preserving (AP) property, in the sense that they preserve the diffusive limits of the equation in the discrete setting, without requiring excessive refinement of the discretization.","However, compared to finite element methods or finite volume methods, the employment of DG methods also increases the number of unknowns, which requires more memory and computational time to solve the problems.","In this paper, when the spherical harmonic method is applied for the angular discretization, we perform an asymptotic analysis which shows that to retain the uniform convergence, it is only necessary to employ non-constant elements for the degree zero moment only in the DG spatial discretization.","Based on this observation, we propose a heterogeneous DG method that employs polynomial spaces of different degrees for the degree zero and remaining moments respectively.","To improve the convergence order, we further develop a spherical harmonics hybrid DG finite volume method, which preserves the AP property and convergence rate while tremendously reducing the number of unknowns.","Numerical examples are provided to illustrate the effectiveness and accuracy of the proposed scheme."],"url":"http://arxiv.org/abs/2404.10159v1","category":"math.NA"}
{"created":"2024-04-15 22:12:05","title":"SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation","abstract":"The adoption of Vision Transformers (ViTs) based architectures represents a significant advancement in 3D Medical Image (MI) segmentation, surpassing traditional Convolutional Neural Network (CNN) models by enhancing global contextual understanding. While this paradigm shift has significantly enhanced 3D segmentation performance, state-of-the-art architectures require extremely large and complex architectures with large scale computing resources for training and deployment. Furthermore, in the context of limited datasets, often encountered in medical imaging, larger models can present hurdles in both model generalization and convergence. In response to these challenges and to demonstrate that lightweight models are a valuable area of research in 3D medical imaging, we present SegFormer3D, a hierarchical Transformer that calculates attention across multiscale volumetric features. Additionally, SegFormer3D avoids complex decoders and uses an all-MLP decoder to aggregate local and global attention features to produce highly accurate segmentation masks. The proposed memory efficient Transformer preserves the performance characteristics of a significantly larger model in a compact design. SegFormer3D democratizes deep learning for 3D medical image segmentation by offering a model with 33x less parameters and a 13x reduction in GFLOPS compared to the current state-of-the-art (SOTA). We benchmark SegFormer3D against the current SOTA models on three widely used datasets Synapse, BRaTs, and ACDC, achieving competitive results. Code: https://github.com/OSUPCVLab/SegFormer3D.git","sentences":["The adoption of Vision Transformers (ViTs) based architectures represents a significant advancement in 3D Medical Image (MI) segmentation, surpassing traditional Convolutional Neural Network (CNN) models by enhancing global contextual understanding.","While this paradigm shift has significantly enhanced 3D segmentation performance, state-of-the-art architectures require extremely large and complex architectures with large scale computing resources for training and deployment.","Furthermore, in the context of limited datasets, often encountered in medical imaging, larger models can present hurdles in both model generalization and convergence.","In response to these challenges and to demonstrate that lightweight models are a valuable area of research in 3D medical imaging, we present SegFormer3D, a hierarchical Transformer that calculates attention across multiscale volumetric features.","Additionally, SegFormer3D avoids complex decoders and uses an all-MLP decoder to aggregate local and global attention features to produce highly accurate segmentation masks.","The proposed memory efficient Transformer preserves the performance characteristics of a significantly larger model in a compact design.","SegFormer3D democratizes deep learning for 3D medical image segmentation by offering a model with 33x less parameters and a 13x reduction in GFLOPS compared to the current state-of-the-art (SOTA).","We benchmark SegFormer3D against the current SOTA models on three widely used datasets Synapse, BRaTs, and ACDC, achieving competitive results.","Code: https://github.com/OSUPCVLab/SegFormer3D.git"],"url":"http://arxiv.org/abs/2404.10156v1","category":"cs.CV"}
{"created":"2024-04-15 21:24:18","title":"Computing with Hypergeometric-Type Terms","abstract":"Take a multiplicative monoid of sequences in which the multiplication is given by Hadamard product. The set of linear combinations of interleaving monoid elements then yields a ring. We consider such a construction for the monoid of hypergeometric sequences, yielding what we call the ring of hypergeometric-type sequences -- a subring of the ring of holonomic sequences. We present two algorithms in this setting: one for computing holonomic recurrence equations from hypergeometric-type normal forms and the other for finding products of hypergeometric-type terms. These are newly implemented commands in our Maple package $\\texttt{HyperTypeSeq}$, which we also describe.","sentences":["Take a multiplicative monoid of sequences in which the multiplication is given by Hadamard product.","The set of linear combinations of interleaving monoid elements then yields a ring.","We consider such a construction for the monoid of hypergeometric sequences, yielding what we call the ring of hypergeometric-type sequences -- a subring of the ring of holonomic sequences.","We present two algorithms in this setting: one for computing holonomic recurrence equations from hypergeometric-type normal forms and the other for finding products of hypergeometric-type terms.","These are newly implemented commands in our Maple package $\\texttt{HyperTypeSeq}$, which we also describe."],"url":"http://arxiv.org/abs/2404.10143v1","category":"cs.SC"}
{"created":"2024-04-15 20:48:33","title":"WB LUTs: Contrastive Learning for White Balancing Lookup Tables","abstract":"Automatic white balancing (AWB), one of the first steps in an integrated signal processing (ISP) pipeline, aims to correct the color cast induced by the scene illuminant. An incorrect white balance (WB) setting or AWB failure can lead to an undesired blue or red tint in the rendered sRGB image. To address this, recent methods pose the post-capture WB correction problem as an image-to-image translation task and train deep neural networks to learn the necessary color adjustments at a lower resolution. These low resolution outputs are post-processed to generate high resolution WB corrected images, forming a bottleneck in the end-to-end run time. In this paper we present a 3D Lookup Table (LUT) based WB correction model called WB LUTs that can generate high resolution outputs in real time. We introduce a contrastive learning framework with a novel hard sample mining strategy, which improves the WB correction quality of baseline 3D LUTs by 25.5%. Experimental results demonstrate that the proposed WB LUTs perform competitively against state-of-the-art models on two benchmark datasets while being 300 times faster using 12.7 times less memory. Our model and code are available at https://github.com/skrmanne/3DLUT_sRGB_WB.","sentences":["Automatic white balancing (AWB), one of the first steps in an integrated signal processing (ISP) pipeline, aims to correct the color cast induced by the scene illuminant.","An incorrect white balance (WB) setting or AWB failure can lead to an undesired blue or red tint in the rendered sRGB image.","To address this, recent methods pose the post-capture WB correction problem as an image-to-image translation task and train deep neural networks to learn the necessary color adjustments at a lower resolution.","These low resolution outputs are post-processed to generate high resolution WB corrected images, forming a bottleneck in the end-to-end run time.","In this paper we present a 3D Lookup Table (LUT) based WB correction model called WB LUTs that can generate high resolution outputs in real time.","We introduce a contrastive learning framework with a novel hard sample mining strategy, which improves the WB correction quality of baseline 3D LUTs by 25.5%.","Experimental results demonstrate that the proposed WB LUTs perform competitively against state-of-the-art models on two benchmark datasets while being 300 times faster using 12.7 times less memory.","Our model and code are available at https://github.com/skrmanne/3DLUT_sRGB_WB."],"url":"http://arxiv.org/abs/2404.10133v1","category":"cs.CV"}
{"created":"2024-04-15 20:28:21","title":"A general thermodynamical model for finitely-strained continuum with inelasticity and diffusion, its GENERIC derivation in Eulerian formulation, and some application","abstract":"A thermodynamically consistent visco-elastodynamical model at finite strains is derived that also allows for inelasticity (like plasticity or creep), thermal coupling, and poroelasticity with diffusion. The theory is developed in the Eulerian framework and is shown to be consistent with the thermodynamic framework given by General Equation for Non-Equilibrium Reversible-Irreversible Coupling (GENERIC). For the latter we use that the transport terms are given in terms of Lie derivatives. Application is illustrated by two examples, namely volumetric phase transitions with dehydration in rocks and martensitic phase transitions in shape-memory alloys. A strategy towards a rigorous mathematical analysis is only very briefly outlined.","sentences":["A thermodynamically consistent visco-elastodynamical model at finite strains is derived that also allows for inelasticity (like plasticity or creep), thermal coupling, and poroelasticity with diffusion.","The theory is developed in the Eulerian framework and is shown to be consistent with the thermodynamic framework given by General Equation for Non-Equilibrium Reversible-Irreversible Coupling (GENERIC).","For the latter we use that the transport terms are given in terms of Lie derivatives.","Application is illustrated by two examples, namely volumetric phase transitions with dehydration in rocks and martensitic phase transitions in shape-memory alloys.","A strategy towards a rigorous mathematical analysis is only very briefly outlined."],"url":"http://arxiv.org/abs/2404.10126v1","category":"math-ph"}
{"created":"2024-04-15 20:20:16","title":"Numerical methods for solving the linearized model of a hinged-free reduced plate arising in flow structure interactions","abstract":"The problem of partially hinged and partially free rectangular plate that aims to represent a suspension bridge subject to some external forces (for example the wind) is considered in order to model and simulate the unstable end behavior.   Such a problem can be modeled by a plate evolution equation, which is nonlinear with a nonlocal stretching effect in the spanwise direction.   The external forces are periodic in time and cause the vortex shedding on the structure (on the surface of the plate) and thus it may cause damage to the material.   Numerical study of the behavior of steady state solutions for different values of the force velocity are provided with two finite element methods of different type.","sentences":["The problem of partially hinged and partially free rectangular plate that aims to represent a suspension bridge subject to some external forces (for example the wind) is considered in order to model and simulate the unstable end behavior.   ","Such a problem can be modeled by a plate evolution equation, which is nonlinear with a nonlocal stretching effect in the spanwise direction.   ","The external forces are periodic in time and cause the vortex shedding on the structure (on the surface of the plate) and thus it may cause damage to the material.   ","Numerical study of the behavior of steady state solutions for different values of the force velocity are provided with two finite element methods of different type."],"url":"http://arxiv.org/abs/2404.10123v1","category":"math.NA"}
{"created":"2024-04-15 20:18:44","title":"Modelling adhesion in stochastic and mean-field models of cell migration","abstract":"Adhesion between cells plays an important role in many biological processes such as tissue morphogenesis and homeostasis, wound healing and cancer cell metastasis. From a mathematical perspective, adhesion between multiple cell types has been previously analysed using discrete and continuum models including the Cellular Potts models and partial differential equations (PDEs). While these models can represent certain biological situations well, Cellular Potts models can be computationally expensive and continuum models only capture the macroscopic behaviour of a population of cells, ignoring stochasticity and the discrete nature of cell dynamics. Cellular automaton models allow us to address these problems and can be used for a wide variety of biological systems. In this paper, we consider a cellular automaton approach and develop an on-lattice agent-based model (ABM) for cell migration and adhesion in a population composed of two cell types. By deriving and comparing the corresponding PDEs to the ABM, we demonstrate that cell aggregation and cell sorting are not possible in the PDE model. Therefore, we propose a set of stochastic mean equations (SMEs) which better capture the behaviour of the ABM in one and two dimensions.","sentences":["Adhesion between cells plays an important role in many biological processes such as tissue morphogenesis and homeostasis, wound healing and cancer cell metastasis.","From a mathematical perspective, adhesion between multiple cell types has been previously analysed using discrete and continuum models including the Cellular Potts models and partial differential equations (PDEs).","While these models can represent certain biological situations well, Cellular Potts models can be computationally expensive and continuum models only capture the macroscopic behaviour of a population of cells, ignoring stochasticity and the discrete nature of cell dynamics.","Cellular automaton models allow us to address these problems and can be used for a wide variety of biological systems.","In this paper, we consider a cellular automaton approach and develop an on-lattice agent-based model (ABM) for cell migration and adhesion in a population composed of two cell types.","By deriving and comparing the corresponding PDEs to the ABM, we demonstrate that cell aggregation and cell sorting are not possible in the PDE model.","Therefore, we propose a set of stochastic mean equations (SMEs) which better capture the behaviour of the ABM in one and two dimensions."],"url":"http://arxiv.org/abs/2404.10120v1","category":"q-bio.CB"}
{"created":"2024-04-15 20:17:50","title":"Modeling scattering matrix containing evanescent modes for wavefront shaping applications in disordered media","abstract":"We developed an open-source scalar wave transport model to estimate the generalized scattering matrix (S matrix) of a disordered medium in the diffusion regime. Here, the term generalization refers to the incorporation of evanescent wave field modes in addition to propagating modes while estimating the S matrix. For that we used the scalar Kirchhoff-Helmholtz boundary integral formulation together with the Green's function perturbation method to generalize the conventional Fisher-Lee relations to include evanescent modes as well. The estimated S matrix, which satisfies generalized unitarity and reciprocity conditions, is modeled for a 2D disordered waveguide. The generalized transmission matrix contained in the S matrix is used to estimate the optimal phase-conjugate wavefront for focusing onto an evanescent mode. The phenomena of universal transmission value of 2/3 for such an optimal phase conjugate wavefront is also shown in the context of evanescent wave mode focusing through a diffusive disorder. The presented code framework may be of interest to wavefront shaping researchers for visualizing and estimating wave transport properties in general.","sentences":["We developed an open-source scalar wave transport model to estimate the generalized scattering matrix (S matrix) of a disordered medium in the diffusion regime.","Here, the term generalization refers to the incorporation of evanescent wave field modes in addition to propagating modes while estimating the S matrix.","For that we used the scalar Kirchhoff-Helmholtz boundary integral formulation together with the Green's function perturbation method to generalize the conventional Fisher-Lee relations to include evanescent modes as well.","The estimated S matrix, which satisfies generalized unitarity and reciprocity conditions, is modeled for a 2D disordered waveguide.","The generalized transmission matrix contained in the S matrix is used to estimate the optimal phase-conjugate wavefront for focusing onto an evanescent mode.","The phenomena of universal transmission value of 2/3 for such an optimal phase conjugate wavefront is also shown in the context of evanescent wave mode focusing through a diffusive disorder.","The presented code framework may be of interest to wavefront shaping researchers for visualizing and estimating wave transport properties in general."],"url":"http://arxiv.org/abs/2404.10119v1","category":"physics.optics"}
{"created":"2024-04-15 20:07:44","title":"Multiple-Input Fourier Neural Operator (MIFNO) for source-dependent 3D elastodynamics","abstract":"Numerical simulations are essential tools to evaluate the solution of the wave equation in complex settings, such as three-dimensional (3D) domains with heterogeneous properties. However, their application is limited by high computational costs and existing surrogate models lack the flexibility of numerical solvers. This work introduces the Multiple-Input Fourier Neural Operator (MIFNO) to deal with structured 3D fields representing material properties as well as vectors describing the source characteristics. The MIFNO is applied to the problem of elastic wave propagation in the Earth's crust. It is trained on the HEMEW^S-3D database containing 30000 earthquake simulations in different heterogeneous domains with random source positions and orientations. Outputs are time- and space-dependent surface wavefields. The MIFNO predictions are assessed as good to excellent based on Goodness-Of-Fit (GOF) criteria. Wave arrival times and wave fronts' propagation are very accurate since 80% of the predictions have an excellent phase GOF. The fluctuations amplitudes are good for 87% of the predictions. The envelope score is hindered by the small-scale fluctuations that are challenging to capture due to the complex physical phenomena associated with high-frequency features. Nevertheless, the MIFNO can generalize to sources located outside the training domain and it shows good generalization ability to a real complex overthrust geology. When focusing on a region of interest, transfer learning improves the accuracy with limited additional costs, since GOF scores improved by more than 1 GOF unit with only 500 additional specific samples. The MIFNO is the first surrogate model offering the flexibility of an earthquake simulator with varying sources and material properties. Its good accuracy and massive speed-up offer new perspectives to replace numerical simulations in many-query problems.","sentences":["Numerical simulations are essential tools to evaluate the solution of the wave equation in complex settings, such as three-dimensional (3D) domains with heterogeneous properties.","However, their application is limited by high computational costs and existing surrogate models lack the flexibility of numerical solvers.","This work introduces the Multiple-Input Fourier Neural Operator (MIFNO) to deal with structured 3D fields representing material properties as well as vectors describing the source characteristics.","The MIFNO is applied to the problem of elastic wave propagation in the Earth's crust.","It is trained on the HEMEW^S-3D database containing 30000 earthquake simulations in different heterogeneous domains with random source positions and orientations.","Outputs are time- and space-dependent surface wavefields.","The MIFNO predictions are assessed as good to excellent based on Goodness-Of-Fit (GOF) criteria.","Wave arrival times and wave fronts' propagation are very accurate since 80% of the predictions have an excellent phase GOF.","The fluctuations amplitudes are good for 87% of the predictions.","The envelope score is hindered by the small-scale fluctuations that are challenging to capture due to the complex physical phenomena associated with high-frequency features.","Nevertheless, the MIFNO can generalize to sources located outside the training domain and it shows good generalization ability to a real complex overthrust geology.","When focusing on a region of interest, transfer learning improves the accuracy with limited additional costs, since GOF scores improved by more than 1 GOF unit with only 500 additional specific samples.","The MIFNO is the first surrogate model offering the flexibility of an earthquake simulator with varying sources and material properties.","Its good accuracy and massive speed-up offer new perspectives to replace numerical simulations in many-query problems."],"url":"http://arxiv.org/abs/2404.10115v1","category":"cs.LG"}
{"created":"2024-04-15 19:26:26","title":"An Enhanced Hybrid HHL Algorithm","abstract":"We present a classical enhancement to improve the accuracy of the Hybrid variant (Hybrid HHL) of the quantum algorithm for solving liner systems of equations proposed by Harrow, Hassidim, and Lloyd (HHL). We achieve this by using higher precision quantum estimates of the eigenvalues relevant to the linear system, and an enhanced classical processing step to guide the eigenvalue inversion part of Hybrid HHL. We show that eigenvalue estimates with just two extra bits of precision results in tighter error bounds for our Enhanced Hybrid HHL compared to HHL. We also show that our enhancement reduces the error of Hybrid HHL by an average of 57 percent on an ideal quantum processor for a representative sample of 2x2 systems. On IBM Hanoi and IonQ Aria-1 hardware, we see that the error of Enhanced Hybrid HHL algorithm is on average 13 percent and 20 percent (respecitvely) less than that of HHL for a similar set of 2x2 systems. Finally, we use simulated eigenvalue estimates to perform an inversion of a 4x4 matrix on IonQ Aria-1 with a fidelity of 0.61. To our knowledge this is the largest HHL implementation with a fidelity greater than 0.5.","sentences":["We present a classical enhancement to improve the accuracy of the Hybrid variant (Hybrid HHL) of the quantum algorithm for solving liner systems of equations proposed by Harrow, Hassidim, and Lloyd (HHL).","We achieve this by using higher precision quantum estimates of the eigenvalues relevant to the linear system, and an enhanced classical processing step to guide the eigenvalue inversion part of Hybrid HHL.","We show that eigenvalue estimates with just two extra bits of precision results in tighter error bounds for our Enhanced Hybrid HHL compared to HHL.","We also show that our enhancement reduces the error of Hybrid HHL by an average of 57 percent on an ideal quantum processor for a representative sample of 2x2 systems.","On IBM Hanoi and IonQ Aria-1 hardware, we see that the error of Enhanced Hybrid HHL algorithm is on average 13 percent and 20 percent (respecitvely) less than that of HHL for a similar set of 2x2 systems.","Finally, we use simulated eigenvalue estimates to perform an inversion of a 4x4 matrix on IonQ Aria-1 with a fidelity of 0.61.","To our knowledge this is the largest HHL implementation with a fidelity greater than 0.5."],"url":"http://arxiv.org/abs/2404.10103v1","category":"quant-ph"}
{"created":"2024-04-15 19:18:16","title":"Quasilinear differential constraints for parabolic systems of Jordan-block type","abstract":"We prove that linear degeneracy is a necessary conditions for systems in Jordan-block form to admit a compatible quasilinear differential constraint. Such condition is also sufficient for 2x2 systems and turns out to be equivalent to possess the Hamiltonian property. Some explicit solutions of parabolic systems are herein given: two principal hierarchies arising from the associativity theory and the delta-functional reduction of the El's equation in the hard rod case are integrated.","sentences":["We prove that linear degeneracy is a necessary conditions for systems in Jordan-block form to admit a compatible quasilinear differential constraint.","Such condition is also sufficient for 2x2 systems and turns out to be equivalent to possess the Hamiltonian property.","Some explicit solutions of parabolic systems are herein given: two principal hierarchies arising from the associativity theory and the delta-functional reduction of the El's equation in the hard rod case are integrated."],"url":"http://arxiv.org/abs/2404.10101v1","category":"math-ph"}
{"created":"2024-04-15 19:06:37","title":"Synthetic Census Data Generation via Multidimensional Multiset Sum","abstract":"The US Decennial Census provides valuable data for both research and policy purposes. Census data are subject to a variety of disclosure avoidance techniques prior to release in order to preserve respondent confidentiality. While many are interested in studying the impacts of disclosure avoidance methods on downstream analyses, particularly with the introduction of differential privacy in the 2020 Decennial Census, these efforts are limited by a critical lack of data: The underlying \"microdata,\" which serve as necessary input to disclosure avoidance methods, are kept confidential.   In this work, we aim to address this limitation by providing tools to generate synthetic microdata solely from published Census statistics, which can then be used as input to any number of disclosure avoidance algorithms for the sake of evaluation and carrying out comparisons. We define a principled distribution over microdata given published Census statistics and design algorithms to sample from this distribution. We formulate synthetic data generation in this context as a knapsack-style combinatorial optimization problem and develop novel algorithms for this setting. While the problem we study is provably hard, we show empirically that our methods work well in practice, and we offer theoretical arguments to explain our performance. Finally, we verify that the data we produce are \"close\" to the desired ground truth.","sentences":["The US Decennial Census provides valuable data for both research and policy purposes.","Census data are subject to a variety of disclosure avoidance techniques prior to release in order to preserve respondent confidentiality.","While many are interested in studying the impacts of disclosure avoidance methods on downstream analyses, particularly with the introduction of differential privacy in the 2020 Decennial Census, these efforts are limited by a critical lack of data: The underlying \"microdata,\" which serve as necessary input to disclosure avoidance methods, are kept confidential.   ","In this work, we aim to address this limitation by providing tools to generate synthetic microdata solely from published Census statistics, which can then be used as input to any number of disclosure avoidance algorithms for the sake of evaluation and carrying out comparisons.","We define a principled distribution over microdata given published Census statistics and design algorithms to sample from this distribution.","We formulate synthetic data generation in this context as a knapsack-style combinatorial optimization problem and develop novel algorithms for this setting.","While the problem we study is provably hard, we show empirically that our methods work well in practice, and we offer theoretical arguments to explain our performance.","Finally, we verify that the data we produce are \"close\" to the desired ground truth."],"url":"http://arxiv.org/abs/2404.10095v1","category":"cs.CY"}
{"created":"2024-04-15 18:41:02","title":"Multi-domain spectral method for self-force calculations","abstract":"Second-order self-force calculations will be critical for modelling extreme-mass-ratio inspirals, and they are now known to have high accuracy even for binaries with mass ratios $\\sim 1:10$. Many of the challenges facing these calculations are related to slow convergence of spherical-harmonic (or spheroidal harmonic) mode sums in a region containing the small companion. In this paper, we begin to develop a multi-domain framework that can evade those problems. Building on recent work by Osburn and Nishimura, in the problematic region of spacetime we use a puncture scheme and decompose the punctured field equations into a basis of Fourier and azimuthal $m$ modes, avoiding a harmonic decomposition in the $\\theta$ direction. Outside the problematic region, we allow for a complete spherical- or spheroidal-harmonic decomposition. As a demonstration, we implement this framework in the simple context of a scalar charge in circular orbit around a Schwarzschild black hole. Our implementation utilizes several recent advances: a spectral method in each region, hyperboloidal compactification, and an extremely high-order puncture.","sentences":["Second-order self-force calculations will be critical for modelling extreme-mass-ratio inspirals, and they are now known to have high accuracy even for binaries with mass ratios","$\\sim 1:10$. Many of the challenges facing these calculations are related to slow convergence of spherical-harmonic (or spheroidal harmonic) mode sums in a region containing the small companion.","In this paper, we begin to develop a multi-domain framework that can evade those problems.","Building on recent work by Osburn and Nishimura, in the problematic region of spacetime we use a puncture scheme and decompose the punctured field equations into a basis of Fourier and azimuthal $m$ modes, avoiding a harmonic decomposition in the $\\theta$ direction.","Outside the problematic region, we allow for a complete spherical- or spheroidal-harmonic decomposition.","As a demonstration, we implement this framework in the simple context of a scalar charge in circular orbit around a Schwarzschild black hole.","Our implementation utilizes several recent advances: a spectral method in each region, hyperboloidal compactification, and an extremely high-order puncture."],"url":"http://arxiv.org/abs/2404.10083v1","category":"gr-qc"}
{"created":"2024-04-15 18:34:35","title":"Real analytic curves of almost complex structures","abstract":"We prove that, on a compact almost complex manifold, the space of almost complex structures whose Nijenhuis tensor has rank at least $k$ at every point is either empty or dense in each path-connected component of the space of almost complex structures. In particular, this applies to maximally non-integrable almost complex structures.","sentences":["We prove that, on a compact almost complex manifold, the space of almost complex structures whose Nijenhuis tensor has rank at least $k$ at every point is either empty or dense in each path-connected component of the space of almost complex structures.","In particular, this applies to maximally non-integrable almost complex structures."],"url":"http://arxiv.org/abs/2404.10079v1","category":"math.DG"}
{"created":"2024-04-15 18:14:54","title":"Solar reflection of dark matter with dark-photon mediators","abstract":"We consider the scattering of low-mass halo dark-matter particles in the hot plasma of the Sun, focusing on dark matter that interact with ordinary matter through a dark-photon mediator. The resulting ``solar-reflected'' dark matter (SRDM) component contains high-velocity particles, which significantly extend the sensitivity of terrestrial direct-detection experiments to sub-MeV dark-matter masses. We use a detailed Monte-Carlo simulation to model the propagation and scattering of dark-matter particles in the Sun, including thermal effects, with special emphasis on ultralight dark-photon mediators. We study the properties of the SRDM flux, obtain exclusion limits from various direct-detection experiments, and provide projections for future experiments, focusing especially on those with silicon and xenon targets. We find that proposed future experiments with xenon and silicon targets can probe the entire ``freeze-in benchmark,'' in which dark matter is coupled to an ultralight dark photon, including dark-matter masses as low as $\\mathcal{O}$(keV). Our simulations and SRDM fluxes are publicly available.","sentences":["We consider the scattering of low-mass halo dark-matter particles in the hot plasma of the Sun, focusing on dark matter that interact with ordinary matter through a dark-photon mediator.","The resulting ``solar-reflected'' dark matter (SRDM) component contains high-velocity particles, which significantly extend the sensitivity of terrestrial direct-detection experiments to sub-MeV dark-matter masses.","We use a detailed Monte-Carlo simulation to model the propagation and scattering of dark-matter particles in the Sun, including thermal effects, with special emphasis on ultralight dark-photon mediators.","We study the properties of the SRDM flux, obtain exclusion limits from various direct-detection experiments, and provide projections for future experiments, focusing especially on those with silicon and xenon targets.","We find that proposed future experiments with xenon and silicon targets can probe the entire ``freeze-in benchmark,'' in which dark matter is coupled to an ultralight dark photon, including dark-matter masses as low as $\\mathcal{O}$(keV).","Our simulations and SRDM fluxes are publicly available."],"url":"http://arxiv.org/abs/2404.10066v1","category":"hep-ph"}
{"created":"2024-04-15 17:57:54","title":"Asymptotic behavior of solutions of the fractional heat equation on Riemannian symmetric spaces of non-compact type","abstract":"The main goal of this paper is to study the asymptotic behaviour of solutions to the heat equation and the fractional heat equations on a Riemannian symmetric space of non-compact type. For $\\alpha \\in (0,1]$, let $h_t^\\alpha$ denote the fractional heat kernel on a Riemannian symmetric space of non-compact type $X=G/K$ (see Section \\ref{sef1}). We show that if $p \\in [1,2]$ and $f \\in L^1(X)$ is $K$-bi-invariant, then \\[\\lim_{t\\to\\infty}\\frac{\\|f\\ast h_t^\\alpha-\\what f(i \\gamma_p \\rho)h_t^\\alpha\\|_p}{\\|h_t^\\alpha\\|_p}=0,\\] where $\\gamma_p:=\\frac 2p-1$, $\\rho$ is the half sum of positive roots and $\\what f(i \\gamma_p \\rho)$ denotes the spherical Fourier transform of $f $ at $i\\gamma_p\\rho$ (see Section \\ref{prelim}). The above problems for symmetric spaces were studied recently by several authors for the case $p=1$ in some important cases \\cite{Vaz-2, AE, Eff1, Eff2}. However, we believe that our study for the case $p \\in (1,2]$ is new.","sentences":["The main goal of this paper is to study the asymptotic behaviour of solutions to the heat equation and the fractional heat equations on a Riemannian symmetric space of non-compact type.","For $\\alpha \\in (0,1]$, let $h_t^\\alpha$ denote the fractional heat kernel on a Riemannian symmetric space of non-compact type $X=G/K$ (see Section \\ref{sef1}).","We show that if $p \\in [1,2]$ and $f \\in L^1(X)$ is $K$-bi-invariant, then \\[\\lim_{t\\to\\infty}\\frac{\\|f\\ast h_t^\\alpha-\\what f(i \\gamma_p \\rho)h_t^\\alpha\\|_p}{\\|h_t^\\alpha\\|_p}=0,\\] where $\\gamma_p:=\\frac 2p-1$, $\\rho$ is the half sum of positive roots and $\\what f(i \\gamma_p \\rho)$ denotes the spherical Fourier transform of $f $ at $i\\gamma_p\\rho$ (see Section \\ref{prelim}).","The above problems for symmetric spaces were studied recently by several authors for the case $p=1$ in some important cases \\cite{Vaz-2, AE, Eff1, Eff2}.","However, we believe that our study for the case $p \\in (1,2]$ is new."],"url":"http://arxiv.org/abs/2404.09985v1","category":"math.CA"}
{"created":"2024-04-15 17:56:05","title":"One-Click Upgrade from 2D to 3D: Sandwiched RGB-D Video Compression for Stereoscopic Teleconferencing","abstract":"Stereoscopic video conferencing is still challenging due to the need to compress stereo RGB-D video in real-time. Though hardware implementations of standard video codecs such as H.264 / AVC and HEVC are widely available, they are not designed for stereoscopic videos and suffer from reduced quality and performance. Specific multiview or 3D extensions of these codecs are complex and lack efficient implementations. In this paper, we propose a new approach to upgrade a 2D video codec to support stereo RGB-D video compression, by wrapping it with a neural pre- and post-processor pair. The neural networks are end-to-end trained with an image codec proxy, and shown to work with a more sophisticated video codec. We also propose a geometry-aware loss function to improve rendering quality. We train the neural pre- and post-processors on a synthetic 4D people dataset, and evaluate it on both synthetic and real-captured stereo RGB-D videos. Experimental results show that the neural networks generalize well to unseen data and work out-of-box with various video codecs. Our approach saves about 30% bit-rate compared to a conventional video coding scheme and MV-HEVC at the same level of rendering quality from a novel view, without the need of a task-specific hardware upgrade.","sentences":["Stereoscopic video conferencing is still challenging due to the need to compress stereo RGB-D video in real-time.","Though hardware implementations of standard video codecs such as H.264 / AVC and HEVC are widely available, they are not designed for stereoscopic videos and suffer from reduced quality and performance.","Specific multiview or 3D extensions of these codecs are complex and lack efficient implementations.","In this paper, we propose a new approach to upgrade a 2D video codec to support stereo RGB-D video compression, by wrapping it with a neural pre- and post-processor pair.","The neural networks are end-to-end trained with an image codec proxy, and shown to work with a more sophisticated video codec.","We also propose a geometry-aware loss function to improve rendering quality.","We train the neural pre- and post-processors on a synthetic 4D people dataset, and evaluate it on both synthetic and real-captured stereo RGB-D videos.","Experimental results show that the neural networks generalize well to unseen data and work out-of-box with various video codecs.","Our approach saves about 30% bit-rate compared to a conventional video coding scheme and MV-HEVC at the same level of rendering quality from a novel view, without the need of a task-specific hardware upgrade."],"url":"http://arxiv.org/abs/2404.09979v1","category":"cs.CV"}
{"created":"2024-04-15 17:48:28","title":"Global solutions for cubic quasilinear Schroedinger flows in two and higher dimensions","abstract":"In recent work the authors proposed a broad global well-posedness conjecture for cubic defocusing dispersive equations in one space dimension, and then proved this conjecture in two cases, namely for one dimensional semilinear and quasilinear Schr\\\"odinger flows.   Inspired by the circle of ideas developed in the proof of the above conjecture, in this paper we expand the reach of these methods to higher dimensional quasilinear cubic Schr\\\"odinger flows. The study of this class of problems, in all dimensions, was initiated in pioneering work of Kenig-Ponce-Vega for localized initial data, and then continued by Marzuola-Metcalfe-Tataru (MMT) for initial data in Sobolev spaces.   The outcomes of this work are (i) a new, potentially sharp local well-posedness result in low regularity Sobolev spaces, one derivative below MMT and just one half derivative above scaling, (ii) a small data global well-posedness and scattering result at the same regularity level, the first result of its kind at least in two space dimensions, and (iii) a new way to think about this class of problems, which, we believe, will become the standard approach in the future.","sentences":["In recent work the authors proposed a broad global well-posedness conjecture for cubic defocusing dispersive equations in one space dimension, and then proved this conjecture in two cases, namely for one dimensional semilinear and quasilinear Schr\\\"odinger flows.   ","Inspired by the circle of ideas developed in the proof of the above conjecture, in this paper we expand the reach of these methods to higher dimensional quasilinear cubic Schr\\\"odinger flows.","The study of this class of problems, in all dimensions, was initiated in pioneering work of Kenig-Ponce-Vega for localized initial data, and then continued by Marzuola-Metcalfe-Tataru (MMT) for initial data in Sobolev spaces.   ","The outcomes of this work are (i) a new, potentially sharp local well-posedness result in low regularity Sobolev spaces, one derivative below MMT and just one half derivative above scaling, (ii) a small data global well-posedness and scattering result at the same regularity level, the first result of its kind at least in two space dimensions, and (iii) a new way to think about this class of problems, which, we believe, will become the standard approach in the future."],"url":"http://arxiv.org/abs/2404.09970v1","category":"math.AP"}
{"created":"2024-04-15 17:40:00","title":"On the differential geometry of smooth ruled surfaces in 4-space","abstract":"A smooth ruled surface in 4-space has only parabolic points or inflection points of real type. We show, by means of contact with transverse planes, that at a parabolic point, there exist two tangent directions determining two planes along which the parallel projection exhibits $\\mathcal A$-singularities of type butterfly or worse. In particular, such parabolic point can be classified as butterfly hyperbolic, parabolic, or elliptic point depending on the value of the discriminant of a binary differential equation (BDE). Also, whenever such discriminant is positive, we ensure that the integral curves of these directions form a pair of foliations on the ruled surface. Moreover, the set of points that nullify the discriminant is a regular curve transverse to the regular curve formed by inflection points of real type. Finally, using a particular projective transformation, we obtain a simple parametrization of the ruled surface such that the moduli of its 5-jet identify a butterfly hyperbolic/parabolic/elliptic point, as well as we get the stable configurations of the solutions of BDE in the discriminant curve.","sentences":["A smooth ruled surface in 4-space has only parabolic points or inflection points of real type.","We show, by means of contact with transverse planes, that at a parabolic point, there exist two tangent directions determining two planes along which the parallel projection exhibits $\\mathcal A$-singularities of type butterfly or worse.","In particular, such parabolic point can be classified as butterfly hyperbolic, parabolic, or elliptic point depending on the value of the discriminant of a binary differential equation (BDE).","Also, whenever such discriminant is positive, we ensure that the integral curves of these directions form a pair of foliations on the ruled surface.","Moreover, the set of points that nullify the discriminant is a regular curve transverse to the regular curve formed by inflection points of real type.","Finally, using a particular projective transformation, we obtain a simple parametrization of the ruled surface such that the moduli of its 5-jet identify a butterfly hyperbolic/parabolic/elliptic point, as well as we get the stable configurations of the solutions of BDE in the discriminant curve."],"url":"http://arxiv.org/abs/2404.09963v1","category":"math.DG"}
{"created":"2024-04-15 17:37:21","title":"From VIPERS to SDSS: Unveiling galaxy spectra evolution over 9 Gyr through unsupervised machine-learning","abstract":"Aims: This study aims to trace the chronological evolution of galaxy spectra over cosmic time. Focusing on the VIPERS dataset, we seek to understand the diverse population of galaxies within narrow redshift bins, comparing our findings with the previously mapped diversity of SDSS galaxies.   Methods: We use Fisher-EM, an unsupervised subspace model-based classification algorithm to classify a dataset of 79,224 spectra from the VIPERS. The dataset was divided into 26 samples by bins of redshift ranging from 0.4 - 1.2, which were classified independently. Classes of subsequent bins were linked through the k-Nearest Neighbour method to create a chronological tree of classes at different epochs.   Results: Based on the optical spectra, three main chronological galaxy branches have emerged: (i) red passive, (ii) blue star-forming, and (iii) very blue, possibly associated with AGN activity. Each of the branches differentiates into sub-branches discriminating finer properties such as D4000 break, colour, star-formation rate, and stellar masses and/or disappear with cosmic time. Notably, these classes align remarkably well with the branches identified in a previous SDSS analysis, indicating a robust and consistent classification across datasets. The chronological \"tree\" constructed from VIPERS data provides valuable insights into the temporal evolution of these spectral classes.   Conclusions: The synergy between VIPERS and SDSS datasets enhances our understanding of the evolutionary pathways of galaxy spectra. The remarkable correspondence between independently derived branches in both datasets underscores the reliability of our unsupervised machine-learning approach. The three sub-trees show complex branching structures highlighting different physical and evolutionary behaviours. This study contributes to the broader comprehension of galaxy evolution.","sentences":["Aims:","This study aims to trace the chronological evolution of galaxy spectra over cosmic time.","Focusing on the VIPERS dataset, we seek to understand the diverse population of galaxies within narrow redshift bins, comparing our findings with the previously mapped diversity of SDSS galaxies.   ","Methods: We use Fisher-EM, an unsupervised subspace model-based classification algorithm to classify a dataset of 79,224 spectra from the VIPERS.","The dataset was divided into 26 samples by bins of redshift ranging from 0.4 - 1.2, which were classified independently.","Classes of subsequent bins were linked through the k-Nearest Neighbour method to create a chronological tree of classes at different epochs.   ","Results: Based on the optical spectra, three main chronological galaxy branches have emerged: (i) red passive, (ii) blue star-forming, and (iii) very blue, possibly associated with AGN activity.","Each of the branches differentiates into sub-branches discriminating finer properties such as D4000 break, colour, star-formation rate, and stellar masses and/or disappear with cosmic time.","Notably, these classes align remarkably well with the branches identified in a previous SDSS analysis, indicating a robust and consistent classification across datasets.","The chronological \"tree\" constructed from VIPERS data provides valuable insights into the temporal evolution of these spectral classes.   ","Conclusions: The synergy between VIPERS and SDSS datasets enhances our understanding of the evolutionary pathways of galaxy spectra.","The remarkable correspondence between independently derived branches in both datasets underscores the reliability of our unsupervised machine-learning approach.","The three sub-trees show complex branching structures highlighting different physical and evolutionary behaviours.","This study contributes to the broader comprehension of galaxy evolution."],"url":"http://arxiv.org/abs/2404.09958v1","category":"astro-ph.GA"}
{"created":"2024-04-15 17:24:20","title":"Parametric Sensitivities of a Wind-driven Baroclinic Ocean Using Neural Surrogates","abstract":"Numerical models of the ocean and ice sheets are crucial for understanding and simulating the impact of greenhouse gases on the global climate. Oceanic processes affect phenomena such as hurricanes, extreme precipitation, and droughts. Ocean models rely on subgrid-scale parameterizations that require calibration and often significantly affect model skill. When model sensitivities to parameters can be computed by using approaches such as automatic differentiation, they can be used for such calibration toward reducing the misfit between model output and data. Because the SOMA model code is challenging to differentiate, we have created neural network-based surrogates for estimating the sensitivity of the ocean model to model parameters. We first generated perturbed parameter ensemble data for an idealized ocean model and trained three surrogate neural network models. The neural surrogates accurately predicted the one-step forward ocean dynamics, of which we then computed the parametric sensitivity.","sentences":["Numerical models of the ocean and ice sheets are crucial for understanding and simulating the impact of greenhouse gases on the global climate.","Oceanic processes affect phenomena such as hurricanes, extreme precipitation, and droughts.","Ocean models rely on subgrid-scale parameterizations that require calibration and often significantly affect model skill.","When model sensitivities to parameters can be computed by using approaches such as automatic differentiation, they can be used for such calibration toward reducing the misfit between model output and data.","Because the SOMA model code is challenging to differentiate, we have created neural network-based surrogates for estimating the sensitivity of the ocean model to model parameters.","We first generated perturbed parameter ensemble data for an idealized ocean model and trained three surrogate neural network models.","The neural surrogates accurately predicted the one-step forward ocean dynamics, of which we then computed the parametric sensitivity."],"url":"http://arxiv.org/abs/2404.09950v1","category":"physics.ao-ph"}
{"created":"2024-04-15 17:24:18","title":"Measurement of the differential cross section for neutral pion production in charged-current muon neutrino interactions on argon with the MicroBooNE detector","abstract":"We present a measurement of neutral pion production in charged-current interactions using data recorded with the MicroBooNE detector exposed to Fermilab's booster neutrino beam. The signal comprises one muon, one neutral pion, any number of nucleons, and no charged pions. Studying neutral pion production in the MicroBooNE detector provides an opportunity to better understand neutrino-argon interactions, and is crucial for future accelerator-based neutrino oscillation experiments. Using a dataset corresponding to $6.86 \\times 10^{20}$ protons on target, we present single-differential cross sections in muon and neutral pion momenta, scattering angles with respect to the beam for the outgoing muon and neutral pion, as well as the opening angle between the muon and neutral pion. Data extracted cross sections are compared to generator predictions. We report good agreement between the data and the models for scattering angles, except for an over-prediction by generators at muon forward angles. Similarly, the agreement between data and the models as a function of momentum is good, except for an underprediction by generators in the medium momentum ranges, $200-400$ MeV for muons and $100-200$ MeV for pions.","sentences":["We present a measurement of neutral pion production in charged-current interactions using data recorded with the MicroBooNE detector exposed to Fermilab's booster neutrino beam.","The signal comprises one muon, one neutral pion, any number of nucleons, and no charged pions.","Studying neutral pion production in the MicroBooNE detector provides an opportunity to better understand neutrino-argon interactions, and is crucial for future accelerator-based neutrino oscillation experiments.","Using a dataset corresponding to $6.86 \\times 10^{20}$ protons on target, we present single-differential cross sections in muon and neutral pion momenta, scattering angles with respect to the beam for the outgoing muon and neutral pion, as well as the opening angle between the muon and neutral pion.","Data extracted cross sections are compared to generator predictions.","We report good agreement between the data and the models for scattering angles, except for an over-prediction by generators at muon forward angles.","Similarly, the agreement between data and the models as a function of momentum is good, except for an underprediction by generators in the medium momentum ranges, $200-400$ MeV for muons and $100-200$ MeV for pions."],"url":"http://arxiv.org/abs/2404.09949v1","category":"hep-ex"}
{"created":"2024-04-15 17:22:34","title":"On the bubble-bubbleless ocean continuum and its meaning for the lidar equation: Lidar measurement of underwater bubble properties during storm conditions","abstract":"This paper presents the NRL shipboard lidar and the first lidar dataset of underwater bubbles. The meaning of these lidar observations, the algorithms used and their current limitations are discussed. The derivation of the lidar multiple scattering regime is derived from the lidar observations and theory. The detection of the underwater bubble presence and their depth is straightforward to estimate from the depolarized laser return. This dataset strongly suggest that the whitecaps term in the lidar equation formalism needs to be revisited. Void fraction retrieval is possible and the algorithm is stable with a simple ocean backscatter lidar system. The accuracy of the void fraction retrieval will increase significantly with future developments.","sentences":["This paper presents the NRL shipboard lidar and the first lidar dataset of underwater bubbles.","The meaning of these lidar observations, the algorithms used and their current limitations are discussed.","The derivation of the lidar multiple scattering regime is derived from the lidar observations and theory.","The detection of the underwater bubble presence and their depth is straightforward to estimate from the depolarized laser return.","This dataset strongly suggest that the whitecaps term in the lidar equation formalism needs to be revisited.","Void fraction retrieval is possible and the algorithm is stable with a simple ocean backscatter lidar system.","The accuracy of the void fraction retrieval will increase significantly with future developments."],"url":"http://arxiv.org/abs/2404.10033v1","category":"physics.ao-ph"}
{"created":"2024-04-15 17:01:22","title":"Beable-guided measurement theory","abstract":"Measurement model in the de Broglie-Bohm theory is developed. Following von Neumann principles, both the measurement device and the target system are described by the same quantum laws. This investigation includes basic models of coordinates and momentum measurements. Equations for pilot waves are solved analytically, and de Broglie particles trajectories are calculated numerically. The resulting model resolves the issues related to probability distribution in momentum space that were addressed in the works [Kurt Jung 2013 J. Phys.: Conf. Ser. 442 012060], [M. Nauenberg 2014 Quanta 3, 43], [D. M. Heim 2022 arXiv:2201.05971v1]. Origin of quantum contextuality in the de Broglie-Bohm theory is discussed. Further, a new problem is considered: whether the de Broglie-Bohm theory admits Heisenberg uncertainty principle in special thought experiment, where momentum and coordinate are measured in turns several times. By direct calculation, we show that uncertainty relation is restored due to turbulence-like perturbations. We consider also the measurement models which do not admit this mechanism and thus may open up new possibilities for experimental testing of the de Broglie-Bohm theory.","sentences":["Measurement model in the de Broglie-Bohm theory is developed.","Following von Neumann principles, both the measurement device and the target system are described by the same quantum laws.","This investigation includes basic models of coordinates and momentum measurements.","Equations for pilot waves are solved analytically, and de Broglie particles trajectories are calculated numerically.","The resulting model resolves the issues related to probability distribution in momentum space that were addressed in the works [Kurt Jung 2013 J. Phys.:","Conf.","Ser.","442 012060], [M. Nauenberg 2014 Quanta 3, 43], [D. M. Heim 2022 arXiv:2201.05971v1].","Origin of quantum contextuality in the de Broglie-Bohm theory is discussed.","Further, a new problem is considered: whether the de Broglie-Bohm theory admits Heisenberg uncertainty principle in special thought experiment, where momentum and coordinate are measured in turns several times.","By direct calculation, we show that uncertainty relation is restored due to turbulence-like perturbations.","We consider also the measurement models which do not admit this mechanism and thus may open up new possibilities for experimental testing of the de Broglie-Bohm theory."],"url":"http://arxiv.org/abs/2404.09934v1","category":"quant-ph"}
{"created":"2024-04-15 16:43:13","title":"Comprehensive Library of Variational LSE Solvers","abstract":"Linear systems of equations can be found in various mathematical domains, as well as in the field of machine learning. By employing noisy intermediate-scale quantum devices, variational solvers promise to accelerate finding solutions for large systems. Although there is a wealth of theoretical research on these algorithms, only fragmentary implementations exist. To fill this gap, we have developed the variational-lse-solver framework, which realizes existing approaches in literature, and introduces several enhancements. The user-friendly interface is designed for researchers that work at the abstraction level of identifying and developing end-to-end applications.","sentences":["Linear systems of equations can be found in various mathematical domains, as well as in the field of machine learning.","By employing noisy intermediate-scale quantum devices, variational solvers promise to accelerate finding solutions for large systems.","Although there is a wealth of theoretical research on these algorithms, only fragmentary implementations exist.","To fill this gap, we have developed the variational-lse-solver framework, which realizes existing approaches in literature, and introduces several enhancements.","The user-friendly interface is designed for researchers that work at the abstraction level of identifying and developing end-to-end applications."],"url":"http://arxiv.org/abs/2404.09916v1","category":"quant-ph"}
{"created":"2024-04-15 16:39:32","title":"Normalizing flows as an enhanced sampling method for atomistic supercooled liquids","abstract":"Normalizing flows can transform a simple prior probability distribution into a more complex target distribution. Here, we evaluate the ability and efficiency of generative machine learning methods to sample the Boltzmann distribution of an atomistic model for glass-forming liquids. This is a notoriously difficult task, as it amounts to ergodically exploring the complex free energy landscape of a disordered and frustrated many-body system. We optimize a normalizing flow model to successfully transform high-temperature configurations of a dense liquid into low-temperature ones, near the glass transition. We perform a detailed comparative analysis with established enhanced sampling techniques developed in the physics literature to assess and rank the performance of normalizing flows against state-of-the-art algorithms. We demonstrate that machine learning methods are very promising, showing a large speedup over conventional molecular dynamics. Normalizing flows show performances comparable to parallel tempering and population annealing, while still falling far behind the swap Monte Carlo algorithm. Our study highlights the potential of generative machine learning models in scientific computing for complex systems, but also points to some of its current limitations and the need for further improvement.","sentences":["Normalizing flows can transform a simple prior probability distribution into a more complex target distribution.","Here, we evaluate the ability and efficiency of generative machine learning methods to sample the Boltzmann distribution of an atomistic model for glass-forming liquids.","This is a notoriously difficult task, as it amounts to ergodically exploring the complex free energy landscape of a disordered and frustrated many-body system.","We optimize a normalizing flow model to successfully transform high-temperature configurations of a dense liquid into low-temperature ones, near the glass transition.","We perform a detailed comparative analysis with established enhanced sampling techniques developed in the physics literature to assess and rank the performance of normalizing flows against state-of-the-art algorithms.","We demonstrate that machine learning methods are very promising, showing a large speedup over conventional molecular dynamics.","Normalizing flows show performances comparable to parallel tempering and population annealing, while still falling far behind the swap Monte Carlo algorithm.","Our study highlights the potential of generative machine learning models in scientific computing for complex systems, but also points to some of its current limitations and the need for further improvement."],"url":"http://arxiv.org/abs/2404.09914v1","category":"cond-mat.soft"}
{"created":"2024-04-15 16:05:56","title":"Nehari manifold optimization and its application for finding unstable solutions of semilinear elliptic PDEs","abstract":"A Nehari manifold optimization method (NMOM) is introduced for finding 1-saddles, i.e., saddle points with the Morse index equal to one, of a generic nonlinear functional in Hilbert spaces. Actually, it is based on the variational characterization that 1-saddles of the generic functional are local minimizers of the same functional restricted on the associated Nehari manifold. The framework contains two important ingredients: one is the retraction mapping to make the iteration points always lie on the Nehari manifold; the other is the tangential search direction to decrease the generic functional with suitable step-size search rules. Particularly, the global convergence is rigorously established by virtue of some crucial analysis techniques (including a weak convergence method) overcoming difficulties in the infinite-dimensional setting. In practice, combining with an easy-to-implement Nehari retraction and the negative Riemannian gradient direction, the NMOM is successfully applied to compute the unstable ground-state solutions of a class of typical semilinear elliptic PDEs such as H\\'enon equation and the stationary nonlinear Schr\\\"odinger equation. In particular, the symmetry-breaking phenomenon of the ground states of H\\'enon equation is explored numerically in 1D and 2D with interesting numerical findings on the critical value of symmetry-breaking reported.","sentences":["A Nehari manifold optimization method (NMOM) is introduced for finding 1-saddles, i.e., saddle points with the Morse index equal to one, of a generic nonlinear functional in Hilbert spaces.","Actually, it is based on the variational characterization that 1-saddles of the generic functional are local minimizers of the same functional restricted on the associated Nehari manifold.","The framework contains two important ingredients: one is the retraction mapping to make the iteration points always lie on the Nehari manifold; the other is the tangential search direction to decrease the generic functional with suitable step-size search rules.","Particularly, the global convergence is rigorously established by virtue of some crucial analysis techniques (including a weak convergence method) overcoming difficulties in the infinite-dimensional setting.","In practice, combining with an easy-to-implement Nehari retraction and the negative Riemannian gradient direction, the NMOM is successfully applied to compute the unstable ground-state solutions of a class of typical semilinear elliptic PDEs such as H\\'enon equation and the stationary nonlinear Schr\\\"odinger equation.","In particular, the symmetry-breaking phenomenon of the ground states of H\\'enon equation is explored numerically in 1D and 2D with interesting numerical findings on the critical value of symmetry-breaking reported."],"url":"http://arxiv.org/abs/2404.09892v1","category":"math.NA"}
{"created":"2024-04-15 15:51:10","title":"Charmonium Transport in Ultra-Relativistic Heavy-Ion Collisions at the LHC","abstract":"We provide an update on our semi-classical transport approach for quarkonium production in high-energy heavy-ion collisions, focusing on $J/\\psi$ and $\\psi(2S)$ mesons in 5.02 TeV Pb-Pb collisions at the Large Hadron Collider (LHC) at both forward and mid-rapidity. In particular, we employ the most recent charm-production cross sections reported in pp collisions, which are pivotal for the magnitude of the regeneration contribution, and their modifications due to cold-nuclear-matter (CNM) effects. Multi-differential observables are calculated in terms of nuclear modification factors as a function of centrality, transverse momentum, and rapidity, including the contributions from bottom-decay feeddown. For our predictions for $\\psi(2S)$ production, the mechanism of sequential regeneration relative to the more strongly bound $J/\\psi$ meson plays an important role in interpreting recent ALICE data.","sentences":["We provide an update on our semi-classical transport approach for quarkonium production in high-energy heavy-ion collisions, focusing on $J/\\psi$ and $\\psi(2S)$ mesons in 5.02 TeV Pb-Pb collisions at the Large Hadron Collider (LHC) at both forward and mid-rapidity.","In particular, we employ the most recent charm-production cross sections reported in pp collisions, which are pivotal for the magnitude of the regeneration contribution, and their modifications due to cold-nuclear-matter (CNM) effects.","Multi-differential observables are calculated in terms of nuclear modification factors as a function of centrality, transverse momentum, and rapidity, including the contributions from bottom-decay feeddown.","For our predictions for $\\psi(2S)$ production, the mechanism of sequential regeneration relative to the more strongly bound $J/\\psi$ meson plays an important role in interpreting recent ALICE data."],"url":"http://arxiv.org/abs/2404.09881v1","category":"nucl-th"}
{"created":"2024-04-15 15:47:05","title":"Sample-Based Conservative Bias Linear Power Flow Approximations","abstract":"The power flow equations are central to many problems in power system planning, analysis, and control. However, their inherent non-linearity and non-convexity present substantial challenges during problem-solving processes, especially for optimization problems. Accordingly, linear approximations are commonly employed to streamline computations, although this can often entail compromises in accuracy and feasibility. This paper proposes an approach termed Conservative Bias Linear Approximations (CBLA) for addressing these limitations. By minimizing approximation errors across a specified operating range while incorporating conservativeness (over- or under-estimating quantities of interest), CBLA strikes a balance between accuracy and tractability by maintaining linear constraints. By allowing users to design loss functions tailored to the specific approximated function, the bias approximation approach significantly enhances approximation accuracy. We illustrate the effectiveness of our proposed approach through several test cases.","sentences":["The power flow equations are central to many problems in power system planning, analysis, and control.","However, their inherent non-linearity and non-convexity present substantial challenges during problem-solving processes, especially for optimization problems.","Accordingly, linear approximations are commonly employed to streamline computations, although this can often entail compromises in accuracy and feasibility.","This paper proposes an approach termed Conservative Bias Linear Approximations (CBLA) for addressing these limitations.","By minimizing approximation errors across a specified operating range while incorporating conservativeness (over- or under-estimating quantities of interest), CBLA strikes a balance between accuracy and tractability by maintaining linear constraints.","By allowing users to design loss functions tailored to the specific approximated function, the bias approximation approach significantly enhances approximation accuracy.","We illustrate the effectiveness of our proposed approach through several test cases."],"url":"http://arxiv.org/abs/2404.09876v1","category":"eess.SY"}
{"created":"2024-04-15 15:42:12","title":"Explainable Online Unsupervised Anomaly Detection for Cyber-Physical Systems via Causal Discovery from Time Series","abstract":"Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them. State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series. However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance. In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies. On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of $>10$ different anomalies. The code for experimental replication is at http://tinyurl.com/case24causal.","sentences":["Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them.","State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series.","However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance.","In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies.","On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of $>10$ different anomalies.","The code for experimental replication is at http://tinyurl.com/case24causal."],"url":"http://arxiv.org/abs/2404.09871v1","category":"cs.LG"}
{"created":"2024-04-15 15:33:07","title":"Nontrivial Massey products on compact K\u00e4hler manifolds","abstract":"We show that the bigraded quasi-isomorphism type of the bigraded, bidifferential algebra of forms on a compact K\\\"ahler manifold generally contains more information than the de Rham cohomology algebra with its real Hodge structure. More precisely, on any closed Riemann surface of genus at least two, there is a nontrivial ABC-Massey product. Furthermore, starting from dimension three, there are simply connected projective manifolds with a nonzero ABC-Massey product of three divisor classes. In particular, compact K\\\"ahler manifolds are generally not formal in the sense of pluripotential homotopy theory.","sentences":["We show that the bigraded quasi-isomorphism type of the bigraded, bidifferential algebra of forms on a compact K\\\"ahler manifold generally contains more information than the de Rham cohomology algebra with its real Hodge structure.","More precisely, on any closed Riemann surface of genus at least two, there is a nontrivial ABC-Massey product.","Furthermore, starting from dimension three, there are simply connected projective manifolds with a nonzero ABC-Massey product of three divisor classes.","In particular, compact K\\\"ahler manifolds are generally not formal in the sense of pluripotential homotopy theory."],"url":"http://arxiv.org/abs/2404.09867v1","category":"math.AT"}
{"created":"2024-04-15 15:15:36","title":"Complete totally geodesic subsets of the complex hyperbolic plane: an elementary classification","abstract":"The non-trivial complete totally geodesic submanifolds of the complex hyperbolic plane $\\mathbb H_{\\mathbb C}^2$ are the complex geodesics and the real planes. We present two new proofs for this fact. One is a short proof based on an algebraic formula for the Riemann curvature tensor due to S. Anan'in and C. Grossi and resembles the traditional proof using Lie theory. The other is purely elementary and geometric, relying on the structures in $\\mathbb H_{\\mathbb C}^2$ instead of general theories. In this second approach, we prove a slightly stronger result: the only non-trivial complete totally geodesic subsets of $\\mathbb H_{\\mathbb C}^2$ are the complex geodesics and the real planes without assuming that the subsets are submanifolds a priori. This second proof is also intriguing for only making use of elementary geometric constructions.","sentences":["The non-trivial complete totally geodesic submanifolds of the complex hyperbolic plane $\\mathbb H_{\\mathbb C}^2$ are the complex geodesics and the real planes.","We present two new proofs for this fact.","One is a short proof based on an algebraic formula for the Riemann curvature tensor due to S. Anan'in and C. Grossi and resembles the traditional proof using Lie theory.","The other is purely elementary and geometric, relying on the structures in $\\mathbb H_{\\mathbb C}^2$ instead of general theories.","In this second approach, we prove a slightly stronger result: the only non-trivial complete totally geodesic subsets of $\\mathbb H_{\\mathbb C}^2$ are the complex geodesics and the real planes without assuming that the subsets are submanifolds a priori.","This second proof is also intriguing for only making use of elementary geometric constructions."],"url":"http://arxiv.org/abs/2404.09859v1","category":"math.DG"}
{"created":"2024-04-15 15:14:00","title":"Finsler Geometry, Spacetime & Gravity -- From Metrizability of Berwald Spaces to Exact Vacuum Solutions in Finsler Gravity","abstract":"This PhD dissertation covers a range of topics in Finsler geometry and Finsler gravity, most notably: (i) the characterization of Berwald spaces, (ii) pseudo-Riemann (non-)metrizability of Berwald spaces, (iii) $(\\alpha,\\beta)$-metrics, (iv) exact solutions to Pfeifer and Wohlfarth's vacuum field equation in Finsler gravity, and (v) Finsler gravitational waves and their observational signature. An extended abstract can be found in the dissertation itself.","sentences":["This PhD dissertation covers a range of topics in Finsler geometry and Finsler gravity, most notably: (i) the characterization of Berwald spaces, (ii) pseudo-Riemann (non-)metrizability of Berwald spaces, (iii) $(\\alpha,\\beta)$-metrics, (iv) exact solutions to Pfeifer and Wohlfarth's vacuum field equation in Finsler gravity, and (v) Finsler gravitational waves and their observational signature.","An extended abstract can be found in the dissertation itself."],"url":"http://arxiv.org/abs/2404.09858v1","category":"gr-qc"}
{"created":"2024-04-15 15:01:25","title":"Guaranteed Reachability on Riemannian Manifolds for Unknown Nonlinear Systems","abstract":"Determining the reachable set for a given nonlinear system is critically important for autonomous trajectory planning for reach-avoid applications and safety critical scenarios. Providing the reachable set is generally impossible when the dynamics are unknown, so we calculate underapproximations of such sets using local dynamics at a single point and bounds on the rate of change of the dynamics determined from known physical laws. Motivated by scenarios where an adverse event causes an abrupt change in the dynamics, we attempt to determine a provably reachable set of states without knowledge of the dynamics. This paper considers systems which are known to operate on a manifold. Underapproximations are calculated by utilizing the aforementioned knowledge to derive a guaranteed set of velocities on the tangent bundle of a complete Riemannian manifold that can be reached within a finite time horizon. We then interpret said set as a control system; the trajectories of this control system provide us with a guaranteed set of reachable states the unknown system can reach within a given time. The results are general enough to apply on systems that operate on any complete Riemannian manifold. To illustrate the practical implementation of our results, we apply our algorithm to a model of a pendulum operating on a sphere and a three-dimensional rotational system which lives on the abstract set of special orthogonal matrices.","sentences":["Determining the reachable set for a given nonlinear system is critically important for autonomous trajectory planning for reach-avoid applications and safety critical scenarios.","Providing the reachable set is generally impossible when the dynamics are unknown, so we calculate underapproximations of such sets using local dynamics at a single point and bounds on the rate of change of the dynamics determined from known physical laws.","Motivated by scenarios where an adverse event causes an abrupt change in the dynamics, we attempt to determine a provably reachable set of states without knowledge of the dynamics.","This paper considers systems which are known to operate on a manifold.","Underapproximations are calculated by utilizing the aforementioned knowledge to derive a guaranteed set of velocities on the tangent bundle of a complete Riemannian manifold that can be reached within a finite time horizon.","We then interpret said set as a control system; the trajectories of this control system provide us with a guaranteed set of reachable states the unknown system can reach within a given time.","The results are general enough to apply on systems that operate on any complete Riemannian manifold.","To illustrate the practical implementation of our results, we apply our algorithm to a model of a pendulum operating on a sphere and a three-dimensional rotational system which lives on the abstract set of special orthogonal matrices."],"url":"http://arxiv.org/abs/2404.09850v1","category":"eess.SY"}
{"created":"2024-04-15 14:46:28","title":"Dirac quantum walk on tetrahedra","abstract":"Discrete-time Quantum Walks (QWs) are transportation models of single quantum particles over a lattice. Their evolution is driven through causal and local unitary operators. QWs are a powerful tool for quantum simulation of fundamental physics as some of them have a continuum limit converging to well-known physics partial differential equations, such as the Dirac or the Schr\\\"odinger equation. In this work, we show how to recover the Dirac equation in (3+1)-dimensions with a QW evolving in a tetrahedral space. This paves the way to simulate the Dirac equation on a curved spacetime. This also suggests an ordered scheme for propagating matter over a spin network, of interest in Loop Quantum Gravity where matter propagation has remained an open problem.","sentences":["Discrete-time Quantum Walks (QWs) are transportation models of single quantum particles over a lattice.","Their evolution is driven through causal and local unitary operators.","QWs are a powerful tool for quantum simulation of fundamental physics as some of them have a continuum limit converging to well-known physics partial differential equations, such as the Dirac or the Schr\\\"odinger equation.","In this work, we show how to recover the Dirac equation in (3+1)-dimensions with a QW evolving in a tetrahedral space.","This paves the way to simulate the Dirac equation on a curved spacetime.","This also suggests an ordered scheme for propagating matter over a spin network, of interest in Loop Quantum Gravity where matter propagation has remained an open problem."],"url":"http://arxiv.org/abs/2404.09840v1","category":"quant-ph"}
{"created":"2024-04-15 14:37:47","title":"On a degenerate second order traffic model: existence of discrete evolutions, deterministic many-particle limit and first order approximation","abstract":"We propose and analyse a new microscopic second order Follow-the-Leader type scheme to describe traffic flows. The main novelty of this model consists in multiplying the second order term by a nonlinear function of the global density, with the intent of considering the attentiveness of the drivers in dependence of the amount of congestion. Such term makes the system highly degenerate; indeed, coherently with the modellistic viewpoint, we allow for the nonlinearity to vanish as soon as consecutive vehicles are very close to each other. We first show existence of solutions to the degenerate discrete system. We then perform a rigorous discrete-to-continuum limit, as the number of vehicles grows larger and larger, by making use of suitable piece-wise constant approximations of the relevant macroscopic variables. The resulting continuum system turns out to be described by a degenerate pressure-less Euler-type equation, and we discuss how this could be considered an alternative to the groundbreaking Aw-Rascle-Zhang traffic model. Finally, we study the singular limit to first order dynamics in the spirit of a vanishing-inertia argument. This eventually validates the use of first order macroscopic models with nonlinear mobility to describe a congested traffic stream.","sentences":["We propose and analyse a new microscopic second order Follow-the-Leader type scheme to describe traffic flows.","The main novelty of this model consists in multiplying the second order term by a nonlinear function of the global density, with the intent of considering the attentiveness of the drivers in dependence of the amount of congestion.","Such term makes the system highly degenerate; indeed, coherently with the modellistic viewpoint, we allow for the nonlinearity to vanish as soon as consecutive vehicles are very close to each other.","We first show existence of solutions to the degenerate discrete system.","We then perform a rigorous discrete-to-continuum limit, as the number of vehicles grows larger and larger, by making use of suitable piece-wise constant approximations of the relevant macroscopic variables.","The resulting continuum system turns out to be described by a degenerate pressure-less Euler-type equation, and we discuss how this could be considered an alternative to the groundbreaking Aw-Rascle-Zhang traffic model.","Finally, we study the singular limit to first order dynamics in the spirit of a vanishing-inertia argument.","This eventually validates the use of first order macroscopic models with nonlinear mobility to describe a congested traffic stream."],"url":"http://arxiv.org/abs/2404.09834v1","category":"math.AP"}
{"created":"2024-04-15 14:21:01","title":"A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness","abstract":"While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of \\emph{bi-Lipschitzness} has been proved as a beneficial inductive bias in many areas. However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking. In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality. Its desirable properties are illustrated with concrete experiments. We also apply this framework to uncertainty estimation and monotone problem settings to illustrate its broad range of applications.","sentences":["While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood.","To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them.","Especially, throughout the past few years, the concept of \\emph{bi-Lipschitzness} has been proved as a beneficial inductive bias in many areas.","However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking.","In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality.","Its desirable properties are illustrated with concrete experiments.","We also apply this framework to uncertainty estimation and monotone problem settings to illustrate its broad range of applications."],"url":"http://arxiv.org/abs/2404.09821v1","category":"cs.LG"}
{"created":"2024-04-15 14:20:40","title":"Global-in-time weak solutions for an inviscid free surface fluid-structure problem without damping","abstract":"We consider the Cauchy problem for an inviscid irrotational fluid on a domain with a free boundary governed by a fourth order linear elasticity equation. We first derive the Craig-Sulem-Zakharov formulation of the problem and then establish the existence of a global weak solution in two space dimensions, in the general case without a damping term, for any initial data with finite energy.","sentences":["We consider the Cauchy problem for an inviscid irrotational fluid on a domain with a free boundary governed by a fourth order linear elasticity equation.","We first derive the Craig-Sulem-Zakharov formulation of the problem and then establish the existence of a global weak solution in two space dimensions, in the general case without a damping term, for any initial data with finite energy."],"url":"http://arxiv.org/abs/2404.09820v1","category":"math.AP"}
{"created":"2024-04-15 14:20:07","title":"Error Detection and Correction Codes for Safe In-Memory Computations","abstract":"In-Memory Computing (IMC) introduces a new paradigm of computation that offers high efficiency in terms of latency and power consumption for AI accelerators. However, the non-idealities and defects of emerging technologies used in advanced IMC can severely degrade the accuracy of inferred Neural Networks (NN) and lead to malfunctions in safety-critical applications. In this paper, we investigate an architectural-level mitigation technique based on the coordinated action of multiple checksum codes, to detect and correct errors at run-time. This implementation demonstrates higher efficiency in recovering accuracy across different AI algorithms and technologies compared to more traditional methods such as Triple Modular Redundancy (TMR). The results show that several configurations of our implementation recover more than 91% of the original accuracy with less than half of the area required by TMR and less than 40% of latency overhead.","sentences":["In-Memory Computing (IMC) introduces a new paradigm of computation that offers high efficiency in terms of latency and power consumption for AI accelerators.","However, the non-idealities and defects of emerging technologies used in advanced IMC can severely degrade the accuracy of inferred Neural Networks (NN) and lead to malfunctions in safety-critical applications.","In this paper, we investigate an architectural-level mitigation technique based on the coordinated action of multiple checksum codes, to detect and correct errors at run-time.","This implementation demonstrates higher efficiency in recovering accuracy across different AI algorithms and technologies compared to more traditional methods such as Triple Modular Redundancy (TMR).","The results show that several configurations of our implementation recover more than 91% of the original accuracy with less than half of the area required by TMR and less than 40% of latency overhead."],"url":"http://arxiv.org/abs/2404.09818v1","category":"cs.AR"}
{"created":"2024-04-15 14:17:34","title":"The Problem Of Image Super-Resolution, Denoising And Some Image Restoration Methods In Deep Learning Models","abstract":"In this article, we address the challenges of image super-resolution and noise reduction, which are crucial for enhancing the quality of images derived from low-resolution or noisy data. We compared and assessed several approaches for upgrading low-resolution images to higher resolutions and for eliminating unwanted noise, all while maintaining the essential characteristics of the original images and recovering images from poor quality or damaged data using deep learning models. Our analysis and the experimental outcomes on image quality metrics indicate that the EDCNN neural network model, enhanced with pretrained weights, significantly outperforms other methods with a Train PSNR of 31.215, a Valid PSNR of 29.493, and a Test PSNR of 31.6632.","sentences":["In this article, we address the challenges of image super-resolution and noise reduction, which are crucial for enhancing the quality of images derived from low-resolution or noisy data.","We compared and assessed several approaches for upgrading low-resolution images to higher resolutions and for eliminating unwanted noise, all while maintaining the essential characteristics of the original images and recovering images from poor quality or damaged data using deep learning models.","Our analysis and the experimental outcomes on image quality metrics indicate that the EDCNN neural network model, enhanced with pretrained weights, significantly outperforms other methods with a Train PSNR of 31.215, a Valid PSNR of 29.493, and a Test PSNR of 31.6632."],"url":"http://arxiv.org/abs/2404.09817v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-15 14:12:17","title":"Lax structure and tau function for large BKP hierarchy","abstract":"In this paper, we mainly investigate Lax structure and tau function for the large BKP hierarchy, which is also known as Toda hierarchy of B type, or Hirota--Ohta--coupled KP hierarchy, or Pfaff lattice. Firstly, the large BKP hierarchy can be derived from fermionic BKP hierarchy by using a special bosonization, which is presented in the form of bilinear equation. Then from bilinear equation, the corresponding Lax equation is given, where in particular the relation of flow generator with Lax operator is obtained. Also starting from Lax equation, the corresponding bilinear equation and existence of tau function are discussed. After that, large BKP hierarchy is viewed as sub--hierarchy of modified Toda (mToda) hierarchy, also called two--component first modified KP hierarchy. Finally by using two basic Miura transformations from mToda to Toda, we understand two typical relations between large BKP tau function $\\tau_n(\\mathbf{t})$ and Toda tau function $\\tau_n^{\\rm Toda}(\\mathbf{t},-\\mathbf{t})$, that is, $\\tau_n^{{\\rm Toda}}(\\mathbf{t},-{\\mathbf{t}})=\\tau_n(\\mathbf{t})\\tau_{n-1}(\\mathbf{t})$ and $\\tau_n^{{\\rm Toda}}(\\mathbf{t},-{\\mathbf{t}})=\\tau_n^2(\\mathbf{t})$. Further we find $\\big(\\tau_n(\\mathbf{t})\\tau_{n-1}(\\mathbf{t}),\\tau_n^2(\\mathbf{t})\\big)$ satisfies bilinear equation of mToda hierarchy.","sentences":["In this paper, we mainly investigate Lax structure and tau function for the large BKP hierarchy, which is also known as Toda hierarchy of B type, or Hirota--Ohta--coupled KP hierarchy, or Pfaff lattice.","Firstly, the large BKP hierarchy can be derived from fermionic BKP hierarchy by using a special bosonization, which is presented in the form of bilinear equation.","Then from bilinear equation, the corresponding Lax equation is given, where in particular the relation of flow generator with Lax operator is obtained.","Also starting from Lax equation, the corresponding bilinear equation and existence of tau function are discussed.","After that, large BKP hierarchy is viewed as sub--hierarchy of modified Toda (mToda) hierarchy, also called two--component first modified KP hierarchy.","Finally by using two basic Miura transformations from mToda to Toda, we understand two typical relations between large BKP tau function $\\tau_n(\\mathbf{t})$ and Toda tau function $\\tau_n^{\\rm Toda}(\\mathbf{t},-\\mathbf{t})$, that is, $\\tau_n^{{\\rm Toda}}(\\mathbf{t},-{\\mathbf{t}})=\\tau_n(\\mathbf{t})\\tau_{n-1}(\\mathbf{t})$ and $\\tau_n^{{\\rm Toda}}(\\mathbf{t},-{\\mathbf{t}})=\\tau_n^2(\\mathbf{t})$. Further we find $\\big(\\tau_n(\\mathbf{t})\\tau_{n-1}(\\mathbf{t}),\\tau_n^2(\\mathbf{t})\\big)$ satisfies bilinear equation of mToda hierarchy."],"url":"http://arxiv.org/abs/2404.09815v1","category":"nlin.SI"}
{"created":"2024-04-15 14:10:06","title":"Solving the Tree Containment Problem Using Graph Neural Networks","abstract":"Tree Containment is a fundamental problem in phylogenetics useful for verifying a proposed phylogenetic network, representing the evolutionary history of certain species. Tree Containment asks whether the given phylogenetic tree (for instance, constructed from a DNA fragment showing tree-like evolution) is contained in the given phylogenetic network. In the general case, this is an NP-complete problem. We propose to solve it approximately using Graph Neural Networks. In particular, we propose to combine the given network and the tree and apply a Graph Neural Network to this network-tree graph. This way, we achieve the capability of solving the tree containment instances representing a larger number of species than the instances contained in the training dataset (i.e., our algorithm has the inductive learning ability). Our algorithm demonstrates an accuracy of over $95\\%$ in solving the tree containment problem on instances with up to 100 leaves.","sentences":["Tree Containment is a fundamental problem in phylogenetics useful for verifying a proposed phylogenetic network, representing the evolutionary history of certain species.","Tree Containment asks whether the given phylogenetic tree (for instance, constructed from a DNA fragment showing tree-like evolution) is contained in the given phylogenetic network.","In the general case, this is an NP-complete problem.","We propose to solve it approximately using Graph Neural Networks.","In particular, we propose to combine the given network and the tree and apply a Graph Neural Network to this network-tree graph.","This way, we achieve the capability of solving the tree containment instances representing a larger number of species than the instances contained in the training dataset (i.e., our algorithm has the inductive learning ability).","Our algorithm demonstrates an accuracy of over $95\\%$ in solving the tree containment problem on instances with up to 100 leaves."],"url":"http://arxiv.org/abs/2404.09812v1","category":"q-bio.PE"}
{"created":"2024-04-15 14:01:57","title":"The $L_p$ dual Minkowski problem for unbounded closed convex sets","abstract":"The central focus of this paper is the $L_p$ dual Minkowski problem for $C$-compatible sets, where $C$ is a pointed closed convex cone in $\\mathbb{R}^n$ with nonempty interior. Such a problem deals with the characterization of the $(p, q)$-th dual curvature measure of a $C$-compatible set. It produces new Monge-Amp\\`{e}re equations for unbounded convex hypersurface, often defined over open domains and with non-positive unknown convex functions. Within the family of $C$-determined sets, the $L_p$ dual Minkowski problem is solved for $0\\neq p\\in \\mathbb{R}$ and $q\\in \\mathbb{R}$; while it is solved for the range of $p\\leq 0$ and $p<q$ within the newly defined family of $(C, p, q)$-close sets. When $p\\leq q$, we also obtain some results regarding the uniqueness of solutions to the $L_p$ dual Minkowski problem for $C$-compatible sets.","sentences":["The central focus of this paper is the $L_p$ dual Minkowski problem for $C$-compatible sets, where $C$ is a pointed closed convex cone in $\\mathbb{R}^n$ with nonempty interior.","Such a problem deals with the characterization of the $(p, q)$-th dual curvature measure of a $C$-compatible set.","It produces new Monge-Amp\\`{e}re equations for unbounded convex hypersurface, often defined over open domains and with non-positive unknown convex functions.","Within the family of $C$-determined sets, the $L_p$ dual Minkowski problem is solved for $0\\neq p\\in \\mathbb{R}$ and $q\\in \\mathbb{R}$; while it is solved for the range of $p\\leq 0$ and $p<q$ within the newly defined family of $(C, p, q)$-close sets.","When $p\\leq q$, we also obtain some results regarding the uniqueness of solutions to the $L_p$ dual Minkowski problem for $C$-compatible sets."],"url":"http://arxiv.org/abs/2404.09804v1","category":"math.MG"}
{"created":"2024-04-15 13:57:26","title":"A Gray-Box Stability Analysis Mechanism for Power Electronic Converters","abstract":"This paper proposes a gray-box stability analysis mechanism based on data-driven dynamic mode decomposition (DMD) for commercial grid-tied power electronics converters with limited information on its control parameters and topology. By fusing the underlying physical constraints of the state equations into data snapshots, the system dynamic state matrix and input matrix are simultaneously approximated to identify the dominant system dynamic modes and eigenvalues using the DMD with control (DMDc) algorithm. While retaining the advantages of eliminating the need for intrinsic controller information, the proposed gray-box method establishes higher accuracy and interpretable outcomes over the conventional DMD method. Finally, under experimental conditions of a low-frequency oscillation scenario in electrified railways featuring a single-phase converter, the proposed gray-box DMDc is verified to identify the dominant eigenvalues more accurately.","sentences":["This paper proposes a gray-box stability analysis mechanism based on data-driven dynamic mode decomposition (DMD) for commercial grid-tied power electronics converters with limited information on its control parameters and topology.","By fusing the underlying physical constraints of the state equations into data snapshots, the system dynamic state matrix and input matrix are simultaneously approximated to identify the dominant system dynamic modes and eigenvalues using the DMD with control (DMDc) algorithm.","While retaining the advantages of eliminating the need for intrinsic controller information, the proposed gray-box method establishes higher accuracy and interpretable outcomes over the conventional DMD method.","Finally, under experimental conditions of a low-frequency oscillation scenario in electrified railways featuring a single-phase converter, the proposed gray-box DMDc is verified to identify the dominant eigenvalues more accurately."],"url":"http://arxiv.org/abs/2404.09801v1","category":"eess.SY"}
{"created":"2024-04-15 13:51:20","title":"Taper-based scattering formulation of the Helmholtz equation to improve the training process of Physics-Informed Neural Networks","abstract":"This work addresses the scattering problem of an incident wave at a junction connecting two semi-infinite waveguides, which we intend to solve using Physics-Informed Neural Networks (PINNs). As with other deep learning-based approaches, PINNs are known to suffer from a spectral bias and from the hyperbolic nature of the Helmholtz equation. This makes the training process challenging, especially for higher wave numbers. We show an example where these limitations are present. In order to improve the learning capability of our model, we suggest an equivalent formulation of the Helmholtz Boundary Value Problem (BVP) that is based on splitting the total wave into a tapered continuation of the incoming wave and a remaining scattered wave. This allows the introduction of an inhomogeneity in the BVP, leveraging the information transmitted during back-propagation, thus, enhancing and accelerating the training process of our PINN model. The presented numerical illustrations are in accordance with the expected behavior, paving the way to a possible alternative approach to predicting scattering problems using PINNs.","sentences":["This work addresses the scattering problem of an incident wave at a junction connecting two semi-infinite waveguides, which we intend to solve using Physics-Informed Neural Networks (PINNs).","As with other deep learning-based approaches, PINNs are known to suffer from a spectral bias and from the hyperbolic nature of the Helmholtz equation.","This makes the training process challenging, especially for higher wave numbers.","We show an example where these limitations are present.","In order to improve the learning capability of our model, we suggest an equivalent formulation of the Helmholtz Boundary Value Problem (BVP) that is based on splitting the total wave into a tapered continuation of the incoming wave and a remaining scattered wave.","This allows the introduction of an inhomogeneity in the BVP, leveraging the information transmitted during back-propagation, thus, enhancing and accelerating the training process of our PINN model.","The presented numerical illustrations are in accordance with the expected behavior, paving the way to a possible alternative approach to predicting scattering problems using PINNs."],"url":"http://arxiv.org/abs/2404.09794v1","category":"cs.LG"}
{"created":"2024-04-15 13:47:12","title":"Lecture Notes on Comparison Geometry","abstract":"This note is based on Professor Vitali Kapovitch's comparison geometry course at the University of Toronto. It delves into various comparison theorems, including those by Rauch and Toponogov, focusing on their applications, such as Bishop-Gromov volume comparison, critical point theory of distance functions, diameter sphere theorem, and negative and nonnegative curvature. Additionally, it covers the soul theorem, splitting theorem, and covering theorem by Cheeger-Gromoll, as well as Perelman's proof of the soul conjecture. Finally, the note introduces Gromov-Hausdorff convergence, Alexandrov Spaces, and the Finite Homotopy type theorem by Grove-Peterson.","sentences":["This note is based on Professor Vitali Kapovitch's comparison geometry course at the University of Toronto.","It delves into various comparison theorems, including those by Rauch and Toponogov, focusing on their applications, such as Bishop-Gromov volume comparison, critical point theory of distance functions, diameter sphere theorem, and negative and nonnegative curvature.","Additionally, it covers the soul theorem, splitting theorem, and covering theorem by Cheeger-Gromoll, as well as Perelman's proof of the soul conjecture.","Finally, the note introduces Gromov-Hausdorff convergence, Alexandrov Spaces, and the Finite Homotopy type theorem by Grove-Peterson."],"url":"http://arxiv.org/abs/2404.09792v2","category":"math.MG"}
{"created":"2024-04-15 13:39:17","title":"With Andrzej Lasota there and back again","abstract":"The paper below is a written version of the 17th Andrzej Lasota Lecture presented on January 12th, 2024 in Katowice. During the lecture we tried to show the impact of Andrzej Lasota's results on the author's research concerning various fields of mathematics, including chaos and ergodicity of dynamical systems, Markov operators and semigroups and partial differential equations.","sentences":["The paper below is a written version of the 17th Andrzej Lasota Lecture presented on January 12th, 2024 in Katowice.","During the lecture we tried to show the impact of Andrzej Lasota's results on the author's research concerning various fields of mathematics, including chaos and ergodicity of dynamical systems, Markov operators and semigroups and partial differential equations."],"url":"http://arxiv.org/abs/2404.09783v1","category":"math.PR"}
{"created":"2024-04-15 13:34:27","title":"Hyperspectral Reconstruction of Skin Through Fusion of Scattering Transform Features","abstract":"Hyperspectral imagery (HSI) is an established technique with an array of applications, but its use is limited due to both practical and technical issues associated with spectral devices. The goal of the ICASSP 2024 'Hyper-Skin' Challenge is to extract skin HSI from matching RGB images and an infrared band. To address this problem we propose a model using features of the scattering transform - a type of convolutional neural network with predefined filters. Our model matches and inverts those features, rather than the pixel values, reducing the complexity of matching while grouping similar features together, resulting in an improved learning process.","sentences":["Hyperspectral imagery (HSI) is an established technique with an array of applications, but its use is limited due to both practical and technical issues associated with spectral devices.","The goal of the ICASSP 2024 'Hyper-Skin' Challenge is to extract skin HSI from matching RGB images and an infrared band.","To address this problem we propose a model using features of the scattering transform - a type of convolutional neural network with predefined filters.","Our model matches and inverts those features, rather than the pixel values, reducing the complexity of matching while grouping similar features together, resulting in an improved learning process."],"url":"http://arxiv.org/abs/2404.10030v1","category":"eess.IV"}
{"created":"2024-04-15 13:31:31","title":"A replica analysis of under-bagging","abstract":"A sharp asymptotics of the under-bagging (UB) method, which is a popular ensemble learning method for training classifiers from an imbalanced data, is derived and used to compare with several other standard methods for learning from imbalanced data, in the scenario where a linear classifier is trained from a binary mixture data. The methods compared include the under-sampling (US) method, which trains a model using a single realization of the subsampled dataset, and the simple weighting (SW) method, which trains a model with a weighted loss on the entire data. It is shown that the performance of UB is improved by increasing the size of the majority class, even if the class imbalance can be large, especially when the size of the minority class is small. This is in contrast to US, whose performance does not change as the size of the majority class increases, and SW, whose performance decreases as the imbalance increases. These results are different from the case of the naive bagging in training generalized linear models without considering the structure of class imbalance, indicating the intrinsic difference between the ensembling and the direct regularization on the parameters.","sentences":["A sharp asymptotics of the under-bagging (UB) method, which is a popular ensemble learning method for training classifiers from an imbalanced data, is derived and used to compare with several other standard methods for learning from imbalanced data, in the scenario where a linear classifier is trained from a binary mixture data.","The methods compared include the under-sampling (US) method, which trains a model using a single realization of the subsampled dataset, and the simple weighting (SW) method, which trains a model with a weighted loss on the entire data.","It is shown that the performance of UB is improved by increasing the size of the majority class, even if the class imbalance can be large, especially when the size of the minority class is small.","This is in contrast to US, whose performance does not change as the size of the majority class increases, and SW, whose performance decreases as the imbalance increases.","These results are different from the case of the naive bagging in training generalized linear models without considering the structure of class imbalance, indicating the intrinsic difference between the ensembling and the direct regularization on the parameters."],"url":"http://arxiv.org/abs/2404.09779v1","category":"stat.ML"}
{"created":"2024-04-15 13:28:25","title":"Coupled Axial and Transverse Currents Method for Finite Element Modelling of Periodic Superconductors","abstract":"In this paper, we propose the Coupled Axial and Transverse currents (I) (CATI) method, as an efficient and accurate finite element approach for modelling the electric and magnetic behavior of periodic composite superconducting conductors. The method consists of a pair of two-dimensional models coupled via circuit equations to account for the conductor geometrical periodicity. This allows to capture three-dimensional effects with two-dimensional models and leads to a significant reduction in computational time compared to conventional three-dimensional models. After presenting the method in detail, we verify it by comparison with reference finite element models, focussing on its application to twisted multifilamentary superconducting strands. In particular, we show that the CATI method captures the transition from uncoupled to coupled filaments, with accurate calculation of the interfilament coupling time constant. We then illustrate the capabilities of the method by generating a detailed loss map and magnetization curves of given strand types for a range of external transverse magnetic field excitations, with and without transport current.","sentences":["In this paper, we propose the Coupled Axial and Transverse currents (I) (CATI) method, as an efficient and accurate finite element approach for modelling the electric and magnetic behavior of periodic composite superconducting conductors.","The method consists of a pair of two-dimensional models coupled via circuit equations to account for the conductor geometrical periodicity.","This allows to capture three-dimensional effects with two-dimensional models and leads to a significant reduction in computational time compared to conventional three-dimensional models.","After presenting the method in detail, we verify it by comparison with reference finite element models, focussing on its application to twisted multifilamentary superconducting strands.","In particular, we show that the CATI method captures the transition from uncoupled to coupled filaments, with accurate calculation of the interfilament coupling time constant.","We then illustrate the capabilities of the method by generating a detailed loss map and magnetization curves of given strand types for a range of external transverse magnetic field excitations, with and without transport current."],"url":"http://arxiv.org/abs/2404.09775v2","category":"physics.acc-ph"}
{"created":"2024-04-15 13:27:27","title":"Squish Jamming","abstract":"A wide range of disordered materials, from biological to geological assemblies, feature discrete elements undergoing large shape changes. How significant geometrical variations at the microscopic scale affect the response of the assembly, in particular rigidity transitions, is an ongoing challenge in soft matter physics. However, the lack of a model granular-like experimental system featuring large and versatile particle deformability impedes advances. Here, we explore the oscillatory shear response of a sponge-like granular assembly composed of highly compressible elastic rings. We highlight a progressive rigidity transition, switching from a fluid-like to a solid-like response by increasing density or decreasing shear amplitude. The rearranging fluid state consists of crystal clusters separated by melted regions; in contrast, the solid state remains amorphous and absorbs all imposed shear elastically. We rationalise this transition by uncovering an effective, attractive shear force between rings that emerges from a friction-geometry interplay. If friction is sufficiently high compared to shear, the extent of the contacts between rings, captured analytically by elementary geometry, controls the rigidity transition.","sentences":["A wide range of disordered materials, from biological to geological assemblies, feature discrete elements undergoing large shape changes.","How significant geometrical variations at the microscopic scale affect the response of the assembly, in particular rigidity transitions, is an ongoing challenge in soft matter physics.","However, the lack of a model granular-like experimental system featuring large and versatile particle deformability impedes advances.","Here, we explore the oscillatory shear response of a sponge-like granular assembly composed of highly compressible elastic rings.","We highlight a progressive rigidity transition, switching from a fluid-like to a solid-like response by increasing density or decreasing shear amplitude.","The rearranging fluid state consists of crystal clusters separated by melted regions; in contrast, the solid state remains amorphous and absorbs all imposed shear elastically.","We rationalise this transition by uncovering an effective, attractive shear force between rings that emerges from a friction-geometry interplay.","If friction is sufficiently high compared to shear, the extent of the contacts between rings, captured analytically by elementary geometry, controls the rigidity transition."],"url":"http://arxiv.org/abs/2404.09773v1","category":"cond-mat.soft"}
{"created":"2024-04-15 13:20:02","title":"Calculating radio emissions of positive streamer phenomena using 3D simulations","abstract":"We study radio emissions from positive streamers in air using 3D simulations, from which the radiated electric field is computed by solving Jefimenko's equations. The simulations are performed at 0.5 bar using two photoionization methods: the Helmholtz approximation for a photon density and a Monte Carlo method using discrete photons, with the latter being the most realistic. We consider cases with single streamers, streamer branching, streamers interacting with preionization and streamer-streamer encounters. We do not observe a strong VHF radio signal during or after branching, which is confirmed by lab experiments. This indicates that the current inside a streamer discharges evolves approximately continuously during branching. On the other hand, stochastic fluctuations in streamer propagation due to Monte Carlo photoionization lead to more radio emission being emitted at frequencies of 100 MHz and above. Another process that leads to such high-frequency emission is the interaction of a streamer with a weakly preionized region, for example present due to a previous discharge. In agreement with previous work, we observe the strongest and highest-frequency emission from streamer encounters. The amount of total energy that is radiated seems to depend primarily on the background electric field, and less on the particular streamer evolution. Finally, we present approximations for the maximal current along a streamer channel and a fit formula for a streamer's current moment.","sentences":["We study radio emissions from positive streamers in air using 3D simulations, from which the radiated electric field is computed by solving Jefimenko's equations.","The simulations are performed at 0.5 bar using two photoionization methods: the Helmholtz approximation for a photon density and a Monte Carlo method using discrete photons, with the latter being the most realistic.","We consider cases with single streamers, streamer branching, streamers interacting with preionization and streamer-streamer encounters.","We do not observe a strong VHF radio signal during or after branching, which is confirmed by lab experiments.","This indicates that the current inside a streamer discharges evolves approximately continuously during branching.","On the other hand, stochastic fluctuations in streamer propagation due to Monte Carlo photoionization lead to more radio emission being emitted at frequencies of 100 MHz and above.","Another process that leads to such high-frequency emission is the interaction of a streamer with a weakly preionized region, for example present due to a previous discharge.","In agreement with previous work, we observe the strongest and highest-frequency emission from streamer encounters.","The amount of total energy that is radiated seems to depend primarily on the background electric field, and less on the particular streamer evolution.","Finally, we present approximations for the maximal current along a streamer channel and a fit formula for a streamer's current moment."],"url":"http://arxiv.org/abs/2404.09772v2","category":"physics.plasm-ph"}
{"created":"2024-04-15 13:13:03","title":"Electrical conductivity of QGP with quasiparticle quarks and Gribov gluon","abstract":"We investigate the electrical conductivity of a quark-gluon plasma (QGP) medium using non-perturbative resummation via Gribov gluon propagator. To calculate the electrical conductivity, we utilize the relativistic Boltzmann's kinetic equation, within the relaxation-time approximation (RTA). The relaxation times are obtained by computing the microscopic two-body scattering amplitude. We adopt the quasiparticle approach, which allows us to comprehend the transport properties in both the weak and strong coupling regimes. Above the transition temperature, we estimate the electrical conductivity of the quark-gluon plasma using the Gribov prescription and compare our findings with lattice results and various phenomenological models. We find our results in close agreement with the lattice data.","sentences":["We investigate the electrical conductivity of a quark-gluon plasma (QGP) medium using non-perturbative resummation via Gribov gluon propagator.","To calculate the electrical conductivity, we utilize the relativistic Boltzmann's kinetic equation, within the relaxation-time approximation (RTA).","The relaxation times are obtained by computing the microscopic two-body scattering amplitude.","We adopt the quasiparticle approach, which allows us to comprehend the transport properties in both the weak and strong coupling regimes.","Above the transition temperature, we estimate the electrical conductivity of the quark-gluon plasma using the Gribov prescription and compare our findings with lattice results and various phenomenological models.","We find our results in close agreement with the lattice data."],"url":"http://arxiv.org/abs/2404.09767v1","category":"hep-ph"}
{"created":"2024-04-15 13:11:12","title":"Corrections of minor misstatements in several papers on ECS manifolds","abstract":"In the paper [3] the value of the invariant denoted by d and often called \"rank\" was misstated for a narrow class of examples of ECS manifolds: they were identified as having d = 1 instead of the correct value d = 2. The error was repeated, by citing [3], in [1], [2] and [4] -- [7]. We briefly describe the class in question, explain why it has d = 2, and list the required corrections of the affected papers.","sentences":["In the paper [3] the value of the invariant denoted by d and often called \"rank\" was misstated for a narrow class of examples of ECS manifolds: they were identified as having d = 1 instead of the correct value d","= 2.","The error was repeated, by citing [3], in [1], [2] and [4] --","[7].","We briefly describe the class in question, explain why it has d = 2, and list the required corrections of the affected papers."],"url":"http://arxiv.org/abs/2404.09766v1","category":"math.DG"}
{"created":"2024-04-15 13:00:09","title":"Transforming a Non-Differentiable Rasterizer into a Differentiable One with Stochastic Gradient Estimation","abstract":"We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow). We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended. This method is simple and robust but does not scale in dimensionality (number of scene parameters). Our insight is that the number of parameters contributing to a given rasterized pixel is bounded. Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable. Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available. With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters.","sentences":["We show how to transform a non-differentiable rasterizer into a differentiable one with minimal engineering efforts and no external dependencies (no Pytorch/Tensorflow).","We rely on Stochastic Gradient Estimation, a technique that consists of rasterizing after randomly perturbing the scene's parameters such that their gradient can be stochastically estimated and descended.","This method is simple and robust but does not scale in dimensionality (number of scene parameters).","Our insight is that the number of parameters contributing to a given rasterized pixel is bounded.","Estimating and averaging gradients on a per-pixel basis hence bounds the dimensionality of the underlying optimization problem and makes the method scalable.","Furthermore, it is simple to track per-pixel contributing parameters by rasterizing ID- and UV-buffers, which are trivial additions to a rasterization engine if not already available.","With these minor modifications, we obtain an in-engine optimizer for 3D assets with millions of geometry and texture parameters."],"url":"http://arxiv.org/abs/2404.09758v1","category":"cs.GR"}
{"created":"2024-04-16 17:36:21","title":"Laplace-HDC: Understanding the geometry of binary hyperdimensional computing","abstract":"This paper studies the geometry of binary hyperdimensional computing (HDC), a computational scheme in which data are encoded using high-dimensional binary vectors. We establish a result about the similarity structure induced by the HDC binding operator and show that the Laplace kernel naturally arises in this setting, motivating our new encoding method Laplace-HDC, which improves upon previous methods. We describe how our results indicate limitations of binary HDC in encoding spatial information from images and discuss potential solutions, including using Haar convolutional features and the definition of a translation-equivariant HDC encoding. Several numerical experiments highlighting the improved accuracy of Laplace-HDC in contrast to alternative methods are presented. We also numerically study other aspects of the proposed framework such as robustness and the underlying translation-equivariant encoding.","sentences":["This paper studies the geometry of binary hyperdimensional computing (HDC), a computational scheme in which data are encoded using high-dimensional binary vectors.","We establish a result about the similarity structure induced by the HDC binding operator and show that the Laplace kernel naturally arises in this setting, motivating our new encoding method Laplace-HDC, which improves upon previous methods.","We describe how our results indicate limitations of binary HDC in encoding spatial information from images and discuss potential solutions, including using Haar convolutional features and the definition of a translation-equivariant HDC encoding.","Several numerical experiments highlighting the improved accuracy of Laplace-HDC in contrast to alternative methods are presented.","We also numerically study other aspects of the proposed framework such as robustness and the underlying translation-equivariant encoding."],"url":"http://arxiv.org/abs/2404.10759v1","category":"cs.LG"}
{"created":"2024-04-16 17:35:35","title":"Watch Your Step: Optimal Retrieval for Continual Learning at Scale","abstract":"One of the most widely used approaches in continual learning is referred to as replay. Replay methods support interleaved learning by storing past experiences in a replay buffer. Although there are methods for selectively constructing the buffer and reprocessing its contents, there is limited exploration of the problem of selectively retrieving samples from the buffer. Current solutions have been tested in limited settings and, more importantly, in isolation. Existing work has also not explored the impact of duplicate replays on performance. In this work, we propose a framework for evaluating selective retrieval strategies, categorized by simple, independent class- and sample-selective primitives. We evaluated several combinations of existing strategies for selective retrieval and present their performances. Furthermore, we propose a set of strategies to prevent duplicate replays and explore whether new samples with low loss values can be learned without replay. In an effort to match our problem setting to a realistic continual learning pipeline, we restrict our experiments to a setting involving a large, pre-trained, open vocabulary object detection model, which is fully fine-tuned on a sequence of 15 datasets.","sentences":["One of the most widely used approaches in continual learning is referred to as replay.","Replay methods support interleaved learning by storing past experiences in a replay buffer.","Although there are methods for selectively constructing the buffer and reprocessing its contents, there is limited exploration of the problem of selectively retrieving samples from the buffer.","Current solutions have been tested in limited settings and, more importantly, in isolation.","Existing work has also not explored the impact of duplicate replays on performance.","In this work, we propose a framework for evaluating selective retrieval strategies, categorized by simple, independent class- and sample-selective primitives.","We evaluated several combinations of existing strategies for selective retrieval and present their performances.","Furthermore, we propose a set of strategies to prevent duplicate replays and explore whether new samples with low loss values can be learned without replay.","In an effort to match our problem setting to a realistic continual learning pipeline, we restrict our experiments to a setting involving a large, pre-trained, open vocabulary object detection model, which is fully fine-tuned on a sequence of 15 datasets."],"url":"http://arxiv.org/abs/2404.10758v1","category":"cs.CV"}
{"created":"2024-04-16 17:35:25","title":"Deep Learning and LLM-based Methods Applied to Stellar Lightcurve Classification","abstract":"Light curves serve as a valuable source of information on stellar formation and evolution. With the rapid advancement of machine learning techniques, it can be effectively processed to extract astronomical patterns and information. In this study, we present a comprehensive evaluation of deep-learning and large language model (LLM) based models for the automatic classification of variable star light curves, based on large datasets from the Kepler and K2 missions. Special emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries, examining the influence of observational cadence and phase distribution on classification precision. Employing AutoDL optimization, we achieve striking performance with the 1D-Convolution+BiLSTM architecture and the Swin Transformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the latter demonstrating a notable 83\\% accuracy in discerning the elusive Type II Cepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper LightCurve (LC), an innovative Series comprising three LLM-based models: LLM, multimodal large language model (MLLM), and Large Audio Language Model (LALM). Each model is fine-tuned with strategic prompt engineering and customized training methods to explore the emergent abilities of these models for astronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies around 90\\%, significantly reducing the need for explicit feature engineering, thereby paving the way for streamlined parallel data processing and the progression of multifaceted multimodal models in astronomical applications. The study furnishes two detailed catalogs illustrating the impacts of phase and sampling intervals on deep learning classification accuracy, showing that a substantial decrease of up to 14\\% in observation duration and 21\\% in sampling points can be realized without compromising accuracy by more than 10\\%.","sentences":["Light curves serve as a valuable source of information on stellar formation and evolution.","With the rapid advancement of machine learning techniques, it can be effectively processed to extract astronomical patterns and information.","In this study, we present a comprehensive evaluation of deep-learning and large language model (LLM) based models for the automatic classification of variable star light curves, based on large datasets from the Kepler and K2 missions.","Special emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries, examining the influence of observational cadence and phase distribution on classification precision.","Employing AutoDL optimization, we achieve striking performance with the 1D-Convolution+BiLSTM architecture and the Swin Transformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the latter demonstrating a notable 83\\% accuracy in discerning the elusive Type II Cepheids-comprising merely 0.02\\% of the total dataset.","We unveil StarWhisper LightCurve (LC), an innovative Series comprising three LLM-based models: LLM, multimodal large language model (MLLM), and Large Audio Language Model (LALM).","Each model is fine-tuned with strategic prompt engineering and customized training methods to explore the emergent abilities of these models for astronomical data.","Remarkably, StarWhisper LC Series exhibit high accuracies around 90\\%, significantly reducing the need for explicit feature engineering, thereby paving the way for streamlined parallel data processing and the progression of multifaceted multimodal models in astronomical applications.","The study furnishes two detailed catalogs illustrating the impacts of phase and sampling intervals on deep learning classification accuracy, showing that a substantial decrease of up to 14\\% in observation duration and 21\\% in sampling points can be realized without compromising accuracy by more than 10\\%."],"url":"http://arxiv.org/abs/2404.10757v1","category":"astro-ph.IM"}
{"created":"2024-04-16 16:45:47","title":"Dynamic Frequency-Based Fingerprinting Attacks against Modern Sandbox Environments","abstract":"The cloud computing landscape has evolved significantly in recent years, embracing various sandboxes to meet the diverse demands of modern cloud applications. These sandboxes encompass container-based technologies like Docker and gVisor, microVM-based solutions like Firecracker, and security-centric sandboxes relying on Trusted Execution Environments (TEEs) such as Intel SGX and AMD SEV. However, the practice of placing multiple tenants on shared physical hardware raises security and privacy concerns, most notably side-channel attacks.   In this paper, we investigate the possibility of fingerprinting containers through CPU frequency reporting sensors in Intel and AMD CPUs. One key enabler of our attack is that the current CPU frequency information can be accessed by user-space attackers. We demonstrate that Docker images exhibit a unique frequency signature, enabling the distinction of different containers with up to 84.5% accuracy even when multiple containers are running simultaneously in different cores. Additionally, we assess the effectiveness of our attack when performed against several sandboxes deployed in cloud environments, including Google's gVisor, AWS' Firecracker, and TEE-based platforms like Gramine (utilizing Intel SGX) and AMD SEV. Our empirical results show that these attacks can also be carried out successfully against all of these sandboxes in less than 40 seconds, with an accuracy of over 70% in all cases. Finally, we propose a noise injection-based countermeasure to mitigate the proposed attack on cloud environments.","sentences":["The cloud computing landscape has evolved significantly in recent years, embracing various sandboxes to meet the diverse demands of modern cloud applications.","These sandboxes encompass container-based technologies like Docker and gVisor, microVM-based solutions like Firecracker, and security-centric sandboxes relying on Trusted Execution Environments (TEEs) such as Intel SGX and AMD SEV.","However, the practice of placing multiple tenants on shared physical hardware raises security and privacy concerns, most notably side-channel attacks.   ","In this paper, we investigate the possibility of fingerprinting containers through CPU frequency reporting sensors in Intel and AMD CPUs.","One key enabler of our attack is that the current CPU frequency information can be accessed by user-space attackers.","We demonstrate that Docker images exhibit a unique frequency signature, enabling the distinction of different containers with up to 84.5% accuracy even when multiple containers are running simultaneously in different cores.","Additionally, we assess the effectiveness of our attack when performed against several sandboxes deployed in cloud environments, including Google's gVisor, AWS' Firecracker, and TEE-based platforms like Gramine (utilizing Intel SGX) and AMD SEV.","Our empirical results show that these attacks can also be carried out successfully against all of these sandboxes in less than 40 seconds, with an accuracy of over 70% in all cases.","Finally, we propose a noise injection-based countermeasure to mitigate the proposed attack on cloud environments."],"url":"http://arxiv.org/abs/2404.10715v1","category":"cs.CR"}
{"created":"2024-04-16 16:15:19","title":"Integrating knowledge bases to improve coreference and bridging resolution for the chemical domain","abstract":"Resolving coreference and bridging relations in chemical patents is important for better understanding the precise chemical process, where chemical domain knowledge is very critical. We proposed an approach incorporating external knowledge into a multi-task learning model for both coreference and bridging resolution in the chemical domain. The results show that integrating external knowledge can benefit both chemical coreference and bridging resolution.","sentences":["Resolving coreference and bridging relations in chemical patents is important for better understanding the precise chemical process, where chemical domain knowledge is very critical.","We proposed an approach incorporating external knowledge into a multi-task learning model for both coreference and bridging resolution in the chemical domain.","The results show that integrating external knowledge can benefit both chemical coreference and bridging resolution."],"url":"http://arxiv.org/abs/2404.10696v1","category":"cs.CL"}
{"created":"2024-04-16 16:10:23","title":"MathWriting: A Dataset For Handwritten Mathematical Expression Recognition","abstract":"We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of 230k human-written samples and an additional 400k synthetic ones. MathWriting can also be used for offline HME recognition and is larger than all existing offline HME datasets like IM2LATEX-100K. We introduce a benchmark based on MathWriting data in order to advance research on both online and offline HME recognition.","sentences":["We introduce MathWriting, the largest online handwritten mathematical expression dataset to date.","It consists of 230k human-written samples and an additional 400k synthetic ones.","MathWriting can also be used for offline HME recognition and is larger than all existing offline HME datasets like IM2LATEX-100K.","We introduce a benchmark based on MathWriting data in order to advance research on both online and offline HME recognition."],"url":"http://arxiv.org/abs/2404.10690v1","category":"cs.CV"}
{"created":"2024-04-16 15:16:22","title":"Self-playing Adversarial Language Game Enhances LLM Reasoning","abstract":"We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate with respect to a target word only visible to the attacker. The attacker aims to induce the defender to utter the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players should have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Play in this Adversarial language Game (SPAG). With this goal, we let LLMs act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performance uniformly improves on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLM's reasoning ability. The code is at https://github.com/Linear95/SPAG.","sentences":["We explore the self-play training procedure of large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo.","In this game, an attacker and a defender communicate with respect to a target word only visible to the attacker.","The attacker aims to induce the defender to utter the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances.","To win the game, both players should have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation.","Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Play in this Adversarial language Game (SPAG).","With this goal, we let LLMs act as the attacker and play with a copy of itself as the defender on an extensive range of target words.","Through reinforcement learning on the game outcomes, we observe that the LLMs' performance uniformly improves on a broad range of reasoning benchmarks.","Furthermore, iteratively adopting this self-play process can continuously promote LLM's reasoning ability.","The code is at https://github.com/Linear95/SPAG."],"url":"http://arxiv.org/abs/2404.10642v1","category":"cs.CL"}
{"created":"2024-04-16 15:04:55","title":"Contextrast: Contextual Contrastive Learning for Semantic Segmentation","abstract":"Despite great improvements in semantic segmentation, challenges persist because of the lack of local/global contexts and the relationship between them. In this paper, we propose Contextrast, a contrastive learning-based semantic segmentation method that allows to capture local/global contexts and comprehend their relationships. Our proposed method comprises two parts: a) contextual contrastive learning (CCL) and b) boundary-aware negative (BANE) sampling. Contextual contrastive learning obtains local/global context from multi-scale feature aggregation and inter/intra-relationship of features for better discrimination capabilities. Meanwhile, BANE sampling selects embedding features along the boundaries of incorrectly predicted regions to employ them as harder negative samples on our contrastive learning, resolving segmentation issues along the boundary region by exploiting fine-grained details. We demonstrate that our Contextrast substantially enhances the performance of semantic segmentation networks, outperforming state-of-the-art contrastive learning approaches on diverse public datasets, e.g. Cityscapes, CamVid, PASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational cost during inference.","sentences":["Despite great improvements in semantic segmentation, challenges persist because of the lack of local/global contexts and the relationship between them.","In this paper, we propose Contextrast, a contrastive learning-based semantic segmentation method that allows to capture local/global contexts and comprehend their relationships.","Our proposed method comprises two parts: a) contextual contrastive learning (CCL) and b) boundary-aware negative (BANE) sampling.","Contextual contrastive learning obtains local/global context from multi-scale feature aggregation and inter/intra-relationship of features for better discrimination capabilities.","Meanwhile, BANE sampling selects embedding features along the boundaries of incorrectly predicted regions to employ them as harder negative samples on our contrastive learning, resolving segmentation issues along the boundary region by exploiting fine-grained details.","We demonstrate that our Contextrast substantially enhances the performance of semantic segmentation networks, outperforming state-of-the-art contrastive learning approaches on diverse public datasets, e.g. Cityscapes, CamVid, PASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational cost during inference."],"url":"http://arxiv.org/abs/2404.10633v1","category":"cs.CV"}
{"created":"2024-04-16 14:05:29","title":"Data-driven subgrouping of patient trajectories with chronic diseases: Evidence from low back pain","abstract":"Clinical data informs the personalization of health care with a potential for more effective disease management. In practice, this is achieved by subgrouping, whereby clusters with similar patient characteristics are identified and then receive customized treatment plans with the goal of targeting subgroup-specific disease dynamics. In this paper, we propose a novel mixture hidden Markov model for subgrouping patient trajectories from chronic diseases. Our model is probabilistic and carefully designed to capture different trajectory phases of chronic diseases (i.e., \"severe\", \"moderate\", and \"mild\") through tailored latent states. We demonstrate our subgrouping framework based on a longitudinal study across 847 patients with non-specific low back pain. Here, our subgrouping framework identifies 8 subgroups. Further, we show that our subgrouping framework outperforms common baselines in terms of cluster validity indices. Finally, we discuss the applicability of the model to other chronic and long-lasting diseases.","sentences":["Clinical data informs the personalization of health care with a potential for more effective disease management.","In practice, this is achieved by subgrouping, whereby clusters with similar patient characteristics are identified and then receive customized treatment plans with the goal of targeting subgroup-specific disease dynamics.","In this paper, we propose a novel mixture hidden Markov model for subgrouping patient trajectories from chronic diseases.","Our model is probabilistic and carefully designed to capture different trajectory phases of chronic diseases (i.e., \"severe\", \"moderate\", and \"mild\") through tailored latent states.","We demonstrate our subgrouping framework based on a longitudinal study across 847 patients with non-specific low back pain.","Here, our subgrouping framework identifies 8 subgroups.","Further, we show that our subgrouping framework outperforms common baselines in terms of cluster validity indices.","Finally, we discuss the applicability of the model to other chronic and long-lasting diseases."],"url":"http://arxiv.org/abs/2404.10580v1","category":"stat.AP"}
{"created":"2024-04-16 13:47:27","title":"Label merge-and-split: A graph-colouring approach for memory-efficient brain parcellation","abstract":"Whole brain parcellation requires inferring hundreds of segmentation labels in large image volumes and thus presents significant practical challenges for deep learning approaches. We introduce label merge-and-split, a method that first greatly reduces the effective number of labels required for learning-based whole brain parcellation and then recovers original labels. Using a greedy graph colouring algorithm, our method automatically groups and merges multiple spatially separate labels prior to model training and inference. The merged labels may be semantically unrelated. A deep learning model is trained to predict merged labels. At inference time, original labels are restored using atlas-based influence regions. In our experiments, the proposed approach reduces the number of labels by up to 68% while achieving segmentation accuracy comparable to the baseline method without label merging and splitting. Moreover, model training and inference times as well as GPU memory requirements were reduced significantly. The proposed method can be applied to all semantic segmentation tasks with a large number of spatially separate classes within an atlas-based prior.","sentences":["Whole brain parcellation requires inferring hundreds of segmentation labels in large image volumes and thus presents significant practical challenges for deep learning approaches.","We introduce label merge-and-split, a method that first greatly reduces the effective number of labels required for learning-based whole brain parcellation and then recovers original labels.","Using a greedy graph colouring algorithm, our method automatically groups and merges multiple spatially separate labels prior to model training and inference.","The merged labels may be semantically unrelated.","A deep learning model is trained to predict merged labels.","At inference time, original labels are restored using atlas-based influence regions.","In our experiments, the proposed approach reduces the number of labels by up to 68% while achieving segmentation accuracy comparable to the baseline method without label merging and splitting.","Moreover, model training and inference times as well as GPU memory requirements were reduced significantly.","The proposed method can be applied to all semantic segmentation tasks with a large number of spatially separate classes within an atlas-based prior."],"url":"http://arxiv.org/abs/2404.10572v1","category":"cs.CV"}
{"created":"2024-04-16 11:38:55","title":"Membership determination in open clusters using the DBSCAN Clustering Algorithm","abstract":"In this paper, we apply the machine learning clustering algorithm Density Based Spatial Clustering of Applications with Noise (DBSCAN) to study the membership of stars in twelve open clusters (NGC~2264, NGC~2682, NGC~2244, NGC~3293, NGC~6913, NGC~7142, IC~1805, NGC~6231, NGC~2243, NGC 6451, NGC 6005 and NGC 6583) based on Gaia DR3 Data. This sample of clusters spans a variety of parameters like age, metallicity, distance, extinction and a wide parameter space in proper motions and parallaxes. We obtain reliable cluster members using DBSCAN as faint as $G \\sim 20$ mag and also in the outer regions of clusters. With our revised membership list, we plot color-magnitude diagrams and we obtain cluster parameters for our sample using ASteCA and compare it with the catalog values. We also validate our membership sample by spectroscopic data from APOGEE and GALAH for the available data. This paper demonstrates the effectiveness of DBSCAN in membership determination of clusters.","sentences":["In this paper, we apply the machine learning clustering algorithm Density Based Spatial Clustering of Applications with Noise (DBSCAN) to study the membership of stars in twelve open clusters (NGC~2264, NGC~2682, NGC~2244, NGC~3293, NGC~6913, NGC~7142, IC~1805, NGC~6231, NGC~2243, NGC 6451, NGC 6005 and NGC 6583) based on Gaia DR3 Data.","This sample of clusters spans a variety of parameters like age, metallicity, distance, extinction and a wide parameter space in proper motions and parallaxes.","We obtain reliable cluster members using DBSCAN as faint as $G \\sim 20$ mag and also in the outer regions of clusters.","With our revised membership list, we plot color-magnitude diagrams and we obtain cluster parameters for our sample using ASteCA and compare it with the catalog values.","We also validate our membership sample by spectroscopic data from APOGEE and GALAH for the available data.","This paper demonstrates the effectiveness of DBSCAN in membership determination of clusters."],"url":"http://arxiv.org/abs/2404.10477v1","category":"astro-ph.GA"}
{"created":"2024-04-16 10:30:52","title":"Semi-supervised Fr\u00e9chet Regression","abstract":"This paper explores the field of semi-supervised Fr\\'echet regression, driven by the significant costs associated with obtaining non-Euclidean labels. Methodologically, we propose two novel methods: semi-supervised NW Fr\\'echet regression and semi-supervised kNN Fr\\'echet regression, both based on graph distance acquired from all feature instances. These methods extend the scope of existing semi-supervised Euclidean regression methods. We establish their convergence rates with limited labeled data and large amounts of unlabeled data, taking into account the low-dimensional manifold structure of the feature space. Through comprehensive simulations across diverse settings and applications to real data, we demonstrate the superior performance of our methods over their supervised counterparts. This study addresses existing research gaps and paves the way for further exploration and advancements in the field of semi-supervised Fr\\'echet regression.","sentences":["This paper explores the field of semi-supervised Fr\\'echet regression, driven by the significant costs associated with obtaining non-Euclidean labels.","Methodologically, we propose two novel methods: semi-supervised NW Fr\\'echet regression and semi-supervised kNN Fr\\'echet regression, both based on graph distance acquired from all feature instances.","These methods extend the scope of existing semi-supervised Euclidean regression methods.","We establish their convergence rates with limited labeled data and large amounts of unlabeled data, taking into account the low-dimensional manifold structure of the feature space.","Through comprehensive simulations across diverse settings and applications to real data, we demonstrate the superior performance of our methods over their supervised counterparts.","This study addresses existing research gaps and paves the way for further exploration and advancements in the field of semi-supervised Fr\\'echet regression."],"url":"http://arxiv.org/abs/2404.10444v1","category":"math.ST"}
{"created":"2024-04-16 08:25:36","title":"Learning to Score Sign Language with Two-stage Method","abstract":"Human action recognition and performance assessment have been hot research topics in recent years. Recognition problems have mature solutions in the field of sign language, but past research in performance analysis has focused on competitive sports and medical training, overlooking the scoring assessment ,which is an important part of sign language teaching digitalization. In this paper, we analyze the existing technologies for performance assessment and adopt methods that perform well in human pose reconstruction tasks combined with motion rotation embedded expressions, proposing a two-stage sign language performance evaluation pipeline. Our analysis shows that choosing reconstruction tasks in the first stage can provide more expressive features, and using smoothing methods can provide an effective reference for assessment. Experiments show that our method provides good score feedback mechanisms and high consistency with professional assessments compared to end-to-end evaluations.","sentences":["Human action recognition and performance assessment have been hot research topics in recent years.","Recognition problems have mature solutions in the field of sign language, but past research in performance analysis has focused on competitive sports and medical training, overlooking the scoring assessment ,which is an important part of sign language teaching digitalization.","In this paper, we analyze the existing technologies for performance assessment and adopt methods that perform well in human pose reconstruction tasks combined with motion rotation embedded expressions, proposing a two-stage sign language performance evaluation pipeline.","Our analysis shows that choosing reconstruction tasks in the first stage can provide more expressive features, and using smoothing methods can provide an effective reference for assessment.","Experiments show that our method provides good score feedback mechanisms and high consistency with professional assessments compared to end-to-end evaluations."],"url":"http://arxiv.org/abs/2404.10383v2","category":"cs.CV"}
{"created":"2024-04-16 07:53:54","title":"A Survey on Data-Driven Fault Diagnostic Techniques for Marine Diesel Engines","abstract":"Fault diagnosis in marine diesel engines is vital for maritime safety and operational efficiency.These engines are integral to marine vessels, and their reliable performance is crucial for safenavigation. Swift identification and resolution of faults are essential to prevent breakdowns,enhance safety, and reduce the risk of catastrophic failures at sea. Proactive fault diagnosisfacilitates timely maintenance, minimizes downtime, and ensures the overall reliability andlongevity of marine diesel engines. This paper explores the importance of fault diagnosis,emphasizing subsystems, common faults, and recent advancements in data-driven approachesfor effective marine diesel engine maintenance","sentences":["Fault diagnosis in marine diesel engines is vital for maritime safety and operational efficiency.","These engines are integral to marine vessels, and their reliable performance is crucial for safenavigation.","Swift identification and resolution of faults are essential to prevent breakdowns,enhance safety, and reduce the risk of catastrophic failures at sea.","Proactive fault diagnosisfacilitates timely maintenance, minimizes downtime, and ensures the overall reliability andlongevity of marine diesel engines.","This paper explores the importance of fault diagnosis,emphasizing subsystems, common faults, and recent advancements in data-driven approachesfor effective marine diesel engine maintenance"],"url":"http://arxiv.org/abs/2404.10363v1","category":"cs.LG"}
{"created":"2024-04-16 07:24:54","title":"Asset management, condition monitoring and Digital Twins: damage detection and virtual inspection on a reinforced concrete bridge","abstract":"In April 2021 Stava bridge, a main bridge on E6 in Norway, was abruptly closed for traffic. A structural defect had seriously compromised the bridge structural integrity. The Norwegian Public Roads Administration (NPRA) closed it, made a temporary solution and reopened with severe traffic restrictions. The incident was alerted through what constitutes the bridge Digital Twin processing data from Internet of Things sensors. The solution was crucial in online and offline diagnostics, the case demonstrating the value of technologies to tackle emerging dangerous situations as well as acting preventively. A critical and rapidly developing damage was detected in time to stop the development, but not in time to avoid the incident altogether. The paper puts risk in a broader perspective for an organization responsible for highway infrastructure. It positions online monitoring and Digital Twins in the context of Risk- and Condition-Based Maintenance. The situation that arose at Stava bridge, and how it was detected, analyzed, and diagnosed during virtual inspection, is described. The case demonstrates how combining physics-based methods with Machine Learning can facilitate damage detection and diagnostics. A summary of lessons learnt, both from technical and organizational perspectives, as well as plans of future work, is presented.","sentences":["In April 2021 Stava bridge, a main bridge on E6 in Norway, was abruptly closed for traffic.","A structural defect had seriously compromised the bridge structural integrity.","The Norwegian Public Roads Administration (NPRA) closed it, made a temporary solution and reopened with severe traffic restrictions.","The incident was alerted through what constitutes the bridge Digital Twin processing data from Internet of Things sensors.","The solution was crucial in online and offline diagnostics, the case demonstrating the value of technologies to tackle emerging dangerous situations as well as acting preventively.","A critical and rapidly developing damage was detected in time to stop the development, but not in time to avoid the incident altogether.","The paper puts risk in a broader perspective for an organization responsible for highway infrastructure.","It positions online monitoring and Digital Twins in the context of Risk- and Condition-Based Maintenance.","The situation that arose at Stava bridge, and how it was detected, analyzed, and diagnosed during virtual inspection, is described.","The case demonstrates how combining physics-based methods with Machine Learning can facilitate damage detection and diagnostics.","A summary of lessons learnt, both from technical and organizational perspectives, as well as plans of future work, is presented."],"url":"http://arxiv.org/abs/2404.10341v1","category":"cs.LG"}
{"created":"2024-04-16 06:09:33","title":"Long-form music generation with latent diffusion","abstract":"Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure. We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s. Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz). It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure.","sentences":["Audio-based generative models for music have seen great strides recently, but so far have not managed to produce full-length music tracks with coherent musical structure.","We show that by training a generative model on long temporal contexts it is possible to produce long-form music of up to 4m45s.","Our model consists of a diffusion-transformer operating on a highly downsampled continuous latent representation (latent rate of 21.5Hz).","It obtains state-of-the-art generations according to metrics on audio quality and prompt alignment, and subjective tests reveal that it produces full-length music with coherent structure."],"url":"http://arxiv.org/abs/2404.10301v1","category":"cs.SD"}
{"created":"2024-04-16 02:59:52","title":"AniFrame: A Programming Language for 2D Drawing and Frame-Based Animation","abstract":"Creative coding is an experimentation-heavy activity that requires translating high-level visual ideas into code. However, most languages and libraries for creative coding may not be adequately intuitive for beginners. In this paper, we present AniFrame, a domain-specific language for drawing and animation. Designed for novice programmers, it (i) features animation-specific data types, operations, and built-in functions to simplify the creation and animation of composite objects, (ii) allows for fine-grained control over animation sequences through explicit specification of the target object and the start and end frames, (iii) reduces the learning curve through a Python-like syntax, type inferencing, and a minimal set of control structures and keywords that map closely to their semantic intent, and (iv) promotes computational expressivity through support for common mathematical operations, built-in trigonometric functions, and user-defined recursion. Our usability test demonstrates AniFrame's potential to enhance readability and writability for multiple creative coding use cases. AniFrame is open-source, and its implementation and reference are available at https://github.com/memgonzales/aniframe-language.","sentences":["Creative coding is an experimentation-heavy activity that requires translating high-level visual ideas into code.","However, most languages and libraries for creative coding may not be adequately intuitive for beginners.","In this paper, we present AniFrame, a domain-specific language for drawing and animation.","Designed for novice programmers, it (i) features animation-specific data types, operations, and built-in functions to simplify the creation and animation of composite objects, (ii) allows for fine-grained control over animation sequences through explicit specification of the target object and the start and end frames, (iii) reduces the learning curve through a Python-like syntax, type inferencing, and a minimal set of control structures and keywords that map closely to their semantic intent, and (iv) promotes computational expressivity through support for common mathematical operations, built-in trigonometric functions, and user-defined recursion.","Our usability test demonstrates AniFrame's potential to enhance readability and writability for multiple creative coding use cases.","AniFrame is open-source, and its implementation and reference are available at https://github.com/memgonzales/aniframe-language."],"url":"http://arxiv.org/abs/2404.10250v1","category":"cs.PL"}
{"created":"2024-04-16 02:26:15","title":"Improving Disturbance Estimation and Suppression via Learning among Systems with Mismatched Dynamics","abstract":"Iterative learning control (ILC) is a method for reducing system tracking or estimation errors over multiple iterations by using information from past iterations. The disturbance observer (DOB) is used to estimate and mitigate disturbances within the system, while the system is being affected by them. ILC enhances system performance by introducing a feedforward signal in each iteration. However, its effectiveness may diminish if the conditions change during the iterations. On the other hand, although DOB effectively mitigates the effects of new disturbances, it cannot entirely eliminate them as it operates reactively. Therefore, neither ILC nor DOB alone can ensure sufficient robustness in challenging scenarios. This study focuses on the simultaneous utilization of ILC and DOB to enhance system robustness. The proposed methodology specifically targets dynamically different linearized systems performing repetitive tasks. The systems share similar forms but differ in dynamics (e.g. sizes, masses, and controllers). Consequently, the design of learning filters must account for these differences in dynamics. To validate the approach, the study establishes a theoretical framework for designing learning filters in conjunction with DOB. The validity of the framework is then confirmed through numerical studies and experimental tests conducted on unmanned aerial vehicles (UAVs). Although UAVs are nonlinear systems, the study employs a linearized controller as they operate in proximity to the hover condition. A video introduction of this paper is available via this link: https://zh.engr.tamu.edu/wp-content/uploads/sites/310/2024/02/ILCDOB_v3f.mp4.","sentences":["Iterative learning control (ILC) is a method for reducing system tracking or estimation errors over multiple iterations by using information from past iterations.","The disturbance observer (DOB) is used to estimate and mitigate disturbances within the system, while the system is being affected by them.","ILC enhances system performance by introducing a feedforward signal in each iteration.","However, its effectiveness may diminish if the conditions change during the iterations.","On the other hand, although DOB effectively mitigates the effects of new disturbances, it cannot entirely eliminate them as it operates reactively.","Therefore, neither ILC nor DOB alone can ensure sufficient robustness in challenging scenarios.","This study focuses on the simultaneous utilization of ILC and DOB to enhance system robustness.","The proposed methodology specifically targets dynamically different linearized systems performing repetitive tasks.","The systems share similar forms but differ in dynamics (e.g. sizes, masses, and controllers).","Consequently, the design of learning filters must account for these differences in dynamics.","To validate the approach, the study establishes a theoretical framework for designing learning filters in conjunction with DOB.","The validity of the framework is then confirmed through numerical studies and experimental tests conducted on unmanned aerial vehicles (UAVs).","Although UAVs are nonlinear systems, the study employs a linearized controller as they operate in proximity to the hover condition.","A video introduction of this paper is available via this link: https://zh.engr.tamu.edu/wp-content/uploads/sites/310/2024/02/ILCDOB_v3f.mp4."],"url":"http://arxiv.org/abs/2404.10231v1","category":"cs.RO"}
{"created":"2024-04-16 02:18:18","title":"MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints","abstract":"This work proposes a novel learning framework for visual hand dynamics analysis that takes into account the physiological aspects of hand motion. The existing models, which are simplified joint-actuated systems, often produce unnatural motions. To address this, we integrate a musculoskeletal system with a learnable parametric hand model, MANO, to create a new model, MS-MANO. This model emulates the dynamics of muscles and tendons to drive the skeletal system, imposing physiologically realistic constraints on the resulting torque trajectories. We further propose a simulation-in-the-loop pose refinement framework, BioPR, that refines the initial estimated pose through a multi-layer perceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the efficacy of the BioPR is conducted in two separate parts. The accuracy of MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked against two large-scale public datasets and two recent state-of-the-art methods. The results demonstrate that our approach consistently improves the baseline methods both quantitatively and qualitatively.","sentences":["This work proposes a novel learning framework for visual hand dynamics analysis that takes into account the physiological aspects of hand motion.","The existing models, which are simplified joint-actuated systems, often produce unnatural motions.","To address this, we integrate a musculoskeletal system with a learnable parametric hand model, MANO, to create a new model, MS-MANO.","This model emulates the dynamics of muscles and tendons to drive the skeletal system, imposing physiologically realistic constraints on the resulting torque trajectories.","We further propose a simulation-in-the-loop pose refinement framework, BioPR, that refines the initial estimated pose through a multi-layer perceptron (MLP) network.","Our evaluation of the accuracy of MS-MANO and the efficacy of the BioPR is conducted in two separate parts.","The accuracy of MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked against two large-scale public datasets and two recent state-of-the-art methods.","The results demonstrate that our approach consistently improves the baseline methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2404.10227v1","category":"cs.CV"}
{"created":"2024-04-16 01:50:10","title":"GaitPoint+: A Gait Recognition Network Incorporating Point Cloud Analysis and Recycling","abstract":"Gait is a behavioral biometric modality that can be used to recognize individuals by the way they walk from a far distance. Most existing gait recognition approaches rely on either silhouettes or skeletons, while their joint use is underexplored. Features from silhouettes and skeletons can provide complementary information for more robust recognition against appearance changes or pose estimation errors. To exploit the benefits of both silhouette and skeleton features, we propose a new gait recognition network, referred to as the GaitPoint+. Our approach models skeleton key points as a 3D point cloud, and employs a computational complexity-conscious 3D point processing approach to extract skeleton features, which are then combined with silhouette features for improved accuracy. Since silhouette- or CNN-based methods already require considerable amount of computational resources, it is preferable that the key point learning module is faster and more lightweight. We present a detailed analysis of the utilization of every human key point after the use of traditional max-pooling, and show that while elbow and ankle points are used most commonly, many useful points are discarded by max-pooling. Thus, we present a method to recycle some of the discarded points by a Recycling Max-Pooling module, during processing of skeleton point clouds, and achieve further performance improvement. We provide a comprehensive set of experimental results showing that (i) incorporating skeleton features obtained by a point-based 3D point cloud processing approach boosts the performance of three different state-of-the-art silhouette- and CNN-based baselines; (ii) recycling the discarded points increases the accuracy further. Ablation studies are also provided to show the effectiveness and contribution of different components of our approach.","sentences":["Gait is a behavioral biometric modality that can be used to recognize individuals by the way they walk from a far distance.","Most existing gait recognition approaches rely on either silhouettes or skeletons, while their joint use is underexplored.","Features from silhouettes and skeletons can provide complementary information for more robust recognition against appearance changes or pose estimation errors.","To exploit the benefits of both silhouette and skeleton features, we propose a new gait recognition network, referred to as the GaitPoint+.","Our approach models skeleton key points as a 3D point cloud, and employs a computational complexity-conscious 3D point processing approach to extract skeleton features, which are then combined with silhouette features for improved accuracy.","Since silhouette- or CNN-based methods already require considerable amount of computational resources, it is preferable that the key point learning module is faster and more lightweight.","We present a detailed analysis of the utilization of every human key point after the use of traditional max-pooling, and show that while elbow and ankle points are used most commonly, many useful points are discarded by max-pooling.","Thus, we present a method to recycle some of the discarded points by a Recycling Max-Pooling module, during processing of skeleton point clouds, and achieve further performance improvement.","We provide a comprehensive set of experimental results showing that (i) incorporating skeleton features obtained by a point-based 3D point cloud processing approach boosts the performance of three different state-of-the-art silhouette- and CNN-based baselines; (ii) recycling the discarded points increases the accuracy further.","Ablation studies are also provided to show the effectiveness and contribution of different components of our approach."],"url":"http://arxiv.org/abs/2404.10213v1","category":"cs.CV"}
{"created":"2024-04-16 01:20:51","title":"HELLINGER-UCB: A novel algorithm for stochastic multi-armed bandit problem and cold start problem in recommender system","abstract":"In this paper, we study the stochastic multi-armed bandit problem, where the reward is driven by an unknown random variable. We propose a new variant of the Upper Confidence Bound (UCB) algorithm called Hellinger-UCB, which leverages the squared Hellinger distance to build the upper confidence bound. We prove that the Hellinger-UCB reaches the theoretical lower bound. We also show that the Hellinger-UCB has a solid statistical interpretation. We show that Hellinger-UCB is effective in finite time horizons with numerical experiments between Hellinger-UCB and other variants of the UCB algorithm. As a real-world example, we apply the Hellinger-UCB algorithm to solve the cold-start problem for a content recommender system of a financial app. With reasonable assumption, the Hellinger-UCB algorithm has a convenient but important lower latency feature. The online experiment also illustrates that the Hellinger-UCB outperforms both KL-UCB and UCB1 in the sense of a higher click-through rate (CTR).","sentences":["In this paper, we study the stochastic multi-armed bandit problem, where the reward is driven by an unknown random variable.","We propose a new variant of the Upper Confidence Bound (UCB) algorithm called Hellinger-UCB, which leverages the squared Hellinger distance to build the upper confidence bound.","We prove that the Hellinger-UCB reaches the theoretical lower bound.","We also show that the Hellinger-UCB has a solid statistical interpretation.","We show that Hellinger-UCB is effective in finite time horizons with numerical experiments between Hellinger-UCB and other variants of the UCB algorithm.","As a real-world example, we apply the Hellinger-UCB algorithm to solve the cold-start problem for a content recommender system of a financial app.","With reasonable assumption, the Hellinger-UCB algorithm has a convenient but important lower latency feature.","The online experiment also illustrates that the Hellinger-UCB outperforms both KL-UCB and UCB1 in the sense of a higher click-through rate (CTR)."],"url":"http://arxiv.org/abs/2404.10207v1","category":"stat.ML"}
{"created":"2024-04-16 00:56:36","title":"Private Vector Mean Estimation in the Shuffle Model: Optimal Rates Require Many Messages","abstract":"We study the problem of private vector mean estimation in the shuffle model of privacy where $n$ users each have a unit vector $v^{(i)} \\in\\mathbb{R}^d$. We propose a new multi-message protocol that achieves the optimal error using $\\tilde{\\mathcal{O}}\\left(\\min(n\\varepsilon^2,d)\\right)$ messages per user. Moreover, we show that any (unbiased) protocol that achieves optimal error requires each user to send $\\Omega(\\min(n\\varepsilon^2,d)/\\log(n))$ messages, demonstrating the optimality of our message complexity up to logarithmic factors. Additionally, we study the single-message setting and design a protocol that achieves mean squared error $\\mathcal{O}(dn^{d/(d+2)}\\varepsilon^{-4/(d+2)})$. Moreover, we show that any single-message protocol must incur mean squared error $\\Omega(dn^{d/(d+2)})$, showing that our protocol is optimal in the standard setting where $\\varepsilon = \\Theta(1)$. Finally, we study robustness to malicious users and show that malicious users can incur large additive error with a single shuffler.","sentences":["We study the problem of private vector mean estimation in the shuffle model of privacy where $n$ users each have a unit vector $v^{(i)} \\in\\mathbb{R}^d$.","We propose a new multi-message protocol that achieves the optimal error using $\\tilde{\\mathcal{O}}\\left(\\min(n\\varepsilon^2,d)\\right)$ messages per user.","Moreover, we show that any (unbiased) protocol that achieves optimal error requires each user to send $\\Omega(\\min(n\\varepsilon^2,d)/\\log(n))$ messages, demonstrating the optimality of our message complexity up to logarithmic factors.","Additionally, we study the single-message setting and design a protocol that achieves mean squared error $\\mathcal{O}(dn^{d/(d+2)}\\varepsilon^{-4/(d+2)})$.","Moreover, we show that any single-message protocol must incur mean squared error $\\Omega(dn^{d/(d+2)})$, showing that our protocol is optimal in the standard setting where $\\varepsilon = \\Theta(1)$. Finally, we study robustness to malicious users and show that malicious users can incur large additive error with a single shuffler."],"url":"http://arxiv.org/abs/2404.10201v1","category":"cs.DS"}
{"created":"2024-04-15 23:54:45","title":"Insights from the Field: Exploring Students' Perspectives on Bad Unit Testing Practices","abstract":"Educating students about software testing practices is integral to the curricula of many computer science-related courses and typically involves students writing unit tests. Similar to production/source code, students might inadvertently deviate from established unit testing best practices, and introduce problematic code, referred to as test smells, into their test suites. Given the extensive catalog of test smells, it becomes challenging for students to identify test smells in their code, especially for those who lack experience with testing practices. In this experience report, we aim to increase students' awareness of bad unit testing practices, and detail the outcomes of having 184 students from three higher educational institutes utilize an IDE plugin to automatically detect test smells in their code. Our findings show that while students report on the plugin's usefulness in learning about and detecting test smells, they also identify specific test smells that they consider harmless. We anticipate that our findings will support academia in refining course curricula on unit testing and enabling educators to support students with code review strategies of test code.","sentences":["Educating students about software testing practices is integral to the curricula of many computer science-related courses and typically involves students writing unit tests.","Similar to production/source code, students might inadvertently deviate from established unit testing best practices, and introduce problematic code, referred to as test smells, into their test suites.","Given the extensive catalog of test smells, it becomes challenging for students to identify test smells in their code, especially for those who lack experience with testing practices.","In this experience report, we aim to increase students' awareness of bad unit testing practices, and detail the outcomes of having 184 students from three higher educational institutes utilize an IDE plugin to automatically detect test smells in their code.","Our findings show that while students report on the plugin's usefulness in learning about and detecting test smells, they also identify specific test smells that they consider harmless.","We anticipate that our findings will support academia in refining course curricula on unit testing and enabling educators to support students with code review strategies of test code."],"url":"http://arxiv.org/abs/2404.10185v1","category":"cs.SE"}
{"created":"2024-04-15 23:05:57","title":"On the Effects of Fine-tuning Language Models for Text-Based Reinforcement Learning","abstract":"Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task. Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities. The success of these agents in playing such games suggests that semantic understanding may not be important for the task. This raises an important question about the benefits of LMs in guiding the agents through the game states. In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents. Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL). Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games. We believe these results may help develop better strategies to fine-tune agents in text-based RL scenarios.","sentences":["Text-based reinforcement learning involves an agent interacting with a fictional environment using observed text and admissible actions in natural language to complete a task.","Previous works have shown that agents can succeed in text-based interactive environments even in the complete absence of semantic understanding or other linguistic capabilities.","The success of these agents in playing such games suggests that semantic understanding may not be important for the task.","This raises an important question about the benefits of LMs in guiding the agents through the game states.","In this work, we show that rich semantic understanding leads to efficient training of text-based RL agents.","Moreover, we describe the occurrence of semantic degeneration as a consequence of inappropriate fine-tuning of language models in text-based reinforcement learning (TBRL).","Specifically, we describe the shift in the semantic representation of words in the LM, as well as how it affects the performance of the agent in tasks that are semantically similar to the training games.","We believe these results may help develop better strategies to fine-tune agents in text-based RL scenarios."],"url":"http://arxiv.org/abs/2404.10174v1","category":"cs.CL"}
{"created":"2024-04-15 21:33:45","title":"Eyes on the Streets: Leveraging Street-Level Imaging to Model Urban Crime Dynamics","abstract":"This study addresses the challenge of urban safety in New York City by examining the relationship between the built environment and crime rates using machine learning and a comprehensive dataset of street view im- ages. We aim to identify how urban landscapes correlate with crime statistics, focusing on the characteristics of street views and their association with crime rates. The findings offer insights for urban planning and crime pre- vention, highlighting the potential of environmental de- sign in enhancing public safety.","sentences":["This study addresses the challenge of urban safety in New York City by examining the relationship between the built environment and crime rates using machine learning and a comprehensive dataset of street view im- ages.","We aim to identify how urban landscapes correlate with crime statistics, focusing on the characteristics of street views and their association with crime rates.","The findings offer insights for urban planning and crime pre- vention, highlighting the potential of environmental de- sign in enhancing public safety."],"url":"http://arxiv.org/abs/2404.10147v1","category":"cs.CV"}
{"created":"2024-04-15 20:37:52","title":"NOISe: Nuclei-Aware Osteoclast Instance Segmentation for Mouse-to-Human Domain Transfer","abstract":"Osteoclast cell image analysis plays a key role in osteoporosis research, but it typically involves extensive manual image processing and hand annotations by a trained expert. In the last few years, a handful of machine learning approaches for osteoclast image analysis have been developed, but none have addressed the full instance segmentation task required to produce the same output as that of the human expert led process. Furthermore, none of the prior, fully automated algorithms have publicly available code, pretrained models, or annotated datasets, inhibiting reproduction and extension of their work. We present a new dataset with ~2*10^5 expert annotated mouse osteoclast masks, together with a deep learning instance segmentation method which works for both in vitro mouse osteoclast cells on plastic tissue culture plates and human osteoclast cells on bone chips. To our knowledge, this is the first work to automate the full osteoclast instance segmentation task. Our method achieves a performance of 0.82 mAP_0.5 (mean average precision at intersection-over-union threshold of 0.5) in cross validation for mouse osteoclasts. We present a novel nuclei-aware osteoclast instance segmentation training strategy (NOISe) based on the unique biology of osteoclasts, to improve the model's generalizability and boost the mAP_0.5 from 0.60 to 0.82 on human osteoclasts. We publish our annotated mouse osteoclast image dataset, instance segmentation models, and code at github.com/michaelwwan/noise to enable reproducibility and to provide a public tool to accelerate osteoporosis research.","sentences":["Osteoclast cell image analysis plays a key role in osteoporosis research, but it typically involves extensive manual image processing and hand annotations by a trained expert.","In the last few years, a handful of machine learning approaches for osteoclast image analysis have been developed, but none have addressed the full instance segmentation task required to produce the same output as that of the human expert led process.","Furthermore, none of the prior, fully automated algorithms have publicly available code, pretrained models, or annotated datasets, inhibiting reproduction and extension of their work.","We present a new dataset with ~2*10^5 expert annotated mouse osteoclast masks, together with a deep learning instance segmentation method which works for both in vitro mouse osteoclast cells on plastic tissue culture plates and human osteoclast cells on bone chips.","To our knowledge, this is the first work to automate the full osteoclast instance segmentation task.","Our method achieves a performance of 0.82 mAP_0.5 (mean average precision at intersection-over-union threshold of 0.5) in cross validation for mouse osteoclasts.","We present a novel nuclei-aware osteoclast instance segmentation training strategy (NOISe) based on the unique biology of osteoclasts, to improve the model's generalizability and boost the mAP_0.5 from 0.60 to 0.82 on human osteoclasts.","We publish our annotated mouse osteoclast image dataset, instance segmentation models, and code at github.com/michaelwwan/noise to enable reproducibility and to provide a public tool to accelerate osteoporosis research."],"url":"http://arxiv.org/abs/2404.10130v1","category":"cs.CV"}
{"created":"2024-04-15 19:45:05","title":"Rotation and flipping invariant self-organizing maps with astronomical images: A cookbook and application to the VLA Sky Survey QuickLook images","abstract":"Modern wide field radio surveys typically detect millions of objects. Techniques based on machine learning are proving to be useful for classifying large numbers of objects. The self-organizing map (SOM) is an unsupervised machine learning algorithm that projects a many-dimensional dataset onto a two- or three-dimensional lattice of neurons. This dimensionality reduction allows the user to visualize common features of the data better and develop algorithms for classifying objects that are not otherwise possible with large datasets. To this aim, we use the PINK implementation of a SOM. PINK incorporates rotation and flipping invariance so that the SOM algorithm may be applied to astronomical images. In this cookbook we provide instructions for working with PINK, including preprocessing the input images, training the model, and offering lessons learned through experimentation. The problem of imbalanced classes can be improved by careful selection of the training sample and increasing the number of neurons in the SOM (chosen by the user). Because PINK is not scale-invariant, structure can be smeared in the neurons. This can also be improved by increasing the number of neurons in the SOM. We also introduce pyink, a Python package used to read and write PINK binary files, assist in common preprocessing operations, perform standard analyses, visualize the SOM and preprocessed images, and create image-based annotations using a graphical interface. A tutorial is also provided to guide the user through the entire process. We present an application of PINK to VLA Sky Survey (VLASS) images. We demonstrate that the PINK is generally able to group VLASS sources with similar morphology together. We use the results of PINK to estimate the probability that a given source in the VLASS QuickLook Catalogue is actually due to sidelobe contamination.","sentences":["Modern wide field radio surveys typically detect millions of objects.","Techniques based on machine learning are proving to be useful for classifying large numbers of objects.","The self-organizing map (SOM) is an unsupervised machine learning algorithm that projects a many-dimensional dataset onto a two- or three-dimensional lattice of neurons.","This dimensionality reduction allows the user to visualize common features of the data better and develop algorithms for classifying objects that are not otherwise possible with large datasets.","To this aim, we use the PINK implementation of a SOM.","PINK incorporates rotation and flipping invariance so that the SOM algorithm may be applied to astronomical images.","In this cookbook we provide instructions for working with PINK, including preprocessing the input images, training the model, and offering lessons learned through experimentation.","The problem of imbalanced classes can be improved by careful selection of the training sample and increasing the number of neurons in the SOM (chosen by the user).","Because PINK is not scale-invariant, structure can be smeared in the neurons.","This can also be improved by increasing the number of neurons in the SOM.","We also introduce pyink, a Python package used to read and write PINK binary files, assist in common preprocessing operations, perform standard analyses, visualize the SOM and preprocessed images, and create image-based annotations using a graphical interface.","A tutorial is also provided to guide the user through the entire process.","We present an application of PINK to VLA Sky Survey (VLASS) images.","We demonstrate that the PINK is generally able to group VLASS sources with similar morphology together.","We use the results of PINK to estimate the probability that a given source in the VLASS QuickLook Catalogue is actually due to sidelobe contamination."],"url":"http://arxiv.org/abs/2404.10109v1","category":"astro-ph.IM"}
{"created":"2024-04-16 14:28:57","title":"Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences","abstract":"Leveraging multi-view diffusion models as priors for 3D optimization have alleviated the problem of 3D consistency, e.g., the Janus face problem or the content drift problem, in zero-shot text-to-3D models. However, the 3D geometric fidelity of the output remains an unresolved issue; albeit the rendered 2D views are realistic, the underlying geometry may contain errors such as unreasonable concavities. In this work, we propose CorrespondentDream, an effective method to leverage annotation-free, cross-view correspondences yielded from the diffusion U-Net to provide additional 3D prior to the NeRF optimization process. We find that these correspondences are strongly consistent with human perception, and by adopting it in our loss design, we are able to produce NeRF models with geometries that are more coherent with common sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We demonstrate the efficacy of our approach through various comparative qualitative results and a solid user study.","sentences":["Leveraging multi-view diffusion models as priors for 3D optimization have alleviated the problem of 3D consistency, e.g., the Janus face problem or the content drift problem, in zero-shot text-to-3D models.","However, the 3D geometric fidelity of the output remains an unresolved issue; albeit the rendered 2D views are realistic, the underlying geometry may contain errors such as unreasonable concavities.","In this work, we propose CorrespondentDream, an effective method to leverage annotation-free, cross-view correspondences yielded from the diffusion U-Net to provide additional 3D prior to the NeRF optimization process.","We find that these correspondences are strongly consistent with human perception, and by adopting it in our loss design, we are able to produce NeRF models with geometries that are more coherent with common sense, e.g., more smoothed object surface, yielding higher 3D fidelity.","We demonstrate the efficacy of our approach through various comparative qualitative results and a solid user study."],"url":"http://arxiv.org/abs/2404.10603v1","category":"cs.CV"}
{"created":"2024-04-16 13:45:37","title":"Tropical toric maximum likelihood estimation","abstract":"We consider toric maximum likelihood estimation over the field of Puiseux series and study critical points of the likelihood function using tropical methods. This problem translates to finding the intersection points of a tropical affine space with a classical linear subspace. We derive new structural results on tropical affine spaces and use these to give a complete and explicit description of the tropical critical points in certain cases. In these cases, we associate tropical critical points to the simplices in a regular triangulation of the polytope giving rise to the toric model.","sentences":["We consider toric maximum likelihood estimation over the field of Puiseux series and study critical points of the likelihood function using tropical methods.","This problem translates to finding the intersection points of a tropical affine space with a classical linear subspace.","We derive new structural results on tropical affine spaces and use these to give a complete and explicit description of the tropical critical points in certain cases.","In these cases, we associate tropical critical points to the simplices in a regular triangulation of the polytope giving rise to the toric model."],"url":"http://arxiv.org/abs/2404.10567v1","category":"math.AG"}
{"created":"2024-04-16 13:32:25","title":"Decade-Bandwidth RF-Input Pseudo-Doherty Load Modulated Balanced Amplifier using Signal-Flow-Based Phase Alignment Design","abstract":"This paper reports a first-ever decade-bandwidth pseudo-Doherty load-modulated balanced amplifier (PD-LMBA), designed for emerging 4G/5G communications and multi-band operations. By revisiting the LMBA theory using the signal-flow graph, a frequency-agnostic phase-alignment condition is found that is critical for ensuring intrinsically broadband load modulation behavior. This unique design methodology enables, for the first time, the independent optimization of broadband balanced amplifier (BA, as the peaking) and control amplifier (CA, as the carrier), thus fundamentally addressing the longstanding limits imposed on the design of wideband load-modulated power amplifiers (PAs). To prove the proposed concept, an ultra-wideband RF-input PD-LMBA is designed and developed using GaN technology covering the frequency range from 0.2 to 2 GHz. Experimental results demonstrate an efficiency of 51% to 72% for peak output power and 44% to 62% for 10-dB OBO, respectively.","sentences":["This paper reports a first-ever decade-bandwidth pseudo-Doherty load-modulated balanced amplifier (PD-LMBA), designed for emerging 4G/5G communications and multi-band operations.","By revisiting the LMBA theory using the signal-flow graph, a frequency-agnostic phase-alignment condition is found that is critical for ensuring intrinsically broadband load modulation behavior.","This unique design methodology enables, for the first time, the independent optimization of broadband balanced amplifier (BA, as the peaking) and control amplifier (CA, as the carrier), thus fundamentally addressing the longstanding limits imposed on the design of wideband load-modulated power amplifiers (PAs).","To prove the proposed concept, an ultra-wideband RF-input PD-LMBA is designed and developed using GaN technology covering the frequency range from 0.2 to 2 GHz.","Experimental results demonstrate an efficiency of 51% to 72% for peak output power and 44% to 62% for 10-dB OBO, respectively."],"url":"http://arxiv.org/abs/2404.10558v1","category":"cs.AR"}
{"created":"2024-04-16 11:38:44","title":"Efficient optimal dispersed Haar-like filters for face detection","abstract":"This paper introduces a new dispersed Haar-like filter for efficiently detection face. The basic idea for finding the filter is maximising between-class and minimising within-class variance. The proposed filters can be considered as an optimal configuration dispersed Haar-like filters; filters with disjoint black and white parts.","sentences":["This paper introduces a new dispersed Haar-like filter for efficiently detection face.","The basic idea for finding the filter is maximising between-class and minimising within-class variance.","The proposed filters can be considered as an optimal configuration dispersed Haar-like filters; filters with disjoint black and white parts."],"url":"http://arxiv.org/abs/2404.10476v1","category":"cs.CV"}
{"created":"2024-04-16 10:33:06","title":"On estimation of heavy-tailed stable linear regression","abstract":"We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors. Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\\sqrt{n}$-consistency. To derive the $\\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE. The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator. Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood.","sentences":["We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors.","Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\\sqrt{n}$-consistency.","To derive the $\\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE.","The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator.","Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood."],"url":"http://arxiv.org/abs/2404.10448v1","category":"math.ST"}
{"created":"2024-04-16 09:19:11","title":"Comprehensive Survey of Model Compression and Speed up for Vision Transformers","abstract":"Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks. However, their practical deployment is hampered by high computational and memory demands. This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning. We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing ViTs for resource-constrained environments. Our comprehensive experimental evaluation demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices.","sentences":["Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks.","However, their practical deployment is hampered by high computational and memory demands.","This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning.","We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing ViTs for resource-constrained environments.","Our comprehensive experimental evaluation demonstrates that these methods facilitate a balanced compromise between model accuracy and computational efficiency, paving the way for wider application in edge computing devices."],"url":"http://arxiv.org/abs/2404.10407v1","category":"cs.CV"}
{"created":"2024-04-16 08:10:15","title":"Consensus-based algorithms for stochastic optimization problems","abstract":"We address an optimization problem where the cost function is the expectation of a random mapping. To tackle the problem two approaches based on the approximation of the objective function by consensus-based particle optimization methods on the search space are developed. The resulting methods are mathematically analyzed using a mean-field approximation and their connection is established. Several numerical experiments show the validity of the proposed algorithms and investigate their rates of convergence.","sentences":["We address an optimization problem where the cost function is the expectation of a random mapping.","To tackle the problem two approaches based on the approximation of the objective function by consensus-based particle optimization methods on the search space are developed.","The resulting methods are mathematically analyzed using a mean-field approximation and their connection is established.","Several numerical experiments show the validity of the proposed algorithms and investigate their rates of convergence."],"url":"http://arxiv.org/abs/2404.10372v1","category":"math.OC"}
{"created":"2024-04-16 07:26:20","title":"The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report","abstract":"This paper provides a comprehensive review of the NTIRE 2024 challenge, focusing on efficient single-image super-resolution (ESR) solutions and their outcomes. The task of this challenge is to super-resolve an input image with a magnification factor of x4 based on pairs of low and corresponding high-resolution images. The primary objective is to develop networks that optimize various aspects such as runtime, parameters, and FLOPs, while still maintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on the DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In addition, this challenge has 4 tracks including the main track (overall performance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3 (parameters). In the main track, all three metrics (ie runtime, FLOPs, and parameter count) were considered. The ranking of the main track is calculated based on a weighted sum-up of the scores of all other sub-tracks. In sub-track 1, the practical runtime performance of the submissions was evaluated, and the corresponding score was used to determine the ranking. In sub-track 2, the number of FLOPs was considered. The score calculated based on the corresponding FLOPs was used to determine the ranking. In sub-track 3, the number of parameters was considered. The score calculated based on the corresponding parameters was used to determine the ranking. RLFN is set as the baseline for efficiency measurement. The challenge had 262 registered participants, and 34 teams made valid submissions. They gauge the state-of-the-art in efficient single-image super-resolution. To facilitate the reproducibility of the challenge and enable other researchers to build upon these findings, the code and the pre-trained model of validated solutions are made publicly available at https://github.com/Amazingren/NTIRE2024_ESR/.","sentences":["This paper provides a comprehensive review of the NTIRE 2024 challenge, focusing on efficient single-image super-resolution (ESR) solutions and their outcomes.","The task of this challenge is to super-resolve an input image with a magnification factor of x4 based on pairs of low and corresponding high-resolution images.","The primary objective is to develop networks that optimize various aspects such as runtime, parameters, and FLOPs, while still maintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on the DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset.","In addition, this challenge has 4 tracks including the main track (overall performance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3 (parameters).","In the main track, all three metrics (ie runtime, FLOPs, and parameter count) were considered.","The ranking of the main track is calculated based on a weighted sum-up of the scores of all other sub-tracks.","In sub-track 1, the practical runtime performance of the submissions was evaluated, and the corresponding score was used to determine the ranking.","In sub-track 2, the number of FLOPs was considered.","The score calculated based on the corresponding FLOPs was used to determine the ranking.","In sub-track 3, the number of parameters was considered.","The score calculated based on the corresponding parameters was used to determine the ranking.","RLFN is set as the baseline for efficiency measurement.","The challenge had 262 registered participants, and 34 teams made valid submissions.","They gauge the state-of-the-art in efficient single-image super-resolution.","To facilitate the reproducibility of the challenge and enable other researchers to build upon these findings, the code and the pre-trained model of validated solutions are made publicly available at https://github.com/Amazingren/NTIRE2024_ESR/."],"url":"http://arxiv.org/abs/2404.10343v1","category":"cs.CV"}
{"created":"2024-04-16 03:55:12","title":"Optimizing BIT1, a Particle-in-Cell Monte Carlo Code, with OpenMP/OpenACC and GPU Acceleration","abstract":"On the path toward developing the first fusion energy devices, plasma simulations have become indispensable tools for supporting the design and development of fusion machines. Among these critical simulation tools, BIT1 is an advanced Particle-in-Cell code with Monte Carlo collisions, specifically designed for modeling plasma-material interaction and, in particular, analyzing the power load distribution on tokamak divertors. The current implementation of BIT1 relies exclusively on MPI for parallel communication and lacks support for GPUs. In this work, we address these limitations by designing and implementing a hybrid, shared-memory version of BIT1 capable of utilizing GPUs. For shared-memory parallelization, we rely on OpenMP and OpenACC, using a task-based approach to mitigate load-imbalance issues in the particle mover. On an HPE Cray EX computing node, we observe an initial performance improvement of approximately 42%, with scalable performance showing an enhancement of about 38% when using 8 MPI ranks. Still relying on OpenMP and OpenACC, we introduce the first version of BIT1 capable of using GPUs. We investigate two different data movement strategies: unified memory and explicit data movement. Overall, we report BIT1 data transfer findings during each PIC cycle. Among BIT1 GPU implementations, we demonstrate performance improvement through concurrent GPU utilization, especially when MPI ranks are assigned to dedicated GPUs. Finally, we analyze the performance of the first BIT1 GPU porting with the NVIDIA Nsight tools to further our understanding of BIT1 computational efficiency for large-scale plasma simulations, capable of exploiting current supercomputer infrastructures.","sentences":["On the path toward developing the first fusion energy devices, plasma simulations have become indispensable tools for supporting the design and development of fusion machines.","Among these critical simulation tools, BIT1 is an advanced Particle-in-Cell code with Monte Carlo collisions, specifically designed for modeling plasma-material interaction and, in particular, analyzing the power load distribution on tokamak divertors.","The current implementation of BIT1 relies exclusively on MPI for parallel communication and lacks support for GPUs.","In this work, we address these limitations by designing and implementing a hybrid, shared-memory version of BIT1 capable of utilizing GPUs.","For shared-memory parallelization, we rely on OpenMP and OpenACC, using a task-based approach to mitigate load-imbalance issues in the particle mover.","On an HPE Cray EX computing node, we observe an initial performance improvement of approximately 42%, with scalable performance showing an enhancement of about 38% when using 8 MPI ranks.","Still relying on OpenMP and OpenACC, we introduce the first version of BIT1 capable of using GPUs.","We investigate two different data movement strategies: unified memory and explicit data movement.","Overall, we report BIT1 data transfer findings during each PIC cycle.","Among BIT1 GPU implementations, we demonstrate performance improvement through concurrent GPU utilization, especially when MPI ranks are assigned to dedicated GPUs.","Finally, we analyze the performance of the first BIT1 GPU porting with the NVIDIA Nsight tools to further our understanding of BIT1 computational efficiency for large-scale plasma simulations, capable of exploiting current supercomputer infrastructures."],"url":"http://arxiv.org/abs/2404.10270v1","category":"cs.DC"}
{"created":"2024-04-16 03:34:05","title":"Safe Feature Identification Rule for Fused Lasso by An Extra Dual Variable","abstract":"Fused Lasso was proposed to characterize the sparsity of the coefficients and the sparsity of their successive differences for the linear regression. Due to its wide applications, there are many existing algorithms to solve fused Lasso. However, the computation of this model is time-consuming in high-dimensional data sets. To accelerate the calculation of fused Lasso in high-dimension data sets, we build up the safe feature identification rule by introducing an extra dual variable. With a low computational cost, this rule can eliminate inactive features with zero coefficients and identify adjacent features with same coefficients in the solution. To the best of our knowledge, existing screening rules can not be applied to speed up the computation of fused Lasso and our work is the first one to deal with this problem. To emphasize our rule is a unique result that is capable of identifying adjacent features with same coefficients, we name the result as the safe feature identification rule. Numerical experiments on simulation and real data illustrate the efficiency of the rule, which means this rule can reduce the computational time of fused Lasso. In addition, our rule can be embedded into any efficient algorithm and speed up the computational process of fused Lasso.","sentences":["Fused Lasso was proposed to characterize the sparsity of the coefficients and the sparsity of their successive differences for the linear regression.","Due to its wide applications, there are many existing algorithms to solve fused Lasso.","However, the computation of this model is time-consuming in high-dimensional data sets.","To accelerate the calculation of fused Lasso in high-dimension data sets, we build up the safe feature identification rule by introducing an extra dual variable.","With a low computational cost, this rule can eliminate inactive features with zero coefficients and identify adjacent features with same coefficients in the solution.","To the best of our knowledge, existing screening rules can not be applied to speed up the computation of fused Lasso and our work is the first one to deal with this problem.","To emphasize our rule is a unique result that is capable of identifying adjacent features with same coefficients, we name the result as the safe feature identification rule.","Numerical experiments on simulation and real data illustrate the efficiency of the rule, which means this rule can reduce the computational time of fused Lasso.","In addition, our rule can be embedded into any efficient algorithm and speed up the computational process of fused Lasso."],"url":"http://arxiv.org/abs/2404.10262v1","category":"stat.CO"}
{"created":"2024-04-16 02:44:42","title":"Using Multi-Source Data to Identify High-Emitting Heavy-Duty Diesel Vehicles","abstract":"Identifying and managing high-emitters among heavy-duty diesel vehicles is a key to mitigating urban air pollution, as a small number of such vehicles could contribute a significant amount of total transport emissions. On-board monitoring (OBM) systems can directly monitor the real-time emission performance of heavy-duty vehicles on road and have become part of the future emissions compliance framework. The challenge, however, lies in the frequent unavailability of OBM data, affecting the effective screening of high-emitting vehicles. This work proposes to bridge the gap by integrating OBM data with remote sensing data to create a comprehensive monitoring system. OBM data is used to characterize the detailed real-world NOx emission performance of both normally-behaving vehicles and high-emitters at various vehicle operating conditions. Remote sensing data is employed to screen out candidate high-emitting vehicles based on thresholds determined by OBM data. Finally, the dynamic NOx emission reduction potential across all roads is mapped by combining the trajectory data for each vehicle with the emission data. A case study in Chengdu, China, utilizing emission and traffic data from heavy-duty vehicles for transporting construction waste (a.k.a. slag trucks), reveals the national threshold for identifying high-emitters via remote sensing might be too lenient, particularly in the medium speed range. An emission reduction of 18.8% in the China V slag truck fleet could be achieved by implementing this novel method in practice in Chengdu. This approach establishes a reliable and ongoing scheme for pinpointing high-emitters through multi-source data, which allows local authorities to develop more robust and targeted strategies to mitigate urban air pollution from heavy-duty diesel vehicles.","sentences":["Identifying and managing high-emitters among heavy-duty diesel vehicles is a key to mitigating urban air pollution, as a small number of such vehicles could contribute a significant amount of total transport emissions.","On-board monitoring (OBM) systems can directly monitor the real-time emission performance of heavy-duty vehicles on road and have become part of the future emissions compliance framework.","The challenge, however, lies in the frequent unavailability of OBM data, affecting the effective screening of high-emitting vehicles.","This work proposes to bridge the gap by integrating OBM data with remote sensing data to create a comprehensive monitoring system.","OBM data is used to characterize the detailed real-world NOx emission performance of both normally-behaving vehicles and high-emitters at various vehicle operating conditions.","Remote sensing data is employed to screen out candidate high-emitting vehicles based on thresholds determined by OBM data.","Finally, the dynamic NOx emission reduction potential across all roads is mapped by combining the trajectory data for each vehicle with the emission data.","A case study in Chengdu, China, utilizing emission and traffic data from heavy-duty vehicles for transporting construction waste (a.k.a. slag trucks), reveals the national threshold for identifying high-emitters via remote sensing might be too lenient, particularly in the medium speed range.","An emission reduction of 18.8% in the China V slag truck fleet could be achieved by implementing this novel method in practice in Chengdu.","This approach establishes a reliable and ongoing scheme for pinpointing high-emitters through multi-source data, which allows local authorities to develop more robust and targeted strategies to mitigate urban air pollution from heavy-duty diesel vehicles."],"url":"http://arxiv.org/abs/2404.10243v1","category":"stat.AP"}
{"created":"2024-04-16 02:35:44","title":"A novel interpretation of Nesterov's acceleration via variable step-size linear multistep methods","abstract":"Nesterov's acceleration in continuous optimization can be understood in a novel way when Nesterov's accelerated gradient (NAG) method is considered as a linear multistep (LM) method for gradient flow. Although the NAG method for strongly convex functions (NAG-sc) has been fully discussed, the NAG method for $L$-smooth convex functions (NAG-c) has not. To fill this gap, we show that the existing NAG-c method can be interpreted as a variable step size LM (VLM) for the gradient flow. Surprisingly, the VLM allows linearly increasing step sizes, which explains the acceleration in the convex case. Here, we introduce a novel technique for analyzing the absolute stability of VLMs. Subsequently, we prove that NAG-c is optimal in a certain natural class of VLMs. Finally, we construct a new broader class of VLMs by optimizing the parameters in the VLM for ill-conditioned problems. According to numerical experiments, the proposed method outperforms the NAG-c method in ill-conditioned cases. These results imply that the numerical analysis perspective of the NAG is a promising working environment, and considering a broader class of VLMs could further reveal novel methods.","sentences":["Nesterov's acceleration in continuous optimization can be understood in a novel way when Nesterov's accelerated gradient (NAG) method is considered as a linear multistep (LM) method for gradient flow.","Although the NAG method for strongly convex functions (NAG-sc) has been fully discussed, the NAG method for $L$-smooth convex functions (NAG-c) has not.","To fill this gap, we show that the existing NAG-c method can be interpreted as a variable step size LM (VLM) for the gradient flow.","Surprisingly, the VLM allows linearly increasing step sizes, which explains the acceleration in the convex case.","Here, we introduce a novel technique for analyzing the absolute stability of VLMs.","Subsequently, we prove that NAG-c is optimal in a certain natural class of VLMs.","Finally, we construct a new broader class of VLMs by optimizing the parameters in the VLM for ill-conditioned problems.","According to numerical experiments, the proposed method outperforms the NAG-c method in ill-conditioned cases.","These results imply that the numerical analysis perspective of the NAG is a promising working environment, and considering a broader class of VLMs could further reveal novel methods."],"url":"http://arxiv.org/abs/2404.10238v1","category":"math.NA"}
{"created":"2024-04-16 02:19:28","title":"Generative Text Steganography with Large Language Model","abstract":"Recent advances in large language models (LLMs) have blurred the boundary of high-quality text generation between humans and machines, which is favorable for generative text steganography. While, current advanced steganographic mapping is not suitable for LLMs since most users are restricted to accessing only the black-box API or user interface of the LLMs, thereby lacking access to the training vocabulary and its sampling probabilities. In this paper, we explore a black-box generative text steganographic method based on the user interfaces of large language models, which is called LLM-Stega. The main goal of LLM-Stega is that the secure covert communication between Alice (sender) and Bob (receiver) is conducted by using the user interfaces of LLMs. Specifically, We first construct a keyword set and design a new encrypted steganographic mapping to embed secret messages. Furthermore, to guarantee accurate extraction of secret messages and rich semantics of generated stego texts, an optimization mechanism based on reject sampling is proposed. Comprehensive experiments demonstrate that the proposed LLM-Stega outperforms current state-of-the-art methods.","sentences":["Recent advances in large language models (LLMs) have blurred the boundary of high-quality text generation between humans and machines, which is favorable for generative text steganography.","While, current advanced steganographic mapping is not suitable for LLMs since most users are restricted to accessing only the black-box API or user interface of the LLMs, thereby lacking access to the training vocabulary and its sampling probabilities.","In this paper, we explore a black-box generative text steganographic method based on the user interfaces of large language models, which is called LLM-Stega.","The main goal of LLM-Stega is that the secure covert communication between Alice (sender) and Bob (receiver) is conducted by using the user interfaces of LLMs.","Specifically, We first construct a keyword set and design a new encrypted steganographic mapping to embed secret messages.","Furthermore, to guarantee accurate extraction of secret messages and rich semantics of generated stego texts, an optimization mechanism based on reject sampling is proposed.","Comprehensive experiments demonstrate that the proposed LLM-Stega outperforms current state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.10229v1","category":"cs.CL"}
{"created":"2024-04-16 01:55:09","title":"Optimizing Traffic Signal Control for Continuous-Flow Intersections: Benchmarking against a State-of-Practice Model","abstract":"Continuous-Flow Intersections (CFI), also known as Displaced Left-Turn (DLT) intersections, aim to improve the efficiency and safety of traffic junctions. A CFI introduces additional sub-intersections upstream of the main intersection to split the left-turn flow from the through movement before it arrives at the main intersection, decreasing the number of conflict points between left-turn and through movements. This study develops and examines a two-step optimization model for CFI traffic signal control design and demonstrates its performance across more than 300 different travel demand scenarios. The proposed model is compared against a benchmark state-of-practice CFI signal control model. Microsimulation results suggest that the proposed model reduces average delay by 17% and average queue length by 32% for a full CFI compared with the benchmark signal control model.","sentences":["Continuous-Flow Intersections (CFI), also known as Displaced Left-Turn (DLT) intersections, aim to improve the efficiency and safety of traffic junctions.","A CFI introduces additional sub-intersections upstream of the main intersection to split the left-turn flow from the through movement before it arrives at the main intersection, decreasing the number of conflict points between left-turn and through movements.","This study develops and examines a two-step optimization model for CFI traffic signal control design and demonstrates its performance across more than 300 different travel demand scenarios.","The proposed model is compared against a benchmark state-of-practice CFI signal control model.","Microsimulation results suggest that the proposed model reduces average delay by 17% and average queue length by 32% for a full CFI compared with the benchmark signal control model."],"url":"http://arxiv.org/abs/2404.10215v1","category":"math.OC"}
{"created":"2024-04-15 22:46:01","title":"Asymptotic mutual information in quadratic estimation problems over compact groups","abstract":"Motivated by applications to group synchronization and quadratic assignment on random data, we study a general problem of Bayesian inference of an unknown ``signal'' belonging to a high-dimensional compact group, given noisy pairwise observations of a featurization of this signal. We establish a quantitative comparison between the signal-observation mutual information in any such problem with that in a simpler model with linear observations, using interpolation methods. For group synchronization, our result proves a replica formula for the asymptotic mutual information and Bayes-optimal mean-squared-error. Via analyses of this replica formula, we show that the conjectural phase transition threshold for computationally-efficient weak recovery of the signal is determined by a classification of the real-irreducible components of the observed group representation(s), and we fully characterize the information-theoretic limits of estimation in the example of angular/phase synchronization over $SO(2)$/$U(1)$. For quadratic assignment, we study observations given by a kernel matrix of pairwise similarities and a randomly permutated and noisy counterpart, and we show in a bounded signal-to-noise regime that the asymptotic mutual information coincides with that in a Bayesian spiked model with i.i.d. signal prior.","sentences":["Motivated by applications to group synchronization and quadratic assignment on random data, we study a general problem of Bayesian inference of an unknown ``signal'' belonging to a high-dimensional compact group, given noisy pairwise observations of a featurization of this signal.","We establish a quantitative comparison between the signal-observation mutual information in any such problem with that in a simpler model with linear observations, using interpolation methods.","For group synchronization, our result proves a replica formula for the asymptotic mutual information and Bayes-optimal mean-squared-error.","Via analyses of this replica formula, we show that the conjectural phase transition threshold for computationally-efficient weak recovery of the signal is determined by a classification of the real-irreducible components of the observed group representation(s), and we fully characterize the information-theoretic limits of estimation in the example of angular/phase synchronization over $SO(2)$/$U(1)$. For quadratic assignment, we study observations given by a kernel matrix of pairwise similarities and a randomly permutated and noisy counterpart, and we show in a bounded signal-to-noise regime that the asymptotic mutual information coincides with that in a Bayesian spiked model with i.i.d. signal prior."],"url":"http://arxiv.org/abs/2404.10169v1","category":"math.ST"}
{"created":"2024-04-15 22:00:41","title":"Orbits, spirals, and trapped states: Dynamics of a phoretic Janus particle in a radial concentration gradient","abstract":"A longstanding goal in colloidal active matter is to understand how gradients in fuel concentration influence the motion of phoretic Janus particles. Here, we present a theoretical description of the motion of a spherical phoretic Janus particle in the presence of a radial gradient of the chemical solute driving self-propulsion. Radial gradients are a geometry relevant to many scenarios in active matter systems and naturally arise due to the presence of a point source or sink of fuel. We derive an analytical solution for the Janus particle's velocity and quantify the influence of the radial concentration gradient on the particle's trajectory. Compared to a phoretic Janus particle in a linear gradient in fuel concentration, we uncover a much richer set of dynamical behaviors, including circular orbits and trapped stationary states. We identify the ratio of the phoretic mobilities between the two domains of the Janus particle as a central quantity in tuning their dynamics. Our results provide a path for developing novel protocols for tuning the dynamics of phoretic Janus particles and mixing fluid at the microscale. In addition, this work suggests a method for quantifying the surface properties of phoretic Janus particles, which have proven challenging to probe experimentally.","sentences":["A longstanding goal in colloidal active matter is to understand how gradients in fuel concentration influence the motion of phoretic Janus particles.","Here, we present a theoretical description of the motion of a spherical phoretic Janus particle in the presence of a radial gradient of the chemical solute driving self-propulsion.","Radial gradients are a geometry relevant to many scenarios in active matter systems and naturally arise due to the presence of a point source or sink of fuel.","We derive an analytical solution for the Janus particle's velocity and quantify the influence of the radial concentration gradient on the particle's trajectory.","Compared to a phoretic Janus particle in a linear gradient in fuel concentration, we uncover a much richer set of dynamical behaviors, including circular orbits and trapped stationary states.","We identify the ratio of the phoretic mobilities between the two domains of the Janus particle as a central quantity in tuning their dynamics.","Our results provide a path for developing novel protocols for tuning the dynamics of phoretic Janus particles and mixing fluid at the microscale.","In addition, this work suggests a method for quantifying the surface properties of phoretic Janus particles, which have proven challenging to probe experimentally."],"url":"http://arxiv.org/abs/2404.10154v1","category":"cond-mat.soft"}
{"created":"2024-04-15 18:55:27","title":"Intergenerational Insurance","abstract":"How should successive generations insure each other when the young can default on previously promised transfers to the old? This paper studies intergenerational insurance that maximizes the expected discounted utility of all generations subject to participation constraints for each generation. If complete insurance is unattainable, the optimal intergenerational insurance is history-dependent even when the environment is stationary. The risk from a generational shock is spread into the future, with periodic resetting. Interpreting intergenerational insurance in terms of debt, the fiscal reaction function is nonlinear and the risk premium on debt is lower than the risk premium with complete insurance.","sentences":["How should successive generations insure each other when the young can default on previously promised transfers to the old?","This paper studies intergenerational insurance that maximizes the expected discounted utility of all generations subject to participation constraints for each generation.","If complete insurance is unattainable, the optimal intergenerational insurance is history-dependent even when the environment is stationary.","The risk from a generational shock is spread into the future, with periodic resetting.","Interpreting intergenerational insurance in terms of debt, the fiscal reaction function is nonlinear and the risk premium on debt is lower than the risk premium with complete insurance."],"url":"http://arxiv.org/abs/2404.10090v1","category":"econ.TH"}
{"created":"2024-04-15 18:50:30","title":"Empowering Enterprise Development by Building and Deploying Admin Dashboard using Refine Framework","abstract":"This project proposes the development of an advanced admin dashboard tailored for enterprise development, leveraging the Refine framework, Ant Design, and GraphQL API. It promises heightened operational efficiency by optimizing backend integration and employing GraphQL's dynamic data subscription for real-time insights. With an emphasis on modern aesthetics and user-centric design, it ensures seamless data visualization and management. Key functionalities encompass user administration, data visualization, CRUD operations, real-time notifications, and seamless integration with existing systems. The deliverable includes a deployable dashboard alongside comprehensive documentation, aiming to empower enterprise teams with a cutting-edge, data-driven solution.","sentences":["This project proposes the development of an advanced admin dashboard tailored for enterprise development, leveraging the Refine framework, Ant Design, and GraphQL API.","It promises heightened operational efficiency by optimizing backend integration and employing GraphQL's dynamic data subscription for real-time insights.","With an emphasis on modern aesthetics and user-centric design, it ensures seamless data visualization and management.","Key functionalities encompass user administration, data visualization, CRUD operations, real-time notifications, and seamless integration with existing systems.","The deliverable includes a deployable dashboard alongside comprehensive documentation, aiming to empower enterprise teams with a cutting-edge, data-driven solution."],"url":"http://arxiv.org/abs/2404.10086v1","category":"cs.DB"}
{"created":"2024-04-15 18:00:03","title":"Variational quantum simulation: a case study for understanding warm starts","abstract":"The barren plateau phenomenon, characterized by loss gradients that vanish exponentially with system size, poses a challenge to scaling variational quantum algorithms. Here we explore the potential of warm starts, whereby one initializes closer to a solution in the hope of enjoying larger loss variances. Focusing on an iterative variational method for learning shorter-depth circuits for quantum real and imaginary time evolution we conduct a case study to elucidate the potential and limitations of warm starts. We start by proving that the iterative variational algorithm will exhibit substantial (at worst vanishing polynomially in system size) gradients in a small region around the initializations at each time-step. Convexity guarantees for these regions are then established, suggesting trainability for polynomial size time-steps. However, our study highlights scenarios where a good minimum shifts outside the region with trainability guarantees. Our analysis leaves open the question whether such minima jumps necessitate optimization across barren plateau landscapes or whether there exist gradient flows, i.e., fertile valleys away from the plateau with substantial gradients, that allow for training.","sentences":["The barren plateau phenomenon, characterized by loss gradients that vanish exponentially with system size, poses a challenge to scaling variational quantum algorithms.","Here we explore the potential of warm starts, whereby one initializes closer to a solution in the hope of enjoying larger loss variances.","Focusing on an iterative variational method for learning shorter-depth circuits for quantum real and imaginary time evolution we conduct a case study to elucidate the potential and limitations of warm starts.","We start by proving that the iterative variational algorithm will exhibit substantial (at worst vanishing polynomially in system size) gradients in a small region around the initializations at each time-step.","Convexity guarantees for these regions are then established, suggesting trainability for polynomial size time-steps.","However, our study highlights scenarios where a good minimum shifts outside the region with trainability guarantees.","Our analysis leaves open the question whether such minima jumps necessitate optimization across barren plateau landscapes or whether there exist gradient flows, i.e., fertile valleys away from the plateau with substantial gradients, that allow for training."],"url":"http://arxiv.org/abs/2404.10044v1","category":"quant-ph"}
{"created":"2024-04-15 17:58:57","title":"OneChart: Purify the Chart Structural Extraction via One Auxiliary Token","abstract":"Chart parsing poses a significant challenge due to the diversity of styles, values, texts, and so forth. Even advanced large vision-language models (LVLMs) with billions of parameters struggle to handle such tasks satisfactorily. To address this, we propose OneChart: a reliable agent specifically devised for the structural extraction of chart information. Similar to popular LVLMs, OneChart incorporates an autoregressive main body. Uniquely, to enhance the reliability of the numerical parts of the output, we introduce an auxiliary token placed at the beginning of the total tokens along with an additional decoder. The numerically optimized (auxiliary) token allows subsequent tokens for chart parsing to capture enhanced numerical features through causal attention. Furthermore, with the aid of the auxiliary token, we have devised a self-evaluation mechanism that enables the model to gauge the reliability of its chart parsing results by providing confidence scores for the generated content. Compared to current state-of-the-art (SOTA) chart parsing models, e.g., DePlot, ChartVLM, ChartAst, OneChart significantly outperforms in Average Precision (AP) for chart structural extraction across multiple public benchmarks, despite enjoying only 0.2 billion parameters. Moreover, as a chart parsing agent, it also brings 10%+ accuracy gains for the popular LVLM (LLaVA-1.6) in the downstream ChartQA benchmark.","sentences":["Chart parsing poses a significant challenge due to the diversity of styles, values, texts, and so forth.","Even advanced large vision-language models (LVLMs) with billions of parameters struggle to handle such tasks satisfactorily.","To address this, we propose OneChart: a reliable agent specifically devised for the structural extraction of chart information.","Similar to popular LVLMs, OneChart incorporates an autoregressive main body.","Uniquely, to enhance the reliability of the numerical parts of the output, we introduce an auxiliary token placed at the beginning of the total tokens along with an additional decoder.","The numerically optimized (auxiliary) token allows subsequent tokens for chart parsing to capture enhanced numerical features through causal attention.","Furthermore, with the aid of the auxiliary token, we have devised a self-evaluation mechanism that enables the model to gauge the reliability of its chart parsing results by providing confidence scores for the generated content.","Compared to current state-of-the-art (SOTA) chart parsing models, e.g., DePlot, ChartVLM, ChartAst, OneChart significantly outperforms in Average Precision (AP) for chart structural extraction across multiple public benchmarks, despite enjoying only 0.2 billion parameters.","Moreover, as a chart parsing agent, it also brings 10%+ accuracy gains for the popular LVLM (LLaVA-1.6) in the downstream ChartQA benchmark."],"url":"http://arxiv.org/abs/2404.09987v1","category":"cs.CV"}
{"created":"2024-04-15 17:57:24","title":"Robot Positioning Using Torus Packing for Multisets","abstract":"We consider the design of a positioning system where a robot determines its position from local observations. This is a well-studied problem of considerable practical importance and mathematical interest. The dominant paradigm derives from the classical theory of de Bruijn sequences, where the robot has access to a window within a larger code and can determine its position if these windows are distinct. We propose an alternative model in which the robot has more limited observational powers, which we argue is more realistic in terms of engineering: the robot does not have access to the full pattern of colours (or letters) in the window, but only to the intensity of each colour (or the number of occurrences of each letter). This leads to a mathematically interesting problem with a different flavour to that arising in the classical paradigm, requiring new construction techniques. The parameters of our construction are optimal up to a constant factor, and computing the position requires only a constant number of arithmetic operations.","sentences":["We consider the design of a positioning system where a robot determines its position from local observations.","This is a well-studied problem of considerable practical importance and mathematical interest.","The dominant paradigm derives from the classical theory of de Bruijn sequences, where the robot has access to a window within a larger code and can determine its position if these windows are distinct.","We propose an alternative model in which the robot has more limited observational powers, which we argue is more realistic in terms of engineering: the robot does not have access to the full pattern of colours (or letters) in the window, but only to the intensity of each colour (or the number of occurrences of each letter).","This leads to a mathematically interesting problem with a different flavour to that arising in the classical paradigm, requiring new construction techniques.","The parameters of our construction are optimal up to a constant factor, and computing the position requires only a constant number of arithmetic operations."],"url":"http://arxiv.org/abs/2404.09981v1","category":"cs.DM"}
{"created":"2024-04-15 17:51:47","title":"Quantum Error Suppression with Subgroup Stabilisation Projectors","abstract":"Quantum state purification is the functionality that, given multiple copies of an unknown state, outputs a state with increased purity. This is an essential building block for near- and middle-term quantum ecosystems before the availability of full fault tolerance, where one may want to suppress errors not only in expectation values but also in quantum states. We propose an effective state purification gadget with a moderate quantum overhead by projecting $M$ noisy quantum inputs to their symmetric subspace defined by a set of projectors forming a symmetric subgroup with order $M$. Our method, applied in every short evolution over $M$ redundant copies of noisy states, can suppress both coherent and stochastic errors by a factor of $1/M$. This reduces the circuit implementation cost $M$ times smaller than the state projection to the full symmetric subspace proposed more than two decades ago by Barenco et al. We also show that our gadget purifies the depolarised inputs with probability $p$ to asymptotically $O\\left(p^{2}\\right)$ with an optimal choice of $M$ when $p$ is small. Our method provides flexible choices of state purification depending on the hardware restrictions before fully fault-tolerant computing is available. Our method may also find its application in designing robust verification protocols for quantum outputs.","sentences":["Quantum state purification is the functionality that, given multiple copies of an unknown state, outputs a state with increased purity.","This is an essential building block for near- and middle-term quantum ecosystems before the availability of full fault tolerance, where one may want to suppress errors not only in expectation values but also in quantum states.","We propose an effective state purification gadget with a moderate quantum overhead by projecting $M$ noisy quantum inputs to their symmetric subspace defined by a set of projectors forming a symmetric subgroup with order $M$. Our method, applied in every short evolution over $M$ redundant copies of noisy states, can suppress both coherent and stochastic errors by a factor of $1/M$. This reduces the circuit implementation cost $M$ times smaller than the state projection to the full symmetric subspace proposed more than two decades ago by Barenco et al.","We also show that our gadget purifies the depolarised inputs with probability $p$ to asymptotically $O\\left(p^{2}\\right)$ with an optimal choice of $M$ when $p$ is small.","Our method provides flexible choices of state purification depending on the hardware restrictions before fully fault-tolerant computing is available.","Our method may also find its application in designing robust verification protocols for quantum outputs."],"url":"http://arxiv.org/abs/2404.09973v1","category":"quant-ph"}
{"created":"2024-04-15 17:40:23","title":"Design and Analysis of Efficient Attention in Transformers for Social Group Activity Recognition","abstract":"Social group activity recognition is a challenging task extended from group activity recognition, where social groups must be recognized with their activities and group members. Existing methods tackle this task by leveraging region features of individuals following existing group activity recognition methods. However, the effectiveness of region features is susceptible to person localization and variable semantics of individual actions. To overcome these issues, we propose leveraging attention modules in transformers to generate social group features. In this method, multiple embeddings are used to aggregate features for a social group, each of which is assigned to a group member without duplication. Due to this non-duplicated assignment, the number of embeddings must be significant to avoid missing group members and thus renders attention in transformers ineffective. To find optimal attention designs with a large number of embeddings, we explore several design choices of queries for feature aggregation and self-attention modules in transformer decoders. Extensive experimental results show that the proposed method achieves state-of-the-art performance and verify that the proposed attention designs are highly effective on social group activity recognition.","sentences":["Social group activity recognition is a challenging task extended from group activity recognition, where social groups must be recognized with their activities and group members.","Existing methods tackle this task by leveraging region features of individuals following existing group activity recognition methods.","However, the effectiveness of region features is susceptible to person localization and variable semantics of individual actions.","To overcome these issues, we propose leveraging attention modules in transformers to generate social group features.","In this method, multiple embeddings are used to aggregate features for a social group, each of which is assigned to a group member without duplication.","Due to this non-duplicated assignment, the number of embeddings must be significant to avoid missing group members and thus renders attention in transformers ineffective.","To find optimal attention designs with a large number of embeddings, we explore several design choices of queries for feature aggregation and self-attention modules in transformer decoders.","Extensive experimental results show that the proposed method achieves state-of-the-art performance and verify that the proposed attention designs are highly effective on social group activity recognition."],"url":"http://arxiv.org/abs/2404.09964v1","category":"cs.CV"}
{"created":"2024-04-15 17:31:32","title":"How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model","abstract":"Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or ``best-practice'' guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.","sentences":["Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning.","While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise.","However, there are still no systematic analyses or ``best-practice'' guidelines for optimal fine-tuning of SAM for medical image segmentation.","This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities.","Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance.","We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings.","Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM."],"url":"http://arxiv.org/abs/2404.09957v1","category":"cs.CV"}
{"created":"2024-04-15 16:52:53","title":"Autonomous Path Planning for Intercostal Robotic Ultrasound Imaging Using Reinforcement Learning","abstract":"Ultrasound (US) has been widely used in daily clinical practice for screening internal organs and guiding interventions. However, due to the acoustic shadow cast by the subcutaneous rib cage, the US examination for thoracic application is still challenging. To fully cover and reconstruct the region of interest in US for diagnosis, an intercostal scanning path is necessary. To tackle this challenge, we present a reinforcement learning (RL) approach for planning scanning paths between ribs to monitor changes in lesions on internal organs, such as the liver and heart, which are covered by rib cages. Structured anatomical information of the human skeleton is crucial for planning these intercostal paths. To obtain such anatomical insight, an RL agent is trained in a virtual environment constructed using computational tomography (CT) templates with randomly initialized tumors of various shapes and locations. In addition, task-specific state representation and reward functions are introduced to ensure the convergence of the training process while minimizing the effects of acoustic attenuation and shadows during scanning. To validate the effectiveness of the proposed approach, experiments have been carried out on unseen CTs with randomly defined single or multiple scanning targets. The results demonstrate the efficiency of the proposed RL framework in planning non-shadowed US scanning trajectories in areas with limited acoustic access.","sentences":["Ultrasound (US) has been widely used in daily clinical practice for screening internal organs and guiding interventions.","However, due to the acoustic shadow cast by the subcutaneous rib cage, the US examination for thoracic application is still challenging.","To fully cover and reconstruct the region of interest in US for diagnosis, an intercostal scanning path is necessary.","To tackle this challenge, we present a reinforcement learning (RL) approach for planning scanning paths between ribs to monitor changes in lesions on internal organs, such as the liver and heart, which are covered by rib cages.","Structured anatomical information of the human skeleton is crucial for planning these intercostal paths.","To obtain such anatomical insight, an RL agent is trained in a virtual environment constructed using computational tomography (CT) templates with randomly initialized tumors of various shapes and locations.","In addition, task-specific state representation and reward functions are introduced to ensure the convergence of the training process while minimizing the effects of acoustic attenuation and shadows during scanning.","To validate the effectiveness of the proposed approach, experiments have been carried out on unseen CTs with randomly defined single or multiple scanning targets.","The results demonstrate the efficiency of the proposed RL framework in planning non-shadowed US scanning trajectories in areas with limited acoustic access."],"url":"http://arxiv.org/abs/2404.09927v1","category":"cs.RO"}
{"created":"2024-04-15 16:52:47","title":"Lieb-Thirring inequality for the 2D Pauli operator","abstract":"By the Aharonov-Casher theorem, the Pauli operator $P$ has no zero eigenvalue when the normalized magnetic flux $\\alpha$ satisfies $|\\alpha|<1$, but it does have a zero energy resonance. We prove that in this case a Lieb-Thirring inequality for the $\\gamma$-th moment of the eigenvalues of $P+V$ is valid under the optimal restrictions $\\gamma\\geq |\\alpha|$ and $\\gamma>0$. Besides the usual semiclassical integral, the right side of our inequality involves an integral where the zero energy resonance state appears explicitly. Our inequality improves earlier works that were restricted to moments of order $\\gamma\\geq 1$.","sentences":["By the Aharonov-Casher theorem, the Pauli operator $P$ has no zero eigenvalue when the normalized magnetic flux $\\alpha$ satisfies $|\\alpha|<1$, but it does have a zero energy resonance.","We prove that in this case a Lieb-Thirring inequality for the $\\gamma$-th moment of the eigenvalues of $P+V$ is valid under the optimal restrictions $\\gamma\\geq |\\alpha|$ and $\\gamma>0$.","Besides the usual semiclassical integral, the right side of our inequality involves an integral where the zero energy resonance state appears explicitly.","Our inequality improves earlier works that were restricted to moments of order $\\gamma\\geq 1$."],"url":"http://arxiv.org/abs/2404.09926v1","category":"math-ph"}
{"created":"2024-04-15 16:37:24","title":"A fast photometric image alignment algorithm with row and column means","abstract":"This paper introduces an astronomical image alignment algorithm. This algorithm uses the means of the rows and columns of the original image for alignment, and finds the optimal offset corresponding to the maximum similarity by comparing different offsets between images. The similarity is evaluated by the standard deviation of the quotient divided by the means. This paper also discusses the theoretical feasibility of this algorithm. Through practical testing, it has been confirmed that the algorithm is fast and robust.","sentences":["This paper introduces an astronomical image alignment algorithm.","This algorithm uses the means of the rows and columns of the original image for alignment, and finds the optimal offset corresponding to the maximum similarity by comparing different offsets between images.","The similarity is evaluated by the standard deviation of the quotient divided by the means.","This paper also discusses the theoretical feasibility of this algorithm.","Through practical testing, it has been confirmed that the algorithm is fast and robust."],"url":"http://arxiv.org/abs/2404.09912v1","category":"astro-ph.IM"}
{"created":"2024-04-15 16:25:42","title":"Global controllability of Boussinesq flows by using only a temperature control","abstract":"We show that buoyancy driven flows can be steered in an arbitrary time towards any state by applying as control only an external temperature profile in a subset of small measure. More specifically, we prove that the 2D incompressible Boussinesq system on the torus is globally approximately controllable via physically localized heating or cooling. In addition, our controls have an explicitly prescribed structure; even without such structural requirements, large data controllability results for Boussinesq flows driven merely by a physically localized temperature profile were so far unknown. The presented method exploits various connections between the model's underlying transport-, coupling-, and scaling mechanisms.","sentences":["We show that buoyancy driven flows can be steered in an arbitrary time towards any state by applying as control only an external temperature profile in a subset of small measure.","More specifically, we prove that the 2D incompressible Boussinesq system on the torus is globally approximately controllable via physically localized heating or cooling.","In addition, our controls have an explicitly prescribed structure; even without such structural requirements, large data controllability results for Boussinesq flows driven merely by a physically localized temperature profile were so far unknown.","The presented method exploits various connections between the model's underlying transport-, coupling-, and scaling mechanisms."],"url":"http://arxiv.org/abs/2404.09903v1","category":"math.AP"}
{"created":"2024-04-15 16:20:53","title":"Gate tunable enhancement of supercurrent in hybrid planar Josephson junctions","abstract":"Planar Josephson junctions (JJs) have emerged as a promising platform for the realization of topological superconductivity and Majorana zero modes. To obtain robust quasi one-dimensional (1D) topological superconducting states using planar JJs, limiting the number of 1D Andreev bound states' subbands that can be present, and increasing the size of the topological superconducting gap, are two fundamental challenges. It has been suggested that both problems can be addressed by properly designing the interfaces between the JJ's normal region and the superconducting leads. We fabricated Josephson junctions with periodic hole structures on the superconducting contact leads on InAs heterostructures with epitaxial superconducting Al. By depleting the chemical potential inside the hole region with a top gate, we observed an enhancement of the supercurrent across the junction. Such an enhancement is reproduced in theoretical simulations. The theoretical analysis shows that the enhancement of the JJ's critical current is achieved when the hole depletion is such to optimize the matching of quasiparticles' wave-function at the normal/superconductor interface. These results show how the combination of carefully designed patterns for the Al coverage, and external gates, can be successfully used to tune the density and wave functions' profiles in the normal region of the JJ, and therefore open a new avenue to tune some of the critical properties, such as number of subbands and size of the topological gap, that must be optimized to obtain robust quasi 1D superconducting states supporting Majorana bound states.","sentences":["Planar Josephson junctions (JJs) have emerged as a promising platform for the realization of topological superconductivity and Majorana zero modes.","To obtain robust quasi one-dimensional (1D) topological superconducting states using planar JJs, limiting the number of 1D Andreev bound states' subbands that can be present, and increasing the size of the topological superconducting gap, are two fundamental challenges.","It has been suggested that both problems can be addressed by properly designing the interfaces between the JJ's normal region and the superconducting leads.","We fabricated Josephson junctions with periodic hole structures on the superconducting contact leads on InAs heterostructures with epitaxial superconducting Al.","By depleting the chemical potential inside the hole region with a top gate, we observed an enhancement of the supercurrent across the junction.","Such an enhancement is reproduced in theoretical simulations.","The theoretical analysis shows that the enhancement of the JJ's critical current is achieved when the hole depletion is such to optimize the matching of quasiparticles' wave-function at the normal/superconductor interface.","These results show how the combination of carefully designed patterns for the Al coverage, and external gates, can be successfully used to tune the density and wave functions' profiles in the normal region of the JJ, and therefore open a new avenue to tune some of the critical properties, such as number of subbands and size of the topological gap, that must be optimized to obtain robust quasi 1D superconducting states supporting Majorana bound states."],"url":"http://arxiv.org/abs/2404.09901v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-15 15:48:14","title":"Guiding polarizable particles in multi-hole Gaussian beams","abstract":"The present paper discusses certain special Gaussian beams that, thanks to some polynomial prefactors, have uniquely designed holes in the irradiance. Such holes, or rather tubes, can constitute potential valleys for negatively polarizable particles, providing the possibility of guiding several objects of that kind, each along its own trajectory. The mechanism of creating these holes by interference of Gaussian beams which exhibit orbital angular momentum is discussed, and then the trajectories of particles moving in such a wave are numerically calculated. As it turns out, these particles, performing transverse oscillations, follow the designed tunnels of low irradiance. On the contrary, for particles with positive polarizability these areas are inaccessible.","sentences":["The present paper discusses certain special Gaussian beams that, thanks to some polynomial prefactors, have uniquely designed holes in the irradiance.","Such holes, or rather tubes, can constitute potential valleys for negatively polarizable particles, providing the possibility of guiding several objects of that kind, each along its own trajectory.","The mechanism of creating these holes by interference of Gaussian beams which exhibit orbital angular momentum is discussed, and then the trajectories of particles moving in such a wave are numerically calculated.","As it turns out, these particles, performing transverse oscillations, follow the designed tunnels of low irradiance.","On the contrary, for particles with positive polarizability these areas are inaccessible."],"url":"http://arxiv.org/abs/2404.09880v1","category":"physics.optics"}
{"created":"2024-04-15 15:36:38","title":"Table tennis ball spin estimation with an event camera","abstract":"Spin plays a pivotal role in ball-based sports. Estimating spin becomes a key skill due to its impact on the ball's trajectory and bouncing behavior. Spin cannot be observed directly, making it inherently challenging to estimate. In table tennis, the combination of high velocity and spin renders traditional low frame rate cameras inadequate for quickly and accurately observing the ball's logo to estimate the spin due to the motion blur. Event cameras do not suffer as much from motion blur, thanks to their high temporal resolution. Moreover, the sparse nature of the event stream solves communication bandwidth limitations many frame cameras face. To the best of our knowledge, we present the first method for table tennis spin estimation using an event camera. We use ordinal time surfaces to track the ball and then isolate the events generated by the logo on the ball. Optical flow is then estimated from the extracted events to infer the ball's spin. We achieved a spin magnitude mean error of $10.7 \\pm 17.3$ rps and a spin axis mean error of $32.9 \\pm 38.2\\deg$ in real time for a flying ball.","sentences":["Spin plays a pivotal role in ball-based sports.","Estimating spin becomes a key skill due to its impact on the ball's trajectory and bouncing behavior.","Spin cannot be observed directly, making it inherently challenging to estimate.","In table tennis, the combination of high velocity and spin renders traditional low frame rate cameras inadequate for quickly and accurately observing the ball's logo to estimate the spin due to the motion blur.","Event cameras do not suffer as much from motion blur, thanks to their high temporal resolution.","Moreover, the sparse nature of the event stream solves communication bandwidth limitations many frame cameras face.","To the best of our knowledge, we present the first method for table tennis spin estimation using an event camera.","We use ordinal time surfaces to track the ball and then isolate the events generated by the logo on the ball.","Optical flow is then estimated from the extracted events to infer the ball's spin.","We achieved a spin magnitude mean error of $10.7 \\pm 17.3$ rps and a spin axis mean error of $32.9 \\pm 38.2\\deg$ in real time for a flying ball."],"url":"http://arxiv.org/abs/2404.09870v1","category":"cs.CV"}
{"created":"2024-04-15 15:34:36","title":"A Systematic Methodology for Modeling and Attitude Control of Multi-body Space Telescopes","abstract":"This paper derives a symbolic multi-body rigid nonlinear model for a space telescope using Stoneking's implementation of Kane's method. This symbolic nonlinear model is linearized using Matlab symbolic functions {\\tt diff} and {\\tt inv} because the analytic linearization is intractable for manual derivation. The linearized system model is then used to design the controllers using both linear quadratic regulator (LQR) and robust pole assignment methods. The closed-loop systems for the two designs are simulated using both the rigid model as well as a second model containing flexible modes. The performances of the two designs are compared based on the simulation testing results. Our conclusion is that the robust pole assignment design offers better performance than that of the LQR system in terms of actuator usage and pointing accuracy. However, the LQR approach remains an effective first design step that can inform the selection of real eigenvalues for robust pole assignment. The proposed method may be used for the modeling and controller designs for various multi-body systems.","sentences":["This paper derives a symbolic multi-body rigid nonlinear model for a space telescope using Stoneking's implementation of Kane's method.","This symbolic nonlinear model is linearized using Matlab symbolic functions {\\tt diff} and {\\tt inv} because the analytic linearization is intractable for manual derivation.","The linearized system model is then used to design the controllers using both linear quadratic regulator (LQR) and robust pole assignment methods.","The closed-loop systems for the two designs are simulated using both the rigid model as well as a second model containing flexible modes.","The performances of the two designs are compared based on the simulation testing results.","Our conclusion is that the robust pole assignment design offers better performance than that of the LQR system in terms of actuator usage and pointing accuracy.","However, the LQR approach remains an effective first design step that can inform the selection of real eigenvalues for robust pole assignment.","The proposed method may be used for the modeling and controller designs for various multi-body systems."],"url":"http://arxiv.org/abs/2404.09869v1","category":"math.OC"}
{"created":"2024-04-15 15:17:38","title":"Unsupervised Federated Optimization at the Edge: D2D-Enabled Learning without Labels","abstract":"Federated learning (FL) is a popular solution for distributed machine learning (ML). While FL has traditionally been studied for supervised ML tasks, in many applications, it is impractical to assume availability of labeled data across devices. To this end, we develop Cooperative Federated unsupervised Contrastive Learning ({\\tt CF-CL)} to facilitate FL across edge devices with unlabeled datasets. {\\tt CF-CL} employs local device cooperation where either explicit (i.e., raw data) or implicit (i.e., embeddings) information is exchanged through device-to-device (D2D) communications to improve local diversity. Specifically, we introduce a \\textit{smart information push-pull} methodology for data/embedding exchange tailored to FL settings with either soft or strict data privacy restrictions. Information sharing is conducted through a probabilistic importance sampling technique at receivers leveraging a carefully crafted reserve dataset provided by transmitters. In the implicit case, embedding exchange is further integrated into the local ML training at the devices via a regularization term incorporated into the contrastive loss, augmented with a dynamic contrastive margin to adjust the volume of latent space explored. Numerical evaluations demonstrate that {\\tt CF-CL} leads to alignment of latent spaces learned across devices, results in faster and more efficient global model training, and is effective in extreme non-i.i.d. data distribution settings across devices.","sentences":["Federated learning (FL) is a popular solution for distributed machine learning (ML).","While FL has traditionally been studied for supervised ML tasks, in many applications, it is impractical to assume availability of labeled data across devices.","To this end, we develop Cooperative Federated unsupervised Contrastive Learning ({\\tt CF-CL)} to facilitate FL across edge devices with unlabeled datasets.","{\\tt CF-CL} employs local device cooperation where either explicit (i.e., raw data) or implicit (i.e., embeddings) information is exchanged through device-to-device (D2D) communications to improve local diversity.","Specifically, we introduce a \\textit{smart information push-pull} methodology for data/embedding exchange tailored to FL settings with either soft or strict data privacy restrictions.","Information sharing is conducted through a probabilistic importance sampling technique at receivers leveraging a carefully crafted reserve dataset provided by transmitters.","In the implicit case, embedding exchange is further integrated into the local ML training at the devices via a regularization term incorporated into the contrastive loss, augmented with a dynamic contrastive margin to adjust the volume of latent space explored.","Numerical evaluations demonstrate that {\\tt CF-CL} leads to alignment of latent spaces learned across devices, results in faster and more efficient global model training, and is effective in extreme non-i.i.d. data distribution settings across devices."],"url":"http://arxiv.org/abs/2404.09861v1","category":"cs.LG"}
{"created":"2024-04-15 15:00:40","title":"Steering opinion dynamics through control of social networks","abstract":"In this paper we propose a novel control approach for opinion dynamics on evolving networks. The controls modify the strength of connections in the network, rather than influencing opinions directly, with the overall goal of steering the population towards a target opinion. This requires that the social network remains sufficiently connected, the population does not break into separate opinion clusters, and that the target opinion remains accessible. We present several approaches to addressing these challenges, considering questions of controllability, instantaneous control and optimal control. Each of these approaches provides a different view on the complex relationship between opinion and network dynamics and raises interesting questions for future research.","sentences":["In this paper we propose a novel control approach for opinion dynamics on evolving networks.","The controls modify the strength of connections in the network, rather than influencing opinions directly, with the overall goal of steering the population towards a target opinion.","This requires that the social network remains sufficiently connected, the population does not break into separate opinion clusters, and that the target opinion remains accessible.","We present several approaches to addressing these challenges, considering questions of controllability, instantaneous control and optimal control.","Each of these approaches provides a different view on the complex relationship between opinion and network dynamics and raises interesting questions for future research."],"url":"http://arxiv.org/abs/2404.09849v1","category":"physics.soc-ph"}
{"created":"2024-04-15 14:59:21","title":"Statistical learning for constrained functional parameters in infinite-dimensional models with applications in fair machine learning","abstract":"Constrained learning has become increasingly important, especially in the realm of algorithmic fairness and machine learning. In these settings, predictive models are developed specifically to satisfy pre-defined notions of fairness. Here, we study the general problem of constrained statistical machine learning through a statistical functional lens. We consider learning a function-valued parameter of interest under the constraint that one or several pre-specified real-valued functional parameters equal zero or are otherwise bounded. We characterize the constrained functional parameter as the minimizer of a penalized risk criterion using a Lagrange multiplier formulation. We show that closed-form solutions for the optimal constrained parameter are often available, providing insight into mechanisms that drive fairness in predictive models. Our results also suggest natural estimators of the constrained parameter that can be constructed by combining estimates of unconstrained parameters of the data generating distribution. Thus, our estimation procedure for constructing fair machine learning algorithms can be applied in conjunction with any statistical learning approach and off-the-shelf software. We demonstrate the generality of our method by explicitly considering a number of examples of statistical fairness constraints and implementing the approach using several popular learning approaches.","sentences":["Constrained learning has become increasingly important, especially in the realm of algorithmic fairness and machine learning.","In these settings, predictive models are developed specifically to satisfy pre-defined notions of fairness.","Here, we study the general problem of constrained statistical machine learning through a statistical functional lens.","We consider learning a function-valued parameter of interest under the constraint that one or several pre-specified real-valued functional parameters equal zero or are otherwise bounded.","We characterize the constrained functional parameter as the minimizer of a penalized risk criterion using a Lagrange multiplier formulation.","We show that closed-form solutions for the optimal constrained parameter are often available, providing insight into mechanisms that drive fairness in predictive models.","Our results also suggest natural estimators of the constrained parameter that can be constructed by combining estimates of unconstrained parameters of the data generating distribution.","Thus, our estimation procedure for constructing fair machine learning algorithms can be applied in conjunction with any statistical learning approach and off-the-shelf software.","We demonstrate the generality of our method by explicitly considering a number of examples of statistical fairness constraints and implementing the approach using several popular learning approaches."],"url":"http://arxiv.org/abs/2404.09847v1","category":"stat.ML"}
{"created":"2024-04-15 14:55:40","title":"Stable Inversion of Piecewise Affine Systems with Application to Feedforward and Iterative Learning Control","abstract":"Model inversion is a fundamental technique in feedforward control. Unstable inverse models present a challenge in that useful feedforward control trajectories cannot be generated by directly propagating them. Stable inversion is a process for generating useful trajectories from unstable inverses by handling their stable and unstable modes separately. Piecewise affine (PWA) systems are a popular framework for modeling complicated dynamics. The primary contributions of this article are closed-form inverse formulas for a general class of PWA models, and stable inversion methods for these models. Both contributions leverage closed-form model representations to prove sufficient conditions for solution existence and uniqueness, and to develop solution computation methods. The result is implementable feedforward control synthesis from PWA models with either stable or unstable inverses. In practice, feedforward control alone may yield substantial tracking errors due to mismatch between the known system model and the unknowable complete system physics. Iterative learning control (ILC) is a technique for achieving robustness to model error in feedforward control. To demonstrate the primary contributions' validity and utility, this article also integrates PWA stable inversion with ILC in simulations based on a physical printhead positioning system.","sentences":["Model inversion is a fundamental technique in feedforward control.","Unstable inverse models present a challenge in that useful feedforward control trajectories cannot be generated by directly propagating them.","Stable inversion is a process for generating useful trajectories from unstable inverses by handling their stable and unstable modes separately.","Piecewise affine (PWA) systems are a popular framework for modeling complicated dynamics.","The primary contributions of this article are closed-form inverse formulas for a general class of PWA models, and stable inversion methods for these models.","Both contributions leverage closed-form model representations to prove sufficient conditions for solution existence and uniqueness, and to develop solution computation methods.","The result is implementable feedforward control synthesis from PWA models with either stable or unstable inverses.","In practice, feedforward control alone may yield substantial tracking errors due to mismatch between the known system model and the unknowable complete system physics.","Iterative learning control (ILC) is a technique for achieving robustness to model error in feedforward control.","To demonstrate the primary contributions' validity and utility, this article also integrates PWA stable inversion with ILC in simulations based on a physical printhead positioning system."],"url":"http://arxiv.org/abs/2404.09845v1","category":"eess.SY"}
{"created":"2024-04-15 14:48:43","title":"Anatomy of Industrial Scale Multilingual ASR","abstract":"This paper describes AssemblyAI's industrial-scale automatic speech recognition (ASR) system, designed to meet the requirements of large-scale, multilingual ASR serving various application needs. Our system leverages a diverse training dataset comprising unsupervised (12.5M hours), supervised (188k hours), and pseudo-labeled (1.6M hours) data across four languages. We provide a detailed description of our model architecture, consisting of a full-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an RNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation demonstrates competitive word error rates (WERs) against larger and more computationally expensive models, such as Whisper large and Canary-1B. Furthermore, our architectural choices yield several key advantages, including an improved code-switching capability, a 5x inference speedup compared to an optimized Whisper baseline, a 30% reduction in hallucination rate on speech data, and a 90% reduction in ambient noise compared to Whisper, along with significantly improved time-stamp accuracy. Throughout this work, we adopt a system-centric approach to analyzing various aspects of fully-fledged ASR models to gain practically relevant insights useful for real-world services operating at scale.","sentences":["This paper describes AssemblyAI's industrial-scale automatic speech recognition (ASR) system, designed to meet the requirements of large-scale, multilingual ASR serving various application needs.","Our system leverages a diverse training dataset comprising unsupervised (12.5M hours), supervised (188k hours), and pseudo-labeled (1.6M hours) data across four languages.","We provide a detailed description of our model architecture, consisting of a full-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an RNN-T decoder fine-tuned jointly with the encoder.","Our extensive evaluation demonstrates competitive word error rates (WERs) against larger and more computationally expensive models, such as Whisper large and Canary-1B.","Furthermore, our architectural choices yield several key advantages, including an improved code-switching capability, a 5x inference speedup compared to an optimized Whisper baseline, a 30% reduction in hallucination rate on speech data, and a 90% reduction in ambient noise compared to Whisper, along with significantly improved time-stamp accuracy.","Throughout this work, we adopt a system-centric approach to analyzing various aspects of fully-fledged ASR models to gain practically relevant insights useful for real-world services operating at scale."],"url":"http://arxiv.org/abs/2404.09841v2","category":"eess.AS"}
{"created":"2024-04-15 14:45:11","title":"Hierarchical Fault-Tolerant Coverage Control for an Autonomous Aerial Agent","abstract":"Fault-tolerant coverage control involves determining a trajectory that enables an autonomous agent to cover specific points of interest, even in the presence of actuation and/or sensing faults. In this work, the agent encounters control inputs that are erroneous; specifically, its nominal controls inputs are perturbed by stochastic disturbances, potentially disrupting its intended operation. Existing techniques have focused on deterministically bounded disturbances or relied on the assumption of Gaussian disturbances, whereas non-Gaussian disturbances have been primarily been tackled via scenario-based stochastic control methods. However, the assumption of Gaussian disturbances is generally limited to linear systems, and scenario-based methods can become computationally prohibitive. To address these limitations, we propose a hierarchical coverage controller that integrates mixed-trigonometric-polynomial moment propagation to propagate non-Gaussian disturbances through the agent's nonlinear dynamics. Specifically, the first stage generates an ideal reference plan by optimising the agent's mobility and camera control inputs. The second-stage fault-tolerant controller then aims to follow this reference plan, even in the presence of erroneous control inputs caused by non-Gaussian disturbances. This is achieved by imposing a set of deterministic constraints on the moments of the system's uncertain states.","sentences":["Fault-tolerant coverage control involves determining a trajectory that enables an autonomous agent to cover specific points of interest, even in the presence of actuation and/or sensing faults.","In this work, the agent encounters control inputs that are erroneous; specifically, its nominal controls inputs are perturbed by stochastic disturbances, potentially disrupting its intended operation.","Existing techniques have focused on deterministically bounded disturbances or relied on the assumption of Gaussian disturbances, whereas non-Gaussian disturbances have been primarily been tackled via scenario-based stochastic control methods.","However, the assumption of Gaussian disturbances is generally limited to linear systems, and scenario-based methods can become computationally prohibitive.","To address these limitations, we propose a hierarchical coverage controller that integrates mixed-trigonometric-polynomial moment propagation to propagate non-Gaussian disturbances through the agent's nonlinear dynamics.","Specifically, the first stage generates an ideal reference plan by optimising the agent's mobility and camera control inputs.","The second-stage fault-tolerant controller then aims to follow this reference plan, even in the presence of erroneous control inputs caused by non-Gaussian disturbances.","This is achieved by imposing a set of deterministic constraints on the moments of the system's uncertain states."],"url":"http://arxiv.org/abs/2404.09838v1","category":"eess.SY"}
{"created":"2024-04-15 14:31:53","title":"No-Regret Algorithms in non-Truthful Auctions with Budget and ROI Constraints","abstract":"Advertisers increasingly use automated bidding to optimize their ad campaigns on online advertising platforms. Autobidding optimizes an advertiser's objective subject to various constraints, e.g. average ROI and budget constraints. In this paper, we study the problem of designing online autobidding algorithms to optimize value subject to ROI and budget constraints when the platform is running any mixture of first and second price auction.   We consider the following stochastic setting: There is an item for sale in each of $T$ rounds. In each round, buyers submit bids and an auction is run to sell the item. We focus on one buyer, possibly with budget and ROI constraints. We assume that the buyer's value and the highest competing bid are drawn i.i.d. from some unknown (joint) distribution in each round. We design a low-regret bidding algorithm that satisfies the buyer's constraints. Our benchmark is the objective value achievable by the best possible Lipschitz function that maps values to bids, which is rich enough to best respond to many different correlation structures between value and highest competing bid. Our main result is an algorithm with full information feedback that guarantees a near-optimal $\\tilde O(\\sqrt T)$ regret with respect to the best Lipschitz function. Our result applies to a wide range of auctions, most notably any mixture of first and second price auctions (price is a convex combination of the first and second price). In addition, our result holds for both value-maximizing buyers and quasi-linear utility-maximizing buyers.   We also study the bandit setting, where we show an $\\Omega(T^{2/3})$ lower bound on the regret for first-price auctions, showing a large disparity between the full information and bandit settings. We also design an algorithm with $\\tilde O(T^{3/4})$ regret, when the value distribution is known and is independent of the highest competing bid.","sentences":["Advertisers increasingly use automated bidding to optimize their ad campaigns on online advertising platforms.","Autobidding optimizes an advertiser's objective subject to various constraints, e.g. average ROI and budget constraints.","In this paper, we study the problem of designing online autobidding algorithms to optimize value subject to ROI and budget constraints when the platform is running any mixture of first and second price auction.   ","We consider the following stochastic setting: There is an item for sale in each of $T$ rounds.","In each round, buyers submit bids and an auction is run to sell the item.","We focus on one buyer, possibly with budget and ROI constraints.","We assume that the buyer's value and the highest competing bid are drawn i.i.d.","from some unknown (joint) distribution in each round.","We design a low-regret bidding algorithm that satisfies the buyer's constraints.","Our benchmark is the objective value achievable by the best possible Lipschitz function that maps values to bids, which is rich enough to best respond to many different correlation structures between value and highest competing bid.","Our main result is an algorithm with full information feedback that guarantees a near-optimal $\\tilde O(\\sqrt T)$ regret with respect to the best Lipschitz function.","Our result applies to a wide range of auctions, most notably any mixture of first and second price auctions (price is a convex combination of the first and second price).","In addition, our result holds for both value-maximizing buyers and quasi-linear utility-maximizing buyers.   ","We also study the bandit setting, where we show an $\\Omega(T^{2/3})$ lower bound on the regret for first-price auctions, showing a large disparity between the full information and bandit settings.","We also design an algorithm with $\\tilde O(T^{3/4})$ regret, when the value distribution is known and is independent of the highest competing bid."],"url":"http://arxiv.org/abs/2404.09832v1","category":"cs.GT"}
{"created":"2024-04-15 14:21:53","title":"Impact of Preference Noise on the Alignment Performance of Generative Language Models","abstract":"A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human values. Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first elicited from human annotators or AI systems, and then fed into some alignment techniques, e.g., Direct Preference Optimization. However, a substantial percent (20 - 40%) of the preference pairs used in GLM alignment are noisy, and it remains unclear how the noise affects the alignment performance and how to mitigate its negative impact. In this paper, we propose a framework to inject desirable amounts and types of noise to the preferences, and systematically study the impact of preference noise on the alignment performance in two tasks (summarization and dialogue generation). We find that the alignment performance can be highly sensitive to the noise rates in the preference data: e.g., a 10 percentage points (pp) increase of the noise rate can lead to 30 pp drop in the alignment performance (in win rate). To mitigate the impact of noise, confidence-based data filtering shows significant benefit when certain types of noise are present. We hope our work can help the community better understand and mitigate the impact of preference noise in GLM alignment.","sentences":["A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human values.","Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first elicited from human annotators or AI systems, and then fed into some alignment techniques, e.g., Direct Preference Optimization.","However, a substantial percent (20 - 40%) of the preference pairs used in GLM alignment are noisy, and it remains unclear how the noise affects the alignment performance and how to mitigate its negative impact.","In this paper, we propose a framework to inject desirable amounts and types of noise to the preferences, and systematically study the impact of preference noise on the alignment performance in two tasks (summarization and dialogue generation).","We find that the alignment performance can be highly sensitive to the noise rates in the preference data: e.g., a 10 percentage points (pp) increase of the noise rate can lead to 30 pp drop in the alignment performance (in win rate).","To mitigate the impact of noise, confidence-based data filtering shows significant benefit when certain types of noise are present.","We hope our work can help the community better understand and mitigate the impact of preference noise in GLM alignment."],"url":"http://arxiv.org/abs/2404.09824v1","category":"cs.CL"}
{"created":"2024-04-15 14:07:41","title":"The Challenges of Optimization For Data Science","abstract":"Optimization problems arising in data science have given rise to a number of new derivative-based optimization methods. Such methods often use standard smoothness assumptions -- namely, global Lipschitz continuity of the gradient function -- to establish a convergence theory. Unfortunately, in this work, we show that common optimization problems from data science applications are not globally Lipschitz smooth, nor do they satisfy some more recently developed smoothness conditions in literature. Instead, we show that such optimization problems are better modeled as having locally Lipschitz continuous gradients. We then construct explicit examples satisfying this assumption on which existing classes of optimization methods are either unreliable or experience an explosion in evaluation complexity. In summary, we show that optimization problems arising in data science are particularly difficult to solve, and that there is a need for methods that can reliably and practically solve these problems.","sentences":["Optimization problems arising in data science have given rise to a number of new derivative-based optimization methods.","Such methods often use standard smoothness assumptions -- namely, global Lipschitz continuity of the gradient function -- to establish a convergence theory.","Unfortunately, in this work, we show that common optimization problems from data science applications are not globally Lipschitz smooth, nor do they satisfy some more recently developed smoothness conditions in literature.","Instead, we show that such optimization problems are better modeled as having locally Lipschitz continuous gradients.","We then construct explicit examples satisfying this assumption on which existing classes of optimization methods are either unreliable or experience an explosion in evaluation complexity.","In summary, we show that optimization problems arising in data science are particularly difficult to solve, and that there is a need for methods that can reliably and practically solve these problems."],"url":"http://arxiv.org/abs/2404.09810v1","category":"math.OC"}
{"created":"2024-04-15 13:40:37","title":"MPC using mixed-integer programming for aquifer thermal energy storages","abstract":"Aquifer thermal energy storages (ATES) are used to temporally store thermal energy in groundwater saturated aquifers. Typically, two storages are combined, one for heat and one for cold, to support heating and cooling of buildings. This way, the use of classical fossil fuel-based heating, ventilation, and air conditioning can be significantly reduced. Exploiting the benefits of ATES beyond ``seasonal'' heating in winter and cooling in summer as well as meeting legislative restrictions requires sophisticated control. We propose a tailored model predictive control (MPC) scheme for the sustainable operation of ATES systems, which mainly builds on a novel model and objective function. The new approach leads to a mixed-integer quadratic program. Its performance is evaluated on real data from an ATES system in Belgium.","sentences":["Aquifer thermal energy storages (ATES) are used to temporally store thermal energy in groundwater saturated aquifers.","Typically, two storages are combined, one for heat and one for cold, to support heating and cooling of buildings.","This way, the use of classical fossil fuel-based heating, ventilation, and air conditioning can be significantly reduced.","Exploiting the benefits of ATES beyond ``seasonal'' heating in winter and cooling in summer as well as meeting legislative restrictions requires sophisticated control.","We propose a tailored model predictive control (MPC) scheme for the sustainable operation of ATES systems, which mainly builds on a novel model and objective function.","The new approach leads to a mixed-integer quadratic program.","Its performance is evaluated on real data from an ATES system in Belgium."],"url":"http://arxiv.org/abs/2404.09786v1","category":"eess.SY"}
{"created":"2024-04-15 13:28:42","title":"A cut-and-project perspective for linearized Bregman iterations","abstract":"The linearized Bregman iterations (LBreI) and its variants are powerful tools for finding sparse or low-rank solutions to underdetermined linear systems. In this study, we propose a cut-and-project perspective for the linearized Bregman method via a bilevel optimization formulation, along with a new unified algorithmic framework. The new perspective not only encompasses various existing linearized Bregman iteration variants as specific instances, but also allows us to extend the linearized Bregman method to solve more general inverse problems. We provide a completed convergence result of the proposed algorithmic framework, including convergence guarantees to feasible points and optimal solutions, and the sublinear convergence rate. Moreover, we introduce the Bregman distance growth condition to ensure linear convergence. At last, our findings are illustrated via numerical tests.","sentences":["The linearized Bregman iterations (LBreI) and its variants are powerful tools for finding sparse or low-rank solutions to underdetermined linear systems.","In this study, we propose a cut-and-project perspective for the linearized Bregman method via a bilevel optimization formulation, along with a new unified algorithmic framework.","The new perspective not only encompasses various existing linearized Bregman iteration variants as specific instances, but also allows us to extend the linearized Bregman method to solve more general inverse problems.","We provide a completed convergence result of the proposed algorithmic framework, including convergence guarantees to feasible points and optimal solutions, and the sublinear convergence rate.","Moreover, we introduce the Bregman distance growth condition to ensure linear convergence.","At last, our findings are illustrated via numerical tests."],"url":"http://arxiv.org/abs/2404.09776v1","category":"math.OC"}
{"created":"2024-04-15 13:28:13","title":"RandAlign: A Parameter-Free Method for Regularizing Graph Convolutional Networks","abstract":"Studies continually find that message-passing graph convolutional networks suffer from the over-smoothing issue. Basically, the issue of over-smoothing refers to the phenomenon that the learned embeddings for all nodes can become very similar to one another and therefore are uninformative after repeatedly applying message passing iterations. Intuitively, we can expect the generated embeddings become smooth asymptotically layerwisely, that is each layer of graph convolution generates a smoothed version of embeddings as compared to that generated by the previous layer. Based on this intuition, we propose RandAlign, a stochastic regularization method for graph convolutional networks. The idea of RandAlign is to randomly align the learned embedding for each node with that of the previous layer using randomly interpolation in each graph convolution layer. Through alignment, the smoothness of the generated embeddings is explicitly reduced. To better maintain the benefit yielded by the graph convolution, in the alignment step we introduce to first scale the embedding of the previous layer to the same norm as the generated embedding and then perform random interpolation for aligning the generated embedding. RandAlign is a parameter-free method and can be directly applied without introducing additional trainable weights or hyper-parameters. We experimentally evaluate RandAlign on different graph domain tasks on seven benchmark datasets. The experimental results show that RandAlign is a general method that improves the generalization performance of various graph convolutional network models and also improves the numerical stability of optimization, advancing the state of the art performance for graph representation learning.","sentences":["Studies continually find that message-passing graph convolutional networks suffer from the over-smoothing issue.","Basically, the issue of over-smoothing refers to the phenomenon that the learned embeddings for all nodes can become very similar to one another and therefore are uninformative after repeatedly applying message passing iterations.","Intuitively, we can expect the generated embeddings become smooth asymptotically layerwisely, that is each layer of graph convolution generates a smoothed version of embeddings as compared to that generated by the previous layer.","Based on this intuition, we propose RandAlign, a stochastic regularization method for graph convolutional networks.","The idea of RandAlign is to randomly align the learned embedding for each node with that of the previous layer using randomly interpolation in each graph convolution layer.","Through alignment, the smoothness of the generated embeddings is explicitly reduced.","To better maintain the benefit yielded by the graph convolution, in the alignment step we introduce to first scale the embedding of the previous layer to the same norm as the generated embedding and then perform random interpolation for aligning the generated embedding.","RandAlign is a parameter-free method and can be directly applied without introducing additional trainable weights or hyper-parameters.","We experimentally evaluate RandAlign on different graph domain tasks on seven benchmark datasets.","The experimental results show that RandAlign is a general method that improves the generalization performance of various graph convolutional network models and also improves the numerical stability of optimization, advancing the state of the art performance for graph representation learning."],"url":"http://arxiv.org/abs/2404.09774v1","category":"cs.LG"}
{"created":"2024-04-15 13:15:27","title":"Search-Space Reduction Via Essential Vertices Revisited: Vertex Multicut and Cograph Deletion","abstract":"For an optimization problem $\\Pi$ on graphs whose solutions are vertex sets, a vertex $v$ is called $c$-essential for $\\Pi$ if all solutions of size at most $c \\cdot OPT$ contain $v$. Recent work showed that polynomial-time algorithms to detect $c$-essential vertices can be used to reduce the search space of fixed-parameter tractable algorithms solving such problems parameterized by the size $k$ of the solution. We provide several new upper- and lower bounds for detecting essential vertices. For example, we give a polynomial-time algorithm for $3$-Essential detection for Vertex Multicut, which translates into an algorithm that finds a minimum multicut of an undirected $n$-vertex graph $G$ in time $2^{O(\\ell^3)} \\cdot n^{O(1)}$, where $\\ell$ is the number of vertices in an optimal solution that are not $3$-essential. Our positive results are obtained by analyzing the integrality gaps of certain linear programs. Our lower bounds show that for sufficiently small values of $c$, the detection task becomes NP-hard assuming the Unique Games Conjecture. For example, we show that ($2-\\varepsilon$)-Essential detection for Directed Feedback Vertex Set is NP-hard under this conjecture, thereby proving that the existing algorithm that detects $2$-essential vertices is best-possible.","sentences":["For an optimization problem $\\Pi$ on graphs whose solutions are vertex sets, a vertex $v$ is called $c$-essential for $\\Pi$ if all solutions of size at most $c \\cdot OPT$ contain $v$.","Recent work showed that polynomial-time algorithms to detect $c$-essential vertices can be used to reduce the search space of fixed-parameter tractable algorithms solving such problems parameterized by the size $k$ of the solution.","We provide several new upper- and lower bounds for detecting essential vertices.","For example, we give a polynomial-time algorithm for $3$-Essential detection for Vertex Multicut, which translates into an algorithm that finds a minimum multicut of an undirected $n$-vertex graph $G$ in time $2^{O(\\ell^3)} \\cdot n^{O(1)}$, where $\\ell$ is the number of vertices in an optimal solution that are not $3$-essential.","Our positive results are obtained by analyzing the integrality gaps of certain linear programs.","Our lower bounds show that for sufficiently small values of $c$, the detection task becomes NP-hard assuming the Unique Games Conjecture.","For example, we show that ($2-\\varepsilon$)-Essential detection for Directed Feedback Vertex Set is NP-hard under this conjecture, thereby proving that the existing algorithm that detects $2$-essential vertices is best-possible."],"url":"http://arxiv.org/abs/2404.09769v1","category":"cs.DS"}
{"created":"2024-04-15 12:32:20","title":"Federated Learning on Riemannian Manifolds with Differential Privacy","abstract":"In recent years, federated learning (FL) has emerged as a prominent paradigm in distributed machine learning. Despite the partial safeguarding of agents' information within FL systems, a malicious adversary can potentially infer sensitive information through various means. In this paper, we propose a generic private FL framework defined on Riemannian manifolds (PriRFed) based on the differential privacy (DP) technique. We analyze the privacy guarantee while establishing the convergence properties. To the best of our knowledge, this is the first federated learning framework on Riemannian manifold with a privacy guarantee and convergence results. Numerical simulations are performed on synthetic and real-world datasets to showcase the efficacy of the proposed PriRFed approach.","sentences":["In recent years, federated learning (FL) has emerged as a prominent paradigm in distributed machine learning.","Despite the partial safeguarding of agents' information within FL systems, a malicious adversary can potentially infer sensitive information through various means.","In this paper, we propose a generic private FL framework defined on Riemannian manifolds (PriRFed) based on the differential privacy (DP) technique.","We analyze the privacy guarantee while establishing the convergence properties.","To the best of our knowledge, this is the first federated learning framework on Riemannian manifold with a privacy guarantee and convergence results.","Numerical simulations are performed on synthetic and real-world datasets to showcase the efficacy of the proposed PriRFed approach."],"url":"http://arxiv.org/abs/2404.10029v1","category":"math.OC"}
{"created":"2024-04-16 17:11:52","title":"Heuristic-free Verification-inspired Quantum Benchmarking","abstract":"In this paper, we introduce a new approach to quantum benchmarking inspired by quantum verification motivating new paradigms of quantum benchmarking. Our proposed benchmark not only serves as a robust indicator of computational capability but also offers scalability, customizability, and universality. By providing formal statements regarding the quality of quantum devices while assuming device consistency, we eliminate the reliance on heuristics. We establish a deep connection between quantum verification and quantum benchmarking. For practical application, we present a concrete benchmarking protocol derived from a quantum verification protocol, and prove it to match our redefined standards for quantum benchmarking.","sentences":["In this paper, we introduce a new approach to quantum benchmarking inspired by quantum verification motivating new paradigms of quantum benchmarking.","Our proposed benchmark not only serves as a robust indicator of computational capability but also offers scalability, customizability, and universality.","By providing formal statements regarding the quality of quantum devices while assuming device consistency, we eliminate the reliance on heuristics.","We establish a deep connection between quantum verification and quantum benchmarking.","For practical application, we present a concrete benchmarking protocol derived from a quantum verification protocol, and prove it to match our redefined standards for quantum benchmarking."],"url":"http://arxiv.org/abs/2404.10739v1","category":"quant-ph"}
{"created":"2024-04-16 16:36:34","title":"PEARLS: Discovery of Point-Source Features Within Galaxies in the North Ecliptic Pole Time Domain Field","abstract":"$ $The first public 0.9-4.4 $\\mu$m NIRCam images of the North Ecliptic Pole (NEP) Time Domain Field (TDF) uncovered many galaxies that display point-source features in their cores as seen in the longer wavelength filters. We visually identified a sample of 66 galaxies ($\\sim$1 galaxy per arcmin$^2$) with point-like cores and fit their spectral energy distributions (SED)s using EAZY and CIGALE to characterize the sample's active galactic nucleus (AGN) and host galaxy parameters. Single-template fitting best fits $70\\%$ of the sample with a Seyfert-blended SED. With CIGALE we compute the median fractional AGN contribution to the 0.1-30.0 $\\mu$m flux to be $0.30\\pm0.06$, and that $56\\%$ of the 66 galaxies have star-formation rates in the starburst range whereas the remainder are near the star-formation main sequence. There are Very Large Array (VLA) 3 GHz detections for 24/66 galaxies, implying some combination of AGN emission and vigorous star formation. We present a novel sample selection procedure in tandem to the morphological sample selection based on objects' light profiles at 4.4 $\\mu$m. This procedure identifies a parameter space which probes point-source features and automatically recovers our visual sample with minimal contamination from both stars and brighter galaxies. This procedure may be used in other extant and future NIRCam images to streamline the search for galaxies with point-like cores. The morphological approach to recognizing AGN is being resurrected by the James Webb Space Telescope (JWST) by virtue of its superb angular resolution at infrared wavelengths.","sentences":["$ $The first public 0.9-4.4 $\\mu$m NIRCam images of the North Ecliptic Pole (NEP) Time Domain Field (TDF) uncovered many galaxies that display point-source features in their cores as seen in the longer wavelength filters.","We visually identified a sample of 66 galaxies ($\\sim$1 galaxy per arcmin$^2$) with point-like cores and fit their spectral energy distributions (SED)s using EAZY and CIGALE to characterize the sample's active galactic nucleus (AGN) and host galaxy parameters.","Single-template fitting best fits $70\\%$ of the sample with a Seyfert-blended SED.","With CIGALE we compute the median fractional AGN contribution to the 0.1-30.0 $\\mu$m flux to be $0.30\\pm0.06$, and that $56\\%$ of the 66 galaxies have star-formation rates in the starburst range whereas the remainder are near the star-formation main sequence.","There are Very Large Array (VLA) 3 GHz detections for 24/66 galaxies, implying some combination of AGN emission and vigorous star formation.","We present a novel sample selection procedure in tandem to the morphological sample selection based on objects' light profiles at 4.4 $\\mu$m.","This procedure identifies a parameter space which probes point-source features and automatically recovers our visual sample with minimal contamination from both stars and brighter galaxies.","This procedure may be used in other extant and future NIRCam images to streamline the search for galaxies with point-like cores.","The morphological approach to recognizing AGN is being resurrected by the James Webb Space Telescope (JWST) by virtue of its superb angular resolution at infrared wavelengths."],"url":"http://arxiv.org/abs/2404.10709v1","category":"astro-ph.GA"}
{"created":"2024-04-16 16:31:49","title":"Schwinger-Dyson control variates for lattice fermions","abstract":"Previous work has shown that high-quality control variates for lattice Monte Carlo methods may be constructed from lattice Schwinger-Dyson relations. This paper extends that method to theories with lattice fermions, using the Thirring model in $1+1$ spacetime dimensions as a testbed. Past construction of these control variates involved a number of fitting parameters that scaled with lattice volume. By computing the control variate in perturbation theory, the number of fitting parameters required for an order-of-magnitude improvement in the signal-to-noise ratio at weak coupling is reduced to be of order one.","sentences":["Previous work has shown that high-quality control variates for lattice Monte Carlo methods may be constructed from lattice Schwinger-Dyson relations.","This paper extends that method to theories with lattice fermions, using the Thirring model in $1+1$ spacetime dimensions as a testbed.","Past construction of these control variates involved a number of fitting parameters that scaled with lattice volume.","By computing the control variate in perturbation theory, the number of fitting parameters required for an order-of-magnitude improvement in the signal-to-noise ratio at weak coupling is reduced to be of order one."],"url":"http://arxiv.org/abs/2404.10707v1","category":"hep-lat"}
{"created":"2024-04-16 16:29:40","title":"Observation of microquasars high-energy emission with INTEGRAL","abstract":"Microquasars are Black Hole X-ray binaries (BHXB) which can eject material in the form of a bipolar jet, similarly to quasars, but at much smaller scales. Their high-energy emission comes from an accretion disk (~ 1 keV) and from a hot \"corona\" near the black hole that up-scatters photons from the disk in the hard X-ray domain (1--100 keV). A high-energy component above 150 keV has been detected in bright sources and its precise origin is still unknown: it could come either from Compton scattering of disk photons on coronal relativistic non-thermal electrons (a.k.a hybrid Comptonization), or from the synchrotron emission from the very base of the compact jet. The measurement of polarization above 150 keV can provide valuable insights into the processes at play as we expect higher polarization fraction due to synchrotron emission from the jets (up to 70 % with a very ordered magnetic field). We use the INTEGRAL/IBIS telescope to measure the soft gamma-ray polarization of the Crab Nebula and the BHXB Swift J1727.8-1613.","sentences":["Microquasars are Black Hole X-ray binaries (BHXB) which can eject material in the form of a bipolar jet, similarly to quasars, but at much smaller scales.","Their high-energy emission comes from an accretion disk (~ 1 keV) and from a hot \"corona\" near the black hole that up-scatters photons from the disk in the hard X-ray domain (1--100 keV).","A high-energy component above 150 keV has been detected in bright sources and its precise origin is still unknown: it could come either from Compton scattering of disk photons on coronal relativistic non-thermal electrons (a.k.a hybrid Comptonization), or from the synchrotron emission from the very base of the compact jet.","The measurement of polarization above 150 keV can provide valuable insights into the processes at play as we expect higher polarization fraction due to synchrotron emission from the jets (up to 70 % with a very ordered magnetic field).","We use the INTEGRAL/IBIS telescope to measure the soft gamma-ray polarization of the Crab Nebula and the BHXB Swift J1727.8-1613."],"url":"http://arxiv.org/abs/2404.10705v1","category":"astro-ph.HE"}
{"created":"2024-04-16 16:20:02","title":"An empirical study on code review activity prediction in practice","abstract":"During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase. This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author's and the reviewer's experience, leading to median wait times for review feedback of 15-64 hours. Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23%), and participants' file-level hot-spot precision and recall increases to 53% (+13%) and 28% (+8%), respectively, compared to the alphanumeric ordering. Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised). To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features). Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach. For all tasks, F1-scores (median of 40-62%) are significantly better than the state-of-the-art (from +1 to +9%).","sentences":["During code reviews, an essential step in software quality assurance, reviewers have the difficult task of understanding and evaluating code changes to validate their quality and prevent introducing faults to the codebase.","This is a tedious process where the effort needed is highly dependent on the code submitted, as well as the author's and the reviewer's experience, leading to median wait times for review feedback of 15-64 hours.","Through an initial user study carried with 29 experts, we found that re-ordering the files changed by a patch within the review environment has potential to improve review quality, as more comments are written (+23%), and participants' file-level hot-spot precision and recall increases to 53% (+13%) and 28% (+8%), respectively, compared to the alphanumeric ordering.","Hence, this paper aims to help code reviewers by predicting which files in a submitted patch need to be (1) commented, (2) revised, or (3) are hot-spots (commented or revised).","To predict these tasks, we evaluate two different types of text embeddings (i.e., Bag-of-Words and Large Language Models encoding) and review process features (i.e., code size-based and history-based features).","Our empirical study on three open-source and two industrial datasets shows that combining the code embedding and review process features leads to better results than the state-of-the-art approach.","For all tasks, F1-scores (median of 40-62%) are significantly better than the state-of-the-art (from +1 to +9%)."],"url":"http://arxiv.org/abs/2404.10703v1","category":"cs.SE"}
{"created":"2024-04-16 16:11:34","title":"Scaling Law Breaking in Unequal-size Droplet Coalescence","abstract":"This Letter examines the coalescence of two unequal-size spherical liquid droplets in the inviscid regime. We find that the liquid bridge evolution exhibits a breaking from the classical 1/2 power-law scaling [Phys. Rev. Lett. 95, 164503 (2005)]. Employing an energy balance analysis, we attain a theoretical model to collapse bridge evolution data of different droplet size ratios. This model reveals an exponential dependence of the bridge's radial growth on time, which is intrinsically scaling-free owing to the asymmetric movement of the liquid bridge.","sentences":["This Letter examines the coalescence of two unequal-size spherical liquid droplets in the inviscid regime.","We find that the liquid bridge evolution exhibits a breaking from the classical 1/2 power-law scaling [Phys.","Rev. Lett.","95, 164503 (2005)].","Employing an energy balance analysis, we attain a theoretical model to collapse bridge evolution data of different droplet size ratios.","This model reveals an exponential dependence of the bridge's radial growth on time, which is intrinsically scaling-free owing to the asymmetric movement of the liquid bridge."],"url":"http://arxiv.org/abs/2404.10691v1","category":"physics.flu-dyn"}
{"created":"2024-04-16 15:37:09","title":"Discovery of the optical and radio counterpart to the fast X-ray transient EP240315a","abstract":"Fast X-ray Transients (FXTs) are extragalactic bursts of soft X-rays first identified >10 years ago. Since then, nearly 40 events have been discovered, although almost all of these have been recovered from archival Chandra and XMM-Newton data. To date, optical sky surveys and follow-up searches have not revealed any multi-wavelength counterparts. The Einstein Probe, launched in January 2024, has started surveying the sky in the soft X-ray regime (0.5-4 keV) and will rapidly increase the sample of FXTs discovered in real time. Here, we report the first discovery of both an optical and radio counterpart to an FXT, the fourth source publicly released by the Einstein Probe. We discovered a fast-fading optical transient within the 3 arcmin localisation radius of EP240315a with the all-sky optical survey ATLAS, and our follow-up Gemini spectrum provides a redshift, z=4.859+/-0.002. Furthermore, we uncovered a radio counterpart in the S-band (3.0 GHz) with the MeerKAT radio interferometer. The optical (rest-frame UV) and radio luminosities indicate the FXT most likely originates from either a long gamma-ray burst or a relativistic tidal disruption event. This may be a fortuitous early mission detection by the Einstein Probe or may signpost a mode of discovery for high-redshift, high-energy transients through soft X-ray surveys, combined with locating multi-wavelength counterparts.","sentences":["Fast X-ray Transients (FXTs) are extragalactic bursts of soft X-rays first identified >10 years ago.","Since then, nearly 40 events have been discovered, although almost all of these have been recovered from archival Chandra and XMM-Newton data.","To date, optical sky surveys and follow-up searches have not revealed any multi-wavelength counterparts.","The Einstein Probe, launched in January 2024, has started surveying the sky in the soft X-ray regime (0.5-4 keV) and will rapidly increase the sample of FXTs discovered in real time.","Here, we report the first discovery of both an optical and radio counterpart to an FXT, the fourth source publicly released by the Einstein Probe.","We discovered a fast-fading optical transient within the 3 arcmin localisation radius of EP240315a with the all-sky optical survey ATLAS, and our follow-up Gemini spectrum provides a redshift, z=4.859+/-0.002.","Furthermore, we uncovered a radio counterpart in the S-band (3.0 GHz) with the MeerKAT radio interferometer.","The optical (rest-frame UV) and radio luminosities indicate the FXT most likely originates from either a long gamma-ray burst or a relativistic tidal disruption event.","This may be a fortuitous early mission detection by the Einstein Probe or may signpost a mode of discovery for high-redshift, high-energy transients through soft X-ray surveys, combined with locating multi-wavelength counterparts."],"url":"http://arxiv.org/abs/2404.10660v1","category":"astro-ph.HE"}
{"created":"2024-04-16 14:46:02","title":"Azimuthal spin asymmetries in pion-polarized proton induced Drell-Yan process at COMPASS using holographic light-front QCD","abstract":"We compute all the leading-twist azimuthal spin asymmetries in the pion-proton induced Drell-Yan process. These spin asymmetries arise from convolutions of the leading-twist transverse-momentum-dependent parton distributions (TMDs) of both the incoming pion and the target proton. We employ the holographic light-front pion wave functions for the pion TMDs, while for the proton TMDs, we utilize a light-front quark-diquark model constructed by the soft-wall AdS/QCD. The gluon rescattering is crucial to predict nonzero time-reversal odd TMDs. We study the utility of a nonperturbative SU$(3)$ gluon rescattering kernel, which extends beyond the typical assumption of perturbative U$(1)$ gluons. Subsequently, we employ Collins-Soper scale evolution at Next-to-Leading Logarithmic precision for the TMDs evolution. Our predictions for the spin asymmetries are consistent with the available experimental data from COMPASS and other phenomenological studies.","sentences":["We compute all the leading-twist azimuthal spin asymmetries in the pion-proton induced Drell-Yan process.","These spin asymmetries arise from convolutions of the leading-twist transverse-momentum-dependent parton distributions (TMDs) of both the incoming pion and the target proton.","We employ the holographic light-front pion wave functions for the pion TMDs, while for the proton TMDs, we utilize a light-front quark-diquark model constructed by the soft-wall AdS/QCD.","The gluon rescattering is crucial to predict nonzero time-reversal odd TMDs.","We study the utility of a nonperturbative SU$(3)$ gluon rescattering kernel, which extends beyond the typical assumption of perturbative U$(1)$ gluons.","Subsequently, we employ Collins-Soper scale evolution at Next-to-Leading Logarithmic precision for the TMDs evolution.","Our predictions for the spin asymmetries are consistent with the available experimental data from COMPASS and other phenomenological studies."],"url":"http://arxiv.org/abs/2404.10623v1","category":"hep-ph"}
{"created":"2024-04-16 14:36:44","title":"Variations of supersymmetric quantum mechanics and superconformal indices","abstract":"Old studies on supersymmetric quantum mechanics and its deformations, that were initiated by the 1988 joint paper with V. Rubakov, are retrospectively discussed. In the modern circumstances, corresponding results can be related to computations of superconformal indices and associated special functions.","sentences":["Old studies on supersymmetric quantum mechanics and its deformations, that were initiated by the 1988 joint paper with V. Rubakov, are retrospectively discussed.","In the modern circumstances, corresponding results can be related to computations of superconformal indices and associated special functions."],"url":"http://arxiv.org/abs/2404.10609v1","category":"hep-th"}
{"created":"2024-04-16 14:27:02","title":"Anomaly of Subsystem Symmetries in Exotic and Foliated $BF$ Theories","abstract":"We study the mixed 't Hooft anomaly of the subsystem symmetries in the exotic $BF$ theory and the foliated $BF$ theory in 2+1 dimensions, both of which are fractonic quantum field theories describing the equivalent physics. In the anomaly inflow mechanism, the 't Hooft anomaly of the subsystem symmetries can be cancelled by combining a subsystem symmetry-protected topological (SSPT) phase in one dimension higher. In this work, we construct the exotic and foliated $BF$ theories with background gauge fields and the exotic and foliated forms of the SSPT phases using the foliated-exotic duality. In the foliated form, we see that the non-topological operator can be viewed as a symmetry-like operator. We also show that the SSPT phases with different foliation structures cancel the same anomaly. This may provide a clue to the characterization of the 't Hooft anomaly of subsystem symmetries.","sentences":["We study the mixed 't Hooft anomaly of the subsystem symmetries in the exotic $BF$ theory and the foliated $BF$ theory in 2+1 dimensions, both of which are fractonic quantum field theories describing the equivalent physics.","In the anomaly inflow mechanism, the 't Hooft anomaly of the subsystem symmetries can be cancelled by combining a subsystem symmetry-protected topological (SSPT) phase in one dimension higher.","In this work, we construct the exotic and foliated $BF$ theories with background gauge fields and the exotic and foliated forms of the SSPT phases using the foliated-exotic duality.","In the foliated form, we see that the non-topological operator can be viewed as a symmetry-like operator.","We also show that the SSPT phases with different foliation structures cancel the same anomaly.","This may provide a clue to the characterization of the 't Hooft anomaly of subsystem symmetries."],"url":"http://arxiv.org/abs/2404.10601v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 13:47:21","title":"CMU-Flownet: Exploring Point Cloud Scene Flow Estimation in Occluded Scenario","abstract":"Occlusions hinder point cloud frame alignment in LiDAR data, a challenge inadequately addressed by scene flow models tested mainly on occlusion-free datasets. Attempts to integrate occlusion handling within networks often suffer accuracy issues due to two main limitations: a) the inadequate use of occlusion information, often merging it with flow estimation without an effective integration strategy, and b) reliance on distance-weighted upsampling that falls short in correcting occlusion-related errors. To address these challenges, we introduce the Correlation Matrix Upsampling Flownet (CMU-Flownet), incorporating an occlusion estimation module within its cost volume layer, alongside an Occlusion-aware Cost Volume (OCV) mechanism. Specifically, we propose an enhanced upsampling approach that expands the sensory field of the sampling process which integrates a Correlation Matrix designed to evaluate point-level similarity. Meanwhile, our model robustly integrates occlusion data within the context of scene flow, deploying this information strategically during the refinement phase of the flow estimation. The efficacy of this approach is demonstrated through subsequent experimental validation. Empirical assessments reveal that CMU-Flownet establishes state-of-the-art performance within the realms of occluded Flyingthings3D and KITTY datasets, surpassing previous methodologies across a majority of evaluated metrics.","sentences":["Occlusions hinder point cloud frame alignment in LiDAR data, a challenge inadequately addressed by scene flow models tested mainly on occlusion-free datasets.","Attempts to integrate occlusion handling within networks often suffer accuracy issues due to two main limitations: a) the inadequate use of occlusion information, often merging it with flow estimation without an effective integration strategy, and b) reliance on distance-weighted upsampling that falls short in correcting occlusion-related errors.","To address these challenges, we introduce the Correlation Matrix Upsampling Flownet (CMU-Flownet), incorporating an occlusion estimation module within its cost volume layer, alongside an Occlusion-aware Cost Volume (OCV) mechanism.","Specifically, we propose an enhanced upsampling approach that expands the sensory field of the sampling process which integrates a Correlation Matrix designed to evaluate point-level similarity.","Meanwhile, our model robustly integrates occlusion data within the context of scene flow, deploying this information strategically during the refinement phase of the flow estimation.","The efficacy of this approach is demonstrated through subsequent experimental validation.","Empirical assessments reveal that CMU-Flownet establishes state-of-the-art performance within the realms of occluded Flyingthings3D and KITTY datasets, surpassing previous methodologies across a majority of evaluated metrics."],"url":"http://arxiv.org/abs/2404.10571v1","category":"cs.CV"}
{"created":"2024-04-16 13:39:51","title":"Electronic Properties of Electroactive Ferrocenyl-Functionalized MoS2","abstract":"The attachment of redox active molecules to transition metal dichalcogenides (TMDs), such as MoS2, constitutes a promising approach for designing electrochemically switchable devices through the control of the material charge/spin transport properties by the redox state of the grafted molecule and thus the applied electrical potential. In this work, defective plasma treated MoS2 is functionalized by a ferrocene derivative and thoroughly investigated by various characterization techniques, such as Raman, photoluminescence, X-ray photoelectron spectroscopies, atomic force microscopy (AFM) and electrochemistry. Furthermore, in-plane and out-of-plane conductive-AFM measurements (I-V and first derivative dI/dV-V curves) are measured to investigate the effect of the chemical functionalization of MoS2 on the electron transport properties. While the conduction and valence bands are determined at +0.7 and -1.2 eV with respect of the electrode Fermi energy for pristineMoS2, additional states in an energy range of ca. 0.45 eV below the MoS2 conduction band are measured after plasma treatment, attributed to S-vacancies. For ferrocene functionalized MoS2, the S-vacancy states are no longer observed resulting from the defect healing. However, two bumps at lower voltages in the dI/dV-V indicate a contribution to the electron transport through ferrocene HOMO, which is located in the MoS2 band gap at ca. 0.4-0.6 eV below the Fermi energy. These results are in good agreement with theoretical density functional theory (DFT) calculations and UV photoelectron spectroscopy (UPS) measurements.","sentences":["The attachment of redox active molecules to transition metal dichalcogenides (TMDs), such as MoS2, constitutes a promising approach for designing electrochemically switchable devices through the control of the material charge/spin transport properties by the redox state of the grafted molecule and thus the applied electrical potential.","In this work, defective plasma treated MoS2 is functionalized by a ferrocene derivative and thoroughly investigated by various characterization techniques, such as Raman, photoluminescence, X-ray photoelectron spectroscopies, atomic force microscopy (AFM) and electrochemistry.","Furthermore, in-plane and out-of-plane conductive-AFM measurements (I-V and first derivative dI/dV-V curves) are measured to investigate the effect of the chemical functionalization of MoS2 on the electron transport properties.","While the conduction and valence bands are determined at +0.7 and -1.2 eV with respect of the electrode Fermi energy for pristineMoS2, additional states in an energy range of ca. 0.45 eV below the MoS2 conduction band are measured after plasma treatment, attributed to S-vacancies.","For ferrocene functionalized MoS2, the S-vacancy states are no longer observed resulting from the defect healing.","However, two bumps at lower voltages in the dI/dV-V indicate a contribution to the electron transport through ferrocene HOMO, which is located in the MoS2 band gap at ca. 0.4-0.6 eV below the Fermi energy.","These results are in good agreement with theoretical density functional theory (DFT) calculations and UV photoelectron spectroscopy (UPS) measurements."],"url":"http://arxiv.org/abs/2404.10565v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 13:10:41","title":"Statistical analysis of pulsar flux density distribution","abstract":"This study presents a comprehensive analysis of the spectral properties of 886 pulsars across a wide frequency range from 20MHz to 343.5GHz, including a total of 86 millisecond pulsars. The majority of the pulsars exhibit power-law behavior in their spectra, although some exceptions are observed. Five different spectral models, namely simple power-law, broken power-law, low-frequency turn-over, high-frequency cut-off, and double turn-over, were employed to explore the spectral behaviors. The average spectral index for pulsars modeled with a simple power-law is found to be -1.64 +/-0.80, consistent with previous studies. Additionally, significant correlations between the spectral index and characteristic parameters are observed particularly in millisecond pulsars, while no strong correlation is observed in normal pulsars. Different models show variations in the most influential characteristic parameters associated with the spectral index, indicating diverse dominant radiation mechanisms in millisecond pulsars.Finally, this study identifies 22 pulsars of the Gigahertz-peaked Spectra (GPS) type for the first time based on the Akaike information criterion.","sentences":["This study presents a comprehensive analysis of the spectral properties of 886 pulsars across a wide frequency range from 20MHz to 343.5GHz, including a total of 86 millisecond pulsars.","The majority of the pulsars exhibit power-law behavior in their spectra, although some exceptions are observed.","Five different spectral models, namely simple power-law, broken power-law, low-frequency turn-over, high-frequency cut-off, and double turn-over, were employed to explore the spectral behaviors.","The average spectral index for pulsars modeled with a simple power-law is found to be -1.64 +/-0.80, consistent with previous studies.","Additionally, significant correlations between the spectral index and characteristic parameters are observed particularly in millisecond pulsars, while no strong correlation is observed in normal pulsars.","Different models show variations in the most influential characteristic parameters associated with the spectral index, indicating diverse dominant radiation mechanisms in millisecond pulsars.","Finally, this study identifies 22 pulsars of the Gigahertz-peaked Spectra (GPS) type for the first time based on the Akaike information criterion."],"url":"http://arxiv.org/abs/2404.10542v2","category":"astro-ph.HE"}
{"created":"2024-04-16 12:56:19","title":"Universal Displacements in Anisotropic Linear Cauchy Elasticity","abstract":"Universal displacements are those displacements that can be maintained for any member of a specific class of linear elastic materials in the absence of body forces, solely by applying boundary tractions. For linear hyperelastic (Green elastic) solids, it is known that the space of universal displacements explicitly depends on the symmetry group of the material, and moreover, the larger the symmetry group the larger the set of universal displacements. Linear Cauchy elastic solids, which include linear hyperelastic solids as a special case, do not necessarily have an underlying energy function. Consequently, their elastic constants do not possess the major symmetries. In this paper, we characterize the universal displacements of anisotropic linear Cauchy elasticity. We prove the unexpected result that for each symmetry class, the set of universal displacements of linear Cauchy elasticity is identical to that of linear hyperelasticity.","sentences":["Universal displacements are those displacements that can be maintained for any member of a specific class of linear elastic materials in the absence of body forces, solely by applying boundary tractions.","For linear hyperelastic (Green elastic) solids, it is known that the space of universal displacements explicitly depends on the symmetry group of the material, and moreover, the larger the symmetry group the larger the set of universal displacements.","Linear Cauchy elastic solids, which include linear hyperelastic solids as a special case, do not necessarily have an underlying energy function.","Consequently, their elastic constants do not possess the major symmetries.","In this paper, we characterize the universal displacements of anisotropic linear Cauchy elasticity.","We prove the unexpected result that for each symmetry class, the set of universal displacements of linear Cauchy elasticity is identical to that of linear hyperelasticity."],"url":"http://arxiv.org/abs/2404.10529v1","category":"cond-mat.soft"}
{"created":"2024-04-16 12:37:52","title":"Simple $k$-crashing Plan with a Good Approximation Ratio","abstract":"In project management, a project is typically described as an activity-on-edge network (AOE network), where each activity / job is represented as an edge of some network $N$ (which is a DAG). Some jobs must be finished before others can be started, as described by the topology structure of $N$. It is known that job $j_i$ in normal speed would require $b_i$ days to be finished after it is started. Given the network $N$ with the associated edge lengths $b_1,\\ldots,b_m$, the duration of the project is determined, which equals the length of the critical path (namely, the longest path) of $N$.   To speed up the project (i.e. reduce the duration), the manager can crash a few jobs (namely, reduce the length of the corresponding edges) by investing extra resources into that job. However, the time for completing $j_i$ has a lower bound due to technological limits -- it requires at least $a_i$ days to be completed. Moreover, it is expensive to buy resources. Given $N$ and an integer $k\\geq 1$, the $k$-crashing problem asks the minimum amount of resources required to speed up the project by $k$ days. We show a simple and efficient algorithm with an approximation ratio $\\frac{1}{1}+\\ldots+\\frac{1}{k}$ for this problem.   We also study a related problem called $k$-LIS, in which we are given a sequence $\\omega$ of numbers and we aim to find $k$ disjoint increasing subsequence of $\\omega$ with the largest total length. We show a $(1-\\frac{1}{e})$-approximation algorithm which is simple and efficient.","sentences":["In project management, a project is typically described as an activity-on-edge network (AOE network), where each activity / job is represented as an edge of some network $N$ (which is a DAG).","Some jobs must be finished before others can be started, as described by the topology structure of $N$. It is known that job $j_i$ in normal speed would require $b_i$ days to be finished after it is started.","Given the network $N$ with the associated edge lengths $b_1,\\ldots,b_m$, the duration of the project is determined, which equals the length of the critical path (namely, the longest path) of $N$.   To speed up the project (i.e. reduce the duration), the manager can crash a few jobs (namely, reduce the length of the corresponding edges) by investing extra resources into that job.","However, the time for completing $j_i$ has a lower bound due to technological limits -- it requires at least $a_i$ days to be completed.","Moreover, it is expensive to buy resources.","Given $N$ and an integer $k\\geq 1$, the $k$-crashing problem asks the minimum amount of resources required to speed up the project by $k$ days.","We show a simple and efficient algorithm with an approximation ratio $\\frac{1}{1}+\\ldots+\\frac{1}{k}$ for this problem.   ","We also study a related problem called $k$-LIS, in which we are given a sequence $\\omega$ of numbers and we aim to find $k$ disjoint increasing subsequence of $\\omega$ with the largest total length.","We show a $(1-\\frac{1}{e})$-approximation algorithm which is simple and efficient."],"url":"http://arxiv.org/abs/2404.10514v1","category":"cs.DS"}
{"created":"2024-04-16 11:59:13","title":"Efficient structural relaxation based on the random phase approximation: Applications to the water clusters","abstract":"We report an improved implementation for evaluating the analytical gradients of the random phase approximation (RPA) electron-correlation energy based on atomic orbitals and the localized resolution of identity scheme. The more efficient RPA force calculations allow us to relax structures of medium-size water clusters. Particular attention is paid to the structures and energy orderings of the low-energy isomers of (H$_2$O)$_n$ clusters with $n=21$, 22, and 25. It is found that the energy ordering of the low-energy isomers of these water clusters are rather sensitive to how their structures are determined. For the five low-energy isomers of (H$_2$O)$_{25}$, the RPA energy ordering based on the RPA geometries is quite different from that based on the geometries relaxed by lower-level theories, in contrast with the situation of small water clusters like the water hexamer. The standard RPA underbinds the water clusters, and this underbinding behavior gets more pronounced as the complete basis set (CBS) limit is approached. The renormalized single excitation (rSE) correction remedies this underbinding, giving rise to a noticeable overbinding behavior at finite basis sets. However, as the CBS limit is approached, RPA+rSE yields an accuracy for the binding energies that is comparable to the best available double hybrid functionals, as demonstrated for the WATER27 testset.","sentences":["We report an improved implementation for evaluating the analytical gradients of the random phase approximation (RPA) electron-correlation energy based on atomic orbitals and the localized resolution of identity scheme.","The more efficient RPA force calculations allow us to relax structures of medium-size water clusters.","Particular attention is paid to the structures and energy orderings of the low-energy isomers of (H$_2$O)$_n$ clusters with $n=21$, 22, and 25.","It is found that the energy ordering of the low-energy isomers of these water clusters are rather sensitive to how their structures are determined.","For the five low-energy isomers of (H$_2$O)$_{25}$, the RPA energy ordering based on the RPA geometries is quite different from that based on the geometries relaxed by lower-level theories, in contrast with the situation of small water clusters like the water hexamer.","The standard RPA underbinds the water clusters, and this underbinding behavior gets more pronounced as the complete basis set (CBS) limit is approached.","The renormalized single excitation (rSE) correction remedies this underbinding, giving rise to a noticeable overbinding behavior at finite basis sets.","However, as the CBS limit is approached, RPA+rSE yields an accuracy for the binding energies that is comparable to the best available double hybrid functionals, as demonstrated for the WATER27 testset."],"url":"http://arxiv.org/abs/2404.10492v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 11:41:49","title":"Anisotropic polarizability of Dy at 532 nm on the intercombination transition","abstract":"We report experimental measurements of the dynamical polarizability of dysprosium, at a wavelength of 532 nm. We measure all three components (scalar, vector, tensor) of the anisotropic polarizability for the ground and the excited manifolds of the intercombination transition of Dy at 626 nm. The apparatus on which the measurements are performed is first presented. We obtain with this setup imaging of single Dy atoms with fidelity above 99 % and losses below 2.5 % induced by imaging. We then describe the methods used to extract the polarizability. In particular, we combine a measurement of trap frequency and trap depth on single atoms in optical tweezers, allowing us to obtain a measurement of the ground state polarizability free of errors in trap waist calibration. The obtained values give a magic condition between two Zeeman states in the ground and excited manifolds, which was used to image single atoms in optical tweezer arrays. The scalar polarizability of the ground state is in disagreement with theoretical expectations, calling for future investigations to resolve the discrepancy.","sentences":["We report experimental measurements of the dynamical polarizability of dysprosium, at a wavelength of 532 nm.","We measure all three components (scalar, vector, tensor) of the anisotropic polarizability for the ground and the excited manifolds of the intercombination transition of Dy at 626 nm.","The apparatus on which the measurements are performed is first presented.","We obtain with this setup imaging of single Dy atoms with fidelity above 99 % and losses below 2.5 % induced by imaging.","We then describe the methods used to extract the polarizability.","In particular, we combine a measurement of trap frequency and trap depth on single atoms in optical tweezers, allowing us to obtain a measurement of the ground state polarizability free of errors in trap waist calibration.","The obtained values give a magic condition between two Zeeman states in the ground and excited manifolds, which was used to image single atoms in optical tweezer arrays.","The scalar polarizability of the ground state is in disagreement with theoretical expectations, calling for future investigations to resolve the discrepancy."],"url":"http://arxiv.org/abs/2404.10480v1","category":"physics.atom-ph"}
{"created":"2024-04-16 11:10:11","title":"Community detection and anomaly prediction in dynamic networks","abstract":"Anomaly detection is an essential task in the analysis of dynamic networks, as it can provide early warning of potential threats or abnormal behavior. We present a principled approach to detect anomalies in dynamic networks that integrates community structure as a foundational model for regular behavior. Our model identifies anomalies as irregular edges while capturing structural changes. Leveraging a Markovian approach for temporal transitions and incorporating structural information via latent variables for communities and anomaly detection, our model infers these hidden parameters to pinpoint abnormal interactions within the network. Our approach is evaluated on both synthetic and real-world datasets. Real-world network analysis shows strong anomaly detection across diverse scenarios. In a more specific study of transfers of professional male football players, we observe various types of unexpected patterns and investigate how the country and wealth of clubs influence interactions. Additionally, we identify anomalies between clubs with incompatible community memberships, but also instances of anomalous transactions between clubs with similar memberships. The latter is due in particular to the dynamic nature of the transactions, as we find that the frequency of transfers results in anomalous behaviors that are otherwise expected to interact as they belong to similar communities.","sentences":["Anomaly detection is an essential task in the analysis of dynamic networks, as it can provide early warning of potential threats or abnormal behavior.","We present a principled approach to detect anomalies in dynamic networks that integrates community structure as a foundational model for regular behavior.","Our model identifies anomalies as irregular edges while capturing structural changes.","Leveraging a Markovian approach for temporal transitions and incorporating structural information via latent variables for communities and anomaly detection, our model infers these hidden parameters to pinpoint abnormal interactions within the network.","Our approach is evaluated on both synthetic and real-world datasets.","Real-world network analysis shows strong anomaly detection across diverse scenarios.","In a more specific study of transfers of professional male football players, we observe various types of unexpected patterns and investigate how the country and wealth of clubs influence interactions.","Additionally, we identify anomalies between clubs with incompatible community memberships, but also instances of anomalous transactions between clubs with similar memberships.","The latter is due in particular to the dynamic nature of the transactions, as we find that the frequency of transfers results in anomalous behaviors that are otherwise expected to interact as they belong to similar communities."],"url":"http://arxiv.org/abs/2404.10468v1","category":"cs.SI"}
{"created":"2024-04-16 10:42:25","title":"Polycyclic codes over serial rings and their annihilator CSS construction","abstract":"In this paper, we investigate the algebraic structure for polycyclic codes over a specific class of serial rings, defined as $\\mathscr R=R[x_1,\\ldots, x_s]/\\langle t_1(x_1),\\ldots, t_s(x_s) \\rangle$, where $R$ is a chain ring and each $t_i(x_i)$ in $R[x_i]$ for $i\\in\\{1,\\ldots, s\\}$ is a monic square-free polynomial. We define quasi-$s$-dimensional polycyclic codes and establish an $R$-isomorphism between these codes and polycyclic codes over $\\mathscr R$. We provide necessary and sufficient conditions for the existence of annihilator self-dual, annihilator self-orthogonal, annihilator linear complementary dual, and annihilator dual-containing polycyclic codes over this class of rings. We also establish the CSS construction for annihilator dual-preserving polycyclic codes over the chain ring $R$ and use this construction to derive quantum codes from polycyclic codes over $\\mathscr{R}$.","sentences":["In this paper, we investigate the algebraic structure for polycyclic codes over a specific class of serial rings, defined as $\\mathscr R=R[x_1,\\ldots, x_s]/\\langle t_1(x_1),\\ldots, t_s(x_s) \\rangle$, where $R$ is a chain ring and each $t_i(x_i)$ in $R[x_i]$ for $i\\in\\{1,\\ldots, s\\}$ is a monic square-free polynomial.","We define quasi-$s$-dimensional polycyclic codes and establish an $R$-isomorphism between these codes and polycyclic codes over $\\mathscr R$.","We provide necessary and sufficient conditions for the existence of annihilator self-dual, annihilator self-orthogonal, annihilator linear complementary dual, and annihilator dual-containing polycyclic codes over this class of rings.","We also establish the CSS construction for annihilator dual-preserving polycyclic codes over the chain ring $R$ and use this construction to derive quantum codes from polycyclic codes over $\\mathscr{R}$."],"url":"http://arxiv.org/abs/2404.10452v1","category":"cs.IT"}
{"created":"2024-04-16 09:55:48","title":"The evolutionary pathways of disk galaxies with different sizes","abstract":"From the IllustrisTNG-50 simulation, a sample of 836 central disk galaxies with tiny stellar halos is chosen to study the inherent evolution of galaxies driven by nature. These galaxies are classified as compact, normal, or extended by referencing their locations on the mass-size ($M_\\star-R_{\\rm 1/2}$) diagram. This research demonstrates the distinctive evolutionary pathways of galaxies with different sizes in IllustrisTNG simulations, primarily driven by nature. It is confirmed that disk galaxies inherit the angular momentum of their parent dark matter halos. More compact galaxies form earlier within halos possessing lower specific angular momentum through heightened star formation during the early phase at redshifts above 2. During the later phase, the size of extended galaxies experiences more pronounced growth by accreting gas with high angular momentum. Additionally, we reveal that many key characteristics of galaxies are linked to their mass and size: (1) compact galaxies tend to exhibit higher metal content, proportional to the potential well $\\frac{M_\\star}{R_{\\rm 1/2}}$, (2) compact galaxies host more massive bulges and black holes, and higher central concentration. Furthermore, our analysis indicates that galaxies of all types continue to actively engage in star formation, with no evident signs of quenching attributed to their varying sizes and angular momenta.","sentences":["From the IllustrisTNG-50 simulation, a sample of 836 central disk galaxies with tiny stellar halos is chosen to study the inherent evolution of galaxies driven by nature.","These galaxies are classified as compact, normal, or extended by referencing their locations on the mass-size ($M_\\star-R_{\\rm 1/2}$) diagram.","This research demonstrates the distinctive evolutionary pathways of galaxies with different sizes in IllustrisTNG simulations, primarily driven by nature.","It is confirmed that disk galaxies inherit the angular momentum of their parent dark matter halos.","More compact galaxies form earlier within halos possessing lower specific angular momentum through heightened star formation during the early phase at redshifts above 2.","During the later phase, the size of extended galaxies experiences more pronounced growth by accreting gas with high angular momentum.","Additionally, we reveal that many key characteristics of galaxies are linked to their mass and size: (1) compact galaxies tend to exhibit higher metal content, proportional to the potential well $\\frac{M_\\star}{R_{\\rm 1/2}}$, (2) compact galaxies host more massive bulges and black holes, and higher central concentration.","Furthermore, our analysis indicates that galaxies of all types continue to actively engage in star formation, with no evident signs of quenching attributed to their varying sizes and angular momenta."],"url":"http://arxiv.org/abs/2404.10432v1","category":"astro-ph.GA"}
{"created":"2024-04-16 09:30:12","title":"Classification of the Mott gap","abstract":"In this paper, we demonstrate the classification of the gap in a holographic setup by studying the density of states. A gap can be classified into order gap and Mott gap depending on the presence of the order due to the symmetry breaking or not. A Mott insulating gap appears in the fermion spectrum due to the strong Coulomb interaction between the electrons. We then classify all Mott gaps as well as order gaps in one-flavor and two-flavor fermions. We also identified possible non-minimal interactions that may produce a flatband.","sentences":["In this paper, we demonstrate the classification of the gap in a holographic setup by studying the density of states.","A gap can be classified into order gap and Mott gap depending on the presence of the order due to the symmetry breaking or not.","A Mott insulating gap appears in the fermion spectrum due to the strong Coulomb interaction between the electrons.","We then classify all Mott gaps as well as order gaps in one-flavor and two-flavor fermions.","We also identified possible non-minimal interactions that may produce a flatband."],"url":"http://arxiv.org/abs/2404.10412v1","category":"hep-th"}
{"created":"2024-04-16 09:08:33","title":"Spontaneous splitting of d-wave surface states: Circulating currents or edge magnetization?","abstract":"Pair-breaking edges of $d$-wave superconductors feature Andreev bound states at the Fermi energy. Since these states are energetically highly unfavorable they are susceptible to effects that shift them to finite energy. We investigate the free energy of two different mechanisms: spontaneous phase gradients in the superconducting order parameter and surface ferromagnetism caused by Fermi liquid interaction effects. We find that the surface magnetization appears at lower temperatures than the spontaneous current flow of the phase-crystal state. The magnetic state can, however, be energetically favorable at lower temperatures for sufficiently strong Fermi liquid effects. As a result, first-order transitions between the two states are possible, suggesting a rich low-temperature phase diagram in $d$-wave superconductors.","sentences":["Pair-breaking edges of $d$-wave superconductors feature Andreev bound states at the Fermi energy.","Since these states are energetically highly unfavorable they are susceptible to effects that shift them to finite energy.","We investigate the free energy of two different mechanisms: spontaneous phase gradients in the superconducting order parameter and surface ferromagnetism caused by Fermi liquid interaction effects.","We find that the surface magnetization appears at lower temperatures than the spontaneous current flow of the phase-crystal state.","The magnetic state can, however, be energetically favorable at lower temperatures for sufficiently strong Fermi liquid effects.","As a result, first-order transitions between the two states are possible, suggesting a rich low-temperature phase diagram in $d$-wave superconductors."],"url":"http://arxiv.org/abs/2404.10402v1","category":"cond-mat.supr-con"}
{"created":"2024-04-16 08:29:30","title":"Optical signatures of strain-induced ferromagnetism in LaCoO$_3$ thin film","abstract":"Using spectroscopic ellipsometry, we studied the optical conductivity of LaCoO$_3$ with various degrees of strain. The optical response of the compressively strained \\lco\\ film is qualitatively similar to the one of the unstrained LaCoO$_3$ polycrystalline sample and exhibits redistribution of the spectral weight between about 0.2 and 6 eV, which is most likely related to the thermal excitation of the high-spin states. The optical response of the ferromagnetic tensile strained film exhibits clear signatures due to the ferromagnetic state. Below the Curie temperature $T_c=82$ K, the spectral weight is transferred with the increasing temperature from low energies between 0.2 and 3.3 eV to energies between 3.3 and 5.6 eV. The temperature dependence of the low-energy spectral weight between 0.2 and 3.3 eV can be understood in the framework of the high-spin biexciton model of Sotnikov and Kune\\v{s} as corresponding to the variation of the concentration of high-spin states that are stabilized below $T_c$. The magnitude of redistribution of spectral weight due to the formation of the ferromagnetic state is sizable. We estimate that it corresponds to a lowering of the kinetic energy of 13 meV per Co ion, which is about two times $k_BT_c$. The latter shows that the saving of the kinetic energy is important and may be the leading energy contribution in the formation of the ferromagnetic phase.","sentences":["Using spectroscopic ellipsometry, we studied the optical conductivity of LaCoO$_3$ with various degrees of strain.","The optical response of the compressively strained \\lco\\ film is qualitatively similar to the one of the unstrained LaCoO$_3$ polycrystalline sample and exhibits redistribution of the spectral weight between about 0.2 and 6 eV, which is most likely related to the thermal excitation of the high-spin states.","The optical response of the ferromagnetic tensile strained film exhibits clear signatures due to the ferromagnetic state.","Below the Curie temperature $T_c=82$ K, the spectral weight is transferred with the increasing temperature from low energies between 0.2 and 3.3 eV to energies between 3.3 and 5.6 eV. The temperature dependence of the low-energy spectral weight between 0.2 and 3.3 eV can be understood in the framework of the high-spin biexciton model of Sotnikov and Kune\\v{s} as corresponding to the variation of the concentration of high-spin states that are stabilized below $T_c$. The magnitude of redistribution of spectral weight due to the formation of the ferromagnetic state is sizable.","We estimate that it corresponds to a lowering of the kinetic energy of 13 meV per Co ion, which is about two times $k_BT_c$. The latter shows that the saving of the kinetic energy is important and may be the leading energy contribution in the formation of the ferromagnetic phase."],"url":"http://arxiv.org/abs/2404.10385v1","category":"cond-mat.str-el"}
{"created":"2024-04-16 08:18:11","title":"Sublinear hitting sets for some geometric graphs","abstract":"For an $n$-vertex graph $G$, let $h(G)$ denote the smallest size of a subset of $V(G)$ such that it intersects every maximum independent set of $G$. A conjecture posed by Bollob\\'{a}s, Erd\\H{o}s and Tuza in early 90s remains widely open, asserting that for any $n$-vertex graph $G$, if the independence number $\\alpha(G) =\\Omega(n) $, then $h(G) = o(n)$. In this paper, we establish the validity of this conjecture for various classes of graphs, including disk graphs, even-hole-free graphs, circle graphs, and those hereditary graphs having sublinear balanced separators. We also determine the exact values of smallest possible hitting sets in comparability graphs, incomparability graphs and the graphs with VC-dimension one.","sentences":["For an $n$-vertex graph $G$, let $h(G)$ denote the smallest size of a subset of $V(G)$ such that it intersects every maximum independent set of $G$. A conjecture posed by Bollob\\'{a}s, Erd\\H{o}s and Tuza in early 90s remains widely open, asserting that for any $n$-vertex graph $G$, if the independence number $\\alpha(G) =\\Omega(n) $, then $h(G)","= o(n)$.","In this paper, we establish the validity of this conjecture for various classes of graphs, including disk graphs, even-hole-free graphs, circle graphs, and those hereditary graphs having sublinear balanced separators.","We also determine the exact values of smallest possible hitting sets in comparability graphs, incomparability graphs and the graphs with VC-dimension one."],"url":"http://arxiv.org/abs/2404.10379v1","category":"math.CO"}
{"created":"2024-04-16 07:37:35","title":"Symmetries for the 4HDM. II. Extensions by rephasing groups","abstract":"We continue classification of finite groups which can be used as symmetry group of the scalar sector of the four-Higgs-doublet model (4HDM). We systematically construct non-abelian groups via the group extension procedure, starting from the known abelian groups $A$ and their automorphism groups $\\mbox{Aut}(A)$. Previously, we considered all cyclic groups $A$ available for the 4HDM scalar sector. Here, we further develop the method and apply it to extensions by the remaining rephasing groups $A$, namely $A = \\mathbb{Z}_2\\times\\mathbb{Z}_2$, $\\mathbb{Z}_4\\times \\mathbb{Z}_2$, and $\\mathbb{Z}_2\\times \\mathbb{Z}_2\\times \\mathbb{Z}_2$. As $\\mbox{Aut}(A)$ grows, the procedure becomes more laborious, but we prove an isomorphism theorem which helps classify all the options. We also comment on what remains to be done to complete the classification of all finite non-abelian groups realizable in the 4HDM scalar sector without accidental continuous symmetries.","sentences":["We continue classification of finite groups which can be used as symmetry group of the scalar sector of the four-Higgs-doublet model (4HDM).","We systematically construct non-abelian groups via the group extension procedure, starting from the known abelian groups $A$ and their automorphism groups $\\mbox{Aut}(A)$. Previously, we considered all cyclic groups $A$ available for the 4HDM scalar sector.","Here, we further develop the method and apply it to extensions by the remaining rephasing groups $A$, namely $A = \\mathbb{Z}_2\\times\\mathbb{Z}_2$, $\\mathbb{Z}_4\\times \\mathbb{Z}_2$, and $\\mathbb{Z}_2\\times \\mathbb{Z}_2\\times \\mathbb{Z}_2$. As $\\mbox{Aut}(A)$ grows, the procedure becomes more laborious, but we prove an isomorphism theorem which helps classify all the options.","We also comment on what remains to be done to complete the classification of all finite non-abelian groups realizable in the 4HDM scalar sector without accidental continuous symmetries."],"url":"http://arxiv.org/abs/2404.10349v1","category":"hep-ph"}
{"created":"2024-04-16 07:17:52","title":"Electronic states and quantum transport in bilayer graphene Sierpinski-carpet fractals","abstract":"We construct Sierpinski-carpet (SC) based on AA or AB bilayer graphene by atom vacancies, namely, SC-AA and SC-AB, to investigate the effects of interlayer coupling on the electronic properties of fractals. Compared with monolayer graphene SC, their density of states have similar features, such as Van-Hove singularities and edge states corresponding to the central peaks near zero energy, but remarkable energy broadening of edge states emerges in SC-AA(AB). Calculated conductance spectrum shows that the conductance fluctuations still hold the Hausdorff fractal dimension behavior even with the interlayer coupling. Thus, the high correlation between quantum conductance and fractal geometry dimension is not affected by the interlayer coupling in bilayer graphene SC. We further reveal the quasi-eigenstates in fractal-like pressure-modulated bilayer graphene, namely, SC-pAA and SC-pAB. Numerical results show that the density of states of SC-pAA(pAB) show an asymptotic behavior to those of SC-AA(AB) especially for high energy quasi-eigenstates. Within a certain energy range, stronger pressure can lead to stronger localization, forming an efficient fractal space.","sentences":["We construct Sierpinski-carpet (SC) based on AA or AB bilayer graphene by atom vacancies, namely, SC-AA and SC-AB, to investigate the effects of interlayer coupling on the electronic properties of fractals.","Compared with monolayer graphene SC, their density of states have similar features, such as Van-Hove singularities and edge states corresponding to the central peaks near zero energy, but remarkable energy broadening of edge states emerges in SC-AA(AB).","Calculated conductance spectrum shows that the conductance fluctuations still hold the Hausdorff fractal dimension behavior even with the interlayer coupling.","Thus, the high correlation between quantum conductance and fractal geometry dimension is not affected by the interlayer coupling in bilayer graphene SC.","We further reveal the quasi-eigenstates in fractal-like pressure-modulated bilayer graphene, namely, SC-pAA and SC-pAB.","Numerical results show that the density of states of SC-pAA(pAB) show an asymptotic behavior to those of SC-AA(AB) especially for high energy quasi-eigenstates.","Within a certain energy range, stronger pressure can lead to stronger localization, forming an efficient fractal space."],"url":"http://arxiv.org/abs/2404.10333v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-16 06:48:41","title":"Multiple Mobile Target Detection and Tracking in Active Sonar Array Using a Track-Before-Detect Approach","abstract":"We present an algorithm for detecting and tracking underwater mobile objects using active acoustic transmission of broadband chirp signals whose reflections are received by a hydrophone array. The method overcomes the problem of high false alarm rate by applying a track-before-detect ap- proach to the sequence of received reflections. A 2D time- space matrix is created for the reverberations received from each transmitted probe signal by performing delay and sum beamforming and pulse compression. The result is filtered by a 2D constant false alarm rate (CFAR) detector to identify reflection patterns corresponding to potential targets. Closely spaced signals for multiple probe transmissions are combined into blobs to avoid multiple detections of a single object. A track- before-detect method using a Nearly Constant Velocity (NCV) model is employed to track multiple objects. The position and velocity is estimated by the debiased converted measurement Kalman filter. Results are analyzed for simulated scenarios and for experiments at sea, where GPS tagged gilt-head seabream fish were tracked. Compared to two benchmark schemes, the results show a favorable track continuity and accuracy that is robust to the choice of detection threshold.","sentences":["We present an algorithm for detecting and tracking underwater mobile objects using active acoustic transmission of broadband chirp signals whose reflections are received by a hydrophone array.","The method overcomes the problem of high false alarm rate by applying a track-before-detect ap- proach to the sequence of received reflections.","A 2D time- space matrix is created for the reverberations received from each transmitted probe signal by performing delay and sum beamforming and pulse compression.","The result is filtered by a 2D constant false alarm rate (CFAR) detector to identify reflection patterns corresponding to potential targets.","Closely spaced signals for multiple probe transmissions are combined into blobs to avoid multiple detections of a single object.","A track- before-detect method using a Nearly Constant Velocity (NCV) model is employed to track multiple objects.","The position and velocity is estimated by the debiased converted measurement Kalman filter.","Results are analyzed for simulated scenarios and for experiments at sea, where GPS tagged gilt-head seabream fish were tracked.","Compared to two benchmark schemes, the results show a favorable track continuity and accuracy that is robust to the choice of detection threshold."],"url":"http://arxiv.org/abs/2404.10316v1","category":"cs.SD"}
{"created":"2024-04-16 06:02:24","title":"How does Casimir energy fall in $\u03ba$-deformed space-time?","abstract":"We investigate the response of Casimir energies to fluctuations in a scalar field in a weak gravitational field in the $\\kappa$-deformed space-time. We model the Casimir plates in a gravitational field by $\\kappa$-deformed Rindler coordinates and calculate the Casimir energy using the $\\kappa$-deformed scalar field. We show that the Casimir energy accelerates in a weak gravitational field like a mass. Thus, our calculations show that the mass-energy equivalence principle holds in $\\kappa$-deformed space-time even though a length scale is introduced through space-time non-commutativity.","sentences":["We investigate the response of Casimir energies to fluctuations in a scalar field in a weak gravitational field in the $\\kappa$-deformed space-time.","We model the Casimir plates in a gravitational field by $\\kappa$-deformed Rindler coordinates and calculate the Casimir energy using the $\\kappa$-deformed scalar field.","We show that the Casimir energy accelerates in a weak gravitational field like a mass.","Thus, our calculations show that the mass-energy equivalence principle holds in $\\kappa$-deformed space-time even though a length scale is introduced through space-time non-commutativity."],"url":"http://arxiv.org/abs/2404.10300v1","category":"hep-th"}
{"created":"2024-04-16 05:22:22","title":"The N2HDM, Entropy Production and Stochastic Gravitational Waves","abstract":"This study undertakes a reconsideration of the potential for a first-order electroweak phase transition, focusing on the next-to-minimal two Higgs doublet model (N2HDM). Our exploration spans diverse parameter spaces associated with the phase transition, with a particular emphasis on examining the generation of stochastic Gravitational Waves (GW) resulting from this transition. The obtained results are meticulously compared against data from prominent gravitational wave observatories, and the possibility of their detection in the future GW observations have been established. In passing by we analyse the strength of the phase transition through the production of entropy during the electroweak phase transition.","sentences":["This study undertakes a reconsideration of the potential for a first-order electroweak phase transition, focusing on the next-to-minimal two Higgs doublet model (N2HDM).","Our exploration spans diverse parameter spaces associated with the phase transition, with a particular emphasis on examining the generation of stochastic Gravitational Waves (GW) resulting from this transition.","The obtained results are meticulously compared against data from prominent gravitational wave observatories, and the possibility of their detection in the future GW observations have been established.","In passing by we analyse the strength of the phase transition through the production of entropy during the electroweak phase transition."],"url":"http://arxiv.org/abs/2404.10288v1","category":"hep-ph"}
{"created":"2024-04-16 04:55:59","title":"Majoron-Driven Leptogenesis in Gauged $U(1)_{L_\u03bc-L_\u03c4}$ Model","abstract":"We propose a novel leptogenesis scenario in the gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ model. Achieving successful leptogenesis in the $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetric phase is challenging due to the absence of a CP phase, caused by restriction from the gauge symmetry. To overcome this issue, we introduce an additional global symmetry, $U(1)_{B-L}$, and a scalar field $\\Phi$ responsible for breaking this symmetry. Through the kinetic misalignment mechanism, the majoron field associated with $U(1)_{B-L}$ symmetry breaking has a kinetic motion in the early universe. Subsequently, time-dependent majoron field background induces the background CP phase dynamically, leading to successful leptogenesis in the $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetric phase. Furthermore, majoron itself serves as a dark matter candidate in this scenario. As one of the phenomenological applications, we consider the model that can also explain the muon $g-2$ anomaly.","sentences":["We propose a novel leptogenesis scenario in the gauged $U(1)_{L_{\\mu}-L_{\\tau}}$ model.","Achieving successful leptogenesis in the $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetric phase is challenging due to the absence of a CP phase, caused by restriction from the gauge symmetry.","To overcome this issue, we introduce an additional global symmetry, $U(1)_{B-L}$, and a scalar field $\\Phi$ responsible for breaking this symmetry.","Through the kinetic misalignment mechanism, the majoron field associated with $U(1)_{B-L}$ symmetry breaking has a kinetic motion in the early universe.","Subsequently, time-dependent majoron field background induces the background CP phase dynamically, leading to successful leptogenesis in the $U(1)_{L_{\\mu}-L_{\\tau}}$ symmetric phase.","Furthermore, majoron itself serves as a dark matter candidate in this scenario.","As one of the phenomenological applications, we consider the model that can also explain the muon $g-2$ anomaly."],"url":"http://arxiv.org/abs/2404.10283v1","category":"hep-ph"}
{"created":"2024-04-16 04:47:26","title":"Efficient diffraction control using a tunable active-Raman gain medium","abstract":"We present a new scheme to create all-optical tunable and lossless waveguide using a controllable coherent Raman process in an atomic rubidium vapor in N-type configuration. We employ a Gaussian Raman field and a Laguerre-Gaussian control field to imprint a high-contrast tunable waveguide-like feature inside the atomic medium. We numerically demonstrate that such a waveguide is able to guide arbitrary modes of a weak probe beam to several Rayleigh length without diffraction and absorption. Our results on all-optical waveguide based scheme may have potential application in lossless image processing, high contrast biomedical imaging and image metrology.","sentences":["We present a new scheme to create all-optical tunable and lossless waveguide using a controllable coherent Raman process in an atomic rubidium vapor in N-type configuration.","We employ a Gaussian Raman field and a Laguerre-Gaussian control field to imprint a high-contrast tunable waveguide-like feature inside the atomic medium.","We numerically demonstrate that such a waveguide is able to guide arbitrary modes of a weak probe beam to several Rayleigh length without diffraction and absorption.","Our results on all-optical waveguide based scheme may have potential application in lossless image processing, high contrast biomedical imaging and image metrology."],"url":"http://arxiv.org/abs/2404.10280v1","category":"physics.optics"}
{"created":"2024-04-16 03:48:16","title":"Status and progress of lattice QCD","abstract":"We review recent progress from lattice QCD for the determination of the Cabibbo-Kobayashi-Maskawa matrix elements.","sentences":["We review recent progress from lattice QCD for the determination of the Cabibbo-Kobayashi-Maskawa matrix elements."],"url":"http://arxiv.org/abs/2404.10269v1","category":"hep-lat"}
{"created":"2024-04-16 03:46:30","title":"Modeling Low-Resource Health Coaching Dialogues via Neuro-Symbolic Goal Summarization and Text-Units-Text Generation","abstract":"Health coaching helps patients achieve personalized and lifestyle-related goals, effectively managing chronic conditions and alleviating mental health issues. It is particularly beneficial, however cost-prohibitive, for low-socioeconomic status populations due to its highly personalized and labor-intensive nature. In this paper, we propose a neuro-symbolic goal summarizer to support health coaches in keeping track of the goals and a text-units-text dialogue generation model that converses with patients and helps them create and accomplish specific goals for physical activities. Our models outperform previous state-of-the-art while eliminating the need for predefined schema and corresponding annotation. We also propose a new health coaching dataset extending previous work and a metric to measure the unconventionality of the patient's response based on data difficulty, facilitating potential coach alerts during deployment.","sentences":["Health coaching helps patients achieve personalized and lifestyle-related goals, effectively managing chronic conditions and alleviating mental health issues.","It is particularly beneficial, however cost-prohibitive, for low-socioeconomic status populations due to its highly personalized and labor-intensive nature.","In this paper, we propose a neuro-symbolic goal summarizer to support health coaches in keeping track of the goals and a text-units-text dialogue generation model that converses with patients and helps them create and accomplish specific goals for physical activities.","Our models outperform previous state-of-the-art while eliminating the need for predefined schema and corresponding annotation.","We also propose a new health coaching dataset extending previous work and a metric to measure the unconventionality of the patient's response based on data difficulty, facilitating potential coach alerts during deployment."],"url":"http://arxiv.org/abs/2404.10268v1","category":"cs.CL"}
{"created":"2024-04-16 03:11:47","title":"The Thousand-Pulsar-Array programme on MeerKAT XIV: On the high linearly polarized pulsar signals","abstract":"The S-shaped swing of the linear polarization position angle (PPA) observed in many pulsars can be interpreted by the rotating vector model (RVM). However, efforts to fit the RVM for a large sample of pulsars observed with the MeerKAT telescope as a part of the Thousand-Pulsar-Array (TPA) programme, only succeeded for about half the cases. High time-resolution studies suggest that the failed cases arise due to the presence of orthogonal polarization modes, or highly disordered distribution of PPA points. One such example is PSR~J1645-0317. Recently it has been shown that the RVM can be recovered in this pulsar by using only time samples which are greater than 80% linearly polarized. In this work we test this novel approach on the brightest 249 pulsars from the TPA sample, of which 177 yield sufficient highly polarized samples to be amenable to our method. Remarkably, only 9 of these pulsars (5%) now fail to fit the RVM as opposed to 59% from the original analysis. This result favours the paradigm that the underlying mechanism is coherent curvature radiation.","sentences":["The S-shaped swing of the linear polarization position angle (PPA) observed in many pulsars can be interpreted by the rotating vector model (RVM).","However, efforts to fit the RVM for a large sample of pulsars observed with the MeerKAT telescope as a part of the Thousand-Pulsar-Array (TPA) programme, only succeeded for about half the cases.","High time-resolution studies suggest that the failed cases arise due to the presence of orthogonal polarization modes, or highly disordered distribution of PPA points.","One such example is PSR~J1645-0317.","Recently it has been shown that the RVM can be recovered in this pulsar by using only time samples which are greater than 80% linearly polarized.","In this work we test this novel approach on the brightest 249 pulsars from the TPA sample, of which 177 yield sufficient highly polarized samples to be amenable to our method.","Remarkably, only 9 of these pulsars (5%) now fail to fit the RVM as opposed to 59% from the original analysis.","This result favours the paradigm that the underlying mechanism is coherent curvature radiation."],"url":"http://arxiv.org/abs/2404.10254v1","category":"astro-ph.HE"}
{"created":"2024-04-16 02:58:32","title":"Picturing the Gap Between the Performance and US-DOE's Hydrogen Storage Target: A Data-Driven Model for MgH2 Dehydrogenation","abstract":"Developing solid-state hydrogen storage materials is as pressing as ever, which requires a comprehensive understanding of the dehydrogenation chemistry of a solid-state hydride. Transition state search and kinetics calculations are essential to understanding and designing high-performance solid-state hydrogen storage materials by filling in the knowledge gap that current experimental techniques cannot measure. However, the ab initio analysis of these processes is computationally expensive and time-consuming. Searching for descriptors to accurately predict the energy barrier is urgently needed, to accelerate the prediction of hydrogen storage material properties and identify the opportunities and challenges in this field. Herein, we develop a data-driven model to describe and predict the dehydrogenation barriers of a typical solid-state hydrogen storage material, magnesium hydride (MgH2), based on the combination of the crystal Hamilton population orbital of Mg-H bond and the distance between atomic hydrogen. By deriving the distance energy ratio, this model elucidates the key chemistry of the reaction kinetics. All the parameters in this model can be directly calculated with significantly less computational cost than conventional transition state search, so that the dehydrogenation performance of hydrogen storage materials can be predicted efficiently. Finally, we found that this model leads to excellent agreement with typical experimental measurements reported to date and provides clear design guidelines on how to propel the performance of MgH2 closer to the target set by the United States Department of Energy (US-DOE).","sentences":["Developing solid-state hydrogen storage materials is as pressing as ever, which requires a comprehensive understanding of the dehydrogenation chemistry of a solid-state hydride.","Transition state search and kinetics calculations are essential to understanding and designing high-performance solid-state hydrogen storage materials by filling in the knowledge gap that current experimental techniques cannot measure.","However, the ab initio analysis of these processes is computationally expensive and time-consuming.","Searching for descriptors to accurately predict the energy barrier is urgently needed, to accelerate the prediction of hydrogen storage material properties and identify the opportunities and challenges in this field.","Herein, we develop a data-driven model to describe and predict the dehydrogenation barriers of a typical solid-state hydrogen storage material, magnesium hydride (MgH2), based on the combination of the crystal Hamilton population orbital of Mg-H bond and the distance between atomic hydrogen.","By deriving the distance energy ratio, this model elucidates the key chemistry of the reaction kinetics.","All the parameters in this model can be directly calculated with significantly less computational cost than conventional transition state search, so that the dehydrogenation performance of hydrogen storage materials can be predicted efficiently.","Finally, we found that this model leads to excellent agreement with typical experimental measurements reported to date and provides clear design guidelines on how to propel the performance of MgH2 closer to the target set by the United States Department of Energy (US-DOE)."],"url":"http://arxiv.org/abs/2404.10249v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-16 02:46:17","title":"Quasars with Flare/Eclipse-like Variability Identified in ZTF","abstract":"Active galactic nuclei (AGNs) are known to exhibit optical/UV variability and most of them can be well modeled by the damped random walks. Physical processes that are not related to the accretion disk, such as tidal disruption events (TDE) or moving foreground dusty clouds, can cause flare-like and eclipse-like features in the optical light curve. Both long-term and high-cadence monitoring are needed to identify such features. By combining the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope, and Rapid Response System (Pan-STARRS) with the Zwicky Transient Facility (ZTF) survey, we are able to identify a rare sample (11) out of the SDSS quasar catalog (around 83, 000). These quasars exhibit more or less constant brightness but show rapid optical variation in the ZTF DR2 epochs. To investigate the possible origins of these flare/eclipse-like variabilities, we propose the second epoch spectroscopic observations with the Gran Telescopio CANARIAS (GTC). We find that the change in accretion rate plays a significant role in these quasar variabilities. Among them, we identify two Changing-Look Active Galactic Nuclei (CL-AGN) candidates: SDSS J1427+2930 and SDSS J1420+3757. The luminosity change of the former may be caused by the enhanced SMBH accretion or the tidal disruption event, while the latter is more related to the change in the accretion rate.","sentences":["Active galactic nuclei (AGNs) are known to exhibit optical/UV variability and most of them can be well modeled by the damped random walks.","Physical processes that are not related to the accretion disk, such as tidal disruption events (TDE) or moving foreground dusty clouds, can cause flare-like and eclipse-like features in the optical light curve.","Both long-term and high-cadence monitoring are needed to identify such features.","By combining the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope, and Rapid Response System (Pan-STARRS) with the Zwicky Transient Facility (ZTF) survey, we are able to identify a rare sample (11) out of the SDSS quasar catalog (around 83, 000).","These quasars exhibit more or less constant brightness but show rapid optical variation in the ZTF DR2 epochs.","To investigate the possible origins of these flare/eclipse-like variabilities, we propose the second epoch spectroscopic observations with the Gran Telescopio CANARIAS (GTC).","We find that the change in accretion rate plays a significant role in these quasar variabilities.","Among them, we identify two Changing-Look Active Galactic Nuclei (CL-AGN) candidates: SDSS J1427+2930 and SDSS J1420+3757.","The luminosity change of the former may be caused by the enhanced SMBH accretion or the tidal disruption event, while the latter is more related to the change in the accretion rate."],"url":"http://arxiv.org/abs/2404.10245v1","category":"astro-ph.GA"}
{"created":"2024-04-16 02:28:20","title":"Little Pilot is Needed for Channel Estimation with Integrated Super-Resolution Sensing and Communication","abstract":"Integrated super-resolution sensing and communication (ISSAC) is a promising technology to achieve extremely high sensing performance for critical parameters, such as the angles of the wireless channels. In this paper, we propose an ISSAC-based channel estimation method, which requires little or even no pilot, yet still achieves accurate channel state information (CSI) estimation. The key idea is to exploit the fact that subspace-based super-resolution algorithms such as multiple signal classification (MUSIC) do not require a priori known pilots for accurate parameter estimation. Therefore, in the proposed method, the angles of the multi-path channel components are first estimated in a pilot-free manner while communication data symbols are sent. After that, the multi-path channel coefficients are estimated, where very little pilots are needed. The reasons are two folds. First, compared to the conventional channel estimation methods purely relying on channel training, much fewer parameters need to be estimated once the multi-path angles are accurately estimated. Besides, with angles obtained, the beamforming gain is also enjoyed when pilots are sent to estimate the channel path gains. To rigorously study the performance of the proposed method, we first consider the basic line-of-sight (LoS) channel. By analyzing the minimum mean square error (MMSE) of channel estimation and the resulting beamforming gains, we show that our proposed method significantly outperforms the conventional methods purely based on channel training. We then extend the study to the more general multipath channels. Simulation results are provided to demonstrate our theoretical results.","sentences":["Integrated super-resolution sensing and communication (ISSAC) is a promising technology to achieve extremely high sensing performance for critical parameters, such as the angles of the wireless channels.","In this paper, we propose an ISSAC-based channel estimation method, which requires little or even no pilot, yet still achieves accurate channel state information (CSI) estimation.","The key idea is to exploit the fact that subspace-based super-resolution algorithms such as multiple signal classification (MUSIC) do not require a priori known pilots for accurate parameter estimation.","Therefore, in the proposed method, the angles of the multi-path channel components are first estimated in a pilot-free manner while communication data symbols are sent.","After that, the multi-path channel coefficients are estimated, where very little pilots are needed.","The reasons are two folds.","First, compared to the conventional channel estimation methods purely relying on channel training, much fewer parameters need to be estimated once the multi-path angles are accurately estimated.","Besides, with angles obtained, the beamforming gain is also enjoyed when pilots are sent to estimate the channel path gains.","To rigorously study the performance of the proposed method, we first consider the basic line-of-sight (LoS) channel.","By analyzing the minimum mean square error (MMSE) of channel estimation and the resulting beamforming gains, we show that our proposed method significantly outperforms the conventional methods purely based on channel training.","We then extend the study to the more general multipath channels.","Simulation results are provided to demonstrate our theoretical results."],"url":"http://arxiv.org/abs/2404.10233v1","category":"eess.SP"}
{"created":"2024-04-16 02:04:47","title":"A Quantum Eigenvalue Solver Based on Tensor Networks","abstract":"We introduce a hybrid quantum-classical eigenvalue solver that constructs a wavefunction ansatz from a linear combination of matrix product states in rotated orbital bases, enabling the characterization of chemical ground states that are not subject to the constraint of a one-dimensional area law of entanglement. The energy is converged via a gradient-free generalized sweep algorithm based on quantum subspace diagonalization, with a potentially exponential speedup in the off-diagonal matrix element contractions upon translation into shallow quantum circuits of linear depth in the number of qubits. Chemical accuracy is attained in numerical experiments for both a stretched water molecule and an octahedral arrangement of hydrogen atoms, achieving substantially better correlation energies compared to a unitary coupled-cluster benchmark, with orders of magnitude reductions in quantum resource estimates and a surprisingly high tolerance to shot noise. This proof-of-concept study suggests a promising new avenue for scaling up simulations of strongly correlated chemical systems on near-term quantum hardware.","sentences":["We introduce a hybrid quantum-classical eigenvalue solver that constructs a wavefunction ansatz from a linear combination of matrix product states in rotated orbital bases, enabling the characterization of chemical ground states that are not subject to the constraint of a one-dimensional area law of entanglement.","The energy is converged via a gradient-free generalized sweep algorithm based on quantum subspace diagonalization, with a potentially exponential speedup in the off-diagonal matrix element contractions upon translation into shallow quantum circuits of linear depth in the number of qubits.","Chemical accuracy is attained in numerical experiments for both a stretched water molecule and an octahedral arrangement of hydrogen atoms, achieving substantially better correlation energies compared to a unitary coupled-cluster benchmark, with orders of magnitude reductions in quantum resource estimates and a surprisingly high tolerance to shot noise.","This proof-of-concept study suggests a promising new avenue for scaling up simulations of strongly correlated chemical systems on near-term quantum hardware."],"url":"http://arxiv.org/abs/2404.10223v1","category":"quant-ph"}
{"created":"2024-04-16 02:04:11","title":"Simulating electronic structure on bosonic quantum computers","abstract":"Computations with quantum harmonic oscillators or qumodes is a promising and rapidly evolving approach towards quantum computing. In contrast to qubits, which are two-level quantum systems, bosonic qumodes can in principle have infinite discrete levels, and can also be represented with continuous variable bases. One of the most promising applications of quantum computing is simulating many-fermion problems such as molecular electronic structure. Although there has been a lot of recent progress on simulating many-fermion systems on qubit-based quantum hardware, they can not be easily extended to bosonic quantum devices due to the fundamental difference in physics represented by qubits and qumodes. In this work, we show how an electronic structure Hamiltonian can be transformed into a system of qumodes with a fermion to boson mapping scheme and apply it to simulate the electronic structure of dihydrogen molecule as a system of two qumodes. Our work opens the door for simulating many-fermion systems by harnessing the power of bosonic quantum devices.","sentences":["Computations with quantum harmonic oscillators or qumodes is a promising and rapidly evolving approach towards quantum computing.","In contrast to qubits, which are two-level quantum systems, bosonic qumodes can in principle have infinite discrete levels, and can also be represented with continuous variable bases.","One of the most promising applications of quantum computing is simulating many-fermion problems such as molecular electronic structure.","Although there has been a lot of recent progress on simulating many-fermion systems on qubit-based quantum hardware, they can not be easily extended to bosonic quantum devices due to the fundamental difference in physics represented by qubits and qumodes.","In this work, we show how an electronic structure Hamiltonian can be transformed into a system of qumodes with a fermion to boson mapping scheme and apply it to simulate the electronic structure of dihydrogen molecule as a system of two qumodes.","Our work opens the door for simulating many-fermion systems by harnessing the power of bosonic quantum devices."],"url":"http://arxiv.org/abs/2404.10222v1","category":"quant-ph"}
{"created":"2024-04-16 01:49:35","title":"LWIRPOSE: A novel LWIR Thermal Image Dataset and Benchmark","abstract":"Human pose estimation faces hurdles in real-world applications due to factors like lighting changes, occlusions, and cluttered environments. We introduce a unique RGB-Thermal Nearly Paired and Annotated 2D Pose Dataset, comprising over 2,400 high-quality LWIR (thermal) images. Each image is meticulously annotated with 2D human poses, offering a valuable resource for researchers and practitioners. This dataset, captured from seven actors performing diverse everyday activities like sitting, eating, and walking, facilitates pose estimation on occlusion and other challenging scenarios. We benchmark state-of-the-art pose estimation methods on the dataset to showcase its potential, establishing a strong baseline for future research. Our results demonstrate the dataset's effectiveness in promoting advancements in pose estimation for various applications, including surveillance, healthcare, and sports analytics. The dataset and code are available at https://github.com/avinres/LWIRPOSE","sentences":["Human pose estimation faces hurdles in real-world applications due to factors like lighting changes, occlusions, and cluttered environments.","We introduce a unique RGB-Thermal Nearly Paired and Annotated 2D Pose Dataset, comprising over 2,400 high-quality LWIR (thermal) images.","Each image is meticulously annotated with 2D human poses, offering a valuable resource for researchers and practitioners.","This dataset, captured from seven actors performing diverse everyday activities like sitting, eating, and walking, facilitates pose estimation on occlusion and other challenging scenarios.","We benchmark state-of-the-art pose estimation methods on the dataset to showcase its potential, establishing a strong baseline for future research.","Our results demonstrate the dataset's effectiveness in promoting advancements in pose estimation for various applications, including surveillance, healthcare, and sports analytics.","The dataset and code are available at https://github.com/avinres/LWIRPOSE"],"url":"http://arxiv.org/abs/2404.10212v1","category":"cs.CV"}
{"created":"2024-04-16 01:11:32","title":"Testing the TDE Outflow Model for the Bipolar Sgr A Lobes at the Galactic Center","abstract":"Sgr A lobes are a pair of 15-pc-sized bipolar bubbles with co-aligned major axes perpendicular to the Galactic plane found in X-ray and radio observations of the Galactic center (GC). Their elusive origin is a vital ingredient in understanding the ongoing high energy processes at the GC. Here we perform a suite of hydrodynamic simulations to explore the tidal disruption event (TDE) outflow model for the origin of the Sgr A lobes. By following the outflow evolution in the circumnuclear medium, we find that TDE outflows naturally produce bipolar lobes delimited by forward shocks, and the dense postshock shell contributes significantly to the lobe's X-ray emission. Our fiducial run reproduces the morphology, density, temperature, and X-ray surface brightness distribution of the observed Sgr A lobes reasonably well. The current lobe age is ~3300 yr. Our model further predicts that the uplifted wake flow breaks through the ejecta-induced shock front, producing a shock-enclosed head region which is relatively dim in X-rays compared to other lobe regions. Both the dim head region and the predicted limb-brightening feature of the lobes are slightly inconsistent with current X-ray observations, and may be used to further test our model with more sensitive future X-ray observations. While light narrow jets and massive wide winds from TDE events usually do not reproduce the observed oval-shaped morphology of the lobes, the TDE outflow in our fiducial run is massive and yet narrow. Whether it is a jet or wind remains uncertain and future simulations covering both the outflow acceleration region and its pc-scale evolution would be very helpful in determining whether the Sgr A lobes indeed originate from a pair of TDE jets or winds.","sentences":["Sgr A lobes are a pair of 15-pc-sized bipolar bubbles with co-aligned major axes perpendicular to the Galactic plane found in X-ray and radio observations of the Galactic center (GC).","Their elusive origin is a vital ingredient in understanding the ongoing high energy processes at the GC.","Here we perform a suite of hydrodynamic simulations to explore the tidal disruption event (TDE) outflow model for the origin of the Sgr A lobes.","By following the outflow evolution in the circumnuclear medium, we find that TDE outflows naturally produce bipolar lobes delimited by forward shocks, and the dense postshock shell contributes significantly to the lobe's X-ray emission.","Our fiducial run reproduces the morphology, density, temperature, and X-ray surface brightness distribution of the observed Sgr A lobes reasonably well.","The current lobe age is ~3300 yr.","Our model further predicts that the uplifted wake flow breaks through the ejecta-induced shock front, producing a shock-enclosed head region which is relatively dim in X-rays compared to other lobe regions.","Both the dim head region and the predicted limb-brightening feature of the lobes are slightly inconsistent with current X-ray observations, and may be used to further test our model with more sensitive future X-ray observations.","While light narrow jets and massive wide winds from TDE events usually do not reproduce the observed oval-shaped morphology of the lobes, the TDE outflow in our fiducial run is massive and yet narrow.","Whether it is a jet or wind remains uncertain and future simulations covering both the outflow acceleration region and its pc-scale evolution would be very helpful in determining whether the Sgr A lobes indeed originate from a pair of TDE jets or winds."],"url":"http://arxiv.org/abs/2404.10205v1","category":"astro-ph.HE"}
{"created":"2024-04-16 01:08:15","title":"Finitely and non-finitely related words","abstract":"An algebra is finitely related (or has finite degree) if its term functions are determined by some finite set of finitary relations. Nilpotent monoids built from words, via Rees quotients of free monoids, have been used to exhibit many interesting properties with respect to the finite basis problem. We show that much of this intriguing behaviour extends to the world of finite relatedness by using interlocking patterns called chain, crown, and maelstrom words. In particular, we show that there are large classes of non-finitely related nilpotent monoids that can be used to construct examples of: ascending chains of varieties alternating between finitely and non-finitely related; non-finitely related semigroups whose direct product are finitely related; the addition of an identity element to a non-finitely related semigroup to produce a finitely related semigroup.","sentences":["An algebra is finitely related (or has finite degree) if its term functions are determined by some finite set of finitary relations.","Nilpotent monoids built from words, via Rees quotients of free monoids, have been used to exhibit many interesting properties with respect to the finite basis problem.","We show that much of this intriguing behaviour extends to the world of finite relatedness by using interlocking patterns called chain, crown, and maelstrom words.","In particular, we show that there are large classes of non-finitely related nilpotent monoids that can be used to construct examples of: ascending chains of varieties alternating between finitely and non-finitely related; non-finitely related semigroups whose direct product are finitely related; the addition of an identity element to a non-finitely related semigroup to produce a finitely related semigroup."],"url":"http://arxiv.org/abs/2404.10203v1","category":"math.GR"}
{"created":"2024-04-15 23:44:52","title":"On the spectrum of open strings in the Hard-Wall model of AdS/QCD: the role of $S^5$","abstract":"We revisit the problem of a classical open string representing a meson in the hard-wall geometry and consider the $S^5$ compact space. The existence of a confining region due to the hard cut-off leads to a discrete set of string configurations parameterized by the number of complete turnarounds $S^5$ great circles. On the one hand, for a given angular momentum, there is a discrete set of possible separations, and the confined quarks on the boundary are restrict to a lattice. On the other hand, for a given separation in the confining region, there is a discrete set of permitted values for the angular momentum. The $q\\bar{q}$ potential is linear for large separations, and the string tension is very sensitive to the angular momentum.","sentences":["We revisit the problem of a classical open string representing a meson in the hard-wall geometry and consider the $S^5$ compact space.","The existence of a confining region due to the hard cut-off leads to a discrete set of string configurations parameterized by the number of complete turnarounds $S^5$ great circles.","On the one hand, for a given angular momentum, there is a discrete set of possible separations, and the confined quarks on the boundary are restrict to a lattice.","On the other hand, for a given separation in the confining region, there is a discrete set of permitted values for the angular momentum.","The $q\\bar{q}$ potential is linear for large separations, and the string tension is very sensitive to the angular momentum."],"url":"http://arxiv.org/abs/2404.10183v1","category":"hep-th"}
{"created":"2024-04-15 23:33:28","title":"Four Changing-Look Active Galactic Nuclei Found From Optical Variations","abstract":"We report the finding of four changing-look (CL) active galactic nuclei (AGN). We selected these sources due to their potential as interesting targets when considering their relatively-large optical flux variations and related mid-infrared flux variations. To identify their CL feature, we use archival spectra from the Sloan Digital Sky Survey (SDSS) taken at least 8 years ago as well as spectra taken recently from the Transient Name Server (TNS) and with the 2.4-m LiJiang telescope (LJT). We study the sources' spectral changes by fitting and determining the H$_\\alpha$ and H$_\\beta$ components and verify their CL behavior. When comparing the TNS and/or LJT spectra to the SDSS ones, all four sources showed the appearance of a broad or a stronger broad H$_\\alpha$ component and a relatively weak broad H$_\\beta$ component. As two of the four sources are established to have a brighter-and-bluer feature in the photometric data, during the time periods in which the TNS and LJT spectra were taken, this feature likely accompanied the turn-on of the broad components. Thus, we suggest that this brighter-and-bluer feature can be used as a criterion for efficiently finding CL sources among previously spectroscopically classified type 2 AGN, such as from among the sources provided by the SDSS.","sentences":["We report the finding of four changing-look (CL) active galactic nuclei (AGN).","We selected these sources due to their potential as interesting targets when considering their relatively-large optical flux variations and related mid-infrared flux variations.","To identify their CL feature, we use archival spectra from the Sloan Digital Sky Survey (SDSS) taken at least 8 years ago as well as spectra taken recently from the Transient Name Server (TNS) and with the 2.4-m LiJiang telescope (LJT).","We study the sources' spectral changes by fitting and determining the H$_\\alpha$ and H$_\\beta$ components and verify their CL behavior.","When comparing the TNS and/or LJT spectra to the SDSS ones, all four sources showed the appearance of a broad or a stronger broad H$_\\alpha$ component and a relatively weak broad H$_\\beta$ component.","As two of the four sources are established to have a brighter-and-bluer feature in the photometric data, during the time periods in which the TNS and LJT spectra were taken, this feature likely accompanied the turn-on of the broad components.","Thus, we suggest that this brighter-and-bluer feature can be used as a criterion for efficiently finding CL sources among previously spectroscopically classified type 2 AGN, such as from among the sources provided by the SDSS."],"url":"http://arxiv.org/abs/2404.10181v1","category":"astro-ph.GA"}
{"created":"2024-04-15 23:02:50","title":"Coherent control of an optical tweezer phonon laser","abstract":"The creation and manipulation of coherence continues to capture the attention of scientists and engineers. The optical laser is a canonical example of a system that, in principle, exhibits complete coherence. Recent research has focused on the creation of coherent, laser-like states in other physical systems. The phonon laser is one example where it is possible to amplify self-sustained mechanical oscillations. A single mode phonon laser in a levitated optical tweezer has been demonstrated through appropriate balance of active feedback gain and damping. In this work, coherent control of the dynamics of an optical tweezer phonon laser is used to share coherence between its different modes of oscillation, creating a multimode phonon laser. The coupling of the modes is achieved by periodically rotating the asymmetric optical potential in the transverse focal plane of the trapping beam via trap laser polarization rotation. The presented theory and experiment demonstrate that coherence can be transferred across different modes of an optical tweezer phonon laser, and are a step toward using these systems for precision measurement and quantum information processing.","sentences":["The creation and manipulation of coherence continues to capture the attention of scientists and engineers.","The optical laser is a canonical example of a system that, in principle, exhibits complete coherence.","Recent research has focused on the creation of coherent, laser-like states in other physical systems.","The phonon laser is one example where it is possible to amplify self-sustained mechanical oscillations.","A single mode phonon laser in a levitated optical tweezer has been demonstrated through appropriate balance of active feedback gain and damping.","In this work, coherent control of the dynamics of an optical tweezer phonon laser is used to share coherence between its different modes of oscillation, creating a multimode phonon laser.","The coupling of the modes is achieved by periodically rotating the asymmetric optical potential in the transverse focal plane of the trapping beam via trap laser polarization rotation.","The presented theory and experiment demonstrate that coherence can be transferred across different modes of an optical tweezer phonon laser, and are a step toward using these systems for precision measurement and quantum information processing."],"url":"http://arxiv.org/abs/2404.10173v1","category":"physics.optics"}
{"created":"2024-04-15 21:51:11","title":"Distributing Context-Aware Shared Memory Data Structures: A Case Study on Unordered Linked List","abstract":"In this paper, we focus on partitioning a context-aware shared memory data structure so that it can be implemented as a distributed data structure running on multiple machines. By context-aware data structures, we mean that the result of an operation not only depends upon the value of the shared data but also upon the previous operations performed by the same client. While there is substantial work on designing distributed data structures that are not context-aware (e.g., hash tables), designing distributed context-aware data structures has not received much attention. We focus on the unordered list as a case study of the context- aware data structure. We start with a shared memory context-aware lock-free unordered linked list and show how it can be transformed into a distributed lock-free context-aware unordered linked list. The main challenge in such a transformation is to preserve properties of client-visible operations of the underlying data structure. We present two protocols that preserve these properties of client-visible operations of the linked list. In the first protocol, the distribution is done in the background as a low priority task, while in the second protocol the client-visible operations help the task of distribution without affecting client latency. In both protocols, the client-visible operations remain lock-free. Also, our transformation approach does not utilize any hardware primitives (except a compare-and-swap operation on a single word). We note that our transformation is generic and can be used for other lock-free context-aware data structures.","sentences":["In this paper, we focus on partitioning a context-aware shared memory data structure so that it can be implemented as a distributed data structure running on multiple machines.","By context-aware data structures, we mean that the result of an operation not only depends upon the value of the shared data but also upon the previous operations performed by the same client.","While there is substantial work on designing distributed data structures that are not context-aware (e.g., hash tables), designing distributed context-aware data structures has not received much attention.","We focus on the unordered list as a case study of the context- aware data structure.","We start with a shared memory context-aware lock-free unordered linked list and show how it can be transformed into a distributed lock-free context-aware unordered linked list.","The main challenge in such a transformation is to preserve properties of client-visible operations of the underlying data structure.","We present two protocols that preserve these properties of client-visible operations of the linked list.","In the first protocol, the distribution is done in the background as a low priority task, while in the second protocol the client-visible operations help the task of distribution without affecting client latency.","In both protocols, the client-visible operations remain lock-free.","Also, our transformation approach does not utilize any hardware primitives (except a compare-and-swap operation on a single word).","We note that our transformation is generic and can be used for other lock-free context-aware data structures."],"url":"http://arxiv.org/abs/2404.10151v1","category":"cs.DC"}
{"created":"2024-04-15 21:38:26","title":"$\u03b1$-attractor potentials in loop quantum cosmology","abstract":"We perform in this work an analysis of the background dynamics for $\\alpha$-attractor models in the context of loop quantum cosmology. Particular attention is given to the determination of the duration of the inflationary phase that is preceded by the quantum bounce in these models. From an analysis of the general predictions for these models, it is shown that we can be able to put constraints in the parameter $\\alpha$ of the potentials and also on the quantum model itself, in special the Barbero-Immirzi parameter.","sentences":["We perform in this work an analysis of the background dynamics for $\\alpha$-attractor models in the context of loop quantum cosmology.","Particular attention is given to the determination of the duration of the inflationary phase that is preceded by the quantum bounce in these models.","From an analysis of the general predictions for these models, it is shown that we can be able to put constraints in the parameter $\\alpha$ of the potentials and also on the quantum model itself, in special the Barbero-Immirzi parameter."],"url":"http://arxiv.org/abs/2404.10149v1","category":"gr-qc"}
{"created":"2024-04-15 21:12:48","title":"Beyond Endoscopy via Poisson Summation for GL(2,K)","abstract":"In the early 2000's, R. Langlands proposed a strategy called Beyond Endoscopy to attack the principle of functoriality, which is one of the central questions of present day mathematics. A first step was achieved by A. Altug who worked with the group $GL(2,\\mathbb{Q})$ in a series of three papers. We generalize the first part of this result to a class of totally real number fields. In particular, we cancel the contribution of the trivial and special representations to the trace formula using an additive Poisson summation on the regular elliptic terms.","sentences":["In the early 2000's, R. Langlands proposed a strategy called Beyond Endoscopy to attack the principle of functoriality, which is one of the central questions of present day mathematics.","A first step was achieved by A. Altug who worked with the group $GL(2,\\mathbb{Q})$ in a series of three papers.","We generalize the first part of this result to a class of totally real number fields.","In particular, we cancel the contribution of the trivial and special representations to the trace formula using an additive Poisson summation on the regular elliptic terms."],"url":"http://arxiv.org/abs/2404.10139v1","category":"math.NT"}
{"created":"2024-04-15 20:53:03","title":"On the gauge invariance of the higher-derivative Yang-Mills-Chern-Simons action","abstract":"In this work, we study the perturbative generation of the gauge invariant effective action for the non-Abelian gauge field in a $(2+1)$-dimensional spacetime. We present a detailed analysis of the two, three and four-point functions in order to determine the non-Abelian Chern-Simons terms (parity odd) and Yang-Mills terms (parity even). Moreover, these terms are supplemented by the higher-derivative corrections which resulted in the Alekseev-Arbuzov-Baikov effective action (parity even) plus the higher-derivative corrections to the Chern-Simons terms (parity odd). In order to highlight some features about the perturbative generation of the effective action, we present a discussion based on the dimensional analysis, which allows us to establish the general structure of the permissible terms to guarantee the gauge invariance of the higher-derivative parts.","sentences":["In this work, we study the perturbative generation of the gauge invariant effective action for the non-Abelian gauge field in a $(2+1)$-dimensional spacetime.","We present a detailed analysis of the two, three and four-point functions in order to determine the non-Abelian Chern-Simons terms (parity odd) and Yang-Mills terms (parity even).","Moreover, these terms are supplemented by the higher-derivative corrections which resulted in the Alekseev-Arbuzov-Baikov effective action (parity even) plus the higher-derivative corrections to the Chern-Simons terms (parity odd).","In order to highlight some features about the perturbative generation of the effective action, we present a discussion based on the dimensional analysis, which allows us to establish the general structure of the permissible terms to guarantee the gauge invariance of the higher-derivative parts."],"url":"http://arxiv.org/abs/2404.10134v1","category":"hep-th"}
{"created":"2024-04-15 20:30:38","title":"Higher-curvature gravity in AdS$_3$, holographic $c$-theorems and black hole microstates","abstract":"We construct higher-derivative gravity theories in three dimensions that admit holographic $c$-theorems and exhibit a unique maximally symmetric vacuum, at arbitrary order $n$ in the curvature. We show that these theories exhibit special properties, the most salient ones being the decoupling of ghost modes around Anti-de Sitter (AdS) space, the enhancement of symmetries at linearized level, and the existence of a one-parameter generalization of the Ba\\~nados-Teitelboim-Zanelli (BTZ) black hole that, while being asymptotically AdS, is not of constant curvature but rather exhibits a curvature singularity. For such black holes, we provide a holographic derivation of their thermodynamics. This gives a microscopic picture of black hole thermodynamics for non-supersymmetric solutions, of non-constant curvature in higher-derivative theories of arbitrary order in the curvature.","sentences":["We construct higher-derivative gravity theories in three dimensions that admit holographic $c$-theorems and exhibit a unique maximally symmetric vacuum, at arbitrary order $n$ in the curvature.","We show that these theories exhibit special properties, the most salient ones being the decoupling of ghost modes around Anti-de Sitter (AdS) space, the enhancement of symmetries at linearized level, and the existence of a one-parameter generalization of the Ba\\~nados-Teitelboim-Zanelli (BTZ) black hole that, while being asymptotically AdS, is not of constant curvature but rather exhibits a curvature singularity.","For such black holes, we provide a holographic derivation of their thermodynamics.","This gives a microscopic picture of black hole thermodynamics for non-supersymmetric solutions, of non-constant curvature in higher-derivative theories of arbitrary order in the curvature."],"url":"http://arxiv.org/abs/2404.10128v1","category":"hep-th"}
{"created":"2024-04-15 20:30:37","title":"Quantum backreactions in (A)dS3 massive gravity and logarithmic asymptotic behavior","abstract":"We study the interplay between higher curvature terms and the backreaction of quantum fluctuations in 3-dimensional massive gravity in asymptotically (Anti-)de Sitter space. We focus on the theory at the special point of the parameter space where the two maximally symmetric vacua coincide. In the case of positive cosmological constant, this corresponds to the partially massless point, at which the classical theory admits de Sitter black holes and exhibits an extra conformal symmetry at linear level. We explicitly find the quantum corrected black hole geometry in the semiclassical approximation and show that it induces a relaxation of the standard asymptotic conditions. Nonetheless, the new asymptotic behavior is still preserved by an infinite-dimensional algebra, which, in addition to Virasoro, contains logarithmic supertranslations. Finally, we show that all the results we obtain for the quadratic massive gravity theory can be extended to theories including cubic and quartic terms in the curvature.","sentences":["We study the interplay between higher curvature terms and the backreaction of quantum fluctuations in 3-dimensional massive gravity in asymptotically (Anti-)de Sitter space.","We focus on the theory at the special point of the parameter space where the two maximally symmetric vacua coincide.","In the case of positive cosmological constant, this corresponds to the partially massless point, at which the classical theory admits de Sitter black holes and exhibits an extra conformal symmetry at linear level.","We explicitly find the quantum corrected black hole geometry in the semiclassical approximation and show that it induces a relaxation of the standard asymptotic conditions.","Nonetheless, the new asymptotic behavior is still preserved by an infinite-dimensional algebra, which, in addition to Virasoro, contains logarithmic supertranslations.","Finally, we show that all the results we obtain for the quadratic massive gravity theory can be extended to theories including cubic and quartic terms in the curvature."],"url":"http://arxiv.org/abs/2404.10127v1","category":"hep-th"}
{"created":"2024-04-15 20:12:50","title":"Random sampling and polynomial-free interpolation by Generalized MultiQuadrics","abstract":"We prove that interpolation matrices for Generalized MultiQuadrics (GMQ) of order greater than one are almost surely nonsingular without polynomial addition, in any dimension and with any continuous random distribution of sampling points. We also include a new class of generalized MultiQuadrics recently proposed by Buhmann and Ortmann.","sentences":["We prove that interpolation matrices for Generalized MultiQuadrics (GMQ) of order greater than one are almost surely nonsingular without polynomial addition, in any dimension and with any continuous random distribution of sampling points.","We also include a new class of generalized MultiQuadrics recently proposed by Buhmann and Ortmann."],"url":"http://arxiv.org/abs/2404.10117v1","category":"math.NA"}
{"created":"2024-04-15 20:11:09","title":"Symmetries and Thermal Radiation: A Classical Derivation of the Planck Spectrum","abstract":"A derivation of the Planck spectrum for thermal radiation is given based upon wave fluctuations within relativistic classical physics. The derivation depends crucially on thermal fluctuations existing above the fundamental inertial-frame-independent fluctuations of classical zero-point radiation. Such frame-independent zero-point fluctuations exist only in a relativistic wave theory and cannot exist in a nonrelativistic wave theory. Thus such a classical derivation of the Planck spectrum exists in a Lorentz-covariant classical theory, such as classical electrodynamics, but not in a Galilean-covariant theory where all waves are based upon material media. Classical zero-point radiation provides a purely classical alternative to quanta in the analysis of the Planck spectrum.","sentences":["A derivation of the Planck spectrum for thermal radiation is given based upon wave fluctuations within relativistic classical physics.","The derivation depends crucially on thermal fluctuations existing above the fundamental inertial-frame-independent fluctuations of classical zero-point radiation.","Such frame-independent zero-point fluctuations exist only in a relativistic wave theory and cannot exist in a nonrelativistic wave theory.","Thus such a classical derivation of the Planck spectrum exists in a Lorentz-covariant classical theory, such as classical electrodynamics, but not in a Galilean-covariant theory where all waves are based upon material media.","Classical zero-point radiation provides a purely classical alternative to quanta in the analysis of the Planck spectrum."],"url":"http://arxiv.org/abs/2404.10116v1","category":"hep-ph"}
{"created":"2024-04-15 19:38:48","title":"Statistics for the triangle density in ERGM and its mean-field approximation","abstract":"We consider the edge-triangle model (or Strauss model), and focus on the asymptotic behavior of the triangle density when the size of the graph increases to infinity. This random graph belongs to the class of exponential random graphs, which follows the statistical mechanics approach of introducing a Hamiltonian to weigh the probability measure on the state space of graphs. In the analyticity region of the free energy, we prove a law of large numbers for the triangle density. Along the critical curve, where analyticity breaks down, we show that the triangle density concentrates with high probability in a neighborhood of its typical value. A predominant part of our work is devoted to the study of a mean-field approximation of the edge-triangle model, where explicit computations are possible. In this setting we can go further, and additionally prove a standard and non-standard central limit theorem at the critical point, together with many concentration results obtained via large deviations and statistical mechanics techniques. Despite a rigorous comparison between these two models is still lacking, we believe that they are asymptotically equivalent in many respects, therefore we formulate conjectures on the edge-triangle model, partially supported by simulations, based on the mean-field investigation.","sentences":["We consider the edge-triangle model (or Strauss model), and focus on the asymptotic behavior of the triangle density when the size of the graph increases to infinity.","This random graph belongs to the class of exponential random graphs, which follows the statistical mechanics approach of introducing a Hamiltonian to weigh the probability measure on the state space of graphs.","In the analyticity region of the free energy, we prove a law of large numbers for the triangle density.","Along the critical curve, where analyticity breaks down, we show that the triangle density concentrates with high probability in a neighborhood of its typical value.","A predominant part of our work is devoted to the study of a mean-field approximation of the edge-triangle model, where explicit computations are possible.","In this setting we can go further, and additionally prove a standard and non-standard central limit theorem at the critical point, together with many concentration results obtained via large deviations and statistical mechanics techniques.","Despite a rigorous comparison between these two models is still lacking, we believe that they are asymptotically equivalent in many respects, therefore we formulate conjectures on the edge-triangle model, partially supported by simulations, based on the mean-field investigation."],"url":"http://arxiv.org/abs/2404.10106v1","category":"math.PR"}
{"created":"2024-04-15 19:32:59","title":"Chiral phase transition in soft-wall AdS/QCD with scalar-dilaton coupling","abstract":"The chiral phase boundary of nuclear matter is expected to have a critical point where the rapid crossover of lattice methods at zero chemical potential becomes a first-order phase transition. Phenomenological models based on the AdS/CFT correspondence, known as AdS/ QCD, have succeeded in capturing many features of nuclear matter, with recent progress in producing the expected critical point. We study a model that produces a critical point in the chiral phase diagram by introducing a coupling between the scalar chiral field and the dilaton. We examine the effect of the scalar-dilaton coupling on the critical point. We also study the zero-temperature chiral dynamics, which must allow for spontaneous chiral symmetry breaking in the limit of zero quark mass. We find that when the scalar-dilaton coupling is large enough to ensure correct zero-temperature chiral dynamics, a critical point is present only if the quark mass is greater than 12.8 MeV.","sentences":["The chiral phase boundary of nuclear matter is expected to have a critical point where the rapid crossover of lattice methods at zero chemical potential becomes a first-order phase transition.","Phenomenological models based on the AdS/CFT correspondence, known as AdS/ QCD, have succeeded in capturing many features of nuclear matter, with recent progress in producing the expected critical point.","We study a model that produces a critical point in the chiral phase diagram by introducing a coupling between the scalar chiral field and the dilaton.","We examine the effect of the scalar-dilaton coupling on the critical point.","We also study the zero-temperature chiral dynamics, which must allow for spontaneous chiral symmetry breaking in the limit of zero quark mass.","We find that when the scalar-dilaton coupling is large enough to ensure correct zero-temperature chiral dynamics, a critical point is present only if the quark mass is greater than 12.8 MeV."],"url":"http://arxiv.org/abs/2404.10104v1","category":"hep-ph"}
